{"id": "fmWlDfCFMR", "number": 3962, "cdate": 1757575024619, "mdate": 1759898060792, "content": {"title": "Johnson-Lindenstrauss Lemma Guided Network for Efficient 3D Medical Segmentation", "abstract": "Lightweight 3D medical image segmentation remains constrained by a fundamental \"efficiency / robustness conflict'', particularly when processing complex anatomical structures and heterogeneous modalities. In this paper, we study how to redesign the framework based on the characteristics of high-dimensional 3D images, and explore data synergy to overcome the fragile representation of lightweight methods. Our approach, VeloxSeg, begins with a deployable and extensible dual-stream CNN-Transformer architecture composed of Paired Window Attention (PWA) and Johnson-Lindenstrauss lemma-guided convolution (JLC). For each 3D image, we invoke a \"glance-and-focus'' principle, where PWA rapidly retrieves multi-scale information, and JLC ensures robust local feature extraction with minimal parameters, significantly enhancing the model's ability to operate with low computational budget. Followed by an extension of the dual-stream architecture that incorporates modal interaction into the multi-scale image-retrieval process, VeloxSeg efficiently models heterogeneous modalities. Finally, Spatially Decoupled Knowledge Transfer (SDKT) via Gram matrices injects the texture prior extracted by a self-supervised network into the segmentation network, yielding stronger representations than baselines at no extra inference cost. Experimental results on multimodal benchmarks show that VeloxSeg achieves a 26\\% Dice improvement, alongside increasing GPU throughput by 11$\\times$ and CPU by 48$\\times$.", "tldr": "This paper proposes VeloxSeg, a theory-based lightweight framework that systematically alleviates the ''efficiency / robustness conflict'' in 3D medical segmentation.", "keywords": ["Efficient Medical segmentation", "multimodal learning", "Knowledge Transfer"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3599b14c3945f4a5022dfb4e847093b5a5662e07.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces VeloxSeg, a lightweight 3D medical segmentation model that aims to keep robustness while cutting compute. It uses Paired Window Attention to capture global context with near-linear cost, a Johnson–Lindenstrauss–guided rule to set grouped-convolution sizes so features preserve distance structure, and Spatially Decoupled Knowledge Transfer with Gram-matrix supervision to inject texture priors without extra inference cost. Across some datasets, it reports competitive Dice with markedly higher GPU/CPU throughput than common baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Creative combo of (i) Paired Window Attention for global–local context with near-linear cost, (ii) a Johnson–Lindenstrauss–guided rule to size grouped convolutions, and (iii) Gram-matrix (SDKT) supervision that adds no inference overhead. The JL connection to architectural sizing is a fresh angle for lightweight 3D segmentation.\n\n* Solid empirical scope across multiple 3D medical benchmarks with both accuracy and throughput (GPU/CPU) reported alongside params and GFLOPs. Comparisons include lightweight and multimodal baselines, giving a practical sense of trade-offs."}, "weaknesses": {"value": "* **Inconsistent superiority across metrics.** The model wins on some averages but lags on others (e.g., HD95/precision/recall) with non-trivial gaps. \n\n* **Comparison fairness is under-specified.** Throughput (GPU/CPU) and accuracy comparisons may be skewed by different patch sizes, AMP/FP32, dataloader workers, I/O, pre/post-processing, and TTA.\n\n* **Statistical robustness is weak.** Results appear single-seed without CIs.\n\n* **SDKT attribution unclear.** Teacher details and potential data leakage aren’t fully specified; gains could stem from extra supervision rather than architecture.\n\n\n* **External validity.** Limited evidence under domain shift (site/scanner)."}, "questions": {"value": "* What is the primary target metric (e.g., Dice vs. HD95), and do your claims hold on that metric with mean ± std over ≥3 seeds? Several baselines beat VeloxSeg on non-primary metrics—please provide confidence intervals, exact p-values, and a Pareto plot to make the trade-off explicit.\n\n* Were patch size, AMP/FP32 policy, I/O, dataloader workers, caching, pre/post-processing, and TTA identical across methods? Please re-run baselines in the same codebase and report end-to-end latency on the same hardware, with CPU thread counts fixed.\n\n* All headline results appear single-seed. Can you report 3–5 seeds for every table, include Box/Violin plots, and discuss sensitivity to random init and data order?\n\n* What are the teacher architectures, pretext tasks, and training data? Confirm there is no split leakage. Report the extra training compute and quantify SDKT’s incremental gain vs. strong alternatives under matched budgets.\n\n* At fixed GFLOPs/latency, do PWA and JL still win? Please include budgeted comparisons and scaling curves showing how accuracy changes as compute increases.\n\n\n**I am open to changing my score based on the author's responses.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3mYXHdoXYW", "forum": "fmWlDfCFMR", "replyto": "fmWlDfCFMR", "signatures": ["ICLR.cc/2026/Conference/Submission3962/Reviewer_PE39"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3962/Reviewer_PE39"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713048070, "cdate": 1761713048070, "tmdate": 1762917115088, "mdate": 1762917115088, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VeloxSeg, a lightweight and theoretically grounded 3D medical segmentation framework designed to resolve the efficiency–robustness conflict in medical imaging. It integrates: Paired Window Attention (PWA) for efficient multi-scale context capture, Johnson–Lindenstrauss lemma-guided convolution (JLC) to preserve spatial adjacency while reducing parameters, and Spatially Decoupled Knowledge Transfer (SDKT) using Gram matrices for knowledge distillation without inference overhead. Evaluations on four multimodal datasets (AutoPET-II, Hecktor2022, BraTS2021, MSD2019) show superior Dice scores and computational efficiency compared to state-of-the-art models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Evaluations span multiple datasets and architectures (CNN, Transformer, KAN, RWKV, and Mamba hybrids). The paper includes comprehensive ablation studies, efficiency tests (GPU/CPU throughput), and shows consistent performance gains.\n\n- Strong theoretical grounding via the Johnson–Lindenstrauss lemma. The proposed JLC introduces a sound theoretical perspective into the design of lightweight segmentation networks."}, "weaknesses": {"value": "- In medical image segmentation tasks, using a fixed train/validation/test split (e.g., 6:2:2) can lead to unstable or biased results, particularly due to typically small dataset sizes and high inter-sample variability. A more robust and widely accepted approach in this domain is k-fold cross-validation, which helps assess the model’s generalizability more reliably. It is unclear whether the evaluation results reported in Table 1 are obtained using multiple folds or a single split. If only a single split is used, the reported performance may not be representative and could be sensitive to the particular partitioning. I recommend clarifying this point and, if not already done, performing evaluation using multiple folds or reporting variance across different splits.\n\n- I noticed that the baseline model (nnU-Net) is, by default, trained for 1000 epochs on the AutoPET-II dataset, whereas in your paper, all models are trained for only 300 epochs. This raises concerns about whether all models were evaluated at convergence and whether the comparisons reflect fully optimized performance for each method.\n\n- The AMOS dataset is currently one of the most widely adopted benchmarks for large-scale 3D medical image segmentation, and it also includes multimodal data (CT and MRI). It is unclear why your experiments do not include evaluation on AMOS. I recommend that the authors justify their dataset choices and discuss whether the proposed method generalizes well to other benchmarks."}, "questions": {"value": "See the Weaknesses. I am open to discussing this further during the rebuttal and will be happy to increase my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hjRkS0Oh4W", "forum": "fmWlDfCFMR", "replyto": "fmWlDfCFMR", "signatures": ["ICLR.cc/2026/Conference/Submission3962/Reviewer_L51F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3962/Reviewer_L51F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950999750, "cdate": 1761950999750, "tmdate": 1762917114879, "mdate": 1762917114879, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an efficient 3D medical image segmentation framework called VeloxSeg. The framework consists of three key components working together: 1) a theory-driven group convolution (JLC) inspired by the Johnson-Lindenstrauss (JL) lemma, serving as the encoder backbone to efficiently compress and encode features; 2) a progressive window attention (PWA) mechanism in the decoder to capture multi-scale contextual information; and 3) a size-aware dynamic knowledge transfer strategy (SDKT) to improve the model's segmentation performance for lesions of different sizes through knowledge distillation. Experimental results on public datasets such as AutoPET-II and Hecktor2022 demonstrate that the proposed method achieves competitive segmentation performance while maintaining high efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Theoretical grounding for depthwise separable convolution: The use of the JL lemma to derive a lower bound on convolution group size is a notable attempt to bridge architectural design with theoretical guarantees, particularly in preserving adjacency relationships within high-dimensional feature space.\n\n- Comprehensive ablation and visualization analysis: The paper provides detailed visualizations that help validate the contribution of each proposed component, enhancing the overall interpretability and credibility of the approach.\n\n- Good performance-efficiency balance: The most significant advantage of this paper is that it achieves state-of-the-art (SOTA) or highly competitive performance on challenging 3D medical image segmentation tasks while maintaining low computational costs. This is especially important for resource-constrained scenarios in clinical applications."}, "weaknesses": {"value": "### Mismatch Between Claimed Contributions and Experimental Evidence\n\nThe paper’s title and introduction highlight **JLC** as the core and primary contribution. However, based on the ablation study (Table 2), the impact of the JLC module does not appear to be the most significant. For instance, adding the PWA module on top of the JLC encoder brings a substantial performance boost (Dice score increases from around 55% to 61%). This suggests that the model’s success heavily relies on the strong capabilities of **PWA** and **SDKT**, as well as their synergistic interaction, rather than solely on the superiority of the JLC encoder. The current narrative may overemphasize the dominance of JLC. It is recommended that the authors reconsider and more accurately position the core contribution of the paper. Perhaps as the design of an efficient and cohesive system, rather than a single JLC module.\n\n---\n\n### Theoretical vs Empirical Gap\n\nWhile the theoretical motivation behind JLC is a notable strength, the leap from the theoretical boundary  \n$ C_{group} \\geq c_{JL} \\varepsilon^{-2} \\log N(M, v) $  \nto the empirical approximation $ N(M, v) \\approx (M \\cdot v)^{\\alpha} $ is substantial. The paper acknowledges this, but in practice, the final group size configuration (e.g., {n, 2n, 2n, 4n}) is still determined empirically by tuning α or n. This weakens the claim that the design is “fully theory-driven.” It might be more accurate to describe it as theory-inspired or theory-motivated.\n\n---\n\n### System Complexity and Research Depth\n\nVeloxSeg integrates three independent and non-trivial modules, making it a complex system. Although the ablation results demonstrate overall effectiveness, such complexity makes it difficult to fully disentangle and analyze the individual contributions of each module. For a venue like ICLR, which often values deep exploration of a single core idea, a paper that focuses exclusively on JLC—examining its representational properties, limitations, and task adaptability in depth—might be equally valuable. From a practical standpoint, this complexity could also pose challenges for researchers or practitioners attempting to adapt and fine-tune the model.\n\n---\n\n### Hyperparameter Sensitivity of the PWA Module\n\nThe design of the PWA module, particularly the choices of minimum/maximum window sizes and dilation rates, appears to be dataset-dependent (Appendix D.1 notes different setups for AutoPET-II and Hecktor2022). While such dependency is common in deep learning models, providing a sensitivity analysis on these hyperparameters and offering general configuration guidelines would greatly enhance the reproducibility and generalizability of the proposed method.\n\n---\n\n### Others\n\n- PWA: Choosing and aligning paired small and large windows is nontrivial, especially for 3D data. In real-world scenarios where modalities (e.g., PET/CT or MRI/CT) have different voxel spacings or mis-alignment, the spatial correspondence between large and small windows can become ambiguous. It remains unclear whether the proposed spatial window mechanism can effectively handle such cases.\n\n- Long-range understanding: The long-range dependency modeling capability of PWA should be further justified. Including attention visualizations would strengthen the understanding of how multi-modal information is integrated through attention mechanisms.\n\n- Depthwise convolution and adjacency: In Section 2.3, the authors state that “depthwise convolution destroys the adjacency relationship between data in the feature space” (Figure 3(b)). However, it is unclear whether this claim holds true in practice. Providing theoretical justification or visual evidence would help substantiate this assertion.\n\n- Efficiency and SDKT: The teacher training cost of the Spatially Decoupled Knowledge Transfer (SDKT) module remain concerns, as it requires an additional training phase for the teacher network."}, "questions": {"value": "### On the Design of JLC\n\nCould you elaborate on the reasoning behind choosing the approximation  \n$ N(M, v) \\approx (M \\cdot v)^{\\alpha} $?  \nDid you explore alternative functional forms or methods to estimate the complexity of the data manifold?  \nHow sensitive is the final model’s performance to the scaling factor **α** (or the base channel number n)?  \nA more detailed sensitivity analysis would greatly help readers understand the robustness of the JLC design.\n\n---\n\n### Further Clarification on JLC’s Contribution\n\nTo better distinguish the unique theoretical contribution of JLC, did you attempt to combine a more naive lightweight convolutional module, such as a standard group convolution with empirically chosen group sizes, or a depthwise separable convolution, with PWA and SDKT?  \nThis comparative experiment is crucial. It helps determine whether the key advantage truly lies in JLC’s theory-guided design principle itself, or whether any reasonably designed lightweight encoder could achieve comparable performance when integrated with strong modules like PWA and SDKT.\n\n---\n\nOthers please see the Weaknesses section for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rlbla4SitR", "forum": "fmWlDfCFMR", "replyto": "fmWlDfCFMR", "signatures": ["ICLR.cc/2026/Conference/Submission3962/Reviewer_syAZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3962/Reviewer_syAZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976549562, "cdate": 1761976549562, "tmdate": 1762917114687, "mdate": 1762917114687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose VeloxSeg, a dual stream convolution and transformed based medical image segmentation method. The authors introduce a Johnson-Lindenstrauss theorem guided setting for grouped convolutions for extracting local features at minimized cost, a Paired Window Attention block for extracting local and global features across modalities and a Spatially Decoupled Knowledge Transfer strategy for learning rich textual features. They demonstrate superior performance against 19 baselines on 2 datasets, alongside ablation experiments and analysis of their model performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis of the authors on their proposed methods and against their 19 baselines is impressive, as is their ablation experiments.\n2. The authors use volumetric and surface based metrics to demonstrate the efficacy of their methods.\n3. The authors compare against a large number of baselines including categories of models such as multimodal and lightweight methods.\n4. The use of Johnson-Lindenstrauss theorem to add grouped convolutions instead of depthwise convolutions is an interesting approach.\n5. The SKDT supervision is a novel way to add to their method."}, "weaknesses": {"value": "1. With research papers on advanced architectures, the results are significantly more convincing when compared against equivalent powerful architectures in the field, such as nnUNet ResEncL [4], STU-Net [2], MedNeXt-L [3] which have been compared in public benchmarks as described in [1, 4]. Architectures like MedNeXt are also dependent on depthwise convolutions, which the authors should demonstrate struggle to perform against their JL Convs.\n\n2. The Paired Window Attention is rather difficult to understand in the main text - a bit more explanation would have been useful. This includes while looking at the code in the appendix. The idea of modal features is also somewhat confusing - it is unclear if the authors mean features from different imaging modalities - It should also then be made clear to the readers that their modes range from 2-4 given their choice of datasets.\n\n3. The clever application of the Johnson-Lindenstrauss theorem is appreciated. However, I am not completely convinced of the idea of depthwise convs completely destroying spatial adjacency in feature learning. I agree that they would do so in principle if they were ever applied completely in isolation. But in execution, they are likely followed by a layer of pointwise convs. This is true for some of the authors’ own references - Shufflenet V2 and MedNeXt.\n\n4. I have some concerns regarding the domain generalization experiments by training on BraTS2021 and evaluating on MSD01 Brain Tumor. BraTS2021 reuses data from older BraTS cohorts while MSD01 reuses subsets of BraTS16 and 17. While I could not directly trace sources given the long chain of data reuse in BraTS, there is some concern that both datasets used by the authors might share old cohorts of BraTS 2012 or 2013 data. \n\nReferences:\n\n[1] Bassi, Pedro RAS, et al. \"Touchstone benchmark: Are we on the right way for evaluating ai algorithms for medical segmentation?.\" NeurIPS, 2024.\n\n[2] Huang, Ziyan, et al. \"Stu-net: Scalable and transferable medical image segmentation models empowered by large-scale supervised pre-training.\" arXiv preprint arXiv:2304.06716 (2023)..\n\n[3] Roy, Saikat, et al. \"Mednext: transformer-driven scaling of convnets for medical image segmentation.\" MICCAI, 2023.\n\n[4] Isensee, Fabian, et al. \"nnu-net revisited: A call for rigorous validation in 3d medical image segmentation.\" MICCAI, 2024."}, "questions": {"value": "1. I recommend that the authors consider restructuring the analogy in lines 72-90 in the introduction. It is rather cumbersome to picture and a simple scientific description would be better. I would also request that they take out references to case, clues, detectives and profilers throughout the text. It is difficult to read in my opinion.\n\n2. Comparisons against architectures such as ResEncL, STUNet and MedNeXt-L are prescient especially given the fact that the paper clearly states that the JL Convolution proposed is designed to counter - as stated by the authors - the phenomenon of “depth-wise convolution destroys the adjacency relationship between data in the feature space” which architectures like MedNeXt are based on. This would also address my concerns in the above weaknesses section regarding depthwise convs.\n\n3. Please investigate my concerns regarding the possible unintentional data leakage in the domain generalization experiments. It is acceptable to simply drop the domain generalization experiment if there is or find a replacement dataset.\n\n4. As the authors state that SKDT helps with rich texture details in feature learning, maybe an idea of HD95 in Table 2 might help their case.\n\n5. The authors need to explain PWA somewhat more in the text in a better way. It is difficult to understand as it currently stands."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "glD2q7Nx84", "forum": "fmWlDfCFMR", "replyto": "fmWlDfCFMR", "signatures": ["ICLR.cc/2026/Conference/Submission3962/Reviewer_kwQp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3962/Reviewer_kwQp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3962/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995402075, "cdate": 1761995402075, "tmdate": 1762917114439, "mdate": 1762917114439, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}