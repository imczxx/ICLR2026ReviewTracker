{"id": "SCC11m676G", "number": 9209, "cdate": 1758115286772, "mdate": 1763119157312, "content": {"title": "ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers", "abstract": "Diffusion models have demonstrated strong capabilities in generating high-quality images. However, as model size increases, the growing memory footprint and inference latency pose significant challenges for practical deployment. Recent studies in large language models (LLMs) show that rotation-based techniques can smooth outliers and enable 4-bit quantization, but these approaches often incur substantial overhead and struggle with row-wise outliers in diffusion transformers. To address these challenges, we develop a theoretical framework: we define column discrepancy to quantify imbalance in Hadamard matrices, prove that regular Hadamard matrices attain minimal discrepancy, and provide a Kronecker-based construction for powers-of-four orders, effectively controlling row- and column-wise outliers. Based on this, we propose ConvRot, a group-wise rotation-based quantization that reduces computation from quadratic to linear complexity while smoothing outliers, and ConvLinear4bit, a plug-and-play module fusing rotation, quantization, GEMM, and dequantization for W4A4 inference without retraining. Experiments on FLUX.1-dev achieve a 2.26$\\times$ speedup and 4.05$\\times$ memory reduction, while preserving image quality.", "tldr": "We propose ConvRot, a convolution-like group-wise rotation method with a plug-and-play module that enables efficient W4A4 quantization for diffusion models, achieving up to 4× DiT memory savings and 2× speedup while preserving image quality.", "keywords": ["Diffusion Transformer", "Quantization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/d6819e4f536d9e828d18536e1a868f8abf771972.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ConvRot. ConvRot applies blocked Hadamard-like transforms to improve activation quantization in DiT models. In contrast to prior works, they construct the Hadamard transform so that the sum per column/row is constant, $\\pm\\sqrt(d)$. This is valuable, as it avoids having a column (as done the standard Hadamard Transform) with only 1s---the latter creates an outlier if there is a non-zero mean in the channels."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "I like the paper overall! Some strengths:\n* The presentation of the paper is strong, with clear motivation, illustrations, and a clear method.\n* The idea of deviating from the standard FHT by including the $H_4$ (Eq. 10) term is novel and useful. 0th channel outliers due to a Naive Hadamard transform is a real problem and addressing this directly makes sense to me.\n* The results are promising for a method without calibration/QAT."}, "weaknesses": {"value": "**1. Baselines.**\n\nMy major concern are the limited results. \n(a) Arguably the closest baselines, QuaRot or FWHT applied to a smaller size (e.g. 256), is not compared against in the end-to-end results of Table 2. Recent transformation literature, e.g. FlatQuant, OstQuant, HadaNorm, are also all missing, and may well do better than ConvRot. \n(b) I'm also a bit doubtful about the speed comparison w.r.t. FWHT---why would RHT be any faster?\n\n**2. Rollout.**\n\nThe authors mention they use \"rollout\", i.e. keeping some sensitive layers in high-precision. Without this rollout, ConvRot seems to perform poorly. It is unclear how e.g. a method like QuaRot/FlatQuant would also benefit from this, consequently it is not clear whether ConvRot would outperform these methods if they were compared fairly. In particular, the poor QuaRot results of Table 1 are represented for exactly the layers that rollout keeps in high-precision anyway.\n\n**3. Computational cost**\n\n> Our first motivation comes from the high cost of existing rotation-based quantization methods, such\nas QuaRot (Ashkboos et al., 2024) and SpinQuant (Liu et al., 2024), which apply a global Hadamard\nrotation. This redistributes outliers across all channels but incurs quadratic complexity\n\nFHT may be quadratic in the number of Hadamard levels $K$, but since $K=\\log_2(dim)$, $K$ hardly grows for larger models. All in all, I think the theoretical cost of FHT is already marginal compared to matmuls, and any additional *theoretical* gains are even more marginal. In any case, the idea of using smaller blocks has been explored in prior work (e.g. DuaRot at NeurIPS 2024)\n\n___\n\nOverall, I see true value in the paper, but the current results do not provide enough evidence for it. I'm willing to improve my score if 1 and 2 are addressed in detail."}, "questions": {"value": "The problem of the Sylvester-type HT is a non-zero mean in the channels, leading to the $\\mathbf{1}$ column approximating `mean*sqrt(d)`. In Figure 3, the top-right figure seems to have a one-sided distribution for 3/4 of the channels, hence the mean would indeed be non-zero. In general, I assume many distributions do not have such off-distribution and may be quite zero-centered. Do you agree? If so, does this have implications on where ConvRot may improve over FWHT?\n\nIn any case, we could zero-center it by multiplying the channels by -1,1 (e.g. as done in FlatQuant and OstQuant), which could also approximately zero-center the channels."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vaKwrsw9Sg", "forum": "SCC11m676G", "replyto": "SCC11m676G", "signatures": ["ICLR.cc/2026/Conference/Submission9209/Reviewer_XHML"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9209/Reviewer_XHML"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760461442181, "cdate": 1760461442181, "tmdate": 1762920874138, "mdate": 1762920874138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We sincerely thank all reviewers for your valuable and constructive feedback. Upon careful reflection, we realize that the current experimental results in our submission are not sufficient to fully support our conclusions. Therefore, we have decided to withdraw the paper.\n\nSince the initial submission, we have already conducted additional experiments, and we plan to further strengthen our analysis. We hope to present a more complete and well-supported version in our future submission."}}, "id": "xqC8U5Ww4u", "forum": "SCC11m676G", "replyto": "SCC11m676G", "signatures": ["ICLR.cc/2026/Conference/Submission9209/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9209/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119156310, "cdate": 1763119156310, "tmdate": 1763119156310, "mdate": 1763119156310, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ConvRot: a simple yet effective strategy to enable 4 bits activation and weight quantization on FLUX1-dev. ConvRot introduces 2 major changes to the standard Hadamard transformations used for activation and weight quantization: 1) instead of using Sylvester Hadamard constructions, the paper motivates the use of Regular Hadamard matrices, and 2) Instead of using the FWHT algorithm, Hadamard rotations are applied using a convolutional operation. This strategy results in a substantial memory usage reduction and speed-up in the FLUX1-dev architecture without requiring any calibration or re-training procedure."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes a simple yet elegant method to deal with issues introduced by Hadamard rotations for activation and weight quantization. \n\n* The paper introduces a simple Plug and Play method that can effectively reduce the memory and compute requirements of LVMs without substantially changing the architectures nor requiring a calibration/retraining procedure."}, "weaknesses": {"value": "* The paper uses Outlier Amplitude as a metric to measure the effectiveness of the proposed method. However, the metric is not clearly introduced or motivated. \n\n* The comparison with QuaRot is not exhaustive since the benefits of using (block) Regular Hadamard instead of (block) Sylvester Hadamard are not compared directly in terms of end-to-end performance. \n\n* Some of the computational considerations behind the choice of the use of a convolutional matrix multiplication instead of the FHWT algorithms are not clear nor well motivated. In particular, Sylvester (block) Hadamard matrices can also be applied using convolutional operations. Hence, the paper could do a better job at isolating latency improvements by comparing FWHT and Convolutional matrix multiplication directly. Furthermore, the empirical increase in latency reported for FWHT on small matrices is not clearly explained. \n\n* The paper reports experiments on one LVM only. Even if similar trends are to be expected on different LVMs, additional results can strengthen the submission."}, "questions": {"value": "1. Line 282-284 the paper mentions that global Hadamard transform of order $K$ incurs in quadratic complexity. However, the paper also mentions that FWHT has complexity $K\\log K$. Can the authors clarify the reason behind this inconsistency? Is this related to the use of Tensor Cores? Can the authors clarify why butterfly-type operations are not suitable for Tensor Cores?  \n\n2. Can the authors further clarify the statement on lines 301-302? Why does the computational complexity decrease from $K^2$ to $K$ by performing group-wise operations? Shouldn’t this hold also for the Sylvester construction? \n\n3. How does the proposed outlier magnitude metric relate to the quantization error? Do the values of outlier magnitude reported in Table 1 refer to the activations or the weights? Are the values averaged across multiple rows/columns or do they refer to the maximum over the whole matrix/tensor?  \n\n4. Why does the latency for FWHT increase with decreasing block size (Table 1)? Shouldn’t all block rotations be performed in parallel?  \n\n5. The same convolutional style-operation proposed in this paper can also be applied to the Sylvester Hadamard construction. In this scenario, the latency would be identical, so the benefit of ConvRot should come from the use of Regular instead of Sylvester Hadamard matrices. The benefit of the former option is not clear since outliers and maximum error seem lower only at large block size, at which point FWHT is faster. Furthermore, there is no end2end performance comparison/ablation between the two versions in Table 2. How does (block) QuaRot compare against ConvRot for the setups reported in Table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rInVKJq0tM", "forum": "SCC11m676G", "replyto": "SCC11m676G", "signatures": ["ICLR.cc/2026/Conference/Submission9209/Reviewer_7z1x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9209/Reviewer_7z1x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761127157569, "cdate": 1761127157569, "tmdate": 1762920873623, "mdate": 1762920873623, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ConvRot, a training-free W4A4 quantization method for diffusion transformers. The method uses a principled, Kronecker-based Hadamard rotation to balance weights, guided by a novel discrepancy metric. For efficiency, ConvRot employs group-wise rotations and a fused `ConvLinear4b` kernel, achieving up to a 2.26× speedup and 4.05× memory reduction on models like FLUX-1-dev with minimal impact on image quality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical design with group-wise rotations that reduce computational cost and enable a plug-and-play, training-free W4A4 quantization scheme.\n2. The method does not require a calibration dataset, which simplifies implementation.\n3. Systems-level contribution via the `ConvLinear4b` kernel, bridging algorithmic design with practical engineering to foster adoption."}, "weaknesses": {"value": "1. The evaluation scope is limited, focusing primarily on FLUX-1-dev. Broader testing across more diffusion transformer variants would be necessary to establish the generality of the method.\n2. The paper lacks a detailed, operator-level breakdown of performance. A thorough analysis of time and memory overheads for rotation, quantization/dequantization, and GEMM would provide a clearer picture of the practical costs.\n3. The paper does not sufficiently contrast the proposed technique with existing methods, such as SVDQuant, making it difficult to assess its relative advantages and contributions."}, "questions": {"value": "1. What criteria are used to determine which layers are kept in FP16/BF16 precision and which are quantized?\n2. Could you provide results on additional models to better demonstrate the method's generality?\n3. How does the method perform with FP4 quantization, and would any modifications be required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BmQB3IKzjf", "forum": "SCC11m676G", "replyto": "SCC11m676G", "signatures": ["ICLR.cc/2026/Conference/Submission9209/Reviewer_hUxq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9209/Reviewer_hUxq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761760946294, "cdate": 1761760946294, "tmdate": 1762920873026, "mdate": 1762920873026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a drawback of previous rotation-based quantization methods that could amplify row-wise outliers in certain circumstances. They propose a group-wise rotation-based quantization algorithm that uses a regular Hadamard Transform to smooth both row-wise and column-wise outliers and reduce the computational complexity. They also implement a kernel to fuse rotation, quantization, GEMM, and dequantization, and demonstrate a 2.26x speedup over the BF16 baseline."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This work addresses a known failure mode of global Hadamards in DiTs.\n- The quantization method is demonstrated on real GPUs and can provide end-to-end speedup."}, "weaknesses": {"value": "- The quality and performance metrics still fall behind the state-of-the-art quantization method, SVDQuant.\n- Although they claimed their method could smooth both row-wise and column-wise outliers, the evaluation results still show a significant quality drop if proj_out is quantized to 4 bits, limiting the usefulness of the proposed method.\n- Only one workload (FLUX.1-dev) is used to evaluate the results.\n- Code is not provided."}, "questions": {"value": "- Please provide an analysis of the performance and quality when comparing to SVDQuant. Is this an engineering issue or a fundamental limitation of the proposed algorithm? Can QuaRot combine with SVDQuant to provide higher performance or quality?\n- How does the proposed method work on newer quantization schemes (like NVFP4 on latest generation GPUs)?\n- How does the kernel fusion of quantization and 4-bit GEMM work? I think the quantization results of a single activation tile are reused by multiple GEMM tiles. Does this fusion lead to duplicated computation and lower the performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pBdduuqdxh", "forum": "SCC11m676G", "replyto": "SCC11m676G", "signatures": ["ICLR.cc/2026/Conference/Submission9209/Reviewer_TuoY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9209/Reviewer_TuoY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9209/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762131855969, "cdate": 1762131855969, "tmdate": 1762920872729, "mdate": 1762920872729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}