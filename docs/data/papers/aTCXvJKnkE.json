{"id": "aTCXvJKnkE", "number": 14687, "cdate": 1758241748657, "mdate": 1759897354871, "content": {"title": "Towards Strategic Persuasion with Language Models", "abstract": "Large language models (LLMs) have demonstrated strong persuasive capabilities comparable to those of humans, offering promising benefits while raising societal concerns about their deployment. However, systematically evaluating persuasive capabilities is inherently challenging, as the effectiveness of persuasion among humans varies significantly across different domains. In this paper, we take a theory-driven approach to provide a scalable and principled framework to measure the persuasive capabilities of LLMs in strategic interactions. Grounded in the Bayesian Persuasion (BP) framework, we repurpose existing human–human persuasion datasets to construct environments for evaluating and training LLMs in strategic persuasion. Our results reveal that frontier models can consistently achieve high persuasion gains and exhibit sophisticated persuasion strategies that align with theoretical predictions. Building on this, we use reinforcement learning to train LLMs for strategic persuasion in our environments. Our results also demonstrate that even small LLMs can obtain significantly higher persuasion gains through reinforcement learning.", "tldr": "", "keywords": ["Large Language Models", "Strategic Behavior", "Information Design"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/21eb230ddbbebe386be6db122c392f383bdbcbc4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper is about measuring the persuasive capabilities of LLMs. To begin with, motivated by the established theory of Bayesian Persuasion, a framework is further adopted for the case of LLMs. Focusing on opinion change tasks, a benchmark is provided for both static and dynamic settings. A reinforcement learning type of training method is also considered to further improve the capabilities. With the proposed metric, frontier LLMs are shown to have strong capabilities in strategic persuasion. The RL framework is also shown to be promising in training specific small LLM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "> **Originality**\n- The paper, inspired by the Bayesian Persuasion theory, provides a scalable and systematic approach to measuring persuasion capability of LLMs.\n\n> **Quality**\n- This work incorporates a group of frontier LLMs with various sizes in the experiments for the evaluation of persuasion gains.\n- There is follow-up numerical analysis on prior, semantic diversity and persuasion strategies."}, "weaknesses": {"value": "> **Originality**\n- It would be helpful if the authors clarify whether the framework in Section 2.2 are based on the terms in former literature or proposed in this work.\n\n> **Quality**\n- Line 234: Is the size of human participants (45) large enough to draw the conclusions?\n\n> **Clarity**\n- Line 148: should the expectation be taken over $\\omega\\sim\\mu_s$?\n- The distribution of posteriors, $\\tau$, could be explicitly defined with more details.\n- The review of Dynamic Bayesian Persuasion could benefit from providing more details, with mathematical description of the target.\n\n> **Significance**\n- According to Line 268, could the authors clarify whether the RL framework is only concerning static persuasion? If so, the contribution covered in the abstract might need a revision."}, "questions": {"value": "- Line 191: why, compared to the introduction in Line 162-170, notations are capitalized? e.g. the state $\\omega_t$ changed to $\\Omega_t$\n- Line 215: How is the function $g$ defined in general (if not an index retrieval)?\n- What is the reason in fixing Llama-3.1-8B-Instruct (or models with similar sizes) as Receiver models? Would using other models further restrict the persuasion gains or the improvement from RL framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JFZzWDKDGg", "forum": "aTCXvJKnkE", "replyto": "aTCXvJKnkE", "signatures": ["ICLR.cc/2026/Conference/Submission14687/Reviewer_rDdu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14687/Reviewer_rDdu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889115241, "cdate": 1761889115241, "tmdate": 1762925055664, "mdate": 1762925055664, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a theoretically grounded framework to study and train persuasive behavior in large language models (LLMs) using Bayesian Persuasion (BP) as the conceptual backbone. The authors design persuasion games, where a Sender model strategically communicates to influence a Receiver model’s posterior beliefs. Persuasion performance is measured through persuasion gains (utility improvement) and information-theoretic signals (conditional mutual information).\nThey evaluate across multiple datasets (Anthropic, DDO, Perspectrum, CMV) and show that:\n\nLarger models (e.g., Claude 3.7, DeepSeek-R1) are more persuasive.\nRL fine-tuning (PPO, GRPO) can enhance persuasion effectiveness, especially in smaller models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The conceputal Bayesian Persuasion framing isan elegant and principled conceptual framework rarely explored in NLP.\n2. Scalable LLM interactions reduce dependence on costly human evaluations\n3. Multi-dataset and multi-model experiments clearly demonstrate consistent persuasion trends with model scale and fine-tuning."}, "weaknesses": {"value": "1. The LLM-as-Bayesian-updater assumption is central but weakly validated; human alignment or calibration tests are minimal. Singh et al shows that LLMs as a judge method is weak\n2. Prior arts (Singh et al, Hackenberg et al) have shown the effect of finetuning / model size on persuasive capabilities, in this view I woul like the authors to describe in more details how their findings are novel or what's the conceptual difference"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hmzXLg1GEB", "forum": "aTCXvJKnkE", "replyto": "aTCXvJKnkE", "signatures": ["ICLR.cc/2026/Conference/Submission14687/Reviewer_gCBZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14687/Reviewer_gCBZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996067517, "cdate": 1761996067517, "tmdate": 1762925055261, "mdate": 1762925055261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work developed a theory-driven framework to study how LLMs perform strategic persuasion, grounding its approach in Bayesian persuasion theory. It repurposes several human-human debate datasets (e.g., CMV, DDO, Perspectrum, and Anthropic) to simulate persuasion games between two LLMs, a “Sender” trying to persuade and a “Receiver” updating beliefs. The authors measure persuasion gain and signaling behaviors, and they also fine-tune smaller models using reinforcement learning (PPO and GRPO) to improve persuasion effectiveness. The experiments show that larger models (e.g., DeepSeek-R1, Claude 3.7 Sonnet, and GPT-4o) achieve higher persuasion gains, while smaller models can catch up with training. In summary, the work claims to offer a scalable and principled way to quantify and enhance persuasive capabilities in LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "+ The work introduces a novel and principled bridge between Bayesian persuasion theory and LLM evaluation, giving it a strong conceptual foundation.\n\n+ The simulated Sender–Receiver setup provides a clean and scalable framework for studying persuasion without involving human participants.\n\n+ The work reuses multiple debate datasets to develop diverse environments, increasing coverage and reproducibility.\n\n+ The RL experiments are well-motivated and show clear improvement trends, even in small models.\n\n+ The analysis section connects empirical results with theoretical predictions, such as showing the effectiveness of persuasion for moderate priors."}, "weaknesses": {"value": "- The entire evaluation relies on LLM-to-LLM simulations rather than human subjects, introducing inherent limitations on realism.\n\n- The use of Bayesian persuasion as the theoretical backbone is somewhat inflexible, as real-world persuasion often involves emotion, identity, and irrational behaviors that this model can’t capture.\n\n- Discussing the ethical implications of optimizing LLMs for persuasion only shows limitations, particularly because RL could make them more manipulative.\n\n- The reported persuasion “gains” (like +0.23 or +1.27) are hard to interpret; what does that actually mean in human terms?\n\n- The paper treats the Receiver LLM as a rational Bayesian updater, which is questionable given how language models actually process information (hidden).\n\n- The human validation study (with 45 Prolific annotators) is small and focuses mainly on plausibility, not on persuasion effectiveness or realism.\n\n- The RL setup seems simplified, with few details on reward design or stability; could the gains be due to overfitting to Receiver quirks?\n\n- The interpretation of “strategic information disclosure” could be more carefully supported, since it relies on mutual information as a proxy rather than direct analysis of message content or strategy patterns.\n\n- The results show that even small models can become better persuaders, but there is little discussion about the societal consequences of making persuasion more efficient.\n\n- The writing occasionally mixes theoretical and empirical points without clear transitions, which can make the paper feel dense and slightly unfocused to some extent."}, "questions": {"value": "How exactly were \"persuasion gains\" computed and normalized across datasets with different scales and topics?\n\nTo what extent do Receiver models actually simulate belief updating rather than just mimicking cooperative agreement?\n\nDid the RL models learn general persuasion principles, or did they just adapt to the Receiver’s specific response patterns?\n\nHow are dynamic persuasion \"turns\" evaluated? Does a higher number of turns always increase gains, or can models over-persuade?\n\nHow would this framework extend to human-in-the-loop validation, especially given the ethical limits of testing harmful persuasion topics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper includes a small human annotation study (45 Prolific participants) to validate LLM belief-updating plausibility. Participants only evaluated pre-generated text and were not exposed to harmful or deceptive content. The study presents minimal ethical risk and appears to follow standard crowd-sourcing practices. No formal ethics flag is necessary, but the authors could clarify IRB or platform-level approval."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "siMW23lsMZ", "forum": "aTCXvJKnkE", "replyto": "aTCXvJKnkE", "signatures": ["ICLR.cc/2026/Conference/Submission14687/Reviewer_FWCT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14687/Reviewer_FWCT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032552263, "cdate": 1762032552263, "tmdate": 1762925054812, "mdate": 1762925054812, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper frames LLM persuasion as Bayesian Persuasion (BP): a sender strategically reveals information to shift a receiver’s beliefs and actions. It repurposes multiple human–human persuasion datasets to build static (1-turn) and dynamic (multi-turn) evaluation environments, measuring “persuasion gains” aligned with BP theory. Experiments show larger, frontier models and multi-turn interaction yield substantially higher gains; small models further improve via reinforcement learning (PPO/GRPO). The work offers a principled benchmark, analyses, and an RL recipe to enhance strategic persuasion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Integration of Theory: The paper’s strongest aspect is its original framework grounding LLM persuasion in Bayesian persuasion theory. This bridges a well-established game-theoretic model with modern LLM evaluation, bringing conceptual rigor to a domain that lacked principled metrics. By using persuasion gain and information design notions, the authors provide a clear, unified lens to measure and reason about persuasive behavior across different contexts, which is a significant innovation.\n\n2. Propose new benchmark: Propose a new benchmark for strategic persuasion by reusing and consolidating multiple human debate/persuasion datasets.\n\n3. Perform thorough empirical evaluation: The experimental evaluation is comprehensive and insightful. The authors test a wide range of models – from a 7B open model up to GPT-4/Claude – under identical conditions, providing a clear picture of how model scale and architecture affect persuasive ability.\n\n4. RL significantly boosts small models and generalizes across receivers. A 3B Sender trained with PPO/GRPO on ~2.7k instances improves meaningfully over base in both static and dynamic settings, and performance transfers when paired with other Receivers (Mistral-7B, Qwen-7B) not seen in training. This suggests the agent really learns strategy."}, "weaknesses": {"value": "1. Metric and Receiver calibration. Persuasion gain is measured via Likert shifts from a particular Receiver; Table 3 shows Receivers differ notably in susceptibility (e.g., Mistral-7B vs Llama-8B), raising questions about and cross-Receiver comparability of scores. Some normalization/robustness analysis would help.\n\n2. Missing supervised fine-tuning baselines & strategy ablations. The Sender is improved via RL (PPO/GRPO with KL regularization), but there’s no direct comparison to supervised fine-tuning on the same instances or to non-strategic baselines."}, "questions": {"value": "1. RL vs supervised fine-tuning. On the same ~2.7k instances, how would instruction/SFT compare to PPO/GRPO? An SFT baseline (and a combined SFT→RL variant) would clarify whether policy optimization is key versus exposure to data alone.\n\n2. Safety auditing during RL. Did your RL Sender ever produce undesirable tactics (exaggeration, undue pressure) despite the truthful framing? Beyond a KL penalty, were there filters or audits to detect and penalize such behaviors during training or eval? Examples and frequency would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3lH7vBshM4", "forum": "aTCXvJKnkE", "replyto": "aTCXvJKnkE", "signatures": ["ICLR.cc/2026/Conference/Submission14687/Reviewer_paHu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14687/Reviewer_paHu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14687/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762321968730, "cdate": 1762321968730, "tmdate": 1762925054048, "mdate": 1762925054048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}