{"id": "yTJFPe5dBo", "number": 13293, "cdate": 1758216079163, "mdate": 1762932268231, "content": {"title": "Balance Your Move: Investigating Text to Motion from a Signal Density Measuring View", "abstract": "Text-to-motion generation has advanced significantly with diffusion models. However, existing approaches typically assume that text and motion guidance contribute equally across time. This oversimplification often leads to either semantically faithful but unnatural motions or smooth trajectories that drift from the intended meaning. To address these limitations, we introduce the Information-Balanced Motion Generator (IBMG), a simple yet effective framework that dynamically balances text and motion guidance through the perspective of information density. Specifically, we define text information density by measuring the semantic alignment between motion segments and textual tokens, while motion information density is captured via temporal variation across segments. These distributions are encoded into an information-balance embedding that adaptively modulates the relative influence of text and motion during generation, thereby balancing semantic fidelity with motion naturalness. Extensive experiments across two benchmarks and three backbones demonstrate that IBMG consistently improves generation quality. On HumanML3D, it reduces FID by 60.5\\% and lowers trajectory error by 4.3\\% compared to baselines. These results highlight information density as a key principle for harmonizing semantic fidelity with temporal coherence, establishing IBMG as a competitive baseline for text-to-motion generation.", "tldr": "This paper introduces a simple yet effective framework that leverages information density to dynamically balance text and motion guidance, establishing a competitive baseline for text-to-motion generation.", "keywords": ["Motion Generation", "Information Density", "Information Distribution Mining"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c36ba9d66cca9f7cd61eeda0452dd8939ce6572e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes SBMG, a motion generation method that dynamically balances textual and trajectory signals based on their temporal information density. It significantly improves semantic alignment and motion control accuracy in multimodal motion generation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1.It explains the relationship between signal density and control effectiveness.\n2.The proposed method achieves state-of-the-art performance."}, "weaknesses": {"value": "Major issues:\n\n1.What exactly are static or simple fusion strategies? Please provide an explanation.\n2.Provide a quantitative example of trajectory deviation or semantic drift caused when one signal dominates. Conduct both quantitative and qualitative analyses of the relationship between signal density and control effectiveness to justify the value of this contribution.\n3.In Equation 1, the paper defines it as a score computation, but after multiplying the softmax-normalized attention scores with X, it becomes a feature rather than the intended score. Therefore, the definitions of dense and sparse semantic scenarios are questionable.\n4.Why is the TSB module not mentioned in the abstract or introduction? Does TSB contribute to behavior improvement? Please include relevant experiments.\n5.What are the limitations of the proposed method? When textual descriptions become complex, does the information density theory still hold?\n\nMinor issues:\n\n1.In Figure 3, do the different colored text boxes represent different tokens, and how are they related to semantic signal density?\n\n2.Please bold vectors in equations, use commas “,” in the middle of equations, and periods “.” at the end.\n\n3.What is SDM, and why can it extract information density?\n\n4.Why are the drawn framework diagrams not cited?"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pamncT147B", "forum": "yTJFPe5dBo", "replyto": "yTJFPe5dBo", "signatures": ["ICLR.cc/2026/Conference/Submission13293/Reviewer_oXUy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13293/Reviewer_oXUy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666756219, "cdate": 1761666756219, "tmdate": 1762923962614, "mdate": 1762923962614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "vWYrBcQz8t", "forum": "yTJFPe5dBo", "replyto": "yTJFPe5dBo", "signatures": ["ICLR.cc/2026/Conference/Submission13293/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13293/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762932212673, "cdate": 1762932212673, "tmdate": 1762932212673, "mdate": 1762932212673, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that prior work mostly fuses or averages trajectory and textual signations when injecting them into the model, which leads to conflicts like following the text but drifting off the trajectory or tracking the path but losing semantics. The authors propose SBMG, which has a Signal Density Measuring (SDM) module that at each timestep t estimates how dense the conditioning information is, and also a Text–Trajectory Signal Balancing (TSB) module that uses those dense/sparse signals to generate a gating factor to modulate both conditioning information. The proposed module can be easily plugged into multiple backbones (OmniControl, MoMask, SALAD). And experimental results indicate better performance in metrics like FID and R-Precision on standard motion generation benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The perspective to rethink about condition injection is quite interesting. Though previous works such as SALAD, MARDM, CLOSD, and ACMDM also test different conditioning injection methods (from in-context to cross attention or AdaLN) that significantly enhance, I believe this is the first work to rethink in an information density way.\n\n2. The controlled experiment results show great potential for this method, as using the same baseline, using SBMG consistently outperforms their baseline methods.\n\n[SALAD]Hong, Seokhyeon, et al. \"SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing.\" CVPR 2025\n\n[CLoSD]Tevet, Guy, et al. \"CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control.\" ICLR 2025.\n\n[MARDM]Meng, Zichong, et al. \"Rethinking Diffusion for Text-Driven Human Motion Generation.\" CVPR 2025.\n\n[ACMDM]Meng, Zichong, et al. \"Absolute Coordinates Make Motion Generation Easy.\" ArXiv 2025"}, "weaknesses": {"value": "1. Many works (MARDM, MotionStreamer) have questioned the validity of the original evaluation method from HumanML3D, as the method significantly surpasses ground truth, while the visual quality does not align with the evaluation results. This is a valid concern, as methods such as the proposed one in the paper and SALAD are way better than the ground truth itself, but SALAD's visualization still exhibits noticeable jittering shown on their website. Would the author follow new evaluation methods for fairer comparison?\n\n2. Though noted as optional by the conference, video results are an important part of motion generation methods. It is quite hard to tell if the method improves qualitatively without any video results.\n\n3. There are many new methods after OmniControl that enhance motion control tasks, such as MotionLCMV2, MaskedControl, and ACMDM. Some of these methods show flawless control with original injection methods. I believe including a comparison with newer methods would make the experiment more complete.\n\n[MotionStreamer]Xiao, Lixing, et al. \"MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space.\" ICCV 2025.\n\n[MotionLCMV2]Dai, Wenxun, et al. \"Real-time Controllable Motion Generation via Latent Consistency Model.\" ArXiv 2024\n\n[MaskControl]Pinyoanuntapong, Ekkasit, et al. \"Controlmm: Controllable masked motion generation.\" ICCV 2025"}, "questions": {"value": "1. I am curious for an explanation from the author on SALAD(ours), where R-Precision is significantly higher than ground truth itself, but FID is much worse. Intuitively, this means the generated motions are much better following text, but are not the same as the ground truth motion. This counters common sense in a way, since better text following should usually mean similar to real motions, as HumanML3D data are real-human-collected data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mIOoGw0tUZ", "forum": "yTJFPe5dBo", "replyto": "yTJFPe5dBo", "signatures": ["ICLR.cc/2026/Conference/Submission13293/Reviewer_rGUn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13293/Reviewer_rGUn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936594121, "cdate": 1761936594121, "tmdate": 1762923962348, "mdate": 1762923962348, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Signal Density Measuring (SDM) to detect dense/sparse semantic periods from cross-attention and a Time-Series Balancing (TSB) block that dynamically trades off text vs. trajectory through time. The proposed modules are plug-and-play and are inserted into MoMask, SALAD, and OmniControl with reported gains on HumanML3D/KIT-ML."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple, backbone-agnostic plug-and-play modules that can be applied to MoMask/SALAD/OmniControl without architecture changes.\n\n2. The motivation of the paper and the method are intuitive and clear, that is to balance text and trajectory.\n\n3. Good quantitative results that improve baseline models."}, "weaknesses": {"value": "The major concern is for the experiment.\n\n1. No demo videos in the supplementary file. Quantitative metrics in text-to-motion is proven to be fragile and sometimes misaligned with human judgment. For motion, demo videos are necessary. I don’t see any supplementary videos, which makes it hard to judge the actual quality. The R-Precision being even higher than the ground-truth is meaningless and cannot reflect visual quality.\n\n2. Missing strong baseline results. Compare against recent, stronger works (MARDM [1], MotionStreamer [2], MotionLCM v2 [3]) for the text-to-motion generation task, and MaskControl [4] for the control task, with both qualitative and quantitative results.\n\n3. The main contribution of this paper is balancing text and trajectory, yet the main Table 1 for MoMask/SALAD reports only text-based metrics. I'm a bit confused why balancing text and trajectory can improve the T2M generation itself (If I'm wrong the author can correct me on this point)\n\n4. From the qualitative evaluation in the appendix, I can’t see any clear difference between MoMask and Ours. I feel like the MoMask results are quite good. This echoes point 1 that video results are required.\n\nReferences:\n\n[1] Rethinking Diffusion for Text-Driven Human Motion Generation\n\n[2] MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space\n\n[3] MotionLCM-V2: Improved Compression Rate for Multi-Latent-Token Diffusion\n\n[4] MaskControl: Spatio-Temporal Control for Masked Motion Synthesis"}, "questions": {"value": "Refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UOl0tVZEXY", "forum": "yTJFPe5dBo", "replyto": "yTJFPe5dBo", "signatures": ["ICLR.cc/2026/Conference/Submission13293/Reviewer_bjoX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13293/Reviewer_bjoX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971467243, "cdate": 1761971467243, "tmdate": 1762923961938, "mdate": 1762923961938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an innovative framework for human motion generative with both textual description and trajectory control. These two signals are processed with balaced information density to improve the generation quality."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper frames the relationship between text and trajectory as 'text-primary with trajectory as a generation-time control signal' which is conceptually clean and potentially general."}, "weaknesses": {"value": "1. Introduction issues. The works cited at L36 are not methods that use only text as input; L39 should cite relevant prior work but currently does not. PriorMDM is not mentioned at L50, yet it is later singled out for targeted analysis. In addition, the discussion is also not rigorous. For example, it omits the fine-tuning stage of PriorMDM and overlooks more recent developments such as InterControl's gradient-based correction. Overall, the exposition is loose and unconvincing.\n\n2. The quantitative claims in the abstract (e.g., 60.5% FID reduction) do not reconcile with Table 1 under MoMask/SALAD backbones; please provide exact computations and per-seed raw results, or revise the claim accordingly.\n\n3. The main text centers on MoMask, with cross-backbone evidence limited; moving OmniControl to the appendix reinforces the impression that the approach is MoMask-specific. Qualitative examples also focus mainly on MoMask. The main motivation is the balancing of textual control and spatial control. Hence, it is strange to use MoMask as the main baseline network. In addition, the authors should clarify more on how the proposed method can help the method with sole textual input.\n\n4. More qualitative results are needed. Currently, there are only four examples in the appendix and only on MoMask (with does not receive sptial signal). The authors should supply more examples on OmniControl and provide comprehensive user study to support their claims."}, "questions": {"value": "1. In table 6, is it correct that the performance of OmniControl on \"Left Hand\" setting is exactly same as that on \"Right Hand\" setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZDGpvYF5Tq", "forum": "yTJFPe5dBo", "replyto": "yTJFPe5dBo", "signatures": ["ICLR.cc/2026/Conference/Submission13293/Reviewer_7QtU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13293/Reviewer_7QtU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990127474, "cdate": 1761990127474, "tmdate": 1762923961613, "mdate": 1762923961613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}