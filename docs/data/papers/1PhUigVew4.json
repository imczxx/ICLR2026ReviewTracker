{"id": "1PhUigVew4", "number": 17749, "cdate": 1758280133127, "mdate": 1759897156155, "content": {"title": "Parallax: Efficient LLM Inference Service over Decentralized Environment", "abstract": "Deploying a large language model (LLM) inference service remains costly because centralized serving depends on specialized GPU clusters and high‑bandwidth interconnects in datacenters. An appealing alternative is to leverage collaborative decentralized GPU pools. However, heterogeneity in GPU and limited interconnected network bandwidth, along with potentially dynamic availability, make efficient scheduling the central challenge in this scenario. In this paper, we present Parallax, a decentralized LLM serving system that turns a pool of heterogeneous GPUs into an efficient inference platform via a two‑phase scheduler. Parallax decomposes planning into (i) model allocation, which places layers of each replica across diverse GPUs to jointly optimize latency and throughput under memory and link‑bandwidth constraints, and (ii) request‑time GPU pipeline selection, which stitches layers from different replicas into end‑to‑end execution chains that balance load and adapt to current conditions. We implement Parallax and evaluate it on open‑source LLMs deployed over real volunteer nodes. Parallax consistently reduces latency and increases throughput relative to decentralized baselines, demonstrating that principled scheduling can make volunteer compute a practical, affordable substrate for LLM inference.", "tldr": "", "keywords": ["Distributed", "Parallel", "and Cluster Computing"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e4e8ecae5bdf6fdb8cb985fd0782d0b09e2d88a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PARALLAX, a system that uses a novel two-phase scheduler to enable efficient, low-latency LLM inference across decentralized, heterogeneous GPUs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and logically presented, making it clear and easy to follow.\n2. It rightly identifies and addresses the necessity of dynamic membership management, a critical and practical challenge in decentralized inference systems."}, "weaknesses": {"value": "1. The promising results could be strengthened by more extensive experiments across a wider variety of scenarios.\n2. The strategy for handling multiple concurrent requests on a single GPU could be clarified for the reader.\n3. A helpful future step would be an ablation study to understand the individual impact of Phase 1 and Phase 2."}, "questions": {"value": "Could you please clarify how the KV Cache is managed with k replications? Is this handled within Phase II of the proposed framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j4O9To6F2z", "forum": "1PhUigVew4", "replyto": "1PhUigVew4", "signatures": ["ICLR.cc/2026/Conference/Submission17749/Reviewer_74hi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17749/Reviewer_74hi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828027677, "cdate": 1761828027677, "tmdate": 1762927591256, "mdate": 1762927591256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Parallax, a system for decentralized (and heterogeneous-cluster) language model inference. Parallax applies a two-phase scheduling, where the first phase is responsible for the allocation of pipeline stages onto each device, while the second phase select the GPU replica of each PP stage to execute each request."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well organized and easy to follow.\n- The contribution of decentralized and dynamic cluster is significant."}, "weaknesses": {"value": "- The two phase scheduling algorithm looks very similar to the idea in Helix[1], which also first plan a model placement, then schedule a per request pipeline parallel. It will be more supportive if the paper can discuss the fundamental difference with Helix, and show the performance gain.\n  - Helix claims to support a device placement that supports each model layer having different number of replicas (Figure 3 in [1]), while the Parallax Phase 1 schedules in unit of pipeline replicas, so each model layer has the same number of replicas (same DP). Does this prevent the search space from optimal?\n  - Alike Parallax's Phase-2 scheduling, Helix also employs a runtime determined per-request pipeline path selection. Are they share the same search space and scheduling metrics?\n- The experiment is only tested on 7 devices (at most 6 replicas of the BF16 model, even if assuming no KV cache space is needed). Using more GPUs will be more supportive to show the advantage of Parallax against baselines.\n\n[1] Mei, Yixuan, et al. \"Helix: Serving large language models over heterogeneous gpus and network via max-flow.\" Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1. 2025."}, "questions": {"value": "- All citations miss brackets\n- Given the intra-pipeline rebalancing in Phase 1, the previous step's DP exploration degenerates to simply partitioning devices to reach a balanced compute capacity among data parallel replicas.\n- When rebalancing the cluster with many newly joined and exited devices, is the current layer partition take into consideration, so that the weight movement is minimal?\n- What is the expected result if there are more type of GPUs (e.g. weaker consumer level GPUs, elder generation cloud GPUs like L4 or H20) used in Parallax?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DMEP8Kxt6u", "forum": "1PhUigVew4", "replyto": "1PhUigVew4", "signatures": ["ICLR.cc/2026/Conference/Submission17749/Reviewer_rTf1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17749/Reviewer_rTf1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949060145, "cdate": 1761949060145, "tmdate": 1762927590807, "mdate": 1762927590807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the opportunity to leverage collaborative decentralized GPU pools. The paper notes that it essentially comes at a cost of limited interconnect bandwidth and dynamic availability that makes it challenging for efficient scheduling.\nThe paper proposes Parallax, a two-phase scheduler. First phase is model allocation and second phase is GPU pipeline chain selection. Model allocation determines how to effectively distribute across diverse set of GPUs using dynamic programming with some heuristics. It also uses a water-filling algorithm for rebalancing layers intra-pipeline. When the request comes in, the GPU pipeline chain selection phase again uses dynamic programming to sweep over DAG of model allocation to find the path with minimum latency based on prior stats.\nThe paper is evaluated on a testbed that comprises 5 RTX 5090 and 2 RTX 4090 machines with inter-machine communication latency of 10 ms over a public network."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper makes a good observation that specialized GPU clusters and high-bandwidth interconnects could be prohibitive for some users.\n* Nice use of algorithms to solve the distribution problem."}, "weaknesses": {"value": "* The algorithm seems to have a narrow focus of pipeline parallelism.\n* Rather shallow evaluation that only covers 1 network, 1 baseline, and rather simplified testbed seems to give limited confirmation about the generalization of the approach."}, "questions": {"value": "* How can this be extended to other forms of parallelism?\n* How does this perform for a serving scenario with multi-user with dynamic requests?\n* How does this perform for other models such as Llama, Deepseek, and other widely explored LLMs. In fact, OpenAI released OSS which could also be a nice model to explore.\n* The paper mentions that the maximum inter-machine communication latency was 10 ms. Can you provide evidence on how this can be generalized to environments where it is higher or where latency has large variation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "49AlWONiLo", "forum": "1PhUigVew4", "replyto": "1PhUigVew4", "signatures": ["ICLR.cc/2026/Conference/Submission17749/Reviewer_FeQq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17749/Reviewer_FeQq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980929933, "cdate": 1761980929933, "tmdate": 1762927589345, "mdate": 1762927589345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Current state of the art for distributed parallel serving is difficult and most inference systems are built assuming fast collective communication. Parallax builds a allocation strategy that attempts to balance the work/cost per gpu as a form of pipeline parallelism. They also attempt to use a form of batching that selects pipeline paths based on the work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The DP based/water-filling inorder to better allocate nodes within/across region was interesting.\n2. It handles GPU’s leaving/joining well using the distributed hash table, which seem like a core requirement for working with decentralized GPUs\n3. The scheduler scalability at even up to 256 GPUs seem strong."}, "weaknesses": {"value": "1. I’d like a bit more ablation on the results of Parallax. Possibly a description of a baseline without phase1/phase2, especially because the engine implementations could be varied. \n2. The paper describes being able to handle failures of GPUs/arrivals but there are no results demonstrating the effect of these."}, "questions": {"value": "1. Can you add a bit more ablation on the breakdown of the results? I think this make the results of the paper much stronger\n2. Can you add an experiment that simulates a gpu failure/arrival? This would be a nice to have since it was mentioned in the writing but not in the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IrBfPzbNAl", "forum": "1PhUigVew4", "replyto": "1PhUigVew4", "signatures": ["ICLR.cc/2026/Conference/Submission17749/Reviewer_o2Tb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17749/Reviewer_o2Tb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17749/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762736113897, "cdate": 1762736113897, "tmdate": 1762927588578, "mdate": 1762927588578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}