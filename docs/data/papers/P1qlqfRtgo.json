{"id": "P1qlqfRtgo", "number": 17115, "cdate": 1758272392541, "mdate": 1759897196012, "content": {"title": "Comparison of Neural Network Architectures in the Thermal Explosion Approximation Problem", "abstract": "This study investigates the effect of neural network architecture on the accuracy of data-driven modeling of thermal explosions in a hydrogen–oxygen–air mixture. Using a reduced kinetic mechanism for 11 reagents, the thermal explosion process is simulated under specified initial pressure and temperature conditions, generating time-resolved data. We compare three architectures: a standard multilayer perceptron (MLP), a DeepONet–inspired model, and our U-Net–style residual network, evaluating their ability to capture transient dynamics and key reaction regimes.\n\nOur results demonstrate that network architecture has a decisive impact on predictive performance. The U-Net architecture consistently outperformed the other models, achieving a mean squared error (MSE) of 0.0013 with a standard deviation (STD) of 0.0218, demonstrating high fidelity in capturing both rapid transients and slower reaction dynamics. In contrast, the DeepONet-inspired model and the MLP achieved MSEs of 0.0181 (STD 0.0581) and 0.0202 (STD 0.0682), respectively, indicating reduced accuracy and greater variability in predictions. The large spread in error is due to the fact that neural networks are not always able to accurately approximate the various modes of the combustion process. Despite testing various architectures and using a fairly large dataset, the problem remains unresolved.\n\nThese findings highlight the importance of selecting appropriate network architectures for combining deep learning with chemically detailed kinetic simulations. Such careful selection paves the way for more reliable and interpretable predictive models in combustion and reactive-flow applications.", "tldr": "", "keywords": ["Neural networks", "Neural network architecture", "Hydrogen detonation", "Thermal explosion", "Deep learning in chemical kinetics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/21739338988445489bfbade9c40020f65c2cf751.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper compares three neural network architectures—MLP, U-Net–like residual, and DeepONet—for modeling the thermochemical evolution of reactive hydrogen–oxygen mixtures. Using a dataset generated from stiff ODE simulations across wide ranges of temperature, pressure, and timestep, each model predicts the temporal evolution of species concentrations and temperature. Despite identical training conditions, results show that the U-Net–like residual network achieves the lowest mean squared error and narrowest confidence interval, indicating superior accuracy and stability compared to MLP and DeepONet. The hierarchical skip connections in the U-Net effectively capture both global and localized transients without increasing computational cost, while DeepONet tends to oversmooth nonlinear dynamics. Overall, the study concludes that for stiff chemical–kinetic systems, architectures incorporating hierarchical feature extraction and residual connections substantially improve predictive accuracy and robustness over conventional feedforward or operator-learning models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a systematic comparison of three neural network architectures using a well-defined combustion kinetics dataset, which provides useful benchmarking for engineering and scientific modeling tasks.\n\nThe U-Net–like residual network is implemented thoughtfully, with careful consideration of physical invariants and stability, demonstrating solid experimental design and clear reporting."}, "weaknesses": {"value": "The methodological novelty is limited; the architectures (MLP, U-Net, DeepONet) are standard, and the study mainly applies them rather than proposing new theoretical or algorithmic contributions.\n\nThe scientific insight gained is incremental, focusing on empirical error comparison without deeper analysis of why the architectures differ in performance (e.g., sensitivity analysis, interpretability, or dynamical reasoning).\n\nOverall, while the work is technically sound, it aligns more with engineering model assessment than with the innovation or conceptual depth expected at a venue like ICLR."}, "questions": {"value": "Have the authors considered extending their comparison to physics-informed neural networks (PINNs) or Fourier-based architectures (e.g., FNOs), which might capture the multiscale coupling and stiffness of chemical kinetics more effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fyT6Vlo3XN", "forum": "P1qlqfRtgo", "replyto": "P1qlqfRtgo", "signatures": ["ICLR.cc/2026/Conference/Submission17115/Reviewer_Qy6Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17115/Reviewer_Qy6Y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760905451998, "cdate": 1760905451998, "tmdate": 1762927116384, "mdate": 1762927116384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the effect of neural networks models on modeling of thermal explosions in hydrogen-oxygen-air mixture, simulated under specified initial conditions. Three architectures are compared, an MLP, an adapted DeepONet and a UNet for the ability to capture transient dynamics and reaction regimes. Data is gathered by running an ODE solver for the problem with a range of parameters. The paper concludes that the UNet model consistently outperforms other models with the MLP and DeepONet showing worse performance and greater variability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper attempts to provide a comparison of existing neural network architectures for dynamical problems from applied science, which should be of general interest to the community."}, "weaknesses": {"value": "The paper does not propose a new method. There is no novelty either in the experiments, analysis or results.\n\nThere is no real analysis of the models employed in terms of identifying limitations of each with regard to performance.\n\nThe experimental setup is simplistic and involves training prior models on a single problem and comparing the resulting losses, standard deviations and confidence intervals in Table 1. Figure 3, 4 shows learned trajectories for two cases.\n\nLiterature review is sparse and there is no extensive review of the state-of-the-art.\n\nOnly three models are compared on a non-standard dataset without making any reference to prior work in comparing neural operator models.\n\nOnly a single custom dataset appears to have been used in the experiments. That dataset itself appears to be small with 50k examples of 13d vectors."}, "questions": {"value": "Are there comparisons on more complex problems and would they lead to the same conclusion?\n\nGiven that the problem has an ODE formulation have you tried ODE based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2PPpjiRDTZ", "forum": "P1qlqfRtgo", "replyto": "P1qlqfRtgo", "signatures": ["ICLR.cc/2026/Conference/Submission17115/Reviewer_sEcu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17115/Reviewer_sEcu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761655397307, "cdate": 1761655397307, "tmdate": 1762927116135, "mdate": 1762927116135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper compares three neural network architectures (U-net, DeepOnet and MLP) for learning the solution of an ODE modeling chemical kinetics at play in thermal explosion of a hydrogen-oxygen mixture. The authors find out that a U-net adapted for this low dimensional learning problem yields the best results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written and the scope of the contribution is clearly introduced. As a result, the paper is pleasant to read.\n- The methodology is correct"}, "weaknesses": {"value": "- The main contribution of the paper is to test several existing architectures on a physical problem. There is no novelty in the types of neural networks introduced, and the learning problem does not seem challenging (13 dimensions, with smooth functions for every dimension), which makes the paper more suited to a journal focused on physical sciences.\n\n- The authors claim that the paper demonstrate that \"network architecture has an important impact on predictive performances\". This statement is commonly accepted and has been the main motivation for decades of research in deep learning, which makes this claim unsound.\n\n- There are plenty of more modern neural architectures in scientific ML which could be compared to those implemented in the paper.\n\nIn the end the paper comes down to a comparison of three architectures on one dataset, which is too light as a contribution for ICLR."}, "questions": {"value": "**Q1** The dimensions of the dataset consists of:\n\n- The temperature\n    \n- The time\n    \n- The 11 chemical species\n    \n\nBut what about the pressure that has been sampled along with the temperature and the time for building the dataset (l.137)\n\n**Q2** How is the time included in the dataset ? For a given sample, what is the value of T for the output given the value of t in the input ? (Is it $t$ and $t + \\Delta t$?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Sr6LVvOKft", "forum": "P1qlqfRtgo", "replyto": "P1qlqfRtgo", "signatures": ["ICLR.cc/2026/Conference/Submission17115/Reviewer_d5bQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17115/Reviewer_d5bQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685689550, "cdate": 1761685689550, "tmdate": 1762927115458, "mdate": 1762927115458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a comparative study of three different neural network architectures for modeling the thermal explosion process in a hydrogen-oxygen-air mixture. The authors generate a dataset by simulating the chemical kinetics using a reduced 11-reagent mechanism under a wide range of initial conditions. The goal is to predict the temporal evolution of temperature and species concentrations. The architectures compared are a standard Multi-Layer Perceptron (MLP), a DeepONet-inspired model, and a custom U-Net-style residual network. The study finds that the U-Net architecture significantly outperforms the other two, achieving a much lower Mean Squared Error (MSE) and smaller standard deviation in its predictions. The authors conclude that architectural choice is of paramount importance for accurately modeling stiff, multi-scale chemical kinetics, and that hierarchical, residual-based designs like the U-Net are better suited for this task than simpler feed-forward or operator-learning models like DeepONet."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Relevant Problem:** The work tackles the important and challenging task of accelerating stiff chemical kinetics simulations, a key bottleneck in computational combustion.\n2.  **Clear Empirical Result:** The paper demonstrates a clear and statistically significant performance advantage of the U-Net architecture over the other two models on the given dataset.\n3.  **Well-Generated Dataset:** The authors have created a comprehensive dataset covering a wide range of conditions, including extreme combustion regimes, which is valuable for benchmarking models."}, "weaknesses": {"value": "1.  **Questionable DeepONet Implementation:** The design of the \"DeepONet-style model\" is non-standard and arguably misrepresents the operator-learning paradigm it is supposed to evaluate. The choice to use the scalar time step `dt` as the input to the trunk network is not well-justified and likely responsible for its poor performance.\n2.  **Narrow Architectural Scope:** The comparison is limited to three architectures, omitting many modern and relevant models for sequence modeling, such as Transformers or recurrent networks, which could be strong contenders for this task.\n3.  **Lack of Deeper Analysis:** The paper successfully shows *that* U-Net works better, but it does not provide a deep analysis of *why*. There is no investigation into how the multi-scale features learned by the U-Net correspond to the different timescales of the chemical reactions, for instance.\n4.  **Conflation of One-Step and Rollout Error:** The analysis does not distinguish between the models' single-step predictive accuracy and their stability in autoregressive rollouts, making it difficult to pinpoint the exact source of the U-Net's superiority."}, "questions": {"value": "1.  Can the authors justify their specific implementation of the DeepONet-style model? Why was the scalar time step `dt` chosen as the input to the trunk network, rather than, for example, a time variable `t` that would be more aligned with learning a time-dependent operator? Have you considered a more standard DeepONet setup where the branch network might encode initial conditions and the trunk network predicts the state at a query time `t`?\n2.  Why were other prominent architectures for sequence modeling, such as Transformers or LSTMs, not included in this comparison? Given their success in capturing long-range dependencies in time-series data, they seem like natural candidates for this problem.\n3.  Could you provide results for the single-step prediction error (i.e., `n_steps = 1`) for all three models? This would help clarify whether the U-Net's advantage comes from better single-step accuracy, better stability during rollouts, or both.\n4.  The paper concludes that the problem of approximating combustion modes \"remains unresolved.\" Could you clarify this statement? Does this refer to the existence of a few high-error trajectories for all models, or a more fundamental limitation? Given the U-Net's strong performance, what specific aspects of the problem do you believe are still unresolved?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ynY8gKk952", "forum": "P1qlqfRtgo", "replyto": "P1qlqfRtgo", "signatures": ["ICLR.cc/2026/Conference/Submission17115/Reviewer_nHmd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17115/Reviewer_nHmd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920234818, "cdate": 1761920234818, "tmdate": 1762927114891, "mdate": 1762927114891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}