{"id": "FNA0AlYBMS", "number": 12876, "cdate": 1758211123509, "mdate": 1763768268560, "content": {"title": "Top-K Structure Search with Solution Path", "abstract": "Structure learning algorithms often output a single estimated graph without offering alternative candidates or a way to capture model uncertainty. This is limiting in finite-sample settings with weak signals or noise, where multiple structures can explain the data equally well. In this work, we propose Top-K Structure Search with Solution Path, an algorithm that systematically tracks the evolution of edge weights across a range of values of the $\\ell_1$ sparsity regularization parameter $\\lambda$. By scoring candidate structures with the Bayesian Information Criterion (BIC), our method ranks and returns the Top-K most plausible structures. Unlike traditional approaches that yield a single solution, our framework provides a ranked set of candidates, enabling better uncertainty assessment. Experiments on synthetic and real-world datasets demonstrate the effectiveness of our approach in capturing structural variability. This highlights the advantage of leveraging solution paths for structure learning, especially in scenarios where committing to a single graph is unreliable. Our framework offers a complementary perspective on structure learning by considering multiple candidate solutions, thereby mitigating the practical instability of solely relying on a single result.", "tldr": "We introduce a Top-K structure learning algorithm that tracks edge evolution along the L1-regularization path, returning multiple high-scoring candidate graphs to better capture structural uncertainty in noisy or finite-sample settings.", "keywords": ["Structure learning", "Bayesian networks", "Top-K solutions", "Solution path", "Model uncertainty"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06f40b791ae9193dffe14260bd216cd1deb5acec.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study proposes the \"Top-K Structure Search with Solution Path\" algorithm to fix the issue that most Bayesian structure learning algorithms only output one \"optimal\" graph and fail to capture model uncertainty. It tracks edge weight changes via the L1 sparsity regularization parameter (lambda) to find structural critical points, scores candidates with BIC, and selects top K plausible graphs. With a gradient-optimized objective function (likelihood, L1 penalty, soft acyclicity constraint) and temperature-scaled BIC-based uncertainty quantification, experiments on synthetic (varying samples, variables, density) and Sachs datasets show it outperforms GES/PC/BOSS/Top-K A* in F1/recall for small samples/weak signals/high dimensions, with better scalability for medium-high dimensional networks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Combines L1 regularization solution paths with Top-K selection, avoiding exhaustive graph search and super-exponential complexity of dynamic programming/A*.\n2. Provides dual uncertainty quantification (graph-level probability, edge occurrence probability) with temperature scaling for reasonable entropy, boosting interpretability.\n3. Systematic experiments (synthetic scenarios, Sachs data) confirm it captures weak edges missed by traditional methods, verifying robustness.\n4. Scalable computationally (complexity unrelated to K), handling 60-variable networks where Top-K A* fails."}, "weaknesses": {"value": "1. This motivation is open to debate, as the core idea of generating multiple candidate graphs seems somewhat forced. In reality, many existing methods can also generate multiple candidate graphs, though they have not been applied in this specific context. From this perspective, the work has certain limitations.\n2. Hyperparameter selection lacks data-driven automation. Key hyperparameters like lambda grid granularity (epsilon), Top-K value (K), temperature parameter (T), and edge weight threshold (tau) rely on experience or heuristics. For example, T is set to ensure the probability of the K-th graph equals 1/(2K), and epsilon is required to be \"small enough to capture all critical points\"—but there is no clear data-driven method to determine these values.\n3. Insufficient discussion on the soft acyclicity constraint. The objective function uses the soft acyclicity constraint expressed as \"trace of the matrix exponential of the Hadamard product of the adjacency matrix (B) with itself minus the number of variables (d)\". While referencing prior work (Zheng et al., 2018), the study does not analyze its applicability in special scenarios: for instance, whether it introduces bias in extremely high-density graphs (e.g., density 0.8) or when there are pseudo-cycles from unobserved confounders; nor does it discuss if combining with hard acyclicity constraints (e.g., node ordering) could improve performance."}, "questions": {"value": "1. The study states epsilon needs to be \"small enough to capture all critical points\", but there is no quantitative standard for \"small enough\"—for example, how does epsilon relate to the number of variables (d) and graph density? Can supplementary experiments compare the impact of different epsilon values (e.g., epsilon = maximum lambda/50, epsilon = maximum lambda/200) on the number of identified critical points and Top-K structure quality (e.g., BIC score stability)? For K selection, only K=1,5,10 are tested; when K exceeds 20, will performance saturate (F1 score no longer improves) or decline (introduce too many low-quality structures)? Is there a way to determine the optimal K based on dataset characteristics?\n2. Edge-level uncertainty (probability of edges appearing in Top-K graphs) can be a \"soft confidence score\", but its practical utility is unproven. For example, on the Sachs dataset, if edges with a confidence score >0.3 are considered high-confidence, can this improve downstream task accuracy (e.g., protein interaction prediction)? How does this uncertainty quantification method compare to existing ones like Bootstrap resampling in computational efficiency and accuracy? Is there a way to validate the reliability of the uncertainty results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YC21LLtyrC", "forum": "FNA0AlYBMS", "replyto": "FNA0AlYBMS", "signatures": ["ICLR.cc/2026/Conference/Submission12876/Reviewer_exWs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12876/Reviewer_exWs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760880273622, "cdate": 1760880273622, "tmdate": 1762923665174, "mdate": 1762923665174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new structure learning framework for linear causal models, named Top-K Structure Search with Solution Path. The method explores multiple candidate DAGs by following the evolution of edge weights along the ℓ₁ regularization path and selecting the top-K graphs based on their BIC scores. The goal is to quantify model uncertainty and capture alternative plausible structures rather than committing to a single learned DAG. Experiments are conducted on synthetic datasets and the Sachs protein network, with comparisons against PC, GES, and BOSS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1- The paper is mathematically sound and clearly written, with consistent notation and reasonable theoretical grounding.\n\n\n2- The idea of tracing the solution path for structure learning and selecting the Top-K BIC-scored DAGs is well-motivated for exploring structural uncertainty.\n\n\n3- The inclusion of uncertainty quantification using temperature-scaled probabilities adds an interesting perspective to the interpretation of multiple candidate structures.\n\n\n4- The paper is well-organized and technically detailed."}, "weaknesses": {"value": "1-  The proposed approach extends existing linear DAG learning formulations (notably GOLEM and NOTEARS) by varying the regularization parameter λ and collecting multiple solutions along the path. While the idea of leveraging the Lasso solution path for DAG estimation is interesting, it is a relatively small methodological step beyond prior work. The novelty is modest compared to established continuous optimization frameworks for causal structure learning.\n\n2- The experimental section compares the proposed method only with PC, GES, and BOSS—methods that handle both linear and nonlinear models but use very different principles (constraint- and order-based search).\n Crucially, the paper does not include comparisons with NOTEARS, GOLEM, or other recent continuous-optimization DAG-learning methods, which are the most relevant baselines given that the proposed model is also linear and differentiable. Without these comparisons, it is difficult to assess the relative performance or real improvement of the proposed method.\n\n3- The method is limited to linear structural equation models (SEMs). This restriction significantly limits its generality, as many real-world datasets exhibit nonlinear dependencies.\n\n4- While the Top-K framework for structure learning is conceptually appealing, the practical impact of ranking multiple linear DAGs is unclear. The paper does not convincingly show that the Top-K set provides substantial benefits beyond what standard resampling or Bayesian averaging techniques could achieve.\n\n5- Evaluation focuses on small to moderately sized networks (≤60 variables) and does not demonstrate scalability beyond what has already been achieved by existing gradient-based methods."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7DOR8apMNX", "forum": "FNA0AlYBMS", "replyto": "FNA0AlYBMS", "signatures": ["ICLR.cc/2026/Conference/Submission12876/Reviewer_iRGe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12876/Reviewer_iRGe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664950818, "cdate": 1761664950818, "tmdate": 1762923664482, "mdate": 1762923664482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a solution-path approach to structure learning for linear Gaussian SEMs that returns a ranked set of K candidate graphs rather than a single estimate. The method traces the evolution of edge weights along a sparsity path (varying the ℓ1 penalty λ), identifies “critical points” where supports change, and selects the Top-K graphs based on BIC after an OLS refit on the selected supports. It also provides graph- and edge-level uncertainty via a temperature-scaled softmax over BIC scores. Experiments on synthetic data and the Sachs protein signaling dataset suggest that the approach improves recall and F1—especially in low-sample, low-SNR regimes. The submission is clear and well framed, and the empirical results indicate measurable benefits in regimes where committing to a single structure is unreliable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "OLS refit on supports is a sensible correction for Lasso shrinkage before BIC scoring.\nSynthetic stress tests in low‑n and weak‑edge regimes are well chosen; you vary n, K, density ρ, and d. Gains in recall/F1—and often accuracy—are consistent with the method’s design. Sachs analysis is illustrative: Top‑2 matches baseline skeleton; Top‑7 yields best F1/accuracy, showing the value of exploring K>1."}, "weaknesses": {"value": "The paper states the objective is differentiable, but the ℓ1 norm is non-differentiable at zero. While the active-set approach and subgradients are often used in practice, you should explicitly acknowledge non-differentiability and clarify whether you use subgradients or a proximal step/soft-thresholding. As written, there is an inconsistency.\nAdd a concise algorithm box with all steps.\nAdd continuous‑optimization baselines (NOTEARS, GOLEM, and possibly DAGMA/GraN‑DAG) which are most methodologically comparable.\nSince observational data typically identify only MECs, report SHD to CPDAG, orientation precision/recall (when meaningful), and possibly F1 on CPDAG edges. Skeleton‑only is incomplete for many readers.\nIf GIES appears in figures, clarify whether interventional data in Sachs were used; if not, remove GIES or explain its role."}, "questions": {"value": "How sensitive are the results to ε, δ, τ, and α?On Sachs: Did you use interventional data, and if so, how was scoring adjusted? If not, why is GIES referenced in figures?Can you provide CPDAG-oriented metrics and SHD to evaluate orientations more fully?On Sachs: Did you use interventional data, and if so, how was scoring adjusted? If not, why is GIES referenced in figures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jrXY1Imkh2", "forum": "FNA0AlYBMS", "replyto": "FNA0AlYBMS", "signatures": ["ICLR.cc/2026/Conference/Submission12876/Reviewer_PPPD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12876/Reviewer_PPPD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960726548, "cdate": 1761960726548, "tmdate": 1762923664219, "mdate": 1762923664219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Top-K Structure Search with Solution Path, a method for Bayesian structure learning that goes beyond predicting a single graph. Instead, it traces how edges evolve as the L-1 sparsity parameter varies and identifies critical points where structural changes occur. These candidate graphs are then scored using BIC, and the Top-K most plausible structures are returned, allowing better modeling of uncertainty in noisy or limited-sample scenarios. The algorithm uses gradient-based optimization with soft DAG constraints and re-estimates weights to correct Lasso shrinkage. It also provides graph- and edge-level uncertainty estimates. Experiments on synthetic and real-world datasets show improved robustness and accuracy compared to PC, GES, BOSS, and Top-K A*, especially in cases where multiple structures fit the data similarly."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a systematic method to generate and rank multiple plausible graph structures instead of relying on a single estimate.\n2. Efficiently identifies candidate structures by tracking edge support changes along the $\\ell_1$ regularization path with detailed mathematical backing.\n3. Demonstrates robustness across synthetic and real-world datasets with comprehensive evaluations over sample size, dimensionality, graph density, and $K$, using clear performance metrics."}, "weaknesses": {"value": "1. The paper does not engage with Top-K search work beyond Bayesian graphs, limiting its relevance framing.\n2. Unclear Hyperparameter Design: Thresholds such as $\\delta$ (active set) and $\\tau$ (binarization) lack principled justification or sensitivity analysis, making results potentially hyperparameter-dependent.\n3. The soft acyclicity penalty may fail to ensure strict DAGs, yet failure cases and practical violations are not analyzed.\n4. Using a uniform $\\lambda$ grid instead of recovering the full continuous LASSO path can miss critical support transitions.\n5. Limited Real-World Evaluation: Only the small Sachs dataset is used, leaving scalability to large real-world graphs untested.\n6. No Comparison for Uncertainty Estimation: The uncertainty framework is not compared against Bayesian model averaging, bootstrap, or sampling-based alternatives.\n7. The method lacks guarantees that the grid-based candidate set captures all key structures or modes.\n8. No Ablation Studies: Important design choices (temperature scaling, $\\epsilon$, and $K$) are not ablated despite noted importance."}, "questions": {"value": "Refer weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kz2YsdEtKk", "forum": "FNA0AlYBMS", "replyto": "FNA0AlYBMS", "signatures": ["ICLR.cc/2026/Conference/Submission12876/Reviewer_dbFs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12876/Reviewer_dbFs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762066940904, "cdate": 1762066940904, "tmdate": 1762923663692, "mdate": 1762923663692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}