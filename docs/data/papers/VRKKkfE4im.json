{"id": "VRKKkfE4im", "number": 6114, "cdate": 1757953081399, "mdate": 1759897934685, "content": {"title": "HyenaMoE: A Hybrid and Scalable Architecture for Efficient Genomic Modeling", "abstract": "DNA sequences serve as the fundamental blueprint of cellular life, encoding critical information for gene regulation, protein synthesis, and a broad spectrum of essential biological processes. Owing to their sequential structure, DNA sequences bear similarities to natural language, motivating the adaptation of large language model architectures and the pretraining–finetuning paradigm in genomics. This has led to the emergence of genomic foundation models that perform well across a wide range of downstream tasks. Nonetheless, current approaches face structural limitations. Transformer-based models possess strong representational capacity for local contexts, making them well-suited for tasks involving short sequences. However, their scalability is limited by the quadratic complexity of attention mechanisms. In contrast, methods based on state space models offer high computational efficiency and can process long-range genomic inputs, but they generally perform less strongly than Transformer counterparts on shorter sequences. To address these limitations, we introduce HyenaMoE, a unified hybrid architecture designed for genomic modeling using 3-mer tokenization. HyenaMoE combines efficient HyenaLite blocks for long-range dependency modeling with attention layers enhanced by Mixture-of-Experts routing, enabling scalable capacity expansion and more efficient allocation of model resources across diverse inputs. This design supports a favorable balance between model expressiveness and computational efficiency. Experiments on three representative benchmarks demonstrate that HyenaMoE achieves state-of-the-art performance across a diverse array of genomic prediction tasks.", "tldr": "HyenaMoE: A Hybrid and Scalable Architecture for Efficient Genomic Modeling", "keywords": ["Genomics", "Hyena", "Foundation Models", "Large Language Models", "Mixture of Experts"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76754a83bdee6a690d58fdd3f841ebb61d615500.pdf", "supplementary_material": "/attachment/f0d8c325069aa1f4a199fd7592e91828d0aaf7ff.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes HyenaMoE, a hybrid architecture for genomic sequence modeling that has\n(i) an optimized HyenaLite block—intended to preserve Hyena’s long-range convolutional mixing while adding implementation improvements such as cached filters and shared time indices\nand (ii) MoE-enhanced attention with shared and routed experts and token-wise top‑K routing. \nArchitecture of the model is a hybrid of HyenaLite blocks and Transformers MoE blocks. The model uses 3‑mer tokenization and an MLM pretraining objective on a corpus matching DNABERT‑2 pretraining corpus, and is fine‑tuned for classification/regression tasks on GUE, NT and Genomics Benchmark. HyenaMoE variants—especially the “Expressive” MoE‑heavy one (3 Transformers MoE block interleaved with 1 HyenaLite block)—report strong average accuracy at modest parameter counts. Efficiency plots indicate lower FLOPs scaling than dense attention and higher throughput at long sequences."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) Implementation improvements to Hyena are concrete (filter caching, shared time vector, grouped depthwise convs) and plausibly useful beyond this paper; the claim of more than 2x speedup for the operator is interesting could be beneficial for long-context DNA modeling.\n\n(2) Transformers MoE block further improve efficiency. The paper proposes an architecture in which HyenaLite provides efficient long-range temporal mixing while MoE‑attention supplies local expressivity and conditional capacity.\n\n(3) Efficiency characterization. FLOPs scaling and throughput vs. dense attention/StripedHyena are useful; the curves show that with fewer attention layers the overall complexity grows much more gently with length."}, "weaknesses": {"value": "1. No true long context tasks demonstrated: the major contribution proposed by the author is to use the efficient HyenaLite block interleved with Transformers MoE blocks to improve the ultra-long genomics sequence modeling. Despite positioning HyenaMoE for ultra‑long genomics, all task evaluations remain in the short‑to‑moderate regim for all GUE, NT and Genomics Benchmark tasks. The paper cites sliding‑window inference and long‑range efficiency, but never demonstrates performance on modern long‑range genomics benchmarks that explicitly require tens of kbps context (e.g., enhancer–gene interactions, eQTLs, 3D chromatin). Two recent public suites are: DNALongBench and the Genomics Long‑Range Benchmark (LRB). Evaluate on either of them would propose better illustration of the model's usefulness.\n\n2. While the author claims that HyenaLite block interleaved with Transformers-MoE block yields better performance than pure Transformers model, the HyenaMoE model with highest performance has 3 out of every 4 blocks being Transformers-MoE block, which basically has a similar computational efficiency as a pure Transformers-MoE block. Thus I remain suspicious of whether this architecture can scale to longer sequence, which is still restricted by the large portion of Transformers blocks.\n\n3. Another core contribution and difference is the applicaton of MoE blocks, which the author claims to exclude the load balancing loss and other aspects. But the author haven't evaluate on the experiment about how the experts work in the downstream tasks and whether an MoE design choice is better than a full dense Transformers block. \n\n3. The author chose a 3-mer tokenization, which is unusal for gLMs. Maybe the author can better illustrate why they choose this tokenization methodology by ablation studies."}, "questions": {"value": "1. Please demonstrate the performance on long-context tasks like Genomics LRB [1] or DNALongBench [2], which shows the motivation of long-context modeling advantage by HyenaMoe Architecture.\n\n2. Can the authors show the reason of choice of 3-mer tokenization? Is it a balance between efficiency and model performance or some arbitrary choice?\n\n3. Can the authors provide a more granular ablation of the MoE mechanism—varying expert count, routing sparsity, and gating dynamics—to separate the effects of conditional computation from parameter count? In addition, since the paper claims expert specialization across genomic regions, could the authors include interpretability analyses (e.g., expert activation maps, motif enrichment, or token-to-expert distributions) to substantiate that experts learn biologically distinct regulatory functions rather than merely adding capacity?\n\n[1] E. Trop, Y. Schiff, E. M. Marroquin, C. H. Kao, A. Gokaslan, McKinley Polen, M. Shao, A. Kallala, B. P. de Almeida, T. Pierrot, Y. I. Li, V. Kuleshov, The Human Genomics Long-Range Benchmark (LRB): Advancing DNA Language Models, arXiv, 2024. \n\n[2] W. Cheng et al., A Benchmark Suite for Long-Range DNA Prediction Tasks (DNALongBench), bioRxiv preprint, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZgQArPP46P", "forum": "VRKKkfE4im", "replyto": "VRKKkfE4im", "signatures": ["ICLR.cc/2026/Conference/Submission6114/Reviewer_1Q4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6114/Reviewer_1Q4o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760580055986, "cdate": 1760580055986, "tmdate": 1762918473455, "mdate": 1762918473455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HyenaMoE, a unified hybrid architecture designed for genomic modeling using 3-mer tokenization. HyenaMoE combines efficient HyenaLite blocks for long-range dependency modeling with attention layers enhanced by Mixture-of-Experts routing, enabling scalable capacity expansion and more efficient allocation of model resources across diverse inputs. This design supports a favorable balance between model expressiveness and computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces HyenaLite, a lightweight variant of the Hyena operator that supports sliding-window inference with hierarchical time decomposition, achieving over 2× speedup and reduced memory usage compared to the original design.\n2. This paper adapts MoE to genomic sequence modeling, simplifying its design to align with their hybrid framework.\n3. This paper presents HyenaMoE, a parameter-efficient hybrid model that integrates HyenaLite for long-range dependencies and MoE-attention for flexible local specialization, offering robust scalability from short to ultra-long input sequences."}, "weaknesses": {"value": "1. The writing of the paper could be further improved. After reading the Introduction, I still do not understand the motivation and new insights of this work. It feels as though the authors have simply stacked together a number of previous advances and combined them, without clearly articulating the research problem, motivation, or findings of this study.\n2. This paper appears to rely considerably on assembling and adapting existing modules, with many of the methods following established approaches for standard tasks. The motivation for choosing these methods and the novel observations underlying the framework could be clarified further."}, "questions": {"value": "1. How do HyenaLite support sliding-window inference with near capacity with full sequence attention?\n2. It would be helpful if the authors could further discuss the advantages of the hybrid architecture over specific architectures, especially when dealing with short and long sequences. In particular, I am curious about how the hybrid approach avoids inheriting the respective limitations of each architecture—for instance, the limited expressiveness of SSMs for short sequences and the challenges Attention mechanisms face with long sequences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fObO9l1AUm", "forum": "VRKKkfE4im", "replyto": "VRKKkfE4im", "signatures": ["ICLR.cc/2026/Conference/Submission6114/Reviewer_YWbb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6114/Reviewer_YWbb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713249271, "cdate": 1761713249271, "tmdate": 1762918472998, "mdate": 1762918472998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HyenaMoE, a hybrid architecture for efficient genomic sequence modeling. The authors make three main contributions: (1) HyenaLite, an optimized Hyena operator that achieves over 2× speedup through filter caching, grouped convolutions, and hierarchical time decomposition; (2) adaptation of Mixture-of-Experts (MoE) to genomic modeling with simplified routing, claimed to enable specialization across genomic elements like promoters and enhancers; (3) a hybrid architecture interleaving HyenaLite blocks with MoE-enhanced attention, offering three variants (Fast, Standard, Expressive) with different capacity-efficiency trade-offs. Evaluated on 32 classification tasks across three benchmarks (GUE, Nucleotide Transformer Tasks, Genomics Benchmark), HyenaMoE-E achieves 82.87% average accuracy, marginally outperforming Generator (82.79%) while maintaining computational efficiency with constant FLOPs per token scaling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed HyenaLite operator demonstrates measurable improvements over the vanilla Hyena operator, providing a valid technical contribution in terms of computational efficiency.\n\n2. The experimental validation is thorough, covering both benchmark performance across multiple genomic tasks and computational efficiency analysis from multiple perspectives."}, "weaknesses": {"value": "1. Excessive space is devoted to community common knowledge that could be condensed. The abstract spends substantial text discussing basic SSM versus Transformer trade-offs, and Section 2 dedicates extensive coverage to fundamental concepts like attention mechanisms that are well-established in the community.\n\n2. Figure 1 provides insufficient motivation. (a) Species classification lacks biological significance and does not justify long-sequence modeling, demonstrating benefits would require scaling to near whole-genome lengths, while the 512→2048 range offers little biological insight. (b) The parameter count disparity between SSM-based and Transformer-based models is too large, making direct performance comparisons questionable.\n\n3. Despite the stated goal of modeling long sequences (following HyenaDNA's direction), the paper never specifies the pretraining sequence length. All downstream tasks evaluate on short sequences, failing to validate actual long-sequence performance capabilities.\n\n4. Figure 3(b-c) validates efficiency at 2k token length, while Section 4.3 benchmarks use substantially shorter sequences. Performance and efficiency are evaluated on different settings, preventing understanding of the model's behavior under consistent conditions.\n\n5. HyenaMoE differs from StripedHyena in multiple aspects (Hyena blocks, attention blocks, interleaving strategy), yet no complete ablation is provided. The paper directly compares end-to-end systems without component-level insights. Section 4.4 should at minimum include pure HyenaLite-only and pure MoE-attention ablations.\n\n6. Figure 3(a) fails to demonstrate HyenaMoE-Expressive's advantage over Dense Attention. The curves appear quite similar.\n\n7. Line 469 acknowledges MoE has no advantage at small batch sizes, but long-sequence training inherently precludes large batch sizes due to memory constraints, undermining the practical applicability.\n\n8. Figure 3(b-c) legends require clearer textual explanation. The '/' symbol is easily misinterpreted without explicit clarification that it represents a ratio."}, "questions": {"value": "1. What is the pretraining sequence length? Do all benchmarks share a single pretrained model or are they task-specific?\n\n2. How are Hyena and attention layers interleaved in StripedHyena? Figure 3(a) shows StripedHyena and HyenaMoE-Standard have similar curves. Are their stacking strategies comparable?\n\n3. Why choose 3-mer tokenization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OxNqAzxbJJ", "forum": "VRKKkfE4im", "replyto": "VRKKkfE4im", "signatures": ["ICLR.cc/2026/Conference/Submission6114/Reviewer_PUmF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6114/Reviewer_PUmF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833456684, "cdate": 1761833456684, "tmdate": 1762918472560, "mdate": 1762918472560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes HyenaMoE, a hybrid genomic sequence model that interleaves HyenaLite blocks for long‑range temporal mixing with Mixture‑of‑Experts (MoE)–augmented transformer modules, trained with 3‑mer tokenization and an MLM objective, and then fine‑tuned for downstream tasks. HyenaLite is presented as a streamlined Hyena operator with cached filters, shared time vectors, and grouped convolutions to reduce runtime and memory; the MoE block uses both shared and routed experts with token‑wise top‑K routing and no explicit load‑balancing loss, implemented in a communication‑efficient way. The authors instantiate Fast/Standard/Expressive variants and evaluate on GUE, Nucleotide Transformer (NT) tasks, and Genomics Benchmark, reporting competitive averages, modest ablation gains for adding MoE, and favorable FLOPs/throughput scaling relative to dense attention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **(S1) Clear problem framing and motivation.** The paper crisply states the central tension in genomic modeling: transformers excel on local patterns but scale quadratically, whereas SSM/Hyena‑style operators scale better but often underperform on short‑range tasks. Hybrids (e.g., StripedHyena/Evo) motivate the proposed direction. Figure 1 concisely illustrates performance vs. context length and the transformer-SSM gap on NT benchmarks.\n\n- **(S2) Simplified MoE that avoids explicit balancing losses.** The dual pathway (shared + routed experts) with top‑K gating is clean and deployable; equations fully define the mechanism, and the paper claims communication‑efficient dispatch. If robust, this could be valuable to practitioners who struggle to stabilize MoE with auxiliary losses.\n\n- **(S3)** The manuscript is well organized and written in clear and professional English with comprehensive figures and tables. The methodology section is well structured; Fig. 2 provides a helpful end‑to‑end view of the architecture and training pipeline (3‑mer tokenization, MLM, fine‑tuning). The technical descriptions are coherent and self-contained."}, "weaknesses": {"value": "- **(W1) Novelty is incremental and leans heavily on prior art without deeper analysis.** HyenaLite’s gains are primarily implementation simplifications of Hyena (cache/reuse/grouping) rather than a new operator; the MoE block is a simplified variant of known sparse‑expert mechanisms (shared+routed experts with top‑K gating), akin to DeepSeekMoE/GShard/Switch‑style MoE, but without showing theoretical or empirical guarantees for stability or specialization. The paper would benefit from principled analysis showing why removing balancing losses preserves performance and utilization. Background: Hyena long convolutions and SSMs (Mamba) are established; hybrid interleaving is known (StripedHyena/StripedHyena‑2); and stability/load balancing for MoE is a well‑studied concern in GShard/Switch/Expert‑Choice.\n\n- **(W2) Claim–number inconsistencies and uneven task performance.** The narrative asserts HyenaMoE “achieves top scores on … Splice Sites Donor” (Sec. 4.3), yet Table 1 shows DNABERT is substantially higher on this task (0.980 vs 0.772 for HyenaMoE‑E), with similar gaps on Splice Sites All/Acceptor. This contradiction weakens confidence in the “first on 9/17 NT tasks” summary and suggests that short‑range, motif‑centric tasks remain a weakness—precisely where the hybrid is expected to help.\n\n- **(W3) Comparability and fairness are not convincingly controlled.** Although Sec. 3.3.2 adopts DNABERT‑2’s corpus and full‑parameter fine‑tuning, Evo2 is evaluated by linear probing due to scale (Sec. 4.1), and tokenization/protocols differ across baselines. The paper does not supply size/budget‑matched baselines (same data/steps/tokens/context) for a transformer and a Hyena‑family hybrid under the same regimen, making it difficult to ascribe gains to the proposed architecture rather than tokens/corpora/protocol differences.\n\n- **(W4) Efficiency claims are not isolated to the proposed components.** The Introduction promises “over 2× speedup” from HyenaLite and “sliding‑window inference with hierarchical time decomposition” (contributions, p. 3), yet Sec. 4.4 benchmarks *architectures* (StripedHyena/Pure Hyena/HyenaMoE variants) rather than a direct Hyena vs HyenaLite micro‑ablation on identical backbones/hardware, and the sliding‑window algorithm/interface is not specified. Hence the claimed factor‑of‑two improvement cannot be verified.\n\n- **(W5) Minor issues of writing and limitations.** Firstly, the authors should double-check the notation issues. Several equations are mislabeled or ambiguous (e.g., a line presented as both “algorithm” and “loss”); equalities are used where approximations should be stated under restricted function classes; the “prox” role and “TC‑loss” are not defined with solution properties. Meanwhile, there are several limitations. (i) The approach emphasizes classification tasks; continuous, span‑level, or base‑resolution tasks (e.g., quantitative chromatin signals, TSS regression) are only briefly mentioned and mostly deferred to the appendix; the main narrative would benefit from more regression/sequence‑labeling results. (ii) 3‑mer tokenization trades off vocabulary size and sequence compression; this may complicate transfer to tasks where longer k‑mers/BPE are advantageous for motif capture or rare pattern composition. (iii) FLOPs‑per‑token curves (Fig. 3a) do not account for dispatch overheads (routing, padding to expert shards) that may dominate at small batch sizes on real systems; deployment results may therefore vary across clusters.\n\n---\n\nOverall, I am leaning towards reject at this stage. The paper addresses an important problem with a practical, well‑engineered hybrid, and it presents broad empirical coverage and clean system ideas. However, (i) novelty beyond known hybrids + MoE is modest; (ii) some headline claims conflict with the reported numbers (notably NT splicing), and (iii) critical ablations and fairness controls are missing, leaving the extent of the contribution ambiguous. If the rebuttal provides solid utilization analyses for the MoE, a strict Hyena vs HyenaLite efficiency ablation, corrected and strengthened tables (with significance), and a controlled baseline study (size/budget matched), I would be open to revisiting my score."}, "questions": {"value": "- **(Q1)** Expert utilization: Please provide per‑layer expert‑load statistics (mean/variance, coefficient of variation) and collapse diagnostics across pretraining/fine‑tuning, with and without your score‑normalization trick, and an apples‑to‑apples comparison against a Switch‑like balancing loss.\n\n- **(Q2)** Where is MoE applied? Confirm whether the experts augment the FFN pathway (as eqs. (8–11) suggest) or the attention sublayer; if the former, please revise terminology (“MoE‑augmented FFN”) and update FLOPs accounting accordingly.\n\n- **(Q3)** Tokenization confound: How sensitive is HyenaMoE to 3‑mer vs BPE tokenization? Please include an ablation (pretrain short run) and discuss how tokenization affects context length in tokens vs bases and downstream results.\n\n- **(Q4)** Failure analysis on splicing tasks: What patterns do routed experts pick up on these datasets? Could adding an inexpensive local attention or short‑kernel conv around candidate splice junctions recover the performance gap? Provide saliency/motif analyses per expert.\n\n- **(Q5)** Long‑context inference beyond 8k & sliding‑window details: Include at least one >100kbp evaluation (even synthetic) with accuracy degradation vs full context, plus the algorithmic description and cache interface for the proposed sliding‑window inference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l4EB7SyIPX", "forum": "VRKKkfE4im", "replyto": "VRKKkfE4im", "signatures": ["ICLR.cc/2026/Conference/Submission6114/Reviewer_6WF8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6114/Reviewer_6WF8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6114/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006131284, "cdate": 1762006131284, "tmdate": 1762918472048, "mdate": 1762918472048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}