{"id": "Tq0oPUyVTz", "number": 23472, "cdate": 1758344288480, "mdate": 1762968514463, "content": {"title": "PolyAudio: Advancing Multi-Audio Analysis & Reasoning in Large Audio Language Models", "abstract": "While Large Audio Language Models (LALMs) have achieved superior performance on reasoning over single audio clips, their ability to understand and reason over multiple audio clips remains a significant challenge. In this paper, we introduce PolyAudio, a novel LALM specifically designed for this complex task. To systematically train and evaluate our model, we first identify and formalize eleven foundational multi-audio reasoning capabilities. These capabilities, spanning sound, music, and speech, are designed to represent a broad range of challenging real-world scenarios. To enhance these skills, we fine-tune the Qwen2-Audio-7B-Instruct model using Group Relative Policy Optimization (GRPO). This approach mitigates common issues associated with Supervised Fine-Tuning (SFT), such as catastrophic forgetting. Specifically, we construct preference data that explicitly rewards the model for correctly synthesizing information across multiple audio clips. Our model, PolyAudio, achieves 58.6% on the MMAU-Pro multi-audio subset and 71.2% on our PolyAudio-Bench, substantially outperforming baselines on multi-audio reasoning tasks while maintaining its performance on single-audio tasks. To promote research in this space, we will publicly release the model, data generators, evaluation scripts, and training recipes at the time of publication.", "tldr": "Multi-audio analysis and reasoning capable Large Audio Language Model, data and benchmark.", "keywords": ["large audio language model", "mult-audio", "audio understanding", "audio reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/0947039783e286d3a5abe709aae48e555d257ae9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents POLYAUDIO, a large audio-language model (LALM) designed for reasoning across multiple audio clips (up to five). The authors introduce a new benchmark, POLYAUDIO-BENCH, covering 11 distinct reasoning categories, and propose a two-stage training framework that combines LoRA fine-tuning with Group Relative Policy Optimization (GRPO) alignment. Experimental results show that POLYAUDIO significantly outperforms Qwen2-Audio, Audio Flamingo 3, and Gemini 2.5 Pro on multi-audio reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The paper targets an underexplored yet important problem of multi-audio compositional reasoning, extending beyond the conventional pairwise paradigm in audio-language models.\n\n(2) The proposed benchmark POLYAUDIO-BENCH is comprehensive and systematically designed reasoning tasks."}, "weaknesses": {"value": "(1) While POLYAUDIO itself natively supports discrete multi-clip inputs, baselines use concatenation for evaluation, which complicates fair comparison on compositional reasoning.\n\n(2) The paper claims to interleave audio embeddings with text tokens, but never describes how positional encoding, cross-clip ordering, or temporal alignment are handled. Without explicit fusion mechanisms (e.g., cross-attention across clip embeddings or learned temporal gates), the model likely treats multi-clip inputs as a bag-of-embeddings, which weakens compositional reasoning fidelity.\n\n(3) Unlike recent Audio LLMs (e.g., Qwen2.5-Omni, Audio Flamingo 3) that introduce contrastive or embedding-consistency losses to stabilize audio–text fusion, POLYAUDIO relies purely on LoRA + GRPO without auxiliary alignment. This design risks latent drift between audio and language representations, especially under synthetic prosody conditions.\n\n(4) Evaluation covers only synthetic and benchmark data, without tests on human-recorded or cross-domain audio.\n\n(5) The paper omits any mention of temporal windowing, stride length, or max tokenization limit. This makes it unclear whether temporal grounding tasks rely on genuine reasoning or implicit positional priors learned from synthetic prompts.\n\n(6) Using Qwen3-7B-Instruct as a scalar scorer measures lexical/semantic similarity rather than true causal or compositional reasoning accuracy. This conflates linguistic fluency with reasoning validity, which is particularly problematic for audio-based tasks like counting or factual consistency.\n\n(7) The paper lacks examples of reasoning errors, such as miscounting, misattributed speakers, or cross-clip hallucination.\n\n(8) The preference dataset is auto-generated by GPT-5, but there’s no evidence of label noise filtering or inter-sample consistency checks. No statistics on reward variance, prompt diversity, or reward drift are reported.\n\nTypo: \n\nLine 50: invalid citation"}, "questions": {"value": "(1) Since POLYAUDIO supports discrete multi-clip inputs but several baselines (e.g., ADIFF, Mellow) are evaluated using concatenated audio with 2-second silence, could the authors clarify how this affects fairness and comparability? Have you considered an ablation where all models are tested under the same concatenation protocol to quantify the potential advantage?\n\n(2) The paper states that audio embeddings are interleaved with text tokens, but it remains unclear how positional encoding, clip ordering, or temporal alignment are handled internally. Could the authors elaborate on whether any cross-clip attention or learned gating mechanism is used to preserve temporal structure and compositional dependencies across clips?\n\n(3) POLYAUDIO relies solely on LoRA fine-tuning followed by GRPO alignment, whereas recent Audio LLMs employ additional contrastive or consistency regularization. Have the authors tested whether adding a lightweight cross-modal loss (e.g., audio–text contrastive alignment) improves stability or mitigates latent drift?\n\n(4) Given that both training and evaluation rely on GPT- and TTS-generated data, can the authors assess POLYAUDIO on any human-recorded or real-world acoustic benchmarks (e.g., VoxConverse, AVA-Speech)? Such results would strengthen claims about real-world reasoning generalization.\n\n(5) The model processes up to five clips of 30 s each, but implementation details such as window size, stride, and tokenization limit are not described. Could the authors clarify how long clips are segmented and whether the model encodes explicit temporal order or duration information during reasoning?\n\n(6) Since Qwen3-7B-Instruct serves as the sole judge, how do the authors ensure that the metric captures reasoning correctness rather than linguistic fluency or paraphrase similarity? Would you consider adding human evaluations or multiple LLM-as-a-Judge setups to assess correlation and robustness?\n\n(7) The paper reports only aggregate scores without showing examples of reasoning errors or failure cases. Could the authors provide qualitative analyses (e.g., miscounting, cross-clip confusion, hallucinated entities) to illustrate current limitations and guide future improvements?\n\n(8) For the GPT-5-generated POLYAUDIO-PREF dataset, have the authors measured label noise, prompt diversity, or reward variance? Including such diagnostics or filtering criteria would clarify the stability of GRPO training and the reliability of the reward signal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cShACzOkgG", "forum": "Tq0oPUyVTz", "replyto": "Tq0oPUyVTz", "signatures": ["ICLR.cc/2026/Conference/Submission23472/Reviewer_T4tF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23472/Reviewer_T4tF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760676043584, "cdate": 1760676043584, "tmdate": 1762942675458, "mdate": 1762942675458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "GhAftMS4nx", "forum": "Tq0oPUyVTz", "replyto": "Tq0oPUyVTz", "signatures": ["ICLR.cc/2026/Conference/Submission23472/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23472/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762968513681, "cdate": 1762968513681, "tmdate": 1762968513681, "mdate": 1762968513681, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of multi-audio understanding and reasoning (i.e., between 2 and 5 clips). They propose Polyaudio, a LALM specifically tailored for this task which is trained via SFT and GRPO. They also provide two curated datasets, PolyAudio-Train and PolyAudio-Pref, to train their proposed method, and a benchmark to test multi-clip reasoning. Evaluations on PolyAudio-Bench and a subset of MMAU-Pro reveals the superiority of Polyaudio over the other baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written.\n- The objectives of the papers and the main contributions are clearly described.\n- The problem of multi-audio reasoning is of sufficient significance."}, "weaknesses": {"value": "- The authors claims at lines 88-90: “We introduce POLYAUDIO, a novel Large Audio Language Model specifically designed for complex reasoning across multiple (up-to 5)…”. However, the proposed model does not bring any novelty. In fact, it simply concatenates the audio tokens from different clips and feed them to the LLM. It is a straightforward idea and I don’t see any novelty on doing this, just a natural way of processing multiple clips. In addition to this, it simply uses a SFT stage followed by GRPO alignment, a common strategy in the literature.\n- The Polyaudio-train and -pref datasets are a collection of existing datasets + syntethic speech generated using LLMs. It is not clear how much data comes from each dataset and from the synthetic ones. It is not clear to me why the authors have decided to generate synthetic data, they should elaborate more on this. \n- The authors do not include any real examples coming from the proposed training and evaluation benchmarks to understand better the tasks the models need to reason about.\n- I don’t think it’s fair to compare a model which follows SFT + GRPO alignment on 150k data on the task of multi-clip reasonings with models which have been trained on single clips.\n- The model is only tested on multi-audio tasks, however it should be also tested on single clips reasoning. Does the model still retain good performance on these tasks or is it subject to catastrophic forgetting? \n- I would expect the authors to at least mention that the datasets Polyaudio-Train and -Pref will be released together with Polyaudio-benchmark, but I don’t see any words in this direction. Do the authors plan to release the datasets?\n- Insufficient related work discussion on Large Audio Language Models (section 2.1). While the authors touch upon some LALMs in the introduction, in section 2.1, which should elaborate more on this topic, only includes two references."}, "questions": {"value": "**Questions**\n \nHow does the model perform on the rest of the tasks of MMAU Pro? Does it retain good performance albeit the massive training on multi-clips reasoning? A good model should be able to deal with both multi-clips but also single clips, the latter being the most common case in practice.  \n\n**Typos/Issues/Suggestions:**\n\n- Line 50: question mark —> missing reference.\n- Lines 92-94: Missing comma after “POLYAUDIO-PREF”. Extra full stop at the end of the sentence.\n- I suggest that the authors add the citation to each method/benchmark on Table 1, first column, rather than only including the name of them.\n- Line 315: a question mark —> wrong Appendix reference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Cyu2xLWdxa", "forum": "Tq0oPUyVTz", "replyto": "Tq0oPUyVTz", "signatures": ["ICLR.cc/2026/Conference/Submission23472/Reviewer_ov1n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23472/Reviewer_ov1n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761507097341, "cdate": 1761507097341, "tmdate": 1762942675250, "mdate": 1762942675250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge that Large Audio Language Models (LALMs) struggle with reasoning across multiple audio clips. It introduces POLYAUDIO, a new model, and POLYAUDIO-BENCH, a new benchmark with 11 tasks, to address this gap. The model, fine-tuned from Qwen2-Audio using GRPO , significantly outperforms baselines on multi-audio tasks while maintaining its original single-audio performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a critical and previously under-addressed gap in LALM capabilities—reasoning across multiple (N-way) discrete audio clips. It moves beyond prior work, which was largely limited to single or pairwise (N=2) audio, by formalizing and evaluating complex reasoning for up to 5 clips.\n2. The introduction of POLYAUDIO-BENCH is a significant contribution, providing a comprehensive suite of eleven distinct tasks (detailed in Table 2). This benchmark systematically evaluates a wide range of reasoning skills, from perceptual (e.g., Counting across clips) to compositional (e.g., Factual Consistency), offering a valuable new tool for the research community."}, "weaknesses": {"value": "1. The model's training relies heavily on synthetically generated content, including text from GPT-5 and audio from a TTS model (Higgs Audio v2). The authors acknowledge this limitation , which raises concerns about how well the model will generalize to the acoustic properties, noise, and linguistic diversity of real-world audio.\n2. The baseline models, such as Gemini 2.5 Pro and Audio Flamingo 3, do not natively support discrete multi-audio inputs. They were evaluated by concatenating audio clips with silent separators. This is an architectural mismatch that puts them at a fundamental disadvantage, making it difficult to isolate the benefit of POLYAUDIO's training method versus its (more suitable) base architecture."}, "questions": {"value": "1. The benchmark and model are designed for reasoning across \"up-to 5\" clips. How does the model's performance scale as the number of audio clips increases? For example, how would performance degrade on these 11 tasks if presented with 10 or 20 clips?\n2. How would POLYAUDIO perform on a benchmark composed entirely of \"in-the-wild\" audio? How well can it handle real-world challenges like overlapping speakers, diverse accents, and ambient noise, which are not present in the clean, synthetic training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ygp7IzuAh7", "forum": "Tq0oPUyVTz", "replyto": "Tq0oPUyVTz", "signatures": ["ICLR.cc/2026/Conference/Submission23472/Reviewer_ZjPX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23472/Reviewer_ZjPX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761763632406, "cdate": 1761763632406, "tmdate": 1762942675066, "mdate": 1762942675066, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors fine-tune Qwen2-Audio to tackle the \"reasoning over multiple audio clips\" task, achieving SOTA results for their proposed PolyAudio-Bench and the multi-audio subset of MMAU-Pro."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Interesting idea of exploring the multi-audio setup for LALM.\n- The results for the proposed multi-audio setup look good compared to current SOTA."}, "weaknesses": {"value": "The paper is really hard to follow overall. It seems to be a mix of bad presentation (writing, text structuring) and lack of methodological contributions (e.g., the methodology section is basically non-existent).\n\nThe paper's claims are that their contributions rely on a \"new Large Audio Language Model\" and a new dataset family (PolyAudio-{Train, Pref, Bench}). Neither of them is properly explained in the text.\n\nA few points I collected:\n1. The authors multiple times in the text use the term \"novel\" or \"new large audio language model,\" which is a bit of an overstatement for a fine-tuned version of Qwen2-Audio. \n2. The text is missing critical methodology details:\n- How the training was done\n- Selection of hyperparameters\n- How the dataset is composed\n- What decisions were made to ensure the quality of the dataset\n- What is the difference between the selection criteria of the three different sub-datasets that you compose?\n- \"We create question–answer pairs using GPT-5 and GPT-OSS120B (OpenAI, 2025).\" Which model was used for which questions? Why using a combination of these two models?\n3. There are a few references missing (citation appears as ? in the text)\n4. Lack of any ablation study\n5. Missed opportunity of exploring the performance degradation of curreny LALMs as the number of input audio increases."}, "questions": {"value": "Please address the points in the Weaknesses section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3fIHoFvVvR", "forum": "Tq0oPUyVTz", "replyto": "Tq0oPUyVTz", "signatures": ["ICLR.cc/2026/Conference/Submission23472/Reviewer_9qpU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23472/Reviewer_9qpU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903399155, "cdate": 1761903399155, "tmdate": 1762942674876, "mdate": 1762942674876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PolyAudio and extends existing large audio language models from single audio to multiple audio by using GRPO. The paper introduces two datasets for training and evaluating the proposed model. The paper achieves strong performance on the multi-audio benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The proposed method is straightforward and achieves good performance on multi-audio reasoning tasks."}, "weaknesses": {"value": "The novelty and the experimental section are very limited. The task-specific synthetic data generation, followed by GRPO fine-tuning pipeline, has been used before to improve performance. Prior works such as R1-Qwen and R1-Omni already demonstrate GRPO + Qwen2audio can achieve SOTA performance on MMAU/MMAR benchmarks.  The proposed method is only evaluated on two datasets, one of which is proposed by the authors. Also, the experimental results for single audio are very limited. PolyAudio is only compared with Qwen2-Audio-7B (Table 7). How does PolyAudio compare to other state-of-the-art models, such as Audio Flamingo 3?\n\nThere are several small typos in the paper. For example, lines 50 and 314 have ? for missing references. Line 258 has a missing space between Qwen2-Audio-7B-InstructChu et al."}, "questions": {"value": "How does PolyAudio compare to a simple baseline where the output of each audio is merged via an LLM to produce a unified output?\n\nDoes the model generalize across the number of audios? It’ll be interesting to see the impact of the number of audios during training on the multi-audio inference. For example, varying the number of audios during training from 2 to 5 and keeping the number of audios during test as 5. \n\nDoes the number of test audios impact the performance?\n\nDoes the order of audios impact the performance? For example, do audio1,audio2,audio3 and audio3,audio1,audio2 have similar performance?\n\nWhat is the impact on performance on different tasks outside of MMAU, such as ASR or speech-to-speech translation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mbkk2ZDi9r", "forum": "Tq0oPUyVTz", "replyto": "Tq0oPUyVTz", "signatures": ["ICLR.cc/2026/Conference/Submission23472/Reviewer_Vred"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23472/Reviewer_Vred"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23472/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961369412, "cdate": 1761961369412, "tmdate": 1762942674556, "mdate": 1762942674556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}