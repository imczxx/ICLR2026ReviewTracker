{"id": "iFjqavBNWD", "number": 22757, "cdate": 1758335088972, "mdate": 1763134638042, "content": {"title": "Heterogeneous Quantum Federated Learning via Adaptive Circuit Search and Model Aggregation", "abstract": "Quantum Federated Learning (QFL) is an emerging framework for privacy-preserving, collaborative training of quantum neural networks across a network of quantum nodes operating under qubit resource constraints. Although promising, existing QFL approaches enforce a uniform quantum circuit architecture across nodes, failing to account for data heterogeneity and leading to suboptimal global model performance. To tackle these challenges, we propose a BO-QFL framework, which is based on Bayesian optimization to discover node-specific quantum circuit architectures and a novel aggregation rule to unify heterogeneous models at the quantum server. The novel contributions of this paper are twofold: (i) an adaptive circuit architecture search mechanism for heterogeneous quantum nodes, utilizing Bayesian optimization to automatically discover optimal quantum circuit configuration, and (ii) an effective and innovative aggregation strategy that integrates these locally optimized heterogeneous circuits into a unified global model through element-wise logical union. Through rigorous simulations on spatial and temporal datasets, the proposed framework demonstrates a significant improvement in the global model performance over fixed-architecture QFL baselines. Additionally, evaluations in both noisy and ideal quantum environments further substantiate its robustness in realistic quantum settings.", "tldr": "Quantum Federated Learning with adaptive circuit search and heterogeneous aggregation for robust performance under data heterogeneity.", "keywords": ["Quantum Federated Learning", "Bayesian Optimization", "Quantum Architecture Search"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7bfa8d54d24e7ce834fe21d0a654e2fe175af369.pdf", "supplementary_material": "/attachment/a89a0d1f988a93f8f8c2afc6fda5949e0b588c01.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces BO-QFL, a framework for heterogeneous quantum federated learning (QFL) that addresses two limitations in existing QFL systems: the use of identical quantum circuit architectures across clients and the absence of an aggregation mechanism for structurally distinct models. The paper proposes (i) a Bayesian Optimization (BO)-based search for client-specific quantum circuit architectures, and (ii) a heterogeneous model aggregation rule using element-wise logical unions to unify diverse parameterized quantum circuits. The paper includes a detailed convergence analysis, extensive simulation results on MNIST, Fashion-MNIST, and HAR datasets, and comparisons with both reinforcement learning (RL)-based architecture search and baseline QFL approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces an original combination of Bayesian optimization with federated quantum learning.\n2.\tThe paper provides a formal convergence analysis, albeit under strong simplifying assumptions.\n3.\tThe method is evaluated on multiple datasets and the results report consistent performance improvements over a baseline QFL model."}, "weaknesses": {"value": "1.\tIn the convergence analysis in Section 5, the proof framework is nearly identical to that of Ajalloeian & Stich (2020) and FedAvg variants, with only superficial adaptation to “quantum heterogeneity.” No empirical verification of the claimed convergence rate is presented, making the theoretical section largely disconnected from the proposed BO-QFL mechanism.\n2.\tThe algorithmic pipeline in Figure 1 and Algorithm 1 is conceptually unclear. The server aggregation rule (Eq. 3) is presented without justification of its stability or gradient equivalence to a consistent loss function, raising doubts about whether the aggregation preserves differentiability or introduces bias in parameter updates.\n3.\tThe experiments rely entirely on classical simulations using TorchQuantum and PyTorch on a GPU, without validating the method on an actual quantum processor. The reported “quantum noise” is synthetically modeled, making it difficult to claim robustness in real-world settings.\n4.\tIn Table 1, the numerical results are modest and occasionally inconsistent. For instance, in HAR dataset results, BO-QFL’s accuracy decreases with more clients even in ideal settings, which contradicts the stated theoretical remark (Remark 2). No discussion is provided to explain this anomaly.\n5.\tThe assumed client model heterogeneity (up to 12 clients, 10 qubits each) is small relative to real-world FL settings. The communication and circuit reconstruction costs are not quantified, so scalability claims are speculative.\n6.\tThe two stated contributions (adaptive BO circuit search and heterogeneous aggregation) are intertwined in the implementation, making it unclear which component drives the performance gains. An ablation study is required to identify the gains derived from each contribution."}, "questions": {"value": "1.\tHow does the proposed aggregation (Eq. 3) preserve gradient consistency across heterogeneous circuits? Has this been validated empirically?\n2.\tCould the paper quantify the computational cost of Bayesian optimization compared to RL in terms of wall-clock time, not only the number of evaluations?\n3.\tHow would BO-QFL behave under partial client participation or asynchronous updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yNEOdLU8mN", "forum": "iFjqavBNWD", "replyto": "iFjqavBNWD", "signatures": ["ICLR.cc/2026/Conference/Submission22757/Reviewer_xALw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22757/Reviewer_xALw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761292052062, "cdate": 1761292052062, "tmdate": 1762942373639, "mdate": 1762942373639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "uzXK37oSEx", "forum": "iFjqavBNWD", "replyto": "iFjqavBNWD", "signatures": ["ICLR.cc/2026/Conference/Submission22757/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22757/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763134637232, "cdate": 1763134637232, "tmdate": 1763134637232, "mdate": 1763134637232, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors explore how quantum federated learning (QFL) can be made more flexible and effective when quantum devices differ in both hardware capacity and data distribution. The authors note that existing QFL systems assume that all quantum clients use the same quantum circuit design, which is unrealistic when devices have different numbers of qubits or data types. To address this issue, they propose a framework called BO-QFL that automatically enables each client to identify its best-performing quantum circuit architecture using Bayesian optimization. These locally optimized circuits are then combined at a central server through a new aggregation method that merges all clients’ circuit structures using a logical “union” rule and averages parameters only where applicable."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper focuses on a real challenge in quantum federated learning, so that different quantum devices (clients) have different hardware limits and data types, so one shared circuit design doesn’t fit all.\n\n2. The authors provide a theoretical analysis to show that the algorithm is stable and can converge under reasonable conditions, giving the method more credibility. \n\n3. The proposed BO-QFL framework allows each client to automatically find its own best quantum circuit using Bayesian optimization instead of forcing everyone to use the same one."}, "weaknesses": {"value": "1. The proposed BO-QFL mainly combines two known ideas: Bayesian optimization for quantum circuit search and masked averaging for model aggregation. The integration is logical but not fundamentally new compared to existing quantum federated learning (QFL) or quantum architecture search works. \n\n2. All experiments are done on classical simulators rather than real quantum hardware, so the framework’s ability to handle noise, qubit limitations, and circuit execution costs in practice is not truly verified. \n\n3. The “union” of different client circuits may produce a much larger and deeper global circuit, which could become inefficient or even infeasible to implement on real devices. The paper provides no analysis or results on this issue. \n\n4. The convergence proof relies on strong assumptions (e.g., smoothness, Polyak-Łojasiewicz condition, bounded bias of Bayesian optimization) that make the result theoretically correct but practically weak. The analysis does not cover how heterogeneous circuits change these bounds.\n\n5. The benchmarks (MNIST, Fashion-MNIST, HAR) are small and classical datasets, so the results do not convincingly demonstrate QFL advantages on large-scale or quantum-native problems. \n\n6. The paper does not report the runtime, number of evaluations, or communication overhead of Bayesian optimization across clients, leaving uncertainty about real-world feasibility."}, "questions": {"value": "1. How large does the global “union” circuit become? In particular, when each client has its own circuit structure, the logical “union” might make the global circuit much larger. Could the authors provide quantitative results on how the circuit depth and gate count scale with the number of clients?\n\n2. How is hardware heterogeneity actually simulated? The paper claims to model clients with different qubit resources, but all tests seem to use classical simulators. How exactly do you simulate differences in qubit counts or noise levels across clients? \n\n3. Why is Bayesian optimization preferred over reinforcement learning or gradient-based circuit search? What advantages does BO offer in this federated quantum context? \n\n4. What happens if two clients find very different optimal architectures? How does the aggregation rule handle extreme cases where there is little overlap between client circuits? Does performance degrade as architectural diversity grows?\n\n5. How are the local objectives for Bayesian optimization defined? Is the optimization guided by validation accuracy, loss, or fidelity metrics? And how many evaluations are needed for each local BO step? \n\n6. How much communication and computation overhead does BO-QFL introduce? Since each client runs its own optimization and sends its parameters back to the server, could the authors provide a timing or cost analysis compared with the standard FedAvg-QFL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "xCb2S78Ssf", "forum": "iFjqavBNWD", "replyto": "iFjqavBNWD", "signatures": ["ICLR.cc/2026/Conference/Submission22757/Reviewer_ruBB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22757/Reviewer_ruBB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923155567, "cdate": 1761923155567, "tmdate": 1762942373419, "mdate": 1762942373419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of this paper is to develop a quantum federated learning algorithm that can  handle heterogeneity. The authors address this problem by leveraging techniques from Bayesian optimization, then they provide analytical and experimental results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The idea of using Bayesian optimization to find optimal quantum circuits is meaningful.\n+ Heterogeneity is a problem less explored in the context of QFL and thus the work is timely.\n+ The theoretical results are rigorous."}, "weaknesses": {"value": "- Despite their rigor, the theoretical results provide very limited new insights.\n- The aggregation seems to follow an incremental version of FedAvg and, thus, it is not clear if it is the most effective way to capture heterogeneous systems.\n- It is not clear why one needs a federated learning in this space of quantum networks.\n- The experimental results are very limited to basic MNIST datasets. The baselines are also very rudimentary."}, "questions": {"value": "- What is the benefit from the insights gained out of Theorem 1? It seems these are known notion in classical FL. What is unique to quantum here?\n- Can you modify your algorithm to introduce a more effective aggregation rule?\n- Can your approach properly handle non-IIDness?\n- Can you expand the experiments on more advanced datasets like CIFAR?\n- What is the complexity of the BO piece?\n- What are the quantum hardware requirements for your approach to actually be deployed?\n- What is the communication overhead and would you require entanglement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OU8T7mgupb", "forum": "iFjqavBNWD", "replyto": "iFjqavBNWD", "signatures": ["ICLR.cc/2026/Conference/Submission22757/Reviewer_eeHq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22757/Reviewer_eeHq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22757/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004664095, "cdate": 1762004664095, "tmdate": 1762942373175, "mdate": 1762942373175, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}