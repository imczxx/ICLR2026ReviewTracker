{"id": "FcOEo7iLQx", "number": 9826, "cdate": 1758142370762, "mdate": 1759897693457, "content": {"title": "ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty", "abstract": "Robust reinforcement learning (Robust RL) seeks to handle epistemic uncertainty in environment dynamics, but existing approaches often rely on nested min--max optimization, which is computationally expensive and yields overly conservative policies. We propose \\textbf{Adaptive Rank Representation (AdaRL)}, a bi-level optimization framework that improves robustness by aligning policy complexity with the intrinsic dimension of the task. At the lower level, AdaRL performs policy optimization under fixed-rank constraints with dynamics sampled from a Wasserstein ball around a centroid model. At the upper level, it adaptively adjusts the rank to balance the bias--variance trade-off, projecting policy parameters onto a low-rank manifold. This design avoids solving adversarial worst-case dynamics while ensuring robustness without over-parameterization. Empirical results on MuJoCo continuous control benchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank baselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC, Parseval), but also converges toward the intrinsic rank of the underlying tasks. These results highlight that adaptive low-rank policy representations provide an efficient and principled alternative for robust RL under uncertainty.", "tldr": "", "keywords": ["Reinforcement Learning", "robust reinforcement learning", "model uncertainty", "low rank"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f48fd1130ed84dc72e53c43b3e56bdda5ea55335.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces AdaRL (Adaptive Rank Representation for Reinforcement Learning), a novel framework for robust RL under epistemic uncertainty. Traditional robust RL methods rely on nested min–max optimization, which is computationally expensive and tends to produce overly conservative policies. AdaRL instead leverages adaptive low-rank policy representations to align model complexity with the intrinsic dimensionality of the task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The bias–variance trade-off (Theorem 1) under epistemic uncertainty is well-motivated and technically non-trivial. \n* The low-rank structure itself as a helpful prior can be useful to handle the uncertainty in the system. Interstingly, the results of the low-rank policy correspond to the rank of the task."}, "weaknesses": {"value": "* The main concern lies in the paper’s positioning. The authors claim that AdaRL is a principled alternative for robust RL under model uncertainty; however, it appears to be more accurately described as a method for solving (soft) robust RL problems rather than redefining the framework itself.\n* The proposed method seems somewhat disconnected from the experimental design. While the algorithm assumes uniform sampling from a Wasserstein uncertainty set over dynamics, the experiments rely on predefined modifications to system parameters. This discrepancy weakens the rigour of the robustness claims.\n\n* There is still room for improvement in presentation and clarity:\n\n  - Figure 1 is blurry and mostly reiterates text descriptions; the framework could be visualized more intuitively.\n  - Line 113: it should be **P** instead of **\\mathcal{P}** in the MDP tuple.\n  - $\\theta$ is the parameter of the Q-function of the policy function? \n  - Theorem 1, can you further explain what is $b_{\\mathcal{P},\\omega^o}$  since in Equation 8 $\\omega^o$ should be calculated based on $\\mathcal{P}^o$ , and where is $\\sigma_{\\mathcal{P},r}$ defined? \n\n  - Figure 2 contains text that is too small to read.\n\n  - Figure 3 includes unclear or incomplete titles.\n\n* The experimental evaluation is limited to a small set of tasks and parameter variations. Including comparisons such as training with predefined fixed-rank policies would provide a stronger validation of the adaptive mechanism."}, "questions": {"value": "* Is there a way to mitigate or handle the temporary performance drop observed after rank changes?\n* The paper shows that the learned rank converges to the intrinsic rank of each task. Could you briefly explain how this intrinsic rank is defined? Is it a property of the environment, the policy representation, or both?\n* In Equation (7), why not explicitly define the expectation over both sss and aaa? The current notation could be clarified for better readability.\n* In Equation (11), what is the purpose of the projection operator if Equation (12) already constrains the policy to lie in $\\mathcal{M}_r$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SuJRdxDmif", "forum": "FcOEo7iLQx", "replyto": "FcOEo7iLQx", "signatures": ["ICLR.cc/2026/Conference/Submission9826/Reviewer_ucxQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9826/Reviewer_ucxQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860939509, "cdate": 1761860939509, "tmdate": 1762921309360, "mdate": 1762921309360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AdaRL, a robust-RL method that improves generalization under epistemic dynamics uncertainty by adapting the rank of policy/critic representations during training. Uncertainty is modeled as a Wasserstein ball around a nominal kernel; trajectories are sampled from this ball rather than solving an inner worst-case problem. Learning is posed as a bi-level program: the lower level optimizes an entropy-regularized return under a fixed rank; the upper level selects the rank using a cumulative-spectrum rule and projects parameters to a low-rank manifold. Rank is updated at scheduled checkpoints. Experiments on MuJoCo (Hopper, Walker2d, Ant, Humanoid) show gains over fixed-rank SAC and robust baselines (RNAC, Parseval)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear positioning versus min–max robust RL: AdaRL avoids repeatedly solving the inner worst-case kernel by sampling from a Wasserstein ball and controlling model capacity via rank. This is computationally attractive and easy to implement in deep RL.\n- Principled and practical rank search: the cumulative singular-value rule plus projection yields a usable upper-level step; a hard-threshold alternative is ablated and shown to stagnate.\n- Simple enforcement: the rank constraint is implemented by inserting a linear bottleneck between layers; occasional SVD refines factors."}, "weaknesses": {"value": "- Ambiguity-set realism and certification: sampling uniformly from a Wasserstein ball is less conservative than worst-case evaluation. It would be valuable to report worst-case performance or couple AdaRL with a tractable DRO evaluation to support robustness claims.\n- Baseline parity: RNAC is presented with its PPO implementation while AdaRL uses SAC; this can confound comparisons. Reference implementations exist and should be matched where possible.\n- Exact placement of rank constraints: clarify which actor/critic layers are factorized, and add ablations (actor-only, critic-only, both)."}, "questions": {"value": "- Can you report performance under explicit worst-case dynamics for small tasks (e.g., via a tractable Wasserstein-DRO evaluation) to complement nominal and perturbed-physics tests?\n- Do you adapt rank for both critics as well as the actor? Please include actor-only/critic-only/both ablations and a timing table for SVD/projection overhead.\n- For RNAC, did you explore an SAC backbone or stronger PPO tuning to equalize compute and tuning budgets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9zZTDM8IL8", "forum": "FcOEo7iLQx", "replyto": "FcOEo7iLQx", "signatures": ["ICLR.cc/2026/Conference/Submission9826/Reviewer_niSV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9826/Reviewer_niSV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953313328, "cdate": 1761953313328, "tmdate": 1762921308710, "mdate": 1762921308710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle the problem of epistemic uncertainty in robust reinforcement learning (RL) by providing a bi-level optimization framework that analyzes the bias-variance trade-off in entropy regularized RL. This framework combats the over-conservatism of a learned policy that often arises due to the nested min-max structure of the optimization problem. This is accomplished by optimizing under fixed, low-rank constraints when in the inner loop, then dynamically adjusting the rank in the outer loop and projecting the parameters on to a low-rank manifold."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper was neat, organized, and well written. It naturally follows and addresses some of the questions that would arise as a reader is going through it. The authors did a good job discussing the bias-variance tradeoff.\n2. Adaptively controlling the policy's rank was a fascinating twist, allowing for the incorporation of conservatism during learning. Namely, I found the improved efficiency particularly interesting as it stemmed from repeatedly swapping between traditional policy optimization with a fixed rank to dynamically change the rank.\n3. I found the way the algorithm was structured interesting. Namely, by swapping between the lower-level policy optimization and upper level adaptation of the rank and subsequent projection.\n4. The experiments were conducted on varied environments, showing empirical validation for the author's claims. Additionally, the author backs up their claims with recent relevant work [1].\n\n[1] Saket Tiwari, Omer Gottesman, and George Konidaris. Geometry of neural reinforcement learning in continuous state and action spaces. 2025."}, "weaknesses": {"value": "1. While I found the problem that this paper tackles interesting, the work surrounds a single theorem with minimal additional contributions while relying on some rather strong assumptions. Ultimately, this may not sufficiently capture the complex nuances of more complicated tasks.\n2. There is a gap between the theoretical analysis and the practical implementation of the experiments. Namely, the theory analyzes rank $r$ of the covariance matrix in a linear model, but in Appendix A.4.1, the authors the authors instead constrain the rank of the weight matrix $W$ within a non-linear deep neural network. The authors perform a sanity check in figure 2, and while doing so is responsible, it is not a substitute for a formal theoretical argument.\n3. The authors briefly discuss the difficulty of deriving convergence guarantees, however, such results are not presented in this work. Though an interesting problem to tackle, the only theoretical result being Theorem 1 leaves significant open questions in this topic.\n4. On line 417, it is mentioned that environment dynamics vary across episodes. How are these dynamics varied?\n5. Is the uncertainty set also compact?"}, "questions": {"value": "1. Line 112, specifying a \"Discounted Markov decision process\" helps provide additional clarity between this setting and the average reward setting.\n2. Typo on line 142. It looks like there is a period instead of a comma.\n3. It would be nice to see $\\pi^*_\\mathcal{P}$ defined precisely where it is first mentioned on line 148.\n4. I would've liked to see all three assumptions listed in the main body prior to theorem 1.\n5. Repeated/similar wording on lines 243 and 246.\n6. Extra period on line 312.\n7. $\\tau$ is not defined in equation 12.\n8. Line 740 mislabels the assumption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XseZrr8NHv", "forum": "FcOEo7iLQx", "replyto": "FcOEo7iLQx", "signatures": ["ICLR.cc/2026/Conference/Submission9826/Reviewer_5kUX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9826/Reviewer_5kUX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966171128, "cdate": 1761966171128, "tmdate": 1762921308139, "mdate": 1762921308139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The guiding principal of the paper is 'simple models (non-overly fitted) generalizes better'.  The paper tries to learn low rank policy for the given nominal environment which may perform better for other environment in the uncertainty set. It adaptively learn the rank of the policy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of aproaching robust RL from low-rank perspective is potent."}, "weaknesses": {"value": "See questions below."}, "questions": {"value": "Q1) The approach remains the same for the all uncertainty levels. That is, suppose we are given two secnarios: First where model uncertainty is small and other where its large. The two scenarios  have different robust optimal policies, however the approach in the paper remains the same for two, hence rendering the same policy. How it can be tackled effectively.\n\nQ2) The intuitive relation between low rank policies and robustness is interesitng, however I don't see sufficient concrete theoretical support for it.\n\nQ3) How can this approach be extended to large MDPs in online settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9HZblNJBHZ", "forum": "FcOEo7iLQx", "replyto": "FcOEo7iLQx", "signatures": ["ICLR.cc/2026/Conference/Submission9826/Reviewer_8nqM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9826/Reviewer_8nqM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978503945, "cdate": 1761978503945, "tmdate": 1762921307719, "mdate": 1762921307719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}