{"id": "XekKGk5Fcd", "number": 22853, "cdate": 1758336341801, "mdate": 1759896842734, "content": {"title": "AMPS: Adaptive Modality Preference Steering via Functional Entropy", "abstract": "Multimodal Large Language Models (MLLMs) often exhibit significant modal-\nity preference, which is a tendency to favor one modality over another. Prior\nwork has applied steering methods to adjust the modality preference of MLLMs.\nHowever, these conventional approaches apply a uniform steering intensity to all\nsamples. This lack of adaptation is problematic because strong steering can dis-\nrupt a model’s standard inference capabilities, leading to high error rates, while\nweak steering may be ineffective. To address this limitation, a sample-wise diag-\nnostic tool is required to measure MLLMs’ susceptibility to steering across differ-\nent multimodal samples. To reduce the disruption of strong steering to MLLMs’\ninference capabilities, we first introduce a diagnostic metric that quantifies the in-\nformation contribution ratio from each modality in MLLMs. This metric reveals\nvarying susceptibility to steering across different samples. Building on these di-\nagnostic insights, we further propose a steering scaling strategy that applies lower\nsteering intensity for samples highly sensitive to steering, and design a learnable\nsteering module that automatically learns appropriate scaling patterns, enabling\ncontext-aware adjustment of modality preference. Experimental results show that\nour context-aware scaling method outperforms conventional steering strategies\nin modulating modality preference, achieving effective adjustment while signif-\nicantly reducing generation errors.", "tldr": "", "keywords": ["Modality Preference", "Model Steering", "Multimodal Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/656d99e3de9ef3a4799f78b0c236898f9751df0b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Adaptive Modality Preference Steering (AMPS) to tackle the significant modality preference problem in MLLMs. The authors propose a novel, sample-wise diagnostic metric, Modality Contribution Score (MCS), derived from functional entropy, which quantifies each modality's information contribution and reveals varying steering susceptibility. Building on MCS insights, AMPS employs a learnable module that adaptively adjusts steering intensity: applying weaker steering for highly sensitive samples to prevent errors, and stronger steering for robust ones to ensure an effective preference shift. Experimental results on the MC2 dataset demonstrate that AMPS significantly outperforms conventional strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel MCS diagnostic, grounded in functional entropy, combined with an original adaptive, learnable steering framework (AMPS).\n2. This is a high-quality research with sound methodology, comprehensive experimental validation across models and tasks, strong baselines, and clear ablation studies.\n3. This paper is well-structured, clear articulation of problem, solution, and benefits. Effective use of figures and explanations.\n4. Significantly advances MLLM control by providing both a powerful diagnostic and an effective adaptive steering mechanism."}, "weaknesses": {"value": "1. The paper frequently mentions reducing generation errors. Could the authors provide a more detailed and quantitative definition of these errors and explain the evaluation methodology?\n2. Figure 2 appears wrong expression “Tuxtual”."}, "questions": {"value": "1. The paper's related work acknowledges modality bias in video QA. Why were no experiments conducted on video tasks, and what specific challenges would arise from such an application?\n2. Why is the log-Sobolev bound valid in your setup, and what evidence support replacing the true KV-state distribution with a Gaussian?\n3. While the theoretical foundation of MCS is rooted in functional entropy and Fisher information, please elaborate on the practical approximations made in Algorithm 1 and discuss their implications for the fidelity and robustness of the MCS measurement.\n4. Without online MCS estimation when inference, isn’t the “adaptive” scaling effectively an offline fit? How do you validate robustness and OOD generalization?\n5. Do the paper include results in other general MLLM understanding or reasoning benchmarks? Like VQA, OCR and multi-round QA.\n6. Can the authors include results versus more relevant adaptive controllers (AutoSteer, CausalMM, decoding-time reweighting) and clarify how metric fairness is maintained when comparing to prompt-based methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cqguNaSdlG", "forum": "XekKGk5Fcd", "replyto": "XekKGk5Fcd", "signatures": ["ICLR.cc/2026/Conference/Submission22853/Reviewer_2wpA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22853/Reviewer_2wpA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761457021515, "cdate": 1761457021515, "tmdate": 1762942415004, "mdate": 1762942415004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on adaptive modality preference steering in multimodal large language models (MLLMs), which can be prone to modality preference conflicts during inference. The authors identify the challenge of using a fixed steering strength that can either over- or under-adjust the model’s behavior depending on the sample. \n\nTo resolve this issue, they propose a new methodology called AMPS (Adaptive Modality Preference Steering). It uses a sample-level diagnostic metric, Modality Contribution Score (MCS), to measure the sensitivity of the model to modality preference shifts, which is based on functional entropy. AMPS adaptively adjusts the strength of modality preference steering based on the MCS. Experimental results demonstrate that AMPS significantly improves modality preference transfer compared to traditional static steering approaches, while reducing the generation errors, particularly in more sensitive tasks.\n\nOverall, the idea of the proposed method is reasonable and somewhat interesting. However, the paper writing can be further improved, where the intuition of modules is not very clearly clarified and may impede readability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis research addresses the valuable and practical task of mitigating modality preference bias in multi-modal large language models (MLLMs), which directly impacts real-world performance and application versatility.\n2.\tThe proposed method introduces a novel modality contribution score (MCS) mechanism for adaptive steering, effectively resolving limitations of uniform steering strength through sample-specific sensitivity analysis. The functional entropy is interesting to measure the sensitivity of modalities. The data-adaptive steering is also reasonable.\n3.\tExperiments demonstrate that the AMPS framework significantly improves modality preference shifting while reducing task errors, providing empirical validation for the approach."}, "weaknesses": {"value": "1.\tWhile the use of modality contribution score (MCS) is innovative and interesting, the detailed intuition and theoretical justification can be further enhanced. This lack of background knowledge (especially in Eq. 3-5) may confuse broader readers. Besides, it would be better to provide a more detailed theoretical analysis or evidence supporting MCS.\n2.\tThe paper compares AMPS with static steering methods but lacks a comparison to recent approaches in modality preference steering. The baselines are introduced unclearly. It would be better to include comparisons with state-of-the-art methods to better highlight the advantages of AMPS.\n3.\tThe experiments primarily use a limited set of datasets (e.g., MC2, Qwen-VL, LLaVA), which may not fully represent the diversity of tasks where modality preference steering is important. It would be better to evaluate the method on a broader range of datasets and include examples that stress-test the method under various real-world conditions. For instance, incorporating datasets that involve noisy or ambiguous inputs could provide further insight into the robustness of AMPS.\n4.\tThe paper lacks ablation studies to assess the impact of individual components of AMPS. It would be helpful to include ablation experiments to understand the contribution of each part of the framework.\n5.\tThe current experimental results fail to sufficiently rule out the possibility of overfitting. It is suggested that supplementary validations be conducted across heterogeneous datasets and multi-scale model architectures to ensure performance improvements are generalizable rather than contingent upon specific training data or model configurations.\n\nTypos：\nThere are several typos. For example: \n1. Line 185: We-> we\n2. Line 198: f -> $f$\n3. Line 204: We -> we\n4. Line 265: “Previous studies ()” no citations.\nBy the way, some equations do not have punctuation marks at the end."}, "questions": {"value": "Please see strengths and weaknesses. Besides:\n\n1.\tHow does MCS perform on more complex multi-modal tasks?\n\n2.\tWhy does Eq.(11) apply the scaling factor (1+γ) when integrating Eq.(9) and Eq.(10) to formulate the steering module’s prediction target?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HxodmQoGQl", "forum": "XekKGk5Fcd", "replyto": "XekKGk5Fcd", "signatures": ["ICLR.cc/2026/Conference/Submission22853/Reviewer_oYWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22853/Reviewer_oYWu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813737462, "cdate": 1761813737462, "tmdate": 1762942414806, "mdate": 1762942414806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the modality preference in Multimodal LLMs, where models may over-rely on text or visual inputs irrespective of user intention or task requirements. Existing steering methods often apply uniform intervention strengths, which can degrade performance. To address this, the authors propose the Modality Contribution Score (MCS), a diagnostic metric that evaluates the contribution of each modality in a context-sensitive manner. Building on MCS, they introduce AMPS, an adaptive steering framework that dynamically adjusts intervention strength on a per-sample basis via a learnable module. The approach is supported by theoretical grounding (functional entropy, Sobolev inequality), clear algorithmic implementation, and extensive experiments showing improved preference alignment and reduced errors compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated: The proposal of the Modality Contribution Score (MCS) based on functional entropy and Fisher information is rigorous and well-motivated.\n2. Extensive empirical results: The paper provides comprehensive empirical analysis—including comparisons with prompt-based, static steering, and prior adaptive approaches—across multiple model families (LLaVA, Qwen-VL) and sizes. In Table 1 and Table 2, AMPS shows consistently superior performance for controlling preference while minimizing error rates."}, "weaknesses": {"value": "1. Experiments on more benchmarks are needed: the experiments are executed with the $M C^{2}$ dataset only, and lack evaluation on broader, more real-world multimodal tasks, such as MME, MM-Vet, LLaVA Bench, and MMstar.\n2. To avoid a tendency on one modality, the easiest way is to move the tokens or replace the tokens with pad tokens. Have you tried this strategy?\n3. Different models and evaluation benchmarks are mixed across Tables 1 and 3, creating confusion and undermining the interpretability of results. \n\nIf possible, the author can reorganize the experiments, employing more advanced benchmarks for evaluation.\nIf you can provide convincing clarification, I would be open to increasing the score."}, "questions": {"value": "My main concern is the evaluation strategy; the current evaluation is limited, can not demonstrate the effectiveness of this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hBDczzsYj9", "forum": "XekKGk5Fcd", "replyto": "XekKGk5Fcd", "signatures": ["ICLR.cc/2026/Conference/Submission22853/Reviewer_ofSL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22853/Reviewer_ofSL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997015751, "cdate": 1761997015751, "tmdate": 1762942414258, "mdate": 1762942414258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AMPS, an adaptive steering framework for controlling modality preference in multimodal large language models (MLLMs). The authors first propose the Modality Contribution Score (MCS) as a diagnostic metric with functional entropy and Fisher information to quantify the per-sample reliance on each modality. Leveraging this insight, they design a context-aware, learnable module to apply sample-specific steering intensities, aiming to shift modality preference more effectively than traditional uniform (static) steering. Experimental results demonstrate improved preference adjustment and robustness."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper introduces the Modality Contribution Score (MCS), grounded in functional entropy and Fisher information, to provide a rigorous quantification of modality contribution at the sample level.\n2.\tInstead of the uniform steering of traditional methods, the authors propose the sample-adaptive steering via a scaling coefficient, justified by the diagnostic metric. The inclusion of the learnable module further enables context-sensitive adjustment.\n3.\tThe paper provides a comprehensive evaluation across 2 model families and scales, and the results show consistent improvements over previous baselines."}, "weaknesses": {"value": "1.\tThe context-aware scaling factor $\\gamma$ (Equation 9, Page 6) is constructed as a linear deviation from an anchor ratio, modulated by $\\beta$. It seems somewhat heuristic. A more detailed justification for why this specific formula is the right way to quantify \"severity of preference\" would strengthen the method.\n2.\tThe MCS measurement requires multiple forward passes with KV-cache perturbations for a single input. The computational cost of this diagnostic process is not discussed.\n3.\tIt’s better for the authors to take more recent and highly relevant works and benchmarks on modality preference steering into comparison."}, "questions": {"value": "1.\tThe MCS metric is central to the method. Did the authors explore alternative ways to quantify modality contribution or susceptibility to steering?\n2.\tWill the authors release code, data splits, and detailed hyperparameter configuration to facilitate reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rGl2EVmE1e", "forum": "XekKGk5Fcd", "replyto": "XekKGk5Fcd", "signatures": ["ICLR.cc/2026/Conference/Submission22853/Reviewer_6Vuj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22853/Reviewer_6Vuj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22853/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997231219, "cdate": 1761997231219, "tmdate": 1762942413967, "mdate": 1762942413967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}