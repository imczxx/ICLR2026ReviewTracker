{"id": "Suja4AQ9Fo", "number": 17287, "cdate": 1758274242484, "mdate": 1759897184550, "content": {"title": "Both Local Validity and Global Effectiveness Matter:  Decoupled Credit Assignment for Long‑Horizon Agentic Learning", "abstract": "The natural-language action space of Large Language Model (LLM) agents creates a real risk of invalid outputs (e.g., API rejections, parsing errors). Consequently, in Reinforcement Learning (RL) for long-horizon LLM agents, learning to generate a locally valid action in each turn is as crucial as selecting globally effective one. However, this requirement was overlooked by the prevailing additive paradigm for credit assignment in agentic RL. Specifically, it computes an action's credit by summing an estimated local score with the trajectory-level score. This paradigm assigns a ``contribution\" score to all actions regardless of their validity, allowing invalid actions to be assigned positive credit, especially in positive trajectories. To address this, we propose Multiplicative Gated Rewards (MGR), which decouples local action-level validity from global effectiveness. MGR uses a fact-based validity signal, derived from direct environment feedback and syntactic validity, to determine the action-level score (e.g., $\\pm$1). This score is then multiplied by the magnitude of the trajectory-level score. This ensures the action's validity strictly governs the reward's polarity, preventing credit misassignment. Experiments demonstrate that our method improves training stability and achieves SOTA performance on long-horizon LLM agent benchmarks. Code of MGR has been uploaded in the Supplementary Material.", "tldr": "We fix RL for LLM agents by multiplying a \"is the action valid?\" signal with the \"was the action good?\" signal, instead of adding them, which stops the agent from being rewarded for writing bad code.", "keywords": ["Agentic RL; Long‑Horizon Agent;"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/60baf1dca73ed65a8ccdbd8c7abd6f3128098338.pdf", "supplementary_material": "/attachment/8c6d41f6ddafdbd6b6192fa5f56ee3b553b0f325.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Multiplicative Gated Rewards (MGR), a novel reinforcement learning framework for LLM agents that decouples local action validity from global trajectory effectiveness in long-horizon tasks. The key idea is that every action produced by an LLM agent must be both locally valid (syntactically and executably correct) and globally effective (helpful toward task success). Existing additive credit-assignment methods conflate these dimensions, rewarding invalid actions that appear in successful trajectories and penalizing valid actions in failed ones. MGR addresses this by introducing a multiplicative gating mechanism that strictly enforces validity at the action level before scaling by global success magnitude."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important problem in LLM-agent RL, credit assignment, which directly affects training stability and generalization.\n2. The proposed decoupling between sign (validity) and magnitude (strategy) is both intuitive and empirically effective, leading to clearer reward signals and better long-term reasoning behavior.\n3. Across ALFWorld and AppWorld, MGR outperforms SFT, GRPO, GiGPO, and Loop, especially on harder tasks requiring longer reasoning chains."}, "weaknesses": {"value": "1. While MGR’s “factual validity” is well-defined, its design relies on hard-coded heuristics, error detection, format checks, repetition counts, rather than learned notions of action value. This may incentivize the agent to game surface-level rules rather than develop deeper understanding of action utility, leading to potential reward hacking or brittleness across unseen domains.\n2. Since MGR’s local reward is purely syntactic or execution-based, it lacks any semantic notion of whether the action actually helps the task. The trajectory-level term compensates only indirectly. The agent could thus learn to maximize validity without improving task-level reasoning if the environment reward is sparse or delayed.\n3. The method introduces several coefficients ($\\alpha$, $\\beta$, $\\gamma$, $q$, $p_min$, schedule thresholds) whose interdependence could make it sensitive to domain or dataset scale. The paper provides defaults but lacks a robustness or sensitivity analysis beyond $\\beta$."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hxZlwC7Gcs", "forum": "Suja4AQ9Fo", "replyto": "Suja4AQ9Fo", "signatures": ["ICLR.cc/2026/Conference/Submission17287/Reviewer_tWSq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17287/Reviewer_tWSq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893874724, "cdate": 1761893874724, "tmdate": 1762927230360, "mdate": 1762927230360, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel technique for credit assignments in long-horizon agent tasks for LLMs. Specifically, they propose two orthogonal components that make up their reward through a multiplicative gate, local action validity reward (+1 or -1 sign based on the action validity) and a trajectory level advantage score. The method outperforms other techniques like GRPO and Loop on ALFworld and AppWorld tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The credit assignment problem being tackled is important\n2. The paper's idea is interesting and well motivated\n3. The ablations study and analysis are insightful"}, "weaknesses": {"value": "1. The method mostly relies on heuristics for the local-level action feedback, which seems like added bias and transfer on other tasks is uncertain\n2. It's likely quite tricky to use this method on a variety of problems where it's not always clear what constitute as an invalid action, or where there is not an automated feedback for invalid actions\n3. There are no uncertainty/error bounds in the experiments.\n4. Introduction of various new hyperparameters makes me wonder how sensitive the method is to them, and how difficult they would be to tune on new tasks.\n4. The method is tested on only two tasks, it's possible that the method and hyperparameters have been overfitted on just these two tasks."}, "questions": {"value": "1. How would the method be applied to tasks where action-level validity signals are not available? \n2. How sensitive is the method to the various hyperparameters introduced, and how well does it transfer to other tasks?\n3. Is it possible to add more seeds to get error bounds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GGWJMTDpcx", "forum": "Suja4AQ9Fo", "replyto": "Suja4AQ9Fo", "signatures": ["ICLR.cc/2026/Conference/Submission17287/Reviewer_LUMB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17287/Reviewer_LUMB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761912954103, "cdate": 1761912954103, "tmdate": 1762927228946, "mdate": 1762927228946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Multiplicative Gated Rewards (MGR), a reinforcement learning framework for long-horizon LLM agents that separates local action validity from global effectiveness. Instead of adding rewards, MGR multiplies a validity signal with a trajectory-level score, ensuring invalid actions are penalized even in successful trajectories. A dynamic gating mechanism creates an implicit curriculum, first learning valid actions, then strategic sequencing. Experiments on ALFWorld and AppWorld show MGR achieves state-of-the-art performance over more challenging tasks compared with baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is easy to follow.\n2. The paper systematically analyzes the key issue of credit assignment in long-trajectory learning for LLM agents and proposes a practical solution to address it.\n3. The experiments are convincing, as MGR demonstrates a remarkable advantage on more challenging L2 tasks."}, "weaknesses": {"value": "1. The method introduces several hyperparameters for control, such as α and β, whose sensitivity remains unclear, which is critical for assessing the robustness and practical applicability of this approach.\n2. The method relies on explicit action validity signals from the environment, limiting its applicability to tasks with well-defined executable actions and making it unsuitable for open-ended or semantic language tasks without clear feedback."}, "questions": {"value": "1. Can the method be applied to more complex environments, such as embodied settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ih6jxymW2f", "forum": "Suja4AQ9Fo", "replyto": "Suja4AQ9Fo", "signatures": ["ICLR.cc/2026/Conference/Submission17287/Reviewer_Ao4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17287/Reviewer_Ao4r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958193412, "cdate": 1761958193412, "tmdate": 1762927228714, "mdate": 1762927228714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reward function, Multiplicative Gated Rewards (MGR), for long-horizon LLM agents, which decouples \"local action validity\" (step is syntactically/executably valid) from \"global effectiveness\" (trajectory quality), then fuse them multiplicatively so invalid steps contribute zero (or negative) credit even in successful rollouts. A stochastic gate balances valid steps in failed trajectories to avoid overwhelming positives. Experiments on ALFWorld and AppWorld with Qwen2.5-7B/Qwen3-8B show higher success rates than baselines; ablations indicate the gate, critic, and repetition penalty matter materially."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses credit assignment problem, i.e. additive credit lets invalid steps get positive credit in successful trajectories.\n- Method is simple, orthogonal to PPO/GRPO training, and given in clear pseudocode; implementation details for validity checks are explicit for both environments.\n- Consistent gains on ALFWorld/AppWorld; learning curves show improved stability and faster rise in both action-validity and task success.\n- Some ablations provided: removing gating/critic/repetition penalty degrades sharply, supporting design choices.\n- Method naturally induces a curriculum"}, "weaknesses": {"value": "- Novelty is incremental. Multiplicative gating is a principled reweighting/masking; not a new credit-assignment theory.\n- Local validity is heuristically defined and environment-specific; portability beyond text/code interfaces is unclear. Provide a (ideally formal) definition up front\n- Gating schedule is heuristic. The batch-stat–driven sign flip lacks theory; sensitivity and failure modes aren’t fully characterized.\n- Reporting focuses on success rates; analysis on sample efficiency and exploration would be appreciated."}, "questions": {"value": "- Does the stochastic sign gate ever destabilize learning or introduce bias, especially early in training when the proportion of valid actions is low?\n- Can MGR be applied on top of learned or preference-based reward models (e.g., PRM, CAPO, RLAIF)? If so, is the gating still beneficial, or redundant?\n- Is there any quantitative evidence that the multiplicative formulation improves gradient signal quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GeLt1cn3w2", "forum": "Suja4AQ9Fo", "replyto": "Suja4AQ9Fo", "signatures": ["ICLR.cc/2026/Conference/Submission17287/Reviewer_BvSX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17287/Reviewer_BvSX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998029298, "cdate": 1761998029298, "tmdate": 1762927228429, "mdate": 1762927228429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}