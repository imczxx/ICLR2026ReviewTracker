{"id": "YPIqYsW2Yd", "number": 1848, "cdate": 1756952474151, "mdate": 1763704770664, "content": {"title": "Prune to Fit: Enabling Federated Fine-Tuning within Edge Memory Budgets", "abstract": "Federated fine-tuning enables privacy-preserving Large Language Model (LLM) adaptation, but its high memory cost limits participation from resource-constrained devices. We propose FedPruner, an innovative federated fine-tuning paradigm that tackles this via intelligent layer pruning. FedPruner flexibly prunes the global model, creating personalized submodels based on device memory constraints. It employs a macro-micro synergistic pruning framework: a macro-level functionality-driven layer orchestration mechanism groups layers, while a micro-level importance-aware layer selection strategy prunes within groups to build device-specific submodels. We further introduce a fine-grained variant that independently prunes Multi-Head Attention and Feed-Forward Network components to precisely preserve critical architectural elements. Extensive experiments demonstrate that FedPruner significantly outperforms state-of-the-art methods with average accuracy gains of up to 11.11\\%. Moreover, it maintains strong robustness under varying memory constraints, yielding a 1.98\\% average performance improvement while reducing peak memory usage by 75\\%.", "tldr": "We propose FedPruner, an innovative federated fine-tuning paradigm that tackles memory constraints of participating devices via intelligent layer pruning.", "keywords": ["Federated Fine-Tuning", "Memory Efficient", "Layer Pruning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/38cee82c1ae3a35da075d8b430ce6e88ff722f95.pdf", "supplementary_material": "/attachment/81168c625879bdce68e63aa1d0e945ab1ea3f075.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes FedPruner, an innovative federated fine-tuning paradigm that addresses the memory constraints of edge devices via intelligent layer pruning. Specifically, it employs a macro-micro synergistic pruning framework that jointly considers layer functionality and layer contribution to coordinate the pruning process. At the macro level, the functionality-driven layer orchestration (FDLO) mechanism adaptively partitions layers into groups based on their functional characteristics using Centered Kernel Alignment (CKA) and graph partitioning. At the micro level, the importance-aware layer selection (IALS) strategy selects one representative layer from each group according to its contribution to model performance. Furthermore, the paper introduces 〖FedPruner〗^+, a fine-grained variant of FedPruner that extends the macro–micro synergistic pruning framework from the layer level to the component level."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is technically sound and supported by comprehensive experiments across multiple model scales and benchmarks.\n\n2. The paper provides the theoretical analysis regarding the convergence of the proposed method.\n\n3. The paper is generally well-motivated and easy to follow.\n\n4. The proposed method improves upon existing solutions and achieves notable performance and efficiency enhancements."}, "weaknesses": {"value": "1. The paper states that K is determined by the number of layers that the device memory can afford, but it does not provide a clear quantitative rule or algorithmic procedure for this determination. This lack of specification may hinder the method’s scalability and generalizability when applied to other model architectures or edge devices with varying memory constraints.\n\n2. The tables (e.g., Table 1) do not explicitly specify the evaluation metrics used for each benchmark. This lack of clarification may cause confusion for readers.\n\n3. The workflow (Algorithm 1) does not clearly specify whether the server distributes LoRA parameters corresponding to all layers of the global model or only to the layers involved in each device’s submodel during every communication round."}, "questions": {"value": "1. How is the number of layer groups (K) determined in practice for devices with different memory capacities? Is it empirically obtained by testing different configurations on real devices, or is it estimated analytically based on parameter size and memory constraints?\n\n2. In Table 5, for the sixth group G_6, many layers show a layer selection probability of 0.05. Could the authors clarify why layer 13 was ultimately selected, despite multiple layers having the same probabilities?\n\n3. In Algorithm 1, it appears that during each communication round r, each device recomputes the local similarity matrix using a batch of data. Since this matrix is computed based on the inference results of the global model, could the authors clarify whether the server distributes the LoRA parameters corresponding to all layers of the global model to each device in every round, or only those corresponding to the layers included in each device’s submodel?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2BkstqdtmY", "forum": "YPIqYsW2Yd", "replyto": "YPIqYsW2Yd", "signatures": ["ICLR.cc/2026/Conference/Submission1848/Reviewer_nDop"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1848/Reviewer_nDop"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761393349302, "cdate": 1761393349302, "tmdate": 1762915912027, "mdate": 1762915912027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedPruner, a novel federated fine-tuning framework that addresses device memory constraints via coordinated layer pruning. By combining macro-micro pruning strategies and fine-grained component-level control, the method achieves strong performance across benchmark datasets while significantly reducing memory usage."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a highly relevant and practical challenge in federated fine-tuning of large language models."}, "weaknesses": {"value": "1. The statement “fine-tuning LLaMA2-7B (Touvron et al., 2023) demands up to 26.9 GB of memory” may not be entirely accurate. Memory usage depends heavily on input token length and batch size, which can significantly increase GPU memory consumption beyond the reported value.\n\n2. In the introduction, the example states: “compared to optimizing LoRA modules and activations, reducing model parameters is more promising to lower training memory usage.” As mentioned, batch size and input token length values can greatly affect memory distribution, as highlighted in Fed-Pilot [1]. Additionally, the memory costs associated with storing gradients and context should also be taken into account.\n\n3. Miss a problem formulation for memory consumption.\n\n4. Several important related works are missing from the literature review. Fed-Pilot [1] provides a comprehensive formulation of memory consumption during fine-tuning and analyzes both static and dynamic activation memory. Fed-HeLLo [2] explores heuristic and Fisher Information-based strategies for layer-wise LoRA allocation. FlexLoRA [3] and HETLoRA [4] are also relevant baselines that should be included in comparative studies.\n\n5. More tasks and metrics should be evaluated in the experiments shown in Figure 2. Different tasks may rely on different levels of model representations. Since perplexity is only suitable for language modeling or summarization tasks, it may not reflect performance well in downstream tasks like question answering, where accuracy or F1-score would be more appropriate.\n\n6. Memory-efficient methods such as Fed-HeLLo, Fed-pilot, FLoRA, HETLoRA, and FlexLoRA are designed to support both homogeneous and heterogeneous resource settings. Can the proposed method also handle such heterogeneity? If so, how does it aggregate updates from clients with heterogeneous model configurations?\n\nReferences:\n\n[1] Fed-Pilot: Optimizing LoRA Allocation for Efficient Federated Fine-Tuning with Heterogeneous Clients. arXiv, 2024.\n\n[2] Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with Heterogeneous LoRA Allocation. IEEE Transactions on Neural Networks and Learning Systems (TNNLS), 2025.\n\n[3] Federated Fine-Tuning of Large Language Models under Heterogeneous Tasks and Client Resources. NeurIPS, 2024.\n\n[4] Heterogeneous LoRA for Federated Fine-Tuning of On-Device Foundation Models. EMNLP, 2024."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5jdFEF8GTT", "forum": "YPIqYsW2Yd", "replyto": "YPIqYsW2Yd", "signatures": ["ICLR.cc/2026/Conference/Submission1848/Reviewer_Lk6t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1848/Reviewer_Lk6t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626068097, "cdate": 1761626068097, "tmdate": 1762915911815, "mdate": 1762915911815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of performing federated fine-tuning under memory constraints on resource-limited devices. The authors propose a pruning-based method that strategically removes redundant layers to reduce model parameters and enable large model fine-tuning across heterogeneous clients.\n\nOverall, I appreciate the problem this paper aims to solve, and the proposed method appears to be novel. However, I still have concerns about the soundness of the method design and the adequacy of the experimental evaluation. I hope the author can solve all my confusion in the rebuttal."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of the paper is reasonable, and Figure 1 presents it effectively. The excessive memory consumption of model parameters is indeed a key challenge in federated fine-tuning, and the authors have clearly illustrated this issue.\n\n2. This paper is well-written and easy to follow. The overall logic and structure are coherent, and the presentation is clear.\n\n3. The experimental results clearly demonstrate the effectiveness of the proposed method compared with the baselines."}, "weaknesses": {"value": "1. The authors assume that resource-constrained devices cannot load the full model parameters. However, Step 1 states that each device performs inference to calculate inter-layer similarity, which inherently requires loading the entire model. This design contradicts the initial assumption about limited device capacity. The authors should clarify how Step 1 can be executed on devices that are unable to host the full model.\n\n2. Steps 1-3 appear to introduce considerable computational overhead, which may be unacceptable for resource-constrained devices. The authors should provide a detailed analysis or justification regarding this issue.\n\n3. The paper lacks discussion and comparison with traditional heterogeneous federated learning methods such as HeteroFL [R1], DepthFL [R2], and AdaptiveFL [R3]. It would be important to clarify whether these conventional approaches can be applied to federated fine-tuning. If they can, the authors should include corresponding baselines for comparison; if not, they should provide explanations and discussions on why these methods are not applicable.\n\n4. The paper lacks experiments evaluating the adaptability of the proposed method to different types of models. It would be valuable to assess how the method performs across various model architectures to better demonstrate its generalization capability.\n\n5. The paper lacks evaluation on the basic settings of federated learning, such as varying the number of clients, the participation ratio of clients in each round, and the distribution of client capabilities. These analyses are important to demonstrate the robustness and scalability of the proposed method.\n\n\n[R1] Heterofl: Computation and communication efficient federated learning for heterogeneous clients\n\n[R2] Depthfl: Depthwise federated learning for heterogeneous clients\n\n[R3] AdaptiveFL: Adaptive heterogeneous federated learning for resource-constrained AIoT systems"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VDPPkTTbaW", "forum": "YPIqYsW2Yd", "replyto": "YPIqYsW2Yd", "signatures": ["ICLR.cc/2026/Conference/Submission1848/Reviewer_qN12"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1848/Reviewer_qN12"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931502631, "cdate": 1761931502631, "tmdate": 1762915911396, "mdate": 1762915911396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FEDPRUNER, a framework designed to address the memory limitations that prevent resource-constrained devices from participating in federated fine-tuning of large language models. The authors develop a macro–micro synergistic pruning framework. Experiments on NLP tasks using LLaMA-based models show that the proposed pruning strategy effectively reduces memory consumption while maintaining model performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper addresses a highly relevant and timely problem. The memory bottleneck could prevent resource-constrained devices from participating in federated fine-tuning of LLMs. \n\n2. The proposed macro–micro synergistic pruning framework introduces a novel perspective for coordinating layer selection at multiple granularity levels.\n\n3. The experimental evaluation includes comprehensive baselines covering both memory-unaware and memory-aware methods, and the results appear promising and consistent across benchmarks."}, "weaknesses": {"value": "1. This paper’s motivation lacks clear logical grounding due to missing details and several reasoning gaps.\na. The discussion of the LLaMA-related OOM issue (lines 37–39, Fig. 1(a)) omits essential information such as data precision or quantization settings, which critically affect actual memory usage.\nb. The performance discrepancy between theoretical and empirical results in Fig. 1(b) when using TinyLLaMA is not analyzed or explained.\nc. Section 2.3 suffers from unclear reasoning and logical jumps. For instance, why must the “memory wall” problem (lines 120–122) necessarily be addressed through pruning? Are there alternative solutions? Moreover, the rationale for multi-layer pruning and the specific choice of pruning ten layers (lines 136–141) is not justified. The paper does not clearly explain why the heuristic baselines underperform or how the proposed method conceptually builds upon and extends these heuristics.\n\n2. The proposed method appears to be a general pruning strategy, and its specific connection to the federated learning setting is not clearly articulated. Moreover, if each client applies FEDPRUNER with different pruning configurations, would the aggregation process become highly complex? The paper would benefit from a deeper analysis of how the proposed framework handles such structural heterogeneity and its implications for global model aggregation and convergence.\n\n3. The experimental comparison suffers from missing and potentially unfair baselines. No other pruning-based methods are included in baselines, making it difficult to assess the effectiveness of FEDPRUNER’s pruning strategy relative to existing state-of-the-art pruning approaches for federated LLMs. Moreover, all compared baselines perform full-model fine-tuning, whereas FEDPRUNER fine-tunes pruned sub-models with inherently lower training time, memory usage, and communication costs, which compromises the fairness of the comparison.\n\n4. The core experiments focus solely on the LLaMA family and the NLP task domain. Given the rapid evolution of open LLMs, the use of LLaMA 2-7B appears somewhat outdated. It is recommended to include evaluations on more recent models, such as LLaMA 3, Gemma 3, or Qwen 2.5, to enhance the relevance and robustness of the findings. Moreover, the applicability of the proposed approach to other generative or multimodal tasks remains to be validated.\n\n5. The related work section overlooks several key studies that are closely related to FEDPRUNER. For instance, FlexLoRA [1] addresses heterogeneous rank adaptation in federated fine-tuning, LEGENDS [2] focuses on managing memory and computational heterogeneity during real-world deployment, and dynamic parameter pruning methods such as FedMef [3] explore similar resource-efficient objectives. A deeper discussion of how FEDPRUNER differs from or builds upon these works would help better position the contribution.\n\n[1] Bai et al. Federated Fine-tuning of Large Language Models under Heterogeneous Tasks and Client Resources. NuerIPS’2024.\n[2] Liu et al. Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous Devices. IEEE TMC’2025.\n[3] Huang et al. FedMef: Towards Memory-efficient Federated Dynamic Pruning. CVPR’2024."}, "questions": {"value": "See weak points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4jyuiFvdJT", "forum": "YPIqYsW2Yd", "replyto": "YPIqYsW2Yd", "signatures": ["ICLR.cc/2026/Conference/Submission1848/Reviewer_Cret"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1848/Reviewer_Cret"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1848/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947818106, "cdate": 1761947818106, "tmdate": 1762915910584, "mdate": 1762915910584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}