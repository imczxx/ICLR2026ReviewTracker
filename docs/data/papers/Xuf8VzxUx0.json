{"id": "Xuf8VzxUx0", "number": 20334, "cdate": 1758304867689, "mdate": 1763687966676, "content": {"title": "Savaal: Scalable Concept-Driven Question Generation to Enhance Human Learning", "abstract": "Assessing and enhancing human learning through question-answering is vital, yet automating this process remains challenging. We propose Savaal, a scalable question-generation system using large language models (LLMs) with three objectives: (i) scalability, enabling question-generation from hundreds of pages of text (ii) depth of understanding, producing questions beyond factual recall to test conceptual reasoning, and (iii) domain-independence, automatically generating questions across diverse knowledge areas. Instead of providing an LLM with large documents as context, Savaal improves results with a three-stage processing pipeline. Our evaluation with 76 human experts on 71 papers and PhD dissertations shows that Savaal generates questions that better test depth of understanding by $6.5\\times$ for dissertations and $1.5\\times$ for papers compared to a direct-prompting LLM baseline. Notably, as document length increases, Savaal's advantages in higher question quality and lower cost become more pronounced.", "tldr": "This paper presents a scalable question generation system for large documents using a novel three-stage pipeline.", "keywords": ["educational applications", "question generation", "human evaluation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe90a1da473483de43ee96ef5ddb568f2bef7bc0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Savaal is a scalable question-generation system using large language models (LLMs) to produce questions that test depth in conceptual understanding. It uses a three-stage pipeline: extracting and ranking key concepts, retrieving relevant passages, and prompting the LLM to generate questions. Savaal ensures that the generated questions are both targeted and conceptually rich, requiring deeper understanding by linking a given concept across different sections of a document. Evaluations show Savaal outperforms direct-prompting baselines in generating questions that better assess understanding, especially for longer documents. This approach is scalable particularly in the context of large documents where we would require large number of unique questions. Overall, the method described in the paper can be easily adapted to various domains, can you almost any LLM to perform various tasks in the pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper proposes a simple yet useful idea to generate deep conceptual questions. The method is well described, illustrated with good examples, good evaluation metrics. The paper is easy to read through and understand. The appendix has a lot of information which helped clear various doubts regarding the evaluation methodology, prompts etc. I believe this paper proposes a solution to a very important task of generating questions which not just test a student's recall ability but their understanding of concepts and their application. The idea seems original to the use case, though many parts of the pipeline are already from existing methods. The main strength of the idea is the ease with which it can be used to address the usecase here."}, "weaknesses": {"value": "The authors could have added some experiments or ablation studies to show if the choice of retriever model has any impact on the final performance. No mention of how this would work in cases where questions require logical reasoning, calculations. It would have been better if the authors provided some metrics using a different LLM than gpt-4o. This would help the readers get confidence that this method can be applied using an LLM at their disposal."}, "questions": {"value": "1. The authors use gpt-4o to generate questions in the direct prompting baseline. In such a case why do you also use the same LLM as a judge? Doesn't using a different LLM as judge make more sense?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XWkuggcEZx", "forum": "Xuf8VzxUx0", "replyto": "Xuf8VzxUx0", "signatures": ["ICLR.cc/2026/Conference/Submission20334/Reviewer_7ujz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20334/Reviewer_7ujz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760468481874, "cdate": 1760468481874, "tmdate": 1762933792635, "mdate": 1762933792635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Savaal, a framework designed to generate high-quality, multiple-choice questions from large documents (e.g., conference papers and PhD dissertations) to enhance human learning. The goal is to create questions that test deep conceptual understanding rather than simple factual recall. The system uses a three-stage pipeline: (1) Main Idea Extraction, which identifies and ranks key concepts from the document using map-reduce; (2) Relevant Passage Retrieval, which fetches text segments related to each key concept using a ColBERT retriever ; and (3) Question Generation, which uses an LLM to create questions based on the paired concepts and passages. The proposed work is evaluated experimentally to demonstrate effectiveness of the proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "S1) Clear Motivation: I appreciate the clear articulation of the limitations of alternative question-generation strategies, such as using the full document context (lost-in-the-middle, superficial questions) , section-based approaches (context fragmentation), or summarization (missing details, hallucinations). It provides concrete examples of how these methods fail to produce questions that test deep understanding.\n\nS2) Strong Evaluation Methodology: The evaluation is a major strength. Instead of relying on proxy metrics, the authors recruited 76 human experts (often the authors of the source documents) to evaluate the generated questions. This provides a strong grounding for the claims of question quality.\n\nS3) Focus on Scalability: The paper makes a strong case for its approach's scalability, not just in maintaining quality for long documents but also in cost. The provided analysis demonstrates that Savaal's incremental cost per question is lower than the 'Direct' baseline as the number of questions grows."}, "weaknesses": {"value": "O1) The primary weakness is the novelty of the proposed framework. The Savaal pipeline is a well-engineered application of existing components: a map-reduce-style process for concept extraction followed by a standard RAG (Retrieval-Augmented Generation) pattern to ground the question generation. While effective, this combination of techniques feels more like an incremental engineering contribution than a fundamentally new methodology.\n\nO2) The finding that the AI judge is misaligned with human experts is intriguing but underdeveloped. The paper reports this misalignment but offers little analysis as to why this is the case. Does the AI judge fail to appreciate conceptual depth, or does it prefer a different style of question? This is especially relevant given the paper also finds that inter-human agreement is surprisingly low (Fig. 8) before scores are binarized (Fig. 9). A deeper qualitative analysis of these disagreements would significantly strengthen this part of the contribution.\n\nO3) The paper lists \"domain-independence\" as one of its three primary objectives. However, the evaluation is exclusively performed on documents from Computer Science and Aeronautics. These are two closely-related, highly-structured, technical fields. The pipeline's effectiveness in domains with different rhetorical structures (e.g., legal analysis, historical narratives, or literary criticism) is entirely unevaluated.\n\nO4) By first identifying \"main ideas\" and then retrieving passages related to them, the system is optimized to ask questions about those pre-defined concepts and it may be systematically unable to generate questions that require a learner to synthesize multiple, unrelated, nuanced details from disparate parts of the document that do not fall under the same \"main idea.\" This approach favors conceptual-silo questions over complex, integrative reasoning."}, "questions": {"value": "Please address the weaknesses O1-O4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "no0wYf4syQ", "forum": "Xuf8VzxUx0", "replyto": "Xuf8VzxUx0", "signatures": ["ICLR.cc/2026/Conference/Submission20334/Reviewer_n6CM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20334/Reviewer_n6CM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016872760, "cdate": 1762016872760, "tmdate": 1762933792057, "mdate": 1762933792057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a three-step pipeline, Savaal, to automatically generate questions from multiple documents. The three steps involve extracting ideas, retrieving passages for these ideas, and prompting an LLM to generate questions given an idea and a retrieved passage."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The stages of the pipeline in Savaal are intuitive, helping reduce task complexity for an LLM, compared with directly prompting an LLM to generate questions.\n* Automatically creating assessment quality questions like MCQs for education, especially from long documents, is a relevant and challenging problem in the EduNLP community.\n* The methodology is clearly presented and well written with descriptive figures.\n* The authors include results from a large-scale human evaluation involving multiple annotators, showing Savaal generated better questions than the direct prompting baseline.\n* Illustrative qualitative examples of questions generated from other baselines are helpful."}, "weaknesses": {"value": "* The primary weakness is the misalignment between human evaluation and LLM-as-a-judge evaluation. The authors report poor correlation. In the appendix, the instructions given to human evaluators are possibly subject to interpretation (fig 11), while a different set of instructions (fig 20 to fig 23) is used in LLM-based evaluation, which precisely define what each score from 1 to 4 stands for. These different instructions could lead to bias and misalignment between the two evaluations. Since these questions are subjectively evaluated without automated metrics, the evaluation protocol is critical. Further, weak inter-human-annotator agreement is reported (A.3), even between authors of the same paper, possibly due to the subjective instructions given during human eval.\n* The authors only compare against a single, simplistic baseline, direct prompting. They show a qualitative example from other baselines in Table 1, with weaknesses for sample questions generated. However, the performance of these baselines on human/llm evaluation is required to compare them to Savaal, rather than qualitative examples. Further, QG generation is well studied; incorporating baselines from prior work would add evidence on the advantage of using Savaal.\n* The various stages of Savaal should be well justified with supporting evidence from ablations. For example, what happens if the retrieval stage is removed, i.e., an LLM is prompted with a concept and the entire document?\n* Multi-hop questions: Given the motivation of the paper of working on multiple long documents and preventing context fragmentation, i.e., to connect concepts spanning multiple sections (line 161), I was expecting the generation and evaluation of multi-hop style questions as a major advantage of Savaal. Does Savaal produce multi-hop questions? What are the evaluation numbers (percentage multi-hop vs not multi-hop)?\n* Extracting key concepts from multiple documents has been done in the context of summary generation (Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries, ACL 2025). How is the concept extraction idea in Savaal different or better than prior work?"}, "questions": {"value": "* For the direct baseline, how was the document inputted? Is it parsed to text and inputted in the prompt, or is the raw PDF attached RAG-style to the 4o API?\n* How is the summary obtained for the document summary baseline?\n* Addition of numbers supporting qualitative observations would be helpful. For example, the authors state that direct generates more duplicate Q than Savaal. What is the duplication rate over the test corpus?\n* Can you include 20Q from Savaal in Fig. 27, comparing against the included 20Q from direct in Fig. 26?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v4M4il1172", "forum": "Xuf8VzxUx0", "replyto": "Xuf8VzxUx0", "signatures": ["ICLR.cc/2026/Conference/Submission20334/Reviewer_cHEm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20334/Reviewer_cHEm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036973579, "cdate": 1762036973579, "tmdate": 1762933791367, "mdate": 1762933791367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a simple yet effective question-generation mechanism called Savaal. They argue that their proposed pipeline outperforms directly prompting a LLM (called Direct) with extensive document corpora as input, citing three key advantages - scalability, depth of understanding, and domain independence. The Savaal pipeline consists of three stages - stage 1: Extracting key ideas from each section using an LLM, stage 2: Retrieving relevant passages via a document retriever, and stage 3: Generating challenging questions and answer choices. To assess the effectiveness of their approach, the authors compare it with Direct, using both human evaluation and LLM-based assessment. The results show that the Savaal pipeline yields notable improvements over Direct for human evaluation. Additionally, they observe a low correlation between LLM and human responses, suggesting a reduced alignment between human understanding and LLM-generated outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces a scalable way of generating questions. \n2. The problem that the authors are trying to solve is very novel.  \n3. The concepts were presented in a really coherent manner."}, "weaknesses": {"value": "1. Missing Ablations on Open-Source LLMs  The authors primarily use GPT-4o, a proprietary LLM, for all experiments. It would be valuable to include ablation studies using open-source models to assess the generality of their approach. Specifically:\na) Evaluate whether open-source Small Language Models (SLMs) or larger models can achieve comparable results, and analyze the trade-offs between model size and performance.\nb) Examine whether the reported trends hold consistently for both the Savaal and Direct pipelines when implemented with small and large LLMs.\nc) Explore direct prompting setups enhanced with in-context examples or chain-of-thought reasoning to provide a stronger baseline comparison.\n\n2. Lack of comparisons with baselines - this builds from my previous point. The authors should compare their pipeline with a LLM (direct prompting) for performance. It would be interesting to see - \na) how good is the performance of Savaal with respect to a LLM finetuned using SFT or RL.\n\n3. Definition of \"depth of understanding\" - authors define \"depth of understanding\" in the work as \"high quality questions with conceptual reasoning and plausible distractors\" (L94, L95). I fail to see these definitions are relayed to the human annotators in the form shown in figures 10 and 11 in supplementary. Overall the structure of form can improved further as follows - \na) the task definition should be mentioned. Simply mentioning depth of understanding or overall quality doesn't seem adequate to me because humans have their judgement and it varies from person to person. It should give the proper definition of depth of understanding. \nb) one or multiple in-context examples should be provided to human annotators. \n\n4. Disconnect Between Introduction and Experiments “ The authors make several claims in the introduction that are not substantiated by the experiments presented in the paper:\n\na) Memorization and Superficiality  In L40, the authors assert that \"memorization\" and \"superficiality\" are minimal in the outputs of their pipeline, yet no quantitative or qualitative evidence is provided to support this claim. There is also no explanation of how their pipeline avoids these issues. An experiment could be designed to test this hypothesis, for example, implementing the Savaal pipeline with GPT-J (an LLM with publicly available training data) and checking whether any instances from the training data are directly reflected in the generated outputs. This would provide insight into whether the pipeline indeed minimizes memorization.\n\nb) Quality of Questions  In  L85 and L86, the authors claim that the \"quality of questions\" generated by Savaal is high, yet no experiments are conducted to measure or quantify this quality. To support this claim, the authors could conduct experiments using reference-free text quality evaluators, such as using ChatGPT as , or use human annotation or apply common metrics like BLEU or BERTScore to assess the quality of the questions generated.\n\n5. Lack of Empirical Support for Claims in Section 2 “ In Section 2, the authors present qualitative examples (table 1) and claim that simple prompting with LLMs fails to meet at least one of their key objectives” scalability, depth of understanding, or domain independence. However, the paper would benefit from additional empirical support. The authors could either cite prior work that demonstrates similar limitations in LLMs or conduct experiments to empirically validate their claims.\n\n6. Ablations with other text retrievers missing - I was wondering if authors could conduct some ablation experiments with other text retrievers such as T5score, CLIPscore or BM25.\n\n7. Qualitative examples in figures 26 and 27 show that direct prompting has duplications while Savaal does not. I don't feel there is much difference in the types of questions both pipelines are outputting. \na) Can authors point out what exactly is the difference between the outputs of Savaal and Direct?\nb) If duplication is the only problem, can we just use some basic engineering techniques to remove duplications?\nc) If cost is the problem, can we just use an open sourced LLM? \n\n8. Computational efficiency of Savaal - The paper has argued multiple times that their pipeline is computationally efficient, but there are no quantitative experiments that show this. Can the authors show the TFLOPs, GPU memory or time required by Savaal and Direct per inference? \n\nMinor weaknesses are as follows -\n1. The paper is very well written but the citations are not well aligned. Please use ~/citep{} for citing papers."}, "questions": {"value": "Overall the paper needs more comparison and ablations as mentioned in weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PDpqArihGU", "forum": "Xuf8VzxUx0", "replyto": "Xuf8VzxUx0", "signatures": ["ICLR.cc/2026/Conference/Submission20334/Reviewer_Pc9m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20334/Reviewer_Pc9m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20334/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762280474193, "cdate": 1762280474193, "tmdate": 1762933790782, "mdate": 1762933790782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}