{"id": "hsBXHpl1tL", "number": 24076, "cdate": 1758352414704, "mdate": 1759896782861, "content": {"title": "Kolmogorov-Arnold Hierarchical Implicit Neural Representation Model for Physical Field Reconstruction", "abstract": "Reconstructing continuous fields from sparse observations poses one of the most persistent challenges in scientific machine learning, with critical implications for understanding geophysical phenomena from limited sensor networks. Although implicit neural representations (INRs) have recently emerged as promising solutions, capturing fine-scale structures in complex domains such as atmospheric and oceanic systems remains elusive. We introduce KHINR (Kolmogorov-Arnold Hierarchical Implicit Neural Representation) that achieves state-of-the-art spatial field reconstruction through a fusion of learnable Gabor filters and Kolmogorov-Arnold Network (KAN) blocks in a hierarchical structure. The sparse spatial data points are first encoded using learnable Gabor filters to extract localized, frequency-aware spatial features that are further processed in the latent space via a hierarchical structure with KAN blocks. For reconstruction, the Gabor-encoded unknown spatial points are passed through a gating mechanism on the latent representation learned by the hierarchical KAN blocks. Rigorous evaluation across four distinct physical fields from meteorological and ocean datasets reveals KHINR's superior performance compared to other leading models on multiple reconstruction tasks under varying sparsity conditions. Comprehensive ablation studies validate the critical contribution of each architectural component, establishing KHINR as a new standard for sparse-to-continuous field reconstruction in scientific applications.", "tldr": "", "keywords": ["Physical Field Reconstruction", "Implicit Neural Representation", "Kolmogorov-Arnold Network"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b541e7f5bb5a75cb1f685779d2c0deb6c74a2510.pdf", "supplementary_material": "/attachment/7c63e1a363800c0bcb9d0011af4990c0eb841549.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents KHINR, a hierarchical implicit neural representation (INR) model for reconstructing continuous physical fields from sparse and irregular data. To address the spectral bias, global basis dependence, and limited multiscale modeling of existing INRs, KHINR incorporates learnable Gabor filters for localized frequency-aware encoding, hierarchical Kolmogorov–Arnold Network (KAN) blocks for capturing complex multiscale dependencies, and a latent cross-attention mechanism for efficient global structure learning.  Evaluated on diverse datasets, KHINR outperforms other baselines. Ablation studies validate the contributions of each component."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper has a clear and logical structure.\n\n2. Experiments show good performance and robustness across diverse datasets and settings."}, "weaknesses": {"value": "1. This paper is also closely related to several very recent studies, such as [R1],  **which is a INR-based approach for physics**; [R2][R3][R4] is about diffusion-based methods for physical field reconstruction. \n[R1] Wang, Honghui, et al. \"GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling.\" The Thirteenth International Conference on Learning Representations. 2025.\n[R2] Du, Pan, et al. \"Conditional neural field latent diffusion model for generating spatiotemporal turbulence.\" Nature Communications 15.1 (2024): 10416.\n[R3] Chen, Panqi, et al. \"Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations.\" The Thirty-ninth Annual Conference on Neural Information Processing Systems.2025.\n[R4] Li, Zeyu, et al. \"Learning spatiotemporal dynamics with a pretrained generative model.\" Nature Machine Intelligence 6.12 (2024): 1566-1579.\nBut the authors do not discover them.\n\n\n2. The contribution appears  largely incremental by combing KANs within INRs (all existing techniques).  The proposed  latent cross-attention mechanism seems not new.\n\n\n3. The baselines are not comprehensive, as they mainly include methods from the computer vision field rather than approaches specifically designed for physical field reconstruction."}, "questions": {"value": "1. Figure 2 and its corresponding description (i.e., Section 2.3) are very vague. I cannot match them up. It is unclear how the data flows and how the model operates during training and testing. Providing a clearer mathematical formulation would help, and it would also be useful to highlight the novelty of the proposed method, if any.\n\n2. The proposed architecture appears similar to Senseiver (Santos et al., 2023), which also employs cross-attention to query points for sparse field reconstruction. Since the authors have already mentioned this work, it would be appropriate to include a comparison with it.\n\n\n3. The reference entitled \"Continuous field reconstruction from sparse observations with implicit neural networks\" is cited twice. Please check if they are the same."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fCq9n4aQei", "forum": "hsBXHpl1tL", "replyto": "hsBXHpl1tL", "signatures": ["ICLR.cc/2026/Conference/Submission24076/Reviewer_z15k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24076/Reviewer_z15k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761017288132, "cdate": 1761017288132, "tmdate": 1762942925620, "mdate": 1762942925620, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces KHINR, a hierarchical implicit neural representation that reconstructs continuous spatio-temporal fields from sparse observations. It integrates hierarchical KAN blocks for multi-scale inductive bias, learnable Gabor encoding for localized frequency representation, and latent cross-attention with gating for efficient global–local fusion. Evaluated on four geophysical datasets under various sparsity regimes, KHINR consistently outperforms INR baselines in accuracy and efficiency, with ablations showing that KAN and Gabor modules drive performance while latent attention improves training efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Directly targets INR pain points in scientific data: spectral bias, anisotropy/locality, and global dependency modeling.\n2. Hierarchical KAN provides multi-scale, interpretable inductive bias and consistently outperforms MLP/sequential KAN.\n3. Localized Gabor encoding adapts to spatially varying frequencies/directions, with notable gains at extreme sparsity (e.g., SST 0.1%).\n4. Comprehensive task suite (fixed/random number & locations) and systematic ablations."}, "weaknesses": {"value": "1. The paper lacks sufficient implementation details regarding key architectural components. In particular, the gating mechanism and the way the temporal variable t is incorporated into the model are not clearly explained.\n2. Baseline coverage: Missing comparisons to strong non-INR baselines common in scientific ML (e.g., FNO, U-Net on gridded data, Kriging/GP, DeepONet/Neural Operators, low-rank/kernel methods). Current evidence is mainly intra-INR.\n3. Reproducibility gaps:\n3.1. Exact Gabor parameterization: μ/γ/W/b initialization, constraints, and regularization;\n3.2. Latent cross-attention/gating specification (heads, scaling, norm, residuals, complexity);\n3.3. KAN spline order, coefficient regularization;"}, "questions": {"value": "1.\tWhat is the precise cross-attention/gating formulation? How are Q/K/V constructed between latent features and Gabor-encoded coordinates? Multi-head? Residual/norm?\n2.\tGabor parameters: Do learned μ cluster around non-stationary hotspots? Any regularization to prevent collapse? How do you bound γ to balance locality/globality?\n3.\tThe paper states that the Hierarchical KAN captures “multi-scale structures”. Could the authors elaborate on how these scales are defined and implemented? For example, do different KAN sub-blocks operate on distinct frequency ranges, or receptive field sizes?\n4.\tTemporal handling: If time is present, is it encoded like space or as a conditioning variable? \n5.\tCould you add comparisons to FNO, U-Net (gridded), Kriging/GP, DeepONet/Koopman or include some in the appendix?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cXqm5X7fFs", "forum": "hsBXHpl1tL", "replyto": "hsBXHpl1tL", "signatures": ["ICLR.cc/2026/Conference/Submission24076/Reviewer_cvbA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24076/Reviewer_cvbA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550712122, "cdate": 1761550712122, "tmdate": 1762942924760, "mdate": 1762942924760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes KHINR (Kolmogorov–Arnold Hierarchical Implicit Neural Representation), a novel model for reconstructing continuous physical fields from sparse sensor data. KHINR consists of three components. [1] Hierarchical Kolmogorov–Arnold Networks (KANs) – to model nonlinear, multiscale relationships via learnable univariate spline functions. [2] Learnable Gabor filters – replacing Fourier features to provide localized, adaptive, frequency-aware spatial encodings. [3]Latent cross-attention mechanism – to fuse sparse observations with global structural context efficiently. Geophysical datasets were tested, and another INR-based approach was compared."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "[1] The experimental results demonstrate strong performance, even under sparse data conditions.\n\n[2] Multiscale capability: The hierarchical structure effectively captures fine-to-coarse dependencies, which is crucial for modeling complex physical phenomena.\n\n[3] The proposed method is novel, and the introduction of Gabor-enhanced latent attention further improves efficiency by reducing training time."}, "weaknesses": {"value": "[1] Other types of Baselines: The paper focuses primarily on physics-based field reconstruction using an INR-based approach. However, comparisons with other state-of-the-art methods are missing. In particular, it would be valuable to compare this approach with Fourier Neural Operator (FNO), Physics-Informed Neural Networks (PINNs), and diffusion-based models,.\n\n[2] Variety of Experiments: All experiments are conducted on 2D static fields, without addressing spatiotemporal coupling or dynamic PDE constraints. It would be important to discuss whether the proposed method can generalize to 3D datasets or to time-dependent problems in 2D problem, as this would better demonstrate its scalability and versatility.\n\n[3] Model Efficiency: KAN-based architectures typically require longer training times and involve more parameters compared to MLP. While the Appendix reports testing time, there is no detailed analysis of training efficiency, parameter count, or computational complexity (e.g., GFLOPs). The training time is reported in Figure 3, but a comprehensive comparison of training times with other methods would provide a better understanding.  Such an analysis would provide a more thorough understanding of the model’s efficiency.\n\n[4] Ablation and Variants: The Hierarchical KAN block appears to be a core contribution of the paper, yet no ablation studies are provided to analyze the effect of different configurations (e.g., different numbers of KAN layers). Moreover, with the recent introduction of KAN 2.0 [r1], it would be interesting to investigate whether replacing the existing KAN layers with KAN 2.0 improves performance or training stability.[r1] Liu, Ziming, et al. \"Kan 2.0: Kolmogorov-arnold networks meet science.\" arXiv preprint arXiv:2408.10205 (2024)."}, "questions": {"value": "See the Weakness for the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y3C9oZrWa1", "forum": "hsBXHpl1tL", "replyto": "hsBXHpl1tL", "signatures": ["ICLR.cc/2026/Conference/Submission24076/Reviewer_Xja4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24076/Reviewer_Xja4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931541162, "cdate": 1761931541162, "tmdate": 1762942923883, "mdate": 1762942923883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to address the sparse-to-dense physical field\nreconstruction problem, i.e., how to recover the complete physical field when only a few\nsensor measurements are available. The authors argue that existing INRs perform poorly\non this specific task because conventional MLP-based INRs suffer from spectral bias and\na poor inductive bias when modeling multiscale, anisotropic geophysical fields.\nMoreover, global positional encodings do not adapt well to spatially varying frequencies\nand orientations. Additionally, there is no efficient mechanism to inject sparse\nobservations into a global latent representation. The proposed KHINR model addresses\nthese issues by incorporating hierarchical KAN blocks to model multiscale structures\nmore effectively than standard MLPs. It further employs latent cross-attention between\nsparse value–location tokens and global latent tokens, followed by a gating mechanism to\nalign latent codes back to dense coordinates."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1). The proposed architecture is carefully designed, rather than being a simple modification of an existing method. The intelligent combination of three powerful components hierarchical KANs, learnable Gabor filters, and latent cross-attention is\nhighly novel within the scientific machine learning domain.\n2). The proposed method (KHINR) obtains superior results across all datasets and all sampling tasks compared to existing methods"}, "weaknesses": {"value": "1). The paper is poorly written, and many parts are difficult to understand. I suggest that the authors rewrite the methodology section, as several important concepts are unclearly or inadequately explained. I feel the authors may need to pay more attention to how they present their methodology, as the proposed method and its architecture are only briefly discussed. If the authors could include their design rationale explaining how and why each component was chosen the methodology section would be significantly strengthened.\n\n\n2). I would like to know what the exact gating mechanism is, as it is barely discussed in the paper. \n\n\n3). The compared INR baselines are old. I suggest authors to compare their method with recent methods like FINER/INCODE. \n\n\n4). Many of the recent INRs have not been cited.\n\n\n5). I would like to know how you conducted the experiments on, for instance, WIRE. Did the authors fine-tune the activation parameters? If so, over what range were they tuned, and were they adjusted for each specific sparsity level?\n\n\n6). I would like to know How does KHINR differ from INR-based image inpainting? I feel both are highly correlated. \n\n\n7). The methodology section defines the target function u(x) over a spatiotemporal domain), and the Figure 2 explicitly(disconnectedly) indicates a time input (t). So, I would like to know, how do you handle this time variable? Do the authors use some encoding? No information regarding this is presented.\n\n\n8). This is a novel research direction with carefully designed architecture. So, I’m willing to accept the paper, if the authors specifically refine their writing part (at least the methodology) and include the latest comparisons."}, "questions": {"value": "Please see the weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o8crozT8NH", "forum": "hsBXHpl1tL", "replyto": "hsBXHpl1tL", "signatures": ["ICLR.cc/2026/Conference/Submission24076/Reviewer_zSNB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24076/Reviewer_zSNB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24076/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947467290, "cdate": 1761947467290, "tmdate": 1762942923588, "mdate": 1762942923588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}