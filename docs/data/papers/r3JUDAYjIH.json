{"id": "r3JUDAYjIH", "number": 8115, "cdate": 1758064385930, "mdate": 1763761043407, "content": {"title": "Self-Supervised Learning from Structural Invariance", "abstract": "Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs.\nWe study the one-to-many mapping problem in SSL,\nwhere each datum may be mapped to multiple valid targets. \nThis arises when data pairs come from naturally occurring generative processes, e.g., successive video frames.\nWe show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a variational distribution that models this uncertainty in the latent space, and derive a lower bound on the pairwise mutual information. We also propose a simpler variant of the same idea using sparsity regularization.\nOur model, AdaSSL, applies to both contrastive and predictive SSL methods, and we empirically show its versatility on identifiability, generalization, fine-grained image understanding, and world modeling on videos.", "tldr": "", "keywords": ["Self-supervised learning", "representation learning", "disentanglement"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ab96e993607d6440ec305212b52d2603cfcca05.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies the Self-Supervised Learning problems. It argues that existing methods fail to model the heteroscedasticity in $p(z^+|z)$  for positive pairs. The paper proposes AdaSSL by introducing a latent variable $r$ that captures the stochastic transformation from one view to another. By jointly learning the encoder f(x), the latent transformation r, and an edit function that reconstructs the target embedding, the method adapts to the heteroscedasticity in the above conditional. The paper performs extensive experiments on synthetic and real datasets and shows improved disentanglement and generalization compared to InfoNCE and BYOL variants."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- I find the paper interesting and enjoyable to read.\n- Learning from pairwise images and the heteroscedasticity in $p(z^+|z)$ is a promising and under-explored area.\n- Strong motivations backed by theoretical and empirical analysis."}, "weaknesses": {"value": "- While proposition 2.1 shows heteroscedasticity exists, and 2.2 explains how existing methods fail to account for it. It remains unclear to me (at least intuitively) how modeling the complex conditional $p(z^+|z)$ contributes towards the ultimate SSL objective, i.e., generalization on a wide range of downstream tasks that predict a subset of factors in z.\n- It is even harder for me to understand how the proposed approach should be better in downstream tasks compared to baselines with similar motivation (e.g., H-InfoNCE).\n- As acknowledged in Section 3.2, the proposed approach is only theoretically justified for contrastive SSL. This introduces a gap due to the popularity and performance of non-contrastive SSL.\n- $q_\\phi,p_\\theta$ are both modeled as factorized Gaussians. This is slightly against the idea that the conditional can be quite complex, as it is up to the edit function $t$ to model the complexity, which can complex model design and reduces learning efficiency."}, "questions": {"value": "- Why is the proposed approach better than H-InfoNCE? How does it encourage disentanglement?\n- How learning an efficient representation of r (line 397) leads to a more disentangled feature f(x) in Table 4?\n- Why is another view $x^{++}$ introduced in CelebA experiment? What if only using $x$ and $x^+$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6UFTq1MkVk", "forum": "r3JUDAYjIH", "replyto": "r3JUDAYjIH", "signatures": ["ICLR.cc/2026/Conference/Submission8115/Reviewer_y8NR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8115/Reviewer_y8NR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761752795848, "cdate": 1761752795848, "tmdate": 1762920094438, "mdate": 1762920094438, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response (part 1)"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback. We are encouraged that the reviewers find our motivation sound **(TJcB: \"sound\"; gRec: \"key problem\", \"well-motivated\"; NVc3: \"interesting\"; y8NR: \"promising and under-explored\", \"strong motivations\")** and our method interesting and rationally-justified **(gRec: \"novel\", \"described in depth\", \"good candidate solutions\"; y8NR: \"interesting\", \"enjoyable to read\", \"extensive experiments backed by theoretical and empirical analysis\"; TJcB: \"relatively clear and well explained\", \"rationally justified\")**.\n\nBelow we address the common questions and respond to the remaining concerns individually.\n\n\n\n# Significance of Prop 2.1 (Reviewers TJcB, NVc3)\n\n**Intuition of Prop 2.1.**\nWe show that when the ground-truth latent space $Z$ has a different geometry from the embedding space $E$, the conditional distribution $h(z^+)\\mid h(z)$ becomes heteroscedastic in these settings. This arises **even when the true latent conditional $z^+\\mid z$ is homoscedastic**: the curvature mismatch forces the encoder to introduce input-dependent uncertainty.\n\nWe demonstrate this for two concrete cases, $Z=R^d, E=S^{d-1}$ (Prop. 2.1) and $Z=S^{d-1}, E=R^d$ (Prop. B.2). While we leave the fully general statement for future work, the geometric intuition is simple: mapping between spaces with different curvature stretches or compresses local neighborhoods differently depending on the location of $h(z)$, which directly induces heteroscedasticity in $h(z^+)\\mid h(z)$.\n\n**Significance for SSL.**\nThis matters for SSL because existing objectives implicitly assume that the variability in $h(z^+)\\mid h(z)$ can be modeled with a fixed-variance similarity function (Sec. 2.2) or predictor (Sec. 2.3). When the geometry of $Z$ and $E$ differ---which is likely, since $Z$ is unknown while $E$ is chosen as either $S^{d-1}$ or $R^d$---this assumption is violated. We show that existing methods struggle even under the simple case of $Z=R^d$.\n\nEmpirically, when $Z$ and $E$ do not match, existing SSL methods struggle to recover the latent factors (Table 1), consistent with [Zim+21] ((Table 4, rows 2 & 4). Modeling this input-dependent noise (via H-InfoNCE) alleviates the issue.\n\nWe clarify the connection between Prop. 2.1 and our problem in Sec. 2.4 of the revised manuscript.\n\n\n# Representation learning and downstream tasks (Reviewers gRec, NVc3, y8NR)\n\nWe adopt the causal representation learning (CRL) view that a central goal of representation learning is to recover the data-generating factors $z$ from observations $x$ [Sch+21, Ben+13, Zim+21]. Under this view, a \"diverse\" representation is one that captures the full latent variability in $z$, because all semantic information in $x$ originates from $z$. This objective is formalized in Sec. 2.1, and we study this in the setting of natural pairs as motivated in the intro. \n\nWe argue that learning a diverse representation is important for general-purpose representations for different downstream tasks, which may need some unknown subset of $z$ for inference [Ben+13]. For example, a world model cannot generate meaningful future trajectories of cars if its representation does not contain velocity. \n\nWe follow the standard evaluation protocols in representation learning with SSL: training a linear probe and evaluating the model’s performance through its ability to predict the data-generating factors [Che+20, Gri+20]. When we don’t have access to these factors, we use proxy tasks, such as fine-grained classification on CelebA and iNaturalist (results shown below).\n\nWe have revised Sec. 2.1 to make explicit how our formal objective connects to downstream performance.\n\n\n\n# Clarity of the paper\n\n\nWe thank the reviewers for their suggestions regarding clarity. We have addressed each point in our individual responses and incorporated the corresponding revisions into the manuscript, with changes highlighted in **red**."}}, "id": "kPhzaiu5bp", "forum": "r3JUDAYjIH", "replyto": "r3JUDAYjIH", "signatures": ["ICLR.cc/2026/Conference/Submission8115/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8115/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8115/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763754209569, "cdate": 1763754209569, "tmdate": 1763754209569, "mdate": 1763754209569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of self-supervised representation learning when the two views of the data used to learn the representation are so-called {\\em natural pairs}  instead of handcrafted augmentations, such that the two views are generated from (unknown) latent factors with the dependency between the latent variable encoded by some unknown conditional probability distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "SSL methods are widely used, and their theoretical study is welcome. The dea of modeling the dependency of the views on some latent variables is interesting."}, "weaknesses": {"value": "Although the topic of the paper is interesting, the presentation is hard to follow without well identified objectives and experiments mostly limited to toy examples. \n\nThe presentation should be clarified. For example:\n- Section 2 defines the data generation process in terms of latent variables z and z+, but in Section 3 these disappear to be replaced by a latent variable r presumably there to parameterize the predictor, simillar to predictive SSL methods.\n- I did not understand the difference between the AdaSSL loss and that typically used in predictive SSL, in part because the dependency of the model on the latent variable r is never defined before giving Eqs. (4) and (5) so I didn't understand how the terms in these equations were computed.\n-psi_1 and psi_2  are used in Eq. (8) before they are defined in Eq. (9).\n-The function t, which was an arbitraty MLP until then, is defined explicitly in Eq. (11) as a modular editing function.\n\nI could not find any justification as to why it should be possible to recover the latent variables since they are never used, as far as I know, in the actual loss of the AdaSSL variants. This is problematic since the experimental evaluation is for the most part dedicated to this recovery.\n\nRemark: although it is frequently used in non-contrastiva approches to SSL, the EMA formulation in Eq. (3) is, as far as I know, ill defined since the exponential moving average is normally taken over the parameters defining psi over time."}, "questions": {"value": "I understand how, from their probabilistic definition in terms of latent factors, natural data pairs may be different from the \"augmented\" pairs typically used in SSL. From an intuitive point of view, however, I do not really see how nearby video frames are qualitatively different from image crops, say. Both can be seen as crops, temporal or spatial, of the data. I would appreciate that the authors comment on this point.\n\nPlease explain the significance of Prop. 2.1.\n\nAs noted by the authors, AdaSSL-V is only justified for contrastive SSL, but it is used for non-contrastive SSL as well. Could you please justifiy this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pLk8p79MxH", "forum": "r3JUDAYjIH", "replyto": "r3JUDAYjIH", "signatures": ["ICLR.cc/2026/Conference/Submission8115/Reviewer_NVc3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8115/Reviewer_NVc3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838694696, "cdate": 1761838694696, "tmdate": 1762920094085, "mdate": 1762920094085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents novel self-supervised learning methods that model the uncertainty in data pairs generated from natural generative processes, using regularized latent variables. For example the uncertainty between consecutive frames in a video. Two variants are presented, one based on variational inference on the other on enforcing sparsity on the latent variable. Experiments on artificial and reel data are conducted to demonstrate the effectiveness of the approach in identifying latent factors of variations and modelling uncertainty."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a key problem in self-supervised learning: modelling conditional uncertainty, which arises in many other problems related to causal prediction. The potential applications are therefore numerous: video prediction, video generation, world modelling and latent action prediction, efficient self-supervised learning ect. The paper could actually do a better job at motivating these applications.\n\n- The ideas presented in the paper are interesting, described in depth, well-motivated, and seem to be good candidate solutions, at least on the toy problems explored in the experimental section."}, "weaknesses": {"value": "- The paper is complex to understand, with lots of formalism (the probabilist framework, Proposition 2.1), complex vocabulary (heteroscedasticity, modular editing, DCI) that is not necessarily introduced, or introduces lots of new vocabulary (CRL, DGP, SSL from structural invariance, Adaptive SSL, natural pairs), all for ideas that are actually fairly simple. I feel like this over–complixification hinders the reading flow and makes it harder to deliver the message it intends to deliver.\n\n- Also, the paper put a lot of emphasis on particular SSL losses such as contrastive vs non-contrastive, as well as several variants of InfoNCE, which does not seem very relevant for this study, and adds too many factors of variation that make the conclusions of the paper less clear and convincing. Section 2.2 and 2.3 are probably unnecessary, and the new variants H-InfoNCE, AnInfoNCE, ect are not well motivated.\n\n- Finally, all this formalism is derived by the JEPA framework and the authors mention that they only take inspiration from JEPA, whereas I see these contributions as instanciations of JEPAs, just with various ways of regularizing the latent variables. In Figure 1, b) and c) are JEPAs.\n\n- The experiments are conducted on toy problems which limits the credibility of the approach. How does it behave on more concrete problems ? I don’t think people care about the artificial numerical problems of section 4.2, these should be more of a tool for you to debug the approach. Section 4.2 is interesting but very artificial, and 4.3 is Moving MNIST which is good again for debugging but nowhere near close to the actual interesting problems.  Finally, the focus is on velocity decoding, which is good for debugging but not as interesting as the problem of prediction. It would be more interesting to show experiments where a model predicts the future trajectory in moving MNIST, and being able to sample several possible future trajectories by sampling from the latent.\n\n- Related to this, the claims made at the beginning of the paper need to be toned down, for example Line 20 “and we empirically show its superiority on identifiability, generalization, fine-grained image understanding, and world modeling on videos”. Superiority against which concurrent method ? And on benchmarks that are too toy.\n\n- The paper ignores the vast literature existing on uncertainty modelling and latent variables. All the work in generative modes, video generative models, video prediction, latent action models.\n\n- In conclusion, the paper is tackling an interesting problem and presents interesting ideas but it is hard to be convinced by the toy experiments. These points would make it much stronger:\n\n- Remove the studies on InfoNCE variants, along with sections 2.2 and 2.3, and focus on AdaSSL. Maybe rename using the JEPA terminology and just name the latent variable regularization methods.\n\n- Remove section 4.2 and focus more on real data experiments.\n\n- Add more motivations in terms of potential applications\n- Acknowledge other literature in uncertainty modelling and world modelling.\n\n- Focus the experiments more on video world modelling, and the prediction capability, rather than training probes to recover properties such as velocity."}, "questions": {"value": "- Line 160: Then the solution is just to project and do prediction in the same space ?\n\n- In AdaSSL-V, how could you make more explicit the mechanism that regularizes the latent variable regularized, basically what is L_reg ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VH8slijS1V", "forum": "r3JUDAYjIH", "replyto": "r3JUDAYjIH", "signatures": ["ICLR.cc/2026/Conference/Submission8115/Reviewer_gRec"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8115/Reviewer_gRec"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847095929, "cdate": 1761847095929, "tmdate": 1762920093621, "mdate": 1762920093621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a model for self supervised learning that extends previous approaches to include an auxiliary variable that capture the variation between representation of related data, e.g. augmentations or naturally occurring variations. The results suggest indicate that the model is better able to adapt to data generated under more flexible assumptions, such as heteroskedasticity, relative to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is relatively clear and well explained (see weaknesses for exceptions). The motivation seems sound and the proposed solutions are rationally justified. Experimental results suggest the proposed models are effective."}, "weaknesses": {"value": "A main weakness of the paper is its occasional lack of clarity. For the most part the paper is easy to follow, which makes the following very noticeable. The paper would improve greatly if these areas were addressed.\n* 33 - this does not seem to be about \"distribution shift\", i.e. a change to the distribution, but rather that artificial augmentations do not span the full variation in the distribution of natural images\n* Prop 2.1 - this is unclear, e.g. in line 156 we already know h maps to the unit sphere so why restate? It is unclear what this proposition adds since it appears to be specific to Gaussian distributions under a mapping between different topologies. This is a contrived scenario so it is unclear how it is necessarily relevant or \"unavoidable\" in practical scenarios. If the point is that the distributions of $z^+|z$ may vary over $z$ for real world datasets, which sounds highly plausible, Prop 2.1 doesn't appear to prove that and seems redundant.\n* 199: Eq 4 does not appear to support the \"intuition\" that follows, e.g. $r$ could convey no information and Eq 4 holds, so \"*should* help\" is untrue.\n* 209: The use of $p$ and $q$ suggest that $p$ is a ground truth posterior distribution over $r$ that $q$ learns to approximate similarly to a VAE, but a VAE is quite different since a ground truth posterior $p(z|x)$ is defined by the model's likelihood and prior, which $q$ provably learns to approximate. Here there is no defined prior over $r$ or ground truth posterior $p$, so the notation seems spurious (it would seem more accurate to refer to $r$ as an \"auxiliary\" variable (as in Khemakhem et al. (2020)) and \"conditional\" $q$.\n* 234: it is unclear how $r$ is included in InfoNCE or how the projector compares to the MLP used in BYOL. This is a key part of describing the model and should be very clear. \n* 308: the distinction between embeddings and representations is unclear from the text.\n* 4.2: this section is very difficult to read, e.g. \n    - \"$f$ is the frozen encoder trained on $p(z)$\" presumably means $f$ was trained under the SSL algorithm given $x, x^+$ pairs sampled under the described generative process? If so, that is hard to parse and should be made clearer. \n    - (a) appears to describe generating data under the described method and training a regressor to predict the true generative $c$ component of $z$ from the representation $f(x)$. It is much less clear what (b) and (c) describe (c appears to be about robustness of the regressor?)\n    - since $\\Sigma$ is unstated, the significance of $5I$ in the \"OOD\" cases has no context (presumably 5 is higher variance than $p(z)$)?\n* \"OOD experiments\" - the theoretical background does not appear to suggest robustness under distribution shift, so there is no clear explanation for the improved results and \"only flexible models generalize OOD\" seems unjustified. It is fair to note the improved OOD results, but it seems unexplained.\n    - 366 - as above, the emphasis on OOD sees out of keeping with the rest of the paper. The model is designed to learn more flexible latent conditionals, this is shown in column 1 of Table 2, which seems the main result justifying the approach. No explanation has been given why this model would be expected to perform better OOD.\n* 377 - \"identifiability\" - what does this refer to?\n\nCelebA results: while the paper considers an adaptation to the InfoNCE loss, I believe that InfoNCE does not achieve state of art performance, so the results are not well contextualised in terms of current model performance."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C3zbWjhEmk", "forum": "r3JUDAYjIH", "replyto": "r3JUDAYjIH", "signatures": ["ICLR.cc/2026/Conference/Submission8115/Reviewer_TJcB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8115/Reviewer_TJcB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8115/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910952565, "cdate": 1761910952565, "tmdate": 1762920093272, "mdate": 1762920093272, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}