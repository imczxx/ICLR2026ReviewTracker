{"id": "7qXmJbjbl8", "number": 25128, "cdate": 1758364466649, "mdate": 1762957594595, "content": {"title": "Attribute-Centric Representation Learning for Interpretable Crime Scene Analysis in Video Anomaly Detection", "abstract": "Automatic crime scene analysis is an important application area for representation learning in Video Anomaly Detection (VAD). Effective interpretation of anomalous events requires models to learn rich, disentangled representations that capture fine-grained, crime-relevant attributes. However, widely used VAD datasets (e.g., UCA, CUVA) primarily offer coarse event-level labels and they lack attribute-level supervision often needed for modeling crime-specific behaviors. To bridge this gap, we propose an attribute-centric learning framework that explicitly conditions video representations on crime-causing attributes. We extend the UCA dataset with over 1.5M new attribute-centric annotations generated using carefully designed prompts and LLMs. These annotations enable supervised fine-tuning of a curated CLIP-based model, leading to more discriminative, attribute-aware video representations, and precise event captions. An LLM-based summarizer then distills these captions into context-rich explanations, facilitating interpretable scene understanding. Our approach answers three core questions in crime scene analysis: \\textbf{What? When? How?} Extensive experiments show that the proposed representation learning framework yields significant improvements ($\\approx 20\\%\\uparrow$) in attribute-centric crime classification accuracy and ($\\approx 6.4\\%\\uparrow$) according to MMEval scores over the baselines. We further analyze and mitigate biases in MMEval to ensure robustness and fair evaluation. These results highlight the importance of attribute-conditioned representation learning for interpretable and reliable VAD.", "tldr": "The paper proposes an attribute-centric framework for crime scene analysis in video anomaly detection by augmenting an existing crime dataset with attribute-level annotations and attribute-enriched captions created using large language models.", "keywords": ["Crime Scene Analysis", "Video Anomaly Detection", "Explainable AI", "Visual Language Reasoning"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/322c90a7e399541fed7e996bddc16530179e2b27.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce an attribute-centric learning framework aimed at enhancing video representations by conditioning them on specific crime-related attributes. They claimed that they have expanded the UCA dataset with new annotations utilizing large language models (LLMs). However, it appears that the critical component for attribute extraction, DeepMAR, along with several other references, does not exist. There are many incorrect or inconsistent statements. Additionally, both the theoretical and experimental sections are notably brief. These factors suggest that the entire work might be AI-generated. It is unfortunate for the authors to waste the time and resources of all reviewers and chairs in ICLR."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The concept of attribute-based crime scene analysis appears to be feasible."}, "weaknesses": {"value": "1. The main issue is that the referenced paper for the key component in attribute extraction, specifically DeepMAR by Li et al. (2018) [1], does not exist. The closest match I could find is a conference paper [2] that indeed shares the method name DeepMAR. I have significant doubts that it might be generated by AI.\n\n2. After a second check, I discovered additional fabricated references, which are listed at the end of the comments as [3-11].\n\n3. The basic idea has been investigated in many previous works. \n\n4. It lacks comparisons with state-of-the-art methods.\n\n5. In the manuscript, the authors mention: UCA (UCF Crime with Attributes) dataset. However, UCA actually stands for UCF-Crime Annotation. Many sections exhibit similar AI-generated characteristics.\n\n[1] Dong Li, Zhen Zhang, Shaogang Gong, and Tao Xiang. Deepmar: Deep learning for person attribute recognition. IEEE Transactions on Image Processing, 27(11):5564–5579, 2018.\n\n[2] D. Li, X. Chen and K. Huang, \"Multi-attribute learning for pedestrian attribute recognition in surveillance scenarios,\" 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR), Kuala Lumpur, Malaysia, 2015, pp. 111-115, doi: 10.1109/ACPR.2015.7486476.\n\n[3] Jianhui Chang and Chen Wang. Deep clip-based temporal attention networks for anomaly detection.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n[4] Marco Del, Sina Marvasti-Zadeh, Reza Yousefzadeh, et al. Temporal anomaly detection in surveillance videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 12426–12435, 2021.\n\n[5] Rakesh Kumar and Priya Singh. Hybrid clip models for real-time detection of high-risk human\u0002 object interactions. arXiv preprint arXiv:2401.01589, 2024\n\n[6] Hao Li and Min Zhou. Contextual causal detection for anomaly localization in videos. arXiv preprint arXiv:2401.01593, 2024.\n\n[7] Weixin Luo, Wen Liu, and Shenghua Gao. Rare event detection in surveillance videos with large crowds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3659–\n 3667, 2017.\n\n[8] Mohammad Sabokrou, Mohammad Khalooei, Mohsen Fayyaz, and Ehsan Adeli. Deep anomaly:Real-world anomaly detection in surveillance videos. In Pattern Recognition, pp. 161–172. Elsevier, 2018.\n\n[9] Waqas Sultani, Samet Akc¸ay, and Saeed Ullah Manzoor. Towards surveillance video-and-language understanding: New dataset baselines and challenges. In Proceedings of the IEEE/CVF Confer\u0002 ence on Computer Vision and Pattern Recognition (CVPR), pp. 1201–1211, 2024.\n\n[10] Yu Wu, Yicheng Zhang, Seah Hock Lim, Luc Van Gool, John Tighe, Austin Reiter, Lan Wang, and Ling Shao. Xd-violence: A benchmark for detecting violence in complex video scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):293–310, 2022b.\n\n[11 ]Lu Zhang, Yiyang Wu, Haoming Wang, and Jian Zhang. Learning clip guided visual-text fusion transformer for video-based pedestrian attribute recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 112–121, 2023."}, "questions": {"value": "1. Please clarify the problem of fake method DeepMAR specified in the weakness.\n2. Please explain the problem of numerous fake references."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "The referenced paper for the key component in attribute extraction, specifically DeepMAR by Li et al. (2018) [1], does not exist. The closest match I could find is a conference paper [2] that indeed shares the method name DeepMAR. I have significant doubts that it might be generated by AI. I further find many more fabricated references [3-11].\n\n[1] Dong Li, Zhen Zhang, Shaogang Gong, and Tao Xiang. Deepmar: Deep learning for person attribute recognition. IEEE Transactions on Image Processing, 27(11):5564–5579, 2018.\n\n[2] D. Li, X. Chen and K. Huang, \"Multi-attribute learning for pedestrian attribute recognition in surveillance scenarios,\" 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR), Kuala Lumpur, Malaysia, 2015, pp. 111-115, doi: 10.1109/ACPR.2015.7486476.\n\n[3] Jianhui Chang and Chen Wang. Deep clip-based temporal attention networks for anomaly detection.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n\n[4] Marco Del, Sina Marvasti-Zadeh, Reza Yousefzadeh, et al. Temporal anomaly detection in surveillance videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 12426–12435, 2021.\n\n[5] Rakesh Kumar and Priya Singh. Hybrid clip models for real-time detection of high-risk human\u0002 object interactions. arXiv preprint arXiv:2401.01589, 2024\n\n[6] Hao Li and Min Zhou. Contextual causal detection for anomaly localization in videos. arXiv preprint arXiv:2401.01593, 2024.\n\n[7] Weixin Luo, Wen Liu, and Shenghua Gao. Rare event detection in surveillance videos with large crowds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3659–\n 3667, 2017.\n\n[8] Mohammad Sabokrou, Mohammad Khalooei, Mohsen Fayyaz, and Ehsan Adeli. Deep anomaly:Real-world anomaly detection in surveillance videos. In Pattern Recognition, pp. 161–172. Elsevier, 2018.\n\n[9] Waqas Sultani, Samet Akc¸ay, and Saeed Ullah Manzoor. Towards surveillance video-and-language understanding: New dataset baselines and challenges. In Proceedings of the IEEE/CVF Confer\u0002 ence on Computer Vision and Pattern Recognition (CVPR), pp. 1201–1211, 2024.\n\n[10] Yu Wu, Yicheng Zhang, Seah Hock Lim, Luc Van Gool, John Tighe, Austin Reiter, Lan Wang, and Ling Shao. Xd-violence: A benchmark for detecting violence in complex video scenes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):293–310, 2022b.\n\n[11 ]Lu Zhang, Yiyang Wu, Haoming Wang, and Jian Zhang. Learning clip guided visual-text fusion transformer for video-based pedestrian attribute recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 112–121, 2023."}}, "id": "oE6SVoWSHw", "forum": "7qXmJbjbl8", "replyto": "7qXmJbjbl8", "signatures": ["ICLR.cc/2026/Conference/Submission25128/Reviewer_ftG8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25128/Reviewer_ftG8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807386911, "cdate": 1761807386911, "tmdate": 1762943335757, "mdate": 1762943335757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We have observed that some of the references cited in the paper got mixed-up while preparing the bibliography. For example, the title of the paper referred in [9] \"Towards surveillance video-and-language understanding: New dataset baselines and challenges.\" has been wrongly mixed-up with the author list. Actually, the original author list should be \"Tongtong Yuan, Xuange Zhang, Kun Liu, Bo Liu, Chen Chen\n, Jian Jin, Zhenzhen Jiao\". Similarly, for reference [11], the title of the paper is \"Learning clip guided visual-text fusion transformer for video-based pedestrian attribute recognition.\" has also been wrongly mixed-up with the author list during bibliography preparation. Moreover, we have found several such unintentional and typographical mistakes in the reference section. Therefore, we have decided to withdraw the paper."}}, "id": "E4CT4kHQ3h", "forum": "7qXmJbjbl8", "replyto": "7qXmJbjbl8", "signatures": ["ICLR.cc/2026/Conference/Submission25128/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25128/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762957593726, "cdate": 1762957593726, "tmdate": 1762957593726, "mdate": 1762957593726, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper applies attribute-based representation learning to video anomaly detection to enhance the performance and interpretability of related tasks. Its core idea is to address the lack of fine-grained supervision in existing datasets by generating over 1.5 million attribute annotations through large language models (GPT-4V). The authors fine-tune CLIP based on these fully labeled datasets to answer \"what,\" \"when,\" and \"how\" questions about anomalous events in videos. Experimental results demonstrate significant improvements in attribute classification accuracy (approximately 20%) compared to baseline methods, along with a moderate increase in MMEval scores (around 6.4%)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Robust quantitative experimental results were obtained, demonstrating the effectiveness of the experimental approach. \n\n2. The experimental framework organizes outputs around the dimensions of time (when), attribute classification (what), and content summarization (how), addressing the interpretability requirements of VAD."}, "weaknesses": {"value": "1. The primary innovation of the article lies in utilizing large language models to generate attribute labels for constructing fine-grained datasets, but it lacks novel ideas for the VAD method itself. \n\n2. Although the article achieves significant improvements in attribute-centered classification, it does not validate whether the new approach harms model performance in detecting coarse-grained anomalies."}, "questions": {"value": "1. The framework uses a large language model to generate a large number of enhanced annotations as the basis for the method. How did the author perform quality control on such a large scale of generated data? How robust is the CLIP model after fine-tuning to the subtle biases introduced by LLM?\n\n2. The paper mentions that \"crime using attributes\" are less distinguishable for certain crime types (e.g., robbery) compared to attribute rich ones (e.g., arson or shooting). Has the author quantified this the performance difference in attribute classification accuracy between attribute-rich classes and attribute-sparse classes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9tiQGQ7oaF", "forum": "7qXmJbjbl8", "replyto": "7qXmJbjbl8", "signatures": ["ICLR.cc/2026/Conference/Submission25128/Reviewer_a5yC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25128/Reviewer_a5yC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867198402, "cdate": 1761867198402, "tmdate": 1762943334797, "mdate": 1762943334797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an attribute-centric representation learning framework for interpretable crime scene analysis in video anomaly detection (VAD). It extends the UCA dataset with 1.5M+ LLM-generated attribute annotations (e.g., weapon, damage, intent), fine-tunes a CLIP-based model for attribute-conditioned video representations, and uses an LLM summarizer to generate context-rich explanations answering What? When? How?. Claims ≈20%↑ in attribute classification and ≈6.4%↑ in MMEval. Also analyzes and mitigates MMEval biases."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Addresses real need: fine-grained, interpretable crime scene understanding.\nLarge-scale attribute annotation pipeline using LLMs is scalable.\nMMEval bias analysis is responsible and sets a good precedent.\nClear motivation and structured evaluation on crime-relevant attributes."}, "weaknesses": {"value": "LLM-generated annotations lack human validation: No inter-annotator agreement, error analysis, or comparison to human labels. Risk of hallucinated or inconsistent attributes (e.g., “intent to harm” from video?).\nNo ground-truth attribute labels in UCA — all supervision is synthetic. How trustworthy are downstream gains?\nMMEval improvements are modest (6.4%) and on a flawed metric; no comparison to human judgment or real deployment logs.\nOverclaims interpretability: “How?” is not truly answered — summarizer just rephrases captions. No causal reasoning or counterfactuals.\nNo ablation on LLM prompt design — are results robust to prompt variation?\nEthical red flag: Using LLMs to infer intent or threat level from surveillance video risks bias and misuse in law enforcement."}, "questions": {"value": "How many attribute annotations were human-verified? Report IAA (Fleiss’ κ) on a sampled subset.\nAblate LLM prompt variants — does performance drop with simpler prompts?\nCompare against human-written explanations on a held-out set — is the LLM summarizer actually helpful?\nDiscuss bias risks in inferring intent from appearance (e.g., race, clothing) — any fairness audit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OkfemtlNQP", "forum": "7qXmJbjbl8", "replyto": "7qXmJbjbl8", "signatures": ["ICLR.cc/2026/Conference/Submission25128/Reviewer_FtsS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25128/Reviewer_FtsS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905509745, "cdate": 1761905509745, "tmdate": 1762943333856, "mdate": 1762943333856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}