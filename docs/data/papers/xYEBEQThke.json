{"id": "xYEBEQThke", "number": 22891, "cdate": 1758336790367, "mdate": 1759896841066, "content": {"title": "MENTALMs: Teaching Language Models to Reason Efficiently in the Language of Thought", "abstract": "Large Reasoning Models (LRMs) achieve state-of-the-art performance in mathematics, code generation, and task planning. However, their reliance on long chains of verbose “thinking” tokens results in high latency, redundancy, and incoherent reasoning paths. Inspired by the Language of Thought Hypothesis —which posits that human reasoning operates over a symbolic, compositional mental language\ncalled Mentalese—we introduce a cognitively motivated framework that trains models to reason in a similar compact style, which we call MENTALMS. Mentalese encodes abstract reasoning as ultra-compressed, structured tokens, enabling models to solve complex problems with far fewer steps. To achieve both efficiency and accuracy, we propose SHORTER LENGTH PREFERENCE OPTIMIZATION (SLPO), a reinforcement learning method that directly optimizes models to generate concise yet correct reasoning by rewarding shorter solutions that maintain high accuracy while flexibly allowing longer reasoning when complexity demands it. When applied to Mentalese-aligned models, SLPO achieves much larger compression rates by enabling compressed reasoning that preserves the benefits of detailed thinking without the computational overhead, allowing us to present the best-performing models at each compression level along the performance-efficiency Pareto frontier. Across mathematical benchmarks—including AIME2024, Minerva, OlympiadBench, Math500, and AMC—our MENTALMS generates reasoning traces with 4–16× fewer tokens, achieves over 5× lower inference latency, and reduces training costs by 7–9× relative to the base DeepSeek R1 Distilled model, while maintaining 90-98% of baseline accuracy. MENTALMS also surpasses Claude and ChatGPT-4o by up to 5% in accuracy while maintaining 2× compression. Our findings demonstrate that Mentalese-style compressed reasoning offers a breakthrough toward human-like cognitive efficiency, opening new possibilities for real-time, cost-effective reasoning without sacrificing accuracy.", "tldr": "Teaching models to reason in a compressed symbolic style, Mentalese, reducing reasoning length by 4–16× with minimal accuracy loss.", "keywords": ["Large Language Models", "symbolic reasoning", "efficiency", "mathematical reasoning", "reinforcement learning with verifier rewards", "finetuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/628df56d4900bfe4b50b5b01cf30aa373059fa9e.pdf", "supplementary_material": "/attachment/e0981c726b8b4f962e8ab507f2f6a61bb0a88f3a.zip"}, "replies": [{"content": {"summary": {"value": "To achieve efficient reasoning, the authors propose a two-stage training strategy: first, fine-tuning using the Mentalese format, followed by SLPO (a variant of GRPO) with a reward design for length penalty. The authors verify the effectiveness of this training strategy with two models."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is good. Enabling LLMs to reason effectively is an important issue\n2. The results are overall significant, mainly attributable to the Mentalese format."}, "weaknesses": {"value": "1.  The method is not novel. Both the first fine-tuning with shorter CoT [1-3] and the length penalty [4-5]  have been largely investigated. From Table 1, the length reduction is mainly attributable to the Mentalese format, and the reward design does not bring consistent performance regarding the response length and its accuracy.\n\n2.  The writing needs improvement. For example, on Page 4, lines 170-172, \"We introduce ... Group Relative Policy Optimization (GRPO), a group-based extension of PPO for reasoning optimization\". GRPO was first introduced in the DeepSeek-Math paper, rather than being this paper's contribution\n\n3.  Regarding weakness 1, the baselines are largely lacking; the only baseline provided is the L1 method.\n\n4.  The generalization of the Mentalese format could have been further investigated. Whether the format can generalize to other non-math tasks has not been investigated.\n\n[1] Distilling System 2 into System 1, in arxiv 2024\n\n[2] C3oT: Generating Shorter Chain-of-Thought without Compromising Effectiveness, in AAAI 2025\n\n[3] TokenSkip: Controllable Chain-of-Thought Compression in LLMs, in EMNLP 2025\n\n[4] Concise Reasoning via Reinforcement Learning, in arxiv 2025\n\n[5] Kimi k1.5: Scaling Reinforcement Learning with LLMs, in arxiv 2025"}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "96vorZrich", "forum": "xYEBEQThke", "replyto": "xYEBEQThke", "signatures": ["ICLR.cc/2026/Conference/Submission22891/Reviewer_rgYf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22891/Reviewer_rgYf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485366550, "cdate": 1761485366550, "tmdate": 1762942428037, "mdate": 1762942428037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MENTALMS, a framework that replaces verbose natural-language reasoning traces with a symbolic “Mentalese” representation and optimizes for brevity and correctness using Shorter Length Preference Optimization (SLPO). The approach achieves significant token compression while maintaining accuracy, demonstrating efficiency–performance trade-offs across several mathematical reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of combining symbolic reasoning formats with RL-based brevity optimization is conceptually elegant and cognitively inspired.\n\n- Experiments show substantial reasoning compression (10–20×) with minimal performance degradation, suggesting practical benefits for inference efficiency."}, "weaknesses": {"value": "- The Mentalese traces are distilled from GPT-generated reasoning, but the paper does not sufficiently verify their semantic fidelity or robustness.\n\n- All experiments are conducted on a single model family, raising concerns about the generalizability of the method to other architectures.\n\n- While efficient reasoning is an important topic, the algorithmic contribution (mainly a variant of preference optimization) is relatively incremental and not highly novel."}, "questions": {"value": "No."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0McuHnkbNW", "forum": "xYEBEQThke", "replyto": "xYEBEQThke", "signatures": ["ICLR.cc/2026/Conference/Submission22891/Reviewer_RJPr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22891/Reviewer_RJPr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733770894, "cdate": 1761733770894, "tmdate": 1762942427728, "mdate": 1762942427728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **MENTALMs**, a framework that trains LLMs to reason in a compact, symbolic “Language of Thought” called **Mentalese**, paired with a reinforcement-learning stage that prefers shorter but still correct solutions (**SLPO**). The pipeline is: (i) supervised finetuning (SFT) on **Mentalese** traces distilled from a ~40k problem corpus (MENTALESE-40K), then (ii) **RLVR** with either GRPO or the proposed SLPO, using a verifier over original QA pairs. Experiments on math benchmarks (AIME 2024/2025, Minerva, AMC, OlympiadBench, MATH-500) and some OOD suites (GPQA, LSAT, MMLU) report large token reductions (4–16×), notable compression rates (sometimes ≥10×), and partial recovery of accuracy lost after Mentalese-SFT. The paper emphasizes a performance–efficiency Pareto frontier and claims lower training/inference costs relative to R1-style baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-specified symbolic format.** Mentalese provides explicit operators and typed steps that are compact and executable, giving a concrete target for alignment.\n- **SLPO design.** The group-relative, *shorter-among-correct* reward avoids over-penalizing uniquely long but correct derivations and is conceptually appropriate for variable problem difficulty. \n- **Comprehensive experiments.** The paper evaluates across multiple math suites and reports **training dynamics** that highlight stability/efficiency benefits of an intermediate Mentalese-SFT stage versus direct SLPO. \n- **Efficiency signal.** Substantial response-length reductions and training-time improvements are documented (e.g., reduced per-step latency after Mentalese alignment), pointing to a plausible path for deployable, lower-latency reasoning."}, "weaknesses": {"value": "1. **Mentalese effectiveness before RL is weakly evidenced.** The results note large performance degradation after Mentalese-SFT, with most accuracy recovered only after RLVR. To argue for Mentalese as an inherently superior representation (rather than a curriculum that relies on heavy RL), the paper needs pre-RL comparisons to other concise formats (token/chunk compression, logic templates, latent reasoning) under matched compute/data, and show Mentalese degrades less before or after policy optimization. As written, the headline improvements plausibly come primarily from RL over a large corpus (MENTALESE-40K derived from DEEPSCALER-PREVIEW) rather than the representation itself. If the authors can demonstrate that the **Mentalese** representation itself, independent of extensive post-training on original QA pairs, confers a consistent advantage over NL CoT and other compression schemes under matched data and compute, I will consider raising my score.\n2. **RLVR stage does not use Mentalese supervision and yields hybrid outputs.** In RLVR, training is performed on original QA pairs with verifier-based rewards; the model’s post-RL generations often mix symbolic primitives with natural language. This raises a causality concern: are the final gains driven mainly by standard NL reasoning + RL, with Mentalese acting only as an SFT regularizer? A direct RLVR-on-Mentalese-targets variant (or a verifier that enforces symbolic validity) is needed to substantiate Mentalese’s unique contribution versus strong NL-only RL baselines. \n3. **Attribution and fairness.** Several baseline comparisons conflate data scale and optimization budget with the proposed ideas. Equalize dataset size (e.g., fixed 40k) and RL steps across (i) NT CoT, (ii) other symbolic/structured formats, and (iii) Mentalese, then ablate the contribution of SLPO versus GRPO versus SFT."}, "questions": {"value": "1. Does RLVR actually optimize for Mentalese? Since RL uses original QA pairs and a verifier on final answers, can you add a variant that (a) trains RLVR with Mentalese-validity rewards (e.g., step-type correctness, syntax executability), or (b) constrains decoding to the symbolic grammar? This would test whether post-RL improvements genuinely stem from Mentalese rather than NL reasoning. \n2. Could you please give any insight for why you SFT on MENTALESE-40K and RLVR on the original QA pairs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t7MElWEaoW", "forum": "xYEBEQThke", "replyto": "xYEBEQThke", "signatures": ["ICLR.cc/2026/Conference/Submission22891/Reviewer_kmXV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22891/Reviewer_kmXV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890000183, "cdate": 1761890000183, "tmdate": 1762942427520, "mdate": 1762942427520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MENTALMS, a cognitively inspired framework that teaches language models to reason efficiently through a symbolic “Language of Thought” representation called Mentalese. Instead of producing verbose natural-language chains of thought, the model generates concise symbolic reasoning traces and is optimized via Shorter Length Preference Optimization (SLPO), a reinforcement learning method that rewards brevity without penalizing necessary reasoning. Experiments on multiple mathematical benchmarks show that MENTALMS achieves 4–16× compression in reasoning length and over 5× lower inference latency while retaining 90–98% of baseline accuracy, occasionally surpassing large commercial systems like GPT-4o and Claude 3.5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of replacing natural language reasoning with symbolic reasoning is impressive and well-motivated. The paper provides a concrete and feasible implementation of this idea, demonstrating that symbolic representations can significantly improve efficiency while maintaining accuracy across models such as DeepSeek and Agentica.\n2. The paper is well-presented, with clear visualizations, examples, and ablation analyses that effectively illustrate how the proposed Mentalese reasoning format operates and how it differs from traditional chain-of-thought approaches."}, "weaknesses": {"value": "1. The main weakness lies in the connection between the proposed symbolic reasoning and the training method. While Shorter Length Preference Optimization (SLPO) is effective, it appears largely orthogonal to the proposed symbolic reasoning, essentially a general reinforcement learning objective that could be applied to any RLVR process, not necessarily one aligned with Mentalese.\n2. The symbolic format, though efficient for mathematical reasoning, may not generalize well to broader or open-ended reasoning domains such as commonsense or multimodal tasks, where discrete symbolic operators are harder to define. It's recommended to evaluate the method on more general tasks."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ASrmNU2P96", "forum": "xYEBEQThke", "replyto": "xYEBEQThke", "signatures": ["ICLR.cc/2026/Conference/Submission22891/Reviewer_rHwC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22891/Reviewer_rHwC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898306425, "cdate": 1761898306425, "tmdate": 1762942427251, "mdate": 1762942427251, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}