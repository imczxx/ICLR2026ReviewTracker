{"id": "7ZpPwTr19l", "number": 21287, "cdate": 1758315919575, "mdate": 1759896930682, "content": {"title": "SCDBench: A Benchmark for LLM-Based Smart Contract Decompilers", "abstract": "Smart contracts are programs deployed on blockchains that manage digital assets and enable decentralized applications. While their bytecode is always accessible on-chain, more than 99% of Ethereum contracts lack verified source code, making decompilation essential for transparency and security analysis.\n\nTraditional decompilers rely on program analysis to produce structured but low-level representations. Recent advances in large language models (LLMs) enable source-like output with higher readability and even recompilability. Yet systematic evaluation is missing: existing tools use narrow datasets and inconsistent metrics, hindering fair comparison and reproducibility.\n\nWe present the first systematic benchmark for smart contract decompilation. Our contributions are: (i) a diverse dataset of real-world contracts, filtered for redundancy and stratified by difficulty; (ii) a staged evaluation framework with metrics for format completeness, compilability, Application Binary Interface (ABI) recovery accuracy, and semantic equivalence; and (iii) baseline evaluations using a fine-tuned reference model, establishing a strong foundation for future research.\n\nOur benchmark establishes a common ground for rigorous, reproducible evaluation and aims to accelerate the development of reliable smart contract decompilers for blockchain security and transparency.", "tldr": "", "keywords": ["Smart Contract", "Decompilation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b5792565df584d5c5f33461a96127d9767c56112.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper is timely, well-written, and conceptually valuable, addressing an important gap — a standardized benchmark for LLM-based smart-contract decompilation. The staged evaluation (format → compile → ABI → semantic equivalence) is thoughtful and reproducible, and the anonymous repository and openness policy meet reproducibility expectations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Smart-contract transparency and LLM decompilation are both emerging and important; this benchmark fills a genuine gap.\n2. The four-stage design (format → compile → ABI → semantics) is systematic and easy to reproduce.\n3. Writing, organization, and graphics are polished."}, "weaknesses": {"value": "1. Novelty clarity: what is truly “first”?\nYou claim “the first systematic benchmark” but do not run widely-used non-LLM decompilers (Gigahorse, Shrnkr, Heimdall-rs, Panoramix) as external baselines on your dataset. Without those, it’s hard to judge whether the benchmark fairly stresses existing approaches and how LLM decompilers compare head-to-head. Add a baseline suite covering at least one static and one symbolic/industry tool and report all four stages on them.\n\n2. Dataset Scale and Representativeness\nThe final benchmark includes only 150 contracts, corresponding to 2,735 unique functions. Although the construction pipeline (deduplication → clustering → stratified sampling) is sound, this sample size is insufficient for a benchmark claiming to evaluate semantic fidelity across the heterogeneous Ethereum ecosystem. Specifically: (1) With just 50 samples per difficulty bin, the variance across compiler versions, patterns, and application domains cannot be captured reliably; (2) The dataset omits Vyper, Yul, proxy upgrade patterns, and non-Ethereum EVMs. The verified-source bias means many prevalent contract archetypes. \n\n3. Stage dependencies and denominators are unclear\nStage 3 (ABI) and Stage 4 (semantic equivalence) are defined as contingent on compilability, but Tables 5–6 report high scores in medium/hard while Table 4 shows very few compilable outputs (e.g., only 3/21 compiled in hard for the reference model). Always report N (eligible) / N (bin) alongside each metric, and show CIs or bootstrap intervals. Provide a small flow diagram per bin: “#total → #format-ok → #compiled → #ABI-scored → #fuzzed.”\n\n4. Lack of Executable Source Code and Pipelines\nThe reference model fine-tuned from “Qwen3-4B-Instruct-2507” is not released in your repository, and 'weights' or 'checkpoints' are unavailable.\n\n5. Fuzzing-based semantic equivalence needs much stronger methodology\nTen random inputs per function is insufficient to establish semantic equivalence for contracts with wide state spaces. Use coverage-guided fuzzing (e.g., Forge’s fuzz with coverage hooks) + boundary heuristics for calldata/value, different msg.sender, msg.value, reentrancies, and environment (block.timestamp/number). Include stateful sequences (multi-call traces), not only single invocations.\n\n6. Missing strong baselines\nExisting non-LLM decompilers like Gigahorse, Elipmoc, Heimdall-rs, Panoramix are cited but not quantitatively compared. Without them, the benchmark cannot demonstrate where LLMs truly add value."}, "questions": {"value": "1. How do you guarantee that near-duplicate contracts or variants compiled from the same template do not appear across train/test splits? \n\n2. Have you performed a global similarity clustering to prevent leakage, or only per-dataset filtering?\n\n3. Beyond bytecode length, did you confirm that the “easy/medium/hard” bins correlate with actual decompilation complexity (e.g., control-flow depth, compiler optimizations, proxy patterns)?\n\n4. How well does SCDBench represent DeFi, NFT, and infrastructure contract types? Are all Solidity versions (0.4–0.8) and compiler settings proportionally covered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T6Cq6DehUb", "forum": "7ZpPwTr19l", "replyto": "7ZpPwTr19l", "signatures": ["ICLR.cc/2026/Conference/Submission21287/Reviewer_pyGw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21287/Reviewer_pyGw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829568760, "cdate": 1761829568760, "tmdate": 1762941672710, "mdate": 1762941672710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SCD-Bench, a benchmark for evaluating the capability of large language models (LLMs) in de-compiling smart contracts to recover the original code. The authors curate a set of 150 smart contracts whose source code is publicly available to construct the benchmark. The authors then evaluate a base LLM with and without fine-tuning to demonstrate the effectiveness of the benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper addresses an important problem, smart contract decompilation, which has not been extensively studied with LLMs. The problem is also well motivated (security and auditing of smart contracts).\n- The writing is very well organized and easy to follow.\n- The benchmark is well designed, with a clear methodology for selecting contracts and evaluating model performance."}, "weaknesses": {"value": "- Limited number of models evaluated -- while it seems like the main focus of the works on the benchmark itself, a more extensive evaluation of different LLMs (coding/reasoning etc.) would strengthen the paper.\n- The benchmark size is quite small (50 contracts per group). Can this really be considered as a representative benchmark for the task?\n- There are some hyperparameter choices that are not fully justified -- e.g. 10 inputs? is there some sort of completeness guarantee? and why $k=50$ and why $\\alpha=0.7$? More details on these choices would be helpful."}, "questions": {"value": "- Was synthetic data considered for any part of the benchmark? Fine-tuning, additional evaluation data, etc.\n- Why the choice Qwen-4B specifically? (other than context length) is there a reason?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GPk6zeTGLB", "forum": "7ZpPwTr19l", "replyto": "7ZpPwTr19l", "signatures": ["ICLR.cc/2026/Conference/Submission21287/Reviewer_DFwe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21287/Reviewer_DFwe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926698039, "cdate": 1761926698039, "tmdate": 1762941672298, "mdate": 1762941672298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SCDBench, the first systematic benchmark for evaluating LLM-based smart contract decompilers. SCDBench consists of a curated dataset of 150 real-world Ethereum contracts and 2,735 unique functions with filtering redundant (e.g., template tokens like ERC20) and duplicate contracts. This paper also proposes a stated evaluation framework with metrics such as ABI accuracy and format completeness."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Important and interesting topics that provide insightful benchmarks for further research.\n2. The benchmark smart contracts are meticulously selected. Duplication is a well-known issue in the world of smart contracts; I really appreciate the authors' efforts in removing duplicate and template contracts and providing a benchmark with high quality.\n3. The benchmark also carries a solid baseline method based on finetuned LLMs."}, "weaknesses": {"value": "1. The size of the dataset is somewhat limited. I would appreciate it if the authors could enlarge the dataset with more unique contracts in the future.\n\n2. The evaluation mainly compares a single fine-tuned model to its base version."}, "questions": {"value": "In general, this is a good benchmark paper with a rigorous and thoughtfully designed framework. I appreciate that the authors recognize the widespread duplication in real-world smart contracts and take concrete steps to remove redundant or trivial instances. I believe the paper introduced a high-quality and diverse dataset. My main concern lies with the baseline evaluation: the paper only reports results for a single fine-tuned LLM (Qwen3-4B-Instruct), without comparisons to other open-source models such as DeepSeek or Llama-based variants. Could the authors justify the reason for that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MHh5HWaSgd", "forum": "7ZpPwTr19l", "replyto": "7ZpPwTr19l", "signatures": ["ICLR.cc/2026/Conference/Submission21287/Reviewer_KxDQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21287/Reviewer_KxDQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977178681, "cdate": 1761977178681, "tmdate": 1762941671979, "mdate": 1762941671979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the first systematic benchmark for smart contract decompilation, including a dataset of 150 real-world contracts and a staged evaluation framework with metrics for format completeness, compilability. The paper conducts baseline evaluations using a fine-tuned reference model, establishing a strong foundation for future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality**\nThe paper introduced the first systematic benchmark for smart contract decompilation, with design principles that balance realism, diversity, and analytical value.\n\n**Quality**\nThe paper constructed the dataset with staged pipeline that consolidates near-duplicates into coherent families and then selects a representative yet diverse subset suitable for benchmarking. \n\n**Clarity**\nThe paper is well-written with description of benchmark construction method and characterstics. The benchmark is open-sourced on anonymous repo.\n\n**Significance**\nThe paper proposed a systematic benchmark which can help researchers evaluate their approach of using LLM to decompile smart contracts."}, "weaknesses": {"value": "- Dataset consisting of 150 smart contracts may still be small even though the authors try to get a representative yet diverse subset, there may be risk of overfitting.\n- Evaluation only on a small finetuned LLM Qwen3-4b, so it's hard to know whether the benchmark can assess the capability of state-of-the-art LLMs."}, "questions": {"value": "1. Why do you choose k = 50 to construct only 150 representative contracts of three different difficulty levels?\n2. Why you choose to only evaluate on a small LLM instead of state-of-the-art LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ip2a6fd2Yl", "forum": "7ZpPwTr19l", "replyto": "7ZpPwTr19l", "signatures": ["ICLR.cc/2026/Conference/Submission21287/Reviewer_YAt2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21287/Reviewer_YAt2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018884944, "cdate": 1762018884944, "tmdate": 1762941671479, "mdate": 1762941671479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}