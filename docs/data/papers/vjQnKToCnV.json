{"id": "vjQnKToCnV", "number": 18823, "cdate": 1758291155458, "mdate": 1759897079414, "content": {"title": "Free Energy Mixer", "abstract": "Standard attention stores keys/values losslessly but reads them via a per-head convex average, blocking channel-wise selection. We propose the Free Energy Mixer (FEM): a free-energy (log-sum-exp) read that applies a value-driven, per-channel log-linear tilt to a fast prior (e.g., from queries/keys in standard attention) over indices. Unlike methods that attempt to improve and enrich the $(q,k)$ scoring distribution, FEM treats it as a prior and yields a value-aware posterior read at unchanged complexity, smoothly moving from averaging to per-channel selection as the learnable inverse temperature increases, while still preserving parallelism and the original asymptotic complexity ($O(T^2)$ for softmax; $O(T)$ for linearizable variants). We instantiate a two-level gated FEM that is plug-and-play with standard and linear attention, linear RNNs and SSMs. It consistently outperforms strong baselines on NLP, vision, and time-series at matched parameter budgets.", "tldr": "", "keywords": ["Sequence Modeling", "Attention", "Transformer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8e8c3e63354a3256e419b5c3ef2694463481ad3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work observes that regular attention, together with most variants, is restricted to outputting a value mixture (before output projection) that lies within the convex hull of values. Certain operations, for example channel-wise argmax, are not fully representable (because the only selection signal comes from the q k product, which is broadcast over the value dimension before a linear operation). The authors propose the free energy mixture (FEM), which is a channel-gated interpolation between a regular weighted mixture of values, and a log-weighted-exp mixture of values with temperature. This can be augmented by a lightweight low-rank convolution at the inputs. Experiments on artificial tasks, language modelling, vision and time series data show the efficacy of this approach, which can be applied on top of a \"prior\" of softmax attention, gated linear attention or Mamba, outperforming regular softmax attention and other variants on a parameter-matched basis."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The key idea seems sound, the empirical investigation shows promising results, and the paper contains many theoretical justifications for the proposed design. While many works focus on making attention more efficient, this work has an equally relevant focus of making it more expressive. I particularly appreciated:\n\n - Comprehensive ablations of the four major architectural components in Table 1 and Table 2.\n - The intuition and results concerning when attention mixing mechanisms are constrained to the convex hull of values (2.2-(4) and C.4).\n - Empirical results from multiple domains and with multiple choices of prior.\n - Parameter-matching for a fair comparison across techniques.\n - Ability to reuse existing attention implementations by stacking/unstacking values."}, "weaknesses": {"value": "While I am generally positive about this paper for the reasons described above, I find it hard to follow, and so may not be as certain as I would like to be. My specific concerns are as follows:\n\n1. Clarity is poor. Details about the FEM design are spread out and interleaved with justifications and theorems. I would find it much easier to follow if the \"final\" (C+L+T+G) function were introduced first & with enough details to implement it, followed by justification & the general framework into which it fits, in a separate section.\n   - For example, Equations 4, 6, 9 and 10 overlap considerably, but with subtle differences.\n   - It wasn't clear to me how $\\beta_{\\text{max}}$ is chosen/parameterised (this detail is in Appendix K, L1681).\n   - The link between $\\beta$, $\\beta_{\\text{hid}}$, $\\beta_{\\text{max}}$ and $\\lambda$ takes some time to understand in the presentation of Section 2.3.\n1. The proposed method is has a privileged basis in the value space. That is, regular attention mixing would be equivariant to rotations of $v$ (i.e. $\\text{mix}(p, v) \\equiv R^{-1} \\text{mix}(p, R v)$ for any rotation $R$), while the proposed method is not equivariant. This is not a problem, but highlights that channel-wise arguments such as \"expressing channel-argmax\" promote methods with this basis-dependence, something that is not obviously necessary, from my perspective.\n1. I am unconvinced by the phrase \"lossy readout\". The readout of a value matrix to a \"mixed\" pre-output vector is inherently compressive. FEM may make the selection function class richer in some ways, but since it shrinks the value dimension when parameter-matched, it loses expressivity elsewhere. I think the paper can be read as \"there are mixing functions that we think are interesting/useful, but cannot be represented with usual convex mixing, while they can with FEM\". So I find it hard to agree that FEM makes the mixing \"less lossy\" in any real sense.\n1. Baseline results in Table 2 don't match Yang et al. (2024), Table 3. E.g. DeltaNet and HGRN2 with 1.3B parameters / 100B tokens, on HellaSwag, ARC-{C,E}, BoolQ. Both use lm-evaluation-harness. Is there a known reason?\n\nMinor comments:\n\n- No discussion about whether/how this is compatible with grouped query attention (Ainslie et al., 2023). I might be concerned that query heads that share a value may become too similar, if $e^{v}$ is sparser/lower-temperature than $e^{q k^T / \\sqrt{D}}$.\n- Since much is made of the channel-argmax limitation, I expected a \"toy problem\" demonstrating the FEM's ability to solve this in a single layer, contrasted with convex mixing.\n- L111 says \"causality ... can be encoded\", but causality is already encoded in the equations above which run \"over past indices\".\n- L156 says \"$H L \\geq D$ (which is not practical)\", but I think $H L$ is commonly $> D$, e.g. Llama 3 8B has 32 blocks and 32 query heads per layer, with head dimension 128.\n- Typo L1635 \"Equation equation 35\"\n- Appendix J.1 would benefit from references, even if these are referenced elsewhere.\n\n---\n\n_Yang, S., Kautz, J. and Hatamizadeh, A., 2024. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464._\n\n_Ainslie, J., Lee-Thorp, J., De Jong, M., Zemlyanskiy, Y., Lebrón, F. and Sanghai, S., 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245._"}, "questions": {"value": "My main questions are in \"specific concerns\" above.\n\n**Clarifying question** I understand the preferred FEM layer (with softmax prior, linearised temperature, conv & gating) to be something like the following pseudocode. I would appreciate any corrections.\n\n```\n# given x\n# given p = softmax((q @ k.T) / sqrt(d))\n# given parameters {W_{c,v,g,lam,o}, w_beta}\n\nc_v, c_g, c_lam = low_rank_conv(x, W_c)\nv = (W_v @ x) * (1 + c_v)\ng = rms_norm(softplus((W_g @ x) * (1 + c_g)))\nlam = sigmoid((W_lam @ x) * (1 + c_lam))\nmu = p @ v\nbeta = softplus(w_beta + 1.8)\nF = log(p @ exp(beta * v)) / beta\no = g * ((1 - lam) * mu + lam * F)\nreturn W_o @ o\n```"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L8ofscflkX", "forum": "vjQnKToCnV", "replyto": "vjQnKToCnV", "signatures": ["ICLR.cc/2026/Conference/Submission18823/Reviewer_ajEo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18823/Reviewer_ajEo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761348958217, "cdate": 1761348958217, "tmdate": 1762999979172, "mdate": 1762999979172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Free Energy Mixer (FEM), a novel inference-time mechanism that addresses a perceived limitation in standard attention: the inability to perform per-channel selection from the key-value cache. The authors argue that the standard convex combination of values within each attention head synchronizes channels, preventing them from independently selecting different past indices. FEM tackles this by reframing the read operation as a variational free-energy optimization. It uses a value-driven, per-channel \"tilt\" on a prior distribution (e.g., from Q/K scores) to produce a posterior, enabling a smooth interpolation from averaging to near-hard selection. The paper presents a two-level gated instantiation of FEM that is plug-and-play with various backbones (softmax/linear attention, RNNs, SSMs) and demonstrates consistent performance gains on NLP, vision, and time-series tasks under matched parameter budgets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Motivation and Problem Identification: The paper compellingly argues that the standard per-head convex combination of values creates a \"lossy read\" bottleneck, preventing channel-wise selection. This is a well-articulated and non-trivial insight into a potential limitation of standard attention mechanisms.\n\nTheoretical and Empirical Rigor: The proposed FEM method is grounded in a solid theoretical framework derived from the Donsker-Varadhan variational principle. The paper is supported by thorough theoretical analysis and extensive experiments across diverse domains (synthetic tasks via MAD, language modeling, image classification, time-series forecasting) and model architectures (FEM-SM, FEM-GLA, etc.). \n\nPracticality: A key strength is the method's plug-and-play nature and its ability to preserve the asymptotic time complexity of the underlying prior mechanism (e.g., O(T²) for softmax, O(T) for linear attention), as demonstrated by the latency/throughput analysis for FEM-SM."}, "weaknesses": {"value": "1.  **Baseline Comparisons:** While the empirical results are comprehensive, the baselines could be strengthened by including more recent architectures that also introduce channel-wise inductive biases. A comparison against methods like Mamba (with its data-dependent SSM) or Multi-head Latent Attention (MLA), which inherently manipulate channel interactions, would provide a more rigorous assessment of FEM's unique contribution beyond simply adding channel-specific gating or modulation.\n2.  **Core Motivation and Justification:** The central premise—that enabling full per-channel selection is a critical goal—may be somewhat overstated.\n    *   It potentially creates a false dichotomy between the roles of the attention mechanism (for token mixing) and the subsequent Feed-Forward Network (FFN, for channel mixing). The universal approximation theorem suggests that a standard Transformer block, with MHA and FFN, is already capable of learning complex functions, and the necessity of extreme channel-wise selection within the attention block itself is not fully justified.\n    *   Many studies indicate that attention heads exhibit significant sparsity and redundancy. Therefore, the performance gains observed with FEM might not stem from achieving perfect channel-wise selection, but rather from introducing a more effective and general form of channel-wise *modulation* or *gating*. Reframing the contribution around enhancing channel *utilization efficiency* rather than enabling a theoretically maximal selection capacity might be a more compelling and accurate narrative.\n3.  **Efficiency Analysis:** The computational cost analysis, while provided for FEM-SM, is incomplete. The overhead of FEM when applied to its linear-time variants (e.g., FEM-GLA, FEM-Mamba) is not thoroughly quantified. In large-scale training and inference, even a modest constant-factor overhead for linear-time models can become a significant practical bottleneck. The performance benefits of FEM might diminish when scaled to very large models and datasets, while the computational cost remains.\n\n**Questions and Suggestions for Authors**\n\n*   Could you include comparisons with other modern sequence models that incorporate strong channel-wise interactions (e.g., Mamba, MLA) to better isolate the benefit of FEM's specific variational formulation?\n*   The paper would be strengthened by a more nuanced discussion on the interplay between attention and the FFN. Could the observed gains be interpreted as FEM providing a more powerful form of channel-wise conditioning *before* the FFN, rather than strictly enabling a previously impossible \"channel-wise selector\"?\n*   Please provide a detailed latency/throughput analysis for the linear-time FEM variants (FEM-GLA, FEM-Mamba) to give a complete picture of the method's computational trade-offs."}, "questions": {"value": "For details, please see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yes1R18dOt", "forum": "vjQnKToCnV", "replyto": "vjQnKToCnV", "signatures": ["ICLR.cc/2026/Conference/Submission18823/Reviewer_bNwC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18823/Reviewer_bNwC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807012452, "cdate": 1761807012452, "tmdate": 1762999979536, "mdate": 1762999979536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a \"lossless-storage versus lossy-processing dilemma\" (Section 1) in standard attention mechanisms. The authors argue that the attention readout, a \"per-head convex average\" , prevents channel-wise selection from the key-value cache, meaning it \"cannot realize a generic channel-wise selector\" (Section 2.1, Corollary 2.3). To address this, the paper proposes the Free Energy Mixer (FEM), a new layer that replaces the standard attention readout. FEM uses a \"free-energy (log-sum-exp) read\". This method treats the standard (q, k) attention distribution as a \"prior\" and applies a \"value-driven, per-channel log-linear tilt\" to generate a \"value-aware posterior read\". This formulation allows the model to \"smoothly moving from averaging to per-channel selection\". The proposed FEM is \"plug-and-play\" with various architectures (e.g., standard attention, linear attention, RNNs, SSMs) and \"preserves the corresponding time complexity\" (Section 2, Contribution 3). The final model is a \"Two-Level Gated FEM\" that uses \"Linearized Temperature Learning (LTL)\" (Section 2.3.1) for efficient dynamic temperature control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a solid contribution to the field.\n\n**Originality**: The primary contribution is the principled reframing of the attention readout as a \"variational free-energy optimization\" (Section 2, Contribution 2). This provides a novel, value-aware alternative to the standard convex-combination readout. The \"Linearized Temperature Learning\" (Section 2.3.1)  is also a clever technique for maintaining efficiency. The originality is slightly tempered by prior work on LSE-based mechanisms, such as \"LASER attention\" (Section 1), which the paper acknowledges but does not sufficiently distinguish itself from.\n\n\n**Significance**: The contribution is significant. The \"plug-and-play\" (Abstract) and \"agnostic\" (Section 2, Contribution 3)  nature of FEM means it could potentially serve as a general-purpose upgrade for many sequence models, including SSMs and linear RNNs. The empirical results, showing consistent gains across \"NLP, vision, and time-series\" (Abstract), validate its value."}, "weaknesses": {"value": "1. **Practical Efficiency and Latency**: The paper's primary weakness is the gap between theoretical and practical efficiency. While FEM preserves \"asymptotic complexity\" (Abstract) , Table 5 shows a significant practical latency cost: \"FEM-SM\" (0.017s) is substantially slower than the baseline Transformer (\"FEM-SM (-G,T,L,C)\", 0.012s)  in the forward pass. The authors' \"Limitation\" (Section 4) section admits this is due to a \"lack of fused CUDA kernels\" (Section 4). This is a critical barrier for adoption, as the community heavily relies on optimized kernels (e.g., FlashAttention).\n\n2. **Novelty in Relation to Prior Work**: The paper needs to more clearly differentiate itself from prior work using LSE. The paper mentions \"nonlinear mixing such as log-sum-exp in LASER attention\" (Section 1) and \"Tropical attention\". The paper claims these \"do not address the lossy processing limitation\" (Section 1), but this distinction is not elaborated upon. A more detailed comparison is needed to establish why FEM's specific free-energy formulation  succeeds where these other LSE-based methods allegedly fail.\n\n3. **Architectural Complexity**: The final \"Two-Level Gated Free Energy Mixer\" (Section 2.3.2) is a complex module. It combines the core LSE mechanism with a \"low-rank convolution\" (Section 2.3.3) , an \"inner (temperature) gate $\\lambda_{t}$\", and an \"outer gate $g_{t}$\". The ablation study in Table 1 (e.g., \"FEM-SM\" Avg 80.2 vs. \"FEM-SM(-G)\" Avg 79.4) suggests all these components are necessary for optimal performance. This adds many new hyperparameters (e.g., $\\beta_{max}$, $H_{c}$) and increases the difficulty of implementation and tuning compared to standard attention."}, "questions": {"value": "1. Regarding Weakness 1, could the authors provide a more fine-grained latency breakdown? The \"LSE Implementation\" (Fig 2f)  computes both the mean and the LSE branch. What is the measured latency and throughput overhead of only the FEM components (L, T, G) when applied to an already-optimized baseline (e.g., FlashAttention), rather than the unoptimized baseline in Table 5?\n\n2. Regarding Weakness 2, please elaborate on the distinction with LASER attention. The claim that it \"do[es] not address the lossy processing limitation\" (Section 1)  is critical. How does FEM's formulation of LSE as a \"value-aware posterior\" derived from a \"prior\" fundamentally differ from LASER's \"Attention with exponential transformation\"  in addressing the channel-wise selection problem?\n\n3. Regarding Weakness 3, how sensitive is the model to the $\\beta_{max}$ hyperparameter? The paper describes it as a \"learnable global maximum inverse temperature\" (Section 2.3.2) and initializes it near 1.0 (Section K). What range of values does $\\beta_{max}$ converge to during training? How does performance vary if $\\beta_{max}$ is fixed at different values (e.g., 1, 5, 10)? (By the way, a more theoretical explaination will be preferred than empirical demonstrations for this question)\n\n4. **KV-Cache Reduction:** The paper states that \"budgeting strategy (i)\" \"reduces the dimension of the value part needed to be stored in the KV-cache by half\" (Section 2.3.3). This seems to be a significant practical benefit (halving the KV-cache memory), but it is not highlighted in the introduction or abstract. Can the authors confirm this interpretation and, if correct, was this memory reduction reflected in the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HMKx5rYj3Y", "forum": "vjQnKToCnV", "replyto": "vjQnKToCnV", "signatures": ["ICLR.cc/2026/Conference/Submission18823/Reviewer_hSiq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18823/Reviewer_hSiq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807548926, "cdate": 1761807548926, "tmdate": 1762999979603, "mdate": 1762999979603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the \"Free Energy Mixer\", a novel way of mixing values within the attention block. In the standard attention operation, the $Q$-$K$ dot-product determines the probability distribution over the sequence length, and the output is a weighted-average of the $V$ vectors accordingly. This operation mixes all channels in the same way however, so it cannot do an operation such as per-channel maximum. By posing the problem as a DV free-energy problem, the authors pose the read-out as a mix between the \"low temperature\" extreme (choose the maximum value per channel), and \"high temperature\" extreme (weighted average), controlled by a learnable parameter, thus enabling smooth control between the two regimes. Similarly, the technique can be applied to recurrent architectures such as RNNs and SSMs, by re-interpreting the prior distribution. The authors showcase that, by using this architecture as a drop-in replacement for attention/recurrence, it can improve the performance of the models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- To the best of my knowledge, the authors introduce a novel technique for mixing the value vectors in the attention block, enriching the expressivity of the operation. This approach can be applied to both attention-based models, as well as recurrent variations.\n- The authors do extensive evaluation on reasonable model/training budgets (1.3B parameters/100B tokens, 340M parameters/15B tokens), covering language modelling, image modelling, and time series forecasting.\n- The authors account for the architectural differences by parameter-matching during their evaluations.\n- Figures 1 and 2 are clear and instructive."}, "weaknesses": {"value": "- I am not entirely convinced that the \"lossy mixing\" of the classical attention is obviously a significant limitation. While the theoretical justification seems compelling, it would be interesting to understand how different the proposed mixing ends up being from the standard attention after training. Nonetheless, this does not detract from the paper’s contribution, as the empirical results indicate consistent benchmark improvements.\n- The proposed method appears to add some constant computational overhead (due to the per-channel log-sum-exp operation), even if the asymptotic complexity remains unchanged. This means that, although the methods might be parameter-matched, they are not necessarily FLOP-matched."}, "questions": {"value": "- Could the authors clarify the exact FLOP overhead relative to standard attention? While they state that the asymptotic complexity is preserved, a quantitative estimate of constant-factor costs would be informative.\n- Could the authors clarify if the method could be fully fused with the hardware-efficient optimisations, such as Flash Attention? Are there any difficulties that might be an issue?\n- Similarly, a report on the total wall-clock time differences between the methods would be helpful.\n- Minor: I was not familiar with the $p \\in \\Delta$ / $p \\in \\Delta(M)$ notation, the authors could potentially define it in the text.\n- Minor: Bold/underline notation could be clarified in the tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NakhoqSmCB", "forum": "vjQnKToCnV", "replyto": "vjQnKToCnV", "signatures": ["ICLR.cc/2026/Conference/Submission18823/Reviewer_YvfL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18823/Reviewer_YvfL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18823/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909199505, "cdate": 1761909199505, "tmdate": 1762930576266, "mdate": 1762930576266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}