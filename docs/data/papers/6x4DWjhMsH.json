{"id": "6x4DWjhMsH", "number": 21710, "cdate": 1758320750256, "mdate": 1759896907691, "content": {"title": "Scaling Laws of Refusal Robustness: Why Bigger LMs Are Not Necessarily Safer", "abstract": "Large language models (LLMs) increasingly exhibit emergent refusal behaviors, yet the scaling laws of safety alignment remain poorly understood. A common assumption — “bigger is safer” — has not been systematically tested under adversarial pressure. We introduce the first general evaluation framework for refusal robustness scaling, defined by three complementary metrics: Refusal Robustness Rate (RRR), Refusal Drift (RD), and Compliance Error (CE). This framework enables reproducible comparison of LLMs under both adversarial fine-tuning attacks (LoRA) and prompt-based jailbreaks (e.g., GCG). Across models from 1.1B to 7B parameters, we reveal a scaling law of refusal robustness: although larger models demonstrate stronger baseline refusal ability, adversarial compute — not model size — dominates post-attack robustness. Specifically, LoRA attacks universally collapse refusal (RRR→0), while stronger prompt-based attacks amplify RD and CE even in larger models. Our contributions are threefold: (1) a reproducible framework for measuring refusal robustness scaling, (2) a comparative analysis of fine-tuning vs. prompt-based attack paradigms, and (3) the first scaling-law characterization showing that adversarial compute systematically overrides safety gains from scale. We further identify a three-stage evolutionary pattern of refusal behavior, providing a conceptual model of how safety features emerge and break under pressure. These results challenge the assumption that scaling guarantees safety and establish refusal robustness scaling as a principled dimension of LLM evaluation.", "tldr": "Scaling up LLMs improves baseline refusal but does not guarantee safety—adversarial compute quickly overrides robustness, and we present the first reproducible framework to quantify this effect.", "keywords": ["Large language models", "refusal robustness", "adversarial fine-tuning", "prompt-based attacks", "scaling laws", "safety alignment", "evaluation framework"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4c2ff7236d3599a6170882a3b3b695a4f15c8ffa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies how safety alignment is compromised in LLMs of different sizes, specifically when they undergo prompt jailbreak attacks & fine-tuning attacks."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper provides a decent study regarding the scaling law of LLM safety -- a problem that has not been fully understood before.\n* The results indicate that refusal robustness cannot be safeguarded by scaling up model size alone.\n* The paper writing is overall smooth."}, "weaknesses": {"value": "* Contributions of this work are limited. The major phenomenon, \"adversarial success often scales smoothly with attack compute rather than being bounded by size,\" is already well-known and discussed in many prior works [1,2,3,4,5,6]. For example:\n  * [1,2] show that when the times of sampling and the number of in-context samples increase, model safety will be gradually compromised.\n  * Results in [3,4] show that when the attack compute increases (i.e., more SFT attack steps), the model safety level inevitably decreases. \n  * Furthermore, various LLM safety benchmarks [5,6] already show that larger models, even within the same family, are not necessarily safer (e.g., see Table 5 of [5]).\n\n  The contributions of this paper over these prior work are not clear enough.\n* Since the paper aims to study the \"scaling law\" of LMs, I would expect experiments on far more models of different sizes and families, in a more controlled manner. For example, I would suggest the authors first compare models within the same model family (e.g., Qwen2-1.5B, Qwen2-7B, and Qwen2-72B), and later check if the conclusion applies to different model families (e.g., Qwen2 v.s. Phi). In contrast, the authors only studied TinyLlama-1.1B, Phi-3-mini-3.8B, and Qwen2-7B -- 3 relatively small models from 3 different families (that are trained with different data and methods). I doubt whether this experimental setting can lead to the conclusion of a \"scaling law.\"\n  * For example, the conclusion in Line 137 -- \"As shown in Figure 1(a), refusal ability does not scale monotonically with parameter count\" --- may be inaccurate. What if the models are conditioned on the same \"pretraining and alignment\" (Line 140) strategies? In that case, will the models' refusal ability scale with parameter count?\n* The references are incorrect (e.g., wrong authors and conference). For example,\n  * \"Best-of-N Jailbreaking\" is not published in \"NeurIPS 2024\"\n  * The authors of \"Safety alignment should be more than just a few tokens\" are not \"Heng Qi et al.\"\n\n[1] Many-shot jailbreaking. https://www.anthropic.com/index/ many-shot-jailbreaking\n\n[2] Best-of-N Jailbreaking. Arxiv\n\n[3] Tamper-Resistant Safeguards for Open-Weight LLMs. ICLR 2025\n\n[4] On Evaluating the Durability of Safeguards for Open-Weight LLMs. ICLR 2025\n\n[5] SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models. ACL 2024\n\n[6] SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal. ICLR 2025"}, "questions": {"value": "See \"Weaknesses.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8pcWm3TV9A", "forum": "6x4DWjhMsH", "replyto": "6x4DWjhMsH", "signatures": ["ICLR.cc/2026/Conference/Submission21710/Reviewer_aNXw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21710/Reviewer_aNXw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761333003272, "cdate": 1761333003272, "tmdate": 1762941899063, "mdate": 1762941899063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study how finetuning a model on harmful prompt-completion pairs leads to increased compliance with future harmful requests. They track decrease in refusal rate, increase in response drift, and mixed effects for whether the initial response (eg \"sorry I can't help you with that\") matches the output (\"here's how to make a bomb...\") or not."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors study three models of different sizes, which is good. They also have a \"reproducibility\" section."}, "weaknesses": {"value": "The main weaknesses of this paper are\n\n1) lack of engagement with the literature\n\nprevious work has looked at scaling properties of robustness, see https://arxiv.org/abs/2407.18213 and https://arxiv.org/abs/2501.18841 for example\n\n2) lack of experiments\n\nthe authors have one set of experiments. It's commendable to have studied three models, but as presented, this does not come across as enough for a paper\n\n3) lack of clear story\n\nit wasn't clear to me what the take-home message is from the experiments and overall paper. \"scale is not enough\" is something that the community already knows. \"training a model to do bad things makes the model do bad things\" is also something the community knows.\n\n4) overall presentation, strange claims\n\nthe plots are hard to understand (small, missing datapoints, no error bars). The related work section and references are strangely formatted. \n\n5) strange threat model\n\nfinetuning a model to do bad things is quite an unusual threat model; it would be good for the authors to explain why it's relevant"}, "questions": {"value": "11: \"exhibit emergent refusal behaviors\" do they? I'm not aware of this\n13: \"has not been systematically tested\" it has, see \"weaknesses\"\n17: \"adversarial fine-tuning attack\" this is quite an unusual attack, as it assumes the attacker has access to training the model. Where do you expect this to be a realistic threat model?\n21: \"stronger prompt-based attacks\" not sure what this means? also, nothing is stronger than fine-tuning\n25: \"the first scaling law characterization\" you don't fit any scaling laws though. you can talk about scaling trends, but that's already been done (see weaknesses)\n36: \"instructions more effectively\" citation please!\n44: \"harmful continuations\" citation please!\n46: I have not heard of LoRA as a threat model. Why is this realistic?\n56: offense-defense scaling: how is the defense scaling in your experiments? I'm not aware of you doing any defense?\n65: \"excluding trivial false positives\" what are these?\n63-66: why not use an LLM judge? that is standard practise nowadays\n78: \"once refusals collapse\" what does this mean?\n82-85: not sure what you're trying to say here\n91: \"more efficiently\" do you mean \"at a lower cost\"?\n102: \"if low-level kernels are unavailable\" were they in your case? is this relevant information?\n105: \"adversarial fine-tuning\" usually \"adversarial training\" means \"training on attacked examples, with the *desired* response (eg refusal) in order to make the model robust\". But here you use it as an attack. This was very confusing!\n107: \"redteam_eval.csv\" where can I find this file?\n113-116: did you make this dataset yourself? if so, please give examples in the paper!\n118: \"high-precision lexical detector\" this sounds like a very flowery way of saying \"string match\". Just say what it is please, no need for fancy language, it just makes it more confusing!\n118-120: why not use a LLM judge?\n128: \"refusal drift\" if you're just looking at the representational shift of the response, then isn't it \"response drift\" vs \"refusal drift\"? because you measure it even when the model doesn't refuse\n145: what happens between 0 and 500 steps? eg at 10, 100, 200? probably lots of interesting stuff\n146: \"Increasing training steps lowers the loss but does not restore safety\" I don't know what this means. Surely training more on bad data would not be expected to \"restore safety\"?\n157-161: please support this claim with examples of responses from the models\nFigure 1:\nplots, especially text, are too small\nlines are overlapping making it hard to tell what's going on. please add an offset so we can see all of them\nwhy is green starting at 500?\nin (b), why are there datapoints at 0, I thought this was not possible?\ntable 1:\nwhy are the 3.8B and 7B models so similar in terms of RD? can you show examples?\n194: \"concrete guidance\" what is this?\n194-199: why is there such different performance across models? what about a smaller qwen model? how does it vary over multiple seeds?\n198: \"stable compliance\" what does this mean?\n203-215: what is the importance/implication of these findings?\n221: \"overwrite refusal subspace\" what does this mean? what gives you grounds to make this claim?\n222: \"refusal robustness follows an offense-defense compute race\" you've only showed attack increasing its strength, not defense, so not sure where the race comes in in these experiments\nSection 5: this section feels weird. Maybe rework this info into the methodology or background section\nsection 6: thanks for having a reproducibility section. You can put this section at the very end (reproducibility statement doesn't need to be in the main body)\n256: \"We release scripts...\" glad to see intention to release scripts. Where can I find them?\n264: \"indicating shallow alignment\" what is this, and how can you be sure?\n269: \"one exploits prompt space, the other rewrites the representation space\" not sure what this means, and this claim comes out of nowhere\nRelated Work: there is some good stuff in here, but it should be a couple of paragraphs telling a story, not a bunch of individual statements\nReferences: generate bibtex entries automatically eg using google scholar. also, llama 2 and 3 have technical reports, cite those instead of the HF pages. (https://arxiv.org/abs/2307.09288 and https://arxiv.org/pdf/2407.21783)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RJMvSNtFRL", "forum": "6x4DWjhMsH", "replyto": "6x4DWjhMsH", "signatures": ["ICLR.cc/2026/Conference/Submission21710/Reviewer_3hn6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21710/Reviewer_3hn6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966894835, "cdate": 1761966894835, "tmdate": 1762941898719, "mdate": 1762941898719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the common assumption that larger LLMs are inherently safer. The authors argue that an LLM's refusal robustness is not primarily determined by its size but rather by the amount of computation used to attack it. To test this, the paper introduces a new evaluation framework with three metrics: Refusal Robustness Rate, Refusal Drift, and Compliance Error. The findings include (1) model size did not correlate with safety; (2) lora attacks are universally effective."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper contributes a valuable framework beyond simple pass/fail evaluation. By introducing Refusal Drift and Compliance Error, it provides a way to measure how a model's safety fails."}, "weaknesses": {"value": "- The paper makes broad claims about \"scaling laws\" but bases its conclusions on only three open-source, instruction-tuned models. \n- The conclusions are drawn from a very small data sample. The adversarial fine-tuning used a training set of only approximately 500 prompt-completion pairs , and the final evaluation for all models and attacks was performed on a unified held-out set of 100 adversarial prompts. This small benchmark size limits the statistical power of the conclusions.\n- The authors acknowledge their new metrics have weaknesses. RRR and CE depend on rule-based detection and LLM-assisted auditing, which are prone to false positives and negatives. \n- The paper reads odd even after, as the authors have stated, using GPT-5 to assist writing. For example, the related work section reads like disconnected sentences without clear logic flows. A further round of revision will be helpful to this paper."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WrEVnC94oA", "forum": "6x4DWjhMsH", "replyto": "6x4DWjhMsH", "signatures": ["ICLR.cc/2026/Conference/Submission21710/Reviewer_bYQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21710/Reviewer_bYQd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028404702, "cdate": 1762028404702, "tmdate": 1762941898381, "mdate": 1762941898381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the scaling properties of adversarial robustness to prompt-based and finetuning attacks by studying TinyLlama-1.1b, Phi-3-mini-3.8b, and Qwen2-7b, using three different metrics to understand refusals."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Originality: The paper studies different metrics that capture more qualitative changes in how the model's refusal mechanisms break down.\nClarity: The paper is reasonably understandable for what it covers."}, "weaknesses": {"value": "W1 (Quality): The paper uses one example from each model family to achieve its scaling, rather than picking one model family (such a Qwen3) and varying the size within that model family. This makes it harder to understand what is a result of model scale, as opposed to other changes in the training recipe.\nW2 (Significance): The paper is missing related work on scaling trends in adversarial robustness (such as https://arxiv.org/pdf/2407.18213 , or its early citations).\nW3) The paper is substantially below the page count, and appears incomplete (i.e. listing prompt-based attacks in the threat model, but not presenting results on them)."}, "questions": {"value": "Q1) Are the Figure 1 subcaption headings mislabeled? It seems like (b) should be the caption for the far left graph, and that the middle graph should have a new caption describing refusal drives.\nQ2) Are the results for prompting attacks missing? Figure 1 and Table 1 seem to only describe the LoRA finetuning attack."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rUxbJ313TG", "forum": "6x4DWjhMsH", "replyto": "6x4DWjhMsH", "signatures": ["ICLR.cc/2026/Conference/Submission21710/Reviewer_qBEE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21710/Reviewer_qBEE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21710/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762323228476, "cdate": 1762323228476, "tmdate": 1762941898150, "mdate": 1762941898150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}