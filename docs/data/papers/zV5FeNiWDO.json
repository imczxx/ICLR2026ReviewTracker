{"id": "zV5FeNiWDO", "number": 14040, "cdate": 1758227490615, "mdate": 1759897394197, "content": {"title": "Lavida-O: Elastic Large Masked Diffusion Models for Unified Multimodal Understanding and Generation", "abstract": "We propose Lavida-O, a unified Masked Diffusion Model (MDM) for multimodal understanding and generation.  Unlike existing multimodal MDMs such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O presents a single framework that enables image-level understanding, object grounding, image editing, and high-resolution (1024px) text-to-image synthesis. Lavida-O incorporates a novel Elastic Mixture-of-Transformers (Elastic-MoT) architecture that couples a lightweight generation branch with a larger understanding branch, supported by token compression,  universal text conditioning and stratified sampling for efficient and high-quality generation. Lavida-O further incorporates planning and iterative self-reflection in image generation and editing tasks, seamlessly boosting generation quality with its understanding capabilities. Lavida-O achieves state-of-the-art performance on a wide range of benchmarks including RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive models and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable speedup at inference. These advances establish Lavida-O as a new paradigm for scalable multimodal reasoning and generation.", "tldr": "We built a state-of-the-art unified masked diffusion model for image understanding ,object grounding, image generation and editing tasks.", "keywords": ["Masked Diffusion Model", "Unified Multi-modal model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/48bf1850e13318341d76c839e437c7ac8cc3614a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Lavida-O, a unified Masked Diffusion Model for multimodal understanding and generation. Building upon LaViDa, the model integrates an Elastic Mixture-of-Transformers (Elastic-MoT) architecture with asymmetric branches for efficient parameter sharing. It supports diverse tasks such as image understanding, object grounding, image editing, and high-resolution text-to-image generation, achieving state-of-the-art performance on multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Proposes a unified framework that effectively bridges image understanding and generation.\n\n2.The Elastic-MoT architecture is efficient, reducing training cost while maintaining strong performance.e\n\n3. Demonstrates impressive empirical results across several benchmarks, indicating strong generalization."}, "weaknesses": {"value": "1.The novelty of the work appears somewhat incremental — the Mixture-of-Transformers (MoT) paradigm has been extensively studied, and the adoption of discrete diffusion is also not particularly new. The paper’s main contribution seems to lie in engineering integration and architectural refinement, rather than proposing a fundamentally new concept.\n\n2.The training cost appears rather high. According to the appendix, the model requires around 34 days of training, which may be excessive for a unified framework. It would be helpful if the authors could discuss potential efficiency improvements or training optimizations that make the approach more practical."}, "questions": {"value": "see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NDpjf6zO8k", "forum": "zV5FeNiWDO", "replyto": "zV5FeNiWDO", "signatures": ["ICLR.cc/2026/Conference/Submission14040/Reviewer_dKrA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14040/Reviewer_dKrA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761626392995, "cdate": 1761626392995, "tmdate": 1762924529595, "mdate": 1762924529595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Lavida-O, a unified Masked Diffusion Model (MDM) designed to handle a wide range of multimodal tasks, including image-level understanding, object grounding, image editing, and high-resolution (1024px) text-to-image synthesis. The key idea is to build a single, efficient framework that surpasses existing specialized models and other unified approaches (like AR or AR+diffusion models)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed architecture is efficient and effective. It intelligently combines a large 8B understanding model with a smaller 2.4B generation model, improving training/inference efficiency (Fig 5) while maximizing performance by leveraging the pre-trained understanding branch.\n\n- The integration of planning (layout generation) and self-reflection (iterative refinement) is a key innovation. It provides a concrete mechanism for the model's understanding and generation capabilities to mutually benefit each other, leading to large, quantifiable gains in prompt-following and editing (Tables 3, 13, 14).\n\n- The paper is packed with valuable technical innovations for MDMs, including stratified random sampling (improves FID, Table 10), universal text conditioning (improves control), and coordinate quantization (enables parallel decoding for grounding)."}, "weaknesses": {"value": "- The paper acknowledges that the model's text rendering capability is \"very limited.\" This is a significant weakness for a model aiming for general-purpose multimodal capabilities, as text is a common element in user requests. The authors attribute this to the VQ tokenizer and data, but it remains a key area for improvement.\n- The model suffers from a \"pixel shift\" problem, where non-edited regions are altered during editing. While attributed to the training data, this lack of precision detracts from the quality of the editing feature.\n- While performance on MathVista improved over the base model, it still \"lags behind state-of-the-art AR models\" (Sec D, Fig 13). This indicates that the MDM architecture, or at least this implementation, has not yet closed the gap on high-level, abstract reasoning."}, "questions": {"value": "- The planning and reflection mechanisms appear to be invoked via specialized prompts (e.g., \"please generate a layout...\", Sec A.7). Does the model have any capability to *autonomously* decide when to use planning or reflection based on its own assessment of the prompt's complexity? Or must the user always explicitly invoke these modes?\n- Regarding the \"pixel shift\" limitation: Beyond identifying the training data as the source, have the authors explored any mitigation strategies? For example, could a mask-preservation loss (e.g., L1 loss on non-edited regions) be applied during the image editing finetuning stage?\n- The modality-aware masking (Sec 3.1.2) is shown for single-turn planning/reflection. How robust is this mechanism for more complex, multi-turn interleaved tasks (e.g., generating a story with multiple images and text blocks)? What are the practical context length limits when history includes multiple VQ-tokenized images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g8SOKBSweP", "forum": "zV5FeNiWDO", "replyto": "zV5FeNiWDO", "signatures": ["ICLR.cc/2026/Conference/Submission14040/Reviewer_VkdP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14040/Reviewer_VkdP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659780322, "cdate": 1761659780322, "tmdate": 1762924529099, "mdate": 1762924529099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LaViDa-O, a unified masked diffusion model that brings image understanding, object grounding, text-to-image generation, image editing, planning and self-reflection into one architecture. It includes several techniques, like Elastic-MoT, modality-aware masking, universal text conditioning, and stratified sampling. Empirically, the planning and reflection produces noticeable gains on text-to-image and editing benchmarks, and the model also improves understanding relative to the LaViDa base."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and well organized.\n\n2. The method combines multiple ideas designed for different challenges (e.g., stratified sampling). There is a clear motivation before proposing each.\n\n3. Elastic-MoT is a sensible design for improving efficiency, and the appendix provides informative ablations exploring its effectiveness.\n\n4. The single model shows clear performance gains across multiple dimensions (e.g., generation and understanding), particularly when incorporating planning and reflection.\n\nOverall, the paper introduces several clever design choices, such as modality-aware masking and stratified sampling, each of which is well-motivated and justified. So, I believe this work will positively influence the development of masked diffusion models."}, "weaknesses": {"value": "1. Reporting results on some components of T2I-CompBench++ would strengthen the evaluation, especially since some of its focused prompts (e.g., for 3D and texture) are not currently covered by GenEval.\n\n2. The contribution of universal text conditioning is not quantitatively assessed. A user study or automated evaluation demonstrating its benefit would be helpful."}, "questions": {"value": "1. For editing images, it may be the case that other conditioning modalities (e.g., keypoints, masks) might be more beneficial than bounding boxes. Is there a way to extend the grounding mechanism to such modalities?\n\n2. The paper states that bounding boxes are denoised in a single step. Could the authors clarify how this is implemented (when to unmask)? Also, is the timestep necessarily before the denoising step of an [exp] token?\n\n3. During training, do image tokens follow a separate masking schedule until $t_{\\text{exp}}$? If so, would unmasking an [exp] token before or after $t_{\\text{exp}}$ introduce a train-inference mismatch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A6hoFoFJ62", "forum": "zV5FeNiWDO", "replyto": "zV5FeNiWDO", "signatures": ["ICLR.cc/2026/Conference/Submission14040/Reviewer_NaEt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14040/Reviewer_NaEt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918199923, "cdate": 1761918199923, "tmdate": 1762924528703, "mdate": 1762924528703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified masked model that is able to tackle multimodal understanding and generation. Specifically, several components, including Elastic Mixture-of-Transformers, planning, and iterative self-reflection, were proposed. Extensive experimental results on understanding and generation benchmarks demonstrated the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-organized and easy to read.  \n2. The proposed approach demonstrates promising results on several understanding and generation tasks, such as grounding and editing.  \n3. Comprehensive evaluations are conducted."}, "weaknesses": {"value": "1. The proposed masked diffusion language model is not relatively interesting, which was explored in UMDD[1].  \n2. The overall methodologies, including mixture-of-transformer, discrete language model, and discrete diffusion, are well-studied by the existing works [1,2,3,4,5]. I cannot find a significant contribution of this work.  \n3. There are several recent works with models of around 2B parameters, which should be fairly compared in the benchmarks.\n\n\n[1] Unified Multimodal Discrete Diffusion.\n[2] LMFusion: Adapting Pretrained Language Models for Multimodal Generation.\n[3] Emerging Properties in Unified Multimodal Pretraining.\n[4] MMaDA: Multimodal Large Diffusion Language Models.\n[5] Show-o: One Single Transformer to Unify Multimodal Understanding and Generation."}, "questions": {"value": "See weaknesses. I would adjust my initial rating according to the authors' response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WUykL3jwuZ", "forum": "zV5FeNiWDO", "replyto": "zV5FeNiWDO", "signatures": ["ICLR.cc/2026/Conference/Submission14040/Reviewer_GnXG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14040/Reviewer_GnXG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14040/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985442103, "cdate": 1761985442103, "tmdate": 1762924528341, "mdate": 1762924528341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}