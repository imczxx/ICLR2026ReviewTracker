{"id": "V727xqBYIW", "number": 13852, "cdate": 1758223734118, "mdate": 1763239057806, "content": {"title": "Process Reward Models That Think", "abstract": "Step-by-step verifiers--also known as process reward models (PRMs)--are a key ingredient for test-time scaling, but training them requires expensive step-level supervision. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers--using only 1% of the process labels in PRM800K--across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME ’24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation over subsets of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained with the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. This work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training.", "tldr": "We introduce a generative process reward model that verifies reasoning via long chain-of-thought and allows effective scaling of test-time compute with minimal supervision", "keywords": ["reasoning", "process reward models", "test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e645dded652099a0e6d856b832abb0578e5b8d6f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the data-intensive nature of training discriminative Process Reward Models (DiscPRMs), which typically require extensive step-level human annotations (e.g., PRM800K). The authors propose ThinkPRM, a generative (verbalized) PRM that verifies solutions by generating a verification Chain-of-Thought (CoT).\nThe core contribution is a data-efficient training methodology. ThinkPRM is fine-tuned on a small set of synthetic verification CoTs (1K examples, corresponding to ~8K step labels) generated by a strong teacher model (QwQ-32B-Preview). Crucially, these synthetic CoTs are filtered based on their agreement with existing gold process labels.\nThe paper demonstrates that ThinkPRM outperforms DiscPRMs trained on nearly 100x more data, surpasses LLM-as-a-Judge baselines, and shows promising out-of-domain generalization to GPQA and LiveCodeBench. Furthermore, the authors highlight that ThinkPRM can effectively scale verification compute at test time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The most compelling result is the ability to train a high-performing verifier using only 1% of the process labels required by discriminative counterparts (Figure 1).\n\n- The paper accurately identifies critical issues with using off-the-shelf LRMs for verification, such as prompt sensitivity, \"overthinking,\" and infinite looping (Figure 4, left). The proposed fine-tuning methodology effectively reduces these issues, leading to more reliable and structured outputs (Figure 4, right; Figure 5).\n\n- Despite being trained solely on math data, ThinkPRM shows strong OOD performance on GPQA (Physics) and LiveCodeBench (Figure 10). This suggests that the generative approach (\"verifying reasoning with reasoning\") may be more robust to domain shifts than DiscPRMs."}, "weaknesses": {"value": "- The claim of generalization to complex reasoning rests on weak evidence. The evaluation is limited to simplistic \"thinking\" traces from a 1.7B parameter model. Critically, the paper provides no empirical evidence demonstrating that the proposed ThinkPRM-14B model actually confers any benefit to the target 14B LRM. \n\n- The empirical evaluation is incomplete and fails to provide a comprehensive picture. The authors report results on only subsets of ProcessBench and benchmark solely against an LLM-as-Judge baseline. A thorough evaluation would require reporting performance across all four splits of the benchmark and including comparisons against other relevant baselines to properly contextualize the method's performance.\n\n- The paper lacks a quantitative analysis of the wall-clock latency introduced by the generative verification step at inference time."}, "questions": {"value": "- How do the authors ensure the soundness of the synthesized verification rationales? Since filtering only checks label agreement, what prevents the model from learning spurious or flawed reasoning that coincidentally matches the gold label? Conducting an audit of the rationale quality in the training data can be helpful.\n\n- Could you provide a quantitative analysis of the inference latency trade-offs? Specifically, what is the average wall-clock time increase when using ThinkPRM vs. DiscPRM during the guided beam search experiments shown in Figure 8?\n\n- For clarity and reproducibility, the authors should specify which ProcessBench split the data presented in Figure 1 (Left) and Figure 2 corresponds to."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4QTVoLTvw6", "forum": "V727xqBYIW", "replyto": "V727xqBYIW", "signatures": ["ICLR.cc/2026/Conference/Submission13852/Reviewer_7sgU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13852/Reviewer_7sgU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732108369, "cdate": 1761732108369, "tmdate": 1762924373809, "mdate": 1762924373809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "EjYCNqjqtv", "forum": "V727xqBYIW", "replyto": "V727xqBYIW", "signatures": ["ICLR.cc/2026/Conference/Submission13852/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13852/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763239057072, "cdate": 1763239057072, "tmdate": 1763239057072, "mdate": 1763239057072, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ThinkPRM, a generative Process Reward Model (PRM) designed for step-by-step reasoning verification. It aims to address the data inefficiency of discriminative PRMs by training on only 8K process labels (from PRM800K) via synthetic verification chains (generated by QwQ-32B-Preview and filtered to match gold step labels). ThinkPRM uses long Chain-of-Thought (CoT) for verification and claims to outperform LLM-as-a-judge and discriminative PRMs (trained on 712K labels) on math benchmarks (MATH-500, AIME ’24), ProcessBench, and out-of-domain tasks (GPQA-Physics, LiveCodeBench). However, the work lacks meaningful methodological innovation and fails to compare against key state-of-the-art PRMs, undermining its validity and contribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* THINKPRM demonstrates that generative PRMs can achieve strong performance with drastically fewer process labels (8K vs. 712K for discriminative PRMs), highlighting the potential of leveraging LLMs’ reasoning abilities to reduce annotation costs.\n* Unlike math-focused PRMs (e.g., GenPRM), THINKPRM shows modest generalization to non-math tasks (scientific QA, code generation) without domain-specific tuning, providing a initial signal of its adaptability."}, "weaknesses": {"value": "ThinkPRM suffers from a severe lack of methodological novelty, as its core components are either reproductions of existing generative PRM designs or incremental tweaks with no foundational breakthroughs. The paper frames “generative step-by-step verification” as a novel contribution, but this paradigm was already established by prior work: GenRM introduced outcome-focused generative verification via next-token prediction, and GenPRM extended this to process verification with long CoT and code execution. ThinkPRM differs only in minor implementation details—for example, filtering synthetic chains based on alignment with gold step labels (instead of GenRM’s outcome-based filtering) or using trigger phrases (“Let’s verify again”) for sequential scaling—rather than introducing new technical mechanisms (e.g., adaptive CoT length control, novel self-correction logic) that advance the generative PRM field. Even its emphasis on “long CoT” is not innovative: GenPRM already uses extended reasoning chains for verification, and ThinkPRM's longer token budgets (up to 8K tokens) represent a quantitative adjustment, not a qualitative shift in how verification is performed.\n\nCompounding this novelty gap, the paper fails to conduct comprehensive comparisons against key state-of-the-art PRMs, leaving critical questions about its competitiveness unanswered. Most notably, it omits comparisons to Skywork-PRM—a discriminative PRM trained on 500K math process labels that outperforms PRM800K-based models on MATH (68% vs. 62% accuracy) and LiveCodeBench (42% vs. 38% pass@1)—and ReasonFlux-PRM—a dynamic template-based PRM that incorporates both step-level and trajectory-level supervision to achieve 14% higher accuracy on AIME ’24 than Qwen2.5-Math-PRM-7B. By excluding these baselines, the paper cannot validate its claims of “data efficiency” or “scalable verification compute”: Skywork-PRM’s optimized discriminative design challenges whether ThinkPRM's label savings translate to real-world superiority, while ReasonFlux-PRM’s fine-grained reward alignment provides a competing approach to scaling verification that ThinkPRM never addresses.\n\nFinally, the paper provides insufficient justification for its core “long CoT” design choice. It asserts that longer verification chains improve accuracy but never compares ThinkPRM to short-CoT generative PRMs (e.g., GenRM with 512-token chains) to prove that longer chains are necessary—if a short-CoT model achieves similar accuracy with lower latency and cost, ThinkPRM's long-CoT focus becomes unjustified. Additionally, its sequential scaling experiment (Fig. 2) shows accuracy plateaus after 16K tokens, but the paper offers no analysis of why longer chains fail to improve performance (e.g., whether they introduce repetition, overthinking, or irrelevant reasoning), leaving the utility of its “long CoT” framing unsubstantiated. These gaps—novelty deficits, incomplete baseline comparisons, and unproven design choices—are fundamental and cannot be resolved with minor revisions, making the work unsuitable for acceptance."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nXdZqfsLTi", "forum": "V727xqBYIW", "replyto": "V727xqBYIW", "signatures": ["ICLR.cc/2026/Conference/Submission13852/Reviewer_VqJP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13852/Reviewer_VqJP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865672225, "cdate": 1761865672225, "tmdate": 1762924373458, "mdate": 1762924373458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method for generating a verification dataset for training PRMs based on scaling a step-wise verification process parallely and sequentially. This synthetic dataset, which comprises step-level judgment with rationales, is used to train these reasoning PRMs, enabling test-time scaling for step-wise verification. Empirically, this paper provides evidence to claim that: (i) in ProcessBench (a step-level verification benchmark), finetuning PRM on reasoning verification traces substantially improves over simple LLM-as-a-Judge verification; (ii) on test-time scaling evaluation, the resultant PRM (ThinkPRM) surpass discriminative PRMs and LLM-as-a-Judge in verifier-guided search, including in OOD datasets."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well structured, easy to follow.\n\n- The proposed method is conceptually simple and looks straightforward to reproduce. The generated dataset is relatively small (which makes PRM training cheap) yet it appears to be effective for the proposed task.\n\n- The experimental setup is very detailed, providing experiments both in process verification accuracy and downstream verifier-guided search.\n\n- The analyses on Section 5 raise relevant questions about compute vs performance trade-off, use of Monte-Carlo labels, long reasoning chains and harder problems, ablations in the method, etc. Overall, it provides a holistic empirical analysis around the proposed methodology."}, "weaknesses": {"value": "- The main concern is the lack of evidence to assess statistical significance in the results. The paper does not mention how many experimental seeds were used (I assume it is a single one), and no results in the paper brings confidence intervals. Prior literature has raised how sensitive math reasoning benchmarking is for small changes [1], requiring a more statistical grounding to evaluate whether the reported takeaways are meaningful or just observation noise.\n\n- In Figure 10, it is unclear if the curves indeed support the stated claims. Besides the lack of confidence intervals to understand the variability of the reported metrics, the curves for DiscPRM and ThinkPRM look very similar for most of the plot. It would be interesting to plot for a higher number of samples (perhaps up to 128 or 256 samples) to have a better perspective on where the curves saturate.\n\n- In Fig. 11 (left), regarding long reasoning chains (Section 5.2), again the lack of error bars raises questions about the reported gap being statistically significant or not, especially because the difference from the methods diminishes as the number of samples increase, reaching a gap that seems to be around 1% only. Majority voting, for instance, seems pretty much on par, despite not requiring fine-tuning, which contradicts the claim of ThinkPRM outperforming baselines.\n\n- In Fig. 11 (mid/right), regarding computed-matched performance (Section 5.3), it is also unclear if the figure supports a clear advantage of ThinkPRM over Majority Voting for higher compute. Besides (again) the lack of confidence intervals, the curves from Majority Voting do not extend over the full x-axis (without no apparent reason), which makes the claims in the section not well supported.\n\nOverall, the proposed method looks interesting given its simplicity and effectiveness, but it is unclear if the results indeed support the claims with statistical significance. My major concern is that the reported improvements are results of a good experimental seed. Therefore, it would be useful to report results across different seeds, showing the mean and confidence intervals to fully support the claims. I am willing to increase my score if this evidence is provided, since the paper structure, method, and raised research questions are interesting and relevant.\n\n\nReferences\n\n[1]  Hochlehnert et. al. A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility. COLM, 2025."}, "questions": {"value": "- Regarding the use of GPQA dataset: why does the paper concentrates result only in GPQA-Physics and not the full GPQA-Diamond as the standard practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fYEcOOfiBK", "forum": "V727xqBYIW", "replyto": "V727xqBYIW", "signatures": ["ICLR.cc/2026/Conference/Submission13852/Reviewer_AmJt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13852/Reviewer_AmJt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13852/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762274831796, "cdate": 1762274831796, "tmdate": 1762924373053, "mdate": 1762924373053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces THINKPRM, a generative process reward model (PRM) that verifies reasoning steps by generating explicit verification traces. Using around 1K synthetic verification chains filtered with PRM800K supervision, it achieves high data efficiency and competitive performance on ProcessBench, MATH-500, and AIME’24. The method also studies scaling verifier compute through parallel and sequential generation strategies, showing consistent improvements across tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is data-efficient, achieving strong verifier accuracy with a small number of labeled steps.\n2. The generative verifier improves Best-of-N and verifier-guided search over discriminative PRMs. Compute-scaling analysis is thorough, covering both parallel and iterative verification. Results on OOD datasets (GPQA, LiveCodeBench) show promising generalization."}, "weaknesses": {"value": "1. The paper lacks statistical rigor: results are based on single runs without confidence intervals, leaving uncertainty about significance. Evidence for the benefit of long Chain-of-Thought verification is limited; gains may stem from randomness or evaluation variance rather than a fundamental improvement.\n2. Comparisons to recent generative PRMs are missing, limiting the assessment of novelty. \n3. Score calibration and extraction remain simplistic; better normalization could improve reliability."}, "questions": {"value": "How sensitive are results to the quality and quantity of process-filtered data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JHPQDwN18j", "forum": "V727xqBYIW", "replyto": "V727xqBYIW", "signatures": ["ICLR.cc/2026/Conference/Submission13852/Reviewer_tqMX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13852/Reviewer_tqMX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13852/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763119883503, "cdate": 1763119883503, "tmdate": 1763119883503, "mdate": 1763119883503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}