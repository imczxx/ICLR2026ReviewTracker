{"id": "tynPzIbFu3", "number": 3798, "cdate": 1757526581426, "mdate": 1759898069339, "content": {"title": "Towards Biological Continual Learning with Spiking Hopfield Networks", "abstract": "Modern Hopfield networks are often viewed as biologically inspired associative memories, yet they lack the spiking dynamics and local learning rules that underpin real neural computation. In this work, we introduce a Spiking Hopfield Network (SHN) that incorporates discrete spike-based communication and a spike-timing–dependent plasticity (STDP) rule, enhancing biological plausibility while retaining the network’s capacity for online learning. To further support continual updates, we propose an Elastic Weight Consolidation (EWC)–inspired mechanism adapted to this local learning setting, reducing catastrophic forgetting. Together, these contributions yield a lightweight and biologically grounded framework that combines efficient memory retrieval with resilience to continual adaptation.", "tldr": "", "keywords": ["Spike-Time Dependent Plasticity", "SNN", "Associative Memory", "Modern Hopfield Network", "Catastrophic Forgetting", "Elastic Weight Consolidation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9a1de9f35a4446def67738f7f28f90df793c304a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a spiking neural network based variant of modern Hopfield networks, with a motivation to design a thoroughly biologically plausible memory model. This spiking Hopfield network (SHN) model is essentially enhanced with adaptive LIF neurons and local learning rules through spike-time-dependent plasticity (STDP). Additionally as a key contribution, based on a first-spike-wins retrieval rule from the SHN memory, the model also introduces a temporal threshold gating (TTG) memory protection mechanism with multiplicative threshold decays. This is presented as an adaptation of the elastic weight consolidation (EWC) method in the context of a Hopfield-STDP framework, to demonstrate how minimal catastrophic forgetting with local learning rules can be achieved."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has a clear biologically inspired motivation in its design of spiking Hopfield networks.\n- Proposed TTG update rule is a novel adaptation of EWC for STDP-based weight updates. Instead of EWC-like parameter penalization, locally gating the updates based on an adaptively decaying firing threshold is an interesting approach to mitigate catastrophic forgetting."}, "weaknesses": {"value": "- The paper essentially aims to adapt and combine several existing machine learning mechanisms in a single framework: adaptive LIF neurons, STDP, and EWC, within a modern Hopfield network. Therefore from a technical perspective, the main contributions seem to be regarding the design and adaptation choices rather than fundamentally novel methodologies (except TTG).\n- Clarity and reproducibility of the paper is a big limitation factor, and it almost appears like there are large gaps in the manuscript. There is no code or experimental details are not present, although there is a lot of page-space to explain the work more in detail."}, "questions": {"value": "- The design of the experiments are not clear. There is not much information in the paper, and it is hard to understand what Section 4.1 really implies. No details in the appendix either. This is particularly important from a reproducibility and a self-contained manuscript perspective. No details or no code is available for any of the experimental results.\n- More details on the architecture, hyperparameters, parameter initializations, etc. should be provided. For instance, how are the input spike train encodings facilitated?\n- There could be a bit more effort in unifying the notation of the paper across sections. In Sec 2.1, ALIF neuron model is described with $\\theta(t)$ indicating adaptive firing thresholds, whereas $\\theta$ is used in Sec 2.4 with EWC to denote parameters to be updated in Eq (6), and in Sec 3.4 and Algorithm 1 the adaptive threshold with multiplicative decay is now denoted with another variable $v_i$.\n- The manuscript could benefit from a better coverage of existing works in this area. There is no discussion on existing studies with SNN-based memory models. Some examples:\n\n[1] “Memory-dependent computation and learning in spiking neural networks through Hebbian plasticity”, 2023.\n\n[2] “STDP-based Associative Memory Model on Spiking Neural Networks”, 2024.\n\n[3] “Toward a Biologically Plausible SNN-Based Associative Memory with Context-Dependent Hebbian Connectivity.\" 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W7VlbRy1kq", "forum": "tynPzIbFu3", "replyto": "tynPzIbFu3", "signatures": ["ICLR.cc/2026/Conference/Submission3798/Reviewer_fh1P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3798/Reviewer_fh1P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761214169422, "cdate": 1761214169422, "tmdate": 1762917038202, "mdate": 1762917038202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They authors derive both learning and update rules for a spiking Hopfield network that allows ongoing learning without catastrophic forgetting, and is relatively insensitive to noise."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors address an important problem, at least from a neuroscience perspective: spiking Hopfield networks. The experimental results were very good, especially since they were learning in an online setting -- something that is difficult for associative memory networks.\n\nThe explanation of the individual parts was, by and large, understandable."}, "weaknesses": {"value": "I might be missing something (not unusual), but I could not figure out what they actually did. The problems start with Eq 1,\n\ntau_m du/dt = -u + RI.\n\nHowever, we were never told what the current, I was. Therefore, I could not figure out what the architecture was, or how the data, x, was presented to the network. Each of the subsections by themselves made sense; I just couldn't figure out how they fit together."}, "questions": {"value": "Please tell me what the architecture was. I can't guarantee it, but I'm almost positive I'll raise my score once I understand the architecture. Assuming I don't find something else wrong, which I doubt will happen.\n\nAlso, I wasn't clear on the training: how many times was each input presented? And was the MSE calculated on a training set?\n\nTypo, line 359 (I think)o :  Figure 5 should be  Figure 4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "erRoOKZkLQ", "forum": "tynPzIbFu3", "replyto": "tynPzIbFu3", "signatures": ["ICLR.cc/2026/Conference/Submission3798/Reviewer_9g8H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3798/Reviewer_9g8H"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761583586718, "cdate": 1761583586718, "tmdate": 1762917037979, "mdate": 1762917037979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment to Reviewers and Area Chair"}, "comment": {"value": "We thank the reviewers and the area chair for their time. Several comments indicate that key aspects of Modern Hopfield Networks (MHNs) and the intended scope of our Spiking Hopfield Network (SHN) may have been interpreted through the lens of feed-forward or autoencoder-style models. Notably, none of the critiques questioned the core novelty of our contribution. To avoid any ambiguity, we restate the central distinctions below:\n\t\n* An MHN is an associative memory system, not a function approximator. Information is stored in the synaptic weights and retrieved through attractor dynamics (Hopfield; Krotov et al.). While prior work such as Ramsauer and Hochreiter treates MHN as a geometric vector-space representation, our SHN reformulates the MHN as a spiking, hippocampal-inspired architecture where recall emerges from biological interaction dynamics.\t\n* Our custom STDP rule is the backbone of the model: it determines memory similarity, recall, and synaptic updates in a biologically consistent manner rather than through vector operations. It is not a mechanism for adjusting layer weights to map inputs to outputs, as in autoencoders.\t\n* The temporal threshold gating (TTG) mechanism provides local, EWC-like stability control that preserves stored memories. This component is essential and new for online associative learning in SNNs without global gradients.\t\t\n* Experiments follow standard protocols for online associative-memory Hopfield networks and demonstrate convergence and stability: our SHN reliably reaches valid attractor states as memory size increases.\n\nThese distinctions will be stated explicitly in the revised version."}}, "id": "OseKgLv5gS", "forum": "tynPzIbFu3", "replyto": "tynPzIbFu3", "signatures": ["ICLR.cc/2026/Conference/Submission3798/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3798/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3798/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763442417759, "cdate": 1763442417759, "tmdate": 1763442417759, "mdate": 1763442417759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Spiking Hopfield Network (SHN) that implements a biologically plausible version of  Modern Hopfield Networks by adding spiking dynamics and neural learning rules. To do so, they add the following to their network, which uses adaptive leaky integrate-and-fire neurons: (1) a simplified STDP-like learning rule, (2) a first-spike-wins retrieval mechanism for memory recall instead of softmax retrieval (3) temporal threshold gating for synaptic updates to prevent catastrophic forgetting. They compare SHN + first-spike-wins with SHN + Hopfield retrieval and show that the former does comparably on EMNIST, CIFAR-100, and MNIST+FashionMNIST tasks with sequential learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper uses a variety of datasets (EMNIST, CIFAR, MNIST/FMNIST) , going beyond random binary patterns.\n- Given that modern Hopfield networks are often connected to long term memory in brains, the problem of understanding what it takes to make them more biologically plausible is important to researchers studying memory in neuroscience."}, "weaknesses": {"value": "- The use of winner-take-all dynamics for storage, but then first-spike-wins for retrieval feels inconsistent. Also first-spike-wins seems unusual as a biological mechanism. It's unclear to me if there's biological evidence for it.\n- The only comparison model shown is SHN + first-spike-wins vs SHN + Hopfield retrieval. Feels like there should be a comparison to the standard, non-spiking MHN. Also, because the comparisons are only done between two versions of SHN and there's no visual examples shown of the retrieved memory, it's hard to interpret the MSE reported. \n- I would have liked more discussion about what neuroscience insights can be gained by the design of a spiking MHN. The lack of the discussion, combined with some of the arbitrary choices of design and the difficulty of interpreting the performance of the model (the two points mentioned above), makes it hard to evaluate the contribution made by the paper."}, "questions": {"value": "- How does the model perform compared to MHNs?\n- Can you discuss/justify more the choice of using two mechanisms for slot selection and retrieval (WTA and FSW, respectively)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J6sCPudnpn", "forum": "tynPzIbFu3", "replyto": "tynPzIbFu3", "signatures": ["ICLR.cc/2026/Conference/Submission3798/Reviewer_mcpL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3798/Reviewer_mcpL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3798/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947186788, "cdate": 1761947186788, "tmdate": 1762917037605, "mdate": 1762917037605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}