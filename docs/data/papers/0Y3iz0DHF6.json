{"id": "0Y3iz0DHF6", "number": 14660, "cdate": 1758241049256, "mdate": 1759897356705, "content": {"title": "ArtifactLinker: Linking Scientific Artifacts for Automatic SOTA Discovery", "abstract": "Scientific artifacts, such as models and benchmarks, are the foundation of machine learning research. With the rapid growth of repositories like HuggingFace, researchers now have access to millions of high-quality artifacts contributed by different researchers, yet the challenge remains: how can we automatically discover the state-of-the-art (SOTA) model for a given benchmark, fully leveraging existing scientific artifacts? We address this task, abbreviated as automatic SOTA discovery, by first modeling HuggingFace as an artifact graph, where nodes represent models or benchmarks and edges capture their relationships, labeled with evaluation results. Within this graph, we formulate the automatic SOTA discovery as the process of identifying new unobserved links with high potential performance that could advance future research. To enable scalable and efficient discovery of SOTA artifact links, we propose ArtifactLinker, a two-stage framework for automatic SOTA discovery: (1) prediction, which identifies promising links with Graph Neural Networks (GNNs) or graph-augmented LLMs, and (2) verification, which validates promising predicted links through reproducible and automatic coding experiments and agents. To evaluate ArtifactLinker, we further propose ArtifactBench, collecting 1,372 models and 308 benchmarks for systematically measuring prediction and verification performance and helping to develop new SOTA discovery agents. Our key results indicate that the graph-based prediction module in ArtifactLinker is effective in prediction. Moreover, an automatic verification pipeline in ArtifactLinker can verify that the identified promising links indeed achieve high performance on existing benchmarks in a fully automatic way.", "tldr": "", "keywords": ["scientific artifact", "automatic discovery", "graph neural network"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/452428454f8f32d4b5ae700c22f8297403aa0c66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper ARTIFACTLINKER frames Hugging Face as a bipartite artifact graph and defines “automatic SOTA discovery” as finding high-potential missing links, then validating them via an agent that executes model→dataset→metric pipelines. The authors release ARTIFACTBENCH, specify the SOTA objective, a δ-thresholded candidate filter, four prediction tasks, and one verification task. Results show simple GNNs and graph-augmented LLMs outperform metadata/random baselines.\n\nThe problem is crisp and the evaluation protocol is mostly sound. That said, several results suggest shortcutting and fragility in the system stack (see Weaknesses).\n\nThe paper is readable and structured, with a useful pipeline schematic (Figure 1) and compact result tables (Tables 1–4). However, verification details are scattered.\n\nThe novelty is largely at the system level (pipeline + benchmark), not in modeling/representation learning. Link prediction uses off-the-shelf GATv2 and prompt-wrapped TextGNN; verification is a ReAct-style agent with HF-specific checks. The dataset/benchmark contribution is helpful but incremental relative to existing artifact knowledge graphs (see, e.g., LinkedPapersWithCode)."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear problem setup: automatic SOTA discovery framed as link prediction on an artifact graph.\n2. Novel system idea: combining prediction and executable verification for reproducible discovery.  \n3. Empirical evaluation across four prediction tasks + verification task.    \n4. Public resource (ARTIFACTBENCH) could stimulate follow-up research."}, "weaknesses": {"value": "1. Limited methodological novelty: uses standard GNNs and ReAct agents; innovation is mostly system-level.  \n2. Strong shortcut bias: node-degree baseline F1=87.2, nearly matching GNN (88.4); no temporal or degree-controlled split.  \n3. Fragile verification: success rises on “hard” cases (1.3→14.3%) but drops on “easy” ones (56→43%).\n4. No scalability or compute analysis: verification throughput and cost per edge are unreported.  \n5. Only Hugging Face is used as test dataset; unclear generalization to other artifact ecosystems and knowledge graphs (e.g., LinkedPapersWithCode, ORKG, etc.)."}, "questions": {"value": "1. How sensitive are your discovery results to the δ threshold in Eq. (5)? Did you analyze how δ affects the precision, recall, and number of verified links?  \n2. Can you provide a detailed breakdown of verification failures by stage (e.g., dataset loading, model initialization, metric evaluation, runtime errors)?  \n3. Have you evaluated the models under temporal or degree-controlled splits to rule out popularity or structural shortcuts in the graph?  \n4. How do you ensure that the prediction models (GNNs and LLMs) are not simply exploiting structural shortcuts like node degree or community membership? Have you tested temporal or degree-controlled splits or examined feature importance to verify genuine generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vznVAusdzh", "forum": "0Y3iz0DHF6", "replyto": "0Y3iz0DHF6", "signatures": ["ICLR.cc/2026/Conference/Submission14660/Reviewer_97c7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14660/Reviewer_97c7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761385368562, "cdate": 1761385368562, "tmdate": 1762925030509, "mdate": 1762925030509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets the problem of efficiently discovering SOTA models for a given benchmark by leveraging existing resources. The authors construct a model-benchmark graph and propose a method to predict the performance of models on benchmarks based on this graph structure. They collect a new benchmark to evaluate their approach and provide insights for future developments in this area."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets at an interesting problem of efficiently discovering SOTA models for a benchmark by using the existing resources.\n2. The paper collected a new benchmark to examine its problem and built a systematic framework to validate the problem.\n3. The paper provides valuable insights for future developments in this area."}, "weaknesses": {"value": "1. The discussion on related works is not thorough enough, please check the references and provided necessary discussions. Essentially, the construction of model-benchmark graph and prediction over it have been studied in previous works, which weaken the novelty of the proposed work.\n2. The paper appears as an engineering prototype, to strengthen its contribution, it is better to reveal some unique patterns from the proposed setting, such as what sort of models are more likely to perform well on certain tasks, etc.\n\n[1] ADGym: Design Choices for Deep Anomaly Detection\n\n[2] Structuring Benchmark into Knowledge Graphs to Assist Large Language Models in Retrieving and Designing Models\n\n[3] Beimingwu: A Learnware Dock System"}, "questions": {"value": "1. Why should there be four evaluation tasks? Selecting SOTA method is more about ranking the existing models in the correct order, as the absolute performance may vary with benchmarks and hard to predict.\n2. What happens if a totally new benchmark is added, i.e., no prior information is available for this benchmark? How does the proposed method perform in this scenario? Since this is an important practical case of finding SOTA."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9zo6gKkxGf", "forum": "0Y3iz0DHF6", "replyto": "0Y3iz0DHF6", "signatures": ["ICLR.cc/2026/Conference/Submission14660/Reviewer_Cdz5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14660/Reviewer_Cdz5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761488995879, "cdate": 1761488995879, "tmdate": 1762925029901, "mdate": 1762925029901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "An interesting topic, automatic SOTA discovery, is proposed, which actually tries to filter and predict the performance of existing models on existing benchmarks. \n\nThe paper first converts HuggingFace data into a bipartite graph. It then uses a straightforward information aggregation mechanism to predict performance, testing two routines based on existing methods. An LLM finally verifies the predicted potential SOTAs and runs the codes to obtain real performance. The experiment removes SOTA edges and then recovers them."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, carefully wrapped up to emphasize its potential contribution.\n\nThe topic has an intriguing perspective, and the whole logic flow is self-contained."}, "weaknesses": {"value": "1. The technical contribution is limited. While some LLM/agent papers adopt a similar high-level approach, this work lacks topic-specific design—such as deeper analysis of benchmark characteristics, dataset distributions, or model architecture encoding. Section 4.2.1 is particularly basic, using only multi-turn aggregations on 1-hop neighbors. The graph construction from HuggingFace is straightforward, and the ReAct-based verification appears to be a simple add-on. Table 3 lists only base models from existing works, which fails to support the claim that \"the graph-based prediction module in ARTIFACTLINKER is effective in prediction.\"\n\n2. The entire work relies on HuggingFace data, described as containing \"millions of high-quality artifacts,\" yet produces a graph with only 1,372 models and 308 benchmarks. This small, single-source dataset is unconvincing. Discovery based on such limited data will be narrow and biased, regardless of method sophistication. \n\n3. Presentation and grammar issues:\n\na. Figure 1 shows edges between model-model and benchmark-benchmark nodes, contradicting the \"bipartite\" design (Line 151, page 3). These edges are not explained in the method section.\n\nb. \"we propose a novel two-stage framework: (1) prediction and (2) verification.\" → \"we propose a novel framework with two stages: (1) prediction and (2) verification.\"\n\nc. \"Prior work has largely relied on static analyses\" → \"Prior works have largely relied on static analyses\""}, "questions": {"value": "Q1. For novelty, I think I can hardly change my mind during the rebuttal phase. I will check the comments from other reviewers. The authors can focus on other issues.\n\nQ2. For the dataset, please explain the unmatched size mentioned above. Is it possible to extend the scope from Huggingface to other data sources (something similar to paperswithcode)? \n\nQ3. Please fix the grammar issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WvWjvGf2Bu", "forum": "0Y3iz0DHF6", "replyto": "0Y3iz0DHF6", "signatures": ["ICLR.cc/2026/Conference/Submission14660/Reviewer_n9MX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14660/Reviewer_n9MX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761711882388, "cdate": 1761711882388, "tmdate": 1762925028850, "mdate": 1762925028850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper models Hugging Face as a bipartite artifact graph between models and benchmarks, where edges carry evaluation scores.\nAuthors proposes a two-stage framework: prediction (GNNs or graph-augmented LLMs rank candidate links) and verification (an agent executes code to reproduce scores). They also build ArtifactBENCH to evaluate link/attribute prediction and reproduction."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is easy to follow. The problem formulation is clear. \n- Authors present a benchmark (ArtifactBench) from hugging face and a set of evaluation tasks based on the benchmark.\n- Authors conduct extensive experiments using heuristics, GNNs and LLMs+graph method."}, "weaknesses": {"value": "- The size of the dataset is relatively small and the graph is sparse. Many edges are concentrated around a few popular datasets (e.g., ImageNet, MMLU, as shown in Figure 4). This may limit the usefulness of the prediction tasks that are around the nodes with smaller degree.\n- For link prediction task, degree-based baselines already achieve high F1 (87.2 vs. 88.4 for the best method), suggesting the task may be too easy and may not present meaningful discovery.  \n- (Minor) ReAct-Linker appears to be a handcrafted pipeline running ReAct three times for different stages derived to solve the benchmark. Improvements are not consistent across easy and hard settings.  The contribution of this part feels small, though the authors also do not claim it as a major one.\n- (Minor) The formulation of experiments is to exclude edges in the graph and predict them as targets. This setup focuses on re-discovery, which differs from discovering new, unobserved edges in a real-world setting (e.g., applying a model to a dataset that actually achieves new SOTA). Moreover, real-world discovery can be biased, especially with modern LLMs that already achieve SOTA across many datasets. This bias may limit the benchmark’s practical relevance."}, "questions": {"value": "- Do you plan to scale the dataset and mitigate the concentration of edges on a few popular datasets?\n- Do you plan to evaluate the framework on real discovery (unseen edges)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sRDozallyH", "forum": "0Y3iz0DHF6", "replyto": "0Y3iz0DHF6", "signatures": ["ICLR.cc/2026/Conference/Submission14660/Reviewer_rnjC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14660/Reviewer_rnjC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971433636, "cdate": 1761971433636, "tmdate": 1762925028474, "mdate": 1762925028474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}