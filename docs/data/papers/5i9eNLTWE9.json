{"id": "5i9eNLTWE9", "number": 2995, "cdate": 1757313868767, "mdate": 1759898114725, "content": {"title": "Towards Adaptive Symmetry Breaking in Vector Neuron Networks", "abstract": "Vector Neuron Networks (VNNs) have been widely adopted in various 3D tasks due to their data efficiency and strong generalization capabilities rooted in equivariance. However, the rigid equivariance constraints of VNNs limit their ability to handle the prevalent problem of symmetry breaking in the 3D world, where models may need to produce outputs with reduced symmetry from inputs with high symmetry. In this paper, we propose an adaptive equivariance paradigm within the Vector Neuron (VN) framework, comprising three key designs: (1) Architecturally, we introduce a residual architecture that transforms the rigid equivariance constraints of VNNs into soft priors, preserving their symmetry-based inductive bias while enabling symmetry breaking. (2) Methodologically, we derive an implicit equivariance regularization method that allows VNNs to dynamically adjust their equivariance constraints according to the symmetry level of input data. (3) Structurally, we design a lightweight and interpretable module that allows VNNs to regulate equivariance in a simpler and more transparent manner. Experiments on 26 categories with varying input symmetries demonstrate that our approach achieves adaptive equivariance, improving the average performance of VNNs on pose estimation tasks by a factor of 5, and by a factor of 33 on highly symmetric inputs.", "tldr": "We propose the first adaptive SO(3)-equivariant model under the vector neuron framework.", "keywords": ["Vector neural network; SO(3)-equivariant; symmetry; symmetry breaking; pose estimation"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a10066f8aa4a1956ec4deee44db153c920a648e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Vector Neuron Networks (VNNs) are a popular form of rotation equivariant neural networks for point cloud processing. The submitted paper studies a symmetry breaking mechanism for VNNs that enables good performance on tasks where the input point cloud might be more symmetric than the expected output. In particular, the problem studied in the experiments is pose regression. When the input point cloud (say, a vase) is symmetric under some rotations then multiple output poses are equally valid. Using a standard VNN works poorly since the network is incapable of breaking the symmetry of the input to output a single pose.\n\nThe specific form of symmetry breaking proposed in this paper is a form of Residual Pathway Priors (RPP), where a non-equivariant layer is applied as a residual path in the network. Experiments show that this form of symmetry breaking works well for pose regression of symmetric objects.\n\nFurther, the paper presents an analysis of the loss function, claiming that it leads to “a subtle form of adaptive equivariance”."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The experimental results are strong, effectively demonstrating the improvement over ordinary VNNs in the pose regression task.\n2. The experimental results are also well presented, in particular Figure 4 is illustrating the differences over different categories nicely."}, "weaknesses": {"value": "It is unclear to me how exactly the method works, see the Questions below. Furthermore, the discussion in Section 4.2 on “Implicit Equivariance Regularisation” seems inaccurate to me. I will write my concerns here in the hope that they can be addressed.\n1. Equation (2) presents an upper bound on the objective function. As it is an upper bound, there is nothing that suggests that (2) would be minimized during training. \n2. Indeed, (2) holds equally well with $\\rho(g)f_\\theta(x)$ in both expectations replaced by $h(x,g)$ where $h$ is any function. This is because the proof of (2) only uses Lipschitz continuity of $D$ and triangle inequality of $d$, i.e. no properties of $G$ or $\\rho$.\n3. On line 246, $\\alpha = \\mu(G_s)$ is introduced, where $\\mu$ is the Haar measure of $G$ and $G_s$ is a subgroup of $G$ corresponding to the symmetries of some input $x$. We note that all proper subgroups of $SO(3)$ have Haar measure 0. $SO(3)$ is relevant to consider, since the experiments use that group.\n4. Thus, for $SO(3)$, equation (5) collapses to two cases, either full symmetry, $\\alpha=1$, or less than full symmetry and $\\alpha=0$. Additionally, (5) is again an upper bound on the objective, so it is not actually minimized during training.\n5. Because Section 4.2 studies upper bounds of the objective, it seems to me like conclusions such as “Therefore, the strength of the equivariance regularization of the $\\Psi$ is negatively correlated with the symmetry strength of the input, and the equivariance constraint of Ada-VNNs is adaptively relaxed.” are not correct.\n\nMinor weaknesses/typos:\n1. Both $G_s$ and $Stab_G(x)$ used for the subgroup that fixes $x$."}, "questions": {"value": "1. It is unclear to me why the proposed method solves the degeneracy better than ordinary VNNs. Regardless of whether $F$ is equivariant or not, if $g$ fixes $x$, then $F(\\rho(g)x)=F(x)$, so informally $F$ will have trouble choosing between regressing $g$ or any other $g’$ that fixes $x$ (since the loss is $d(F(\\rho(g)x), g)$). What is it that enables a non-equivariant architecture to handle this better?\n3. As stated on line 352, the model takes two point clouds as input and predicts their relative rotation. How is this handled in the VNN-framework? Is it equivariant to the simultaneous rotation of both point clouds, or is it equivariant under independent rotations of the point clouds?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TDxc8yMMIP", "forum": "5i9eNLTWE9", "replyto": "5i9eNLTWE9", "signatures": ["ICLR.cc/2026/Conference/Submission2995/Reviewer_wKHN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2995/Reviewer_wKHN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760976625739, "cdate": 1760976625739, "tmdate": 1762916485962, "mdate": 1762916485962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an adaptive framework that unifies equivariance and (different levels of) symmetry breaking into the Vector Neuron networks. The network is designed through a high-level idea of y(x) = equi_net(x) + residual(equi_net(x)). Based on this, the paper proposes some theoretical justifications and network designs. The adjustment to the network architecture is lightweight and easy to understand. Experiments show the network's different levels of equivariance on different object categories with different symmetries."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies an important problem in equivariant learning, and it presents an interesting and inspiring idea on equivariant network symmetry breaking, with theoretical analysis and explanations.\n- The paper proposes a compact network design that unifies different levels of equivariance/symmetry breaking. The adjustments are lightweight.\n- The experimental results are interesting and well-supportive of the different levels of symmetry breaking in the network, especially Figure 3 and 4."}, "weaknesses": {"value": "- It seems that to compute the objective with symmetry breaking (to my understanding, Eq. 5, or Eq.7, is the loss function to train the network), one needs to know what the symmetry group is ahead of time, in order to compute $\\alpha$. The (category-level) pose estimation task is a good justification for this setting, but I feel that for more general applications, knowing the symmetry group (for each object) ahead of time is less practical.\n- How could the symmetry-breaking objective be applied to discrete symmetries? The finite subgroups (for discrete symmetries) are of measure zero in SO(3)?\n- The writing of the paper can be made clearer, especially for some technical details that are crucial to the method. See Questions for more."}, "questions": {"value": "- Is Eq. 5 the ultimate loss function used to train the network?\n- Why is it called an \"upper bound\"? -- I think even if a loss cannot be minimized to zero, which is most common in deep learning, it can just be called the \"objective\" and no need to call it an \"upper bound\". When saying the \"upper bound\", I would wonder what is the actual loss function is to be optimized."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U3QuqOqIqy", "forum": "5i9eNLTWE9", "replyto": "5i9eNLTWE9", "signatures": ["ICLR.cc/2026/Conference/Submission2995/Reviewer_9Nai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2995/Reviewer_9Nai"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775170166, "cdate": 1761775170166, "tmdate": 1762916485817, "mdate": 1762916485817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper proposes a variant of the vector neuron network called Adaptive-VNN. It's core contribution is an architectural modification to relax the strict SO3 equivariance assumption when dealing with symmetry breaking. The authors motivate the work raising an issue that strict equivariance constraints prevent networks from inferring low-symmetry outputs like a precise pose from high-symmetry inputs like a can, and that the larger the object symmetry, the worse the performance. the suggested residual architecture maps equivariance constraints as priors, and the level of actual equivariance is dynamically tuned to the input. Experiments on 3D pose estimation show dramatic performance gains compared to standard VNN."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ the paper tackles a limitation of current fully equivariant networks. It analyses the limitation from a theoretical standpoint and also demonstrates this through experimental evidence. \n+ the suggested solution is simple conceptually, and immediate to implement. It shows convincing perfroamce gains"}, "weaknesses": {"value": "(-) The motivating example (coca cola can) isn't clear to me. Stepping back for a second, the SO3 equivariance isn't about the object itself being symmetric it's about having the predcitions rotate with the input. So the way I understand the problem is that under strict equivariance, given a symmetry transform t that keeps the input sample x in tact, we'd get f(x) = f(t*x) = tf(x), namely the features of a equivairant network become *invariant* to the symmetry. This could hurt tasks that require asymmetry tasks like telling left from right or regressign the exact relative pose. But here it seems the author emphasize a nuanced issue where that the network cannot identify symmetry breakin cues like the can opening direction, since the details are minor. I'm failing to see a principled explanation of why this would be the case -- is this a matter of capacity? I suggest the authors discuss this sensitivity of VNN \n\n(-) the method is shown with VNN and the authors do explain the convenience of having VN maintaining the dimensionality allowing for simple residual connections however seeing this with other architectures would highly push the generality of the idea. I would especially like to see it demonstrated with the frame averaging framework which to my understanding should also allow a residual ingredient pretty much out of the box. \n\n(-) For perfectly symmetric objects, the task of pose estimation also becomes ill posed as many solutions are equally viable for registration. Thus measuring angular error seems problematic -- this changes of course once the symmetry is violated (like in the coca cola can example) but I didn't see a discussion of this symmetry breaking in the experimental design / datasets. \n\nminor:  \n(-) im missing a description of the role of the decoder in the pose estimation task\n(-) 041: \"​​the underlying physics\" is very vague. would be better if tha autrhos spell out what they mean by that\n\nmissing references:  \nRobust Symmetry Detection via Riemannian Langevin Dynamics\nApproximately Piecewise E(3) Equivariant Point Networks\nFrame Averaging for Equivariant Shape Space Learning\nEquivariant Frames and the Impossibility of Continuous Canonicalization"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wp8OAqh9EQ", "forum": "5i9eNLTWE9", "replyto": "5i9eNLTWE9", "signatures": ["ICLR.cc/2026/Conference/Submission2995/Reviewer_BbhT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2995/Reviewer_BbhT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896264580, "cdate": 1761896264580, "tmdate": 1762916485695, "mdate": 1762916485695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to break input symmetry in the context of the Vector Neurons architecture based on residual path priors. A linear non-equivariant layer is added through a residual path to the output of the equivariant vector neuron layer. The paper explains the potential benefits of this approach through an adaptive symmetry breaking effect. Experiments are conducted on pose estimation and confirm this effect on one dataset."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles the interesting problem of breaking input symmetries in equivariant learning, following a recent line of work\n- The results of the experimental analysis provide interesting insights\n- The figures are clear and insightful"}, "weaknesses": {"value": "- I think the contributions of this paper are relatively incremental with respect to the existing works on relaxing equivariance works, especially Wang et al. 2022. The method proposed in this paper is essentially an adaptation of Wang et al 2022 to the Vector Neuron architecture.\n- Definition 4 is inconsistent with the accepted definition of equivariance. It says that a function is equivariant if the equivariance error is below C, but equivariance should be reserved for exactly equivariant functions. The authors should reproduce the definition of Fei and Deng 2024 and name this C-weakly equivariance\n- I found overall section 4.2 confusing:\n    - I don't understand the justification for the encoder-decoder setup in 4.2\n    - Equation 1 is simply training with data augmentation, that should be noted\n    - The loss in equation 2 is an upper bound on the equation 2 loss (can the calculation be provided in the Appendix?). Minimizing 2 is not equivalent to minimizing 1, and will in general lead to a suboptimal minimum with respect to the original loss. The analysis that follows equation 2 does not take properly account for that.\n    - If I understand correctly the analysis of equations 5 and 7 says that the strength of equivariance regularization is reduced when the input exhibits symmetry and that the model focuses on reducing prediction error. I don't think that's true. The bound says that the model will replace equivariance regularization with invariance regularization (which is just equivariant regularization with a different group action that preserves the symmetry) instead. So this does not solve the symmetry breaking problem.\n- At the start of section 4.2 it is said that \"establishing equivariance requires constraining all layers of the model\". That's not true in general, a network can be equivariant without every layer being equivariant\n- There's one thing I don't understand in the training process. For a symmetric shape, the pose is not determined by a unique rotation matrix. Yet training is done by regressing to a single rotation? Amongst the valid rotations, how will the network choose exactly the one used in the dataset?\n- I think there are important issues regarding the experimental claims\n    - I think the experiments are not representative of what the state of the art in pose estimation is. Saying that the method achieves \"near SOTA\" is not true at all. First, this is a toy task on a curated version of ModelNet. Second, the baselines considered are relatively simple and far from the state of the art.\n    - Are non-equivariant models trained with data augmentation? It seems like something has gone wrong in training these models, the performance should not be so poor. I consider these results suspicious."}, "questions": {"value": "- I think the decoupled vector linear layer is just a general linear layer, is that the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OHXclO1hTC", "forum": "5i9eNLTWE9", "replyto": "5i9eNLTWE9", "signatures": ["ICLR.cc/2026/Conference/Submission2995/Reviewer_kASe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2995/Reviewer_kASe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2995/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762130622323, "cdate": 1762130622323, "tmdate": 1762916485492, "mdate": 1762916485492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}