{"id": "E0Qfhma53J", "number": 14484, "cdate": 1758237060484, "mdate": 1759897367623, "content": {"title": "Scalable Chain of Thoughts via Elastic Reasoning", "abstract": "Large reasoning models (LRMs) have achieved remarkable progress on complex tasks by generating extended chains of thought (CoT). However, their uncontrolled output lengths pose significant challenges for real-world deployment, where inference-time budgets on tokens, latency, or compute are strictly constrained. We propose Elastic Reasoning, a novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases—thinking and solution—with independently allocated budgets. At test time, Elastic Reasoning prioritizes the completeness of solution segments, significantly improving reliability under tight resource constraints. To train models that are robust to truncated thinking, we introduce a lightweight budget-constrained rollout strategy, integrated into GRPO, which teaches the model to reason adaptively when the thinking process is cut short and generalizes effectively to unseen budget constraints without additional training. Empirical results on mathematical (AIME, MATH500) and programming (LiveCodeBench, Codeforces) benchmarks demonstrate that Elastic Reasoning performs robustly under strict budget constraints, while incurring significantly lower training cost than baseline methods. Remarkably, our approach also produces more concise and efficient reasoning even in unconstrained settings. Elastic Reasoning offers a principled and practical solution to the pressing challenge of controllable reasoning at scale. Code is available in the supplementary material.", "tldr": "A novel framework for scalable chain of thoughts that explicitly separates reasoning into two phases—thinking and solution—with independently allocated budgets.", "keywords": ["Test time scaling", "Large language models", "Length control Abstract:"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b57198797fc17deb30bd789c5f1d8b2a1d09efa7.pdf", "supplementary_material": "/attachment/426a26a09c88443feed843f6c9a4a45770feba8b.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Elastic Reasoning (ER), a framework that makes large reasoning models more efficient and controllable under limited inference budgets. By separating reasoning into thinking and solution phases with independent token budgets and training via budget-constrained reinforcement learning, ER achieves concise yet reliable reasoning—reducing token use by over 30% while maintaining or improving accuracy on math and coding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel framework for budget-aware reasoning – The proposed Elastic Reasoning introduces a clear separation between thinking and solution phases, enabling fine-grained control over inference cost without sacrificing performance.\n- Strong empirical efficiency and robustness – The method achieves reduction in token usage while maintaining or even improving accuracy on diverse math and coding benchmarks.\n- Excellent generalization under unseen budgets – Models trained with a single budget configuration generalize effectively to new inference constraints, demonstrating strong adaptability and practical scalability."}, "weaknesses": {"value": "- The method is only tested on strong reasoning models (DeepScaleR, DeepCoder); it’s unclear whether it generalizes to weaker models like Qwen2.5-Math, which lack explicit CoT structure or strong reasoning priors.\n- The paper shows that most improvement comes from the solution phase, while increasing the thinking budget (e.g., 2K–3K tokens) brings little additional gain. This suggests that the model may not truly improve its reasoning efficiency—instead, it might rely on memorized solutions rather than performing deeper thinking.\n- The evaluation lacks out-of-domain (OOD) reasoning benchmarks such as MMLU or GPQA. As all experiments focus on math and code, it remains unclear whether Elastic Reasoning generalizes to broader reasoning domains or tasks requiring factual and conceptual knowledge."}, "questions": {"value": "- Can the authors evaluate Elastic Reasoning on out-of-domain reasoning benchmarks (e.g., MMLU, GPQA) to verify whether the method generalizes beyond math and code tasks?\n- Can the authors test this by removing the thinking phase entirely (i.e., prompting the model to output only the solution) and reporting the resulting accuracy?\n- Could the authors explore asymmetric budgets (e.g., 1.75K for thinking and 0.25K for solution) to test whether the model still maintains performance with a shorter solution phase? Most solutions require far fewer tokens than reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6T9Z4mMaPV", "forum": "E0Qfhma53J", "replyto": "E0Qfhma53J", "signatures": ["ICLR.cc/2026/Conference/Submission14484/Reviewer_9D8U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14484/Reviewer_9D8U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761000393833, "cdate": 1761000393833, "tmdate": 1762924884813, "mdate": 1762924884813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Elastic Reasoning, a method that enables large reasoning models to achieve scalable and adaptive length control. To further improve solution quality under incomplete reasoning, we introduce a novel training strategy called budget-constrained rollout, which teaches the model to generate high quality answers even with partial CoT trajectories. This method is integrated into GRPO training. This method produces E1-Math-1.5B and E1-Code-14B."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Very simple method that solves the problem of truncated solution in long reasoning.\n- The thinking-solution ablation (4.4.1) is interesting and is good evidence to understand what the proposed training method improves (i.e., generating a solution under an incomplete thinking process)."}, "weaknesses": {"value": "## Major\n\n- Figure 1, 4, 5, 6 are a bit unclear (This may be a minor weakness, but I assigned this as a major weakness for now because it is a crucial experimental setup):\n    - What are the points? Do they correspond to the whole AIME questions across different budgets?\n    - What is the x-axis? Is it the average tokens used?\n    - Could you include the error bars (x- and y-axis)? This is particularly important as there are cases where the Pass@1 and tokens used are not significantly different\n- Lack of model variations\n    - The authors only experimented with one model variant per setup (one for math, and one for coding).\n    - I understand that such experiments can be costly. However, I cannot confidently argue for the generalizability of this finding.\n- Lack of further analyses\n    - I am curious about the qualitative difference between the thinking process before and after the training (e.g., do the models after fine-tuning commit less backtracking? less circular reasoning?)\n\n## Minor\n\n- Missing details of the GRPO training\n    - What is the reward design? It is unclear whether when the training “converges” around 0.5 reward score is a good thing or not.\n    - What about the other hyperparameters?\n    - Please generally be thorough in describing the experiment\n\n## Additional Suggestions\n\n- Typo Section 4.2 title"}, "questions": {"value": "- Just to confirm that I am understanding the novelty correctly: Two phases of thinking and solution seems to be exactly what reasoning models like DeepSeek is doing, right? Am I missing a certain novelty claim by the authors here? Is it simply that prior works were enforcing the overall tokens count, but now budget-constrained rollout limits the thinking tokens count separately from the solution token?\n- Are there qualitative difference between the reasoning patterns of naive truncation vs Elastic Reasoning?\n- Appendix B mentioned that the training is conducted for only 30 steps, but that does not seem to match Figure 3.\n- Any intuition why the performances of the trained models are still lower than the original model’s?\n- Have you tried varying the solution budget instead? or perhaps a control experiment with 0 thinking token and 1K solution tokens may be interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l9PSFHQg6q", "forum": "E0Qfhma53J", "replyto": "E0Qfhma53J", "signatures": ["ICLR.cc/2026/Conference/Submission14484/Reviewer_apFU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14484/Reviewer_apFU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607953842, "cdate": 1761607953842, "tmdate": 1762924884017, "mdate": 1762924884017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a method to control the reasoning sequence length of large language models. The key idea is to have a separate \"token budget\" for the c-o-t and for the answer, so that when the budget is exhausted for the c-o-t, the model can still produce an answer. Training with GRPO uses rollouts produced with that process, which leads the model to learn to deal with limited budget. The method outperforms alternatives that either limit the complete sequence or train the model to generate tokens that \"terminate\" its reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Token budget is a key issue for reasoning, the method is very simple and sensical, performance are great."}, "weaknesses": {"value": "Part 3.2.3 could probably be clarified, in particular the authors should provide a clearer description of the quantities involved and in particular the meaning of the conditioning in the policy, and the differences with a vanilla GRPO procedure."}, "questions": {"value": "- How is the model informed of the budget during inference? simply because </think> is generated? Hence the model has no information about the budget before it actually exhausts it? Fig 2 gives the impression that additional tokens specify it (red squares)?\n\n- This is not my direct domain of expertise, so are the baselines considered in the experimental part the best available?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FH13jZryPg", "forum": "E0Qfhma53J", "replyto": "E0Qfhma53J", "signatures": ["ICLR.cc/2026/Conference/Submission14484/Reviewer_8Dnp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14484/Reviewer_8Dnp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932241109, "cdate": 1761932241109, "tmdate": 1762924882695, "mdate": 1762924882695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}