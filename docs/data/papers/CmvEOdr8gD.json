{"id": "CmvEOdr8gD", "number": 9876, "cdate": 1758145495772, "mdate": 1759897690102, "content": {"title": "Advancing Drug-Target Interaction Prediction via Graph Transformers and Residual Protein Embeddings", "abstract": "Predicting drug-target interactions (DTIs) is important for the acceleration of drug discovery. Prevailing approaches often assume access to labeled target data or entangle training with opaque unsupervised alignment losses, which makes robustness hard to audit and failure modes difficult to diagnose. To address these gaps, we propose MoleProLink, a domain-shift-aware predictor of DTI for mining bioactive molecules which is based on the integration of methods inspired by measure-theoretic optimal transport, reproducing-kernel embeddings, and information-geometric perspectives. On the theory side, we present two compact risk-transfer control under the following two explicit assumptions: (i) Wasserstein-1 control under Lipschitz regularity assumption of the composed loss, and (ii) RKHS control with Maximum Mean Discrepancy (MMD). These statements are standard IPM-style bounds that are included here in a DTI-specific notation, we use them to motivate diagnostics and feature designing principles not to make any new forward inequalities. On the methodology side, we use a graph Transformer model for molecular graph with a sequence encoder for proteins. Protein embedding is performed with a residue based embedding (named as Residue2vec) and a bi-directional state space model, whereas molecular embedding is achieved through centrality and spatial encodings in a state space model Graph Transformer. Experimental results on three popular benchmarks (Human, C.elegans and Davis) show our method achieving strong AUC/AUPR, using a single protocol. Compared to the baselines, gains are achieved under the same data processing and negative-sampling; these margins are regarded not as inferential statements, but rather, as descriptive. We give implementation details that are sufficient for direct replication, and reproduce the ablative experiments that isolate the contributions of the protein sequence encoder and interaction decoder.", "tldr": "", "keywords": ["Graph Transformer", "Spectral Diagnostic"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96b6f145c2ec90d5e1b6be483ae5e6b42d0c7474.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MoleProLink, a domain-shift–aware framework for drug–target interaction (DTI) prediction. They combine a Graph Transformer for molecular graphs (with centrality + spatial encodings), a Residue2vec + bidirectional state-space encoder for proteins, and an attention-based interaction head.\nThe work reinterprets Wasserstein-1 and MMD-based risk-transfer bounds to guide model design. They evaluate several DTI-prediction models on 3 well-known datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well written, clearly presenting both the theoretical and empirical contributions.\n\n2. The appendix is well organized and effectively supports understanding of the training setup and feature design.\n\n3. The addressed problem remains highly relevant and far from fully solved.\n\n4. The model design is sound, integrating state-of-the-art components such as sequence encoders and graph-based feature extraction.\n\n5. The ablation study is clearly presented and elucidates the specific contribution of each module within the model."}, "weaknesses": {"value": "1. The proposed architecture offers limited novelty, and the observed improvements appear insuficient.\n\n2. The empirical impact of the theoretical components is neither quantified nor clearly demonstrated.\n\n3. The contribution of each model's component is insufficiently justified and seems minor.\n\n4. The rationale behind the choice of source and target domains is not clearly explained, despite being central to the study.\n\n5. The paper omits several strong DTI-specific graph-based approaches, such as GeNNiUs (https://doi.org/10.1093/bioinformatics/btad774) and EEG-DTI (https://doi.org/10.1093/bib/bbaa430). Including such methods would better position the proposed framework within current literature.\n\n6. The manuscript contains several typos (e.g., “insincery numerous,” “withoutadasterizing”) that hinder the readability of the paper. These should be carefully corrected."}, "questions": {"value": "1. Is the code and data preprocessing pipeline publicly available? I can't find it in the paper.\n\n2. How sensitive are the results to the chosen negative sampling ratio, and could potential AUPR inflation be quantified?\n\n3. Could you provide evidence of how the estimated W1 or MMD values correlate with model performance or calibration quality?\n\n4. Have you considered evaluating on larger, multi-domain datasets? Training across several domains while keeping one out-of-sample could strengthen the generalization analysis. DrugBank or BindingDB could serve this purpose.\n\n5. Why are the results not reported as mean ± standard deviation, given that multiple random seeds were used for the experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zgZGo2SrpO", "forum": "CmvEOdr8gD", "replyto": "CmvEOdr8gD", "signatures": ["ICLR.cc/2026/Conference/Submission9876/Reviewer_SujM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9876/Reviewer_SujM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761212076592, "cdate": 1761212076592, "tmdate": 1762921344933, "mdate": 1762921344933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoleProLink, a novel framework for drug target interaction (DTI) prediction designed to handle domain shift in drug discovery. The method combines a Graph Transformer for molecular graphs with a residue level protein encoder (Residue2vec plus a bidirectional state space model) to jointly represent molecules and proteins. Theoretically, it reformulates two standard risk transfer controls based on Wasserstein distance and Maximum Mean Discrepancy to serve as auditable diagnostics for assessing robustness under distributional shifts, rather than proposing new generalization bounds. Empirically, MoleProLink achieves strong AUC and AUPR performance on three benchmark datasets (Human, C. elegans, and Davis) under a unified evaluation protocol. The framework also provides interpretable attention maps and spectral diagnostics for analyzing cross domain behavior."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality：\nThis paper presents a new perspective by explicitly framing drug target interaction (DTI) prediction as a domain shift aware problem. Instead of introducing new unsupervised objectives, the authors reinterpret two classical risk transfer controls, Wasserstein 1 under Lipschitz regularity and RKHS based Maximum Mean Discrepancy, as auditable diagnostics for assessing model robustness. This theoretical reframing emphasizes interpretability and transparency rather than opaque alignment methods. In addition, the work introduces a hybrid architecture that reflects the inherent asymmetry between molecular graphs and protein sequences, combining a Graph Transformer with centrality and spatial encodings for molecules and a residue centric protein encoder (Residue2vec with a bidirectional state space model).\n\nQuality：\nThe study employs a unified data partitioning scheme that separates the source and target domains, using unlabeled target samples only for diagnostic evaluation. The authors report AUC and AUPR scores on three widely used benchmarks under a consistent experimental protocol and explicitly describe improvements as descriptive rather than inferential. Detailed ablation studies isolate the roles of the sequence encoder and the interaction head, supporting the theoretical motivations proposed. The implementation details are comprehensive, including framework versions, hyperparameters, hardware setup, and auditing proxies such as gradient norm monitoring, all of which enhance the reproducibility and credibility of the work.\n\nClarity：\nThe paper is written with conceptual precision. It clearly distinguishes between theoretical guidance and empirical novelty, presenting assumptions and notation in an accessible manner. The explanations of architectural choices, such as the use of centrality and spatial encodings, residue level modeling, and lightweight cross modal attention, are intuitive and well justified. Interpretability tools, including atom residue attention maps and spectral diagnostics, are clearly described, making it easy to understand both model behavior and diagnostic procedures.\n\nSignificance：\nBy providing a framework capable of auditable diagnostics without requiring labeled target data, this work contributes to the development of reliable and interpretable models for drug discovery. The strong performance achieved on the Human, C. elegans, and Davis datasets under a unified protocol highlights its practical value. Furthermore, the framework’s interpretability and diagnostic insights can inform future research on regularization strategies and cold start problems, marking it as an important step toward trustworthy and generalizable AI applications in pharmaceutical research."}, "weaknesses": {"value": "1.\tTheory practice linkage is underpowered:\nThe paper positions Wasserstein and MMD controls as diagnostics but never demonstrates that acting on these diagnostics changes outcomes. Unlabeled target samples are used only for post hoc checks without affecting the supervised objective, which limits practical impact. To strengthen this aspect, the authors could integrate the proposed shift proxies into training as regularizers or early stopping criteria, and conduct paired experiments “with vs. without” these controls to show measurable gains under distributional shift. The current setup explicitly states that diagnostics do not modify training, which supports this concern.\n2.\tClaims rely on descriptive margins without statistical support:\nThe reported improvements are descriptive rather than inferential, and the robustness observations are qualitative, with no additional tables quantifying variance or statistical significance. To address this, the authors should add confidence intervals, paired permutation tests, or stratified bootstrap confidence intervals for AUC and AUPR, and report seed dispersion (mean ± standard deviation across at least five seeds) to substantiate robustness claims. Including calibration metrics such as Expected Calibration Error (ECE) and Brier score with uncertainty estimates would also strengthen claims of “auditable robustness.”\n3.\tEvaluation breadth and split rigor could be stronger:\nResults focus on Human, C. elegans, and Davis datasets under a single protocol, but the paper itself points to future work involving cold start and regularization. The evaluation would be more convincing if it included stricter partitions, such as scaffold and chemical series cold start splits on the ligand side, and family or fold hold out splits on the protein side. Incorporating time aware splits where possible, and reporting performance across shift bins defined by the estimated Wasserstein and MMD proxies, would help demonstrate monotonic robustness. The authors themselves identify cold start evaluation as future work, but it could already be advanced within the current study.\n4.\tBaselines and ablations miss important modern controls:\nWhile the ablations successfully isolate the sequence encoder and interaction head, they do not include comparisons against strong pretrained protein or molecular encoders, such as large protein language models or modern graph and 3D molecular models, configured with lightweight heads under the same experimental settings. To improve rigor, the authors could add baselines that replace Residue2vec with pretrained protein models and compare Graph Transformer encoders against recent 3D aware or foundation style molecular encoders, while controlling total parameters and training budgets. Additionally, the ablation study could test centrality and spatial encodings individually, and replace the state space module with simpler bidirectional Transformers to verify that the state space design is essential rather than incidental.\n5.\tDiagnostics stop at interpretation rather than operational guidance:\nSpectral and attention based diagnostics are informative but remain qualitative. These could be transformed into operational tools by defining thresholds on spectral alignment or attention dispersion that trigger human review, down weighting, or auto calibration. Including prospective filtering curves showing how these diagnostics reduce false positives at fixed recall on a held out target domain would make the diagnostics actionable. Currently, the paper limits these analyses to qualitative visualization, so turning them into quantitative decision rules would increase their practical utility.\n6.\tReporting of efficiency and deployability is thin:\nAlthough implementation details and hardware specifications are provided, there is no measurement of throughput, latency, memory footprint, or training and inference cost relative to baselines under identical batch sizes and input lengths. To substantiate claims of efficiency, the authors should add wall clock metrics, FLOPs or tokens/atoms processed per second, and memory profiles for key model variants. A Pareto plot of AUPR versus inference cost would further support the claim that the architecture is both lean and practical.\n7.\tExternal and prospective validation are limited:\nThe paper mentions data curation beyond the three main benchmarks, but the core results do not include an external prospective screen or a distinct held out collection that reflects real world domain shift. The authors could add a prospective style evaluation or an external dataset such as a GPCR or cross species benchmark with experimentally validated pairs to test generalization and calibration drift under realistic conditions. Including reliability diagrams before and after regularization would also strengthen claims of robustness and calibration. Currently, the main results remain confined to three datasets under a single protocol."}, "questions": {"value": "1.\tHow should practitioners act on your shift diagnostics in training or selection?\nYou position W1 and MMD as diagnostics rather than losses, and unlabeled target data are used only for checks, not to alter the supervised objective. Please specify concrete procedures (for example, early stopping, penalty terms, or model selection rules) that make use of these proxies, and provide ablation studies showing the causal impact of acting on the diagnostics versus ignoring them. A short pseudo-code example for a training loop that incorporates W1 or MMD would help, along with an analysis of sensitivity to proxy estimation noise.\n2.\tWhat exactly is the DTI aware metric dDTI and how robust are results to its design?\nYou note that dDTI may incorporate molecular and protein similarities but do not detail the specific metric used in the main experiments or its alternatives. Please describe the exact construction, justify the weighting across modalities, and report a robustness analysis over metric variants (for example, topological versus stereochemical emphasis or different protein similarity measures) and their effects on W1 estimates and model performance.\n3.\tCan you provide statistical uncertainty for reported AUC, AUPR, and calibration?\nThe results are presented as descriptive margins. For the rebuttal, please include mean ± standard deviation over multiple seeds, confidence intervals (such as stratified bootstrap) for AUC and AUPR, and calibration metrics (ECE and Brier score) with uncertainty for each dataset.\n4.\tHow does MoleProLink perform under stricter and more realistic data splits?\nYou mention future evaluation under cold start conditions. Please add at least one ligand scaffold or series cold start and one protein family or fold hold out split, and, if possible, a time aware split. Also, stratify results by measured shift magnitude (for example, bins of estimated W1 or MMD) to demonstrate whether performance degrades or remains stable as the shift increases.\n5.\tWhat is the comparative value of your encoders versus modern pretrained backbones?\nThe ablations disable modules but do not compare against strong pretrained protein language models or recent three dimensional molecular encoders used with lightweight heads under the same split. Please include swap-in baselines that replace Residue2vec with a pretrained protein model and test a recent three dimensional or foundation style molecular encoder, controlling for model size and training budget.\n6.\tWhy did you choose a state space model for proteins, and is it essential?\nPlease justify this choice over bidirectional Transformers or CNN Transformer hybrids by adding ablations that replace the bidirectional state space module with these alternatives, matched for capacity. Report not only AUC and AUPR but also inference latency and memory usage to assess efficiency.\n7.\tCan you quantify the interpretability claims beyond qualitative examples?\nYou mention attention maps and spectral diagnostics. For the rebuttal, please include quantitative evaluations such as attention mass on known binding site residues, correlation with pocket annotations, and a decision rule based on spectral alignment that triggers human review or score adjustment. Present utility curves (for example, false positives reduced at fixed recall) to demonstrate practical value.\n8.\tHow efficient is the model in practice?\nYou list hardware and configurations but do not report throughput or latency metrics. Please include tokens or atoms processed per second, FLOPs, wall clock time per epoch, peak memory usage, and a Pareto comparison of AUPR versus inference cost to baselines under matched batch sizes and input lengths.\n9.\tHow sensitive are the results to negative sampling choices and class imbalance?\nGiven the known sensitivity of AUPR to negative sampling, please provide an analysis across different sampling ratios and strategies, using fixed seeds and confidence intervals. Include prevalence adjusted precision recall curves to demonstrate stability under varying class balance.\n10.\tCan you supply reproducibility artifacts for exact splits and diagnostics?\nYou emphasize reproducibility. For the rebuttal, please share split manifests, random seeds, and code snippets for computing plug-in MMD and W1 proxies, including kernel bandwidth selection and any transport approximations, so that others can replicate the shift estimation precisely.\n11.\tWhat are the limits of the theoretical framework in your current setting?\nYou state that the bounds are used only for guidance and do not claim new inequalities or equivalences between metrics. Please clarify situations where Lipschitz proxies or RKHS witnesses may give misleading results for DTI, and provide counterexamples or stress tests where the diagnostics diverge from generalization performance.\n12.\tCould you present a small prospective external validation?\nYou mention GPCR curation and broader datasets but focus your main results on three benchmarks. Adding a small, externally curated, and temporally separate dataset with experimental validation would strengthen claims about robustness to domain shift and calibration drift."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cw6YhKpguO", "forum": "CmvEOdr8gD", "replyto": "CmvEOdr8gD", "signatures": ["ICLR.cc/2026/Conference/Submission9876/Reviewer_vik4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9876/Reviewer_vik4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893746330, "cdate": 1761893746330, "tmdate": 1762921344600, "mdate": 1762921344600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a DTI model: a Graph Transformer for molecules plus a residue-centric protein encoder, joined with cross-attention. It also uses standard W1/MMD shift measures as diagnostics. The theory is explicitly not new, the goal is a practical, auditable pipeline."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Coherent architecture for DTI prediciton in aligment with current state of the art. Graph Transformer for molecules + residue-centric protein encoder with cross-attention.\n- Honest positioning of W1/MMD as diagnostics (not new theory).\n- Useful emphasis on auditability (attention maps, shift/shift-proxy checks)."}, "weaknesses": {"value": "- Limited novelty. Shift diagnostics are not integrated into training.\n- Baselines include unrelated works, missing strong, tuned DTI baselines.\n- Dataset descriptions contain errors, splits are unclear and likely leaky (no scaffold or protein splits).\n- Sparse quantitative results (few tables, no stats, no calibration metrics).\n- Key components under-specified (e.g., “Residue2vec” details) and GLASS usage unclear.\n-"}, "questions": {"value": "- What exactly is “Residue2vec” (k, corpus, objective, vocab), and is it fixed or fine-tuned?\n- How is the protein–ligand distance/metric defined and weighted? Sensitivity to this choice?\n- What are the exact split counts, class balance, dedup rules, and negative-sampling policy?\n- How does the model perform under scaffold, cold-protein/family, and (ideally) time-based splits?\n- Do W1/MMD diagnostics quantitatively track generalization/calibration across seeds? Any plan to integrate them into training?\n- Is the GLASS set actually evaluated, and if so, where are the results?\n- What sanity checks validate that attention maps localize meaningful atoms/residues?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DTPckR8yab", "forum": "CmvEOdr8gD", "replyto": "CmvEOdr8gD", "signatures": ["ICLR.cc/2026/Conference/Submission9876/Reviewer_qqAH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9876/Reviewer_qqAH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9876/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899724128, "cdate": 1761899724128, "tmdate": 1762921344328, "mdate": 1762921344328, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}