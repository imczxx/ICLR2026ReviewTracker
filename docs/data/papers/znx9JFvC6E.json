{"id": "znx9JFvC6E", "number": 15808, "cdate": 1758255561661, "mdate": 1759897280670, "content": {"title": "Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models", "abstract": "The security issue of large language models (LLMs) has gained wide attention recently, with various defense mechanisms developed to prevent harmful output, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the output distribution of text embedding models is severely biased with a large mean. Inspired by this observation, we propose novel, efficient methods to search for **universal magic words** that attack text embedding models. Universal magic words as suffixes can shift the embedding of any text towards the bias direction, thus manipulating the similarity of any text pair and misleading safeguards. Attackers can jailbreak the safeguards by appending magic words to user prompts and requiring LLMs to end answers with magic words. Experiments show that magic word attacks significantly degrade safeguard performance on JailbreakBench, cause real-world chatbots to produce harmful outputs in full-pipeline attacks, and generalize across input/output texts, models, and languages. To eradicate this security risk, we also propose defense methods against such attacks, which can correct the bias of text embeddings and improve downstream performance in a train-free manner.", "tldr": "We propose novel algorithms to find universal magic words that jailbreak LLMs' safeguard by leveraging the uneven distribution of text embeddings.", "keywords": ["jailbreak", "large language model", "magic word", "safeguard", "text embedding", "defense"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f0db7bda84c22c7a1e0bcab16067a75529d5acf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates a new vulnerability in LLM that rely on text embedding models. The authors identify a strong bias in the embedding distribution and leverage it to design efficient algorithms for finding universal magic words, which are an adversarial suffixes that can manipulate embedding similarity. In this way, these magic words can bypass text-embedding-based safeguards by altering perceived similarity between harmful and benign text pairs. The paper proposes three methods (brute-force, context-free, and gradient-based), evaluates their efficiency and transferability across models and languages. Moreover, the authors suggest renormalization-based defenses that improve robustness without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies vulnerability from an interesting angle by finding the universal magic words.\n2. The paper shows strong attack and defense results for both the attack and the defense strategy.\n3. The paper is well motivated and well written."}, "weaknesses": {"value": "1. There lack of analysis on the possible number of magic words existing in a model.\n2. The influence of repetition count, token length, or embedding normalization choices is not systematically analyzed.\n3. There lack of analysis on the randomness in learning magic words across different random seeds, etc.\n4. There lack of discussion on the origin/root/insights of the identified magic words."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JYGylQymIl", "forum": "znx9JFvC6E", "replyto": "znx9JFvC6E", "signatures": ["ICLR.cc/2026/Conference/Submission15808/Reviewer_rKMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15808/Reviewer_rKMg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885370945, "cdate": 1761885370945, "tmdate": 1762926038402, "mdate": 1762926038402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that many text-embedding models have a strong mean-bias in their vector space and that short, universal “magic-word” suffixes can push any input toward this bias direction, manipulating cosine similarities that underpin embedding-based safeguards. Building on this observation, the authors present efficient search procedures—including a context-free black-box method aligned to the bias and a one-step white-box gradient approach—to find transferable suffixes. They demonstrate end-to-end jailbreaks by appending these words to user prompts and by requiring the model to end its responses with the same words, thereby bypassing both input and output guards. Experiments across multiple embedding backends and safety detectors report large drops in detection performance and cross-model transfer. To mitigate, the paper proposes a simple, train-free fix—mean-centering plus renormalization of embeddings—which substantially restores guard performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* this paper proposes a bias-direction analysis for text-embedding models, which is new\n* this paper offers a simple, train-free mitigation for the proposed attack"}, "weaknesses": {"value": "* the usage of \"magic\" suffix has been proposed in other works (like GCG), making the contribution of this paper a bit incremental\n* using renormalization for defense is promising but its impact on diverse downstream retrieval/semantic tasks (beyond the reported classifiers) remains underexplored\n* lacks head-to-head experimental comparison with other whitebox attacks\n* some inherited limitations of whitebox attacks"}, "questions": {"value": "* Could you please explain the novelty of the propose methodology comparing to other similar attacks? e.g.: https://arxiv.org/abs/2307.15043, https://people.eecs.berkeley.edu/~daw/papers/iris-naacl25.pdf\n* Some papers question the transferability/generalizability of these universal adversarial triggers (e.g.: https://arxiv.org/abs/2404.16020v1). Under what conditions do your universal suffixes fail, and how does that compare to the existing analyses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z4HaiF4JWJ", "forum": "znx9JFvC6E", "replyto": "znx9JFvC6E", "signatures": ["ICLR.cc/2026/Conference/Submission15808/Reviewer_vDP5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15808/Reviewer_vDP5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908419909, "cdate": 1761908419909, "tmdate": 1762926038027, "mdate": 1762926038027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel attack vector against LLM safeguards that are based on text embedding models. Appending magical words can fool the safeguard, i.e., classifiers trained on text embedding to distinguish harmful and harmless prompts. The core of the work is a key observation: the output distribution of several popular text embedding models is highly anisotropic, concentrating in a specific \"band\" on the unit hypersphere. The authors formalize this bias by identifying a \"bias direction\" (e*), which is the normalized mean of a large corpus of text embeddings. The paper then proposes defense methods against such attacks by fixing the defect of uneven embedding distribution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The discovery and empirical validation of the non-uniform, biased distribution of text embeddings (Fig. 1) is a significant and insightful contribution. It provides a principled and elegant explanation for the existence of universal adversarial attacks, moving beyond simple heuristics. This observation itself is of high value to the representation learning. The paper is well-written and the finding on embedding is interesting. The authors also propose different methods driven by their finding."}, "weaknesses": {"value": "* Does correcting the bias harm the embedding model's performance on its primary tasks (e.g., semantic search, classification)? An empirical evaluation is necessary. The setting on bypassing safeguard may also not be so useful for real-world applicability.\n* The final step of Alg. 3 involves a Cartesian product of candidate tokens, which can lead to a combinatorial explosion. The practical limits on the magic word length and candidate size should be discussed.\n* The defense method has not been tested for adaptive attacks, such as whether the method can defend against changes in jailbreakers.\n* In Alg. 3, the comment \"empirically better than zeros(h,m)\" for random initialization is an interesting detail. A brief sentence of intuition would be helpful for the reader."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IFuvc8rz6u", "forum": "znx9JFvC6E", "replyto": "znx9JFvC6E", "signatures": ["ICLR.cc/2026/Conference/Submission15808/Reviewer_LP3e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15808/Reviewer_LP3e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15808/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984160748, "cdate": 1761984160748, "tmdate": 1762926037445, "mdate": 1762926037445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}