{"id": "hSpA4DAoMk", "number": 24978, "cdate": 1758362726444, "mdate": 1759896739625, "content": {"title": "Adaptive Methods Are Preferable in High Privacy Settings: An SDE Perspective", "abstract": "Differential Privacy (DP) is becoming central to large-scale training as privacy regulations tighten. We revisit how DP noise interacts with *adaptivity* in optimization through the lens of *stochastic differential equations*, providing the first SDE-based analysis of private optimizers. Focusing on DP-SGD and DP-SignSGD under per-example clipping, we show a sharp contrast under fixed hyperparameters: DP-SGD converges at a privacy-utility trade-off $O(1/\\varepsilon^2)$ with speed independent of $\\varepsilon$, while DP-SignSGD converges at a speed *linear* in $\\varepsilon$ with a $O(1/\\varepsilon)$ trade-off, dominating in high-privacy or high-noise regimes. Under optimal learning rates, both methods reach comparable theoretical asymptotic performance; however, the optimal learning rate of DP-SGD scales linearly with $\\varepsilon$, while that of DP-SignSGD is essentially $\\varepsilon$-independent. This makes adaptive methods far more practical, as their hyperparameters transfer across privacy levels with little or no re-tuning. Empirical results confirm our theory across training and test metrics, and extend from DP-SignSGD to DP-Adam.", "tldr": "With SDEs, we show that while DP-SignSGD is better under tight privacy or noisy batches, DP-SGD is better otherwise, and adaptivity needs far less hyperparameter tuning across privacy levels.", "keywords": ["Stochastic Differential Equations", "Differential Privacy"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/401371433d65ebc4f06fe84432b73962d4e25d36.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The main methodological contribution is to approximate the discrete optimization algorithm with a stochastic differential equation (SDE). It focuses on DP-SGD and DP-SignSGD under per-example clipping and provides the first theoretical framework that connects SDE with convergence dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents the first SDE-based theoretical framework for analyzing differentially private optimizers, bridging discrete training dynamics with continuous-time analysis. It provides clear scaling laws for DP-SGD $O(1/\\epsilon^2)$ and DP-SignSGD $O(1/\\epsilon)$, offering strong insight into how adaptivity interacts with privacy noise. Theoretical predictions are thoroughly validated by experiments across multiple datasets. The work also gives practical guidance for tuning and transferring DP optimizers under different privacy budgets."}, "weaknesses": {"value": "While the SDE framework is well motivated, the paper does not clearly justify why the continuous approximation is valid or which small terms are neglected. A short discussion explaining that the discretization error between the algorithm and its SDE counterpart can be ignored would make this assumption more convincing. Additionally, although the authors argue that higher-order SDE approximations are unnecessary, a simple analysis or experiment illustrating how the first-order approximation deviates from the discrete dynamics would strengthen the theoretical rigor. Clarifying these points would improve both the transparency and completeness of the analysis."}, "questions": {"value": "1.The paper could provide more heuristic insight into why DP-SignSGD, which can be seen as a post-processing of DP-SGD and thus should not improve efficiency, nevertheless achieves better empirical and theoretical utility. In particular, what property of the sign operation makes it more robust to DP noise?\n\n2.Additionally, the role of the clipping bound C deserves clarification: while a larger C should intuitively reduce gradient bias, the theory suggests that a larger C moves the iterates further from the optimum.\n\n3.The two-phase assumption (clipped vs. unclipped regime) is central to the analysis but not well justified; a short discussion of why it is valid to analyze the optimization dynamics separately in the clipped and unclipped phases would improve the theoretical clarity of the paper -- it should noted that in a single batch of samples, some gradient would be clipped and some would not but we will only have one update at the end. The analysis in the main theorem is confusing.\n\n4.The paper also lacks comparison with SOTA DP-SGD benchmark, e.g., \"unlocking high-accuracy differentially private image classification through scale\", \"a theory to instruct differentially-private learning via clipping bias reduction\", and \"differentially private image classification\nby learning priors from random processes\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kWeIe2vi63", "forum": "hSpA4DAoMk", "replyto": "hSpA4DAoMk", "signatures": ["ICLR.cc/2026/Conference/Submission24978/Reviewer_g3be"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24978/Reviewer_g3be"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894973117, "cdate": 1761894973117, "tmdate": 1762943271483, "mdate": 1762943271483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies DP stochastic gradient descent and its variants via modelling the DP-SGD dynamics via stochastic differential equations. It develops continuous-time approximations for adaptive algorithms and distinguishes between two training “phases”: an initial exploration phase and a later convergence phase. The analysis aims to explain how noise injection from differential privacy interacts with the optimization dynamics, and how the privacy parameters and optimizers' hyperparameters (clipping bound, batch size and learning rate) influence convergence. The authors attempt to unify previous heuristics (DP-SGD, DP-SignSGD etc.) under a common SDE framework, claiming to provide an interpretable connection between privacy, optimization noise, and generalization. The paper also reports empirical results to illustrate the phase-transition behavior. There is a very interesting phenomenon that is supported both by theory and experiments: DP-SGD has $\\varepsilon^{-2}$ behaviour for small $\\varepsilon$-values whereas DP-Adam and DP-SignSGD have $\\varepsilon^{-1}$ behaviour."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written, seems to be of very high quality.\n\n- The fact that the SDE view is able to capture the experimental behaviour that DP-SGD has $\\varepsilon^{-2}$ behaviour for small $\\varepsilon$-values whereas DP-Adam and DP-SignSGD have $\\varepsilon^{-1}$ behaviour (see Figure 1) is very impressive. \n\n- The SDE view is well motivated and also commonly considered in the literature (e.g., Blei et al. 2018)."}, "weaknesses": {"value": "- The paper focuses on only on few adaptive optimizers, and I am a bit surprised about their choices: DP-Adam (Adam with DP gradients) and DP-SignSGD (which is not that well-known). The reason might be that the analysis is amenable for them (questions below), and I think the contribution is very valuable neertheless.\n\n- Due to the fact that very few adaptive optimizers seem to actually fit into this SDE framework (or can be seen as discretizations of SDEs, meaning that the weakly converge to them in the vanishing step size limit), I have a feeling that this framework does not actually help in designing new hyperparameter adaptive DP optimizers. The paper seems thus to give an analytical explanation of certain differences in the optimizers' behaviors ( $\\varepsilon^{-1}$ vs.  $\\varepsilon^{-2}$ error behaviour)."}, "questions": {"value": "DP-Adam refers to the algorithm considered by (Balles and Hennig, 2018), i.e., to the plain Adam with DP-SGD gradients, right? Why not to analyze other adaptive optimizers like the versions of DP-Adam that are tailored for DP (e.g., Tang and Lécuyer, 2023, or Li et al., 2023, the references you also list)? Were DP-Adam and DP-SignSGD chosen because the analysis is amenable for them?\n\nRecently, certain filtering methods have turned out to give good privacy-utility trade-offs, see e.g.\n\nZhang, X., Bu, Z., Balle, B., Hong, M., Razaviyayn, M., & Mirrokni, V. DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction. In The Thirteenth International Conference on Learning Representations 2025.\n\nCould this SDE view allow analyzing those methods as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6oOv3t1KV9", "forum": "hSpA4DAoMk", "replyto": "hSpA4DAoMk", "signatures": ["ICLR.cc/2026/Conference/Submission24978/Reviewer_B6gH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24978/Reviewer_B6gH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905294815, "cdate": 1761905294815, "tmdate": 1762943271299, "mdate": 1762943271299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "-  This paper investigates differentially private learning through the lens of a stochastic differential equation. Based on theoretical understanding, the paper investigates how the DP-SGD, DP-SignSGD, and its variant DP-Adam perform in various privacy budgets. The authors argue that DP-SignSGD is epsilon-dependent, which reduces the burden of parameter tuning in DPDL."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe paper investigates the optimization process of differentially private learning in terms of SDE, which has not been actively investigated.\n-\tThe authors provide a theoretical analysis of why DP-SGD and DP-SignSGD differ in training dynamics, especially with hyperparameter setups.\n-\tBased on their observations, the authors argue two protocols that cover both fixed and tuning parameters."}, "weaknesses": {"value": "Please refer to the Questions section."}, "questions": {"value": "-\tThe paper investigates the difference between DP-SGD and DP-SignSGD in terms of differentially private deep learning. Is there any related work on non-private optimization, or does this comparison solely rely on a DP sense?\n-\tFor protocol A, how did the authors choose the parameters? For private learning, the clipping value is also as important as the learning rate. Can the authors provide more results while varying hyperparameters in both protocols A and B?\n-\tThe paper’s analysis is based on that the optimal performances of DP-SGD and DP-SignSGD are almost similar (without considering parameter search). However, as far as the reviewer knows, the current methods prefer DP-SGD compared to DP-SignSGD. Does DP-SignSGD still provide comparable results with private fine-tuning or bigger architectures? Refer to [1] or recent tuning methods in larger vision or language-based DP papers.\n\n    [1] Unlocking High-Accuracy Differentially Private Image Classification through Scale, 2022.\n\n-\tWhat about the case of DP-SGD-based Adam, instead of DP-SignSGD-based Adam?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LQKYr5Gvjw", "forum": "hSpA4DAoMk", "replyto": "hSpA4DAoMk", "signatures": ["ICLR.cc/2026/Conference/Submission24978/Reviewer_a1Sd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24978/Reviewer_a1Sd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983614401, "cdate": 1761983614401, "tmdate": 1762943270997, "mdate": 1762943270997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Thru discussing how DP noise interacts with adaptivity in optimization, DP-SGD and DP-SignSGD are proposed in this work, where DP-SGD is shown to be converged at a speed independent of ε, DP-SignSGD is with convergence speed scales linearly in ε. Under optimal learning rates, both methods reach comparable theoretical asymptotic performance, while this leaves potential issues in practice."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- SDE-based analysis of differentially private optimizers, using this framework to expose how DP noise interacts with adaptivity and batch noise.\n- DP-SGD is shown ito be converged at a speed independent of ε.\n- DP-SignSGD: its convergence speed scales linearly in ε, while its privacy-utility trade-off scales as O (1/ε)"}, "weaknesses": {"value": "- The assumptions on SNR (signal-to-noise ratio) are built on linear approximations that are only valid in a high-noise, low-signal regime.\n- A general Student-t distribution for batch noise is used to capture heavy tails, while it is not used consistently in assumption B.2..\n- The experimental validation for Protocol B on the StackOverflow dataset is missing."}, "questions": {"value": "- The theoretical analysis is derived for DP-SignSGD, while the conclusions are empirically with DP-Adam. Please provide more discussions and valiations.\n- The experimental validation for Protocol B on the StackOverflow dataset needs to be provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YRXiRczRwT", "forum": "hSpA4DAoMk", "replyto": "hSpA4DAoMk", "signatures": ["ICLR.cc/2026/Conference/Submission24978/Reviewer_4NeB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24978/Reviewer_4NeB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24978/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762535846668, "cdate": 1762535846668, "tmdate": 1762943270421, "mdate": 1762943270421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}