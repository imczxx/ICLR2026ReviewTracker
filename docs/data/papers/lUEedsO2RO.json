{"id": "lUEedsO2RO", "number": 12469, "cdate": 1758208032319, "mdate": 1759897507999, "content": {"title": "BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs", "abstract": "Recent advances in Large Reasoning Models (LRMs) have shown impressive capabilities in mathematical and logical reasoning. However, current LRMs rarely admit ignorance or respond with “I don’t know”. Instead, they often produce incorrect answers while showing undue confidence, raising concerns about their factual reliability. In this work, we identify two pathological reasoning patterns characterized by overthinking that contribute to the overconfident and incorrect answers: last-minute guessing and second-thought spiraling. To address these issues, we propose BARREL—a novel framework that promotes concise and boundary-aware factual reasoning. Our experiments show that BARREL-training increases the reliability of DeepSeek-R1-Distill-Llama-8B from 39.33% to 61.48%, while still achieving accuracy comparable to models finetuned on reasoning data generated by R1. These results demonstrate that our pilot study is inspiring to build more reliable and factual System 2 LRMs.", "tldr": "We present BARREL, a framework that enhances the factual reliability of Large Reasoning Models by promoting concise, boundary-aware reasoning, allowing models to stay accurate while using “I don’t know” when uncertain.", "keywords": ["Large Reasoning Models", "Factual Alignment", "Knowledge Boundary"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/729be90a93bd8e268278ff55dc848974687c1a87.pdf", "supplementary_material": "/attachment/8a4f8869db745ff70118f611b51a5f9c3656cfdf.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents BARREL, a training paradigm consisting of a SFT and RL stage with the goal of making models aware of their factual knowledge boundaries by learning to say “I don’t know” when they are unsure. In contrast to the standard RL paradigm, BARREL assigns an abstention reward if the model outputs “I don’t know”, with the magnitude of this reward being between the rewards for a correct and incorrect answer. Results show that BARREL maintains competitive overall accuracy in factual domains while significantly increasing precision (when models choose to output an answer, their accuracy is much higher). Ablations highlight the importance of both SFT and RL stages, as well as the significance of the proposed reward."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is well-motivated. Hallucinations are a major problem with LLMs and making models aware of their factual boundaries is an an important problem. The paper also correctly identifies that the standard paradigm does not reward models to express their uncertainty. \n\n- The paper is generally well-written and has a decent flow to it. \n\n- The results are impressive. Models maintain accuracy while also learning to abstain."}, "weaknesses": {"value": "- **Proxy Definition**: The \"*knowledge labeling proxy*\" (classifying a question as \"known to the model\") is a weak indicator of model knowledge. It fails to account for uncertainty when multiple plausible answers exist. Say a question has 10 possible answers. The model might not know the correct answer, but has some sampling probability for each of the answers. The proposed framework would classify this question as something known to the model, which seems strange. A majority-vote–based proxy might align more closely with actual model uncertainty. \n- **Threshold-based Reward**: The rule-based reward design implicitly fixes a single confidence threshold - that is, the model should only answer the question if its confidence is greater than $\\alpha$: \n  \n    $\\alpha \\ge \\frac{r_s - r_w}{r_c - r_w}.$\n    \n    This can be computed by writing the expected reward for the model for different correctness probabilities and checking if it is greater than the abstention reward. A limitation of this framework is that it removes flexibility across use cases — for instance, scenarios requiring high confidence (e.g., 99\\%) versus those tolerant of lower confidence (e.g., 70\\%) cannot be easily tuned.\n\n- **Limited Related Work**: The related work section omits several recent RL-based approaches that explicitly train models to reason about their uncertainty [1,2,3]. \n\n- **Experimental Design**: Some important baselines such as classifier and vanilla RLVR are missing from the main paper (see questions below). \n\n[1]: Stangel, P., Bani-Harouni, D., Pellegrini, C., Özsoy, E., Zaripova, K., Keicher, M., & Navab, N. (2025). Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models. arXiv preprint arXiv:2503.02623. \n\n[2]: Xu, T., Wu, S., Diao, S., Liu, X., Wang, X., Chen, Y., & Gao, J. (2024). Sayself: Teaching llms to express confidence with self-reflective rationales. arXiv preprint arXiv:2405.20974. \n\n[3]: Damani, M., Puri, I., Slocum, S., Shenfeld, I., Choshen, L., Kim, Y., & Andreas, J. (2025). Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806."}, "questions": {"value": "- What value of $\\alpha$ do the authors’ chosen rewards correspond to, and how was this default confidence threshold determined? (I think it corresponds to 25%) \n- The authors mention (lines 356-360) “*it is worth noting the accuracy improvement caused by increasing the ratio does not reflect an actual improvement in model capability—it merely reduces the number of incorrect refusals on known questions.*” If this is the case, then why does the ICL baseline not match the accuracy of BARREL? This hypothesis does not seem to be correct. \n - What is the motivation behind the reliability metric — what does it intuitively measure or indicate?\n - How much variance is observed in BARREL’s outputs due to sampling? Specifically, how often does the model sometimes answer and sometimes abstain on the same question?\n- Could the authors include the abstain rate $\\left( \\frac{N_w}{N} \\right)$ in the main results table for better interpretability?\n- **Probing/Classifier baseline** should also be added to the paper. Train a classifier/probe on the vanilla GRPO (no truthful rewards) outputs to predict the probability that the answer is correct. This probe can then be used to manually replace answers with “idk”. \n- \"*Quick Analysis of the Underlying Mechanism*\": This is not an experiment and should be presented in a different way. \n- \"*Did GRPO sacrifice pass@k for better pass@1*\": I do not think enough evidence has been provided to support this claim. The gap between SFT and GRPO seems to reduce significantly, suggesting that SFT has better scaling performance. \n- It will be useful to ablate the importance of SFT and the impact of RL with/without idk rewards. I think most of these results are already in the appendix, but some of the numbers should be added to the main results table. In particular, RL (without rejection rewards) should be added to the table as that is a very standard baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Opyhg7o6T", "forum": "lUEedsO2RO", "replyto": "lUEedsO2RO", "signatures": ["ICLR.cc/2026/Conference/Submission12469/Reviewer_2MeN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12469/Reviewer_2MeN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605340579, "cdate": 1761605340579, "tmdate": 1762923347347, "mdate": 1762923347347, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies two pathological reasoning modes in LRMs—last-minute guessing and second-thought spiraling—and proposes BARREL, a two-stage training framework (SFT on curated reasoning traces + GRPO with a three-level reward: correct > truthful refusal > incorrect). Experiments on factual QA (TriviaQA, SciQ, NQ-Open) report large gains in a proposed Reliability metric and truthfulness, while preserving accuracy and math ability (MATH-500). The method emphasizes teaching models to admit ignorance and uses reward shaping to avoid penalizing refusals as harshly as wrong answers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Clear diagnosis of failure modes with concrete qualitative examples and a simple taxonomy (last-minute guessing vs. second-thought spiraling). \n* Conceptually clean training recipe: SFT to seed refusal patterns + GRPO that explicitly rewards calibrated abstention. \n* Consistent empirical improvements in reliability/truthfulness across multiple 7–8B backbones, with additional OOD refusal tests and pass@k analysis. \n* Ablations that matter: effect of refusal reward magnitude; role of SFT vs. GRPO."}, "weaknesses": {"value": "1. **Generalizability Issues in Reasoning Trace Construction**  \nThe paper states that reasoning traces are constructed by prompting GPT-4 with prompts from Appendix F and BARREL reasoning examples to generate compliant reasoning patterns. However, this generation approach may lead to insufficient diversity in the resulting traces. Additionally, as a non-reasoning-focused model, GPT-4 may not be optimal for generating high-quality Long-CoT-style reasoning processes. A more effective alternative could involve **rewriting failed trajectories** produced by reasoning models using the BARREL framework—this would likely yield higher-quality datasets. In practice, model evaluations often generate a large number of failed cases characterized by *last-minute guessing* and *second-thought spiraling*; leveraging these failed cases via BARREL’s methodology would enable the creation of a more extensive and useful dataset.  \n\n2. **Unaddressed Response Length in Post-Training Evaluation**  \nThe paper uses BARREL-generated datasets for SFT and GRPO training, and optimizes the model’s original Long-CoT trajectory generation process. This optimization may inadvertently impact the model’s response length—potentially reducing the analysis depth of certain reasoning paths. Notably, Figure 2 explicitly highlights response length differences between correct and incorrect answers in pre-trained models. Given this, it is necessary to **supplement the response length data** for the evaluation results in Table 1, alongside a corresponding analysis of how training affects response length and reasoning depth.  \n\n3. **Lack of Overthinking Analysis in Non-Factual Domains**  \nWhile the paper analyzes and optimizes overthinking in the factual domain, LLMs often exhibit severe overthinking in other domains as well. For example: (1) Mathematical reasoning, where overthinking occurs across multiple problem-solving approaches; (2) Instruction following, where overthinking arises from misinterpreting diverse task requirements. Intuitively, the BARREL framework could be applicable to these domains, but the paper provides no analysis or validation of such cross-domain applicability."}, "questions": {"value": "1. While I understand the authors may have adopted the current data generation approach due to cost considerations, conducting a small-scale ablation study (as proposed in Weakness 1—i.e., comparing GPT-4-generated traces vs. BARREL-rewritten failed traces) would significantly enhance the paper’s validity and the practical significance of BARREL. Could the authors consider adding such an ablation study?  \n\n2. As noted in Weakness 2, response length is a critical indicator linked to overthinking (per Figure 2). Could the authors supplement the response length data for the evaluation results in Table 1, and provide an analysis of the underlying causes (e.g., how SFT/GRPO training modulates response length, and whether shorter/longer traces correlate with improved reliability)?  \n\n3. The paper focuses exclusively on overthinking in the factual domain. Given BARREL’s conceptual design (boundary-aware reasoning + calibrated reward), it may have broader applicability. Could the authors provide a preliminary analysis of BARREL’s suitability for addressing overthinking in other domains (e.g., mathematics, instruction following)—such as a pilot experiment or a discussion of potential adaptations to the framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rAcjBN9TOk", "forum": "lUEedsO2RO", "replyto": "lUEedsO2RO", "signatures": ["ICLR.cc/2026/Conference/Submission12469/Reviewer_cpNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12469/Reviewer_cpNK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902811574, "cdate": 1761902811574, "tmdate": 1762923346987, "mdate": 1762923346987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of Large Reasoning Models rarely admitting ignorance and producing incorrect answers instead. In this work, the authors identify two pathological reasoning patterns, called Last-minute Guessing and Second-thought Spiraling, caused by overthinking.\n\nTo address these problems, the authors proposed a very clean framework, called BARREL. The frameworks clearly build SFT and RL data for these two patterns of wrong CoT. The results on TriviaQA, SciQ, and NQ-Open clearly show the effectiveness of the framework."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper provides a very clear motivation for fixing two pathological reasoning patterns in current Large Reasoning Models, which is a very important topic.\n2. The writing of this paper is fluent and easy to follow.\n3. The experiments and analysis in this paper are comprehensive to show the effectiveness of the proposed framework.\n\nAll in all, this is a solid paper with clear structure."}, "weaknesses": {"value": "The only weaknesses I noticed are the definition of the metric \"Reliability (Rel.).\" The \"ans.\" means the ratio of answered questions. So, why do we need to compute \"ans.·Truth.\" and \"(1−ans.)·Acc.\" and sum them together? What is the rationale behind this, and why do you think this is a more robust metric?"}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AvXxGEUWYu", "forum": "lUEedsO2RO", "replyto": "lUEedsO2RO", "signatures": ["ICLR.cc/2026/Conference/Submission12469/Reviewer_xT1G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12469/Reviewer_xT1G"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12469/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977647996, "cdate": 1761977647996, "tmdate": 1762923346705, "mdate": 1762923346705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}