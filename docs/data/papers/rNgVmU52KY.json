{"id": "rNgVmU52KY", "number": 9938, "cdate": 1758150728717, "mdate": 1763687466321, "content": {"title": "On the Theory of Continual Learning with Gradient Descent for Neural Networks", "abstract": "Continual learning, the ability of a model to adapt to an ongoing sequence of tasks without forgetting the earlier ones, is a central goal of artificial intelligence. To shed light on its underlying mechanisms, we analyze the limitations of continual learning in a tractable yet representative setting. In particular, we study one-hidden-layer quadratic neural networks trained by gradient descent on an XOR cluster dataset with Gaussian noise, where different tasks correspond to different clusters with orthogonal means. Our results obtain bounds on the rate of forgetting during train and test-time in terms of the number of iterations, the sample size, the number of tasks, and the hidden-layer size. Our results reveal interesting phenomena on the role of different problem parameters in the rate of forgetting. Numerical experiments across diverse setups confirm our results, demonstrating their validity beyond the analyzed settings.", "tldr": "We provide bounds on the performance of neural networks for continual learning", "keywords": ["continual learning", "neural networks", "optimization", "deep learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3fcb897d6969e16170d2ecefa0bb8a50bbdbcbca.pdf", "supplementary_material": "/attachment/50369ec4fc65eec4f9ad2089e4d5887419d71de2.pdf"}, "replies": [{"content": {"summary": {"value": "This paper presents a theoretical analysis of continual learning (CL) when using standard gradient descent (GD) on neural networks. To make the problem tractable, the authors focus on a specific, stylized setting: a one-hidden-layer quadratic neural network trained on a sequence of tasks drawn from an XOR cluster dataset with orthogonal means. The main contribution is the derivation of closed-form bounds on both train-time and test-time forgetting. These bounds explicitly characterize the rate of forgetting in terms of key problem parameters: the number of iterations ($T$), the sample size ($n$), the number of tasks ($K$), and the hidden-layer size ($m$). The paper's numerical experiments on this synthetic data (and a small MNIST experiment in the appendix) serve to confirm the derived theoretical relationships."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated by the general lack of deep theoretical understanding of how standard GD behaves in a continual learning setting. Choosing a simplified, tractable model (quadratic network on XOR data) to gain analytical insight is a valid and valuable research methodology.\n\n2. The primary strength is the derivation of explicit, closed-form bounds (e.g., in Theorem 1) that connect forgetting to $n$, $m$, $T$, and $K$. This provides a concrete, analytical basis for understanding the trade-offs that are often observed empirically (e.g., the roles of overparameterization and sample size in mitigating forgetting)."}, "weaknesses": {"value": "1. My main reservation is the highly specific nature of the theoretical setup. The results are derived for one-hidden-layer quadratic networks on XOR cluster data with orthogonal means. It is very difficult to ascertain how—or even if—these specific bounds and polynomial dependencies (e.g., $m=\\tilde{\\Omega}(d^8K^4)$) would generalize to the settings where CL is practically studied: multi-layer ReLU networks, general classification/regression problems, and non-orthogonal tasks.\n\n2. The experiments are not sufficient to bridge the gap between this specific theory and general practice. The main experiments are on the same synthetic XOR data used for the theory, which only confirms the calculations. The appendix includes an experiment on MNIST, but it appears to use only two tasks. A two-task sequence is not a compelling demonstration for continual learning, which is concerned with performance over a sequence of many tasks.\n\n3. The paper would be significantly stronger if it demonstrated that the qualitative insights from its theory hold on more complex, standard CL benchmarks (e.g., Split CIFAR-10 or Split CIFAR-100) with a more realistic number of tasks (e.g., 5, 10, or more). This would provide evidence that the core relationships identified (e.g., between forgetting and $n$ or $m$) are fundamental and not just artifacts of the specific XOR setup."}, "questions": {"value": "1. Could the authors provide some intuition on how they expect these results to change for different, non-quadratic activation functions like ReLU? How much of the analysis is critically dependent on the quadratic activation and the specific XOR cluster structure?\n\n2. The two-task MNIST experiment in the appendix is quite limited. Could the authors provide further empirical validation on a standard benchmark like Split CIFAR-10 with 5 or 10 tasks to demonstrate that the qualitative trends predicted by the theory (e.g., the scaling of forgetting with $n$, $m$, and $K$) hold in a more complex and practical setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RXGCagamIF", "forum": "rNgVmU52KY", "replyto": "rNgVmU52KY", "signatures": ["ICLR.cc/2026/Conference/Submission9938/Reviewer_v8Eo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9938/Reviewer_v8Eo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761500304586, "cdate": 1761500304586, "tmdate": 1762921389875, "mdate": 1762921389875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes continual learning with gradient descent on a two-layer quadratic network trained on a multi-task XOR-cluster distribution (orthogonal task means, Gaussian noise). It shows that test-time forgetting for task $k$ after learning subsequent tasks can be decomposed into (i) train-time forgetting (an increase in empirical loss) and (ii) a delayed generalization gap."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and well structured."}, "weaknesses": {"value": "1. The motivation is not clearly or strongly articulated.\n2. The theoretical results are confusing. In Theorem 1, the logarithmic factors cannot be omitted, because without them it is unclear how the final bounds decay with the dimension $d$. Given the stated conditions on $\\eta T, n$, and $m$, the dependency on $d$ remains unclear.\n3. It is not clear how Theorem 4 improves upon Theorem 3. Under the assumption of sufficiently large model width $m$, the bound is improved, and an example is given (line 311), but this seems unfair: the paper should explain the width assumption in more detail and also report the corresponding Theorem 3 results under the same width condition for a fair comparison."}, "questions": {"value": "Above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YrxD6xb4gP", "forum": "rNgVmU52KY", "replyto": "rNgVmU52KY", "signatures": ["ICLR.cc/2026/Conference/Submission9938/Reviewer_8BnG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9938/Reviewer_8BnG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830392106, "cdate": 1761830392106, "tmdate": 1762921389407, "mdate": 1762921389407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis of continual learning in neural networks trained via gradient descent. Focusing on a simplified setting: a one-hidden-layer quadratic network trained on an XOR cluster dataset with Gaussian noise. The authors derive quantitative bounds on the rate of forgetting across tasks. They show how factors such as the number of iterations, sample size, task count, and hidden-layer width influence forgetting during both training and testing. The theoretical findings are supported by experiments on various setups, suggesting that the identified mechanisms generalize beyond the specific analytical model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important and timely problem: understanding the theoretical performance of continual learning in neural networks. It is well structured, combining rigorous theoretical analysis with supporting numerical experiments that validate the findings."}, "weaknesses": {"value": "Some aspects of the system setup require stronger justification. In particular, the neural network model appears overly simplified and departs from architectures typically used in practice, which may limit the generality of the conclusions."}, "questions": {"value": "1. In the first equation of Section 2.1.1, the function $f$ is not clearly defined. Please specify what types of functions are admissible for $f$, and clarify why its input involves $y_i$ multiplied by $\\Phi$.  \n2. Is the quadratic activation used in prior works? Was it chosen purely for mathematical convenience in analysis, or does it have theoretical or empirical motivation?  \n3. The dimensions of $x$ and $y$ are not specified when first introduced in Section 2.1.1. Based on Eq. (1), it appears $x$ is a $d$-dimensional real vector and $y$ is binary ($\\pm 1$).\n4. The explanation of Eq. (3) is too brief. The paper states that \"in the interpolating regime where the network can achieve zero training loss, we can drop the last term,\" but the explicit form of the training loss is not provided (related to my Question 1). It is unclear whether the loss is always nonnegative and whether omitting this term could lead to a loose approximation.  \n5. The neural network model omits bias terms, i.e., it uses $x^T w_i$ instead of $x^T w_i + b_i$ as input to the activation function. Please justify this modeling choice and discuss its implications for generality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qbj0i3A3jc", "forum": "rNgVmU52KY", "replyto": "rNgVmU52KY", "signatures": ["ICLR.cc/2026/Conference/Submission9938/Reviewer_XcUj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9938/Reviewer_XcUj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919690493, "cdate": 1761919690493, "tmdate": 1762921388937, "mdate": 1762921388937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "```\nThis paper studies the generalization ability of two-layer neural networks (NNs) trained from continual learning (CL). The authors derive one upper bound for the CL training loss (Theorem 1) and two upper bounds for the CL generalization errors (Theorem 3 and Theorem 4). They then conduct experiments on both synthetic and real-world data to analyze their theoretical findings in practice.\n```"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "```\nThe paper itself is well-written and easy to follow.\n```"}, "weaknesses": {"value": "```\n1. The proved theorems in this paper seem to give no novel insight into the studies of CL, such as how to efficiently improve the performance of CL models and how to effectively avoid catastrophic forgetting. For example, from Theorem 3, while one can only know that increasing the number of training samples for each task helps improve the generalizability of models trained from CL, but \"more training data helps generalization\" is actually a trivial conclusion.\n\n2. Both the proved Theorem 3 and Theorem 4 do not seem technically strong enough.\n    - For Theorem 3, the generalization upper bound has a positive correlation with $T$, i.e., the number of SGD iterations for each task in CL. This means that performing more SGD actually hurts the generalizability of CL models, which is a very weird conclusion and is actually contradictory to the \"benign overfitting phenomenon\".\n    - The authors then improve Theorem 3 to Theorem 4 so that the CL generalization upper bound no longer explicitly depends on the SGD iteration number $T$. However, this is achieved by making some very weird and unrealistic assumptions (see the new conditions in Theorem 4). In particular, I think it is inappropriate to assume that $m$ is greater than some training-dependent terms $\\hat F_j(w^{(t)}_j)$, especially given that the parameter $w^{(t)}_j$ also depends on $m$ ($w^{(t)}_j$ is a real vector of length $m\\cdot d$ according to Section 2.1.1). Furthermore, even with those overly strong assumptions, the improved generalization upper bound in Eq.(5) is still a sum of $T$ terms, which means it still implicitly depends on the value $T$.\n\n3. The authors only proved upper bounds for the generalization error, but not lower bounds. As a result, one cannot tell whether the upper bounds proved in this paper are tight or not, which shrinks the contribution of this paper.\n\n4. A vast body of existing literature has studied CL with NNs, such as [r1, r2, r3, r4]. In particular, [r1] also focuses on studying two-layer NNs. So I think the authors should make an in-depth comparison of how their results differ from existing theoretical works on CL with NNs and what the advantages of their proved generalization upper bounds are.\n\n\n**References**\n\n[r1] Li et al. Towards Understanding Catastrophic Forgetting in Two-layer Convolutional Neural Networks. ICML 2025.\n\n[r2] Benjamin et al. Continual learning with the neural tangent ensemble. NeurIPS 2024.\n\n[r3] Andle et al. Theoretical understanding of the information flow on continual learning performance. ECCV 2022.\n\n[r4] Cao et al. Provable lifelong learning of representations. AISTATS 2022.\n```"}, "questions": {"value": "```\nSee **Weaknesses**.\n```"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "s8BKlXvrrr", "forum": "rNgVmU52KY", "replyto": "rNgVmU52KY", "signatures": ["ICLR.cc/2026/Conference/Submission9938/Reviewer_i38r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9938/Reviewer_i38r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762102568331, "cdate": 1762102568331, "tmdate": 1762921388333, "mdate": 1762921388333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}