{"id": "iiLLQhhrtb", "number": 21110, "cdate": 1758313895109, "mdate": 1759896941550, "content": {"title": "Is the Reversal Curse a Binding Problem? Uncovering Limitations of Transformers from a Basic Generalization Failure", "abstract": "Despite their impressive capabilities, LLMs exhibit a basic generalization failure known as the *Reversal Curse*, where they struggle to learn reversible factual associations. Understanding why this occurs could help identify weaknesses in current models and advance their generalization and robustness. In this paper, we conjecture that the Reversal Curse in LLMs is a manifestation of the long-standing *binding problem* in cognitive science, neuroscience and AI. Specifically, we hypothesize two primary causes of the Reversal Curse stemming from transformers' limitations in conceptual binding: the *inconsistency* and *entanglements* of concept representations. We perform a series of experiments that support these conjectures. Our exploration leads to a model design based on JEPA (Joint-Embedding Predictive Architecture) that for the first time breaks the Reversal Curse without side-stepping it with specialized data augmentation or non-causal masking, and moreover, generalization could be further improved by incorporating special memory layers that support disentangled concept representations. Our research opens up the broader fundamental challenge of designing models capable of learning systematic conceptual binding with less human scaffolding.", "tldr": "We conjecture that the Reversal Curse is a manifestation of the long-standing binding problem, with supporting experiments which lead to model designs based on JEPA that could break the curse with high performance.", "keywords": ["Reversal Curse", "The Binding Problem", "Transformers", "Reasoning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/99ca7e3b57f7fbf0f88e59544df70566779248bf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates the \"Curse of Reversal\" problem in LLM that was pointed out in a previous study, where a model that learned \"A is B\" cannot generalize it to \"B is A\".  The paper argues that this problem boils down to the binding problem in the sense that it arises from inconsistency representations of the same concept and entangled representations of separate concepts.  The paper demonstrates this experimentally by introducing JEPA with contrastive learning and showing that this can overcome the reversal problem in their smallish setting; they additionally applies the method to arithmetic tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses interesting question about deficiency of reversal in LLM.  Inconsistency and entanglement are not novel by themselves but common worries about representation in neural networks.  However, connection with reversal problem is an interesting and novel observation.  They provide  demonstrating experiments that are convincing in their smallish settings."}, "weaknesses": {"value": "As mentioned, inconsistency and entanglement are common worries about representation in neural networks.  Usually, one expects that big data can overcome this, but somehow reversal problem still exists in LLM.  My main concern is that, although they demonstrated the connection in toy tasks, it's not clear how much valid their argument is with respect to LLM (line 353-355).  They do not seem to materialize on this point, and this is rather disappointing since they emphasize this in Introduction.\n\nClarity issue:  The authors should briefly introduce JEPA in their context as their experiment relies on this.   From what's written, it's not clear what are recognition and semantic modules in Transformer architecture and Figures 3, 4 are hard to understand.  (I looked at JEPA paper, but still didn't understand precisely because the presentations are not aligned.)"}, "questions": {"value": "In page 7, the authors present the result that, when the model is larger, the model suffers from entanglement even with JEPA.  What's the significance of this result?  Isn't it just usual overfitting?  Since the dataset is small, this result seems to be well expected.  Is this what happens in LLMs, where the dataset is huge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "83oeHmnQ5N", "forum": "iiLLQhhrtb", "replyto": "iiLLQhhrtb", "signatures": ["ICLR.cc/2026/Conference/Submission21110/Reviewer_NLXN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21110/Reviewer_NLXN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761286276636, "cdate": 1761286276636, "tmdate": 1762941275387, "mdate": 1762941275387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the Reversal Curse, a well-known challenge for large language models (LLMs). The authors argue that this limitation stems from an issue related to concept-level binding, rather than being solely due to insufficient data or architectural constraints. To examine this hypothesis, the authors design two controlled toy tasks—one that uses concept-level representations for inverse relations and another that attaches surface-level names to those concepts. Through these experiments, they identify two potential causes of the models’ failure to generalize inverse relations: (a) inconsistency in how concepts are represented across contexts and (b) entanglement between the representations of different concepts. To address these issues, the paper introduces a model based on JEPA-style joint embedding learning and in-batch contrastive learning, enhanced with memory layers to reduce representational interference. The proposed model successfully resolves the Reversal Curse in experiments and further shows that strengthening conceptual binding is crucial for inverse relational generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **S1.** By designing and evaluating two different types of controlled toy tasks, the paper effectively demonstrates limitations of current LLM architectures and clearly motivates why the Reversal Curse is closely tied to conceptual binding.\n- **S2.** The paper relates classical cognitive science concepts—such as the binding problem—to modern transformer shortcomings, offering valuable insights to the community.\n- **S3.** The authors propose a method that is well-aligned with the identified problem and show clear improvement in overcoming the Reversal Curse through experiments.\n- **S4.** The paper extends beyond simple toy tasks and demonstrates the usefulness of resolving inverse relational generalization in more complex settings, such as arithmetic reasoning, illustrating the practical benefits for LLMs."}, "weaknesses": {"value": "- **W1.** Although the paper attempts to show effectiveness beyond toy tasks by including an arithmetic reasoning task, it remains unclear whether the results generalize to more realistic real-world settings.\n- **W2.** The JEPA approach introduced to address inconsistency requires prior knowledge of concepts, yet defining such concepts in practical domains is challenging. This could make it difficult to scale the method to real-world applications.\n- **W3.** While the experiments investigate the effect of entanglement using the multiplicity setting, there is no analysis of whether the proposed memory layers remain effective under high levels of multiplicity. Including such results would help clarify the role of memory layers in reducing representational entanglement.\n- **W4.** The paper lacks sufficient ablation studies on the configuration of the JEPA module and memory layers. More comprehensive ablations enable readers better to understand the contribution and importance of each component."}, "questions": {"value": "- **Q1.** Using JEPA to address the conceptual binding problem appears promising. However, in real-world applications where prior conceptual knowledge is limited, how do the authors envision applying or extending this approach? I would appreciate the authors’ perspective on practical strategies for such scenarios.\n- **Q2.** In line 348, the paper claims: “This strongly indicates that the effect of entanglements scales with model depth.” Could the authors clarify how the results in Figure 3 specifically support the claim that the observed issue is driven by entanglement?\n- **Q3.** Beyond the need for prior knowledge, what computational or optimization challenges might arise when integrating the proposed methods into existing large-scale transformer architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T7fswpGVNG", "forum": "iiLLQhhrtb", "replyto": "iiLLQhhrtb", "signatures": ["ICLR.cc/2026/Conference/Submission21110/Reviewer_3p1K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21110/Reviewer_3p1K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885672441, "cdate": 1761885672441, "tmdate": 1762941273101, "mdate": 1762941273101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the Reversal Curse, a fundamental generalization failure where LLMs trained on “A’s wife is B” fail to infer “B’s husband is A.” The authors argue that this limitation stems from the long-standing binding problem in cognitive science, specifically due to two factors: (1) inconsistency of concept representations when entities switch roles between perceived subjects and predicted objects, and (2) entanglement of concept representations during gradient-based learning (Section 3.2). Critically, the paper demonstrates that standard transformers can learn reversal when inputs are represented at the abstract concept level (Table 1), establishing that the problem arises specifically from surface-level predictions of autoregressive next token prediction. To address these issues, they propose a Joint-Embedding Predictive Architecture (JEPA) (LeCun, 2022), which achieves high reversal performance (Figure 3). They further show that incorporating memory layers (top-k sparsity and softmax activations) dramatically improves generalization by reducing entanglement (Figure 4), and they demonstrate that such insight enables parametric forward-chaining for solving large-scale arithmetic reasoning problems (Figure 5). Overall, the paper explored fundamental limitations of current LLM approach in the lens of binding problem."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong narrative and step-by-step improvement. The work deliberately demonstrates the current problem of the transformer-based LLM Reversal Curse, identifies that the issue comes from surface-level prediction rather than architecture itself, then introduces JEPA as a way to address representational inconsistency and finally introduces the memory layer to address entanglement. The storyline is concrete and provides a clear formulation of the Reversal Curse and current transformer-based models’ limitations. This is likely to be insightful for many LLM researchers.\n\n2. The results are overall clearly presented. Figures and tables are clearly paired with take-away messages, and in general the empirical story is easy to follow.\n\n3. The paper clearly state limitations in conclusion section. For example, the authors explicitly acknowledge that “our current solutions rely heavily on human scaffolding and are specifically tailored to the reversal task, which only deals with the most basic concepts,” and they frame “scaffolding, applicable to more abstract concepts and complex skills” as an open problem rather than claiming it is solved."}, "weaknesses": {"value": "1. Motivation and uniqueness of JEPA are underspecified. The paper motivates JEPA as a fix for role-dependent inconsistency in concept representations, but it does not clearly justify why JEPA in particular is the right solution. There are many possible ways to enforce role-invariant bindings on top of current LLMs (e.g., fine-tuning, RL, etc). The paper would be stronger if it compared JEPA against alternative  appraochesor at least explained why those alternatives were ruled out.\n\n2. Memory layer design feels somewhat ad hoc. The proposed memory layer (top-k sparse retrieval + softmax activation) is claimed to reduce entanglement and improve generalization, but it is not obvious why this particular mechanism was chosen over other forms of memory (e.g., retrieval, external KV cache persistence, etc.). The paper should clarify the choices being memory \n\n3. Limited evidence of generalization to realistic NLP settings. The core problem (reversal) is motivated as a real failure mode of large language models trained on natural language, but most comparisons are then made to purpose-trained JEPA/memory systems on narrowly scoped reversal or arithmetic forward-chaining tasks. It is still unclear whether JEPA + parametric memory can coexist with broad natural-language behaviors like in-context learning, continual knowledge updating, or general QA. In particular, adding parametric memory might reduce flexibility (e.g., by hardening stored associations and making it harder to correct or update them online), which could degrade other capabilities. \n\n4. There is also no report of variance across random seeds, no error bars, and no discussion of robustness. This makes it difficult to assess stability and reproducibility of Figures 3–5, and it weakens claims about systematic improvements."}, "questions": {"value": "I am inclined to increase the score if these big-picture questions are convincingly addressed.\n\n1. Why JEPA? There can be multiple ways to solve role-inconsistent concept representations when entities switch between subject and object positions. Why JEPA specifically? Did you consider alternative architectural or training frameworks, and if so, what failed?\n\n2. Why the memory layer? Why this specific top-k + softmax parametric memory layer? Are there simpler or more standard memory mechanisms that would work as well? How sensitive is performance to this design?\n\n3. Generalization to realistic settings. My biggest concern is whether JEPA + parametric memory can be scaled to standard natural-language-style data. A potential issue with a rigid parametric memory is that it could reduce flexibility on new knowledge, which might hurt other LLM capabilities. Can you provide intuition or preliminary evidence for how reducing inconsistency and entanglement will affect these broader capabilities? I would be strongly convinced by even small-scale preliminary results, or by a clear plan to test this.\n\nMinor questions\n1. What is epsilon_A in Table 1? Please define it in the caption or main text.\n\n2. You claim that transformers “fail entirely to learn reversal” (lines 309–312). Is there quantitative evidence for “entirely”? Please provide a figure or table with those baseline numbers.\n\n3. In Figure 5, do you also evaluate a standard transformer augmented with the same memory layer? If not, please clarify why not; this matters for isolating where the improvement comes from.\n\n4. Do you have multiple random seeds and standard deviations / error bars for Figures 3, 4, and 5? If not, can you comment on stability across runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ykEJSsNbAV", "forum": "iiLLQhhrtb", "replyto": "iiLLQhhrtb", "signatures": ["ICLR.cc/2026/Conference/Submission21110/Reviewer_4J4R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21110/Reviewer_4J4R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950134309, "cdate": 1761950134309, "tmdate": 1762941271579, "mdate": 1762941271579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors demonstrate that JEPA-style architectures do not suffer from the same reversal curse, where transformers are not able to associate the latter parts of the sentence with earlier parts of the sentence and create the right associations (learning A=>B, but not B=>A). They further validate this finding by constructing a toy dataset of entity sets and relations and their reverses. Although an interesting finding (albeit intuitive because JEPA style architectures use the InfoNCE loss over different segments of the sentence, thereby creating an invariant representation bidirectionally), I am not sure that this capability or characteristic alone compels or convinces us the JEPA is better than transformers, given that the JEPA framework would require us to know the concept space a priori."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper validates the existence and how the JEPA-style architecture solves this problem with a simple yet clear toy dataset.\n- The authors also show that solving this reversal curse is also enables parametric forward-chaining and competitive performance on large arithmetic trees, which is a more real-world example of why this could matter."}, "weaknesses": {"value": "- As the authors note, the JEPA setup requires one to know where the concept states live and the memory layer trick presumes unique name equals to unique concept. \n- The dataset is very simplistic and toy-ish. We do not necessary want the models to learn a reverse map of everything (for example, A => B, does not actually imply B => A), and it is unclear to me why the reversal curse is a problem that is necessary to solve. Perhaps, a higher level motivation (beyond it's an important behavior study in cognitive science etc) would be helpful."}, "questions": {"value": "- The memory layer idea is interesting, and it is reminiscent of the Memory Layers at Scale [1]. I wonder how the transformers would fare with a similar memory layer in Figure 5.\n\n[1] Berges, Vincent-Pierre, et al. \"Memory layers at scale.\" arXiv preprint arXiv:2412.09764 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Tc7AU4lca9", "forum": "iiLLQhhrtb", "replyto": "iiLLQhhrtb", "signatures": ["ICLR.cc/2026/Conference/Submission21110/Reviewer_fjq7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21110/Reviewer_fjq7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979284378, "cdate": 1761979284378, "tmdate": 1762941270199, "mdate": 1762941270199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}