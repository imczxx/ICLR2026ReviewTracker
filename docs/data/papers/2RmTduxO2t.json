{"id": "2RmTduxO2t", "number": 21046, "cdate": 1758313191019, "mdate": 1759896945352, "content": {"title": "Membership Inference Attacks for Unseen Classes", "abstract": "The state-of-the-art for membership inference attacks on machine learning models is a class of attacks based on \\emph{shadow models} that mimic the behavior of the target model on subsets of held-out nonmember data. However, we find that this class of attacks is fundamentally limited because of a key assumption---that the shadow models can replicate the target model's behavior on the distribution of interest. As a result, we show that attacks relying on shadow models can fail catastrophically on critical AI safety applications where data access is restricted due to legal, ethical, or logistical constraints, so that the shadow models have no reasonable signal on the query examples. Although this problem seems intractable within the shadow model paradigm, we find that \\emph{quantile regression} attacks are a promising approach in this setting, as these models learn features of member examples that can generalize  to unseen classes. We demonstrate this both empirically and theoretically, showing that quantile regression attacks achieve up to \\textbf{11$\\times$ the TPR} of shadow model-based approaches in practice, and providing a theoretical model that outlines the generalization properties required for this approach to succeed. Our work identifies an important failure mode in existing MIAs and provides a cautionary tale for practitioners that aim to directly use existing tools for real-world applications of AI safety.", "tldr": "The state of the art membership inference attack (shadow models) degrades in performance when some classes are not available to the attacker; we explore when and why quantile regression attacks work as a more robust solution in this setting.", "keywords": ["membership inference attacks", "privacy"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1176b243083a1ad119dcf3bc05f096ffb79752b.pdf", "supplementary_material": "/attachment/4e3d1a99124fc7a5bf04abf95181534f782b30b1.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates membership inference attacks (MIAs) in a novel and challenging setting where the adversary lacks training data from certain classes—termed the \"unseen class\" setting. The work identifies a critical failure mode of state-of-the-art shadow model-based attacks when applied to such scenarios, particularly in high-stakes applications like detecting child sexual abuse material (CSAM) where access to sensitive data is restricted. In response, the authors propose and evaluate quantile regression attacks as a more effective alternative, demonstrating superior performance across image, tabular, and text domains. Theoretical analysis supports the generalization capability of quantile regression under distribution shifts caused by missing classes. Empirical results show significant gains in true positive rate (TPR) at low false positive rates (FPR), with up to 11× improvement over shadow models on CIFAR-100."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Identification of a Novel and Realistic MIA Setting: The motivation is grounded in real-world AI safety concerns, citing legal and ethical barriers to accessing harmful content for audit purposes.\n2. Theoretical Justification for Generalization: The transferability theorem (Theorem 5.3) provides a formal condition under which quantile predictors trained on seen classes remain calibrated on unseen ones, linking performance to linear density ratios in feature space."}, "weaknesses": {"value": "1. In Figure 3, the authors evaluate quantile regression against only the marginal baseline and random guessing. However, comparisons to shadow model–based baselines (e.g., LiRA or RMIA) under the same data-scarcity conditions  are absent. This omission weakens the empirical argument in Section 4.1.\n2. While the paper identifies an important failure mode of existing MIAs, it does not sufficiently explore whether this issue is best understood as a domain shift problem—and if so, whether domain adaptation or calibration techniques could rescue shadow models. This paper is more likely to be a benchmark paper ranther than a methodology paper, so the absence of other baselines limits the paper’s contribution as a comprehensive benchmark.\n3. Superficial empirical validation of theoretical conditions.\n - The paper uses low-dimensional (e.g., PCA or t-SNE) visualizations and fits a linear model to approximate the density ratio between seen and unseen class embeddings, reporting low MSE as evidence. However, in highly reduced dimensions, a linear (or even higher-order polynomial) fit may appear accurate even when the true high-dimensional relationship is nonlinear or the distributions are not genuinely aligned. Thus, observing a good linear fit in 2D does not substantiate the theoretical assumption that the density ratio is linear in the original feature space. \n - The condition may not justify quantile regression’s advantage: Even if the linear density ratio assumption holds, this would imply a structured, learnable relationship between seen and unseen classes—precisely the setting where standard domain adaptation or calibration techniques (e.g., recalibrating LiRA’s per-class thresholds using a small validation set or applying affine corrections) could mitigate shadow models’ failure. The paper does not compare against such adapted baselines, so the claimed necessity or superiority of quantile regression under this condition remains unsubstantiated."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QUZTPRyAaP", "forum": "2RmTduxO2t", "replyto": "2RmTduxO2t", "signatures": ["ICLR.cc/2026/Conference/Submission21046/Reviewer_C19G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21046/Reviewer_C19G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531684200, "cdate": 1761531684200, "tmdate": 1762940621989, "mdate": 1762940621989, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of performing membership inference on data from unseen classes, where standard membership inference attacks based on training shadow models cannot have enough training data from the unseen classes. Under such setting, the authors found that quantile regression based MIA outperforms shadow model based counterparts, for TPR at small FPR metrics. Ablation experiments show that the proposed attack's power nicely interpolates under increasingly large number of unseen classes. Theoretical analysis for a linear quantile regression predictor shows that if the embedding does not change with and without the unseen classes, then the a learned calibrated quantile regression remains calibrated when applied to unseen classes, shedding light on the reason for the effectiveness of quantile regression MIA on unseen classes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- MIA for unseen classes is a practical yet under explored threat model, e.g., in the context of CSAM detection. The proposed MIA method show promises of better generalization to unseen classes compared to shadow-model-based approaches, and is supported by analysis of linear quantile regression predictor under certain assumptions.\n- Experiments cover a good range of Tabular, image and textual learning dataset."}, "weaknesses": {"value": "- It is mainly a comparison and analysis paper, where all evaluated MIA methods are existent. The authors did not propose any new MIAs that could potentially boost MIA on unseen classes, but rather just applied quantile regression MIA and argued it is better than shadow-model-based method.\n- Lack of comparison to shadow-model-free MIA baselines: this includes per-class population attack [Nasr et al. 2019, Ye et al. 2022] which is applicable when adversary has access to even a small pool of target class's data, Neighborhood attack (Mattern et al., 2023) and its similar variant for image (Choquette-Choo et al., 2021), and more text-based MIA methods including Min-K (Shi et al., 2023).\n- The comparisons with shadow-model-based MIAs and RMIA make unrealistic design choices of not allowing MIAs to use ANY samples from unseen classes to train the reference models. In practice, it is often not the case and it is more realistic to assume adversary has access to a SMALL set of samples from target class. Although the authors argue due to legal reasons, training with even small number of samples from unseen classes may not be feasible. But to ensure fair and more realistic comparison, and for research purposes, the authors should also evaluate and report these MIAs when allowing one to include small set of samples from unseen classes in training shadow models. \n\n\nReferences:\n- Nasr, Milad, Reza Shokri, and Amir Houmansadr. \"Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning.\" 2019 IEEE symposium on security and privacy (SP). IEEE, 2019.\n- Ye, J., Maddi, A., Murakonda, S. K., Bindschaedler, V., & Shokri, R. (2022, November). Enhanced membership inference attacks against machine learning models. In Proceedings of the 2022 ACM SIGSAC conference on computer and communications security (pp. 3093-3106).\n- Mattern, J., Mireshghallah, F., Jin, Z., Schölkopf, B., Sachan, M., & Berg-Kirkpatrick, T. (2023). Membership inference attacks against language models via neighbourhood comparison. arXiv preprint arXiv:2305.18462.\n- Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., ... & Zettlemoyer, L. (2023). Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789.\n- Choquette-Choo, C. A., Tramer, F., Carlini, N., & Papernot, N. (2021, July). Label-only membership inference attacks. In International conference on machine learning (pp. 1964-1974). PMLR."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xr0xcbwthB", "forum": "2RmTduxO2t", "replyto": "2RmTduxO2t", "signatures": ["ICLR.cc/2026/Conference/Submission21046/Reviewer_AJSX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21046/Reviewer_AJSX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951588573, "cdate": 1761951588573, "tmdate": 1762940621222, "mdate": 1762940621222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the \"unseen class\" setting for membership inference attacks. Here, an attacker predicts if examples from certain examples were used in training but the attacker has no access to samples from those classes when training the shadow models. Baseline schemes wildly fail, but this paper shows that by applying quantile regression attack (that learn features distinguishing members from non-members rather than modeling reference distributions) achieve up to 10x higher true positive rates prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a new threat model that's compelling. Unseen classes are a new interesting attack angle.\n\nThe paper introduces a 10x better method that does far better than prior techniques.\n\nThe ROC curves are useful to see, the results are convincing, the evaluation is well performed. There are no significant errors in anything."}, "weaknesses": {"value": "This paper doesn't introduce anything really that new. The core method of quantile regression isn't anything that new. The overall scheme is basically applied exactly in the normal way. There's one small difference: instead of using the confidence on the true label (which requires knowing the ground truth label that may be from an unseen class), the paper uses the difference between the top two logits. But with that exception, the techniques are all the same. This isn't a terribly bad limitation, because it works. But it's not fully new.\n\nI didn't find the introduction compelling for \"the real-world AI safety scenario of detecting whether child sexual abuse material (CSAM) was used in a model’s training data.\" For this to work (1) the attacker needs to have come CSAM they want to test (which is illegal itself) but even more (2) wouldn't this have to assume that the original model actually has a \"CSAM\" class which seems .... unlikely? I don't understand this."}, "questions": {"value": "Do you think you have anything that's technically novel in the methodology?\n\nCan you come up with a compelling threat model where unseen classes are common?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ly8WGFFk5o", "forum": "2RmTduxO2t", "replyto": "2RmTduxO2t", "signatures": ["ICLR.cc/2026/Conference/Submission21046/Reviewer_5cCc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21046/Reviewer_5cCc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958054193, "cdate": 1761958054193, "tmdate": 1762940620771, "mdate": 1762940620771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies membership inference attacks in the \"unseen class\" setting, where attackers lack access to certain classes during training but must infer membership on those classes. This scenario captures real-world AI safety auditing constraints (e.g., CSAM detection, medical record auditing). The authors demonstrate that shadow model-based attacks fail catastrophically in this setting, while quantile regression attacks achieve superior performance by learning features that generalize across classes. Theoretical analysis based on multi-accuracy explains this generalization, supported by empirical validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "-  Important Problem: Identifies a critical yet unstudied scenario in practical AI safety auditing with significant real-world implications.\n-  Systematic Evaluation: Comprehensive experiments across image, text, and tabular datasets demonstrate quantile regression's consistent superiority.\n-  Theoretical Support: Provides theoretical explanation for why quantile regression generalizes to unseen classes."}, "weaknesses": {"value": "-  Insufficient Failure Analysis: Improvements are minimal on some datasets (e.g., CINIC-10). The paper lacks analysis of when quantile regression fails or how much data diversity ensures generalization.\n-  Unrealistic Assumptions: Assumes complete knowledge of target model architecture and training process. No systematic evaluation of robustness to architecture mismatch or training differences. RMIA receives unfair advantage (using unseen class samples at evaluation) without adequate discussion of fairness impact."}, "questions": {"value": "This paper makes a valuable contribution by revealing shadow models' failure in the unseen class setting and proposing quantile regression as an effective alternative. However, improvements are needed: (1) deeper analysis of failure conditions and quantitative relationships between dataset characteristics and attack performance, (2) systematic robustness evaluation under realistic \"black-box\" auditing conditions with architecture/training mismatches, and (3) fairer experimental comparisons by addressing RMIA's evaluation advantage. Strengthening these aspects will significantly enhance the paper's depth and practical value."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "65lXlqukJb", "forum": "2RmTduxO2t", "replyto": "2RmTduxO2t", "signatures": ["ICLR.cc/2026/Conference/Submission21046/Reviewer_Y5XR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21046/Reviewer_Y5XR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21046/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961350051, "cdate": 1761961350051, "tmdate": 1762940620446, "mdate": 1762940620446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}