{"id": "tiqfxkYf1o", "number": 611, "cdate": 1756753835795, "mdate": 1763412312735, "content": {"title": "Introducing Accurate 4-Bit Quantization with Hyperspherical Architecture", "abstract": "Due to the hardware support from NVIDIA's Blackwell architecture, 4-bit quantization of large language models promises substantial memory and throughput gains. However, naive 4-bit quantization degrades accuracy and remains challenging in practice. In this work, we revisit the root causes of this degradation and posit a new perspective through analysis of matrix multiplication and the unbounded weight within models. We show that quantization induces errors that are amplified within the attention and MLP submodules, leading to unstable error growth across layers. From this analysis, we propose architectural co-designs that use hyperspherical transformers to jointly normalize activations and constrain weights to unit norm, converting dot-products into bounded cosine similarities and suppressing error expansion. On 0.5–1B models, pretrained hyperspherical models yield new state-of-the-art performance to 4-bit weight-activation quantization, outperforming standard transformer architecture and a strong QAT baseline, while a partial normalization plug-in narrows the degradation gap in existing models. These results position model architectural co-design as a third optimization axis, complementary to existing works, for robust low-bit LLM deployment.", "tldr": "", "keywords": ["LLM", "quantization", "hyperspherical transformer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e6b7f53be6d362d3324af0521241eaab4c5dbfd3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, a hyperspherical architecture has been introduced to enhance the performance after quantization.\n\nThe key idea is to normalize weight matrices (row-wise) and activations, thereby constraining the outputs to be located within the unit-norm circle.\n\nFrom the experimental results under FP4 / INT4 quantization settings, the efficacy of the proposed architecture has been demonstrated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The idea to restrict the output power within a unit norm seems to be a reasonable method for the performance enhancement.\n- I checked the proofs in the main context (I did not check for the proofs in Appendices), and they seem to be correct and provide some insight about the quantization error propagation."}, "weaknesses": {"value": "- The most crucial concern is that the proposed concept has only been validated on small-scale LLMs (up to 1B). The validation on larger models and recent models known to be difficult to be quantized (e.g., Llama3-8B) is needed.\n- I doubt whether the proposed hyperspherical architecture is efficient, since we need to conduct the row-wise normalization for every weight matrix and normalization for every activation. This will incur additional processing time during the actual inference. I also think that further validation is needed to show that the proposed architecture does not limit the training performance for large-scale models.\n- Could the authors provide the performance of v1 under FP4 quantization? While the performance of full hyperspherical architecture has been measured under MXFP4, the performance of partial hyperspherical architecture has been examined under INT4. Please conduct the experiments under the same setting to see the importance of row-wise weight normalization.\n- While the authors claim that their research provides the third direction for quantization, synergy with existing methods has not been investigated. Is the proposed method compatible with existing outlier suppression techniques (e.g., QuaRot, SpinQuant, OSTQuant)? Is the proposed method compatible with existing weight-quantization techniques (e.g., GPTQ, BoA, GPTAQ)?\n- I recommend the authors explain the basic knowledge required for FP4 quantization. While most experimental results are obtained under FP4 quantization, most of preliminary contents are related to the integer quantization, which makes readers who are not familiar with FP quantization difficult to understand the paper."}, "questions": {"value": "See Weaknesses.\n\n- Which LLM model family has been used? I might miss the details, but it seems that the authors reported only model sizes.\n- Which weight quantizer was used to obtain the results in Tables 2 and 3 (v1, v2, not v1+QAT, v2+QAT)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "toGZ2Q7RK3", "forum": "tiqfxkYf1o", "replyto": "tiqfxkYf1o", "signatures": ["ICLR.cc/2026/Conference/Submission611/Reviewer_di54"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission611/Reviewer_di54"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761024766229, "cdate": 1761024766229, "tmdate": 1762915565513, "mdate": 1762915565513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "Most reviewers want to see the model size scale up to 8B. We'd like to emphasize that to enjoy the benefits of hyperspherical architecture fully, the model needs to be **pretrained**. Unfortunately, we do not have the resources to do so for 8B models."}}, "id": "2wSNPU8TI5", "forum": "tiqfxkYf1o", "replyto": "tiqfxkYf1o", "signatures": ["ICLR.cc/2026/Conference/Submission611/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission611/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763412311864, "cdate": 1763412311864, "tmdate": 1763412311864, "mdate": 1763412311864, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper observes that the unbounded projection in standard Transformer architectures leads to quantization error accumulation across layers, amplifying performance degradation after low-bit quantization.\nTo address this, the authors introduce a normalization scheme that constrains activation and weight magnitudes, aiming to stabilize the model and reduce quantization noise propagation.\nThey further provide a mathematical analysis showing that bounding the weights can minimize the quantization error, and present empirical results suggesting that the proposed hyperspherical model mitigates accuracy loss after INT4 quantization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper clearly identifies and articulates the connection between unbounded representations and error amplification in low-bit quantization.\n\n- The proposed normalization-based approach is intuitive and aligns with recent trends toward geometric or norm-constrained representations (e.g., hyperspherical parameterizations).\n\n- The empirical validation demonstrates that constraining activations and weights can indeed improve quantization stability and model robustness."}, "weaknesses": {"value": "- The contribution appears incremental. Similar normalization-driven stabilization has been extensively explored in prior works such as nGPT, Peri-LN, and especially BitNet, which integrates normalization and clipping for ternary/binary quantized networks. The paper does not clearly differentiate its design or novelty from these existing approaches, and a more detailed structural and numerical comparison with BitNet in particular is needed to clarify the distinct contributions of this work.\n\n-No comprehensive comparison with recent INT4 quantization methods (e.g., outlier smoothing, range calibration, or mixed-precision PTQ/QAT techniques) is provided, and it remains unclear how much additional gain the proposed architectural modification offers compared to purely algorithmic approaches."}, "questions": {"value": "- An analysis of the hardware cost introduced by the additional normalization layers seems necessary.\n\n- Given that prior architectures applying normalization for model stabilization (e.g., Peri-LN, nGPTQ) would likely not suffer significant degradation after quantization, how does the proposed method compare against such normalized baselines in terms of both performance and efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kqliZbIzcS", "forum": "tiqfxkYf1o", "replyto": "tiqfxkYf1o", "signatures": ["ICLR.cc/2026/Conference/Submission611/Reviewer_AaNM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission611/Reviewer_AaNM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761634365883, "cdate": 1761634365883, "tmdate": 1762915565060, "mdate": 1762915565060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. This paper focuses on solving accuracy degradation in 4-bit LLM quantization.\n2. It identifies that unbounded weights in standard transformers amplify quantization errors and cause unstable cross-layer propagation.\n3. The authors propose a hyperspherical transformer (normalizing weights/activations to unit norm) and partial hypersphericity plugins.\n4. Experiments on 0.5–1B models show hyperspherical designs outperform standard transformers and QAT baselines.\n5. This work establishes architectural co-design as a third optimization axis for low-bit LLM deployment."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This article proposes a new quantitative paradigm, which I believe is very innovative and worthy of in-depth research in the future. The design of the hypersphere can indeed avoid outliers, which is worthy of further exploration and investigation."}, "weaknesses": {"value": "1. There are also very efficient methods in existing PTQ frameworks; for example, QuaRot adopts the computational invariance of SliceGPT. The paper does not compare efficiency with such methods.\n2. v2 replaces RMSNorm with LayerNorm, but there is no analysis of the impact of this change on model performance.\n3. v1 only modifies the lm_head, but the main weight quantization content of LLM lies in the numerous linear layers within the decoder block, and the paper does not mention how to perform hypersphere transformation on them.\n4. In terms of the performance of the quantized model, the effectiveness of the method proposed in the paper is not high. Rotation methods in PTQ (QuaRot, OSTQuant) can basically achieve an accuracy loss of less than 1%.\n5. Experimental results for W4A4 are missing.\n6. Quantization model size is limited, i.e., less than 1B. Current quantization works often apply 7-70B scales."}, "questions": {"value": "How to extend the method to KV cache quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ggQsi7egY9", "forum": "tiqfxkYf1o", "replyto": "tiqfxkYf1o", "signatures": ["ICLR.cc/2026/Conference/Submission611/Reviewer_eA5R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission611/Reviewer_eA5R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916547068, "cdate": 1761916547068, "tmdate": 1762915564933, "mdate": 1762915564933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors show that quantization induces errors that are amplified within the attention and MLP submodules, leading to unstable error growth across layers. They propose architectural co-designs\nthat use hyperspherical transformers to jointly normalize activations and constrain\nweights to unit norm, converting dot-products into bounded cosine similarities and\nsuppressing error expansion. \nOn 0.5–1B models, pretrained hyperspherical models yield new state-of-the-art performance to 4-bit weight-activation quantization,\noutperforming standard transformer architecture and a strong QAT baseline."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Table 1 Shows that Hyperspherical models’ robustness generalizes across model sizes."}, "weaknesses": {"value": "Experiments are done on models with size up to 1B parameters.\nIt is good start, but evaluation on 8B model would be more valuable.\n\nTable 2 shows INT4 (weight-only) quantization for 0.5B model. But results are mixed, there is no clear winner: some time \"QAT\" is better, some time \"v1\" is better, some time \"v1 + QAT\" is better."}, "questions": {"value": "What is the overhead of applying normalization on activation and weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9MmUlpXpEN", "forum": "tiqfxkYf1o", "replyto": "tiqfxkYf1o", "signatures": ["ICLR.cc/2026/Conference/Submission611/Reviewer_pApf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission611/Reviewer_pApf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941265540, "cdate": 1761941265540, "tmdate": 1762915564794, "mdate": 1762915564794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}