{"id": "HdvI8bkdDG", "number": 14169, "cdate": 1758229589784, "mdate": 1759897386316, "content": {"title": "GREmLN: A Cellular Graph Structure Aware Transcriptomics Foundation Model", "abstract": "The ever-increasing availability of large-scale single-cell profiles presents an opportunity to develop foundation models to capture cell properties and behavior. However, standard language models such as transformers benefits from sequentially structured data with well defined absolute or relative positional relationships, while single cell RNA data have orderless gene features. Molecular-interaction graphs, such as gene regulatory networks (GRN) or protein-protein interaction (PPI) networks, offer graph structure-based models that effectively encode both non-local gene token dependencies, as well as potential causal relationships. We introduce GREmLN (\\textbf{G}ene \\textbf{R}egulatory \\textbf{Em}bedding-based \\textbf{L}arge \\textbf{N}eural model), a foundation model that leverages graph signal processing to embed gene token graph structure directly within its attention mechanism, producing biologically informed single cell specific gene embeddings. Our model faithfully captures transcriptomics landscapes and achieves superior performance relative to state-of-the-art baselines on cell type annotation, graph structure understanding, and fine-tuned reverse perturbation prediction tasks. It offers a unified and interpretable framework for learning high-capacity foundational representations that capture complex, long-range regulatory dependencies from high-dimensional single-cell transcriptomic data. Moreover, the incorporation of graph-structured inductive biases enables more parameter-efficient architectures and accelerates training convergence.", "tldr": "", "keywords": ["Computational Biology", "Foundation Model", "Graph Deep Learning"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac46455aaf10b3ee299574739e4508bbeec2545c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses an important problem in the domain of foundation models for single-cell RNA sequencing (scRNA-seq). The authors propose GREmLN, a single-cell foundation model that incorporates cell-type-specific gene regulatory networks (GRNs) inferred directly from gene expression data. These GRNs inform relationships between genes through the graph Laplacian, offering biologically meaningful inductive biases. The model is evaluated across multiple downstream tasks—including cell type annotation, missing edge prediction, and reverse perturbation prediction—where GREmLN outperforms existing single-cell foundation models such as scGPT."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well-motivated: Incorporating biologically meaningful structures such as cell-type-specific GRNs into foundation models is an important and timely idea in scFMs.\n2. Clear presentation: The paper is generally easy to follow, although some methodological details would benefit from further clarification.\n3. Comprehensive evaluation: The proposed approach is benchmarked across multiple tasks."}, "weaknesses": {"value": "The key limitation is the reliance on an inferred GRN, because GRN inference is itself a challenging problem, particularly for large-scale settings with 20k genes. Does the accuracy of GREmLN depend heavily on the chosen GRN inference method? The accuracy of GRN inference can be substantial different across algorithms and datasets. Furthermore, several methodological and experimental details are insufficiently described. Please see my questions below."}, "questions": {"value": "1. Chebyshev Polynomial Approximation: The use of Chebyshev polynomials to approximate the kernel Gram matrix (Lines 210–213) is motivated by the claim that “matrix exponential and spectral decomposition every batch … is computationally expensive and difficult to scale.” However, spectral decomposition of the fixed graph Laplacian need only be computed once, as the graph is not change during training. Did I miss something here?\n\n2. Details of GRN inference: Was the GRN inferred over all 20k genes? How accurate is the inferred GRN? How sensitive is GREmLN’s performance to the specific GRN inference algorithm used? Different algorithms (e.g., GRNBoost2, DeepSEM, etc.) can yield markedly different GRNs. Recent benchmarks [1] indicate performance close to random for many inference methods in certain cell lines—would the authors consider presenting results across multiple GRN inference approaches to assess robustness?\n\n3. GRN at the test time: The “Bayesian Graph Integration” step entails training a cell-type classifier (e.g., kNN on the expression matrix) to identify the cell type for a test datapoint, and using the averaged GRN. What is the accuracy of this classifier? How does misclassification at this step affect downstream performance? Using kNN on gene expression can hardly give an accurate prediction.\n\n4. Edge prediction evaluation bias: The proposed evaluation masks a subset of edges in the GRNs and attempts to recover them.\nPrior work [2] shows this setup can be biased—prediction accuracy can be spuriously high using only unmasked edges without leveraging any gene information, due to structural imbalances in gene connectivity. A more unbiased design would mask all edges related to specific genes (making those genes entirely unseen during training) and evaluate recovery.\n\n5. Error bars or confidence intervals should be reported alongside mean performance metrics to convey statistical reliability.\n\n6. Did the author use binned gene expression or the rank of gene expression values?\n\n[1] Benchmarking algorithms for gene regulatory network inference from single-cell transcriptomic data, Nature Methods, 2020\n\n[2] InfoSEM: A Deep Generative Model with Informative Priors for Gene Regulatory Network Inference, ICML, 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HESkKVNSuV", "forum": "HdvI8bkdDG", "replyto": "HdvI8bkdDG", "signatures": ["ICLR.cc/2026/Conference/Submission14169/Reviewer_Dz2X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14169/Reviewer_Dz2X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761065692356, "cdate": 1761065692356, "tmdate": 1762924629389, "mdate": 1762924629389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this study, the GREmLN (Gene Regulatory Embedding-based Large Neural model), a foundation model that leverages graph signal processing to embed gene token graph structure was proposed for single cell based gene embedding. The evaluation results showed improved cell type annotation accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A new GREmLN (Gene Regulatory Embedding-based Large Neural model) model, by leveraging (signaling, regulatory, protein interaction) graph signal processing to embed gene token graph structure for single cell based gene embedding. \n\nThe evaluation results showed improved cell type annotation accuracy (the improvement is limited)."}, "weaknesses": {"value": "The theoretical contribution is kind of limited considering that signaling graphs have been incorporated into graph AI models for single cell data analysis. \nThe improvement of performance compared with existing models is limited."}, "questions": {"value": "It is important to have graphical illustrations to show the data analysis pipelines and model architectures.\nIt is unclear how many cells were used to train the proposed model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r3xqFrXlUO", "forum": "HdvI8bkdDG", "replyto": "HdvI8bkdDG", "signatures": ["ICLR.cc/2026/Conference/Submission14169/Reviewer_KWcx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14169/Reviewer_KWcx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705678499, "cdate": 1761705678499, "tmdate": 1762924628934, "mdate": 1762924628934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GREmLN is a novel single-cell transcriptomics foundation model that resolves the orderless gene feature problem by directly embedding gene interaction graph structure into the Transformer via a graph diffusion kernel, yielding biologically meaningful and high-performing gene embeddings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-structured and easy to follow, with clear motivation and comprehensive explanations of each component in the proposed method.\n2. GREmLN effectively captures long-range dependencies between genes by injecting the structural information into  Transformer's self-attention calculation.\n3. This graph-integrated approach provides a powerful biological inductive bias for single-cell transcriptomics data, enabling the model to learn gene embeddings with higher biological significance and achieve outstanding performance in various tasks.\n4. The paper provides the codebase for the baseline methods used and detailed hyperparameter specifications for the proposed method, which greatly facilitates reproducibility."}, "weaknesses": {"value": "1. The motivation is questionable. In the Introduction, the authors state that “These formulations overlook the fact that gene products in the cell, as represented in scRNA-seq profiles, lack inherent sequential order or positional semantics,” where “These formulations” refers to methods such as scGPT [1]. However, in fact, scGPT removes causal masking and replaces positional encoding with gene name–based embeddings. Therefore, previous methods (at least scGPT) have already avoided the sequential design inherent to Transformers.  \\\n[1] Cui, Haotian, et al. \"scGPT: toward building a foundation model for single-cell multi-omics using generative AI.\" Nature methods 21.8 (2024): 1470-1480.\n\n2. The paper lacks an analysis of computational resource requirements. It proposes using a graph structure to provide relational information between genes for the Transformer. Given that the number of genes can be extremely large, this approach may demand substantial computational resources; however, the authors did not provide any analysis regarding these requirements.\n\n2. The paper contains numerous formatting concerns: \\\n* The titles of the tables should appear above the tables, but in the paper, all table titles appear below them. \n* The formatting of equations is inconsistent. Most equations lack numbering, while only some equations in Section 4.1 include equation numbers. \n* The citation format seems incorrect. Except in the Introduction, all citations use the \\citet{} format.\n\n3. The paper lacks a discussion of the limitations of the proposed method.\n\n4. The paper lacks a description of how LLMs are used."}, "questions": {"value": "1. Given the method’s reliance on the gene relationship graph structure, could the authors provide an analysis of its computational resource requirements?\n\n2. Could the authors provide a detailed discussion of the respective advantages of scGPT and the proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XgZGH0omQc", "forum": "HdvI8bkdDG", "replyto": "HdvI8bkdDG", "signatures": ["ICLR.cc/2026/Conference/Submission14169/Reviewer_XLky"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14169/Reviewer_XLky"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935318194, "cdate": 1761935318194, "tmdate": 1762924628425, "mdate": 1762924628425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GREmLN, a foundation model for single-cell transcriptomics designed to address the challenge that scRNA-seq data lacks the inherent sequential order exploited by standard transformers . The core contribution is a novel \"Graph Diffusion Kernel Attention\" mechanism . This method leverages graph signal processing to embed the structure of molecular interaction graphs, such as gene regulatory networks (GRNs) or protein-protein interaction (PPI) networks , directly into the attention mechanism. By applying a diffusion kernel derived from the graph Laplacian to the query embeddings , the model aims to produce biologically informed, single-cell-specific gene embeddings that capture long-range dependencies . The authors demonstrate GREmLN's effectiveness on several downstream tasks, including cell type annotation, graph structure understanding, and reverse perturbation prediction, claiming superior performance and parameter efficiency compared to existing baselines ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles the significant and well-known challenge of meaningfully incorporating biological prior knowledge (in the form of molecular interaction graphs) into foundation models for scRNA-seq data, which is inherently orderless .\n   2. The proposed method, which uses a graph diffusion kernel to structure the attention mechanism , is a technically novel to encoding this complex structural information."}, "weaknesses": {"value": "1. **Overstated Novelty of the Problem Formulation:** The introduction's framing of the \"orderless gene\" problem seems to overlook how prior work has actively addressed this. The paper suggests this challenge remains largely unsolved . However, several baseline models mentioned in the paper have proposed their own explicit solutions. For example, scGPT and scFoundation utilize specific tokenization and/or attention strategies to handle the set-like nature of genes, while Geneformer and Cell2Sentence employ specific gene sorting methods before feeding the data into a transformer. The paper's contribution would be stronger if it were more precisely contextualized against these *existing* solutions rather than presenting the problem as unaddressed.\n   2. **Marginal Performance Gains and Unclear Efficiency Trade-offs:** The performance improvement on the cell type annotation task, particularly when compared to scGPT, is marginal (e.g., 0.937 vs. 0.924 accuracy in Figure 1 ; 0.939 vs. 0.924 Macro F1 in Table 1 ). While the paper rightly notes GREmLN's improved parameter efficiency (10.3M parameters ), it fails to provide a complete picture of the *computational* trade-offs. The Graph Diffusion Kernel Attention, with its reliance on Laplacian operations and Chebyshev approximations , likely introduces non-trivial computational overhead during training and/or inference. The evaluation is missing a direct comparison of training times and inference latencies, making it difficult to assess the method's true efficiency and scalability.\n   3. **Limited Perturbation Benchmarking:** The experimental validation for perturbation analysis in Section 4.5 is incomplete.\n      - First, the paper only evaluates *reverse* perturbation prediction (identifying the perturbation from a perturbed profile). A standard and critical benchmark for many scRNA foundation models is *forward* gene perturbation prediction (i.e., *in silico* prediction of gene expression after a perturbation), which is not included.\n      - Second, scGPT is notably absent from the baselines in the reverse perturbation task (Table 3 ). Given that scGPT is a primary competitor in all other tasks and has been benchmarked on perturbation tasks, its exclusion from this comparison is a significant omission."}, "questions": {"value": "1. Could the authors please clarify the novelty of their approach to handling orderless genes in light of prior work? Specifically, how does GREmLN's graph-based embedding mechanism fundamentally differ from, and improve upon, the tokenization strategies of scGPT/scFoundation or the explicit gene-ordering methods used by models like Geneformer, which also aim to address this exact problem?\n2. Regarding the modest performance gains in cell type annotation (Table 1 ), what is the practical computational cost (e.g., training time, inference latency) of the Graph Diffusion Kernel Attention compared to the standard attention mechanisms in scGPT, Geneformer, and scFoundation? A direct comparison of these efficiency metrics is needed to fully evaluate the model's scalability and utility.\n3. Regarding the perturbation experiments (Section 4.5 ):\n   - a) Why was scGPT omitted as a baseline in the reverse perturbation prediction task (Table 3 )?\n   - b) Why did the authors opt to only evaluate reverse perturbation prediction, and not the more common task of in-silicon single-cell gene perturbation prediction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dOiwlt0ggj", "forum": "HdvI8bkdDG", "replyto": "HdvI8bkdDG", "signatures": ["ICLR.cc/2026/Conference/Submission14169/Reviewer_LeQb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14169/Reviewer_LeQb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14169/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004929702, "cdate": 1762004929702, "tmdate": 1762924627875, "mdate": 1762924627875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}