{"id": "ki4pPq66KR", "number": 7332, "cdate": 1758016312289, "mdate": 1759897859351, "content": {"title": "All in RLVR on Non-Verifiable Domains", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great success on verifiable domains such as math and coding abilities for large language models (LLMs). However, open domains, with their subjectivity and lack of ground truth—have long been considered fundamentally challenging for RLVR, limiting its application. In this work, we challenge this view and pioneer a novel methodology to extend RLVR into open domains. We first reveal that a partial and imperfect sample-level reward is sufficient for RL under certain conditions. On the top of this discovery, we introduce sample-specific Judge Code as programmatic rubrics, which replaces the traditional reward model (RM) to evaluate LLM responses. Specifically, our methodology is centered on a Judge Code Generator (JCG), which programmatically translates evaluation rubrics into executable Judge Code for each sample. Judge code serves as a partial and computationally efficient instantiation of the evaluation rubric. The system supports two operational modes: in Online mode (On-JCG), it dynamically generates (Query, Judge code) pairs on-the-fly to create a reusable dataset for subsequent RL training; in Offline mode (Off-JCG), it directly leverages this pre-generated dataset to enable highly efficient, RM-free training. Through experiments, we demonstrate the promising potential of applying RLVR methods to open domains. Moreover, we particularly emphasize one of the key benefits brought by efficiency:  compared with RM-based methods, specifically the generative reward model (GenRM), Off-JCG achieves more than 2x speedup in wall-time when reaching competitive performance. This work highlights a promising direction of reshaping the understanding of RLVR and open-domain research.", "tldr": "We pioneer applying RLVR to open domains by using auto-generated \"Judge Code\" to provide sufficient, partial reward signals during RL training.", "keywords": ["RLVR", "Programmatic Rubrics", "Non-verifiable", "Open domain"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4576939b928f7dcea4a9b64920d029a3657a75d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper challenges the traditional view that Reinforcement Learning with Verifiable Rewards (RLVR) only applies to verifiable domains and proposes a method to extend it to non-verifiable open domains. First, a pilot study confirms that sample-level partial and imperfect rewards can effectively guide RL training when the dataset has sufficient coverage and diversity. Then, it introduces a Judge Code Generator (JCG) to produce sample-specific Judge Code, replacing traditional reward models, and builds the Judge Code-guided Reinforcement Learning (JC-RL) framework with two modes: On-JCG (generating data in real time) and Off-JCG (using pre-generated data). Experiments show that Off-JCG matches the performance of the generative reward model (GenRM) in non-verifiable domains while achieving over 2x faster training speed. It also verifies the cross-model and cross-scale generalization of the offline dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper breaks the inherent notion that RLVR is limited to verifiable domains, proposes using sample-specific Judge Code to solve reward design problems in non-verifiable domains, opens up a new direction for RLVR applications, and has significant theoretical and practical significance.\n\n2. The Off-JCG mode skips the Judge Code generation step, achieving over 2x faster training speed than GenRM while maintaining comparable performance. It balances efficiency and effectiveness, reducing RL training costs in non-verifiable domains.\n\n3. The offline dataset (Query, Judge Code) can be transferred across different model architectures (e.g., Qwen2.5-7B and Llama-3.1-8B) and model scales (from 7B to 14B, 32B), proving its universality and scalability and enhancing the practical value of the method."}, "weaknesses": {"value": "1. Although the title \"All in RLVR on Non-Verifiable Domains\" is intriguing, its contribution seems somewhat overstated. There is a discrepancy between the actual content and the generally recognized concept of \"Non-Verifiable Domains\". The evaluated benchmarks include the Creative Writing Benchmark v3, AlpacaEval 2.0, and MT-Bench. As a technique originating from the reasoning field, I am more concerned about whether this method can be extended to the verification of reasoning in fields such as mathematical proofs, physics, chemistry, and biology.\n\n2. The paper mentions that \"The Off-JCG mode skips the Judge Code generation step, achieving over 2x faster training speed than GenRM while maintaining comparable performance.\" However, I cannot find the prompts for GenRM as a verifier, nor do I see the corresponding case studies. Without providing strict prompts, using DeepSeek V3 as the RM will lead to significant delays and redundant outputs, which will result in the lack of support for the claim that \"achieving over 2x faster training speed\".\n\n3. There may be unreasonableness in the comparison of baselines. The paper only compares the base model, GenRM, and the proposed method, with GenRM solely using DeepSeek V3. It remains unclear whether this is affected by model bias, and whether using other GenRMs can accelerate training speed and achieve better performance.\n\n4. It seems that training models using the proposed method has not reached the state-of-the-art (SOTA). As mentioned in Weaknesses 2 and 3, if replacing the model can improve training speed and metrics, the advantages of the method in this paper will be further weakened."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5iAYA52zab", "forum": "ki4pPq66KR", "replyto": "ki4pPq66KR", "signatures": ["ICLR.cc/2026/Conference/Submission7332/Reviewer_vjxV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7332/Reviewer_vjxV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761575559899, "cdate": 1761575559899, "tmdate": 1762919450022, "mdate": 1762919450022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to extend Reinforcement Learning with Verifiable Rewards (RLVR) to non-verifiable domains such as creative writing or dialogue. The authors propose the Judge Code-guided Reinforcement Learning (JC-RL) framework, which replaces the reward model (RM) with programmatically generated Judge Code, a piece of executable code that evaluates model responses according to rubrics.\n\nThe idea is motivated by the pilot study: partial, imperfect rewards at the sample level are sufficient for effective RL. The JC-RL framework provides two alternatives: (1) On-JCG, which generates Judge Code on the fly; and (2) Off-JCG, which reuses pre-generated (query, code) pairs for efficient RM-free training.\n\nExperiments on several open-domain datasets (WritingPrompts, No Robots, ShareGPT, WildChat) and verifiable tasks (Aug-IFEval) show that JC-RL achieves performance comparable to generative reward models (GenRM) while being more efficient."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work adapts RLVR from verifiable domains (e.g., math and code) to non-verifiable domains such as creative writing.\n\n2. Across multiple benchmarks, the proposed Off-JCG method achieves performance close to GenRM baselines while being over 2× faster in training time. The experiments include diverse datasets and demonstrate both cross-model and cross-scale generalization of pre-generated Judge Code.\n\n3. The paper is generally well-organized, with clear figures and a logical flow."}, "weaknesses": {"value": "1. Although the Judge Code concept is positioned as novel, it resembles existing rubric-based and programmatic reward generation approaches. The concept of \"generated code as verification\" has been proposed by several prior studies, such as AutoIF [1] and VerIF [2], making the contribution less distinctive.\n\n2. The empirical effectiveness of the proposed JCC methods is limited. As shown in Table 1, the method underperforms GenRM on non-verifiable domain tasks and falls behind rule-based verification on verifiable domain tasks.\n\n3. Some relevant contemporaneous studies—e.g., RLPR [3], Checklists are Better than RMs [4]—are cited but not critically contrasted. A clearer comparison (conceptually and empirically) would strengthen the positioning.\n\n4. It lacks systematic analysis on the quality of generated Judge Code, e.g., how code diversity or correctness affects training stability. Moreover, Off-JCG’s performance drop relative to GenRM (especially on conversational datasets) deserves deeper analysis.\n\n5. The impact of the Judge Code Generator choice is underexplored. The paper only evaluates DeepSeek-V3, leaving open the question of how performance would change when using less capable models, which is crucial for understanding generality and scalability.\n\n6. In Section 5, the unbiased-gradient proof assumes uniformly random sampling of constraints and independence between samples. However, in open-domain tasks, reward functions are not structured as discrete constraints, so the connection between theory and practice is somewhat hand-wavy.\n\n### References:\n\n[1] Dong G, Lu K, Li C, et al. Self-play with execution feedback: Improving instruction-following capabilities of large language models[J]. arXiv preprint arXiv:2406.13542, 2024.\n\n[2] Peng H, Qi Y, Wang X, et al. VerIF: Verification Engineering for Reinforcement Learning in Instruction Following[J]. arXiv preprint arXiv:2506.09942, 2025.\n\n[3] Yu T, Ji B, Wang S, et al. RLPR: Extrapolating RLVR to General Domains without Verifiers[J]. arXiv preprint arXiv:2506.18254, 2025.\n\n[4] Viswanathan V, Sun Y, Ma S, et al. Checklists are better than reward models for aligning language models[J]. arXiv preprint arXiv:2507.18624, 2025."}, "questions": {"value": "1. In Section 2 (Phase 1), the authors note that the effectiveness of partial rewards depends on task complexity. However, in Table 1, it is unclear whether the selected datasets (WritingPrompts, No Robots, ShareGPT, WildChat) are sufficiently complex to validate this claim. A discussion or quantitative measure of dataset complexity would strengthen the argument.\n\n2. The paper compares Off-JCG and GenRM in Figure 6, but the efficiency comparison between On-JCG and GenRM is missing. Providing this comparison would offer a more complete understanding of the trade-offs between different approaches.\n\n3. It would be valuable to report the model’s generalization performance after JCG training on other benchmark tasks such as MMLU. This would help assess whether the proposed method maintains broad capabilities beyond the training objectives.\n\n4. The related work section would be better placed in the main body of the paper rather than the appendix."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k0ykbJj6Jk", "forum": "ki4pPq66KR", "replyto": "ki4pPq66KR", "signatures": ["ICLR.cc/2026/Conference/Submission7332/Reviewer_3vRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7332/Reviewer_3vRC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761819812721, "cdate": 1761819812721, "tmdate": 1762919449418, "mdate": 1762919449418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates reinforcement learning with verifiable rewards (RLVR) on non-verifiable domains. This paper proposed a judge code generator (JCG) to capture partial sample-level rewards. The authors show experimentally and theoretically that JCG (both online and offline versions) effectively improves the base model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The effectiveness of JCG is demonstrated experimentally across multiple tasks.\n- The theoretical justification of the effectiveness of JCG."}, "weaknesses": {"value": "- How is JCG compared to other methods such as reward anchor by Huang et al 2025? Comparison with those methods mentioned in Appendix B (RL with LLM-based reward and RL with proxy reward) will better demonstrate the effectiveness of JCG. The current comparisons focusing on base model or RM seem unfair.\n- The experiments use DeepSeek-V3 to create JCG. Is there any ablation study on the choice of model?\n- In addition to IFeval, the paper could benefit from experiments on math and coding tasks, which are more objective than instruction following, to show JCG does not harm performance on verifiable domains.\n- Results in Appendix seem to indicate JCG induces training stability issues, and the authors introduced an auxiliary judge code generator to regularize the training. The authors should clarify the limitations clearly. I also wonder does the auxiliary JCG introduce any overhead?"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FuV6QdihLr", "forum": "ki4pPq66KR", "replyto": "ki4pPq66KR", "signatures": ["ICLR.cc/2026/Conference/Submission7332/Reviewer_6uoF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7332/Reviewer_6uoF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933228689, "cdate": 1761933228689, "tmdate": 1762919448737, "mdate": 1762919448737, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes extending Reinforcement Learning with Verifiable Rewards, originally effective for verifiable tasks like math and coding, to non-verifiable open-domain tasks. The authors introduce a Judge Code Generator that translates textual evaluation rubrics into sample-specific executable “Judge Code”, which acts as a programmatic reward function.  Extensive experiments using Qwen2.5-7B and Llama-3.1-8B across creative writing, dialog, and instruction-following datasets show that Off-JCG achieves performance close to GenRM-based methods but with over 2× training efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of compiling rubrics into executable Judge Code to replace large reward models is good. It transforms reward modeling into a programmatic supervision task.\n\n\n2. Multiple datasets across creative writing, dialog, and instruction-following show consistent performance over base models and competitive results."}, "weaknesses": {"value": "1. While case studies are mentioned, the paper offers few examples of how the Judge Code captures nuanced open-domain evaluation (e.g., creativity, coherence).\n\n\n2. The generated Judge Code may encode superficial or brittle heuristics; error tolerance, interpretability, and safety aspects of executing auto-generated code need discussion.\n\n\n3. Missing deeper comparisons with recent rubric-based RL or self-rewarding frameworks (e.g., Rubric Anchors: https://arxiv.org/abs/2508.12790, Checklists: https://arxiv.org/abs/2507.18624), which share similar motivations."}, "questions": {"value": "1. How robust is the Judge Code generation process, does it ever produce invalid or unsafe code, and how are such cases handled automatically?\n\n\n2. In non-verifiable domains like creative writing, what specific rubrics (creativity, coherence, emotion) emerge in Judge Code, and how interpretable are they?\n\n\n3. How does Off-JCG handle potential distribution shifts when reusing datasets across different models or domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lYCcr9XHiz", "forum": "ki4pPq66KR", "replyto": "ki4pPq66KR", "signatures": ["ICLR.cc/2026/Conference/Submission7332/Reviewer_mdeE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7332/Reviewer_mdeE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7332/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042159323, "cdate": 1762042159323, "tmdate": 1762919448182, "mdate": 1762919448182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}