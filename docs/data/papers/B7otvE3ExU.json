{"id": "B7otvE3ExU", "number": 20262, "cdate": 1758304220084, "mdate": 1759896987591, "content": {"title": "FusionFactory: Fusing LLM Capabilities with Multi-LLM Log Data", "abstract": "The rapid advancement of large language models (LLMs) has created a diverse\nlandscape of models, each excelling at different tasks. This diversity drives re-\nsearchers to employ multiple LLMs in practice, leaving behind valuable multi-\nLLM log data. This naturally leads to the question of whether such logs can be\nfully leveraged to fuse LLMs’ complementary capabilities. Although prior work\nhas explored various strategies for integrating multiple LLMs, we argue that prac-\ntical fusion must meet two essential requirements: (1) compatibility with real-\nworld serving scenarios (e.g., local and API-based serving), and (2) flexibility to\noperate at different stages of the LLM pipeline to meet varied user needs (e.g.,\nfine-tuning and inference stages). To this end, we introduce LLMFusionBench,\na large-scale benchmark for LLM fusion that spans 14 tasks across five domains,\nwith responses from 20 open-source LLMs (8B–671B) totaling 103M tokens.\nBuilding on LLMFusionBench, we propose FusionFactory, a systematic\nframework with three elaborated levels: (1) query-level fusion via tailored LLM\nrouters, (2) thought-level fusion leveraging retrieved abstract reasoning tem-\nplates, and (3) model-level fusion via distillation from top-ranked responses. Ex-\nperiments show that FusionFactory consistently outperforms the best individ-\nual LLM across all 14 benchmarks, with the optimal fusion configuration varying\nacross benchmarks, highlighting the promise of multi-LLM log data as a practical\nfoundation for fusing diverse LLM capabilities.", "tldr": "We introduce LLMFusionBench, a large-scale routing benchmark, and FusionFactory, a multi-level LLM fusion framework that leverages routing data and thought templates to outperform the best individual LLMs across diverse tasks.", "keywords": ["LLM fusion", "Query-level fusion", "Thought-level fusion", "Model-level fusion"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cad5c9cf3d97feb62ebb1659f0a3c9438dfab0da.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the challenge of leveraging the complementary strengths of diverse Large Language Models (LLMs), which are often captured in real-world multi-LLM log data. The paper argues that practical LLM fusion must satisfy two requirements: 1) compatibility with real-world serving scenarios (local and API-based), and 2) flexibility to operate at different stages of the LLM pipeline (fine-tuning and inference). The introduction of LLMFusionBench spans 14 tasks across 6 domains. It includes responses from 20 open-source LLMs (8B-671B) and rich supervision data, including task performance, cost, and LLM Judge scores. Then, a systematic framework fusionFactory is introduced for stage-aware fusion. Experiments show that FusionFactory consistently outperforms the best individual LLM across all 14 benchmarks. Thought-level fusion achieves the best overall performance, while Query-level fusion offers the best balance of performance and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. It introduces LLMFusionBench, a large-scale, diverse, and well-structured benchmark covering 14 tasks across 6 domains, responses from 20 LLMs (8B-671B), and including critical metadata like performance, cost, and LLM Judge scores.\n2. FusionFactory is an innovative and systematic framework that comprehensively explores fusion at three distinct stages—Query-level (Early), Thought-level (Mid), and Model-level (Late). This stage-aware design satisfies the requirement for practical flexibility and demonstrates broader applicability than prior work."}, "weaknesses": {"value": "1. For model-level fusion. The analysis should include more robust distillation or merging methods (e.g., parameter merging or logit-distillation for open-source models) to truly demonstrate the limit of model-level fusion using the logs, rather than just the limit of the chosen SFT strategy.\n2. While the results claim Query-level fusion has minimal computational overhead, there is no dedicated, quantitative comparison of the latency or API cost of the three FusionFactory levels when deployed in an inference setting."}, "questions": {"value": "1. What is loss funciton L in the Eq. 5. \n2. To fully justify the framework's practical claims, please provide a quantitative comparison of inference latency/cost for a sample task across the optimal configurations of the three FusionFactory levels.\n3. For the Query-level fusion, please provide an ablation on the input features used by the router (e.g., GraphRouter)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vRhE9tATM1", "forum": "B7otvE3ExU", "replyto": "B7otvE3ExU", "signatures": ["ICLR.cc/2026/Conference/Submission20262/Reviewer_7F9G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20262/Reviewer_7F9G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917387286, "cdate": 1761917387286, "tmdate": 1762933744532, "mdate": 1762933744532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces LLMFusionBench, a large-scale benchmark comprising responses from 20 open-source LLMs (ranging from 8B to 671B parameters) across 14 tasks in 6 domains, totaling 103M tokens. Built from real multi-LLM log data, it captures both direct and reasoning-augmented (chain-of-thought) responses, along with performance, cost, and LLM-judged quality scores. On top of this benchmark, the authors propose FusionFactory, a systematic framework for fusing LLM capabilities at three distinct levels:\n\n(1) Query-level fusion: uses tailored routers to select the best LLM per query;\n\n(2) Thought-level fusion: retrieves abstract reasoning templates from high-performing past responses to guide new generations;\n\n(3) Model-level fusion: distills top-quality responses into a single base model via supervised fine-tuning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1)The work is motivated by the widespread practice of using multiple LLMs in real systems (e.g., API platforms, agentic workflows), which naturally generates valuable multi-LLM log data—making the research question highly relevant and actionable.\n\n\n(2)Introduction of LLMFusionBench which is a Comprehensive and Publicly Valuable Resource\n\n(3)The framework is designed to work in both local (weights accessible) and API-based (black-box) serving scenarios, addressing a critical gap in prior fusion methods that often rely on internal model states or logits unavailable via APIs."}, "weaknesses": {"value": "(1)The benchmark is constructed by actively querying 20 open-source LLMs with fixed prompts, rather than using real-world operational logs from actual multi-LLM deployments (e.g., user-facing API platforms). This synthetic setup may not reflect true usage patterns, query distributions, or failure modes seen in practice.\n\n(2)The paper introduces an LLM judge to score “insightfulness,” but this introduces potential circularity: the same type of model used in fusion is also used to evaluate it. Moreover, the judge’s prompt and reliability are not rigorously validated.\n\n(3)While thought-level fusion shows strong gains, the method depends heavily on the quality of the summarizer (LLaMA-3-70B) and the similarity search. There is no ablation on: the impact of summarization errors, the sensitivity to embedding model choice and the performance when retrieved templates are irrelevant or misleading.\n\n(4)Model-level fusion (via SFT) consistently lags behind other methods, yet the analysis stops at “overfitting” and “task heterogeneity.” No experiments probe whether architectural mismatches, training instability, or label noise contribute."}, "questions": {"value": "(1)Would techniques like curriculum learning, response filtering, or reinforcement learning improve distillation?\n\n(2)Is the base model (LLaMA-3-8B) too small to effectively absorb knowledge from diverse, larger LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZElW8OVIsK", "forum": "B7otvE3ExU", "replyto": "B7otvE3ExU", "signatures": ["ICLR.cc/2026/Conference/Submission20262/Reviewer_ZKK6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20262/Reviewer_ZKK6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931098915, "cdate": 1761931098915, "tmdate": 1762933744071, "mdate": 1762933744071, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LLMFusionBench, a large-scale benchmark for LLM fusion that spans 14 tasks across five domains, with responses from 20 open-source LLMs (8B–671B) totaling 103M tokens. Building on LLMFusionBench, we propose FusionFactory, a systematic framework with three elaborated levels: (1) query-level fusion via tailored LLM routers, (2) thought-level fusion leveraging retrieved abstract reasoning templates, and (3) model-level fusion via distillation from top-ranked responses. Experiments benchmark different methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Query-level fusion, Thought-level fusion, and Model-level fusion for LLMs are important.\n\n2. Benchmark datasets are provided.\n\n3. Experiments show the performance on different fusion levels."}, "weaknesses": {"value": "1. The three fusion level is related but not very close. Each fusion level already has a few benchmark papers. It might be suitable for industry pipeline as an all-in-one pipeline for fine tuning a model while the research contribution may be limited.\n\n2. Benchmark on each level is relatively simple and lacks in-depth research analysis.\n\n3. In model-level fusion, the fine-tuned model performs worse than the zero-shot model."}, "questions": {"value": "The primary area should be benchmark track, not \"foundation or frontier models, including LLMs\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WRHTlvNhxB", "forum": "B7otvE3ExU", "replyto": "B7otvE3ExU", "signatures": ["ICLR.cc/2026/Conference/Submission20262/Reviewer_jus6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20262/Reviewer_jus6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762235443454, "cdate": 1762235443454, "tmdate": 1762933743696, "mdate": 1762933743696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents LLMFusionBench, a large-scale benchmark built to study and evaluate how multiple large language models (LLMs) can be combined using multi-LLM log data. While framed under the “FusionFactory” framework, the work does not propose a fundamentally new fusion algorithm; rather, it systematizes and benchmarks existing approaches—routing, reasoning retrieval, and distillation—across three stages: query-level, thought-level, and model-level. LLMFusionBench includes responses from 20 open- and closed-source LLMs across 14 tasks and 6 domains, offering standardized data for analyzing cross-model complementarity. Through empirical comparisons, the paper shows that previously known techniques perform differently across fusion levels, with thought-level fusion achieving the strongest gains."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces LLMFusionBench, a large-scale benchmark that compiles and standardizes multi-LLM log data—responses from diverse language models across multiple tasks and domains—to facilitate systematic studies of model capability fusion."}, "weaknesses": {"value": "1.The paper offers limited to no novelty in term of methodology, as it mainly consolidates previously established techniques, such as routing, reasoning retrieval, and distillation, into a benchmark framework, and most of the findings reported from the proposed setup are already well-known in existing literature, e.g. any \"fusion method\" is better than vanilla LLM.\n2. What are the direct strengths of the curated dataset here over previous datasets that also generate data from LLM for distillation purposes?\n2. The paper provides no valuable insights or actionable suggestions on how to design or improve fusion methods—the benchmarking results are largely descriptive, without yielding deeper understanding or principles that could guide future research."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0hXsjmE4iP", "forum": "B7otvE3ExU", "replyto": "B7otvE3ExU", "signatures": ["ICLR.cc/2026/Conference/Submission20262/Reviewer_918D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20262/Reviewer_918D"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20262/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762385140285, "cdate": 1762385140285, "tmdate": 1762933743255, "mdate": 1762933743255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}