{"id": "DxKiXbjzbc", "number": 24122, "cdate": 1758353008469, "mdate": 1759896780924, "content": {"title": "How Does RL Induce Skill Composition? A Case Study Using Countdown", "abstract": "While reinforcement learning (RL) successfully enhances reasoning in large language models, its role in fostering compositional generalization (the ability to synthesize novel skills from known components) is often conflated with mere length generalization. To this end, we study what RL teaches about skill composition and how the structure of the composition affects the skill transfer. We focus on the Countdown task (given $n$ numbers and a target, form an expression that evaluates to the target) and analyze model solutions as expression trees, where each subtree corresponds to a reusable subtask and thus can be viewed as a ``skill.'' Tracking tree shapes and their success rates over training, we find: (i) out-of-distribution (OOD) generalization to larger $n$ and to unseen tree shapes, indicating compositional reuse of subtasks; (ii) a structure-dependent hierarchy of learnability---models master shallow balanced trees (workload is balanced between subtasks) before deep unbalanced ones, with persistent fragility on right-heavy structures (even when the composition depth is the same as some left-heavy structures). Our diagnostic reveals what is learned, in what order, and where generalization fails, clarifying how RL-only post-training induces OOD generalization beyond what standard metrics such as pass@k reveal. Our insights seem applicable to any domain with a hierarchy of reasoning with structural subtasks, such as program synthesis.", "tldr": "", "keywords": ["compositional generalization; reasoning models;"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfcf51276f6d00572c3c9643fe6b8cc0abebe4cd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper examines how reinforcement learning (RL) fine-tuning on large language models (LLMs) can instill compositional problem-solving skills beyond mere length generalization. Using the COUNTDOWN arithmetic puzzle as a test, the authors represent each solution as a binary expression tree where subtrees correspond to reusable subtasks or “skills.” This framework lets them disentangle two types of generalization: length generalization (solving longer sequences by iterating a known algorithm) versus compositional generalization (synthesizing a novel solution procedure from known components). \n\nThe key findings are: (1) Models trained on puzzles with $n=3,4$ numbers can generalize to solve puzzles with $n=5,6$ numbers, indicating they recombine learned smaller-step skills to tackle larger problems. (2) The structural complexity of the solution has a profound effect on learnability: shallow, balanced expression trees are mastered more easily and reliably than deep or unbalanced ones, even when problem length is the same. (3) RL can enable some compositional extrapolation: when an entire family of solution patterns (a particular tree shape at $n=3$) is held out during training, the RL-fine-tuned model is later able to synthesize that unseen pattern and even solve higher-order versions of it, using the familiar atomic skills in a new combination. These results suggest that RL fine-tuning does impart the ability to compose known operations in novel ways, not just to execute a fixed reasoning template for more steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper addresses an important open question – how does RL improve an LLM’s reasoning abilities – with a very clear focus on compositional generalization. It introduces a well-defined pattern framework to distinguish length vs. compositional generalization formally.\n\n* The authors evaluate generalization along two axes (longer sequences, and novel solution structures) and carefully control for each factor.\n\n* Several interesting findings emerge that deepen our understanding of how RL fine-tuning helps reasoning."}, "weaknesses": {"value": "* Generality to Other Domains: the significance is somewhat tied to this specific testbed; the paper would be stronger if it at least discussed or empirically checked whether similar compositional skill reuse occurs in a different domain or a more natural language reasoning dataset.\n\n* Lack of Baseline Comparisons: The study focuses on RL fine-tuning exclusively, so it is unclear to what extent RL is uniquely responsible for the observed generalization gains. There is little discussion of how a simpler fine-tuning approach would fare. For example, would supervised learning on the same training puzzles (using correct solutions as targets) enable any compositional generalization, or is the reinforcement signal crucial? The paper argues that RL “induces OOD generalization beyond what standard metrics…reveal”, but providing a direct comparison (e.g., a baseline model fine-tuned with supervised learning or guided by chain-of-thought prompting/skill composition based prompting) would substantiate RL’s advantage. Without this, one might wonder if the improvements come partly from exposure to many sampled solutions during RL training rather than the intrinsic benefits of the RL objective. In summary, the experimental evidence for RL’s efficacy would be more convincing if alternative training paradigms were evaluated on the same framework.\n\n* A notable omission is the lack of discussion or citation of some prior works on inducing compositional generalization in language models. For instance, skills-in-context prompting as a strategy to unlock compositional reasoning in LLMs, Learning to Follow Language Instructions with Compositional Policies\n\n* Scope of Skills and Definitions is unclear"}, "questions": {"value": "* How confident are you that the hierarchy of difficulty observed (balanced vs. right-heavy trees) will translate to other compositional reasoning tasks?\n\n* Did you consider training the model on the same COUNTDOWN tasks using supervised learning (e.g. cross-entropy on ground-truth solutions or with step-by-step solutions) and comparing its generalization to the RL-fine-tuned model?\n\n* The compositional generalization achieved through RL in your work is reminiscent of results achieved by “skills-in-context” prompting in Chen et al. (2023), although via a very different mechanism. Have you considered combining these ideas? For example, could providing a few compositional exemplars in context plus RL fine-tuning yield even stronger generalization, or help the model leapfrog the hard right-heavy cases? A discussion contrasting learning via rewards versus prompting would be enlightening – e.g., perhaps RL is better at exploration, while prompting provides strong guidance, and the two could be complementary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IP1s0GaMVg", "forum": "DxKiXbjzbc", "replyto": "DxKiXbjzbc", "signatures": ["ICLR.cc/2026/Conference/Submission24122/Reviewer_1GMS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24122/Reviewer_1GMS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978130123, "cdate": 1761978130123, "tmdate": 1762942946320, "mdate": 1762942946320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the open questions of what RL teaches skill composition and how the structure of the composition affects the skill transfer. It reaches the conclusion that RL-only post-training induces OOD generalization beyond what standard metrics can reveal."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The study of RL behavior regarding LLM’s skill composition capability for more sophisticated tasks is interesting research."}, "weaknesses": {"value": "1. The study is limited on the COUNTDOWN task. This task is currently considered relatively easier than other frontier reasoning tasks such as AIME, FrontierMath, HLE, etc. It is still questionable the conclusions observed on COUNTDOWN task still holds true for other more sophisticated tasks.\n\n2. The paper proposes to analyze the underlying computational structure of arithmetic solutions such as canonical patterns of reasoning. The method is vaguely written and reads empirical. It is unclear whether its source code will be released or not. If not, the proposed method would be hard for the community to reproduce.\n\n3. The paper misses to cite a few important related work. For instance:\n\n[1] J. Chen et al. 2024. Skills-in-Context: Unlocking Compositionality in Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2024."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7gREbb9dUl", "forum": "DxKiXbjzbc", "replyto": "DxKiXbjzbc", "signatures": ["ICLR.cc/2026/Conference/Submission24122/Reviewer_xutx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24122/Reviewer_xutx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990021487, "cdate": 1761990021487, "tmdate": 1762942946068, "mdate": 1762942946068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates how RL fine-tuning improves compositional reasoning in large language models (LLMs). Using the COUNTDOWN arithmetic task as a controlled testbed, the authors aim to disentangle length generalisation (applying known policies to longer problems) from compositional generalisation (synthesizing new policies from known subskills). They represent solutions as canonical expression trees to analyse structural learning dynamics. Their results demonstrate that RL-finetuned models can generalize to larger problem sizes (length generalisation) and can synthesize unseen patterns (compositional generalisation). Importantly, they reveal a hierarchy of difficulty: balanced tree structures are easier to learn than right-heavy ones, highlighting a “lookahead bottleneck” as a key limitation in RL-based reasoning (which speaks to the traditional exploration problem)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Significance and originality**: It is highly relevant to the LLM community, offering a new structural framework to disentangle compositional and length generalisation. This distinction is often conflated in prior works.\n\n- **Empirical depth**: There is a rich set of ablations and diagnostics illustrating how RL finetuning impacts sample-efficiency and generalisation across structural variants of COUNTDOWN tasks.\n\n- **Insightful analysis:** The results in Section 4.2 are particularly interesting, showing that structural pattern complexity plays a significant role in learning difficulty. This is potentially because the balanced and left-heavy patterns allow easier exploration (more ways to solve the task) than the right-heavy ones (due to the bottlenecks). Although the paper does not make this connection. This suggests that RL finetuning remains bounded by classic RL exploration challenges."}, "weaknesses": {"value": "## Major\n- **Lack of grounding in RL composition literature:** The paper cites none of the foundational and recent works on skill composition in RL, missing a connection to established definitions and frameworks. For example, the foundational work of Todorov [2009] that demonstrated that RL agents could provably compose learned skills to solve new tasks, which was then extended to Boolean composition by Tasse et al [2020] and more general classes of composition operators by Adamczyk et al. [2023ab]. See Mendez et al [2023] for a survey.\n\n- **Ambiguous RL setup:** The RL formulation (state, action, discounting, etc.) is unspecified. It is unclear whether the actions correspond to full vocab tokens or restricted arithmetic operators, creating methodological ambiguity.\n\n- **Non-standard definitions of skills and skill composition:** In the paper an atomic skill is defined as action, specifically selecting an operator (-,+,x,/). They then define a skill composition as simply being a regular policy (a sequence of actions corresponding to operators and integers).  However, a skill in RL is usually defined as a policy or value function (that corresponds to some well defined task/MDP) and a skill composition is a set of operators applied to a set of skills to generate a new skill.\n\n- **Vague formalism:** The main idea in this paper is to disentangle the effect of length and compositional generalization on model generalization. However these two notions of generalization are not mathematically formalized and lack concrete illustrative examples (the definitions given on page 3 are not precise enough). For example Length generalisation is defined as \"the ability to apply a known compositional pattern to problems requiring a greater number of atomic operations\", but it is unclear what are examples of such problems using the pattern in Figure 1 as the \"known\" pattern.\n\n- **Missing dataset and statistical details:** Dataset size (total number of examples), number of examples per subtree shape, training vs heldout splits, etc are not fully reported. It is also unclear if any of the results are statistically significant, since no measure of variance or statistical significance are provided for the highly stochastic RL results. This is especially problematic for non-critic based RL methods like GRPO that rely on Monte Carlo value estimates.\n\n- **Questionable experimental claims:** \n  - In Section 4.1, the authors claim \"Models trained only on n ∈ {3, 4} problems show substantial accuracy improvements on held-out n = 5 puzzles\". Improvements over what? Natural baselines like the pretrained model and SFT models are not included for comparison. The authors also claim \"These results confirm that models successfully achieve length generalization\". This is unjustified given the drastic accuracy decline as n increases. It is also unjustified given that the heldout tasks (n=5,6) look like they are not constrained to only \"known\" patterns from the training tasks. This is a problem given that the main point of this experiment was to isolate the amount of length generalisation from compositional generalisation. In general this experiment is not properly controlled.\n  - The claims in Section 4.4 rely solely on the heldout curve in Fig 4 (bottom-left and bottom-center). This experiment is not only not averaged across multiple training seeds, but it is also not averaged across different choices of held-out patterns.\n\n## Minor\n- Undefined abbreviations (e.g., SFT on first use)\n- The accuracy of the greedy policy (i.e. when selecting the highest probability action) is not reported.\n- What are those metrics in Fig 2 bottom? We can guess but the paper should precisely define them.\n- Table 3 is missing the pretrained model without finetuning.\n\nOverall, this paper makes interesting methodological and empirical contribution to understanding how RL induces compositional reasoning in LLMs, but it would benefit from tighter formal definitions, stronger theoretical grounding, and more controlled experimental validation.\n\n[Todorov 2009] E. Todorov. Compositionality of optimal control laws. In Advances in Neural Information Processing Systems.\n\n[Tasse et al. 2020] G. Nangue Tasse, S. James, and B. Rosman. A Boolean task algebra for reinforcement learning. Advances in Neural Information Processing Systems.\n\n[Adamczyk et al. 2023a] J. Adamczyk, A. Arriojas, S. Tiomkin, and R. V. Kulkarni. Utilizing prior solutions for reward shaping and composition in entropy-regularized reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence.\n\n[Adamczyk et al. 2023b] J. Adamczyk, S. Tiomkin, and R. Kulkarni. Leveraging prior knowledge in reinforcement learning via double-sided bounds on the value function. arXiv preprint arXiv:2302.09676, 2023.\n\n[Mendez et al 2023] J. A. Mendez and E. Eaton. How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition. TMLR."}, "questions": {"value": "1. Why depart from the standard RL definitions of “skills” and “skill composition”? How would the results or interpretations change under conventional definitions (e.g., as in Todorov 2009 or Adamczyk et al. 2023)?\n2. Please clarify your RL formulation: what are the exact state and action spaces, and is discounting used? Are the actions restricted to arithmetic tokens or full LLM vocabulary?\n3. Can you provide formal definitions and concrete examples for “length generalization” and “compositional generalization,” perhaps illustrated via Fig. 1 patterns?\n4. What is the total dataset size and distribution of patterns by shape? How would Fig. 6 (dataset pattern distribution) look?\n5. Are results averaged over multiple seeds? If so, please report variance; if not, why?\n6. How can the claims in Sections 4.1 and 4.4 be justified without comparison to pretrained and SFT baselines or statistical validation?\n7. It seems like the additional reward of 0.1 for correct formatting has a large effect on the pattern coverage performances. How would these results change in the absence of that reward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8C7aXnDIPq", "forum": "DxKiXbjzbc", "replyto": "DxKiXbjzbc", "signatures": ["ICLR.cc/2026/Conference/Submission24122/Reviewer_fN4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24122/Reviewer_fN4B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762369357429, "cdate": 1762369357429, "tmdate": 1762942945829, "mdate": 1762942945829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper digs into how RL actually teaches LLMs to combine skills, using COUNTDOWN (an arithmetic puzzles where you make a target number from given numbers using operators such as +, -, etc.) as a clean testbed. The main contribution is a framework that maps expressions to canonical tree structures, allowing us to track exactly which problem-solving patterns models learn and when. They make an important distinction that prior work missed: length generalization (applying things learnt from shorter sequences to longer sequences) vs. compositional generalization (combining known things in novel ways). Training on n=3,4 problems, models generalize pretty well to n=5,6. Interestingly, it's not \"problem length\" that determines difficulty, rather it's the compositional structure. They discover \"lookahead bottleneck\" where right-heavy patterns like A/(B-C/D) are much harder than left-heavy ones at the same depth, because you have to commit to the outer operation before you know what complex thing comes later. The held-out pattern experiments show models can actually synthesize new patterns they've never seen, which is genuine compositional generalization."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Clearly separation of two often conflated generalizations: The authors cleanly split length vs. compositional generalization through careful dataset design (making all patterns equally common, which existing datasets don't do). This helps show that RL teaches real skills combination, not just \"do the same thing but longer\"\n\nThe lookahead bottleneck is a cool finding: Right-heavy patterns are fundamentally harder than left-heavy ones even at equal depth. This reveals something about autoregressive models -- they struggle when they need to plan ahead before generating complex sub-parts. That's not about the task, that's about the architecture\n\nGoes way beyond pass@k: They track pattern coverage and precision over training, showing there's a big gap between \"the model can generate this pattern sometimes\" and \"the model reliably executes it.\" Pass@k completely misses this dynamic, showing a fundamental flaw in the metric."}, "weaknesses": {"value": "The biggest weakness according to me that everything in the paper is just learnt from experiments on COUNTDOWN, which is one synthetic math task. We don't actually know if the lookahead bottleneck shows up in complex tasks such as code generation, theorem proving, or other compositional tasks. The generalizability claims are reasonable speculation but still speculation.\n\nThis sort of puts the work in a kind of foundational category (for me), but since the learnings are all from empirical results, it's neither fully experimental (with more elaborate experiments), or foundational (no guarantees as such -- only speculation)."}, "questions": {"value": "Quoting the authors “right-heavy structures that demand significant lookahead before executing a complex subroutine constitute a fundamental bottleneck for autoregressive models\". What specific feature of the autoregressive models causes this? Are there any fixes that the authors tried?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ggH2h76ur0", "forum": "DxKiXbjzbc", "replyto": "DxKiXbjzbc", "signatures": ["ICLR.cc/2026/Conference/Submission24122/Reviewer_6KuU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24122/Reviewer_6KuU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762496331525, "cdate": 1762496331525, "tmdate": 1762942945544, "mdate": 1762942945544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies on the synthetic task of Countdown to what degree RL training can induce skill composition. The primary contribution is the design of a specific task in which problems with different solution patterns exhibit different degrees of difficulty, irrespective of the total length. Carefully constructed RL experiments reveal that on the task of Countdown, length generalization is possible, but the order in which certain skills are learned, depends on the type of solution pattern."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- the paper is well presented and offers all or many details necessary for reproduction\n- the topic is very relevant (RL + LLMs + reasoning) and the paper provides insights into the learning dynamics and limitations which are currently scarce"}, "weaknesses": {"value": "- the experimental setup appears to be sound and convincing, albeit it could still be more comprehensive with an additional task besides Countdown\n- the SFT baseline is somewhat weak. I understand that having a strong SFT baseline is not the primary goal here, but it would be interesting to study some rejection fine-tuning settings in which one samples from an RLed model correct trajectories and filters them based on the specific distribution one wants to SFT on"}, "questions": {"value": "does increasing the proportion of right-heavy training examples also translate to a more balanced generalization gap? is there a correlation between the fraction of non-zero advantage batches for different patterns during training and the generalization performance of that pattern?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tWcXh8UmX9", "forum": "DxKiXbjzbc", "replyto": "DxKiXbjzbc", "signatures": ["ICLR.cc/2026/Conference/Submission24122/Reviewer_7q35"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24122/Reviewer_7q35"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762753563159, "cdate": 1762753563159, "tmdate": 1762942944961, "mdate": 1762942944961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}