{"id": "rMgSxzqBuk", "number": 7294, "cdate": 1758014553155, "mdate": 1763721503565, "content": {"title": "User-Controllable Dense Video Captioning: A Large-Scale Benchmark and Framework", "abstract": "Dense video captioning (DVC) aims to generate temporally localized captions for multiple events in untrimmed videos. Despite recent advances, existing methods still generate fixed captions because existing benchmarks provide only single-style annotations and methods for handling variations in event granularity and caption specificity remain unexplored. To address this gap, we present User-Controllable Captions (UC Captions), a new dataset with annotations that vary in event density (i.e., how frequently events are detected) and caption depth (i.e., the level of descriptive detail for a given event). This dataset is the first in DVC to explicitly encode controllable dimensions of annotation, establishing a foundation for studying user-driven flexibility. Building on this, we propose User-Controllable DVC (UC-DVC), a framework that incorporates user-defined density and depth parameters to dynamically adjust event localization and caption generation. Extensive experiments demonstrate that UC-DVC flexibly adapts to diverse user requirements while maintaining competitive performance on standard benchmarks. To support further research, both UC Captions and UC-DVC code will be publicly released after review.", "tldr": "We introduce a new dataset and framework for user-controllable dense video captioning. Our work allows users to dynamically control event density and caption depth, overcoming the fixed, single-style captions of existing benchmarks.", "keywords": ["Dense Video Captioning", "User-Control", "Synthetic Data Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5694fdbc628ca40b99e1279fe3cd8a5172be2afc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces UC Captions, a benchmark for user-controllable dense video captioning (DVC), and proposes the UC-DVC framework, which allows users to specify both event density and caption depth. The dataset extends existing DVC benchmarks with multi-level, continuous annotations, and the model incorporates user control via dedicated modules. Experiments show strong results on instructional video datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. System integration: The benchmark and framework are well-integrated and address a practical need for user-controllable video captioning.\n2. Experimental thoroughness (within DVC): The experiments are comprehensive for the DVC task, including ablations and human evaluations.\n3. Clarity: The paper is well-written and the methodology is transparent."}, "weaknesses": {"value": "1. Motivation unclear: The practical necessity and real-world demand for user-controllable dense captioning are not convincingly established. The paper lacks user studies, deployment evidence, or concrete scenarios demonstrating that end-users benefit from or desire explicit control over event density and caption depth. The usability of such controls for non-expert users is not discussed, and it is unclear how these parameters would be presented or utilized in real applications.\n2. Limited generalization: All experiments are restricted to instructional video datasets and the DVC task. There is no empirical evidence that the benchmark or model benefits other video understanding tasks (e.g., event detection, VQA, retrieval, temporal segmentation).\n3. Algorithmic novelty: The modeling and data generation techniques are incremental adaptations of existing ideas, not fundamentally new algorithms.\n4. Synthetic data risks: Heavy reliance on synthetic captions and knowledge graphs may introduce distributional artifacts or biases, which are not analyzed or addressed.\n5. Lack of robustness Analysis: The model’s robustness to noisy, ambiguous, or adversarial user controls is not evaluated.\n6. Unsubstantiated claims: The paper claims flexibility and broad applicability, but provides no experiments or case studies beyond DVC to support these assertions."}, "questions": {"value": "1. Motivation and usability: Can the authors provide evidence (e.g., user studies, deployment feedback, or real-world use cases) that explicit user control over event density and caption depth is needed or beneficial in practice? How would non-expert users interact with these controls in real applications?\n2. Broader applicability: Can the authors provide empirical evidence or at least a concrete case study showing how UC Captions or UC-DVC can benefit tasks beyond video captioning? For example, can the benchmark be used for event detection, VQA, or video retrieval? If not, the claims of generality should be toned down.\n3. Synthetic data bias: What steps have been taken to identify and mitigate biases or artifacts introduced by synthetic data and knowledge graphs? Are there measurable differences between synthetic and human-annotated captions?\n4. Robustness to user input: How does the model handle ambiguous, conflicting, or adversarial user controls? Are there failure cases?\n5. Comparison to other paradigms: How does UC-DVC compare to prompt-based or reinforcement learning-based controllable generation frameworks, both in flexibility and performance?\n6. Generalization to other domains: Have the authors attempted to apply the framework to non-instructional videos or more diverse datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vVA2Fiv83H", "forum": "rMgSxzqBuk", "replyto": "rMgSxzqBuk", "signatures": ["ICLR.cc/2026/Conference/Submission7294/Reviewer_e2hm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7294/Reviewer_e2hm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761836818376, "cdate": 1761836818376, "tmdate": 1762919414007, "mdate": 1762919414007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a key limitation of prior dense video captioning (DVC) systems: existing benchmarks provide only a single annotation style, so models can produce only fixed captions without user control over the level of detail. \nTo address this gap, the authors introduce UC Captions, a new dataset that provides captions with controllable variation along two dimensions: event density (how many events are detected) and caption depth (how detailed the descriptions are). By adjusting these dimensions, users can obtain captions of different granularity.\nThe authors also propose UC-DVC, a framework that conditions caption generation on user-specified density and depth parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation is clearly articulated: enabling controllable video captioning along two key dimensions—event density and caption depth.\n- The introduction of the UC Captions dataset, which provides multi-level annotations for both density and depth, could be useful for future research in this area.\n- Experiments show that UC-DVC performs strongly on standard DVC benchmarks and adapts well across multiple granularity configurations.\n- The paper includes thorough analyses of how varying density and depth affect model outputs."}, "weaknesses": {"value": "- It is unclear whether caption depth is truly reflected by surface measures such as word count. The paper does not provide an analysis on whether depth labels correlate with quantitative metrics (e.g., length, vocabulary richness), nor whether “deeper” captions provide meaningfully more information rather than just being longer.\n- The dataset construction involves automated filtering and refinement, but the paper provides limited discussion on quality control. It is not clear how the authors verify that the revised density and depth annotations are correct. In particular, for the evaluation in Table 3, it appears that the models are evaluated on the newly constructed dataset, where it is also unclear how the filtering process and quality are controlled."}, "questions": {"value": "- For density modeling, the MLP layer appears to take only d_in as input. If it is not conditioned on video features, wouldn’t this produce the same transformation for all videos?\n- In the training data, are the ground-truth control values limited to only three discrete levels (0, 0.5, 1)?\n- In Table 2, what does “our environment” refer to? What changes have you made from the original models?\n- The paper states that prior work lacks mechanisms to model variation in density and depth; however, it is still unclear why performance drops when training with UC Captions. Do you have intuition or analysis explaining this decline?\n- Event density and caption depth seem inherently related. Did you observe interactions between the two? For example, when density is high, it seems to be more difficult to maintain high caption depth, which could potentially increase hallucination."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PzRjQPP1YJ", "forum": "rMgSxzqBuk", "replyto": "rMgSxzqBuk", "signatures": ["ICLR.cc/2026/Conference/Submission7294/Reviewer_4Kqq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7294/Reviewer_4Kqq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860565432, "cdate": 1761860565432, "tmdate": 1762919413652, "mdate": 1762919413652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces User-Controllable Dense Video Captioning (UC-DVC) and a companion dataset, UC Captions, to let users control (i) event density (how many events are detected) and (ii) caption depth (how detailed each description is). The dataset expands YouCook2 and ViTT with a 3×3 grid of density×depth annotations via an automated pipeline (visual-similarity–based splitting/merging and a textual knowledge graph for multi-depth captions). The UC-DVC model adds explicit density and depth encoders plus two new losses (density-aware, depth-aware) to modulate event localization and caption detail. On YouCook2/ViTT, the approach improves METEOR/CIDEr/SODA c/F1 over strong baselines and, unlike baselines, covers all nine granularity settings with a single model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. User control over density and depth addresses practical needs, ranging from fast summaries to detailed descriptions. The paper provides a well-motivated analysis of this gap.\n\n2. A principled pipeline is proposed that defines levels and refines density through Clip Inter-/Intra-Median thresholds and MaxDiv (i.e., action count derived from captions), while depth is refined using a Textual Knowledge Graph to generate captions at low, medium, and high depth levels. Quantitative evaluations reveal expected trends: higher density leads to shorter segments, and higher depth corresponds to longer captions.\n\n3. The model introduces concise and minimal modifications—two control tokens and encoders—integrated into a Vid2Seq/HiCM2-style architecture. The loss functions are well-justified: the density-aware loss applies weights at timestamp tokens where target and reference densities differ, and the depth-aware loss supervises a saliency token through cross-attention to distinguish different levels of detail."}, "weaknesses": {"value": "1. The data generation pipeline appears computationally intensive and involves multiple model stages. The paper could benefit from a clearer discussion on reproducibility and computational cost implications.\n\n2. The experiments are limited to YouCook2 and ViTT. It remains uncertain whether UC-DVC generalizes to other domains (e.g., instructional, surveillance, or movie datasets) where event density and caption depth exhibit natural variability.\n\n3. Although the ablation studies (Tables 4 and 5) identify the contributions of key components, the paper lacks an in-depth analysis of the interaction between density and depth embeddings, as well as how the control parameters influence the latent representations."}, "questions": {"value": "1. How is annotation quality ensured during LLM/MLLM-based generation? Are there any filtering or confidence mechanisms in place to eliminate hallucinated or redundant captions?\n\n2. Can the proposed framework be extended to real-time or streaming scenarios, where density control must dynamically adapt as new frames are continuously received?\n\n3. How sensitive is UC-DVC to inaccuracies in the user-specified control values? Does minor noise in the density or depth inputs result in unstable or incoherent outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n3aIvffhep", "forum": "rMgSxzqBuk", "replyto": "rMgSxzqBuk", "signatures": ["ICLR.cc/2026/Conference/Submission7294/Reviewer_QKKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7294/Reviewer_QKKP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967181856, "cdate": 1761967181856, "tmdate": 1762919413250, "mdate": 1762919413250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a synthetic dataset, named UC Captions, for dense video captioning that enables user control over event density (frequency of events) and caption depth (level of detail). The dataset is constructed based on existing DVC datasets (YouCook2, ViTT), using MLLMs/LLMs to provide nine annotation types (3 density levels × 3 depth levels). The authors also propose UC-DVC, a baseline model that dynamically adjusts event localization and caption generation based on user-defined density and depth inputs. Comprehensive validation of the dataset's quality and the performance improvement for current DVC models is provided."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- I agree with the idea of enriching dense captions to address practical user needs, as different video applications focus on varying levels of semantic granularity. The motivation behind this approach is both natural and reasonable.\n- The organization and writing are generally clear.\n- The caption quality and distinctiveness of the semantic levels of the proposed dataset are validated by human evaluators and statistical metrics.\n- The qualitative visualization results demonstrate reasonable segments and captions."}, "weaknesses": {"value": "I did not see obvious weaknesses in this paper. Generally speaking, the current version of the paper has good motivation, clear organization, and sufficient empirical evidence to verify the effectiveness of the proposed dataset and method. One suggestion to enhance the paper's quality is to discuss its extensibility to LLM-based video captioning methods. As MLLMs have become the dominant models for mainstream video understanding tasks, their decoder-only structure is somewhat different from the encoder-decoder architectures in this paper. It would be useful to discuss how the proposed control mechanism could be adapted for MLLMs. Beyond structural differences, current MLLM models are dominated by language-based queries (controls) to inherit the language understanding abilities of LLMs. Intuitively, the proposed continuous value-based control is less generalizable than language-based control."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bjHdcXnr2L", "forum": "rMgSxzqBuk", "replyto": "rMgSxzqBuk", "signatures": ["ICLR.cc/2026/Conference/Submission7294/Reviewer_fkPC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7294/Reviewer_fkPC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7294/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111273273, "cdate": 1762111273273, "tmdate": 1762919412820, "mdate": 1762919412820, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response to all reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. We are encouraged that our motivation to address practical user needs through controllable dense video captioning is recognized as natural, reasonable, and clearly articulated (Reviewers fkPC, QKKP, 4Kqq, e2hm). We appreciate the positive assessments of our UC Captions dataset and generation pipeline, which reviewers found to be principled, well-justified, and useful for future research (Reviewers QKKP, 4Kqq). We are also glad that the reviewers highlighted the concise and effective system integration of our UC-DVC framework (Reviewers QKKP, e2hm), as well as the comprehensive experimental results and thorough analyses (Reviewers fkPC, 4Kqq, e2hm). We further thank Reviewers fkPC and e2hm for acknowledging the clarity and transparency of our writing and methodology.\n\nFollowing the reviewers' valuable suggestions, we have substantially revised our manuscript and added several new experiments and analyses.\n\n## Key Updates\n\n- **Extensibility to MLLM Architectures & Complementarity with Language-Based Control**: A discussion on how our control mechanism can be extended to decoder-only MLLMs and its complementarity with language-based control (Section 4.5).\n\n- **Caption Depth Analysis**: A rigorous validation of lexical richness (Unique Tokens) and semantic information (LLM-based evaluation) to prove that deeper captions provide meaningfully more information (Section 4.5 & Table 6).\n\n- **Computational Cost Analysis**: A detailed breakdown of dataset generation costs (Section 4.5 & Table 7).\n\n- **Generalization to Other Domains**: Qualitative visualization results demonstrating the model's adaptability to general domain videos beyond instructional datasets (Appendix G & Figure 12).\n\n- **Real-time Scenario Validation**: A visualization of the model's responsiveness in a streaming scenario where density control dynamically changes (Appendix G & Figure 13).\n\n- **Failure Case Analysis**: An analysis of representative error modes, such as over-segmentation and under-segmentation (Appendix G & Figure 14).\n\n- **Latent Space Analysis**: t-SNE visualization of the depth encoder representations to demonstrate the effectiveness of our control parameters (Appendix G & Figure 15).\n\nWe hope that these responses and revisions adequately address all reviewer concerns and further strengthen the contribution of our work."}}, "id": "xcu3Rgebcx", "forum": "rMgSxzqBuk", "replyto": "rMgSxzqBuk", "signatures": ["ICLR.cc/2026/Conference/Submission7294/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7294/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission7294/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763721054182, "cdate": 1763721054182, "tmdate": 1763721693216, "mdate": 1763721693216, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}