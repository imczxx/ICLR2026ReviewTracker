{"id": "zNT60EJgO9", "number": 25011, "cdate": 1758363121631, "mdate": 1763121447942, "content": {"title": "ERS*: A Bounded, Attribution-Agnostic Metric for Explainable Robustness in Image Recognition", "abstract": "Deep vision models can remain accurate under perturbations while shifting their internal reasoning, which is risky for safety-critical use. We introduce ERS*, a bounded metric (in [0,1]) for explainable robustness that jointly scores (i) normalized performance degradation and (ii) explanation stability between clean and perturbed inputs. Stability is computed across multiple attribution families (Grad-CAM/EigenCAM, attention rollout, LRP, RISE), and we define an ensemble-level attribution via probability-weighted fusion to evaluate ensembles directly. We study ViT-B/16, Swin-T, ResNet-50, and their soft-voting ensemble on a traffic-sign benchmark with ten calibrated physical perturbation suites (fading, dirt splatter, scratches, peeling/rust, etc.), and further demonstrate generality on natural corruption benchmarks beyond traffic signs (CIFAR-C, ImageNet-C). ERS* reveals cases where accuracy stays high but explanations become unstable, with ensembles sometimes achieving strong accuracy yet lower explanation stability than expected. Sensitivity analyses show ERS* rankings are stable across weight choices and attribution methods, and localization metrics plus a small human study indicate that higher ERS* aligns with perceived explanation quality. ERS* complements accuracy and standard robustness metrics (e.g., robust accuracy under corruption) by diagnosing explanation stability, providing a practical post-hoc tool for evaluating reliability and explainability in image recognition.", "tldr": "ERS* is a bounded, attribution-agnostic metric that combines performance degradation and explanation stability to expose when vision models and ensembles, stay accurate but reason inconsistently under real-world perturbations.", "keywords": ["Explainable Robustness Score", "attribution stability", "saliency maps", "Grad-CAM", "EigenCAM", "attention rollout", "LRP", "RISE", "ensemble attribution", "Vision Transformer", "Swin Transformer", "ResNet-50", "traffic sign recognition", "physical perturbations", "natural corruptions", "CIFAR-C", "ImageNet-C", "autonomous driving", "post-hoc evaluation", "bounded metric"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a0349cf5eadf4f470ad01d86b9b1a74610045a38.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce ERS*, an interpretability metric for quantifying the attribution robustness under perturbations. The perturbations used are not adversarial, but are instead more naturalistic, like glass cracks.  The authors claim the metric is attribution agnostic. The metric is composed of two main terms, with hyperparameters set to balance the terms. The first term is a ratio between the cross-entropy loss of the clean and perturbed image (clean / perturbed). The second term is a bounded similarity score between clean and perturbed attribution maps. The authors provide some experimental results on model accuracy under clean and perturbed settings, as well as values provided by their metric."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Originality - I am unaware of any other attribution papers that take into account natural perturbations, which adds to the paper novelty.\n- Quality - Poor. Figures need improvement and explanations need improvement. Multiple editing artifacts throughout the paper.\n- Clarity - Poor. I do not understand why the metric is composed the way it is or even what the output number represents.\n- Significance - Due to the poor writing of the paper, the significance is unclear."}, "weaknesses": {"value": "- The abstract and introduction contain too much overlap. There should be some overlap in concepts, but should not say exactly the same thing. \n- The motivation is not strong. The authors state that prior metrics do not check for whether a model is \"right for the right reasons.\" I agree that it would be nice to have a model be right for the right reasons, but as long as the model is making correct predictions, does it make a difference as to why it is correct?\n- The authors are using many small sections to compartmentalize different topics. For the sake of space and also good paper writing practice, it might be worth combining many of these subsections together and including text that allows the topics to naturally flow. Also, by not using as many subsections, the authors can save on paper length.\n- The authors copy-pasted sections 3.3, 3.4, 3.5, 3.6, 3.7 into sections 4.3, 4.4, 4.5. This is bad paper-writing practice. Do not duplicate sections. \n- The methodology is not explained well enough. I don't understand what it is measuring and why certain components are used. \n- The authors claim that the metric is attribution agnostic, but never provide any experiments with attributions other than LIME.\n- The authors say that they will include evaluations over Resnet, but never include those evaluations.\n- The authors never explain how the perturbations are created. A citation to where the perturbations were taken from is needed if they are from another source. Otherwise, the authors need to explain (algorithmically or with equations) how the perturbations are created. \n- Textual artifact on line 283\n- Section 6 is empty. Maybe a duplicate of Section 7 that the authors forgot to delete?\n- Paper is longer than 9 pages\n- The experimental evaluation does not help explain the significance of the work. Table 3 shows the ERS* metrics across these models, but the significance of these numbers are not explained and there are no significant patterns apparent to me. Overall, the explanations of the experiments are lacking. Please take the time to explain what each of the tables/figures shows and the significance of those observations."}, "questions": {"value": "- Why do we need to ensure that a model is making the correct predictions for the right reasons?\n- The whole methodology is extremely unclear in terms of why it is composed the way it is. Why are we taking the SSIM and MSE scores? Are there other similarity metrics that could've been used? Why are they being subtracted? What exactly is the stability term S' measuring? Why is ERS* using cross-entropy? Are there any other metrics that could've been used instead? Why is the cross-entropy ratio significant? What is the dev split used to calibrate lambda? How was lambda calibrated? What is exp() and why is it used? Why is epsilon added in to the ratio? Most importantly, how did the authors come up with either of these terms (S, L)? Is there a different formulation of the metric that might work? Why do we want the metric output to be bounded?\n- What is the significance of the results shown in Table 3? Please explain the table in greater detail. \n- Are there really no other methods or metrics that could've been compared against? Even if they aren't meant for exactly what ERS* is doing, it is still good to include some slant comparisons where the other metrics are failing to show that ERS* is doing something completely novel that others are not doing.\n- What is the Note on alignment with ERS* (394-397)?\n- How are the authors able to claim that the method generalizes to other attribution methods without providing results on the metric with other attribution methods?\n\nFinal Review: Based on the myriad of paper writing issues, it feels as if the authors submitted the paper in an unfinished state. Due to this, I cannot do anything but recommend a rejection (0/10)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I8dzEfmU93", "forum": "zNT60EJgO9", "replyto": "zNT60EJgO9", "signatures": ["ICLR.cc/2026/Conference/Submission25011/Reviewer_BbZP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25011/Reviewer_BbZP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761323588450, "cdate": 1761323588450, "tmdate": 1762943281889, "mdate": 1762943281889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "I want to extend this work and cover all things according to suggestions and submit again."}}, "id": "YgT5u7m4W4", "forum": "zNT60EJgO9", "replyto": "zNT60EJgO9", "signatures": ["ICLR.cc/2026/Conference/Submission25011/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25011/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763121364588, "cdate": 1763121364588, "tmdate": 1763121364588, "mdate": 1763121364588, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ERS*, a bounded, attribution-agnostic metric designed to evaluate the explainable robustness of image recognition models. The core idea is to measure how robust a model’s attributions (e.g., saliency maps, attention heatmaps) remain when subjected to adversarial perturbations, while ensuring the metric is normalized and comparable across models and attribution methods. The main contributions are a formal definition of explainable robustness that is attribution-agnostic — applicable across gradient-based, perturbation-based, and attention-based explainers, a bounded formulation that allows for fair comparison across models, avoiding unbounded metrics that depend on attribution magnitude or scale, and an evaluation framework (ERS*) with theoretical justification and empirical validation across multiple model architectures (CNNs and Vision Transformers) and attribution methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Norvel metric design. ERS* is an important step toward quantifying the stability of explanations, a long-standing issue in explainable AI. Its attribution-agnostic nature is particularly valuable, as most prior robustness metrics (e.g., sensitivity, infidelity) are tied to specific attribution algorithms.\n2. The normalization scheme ensures scores are comparable across methods and datasets, avoiding misleading absolute scale differences that plague other explainability metrics.\n3. The paper articulates and operationalizes the often-ignored concept of “explainable robustness”—robustness not just of predictions but of explanations themselves. This aligns with current concerns in Trustworthy AI, making the contribution timely."}, "weaknesses": {"value": "1. While the metric is theoretically motivated, there is no formal proof connecting ERS* to robustness bounds or causal faithfulness. The paper would be stronger with formal properties (e.g., invariance, monotonicity under perturbations).\n2. Limited scope of perturbations. The adversarial perturbations considered are pixel-space attacks. The framework does not evaluate robustness under semantic or geometric perturbations (e.g., translation, occlusion), which are critical for visual explainability.\n3. Although qualitative results suggest ERS* aligns with human perception, no human-subject evaluation or quantitative perceptual study is provided."}, "questions": {"value": "1. Have you conducted or considered a human study to verify that higher ERS* scores correspond to more human-consistent explanations?\n2. Could ERS* be extended to semantic-level or geometric perturbations (e.g., occlusions, viewpoint changes) rather than purely pixel-level ones?\n3. What is the computational cost of ERS* compared to metrics like Infidelity or Sensitivity-n? Is it feasible for large transformer-based models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uepXniwyt5", "forum": "zNT60EJgO9", "replyto": "zNT60EJgO9", "signatures": ["ICLR.cc/2026/Conference/Submission25011/Reviewer_ACxe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25011/Reviewer_ACxe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730108198, "cdate": 1761730108198, "tmdate": 1762943281680, "mdate": 1762943281680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ERS*, a bounded, attribution-agnostic metric for evaluating explainable robustness in image recognition tasks. This represents an original and timely contribution to the growing intersection of robustness and explainability. Instead of focusing solely on output accuracy under perturbations, ERS* integrates both performance degradation and explanation stability into a single, interpretable score"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The theoretical formulation is precise, equations are consistent, and implementation details (e.g., normalization procedures, bounding functions, and sensitivity analysis) are carefully specified.\nThe inclusion of sensitivity analyses over the ERS* weighting parameters (α, γ), bootstrap confidence intervals, and Kendall–τ ranking stability strengthens the reliability and interpretability of the metric.\nThe ensemble results are particularly insightful, revealing that accuracy gains can mask attribution instability"}, "weaknesses": {"value": "The theoretical foundation of ERS* focuses primarily on local, image-level perturbations and does not yet extend to global or temporal contexts,  the authors acknowledge that the metric does not claim causal faithfulness, but it would be useful to discuss how ERS* might generalize to dynamic or long-term perturbations.\n\nThe experimental scope is limited to a single dataset (Persian Traffic Sign Dataset). While this dataset is appropriate for physical perturbations, demonstrate ERS* on larger or more diverse datasets (e.g., ImageNet-C, CIFAR-10C, or autonomous driving datasets like BDD100K) .\n\nThe paper relies mainly on LIME for attribution generation. Although ERS* is designed to be attribution-agnostic, results using multiple attribution methods (e.g., Grad-CAM, Integrated Gradients, SHAP) would strengthen the claim of attribution independence. Including cross-method comparisons could also illuminate how explanation noise or attribution artifacts affect ERS*."}, "questions": {"value": "The ERS* metric is bounded between 0 and 1, combining performance degradation and attribution similarity. Could the authors provide more intuition or theoretical justification for the specific functional form of this bounding scheme?\n\nThe paper equates robustness in explanations with consistency across perturbations, but consistency does not necessarily imply causal faithfulness. Do the authors believe ERS* could be adapted or extended to capture causal robustness (e.g., invariance to non-causal perturbations)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "owpd18LrsB", "forum": "zNT60EJgO9", "replyto": "zNT60EJgO9", "signatures": ["ICLR.cc/2026/Conference/Submission25011/Reviewer_K3y2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25011/Reviewer_K3y2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094209371, "cdate": 1762094209371, "tmdate": 1762943281194, "mdate": 1762943281194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes ERS*, a bounded [0,1] score for explainable robustness that considers two aspects : (i) a normalized loss-based performance degradation term and (ii) an attribution-stability term comparing explanations on clean vs. perturbed inputs. This proposed metric is post-hoc and attribution-agnostic; the paper instantiates it with LIME and also defines a probability-weighted ensemble attribution so that ensembles can be evaluated directly. The study targets traffic-sign recognition under ten calibrated, physical perturbation suites (fading, dirt splatter, scratches, peeling/rust and their pairwise combos), evaluating ViT-B/16, Swin-T, and a soft-voting ensemble. ERS* shows areas where accuracy remains high but explanations shift, and shows that ensembles can mask backbone-level instability that accuracy alone would miss."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes ERS* which is a bounded  explainable robustness score with interpretable parts. ERS* combines a calibrated, bounded loss-degradation term with a normalized stability term and reports sensitivity over the trade-off weight that is useful for fair model ranking across datasets. \n- The metric is attribution-agnostic and ensemble-aware.\n- The paper shows realistic evaluation setting for traffic sign dataset, focussing on physical, non-differentiable perturbations and per-image pairing enables a practical lens for safety-critical perception other than gradient attacks."}, "weaknesses": {"value": "- The abstract/setup mention ResNet-50, but the main results tables show ViT, Swin, and an ensemble, with no clean/perturbed ERS reporting for ResNet. It would be great to eiither include these runs or adjust the scope statements.\n- Regarding the terminology and definitions, metrics like ASR appear without main-text definition; it would be great o ensure all acronyms are defined where first used and aligned with standard meanings.\n- There are some minor typo issues in the manuscript. There are reference/notation inconsistencies (e.g., ERS* equation numbering cross-refs, “ERS* 3” formatting) that should be cleaned to improve readability, and some fig ?? That are not linked\n- The related work sections feels a bit limited and would be great for readers to have more discussions about some of the attribute robustness techniques and gradient based attacks too (like Robust Attribution Regularization and Attributional Robustness Training using Input-Gradient Spatial Alignment) and evaluate these models using the proposed metric"}, "questions": {"value": "Please refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "n0fCg1jPky", "forum": "zNT60EJgO9", "replyto": "zNT60EJgO9", "signatures": ["ICLR.cc/2026/Conference/Submission25011/Reviewer_S7NR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25011/Reviewer_S7NR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25011/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762385739678, "cdate": 1762385739678, "tmdate": 1762943280919, "mdate": 1762943280919, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}