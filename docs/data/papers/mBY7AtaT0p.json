{"id": "mBY7AtaT0p", "number": 11575, "cdate": 1758201957892, "mdate": 1759897566948, "content": {"title": "HAM: Hierachical Adapters Merging for Scalable Continual Learning", "abstract": "Continual Learning allows models to acquire knowledge incrementally, but is challenged by catastrophic forgetting, a phenomenon in which the learning new tasks disrupts previously acquired knowledge.\nAlthough large pre-trained models can partially mitigate forgetting by leveraging their existing knowledge and over-parameterization, they often struggle when confronted with novel data distributions.\nParameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA, enable efficient adaptation to new data.\nHowever, they still face challenges in scaling to dynamic learning scenarios and long sequences of tasks, as maintaining one adapter per task introduces complexity and increases the potential for interference.\nIn this paper, we introduce Hierarchical Adapters Merging (HAM), a novel framework that dynamically combines adapters from different tasks during training.\nFor each experience, HAM trains a low-rank adapter along with an importance scalar, then dynamically groups tasks based on adapter similarity.\nWithin each group, adapters are pruned, scaled and merged, facilitating transfer learning between related tasks.\nExtensive experiments on three vision benchmarks demonstrate that HAM surpasses state-of-the-art methods, achieving up to 4\\% accuracy improvement over the best baseline and nearly doubling efficiency in both training and inference, with particularly strong advantages as the number of tasks increases.", "tldr": "HAM tackles catastrophic forgetting by dynamically grouping similar task adapters, concatenating within groups after pruning, and merging across groups , showing superior performance on long task sequences", "keywords": ["Continual Learning", "LoRA", "Model Merging", "Class Incremental Learning", "PEFT"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/039e74d10b9a3a8d52f965a8c66b9aea60ac82e0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces the Hierarchical Adapters Merging (HAM) framework, whose main goal is to mitigate forgetting in ViT models. The proposed method is sound, leveraging the idea of first training task-specific experts (adapters) for each task. Then, it groups the adapters via weight similarity to reduce computational cost. Finally, it merges all grouped adapters into a single module to alleviate the need for task identifiers in class-incremental learning settings. It shows promising results on several datasets compared to previous works."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Tackle the crucial problem of how to mitigate the forgetting when fine-tuning the transformer-based models.\n2. The proposed method is sound and logical. Which uses the concept of MoE and weight similarity to merge the weight, to reduce the computational cost while keeping the critical knowledge for each task.\n3. The method shows significant performance gain compared to previous works across 3 datasets."}, "weaknesses": {"value": "1. The testing dataset is limited. In the era of foundation models, testing only on CIFAR/CUB/Tiny-ImageNet seems underwhelming. Please at least add a larger-scale dataset such as ImageNet. Also, it would be much better if the authors could test it on large foundation models such as CLIP and other more complicated tasks, which would show that the proposed method can actually work in real-world use cases.\n\n2. This paper utilizes parameter isolation concept with subsequent weight merging. Since both MoE-like methods [1, 2, 3] and weight merging [4, 5, 6] are established concepts for continual learning, please include citations to these works and provide a concise discussion.\n\n[1] Aljundi, Rahaf, Punarjay Chakravarty, and Tinne Tuytelaars. \"Expert gate: Lifelong learning with a network of experts.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n[2] Chen, Hung-Jen, et al. \"Mitigating forgetting in online continual learning via instance-aware parameterization.\" Advances in Neural Information Processing Systems 33 (2020): 17466-17477.\n\n[3] Yu, Jiazuo, et al. \"Boosting continual learning of vision-language models via mixture-of-experts adapters.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[4] Mallya, Arun, and Svetlana Lazebnik. \"Packnet: Adding multiple tasks to a single network by iterative pruning.\" Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2018.\n\n[5] Serra, Joan, et al. \"Overcoming catastrophic forgetting with hard attention to the task.\" International conference on machine learning. PMLR, 2018.\n\n[6] Hung, Ching-Yi, et al. \"Compacting, picking and growing for unforgetting continual learning.\" Advances in neural information processing systems 32 (2019)."}, "questions": {"value": "1. The way of grouping adapter weight by just computing cosine similarity is surprisingly simple. Have the authors tried the representation-level similarity (e.g. CKA[7]) instead. It will be better if the authors can provide more qualitative result or analysis on does the grouping works as intended.\n2. The concept of merging all trained weights into a single module to alleviate the need for task identifiers is sound. But what if I want to continue training after the merging happens? Will the performance degrade severely since the previously learned knowledge all merges into a single module, which will be brittle? The author should conduct such experiments. This is crucial because if we can’t keep training the model after it is merged, it will defeat the concept of lifelong learning.\n\n[7] Kornblith, Simon, et al. \"Similarity of neural network representations revisited.\" International conference on machine learning. PMlR, 2019."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YvimYclDVC", "forum": "mBY7AtaT0p", "replyto": "mBY7AtaT0p", "signatures": ["ICLR.cc/2026/Conference/Submission11575/Reviewer_y2TX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11575/Reviewer_y2TX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810469669, "cdate": 1761810469669, "tmdate": 1762922662524, "mdate": 1762922662524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HAM (Hierarchical Adapters Merging), a novel continual learning framework that dynamically groups and merges LoRA adapters during training to mitigate catastrophic forgetting and enhance knowledge transfer across tasks. HAM trains task-specific LoRA adapters with importance scalars, clusters similar adapters, prunes less significant weights, and merges them hierarchically. The method is evaluated on three vision benchmarks (CIFAR-100, CUB-200, Tiny-ImageNet) and demonstrates superior performance, especially on long task sequences, achieving up to 4% higher accuracy and nearly double the training/inference efficiency compared to strong baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Provides a scalable solution for real-world continual learning, especially in long-task scenarios.\n+ Propose an interesting dynamically grouping method."}, "weaknesses": {"value": "+ The method is only evaluated in vision tasks; validation in NLP or multimodal settings is missing.\n+ The method is not evaluated on commonly used dataset like imagenet-r in pre-trained model based CL. The used CIFAR, CUB, etc. are included in the pre-trained data.\n+ During the inference time, existing lora-based cl method can be merged into pre-trained weights to achieve the same inference time as the pre-trained model. However, the proposed method may be not able to achieve this."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yjyvtbogBb", "forum": "mBY7AtaT0p", "replyto": "mBY7AtaT0p", "signatures": ["ICLR.cc/2026/Conference/Submission11575/Reviewer_2Ebs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11575/Reviewer_2Ebs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997332932, "cdate": 1761997332932, "tmdate": 1762922662098, "mdate": 1762922662098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes novel methods based on PEFT for continual learning. While there exist a lot of branches of methods for continual learnimg, most of them suffers from lack of plasticity if the network remains same size. Due to this, network expansion should be the most promising way of solving continual learning. However, they lack scalability since network should grow linearly with the number of tasks. In that sense, PEFT naturally gained attention to make network expansion scalable. This paper try to propose new PEFT based extension method which is more efficient than pre-existing algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Paper is well written and easy to follow. The authors well explained the motivation of each component and the existing related works. \n- The authors analyse and disassemble problem well and propose multiple components well aligned with these sub-problems."}, "weaknesses": {"value": "- Although the group number is fixed, if they simply concat the pruned matrices, it is equivalent to concatenating pruned matrices over all tasks. So if this is right, pruning the matrix would be the only component in this method which lacks novelty. Also the authors should compare with this simple approach. \n\n- Backbone is pretrained on ImageNet while the benchmarks seems to be the subset of ImageNet. This can not be fair comparison then.\n\n- Lack of experiments on large scale dataset. Tiny-Imagenet seems to be too small. A lot of existing literatures conducted experiments on at least ImageNet or ImageNet-R scale.\n\n- Lack of comparision with enough baselines. Most of them looks outdated and also the performances are even marginally improved to those.\n\n- Lack of demonstrations on experiments section. There are no demonstrations about how the authors split the task, select the baselines, and which refers to which paper."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PImbAVO0bs", "forum": "mBY7AtaT0p", "replyto": "mBY7AtaT0p", "signatures": ["ICLR.cc/2026/Conference/Submission11575/Reviewer_BE3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11575/Reviewer_BE3v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150868567, "cdate": 1762150868567, "tmdate": 1762922661306, "mdate": 1762922661306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Hierarchical Adapters Merging (HAM), a continual learning framework that enhances Low-Rank Adaptation (LoRA) by dynamically grouping and merging task adapters to prevent forgetting and improve knowledge transfer. By assigning importance weights and hierarchically combining related adapters, HAM achieves up to 4% higher accuracy, 9% less forgetting, and twice the efficiency of prior methods, offering a scalable solution for lifelong learning across many tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-structured and clearly written.\n\n2. The study addresses an important problem in continual learning using low-rank adaptation."}, "weaknesses": {"value": "1. The primary concern lies in the novelty of the proposed approach. The idea of Hierarchical Adapters Merging for LoRA appears similar to prior work, such as *TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree*. The authors should clarify the distinctions and emphasize the novel contributions of their method.\n\n2. The paper overlooks a significant body of research on continual learning (or continual fine-tuning) in large language models, including *O-LoRA*, *LoRA-MoE*, and *TreeLoRA*. A discussion comparing this work with these studies would strengthen the paper’s context and positioning.\n\n3. The experimental evaluation is primarily based on synthetic image datasets. It is recommended to include additional experiments on large language models (LLMs) to demonstrate the broader applicability and effectiveness of the proposed method.\n\nReferences:\n\n[O-LoRA] *Orthogonal Subspace Learning for Language Model Continual Learning*\n\n*LoRA-MoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin*\n\n*TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree*"}, "questions": {"value": "See Weaknesses above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TdB91AJwAV", "forum": "mBY7AtaT0p", "replyto": "mBY7AtaT0p", "signatures": ["ICLR.cc/2026/Conference/Submission11575/Reviewer_mqHm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11575/Reviewer_mqHm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11575/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762410761070, "cdate": 1762410761070, "tmdate": 1762922660844, "mdate": 1762922660844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}