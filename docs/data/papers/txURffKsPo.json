{"id": "txURffKsPo", "number": 17640, "cdate": 1758278698048, "mdate": 1759897163249, "content": {"title": "M3Kang: Evaluating Multilingual Multimodal Mathematical Reasoning in Vision-Language Models", "abstract": "Despite state-of-the-art vision-language models (VLMs) have demonstrated strong reasoning capabilities, their performance in multilingual mathematical reasoning remains underexplored, particularly when compared to human performance. To bridge this gap, we introduce M3Kang, the first highly multilingual, multimodal mathematical reasoning dataset for VLMs. It is derived from the Kangaroo Math Competition, the world’s largest mathematics contest, which annually engages over six million participants under the age of 18 across more than 90 countries. M3Kang includes 1,789 unique multiple-choice problems organized by grade-level difficulty, with translations into 15 culturally diverse languages, some of them including diagrams essential for solving them. Using this dataset, we conduct extensive benchmarking on both closed- and open-source SOTA models. We observe that, despite recent advances, models still struggle with basic math and diagram-based reasoning, with performance scaling with language presence and model size, but not with grade level. We also find that multilingual techniques can be effectively extended to the multimodal setting, resulting in significant improvements over baseline approaches. Our analysis also incorporates performance data from over 68,000 students, enabling direct comparison with human performance. We are open-sourcing M3Kang, including the English-only subset M2Kang, along with the framework and codebase used to construct the dataset.", "tldr": "We introduce M3Kang, the first highly multilingual, multimodal, mathematical reasoning dataset for VLMs, based on the Kangaroo Math Competition, and conduct extensive benchmarking on SOTA models.", "keywords": ["Benchmark", "Dataset", "Vision-Language Models", "VLM", "Mathematics", "Reasoning", "Multilingual"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f05db1e367efd01b3014be02ffd3471343948db6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes M3Kang, the first highly multilingual, graphically-based mathematical reasoning assessment dataset. This dataset draws 1,789 questions from the Kangaroo competition and expands to 15 languages, supported by an open automated translation and quality control pipeline. Benchmark results show that models significantly underperform text-only problems on questions containing diagrams. Cross-lingual performance is strongly correlated with internet coverage (more pronounced for smaller models). A simple \"text + English translation parallel prompt\" (MTR) approach is most effective in multimodal scenarios, significantly narrowing language gaps."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a multilingual, multimodal math reasoning benchmark across 15 languages (with an English subset), enabling fair cross-lingual comparison on a unified problem pool.\n2. This paper provides an open, rigorous translation and quality-control pipeline (reference-free backtranslation metrics, LLM-as-judge) that preserves layout and is easily reusable.\n3. This paper delivers comprehensive evaluations of open/closed VLMs and multilingual techniques, revealing a strong visual reasoning gap and establishing MTR as the most effective method; it also includes human comparisons with 68k students."}, "weaknesses": {"value": "1. Dataset and code links are inaccessible\n2. The dataset's statistical metrics are unclear. How many multimodal questions are there and how many are plain text?\n3. Can you provide a comparison with existing multilingual and multimodal datasets, perhaps in a table format?\n4. Regarding **sec. 4.4 Text-Only vs. FIGURE PROBLEMS**. The article observes that the accuracy rate for plain text questions is higher than for multimodal questions. Could this be because multimodal questions are inherently more difficult? I suggest adding a \"comparative experiment at the same difficulty level\" to make the argument more convincing."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "NNgRwIPesf", "forum": "txURffKsPo", "replyto": "txURffKsPo", "signatures": ["ICLR.cc/2026/Conference/Submission17640/Reviewer_yh29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17640/Reviewer_yh29"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761212872142, "cdate": 1761212872142, "tmdate": 1762927500155, "mdate": 1762927500155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Build a highly multilingual, multimodal benchmark (M3Kang) from Kangaroo Math problems; create an automated, quality-controlled translation pipeline; benchmark SOTA VLMs; test multilingual inference techniques in multimodal settings; compare to human performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- A multilingual and multimodal math benchmark with a reproducible pipeline.\n- This work offers a scalable data translation pipeline.\n- Comprehensive benchmarking across open and closed models."}, "weaknesses": {"value": "- Reliance on backtranslation may systematically disadvantage low-resource languages\n- Cross-language fairness relies on filtered subsets, limited statistical testing of comparability.\n- Some models (Gemma) perform below chance; analysis of why (prompting, vision adapters) is shallow."}, "questions": {"value": "- Provide exact prompts per language and the LLM-as-judge criteria and models used.\n- Clarify licensing and permissions for dataset redistribution.\n- Why not include Chinese?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "- Potential bias amplification across languages; risk of overclaiming language parity.\n- Licensing/consent: clarify rights to redistribute problem statements and images from Kangaroo materials."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IKakHk4zwl", "forum": "txURffKsPo", "replyto": "txURffKsPo", "signatures": ["ICLR.cc/2026/Conference/Submission17640/Reviewer_HBk9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17640/Reviewer_HBk9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761375322435, "cdate": 1761375322435, "tmdate": 1762927498720, "mdate": 1762927498720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces M3Kang, a multilingual and multimodal mathematical reasoning benchmark. The dataset includes both text-only and diagram-based questions organized by grade-level difficulty. The authors developed an automated translation pipeline with backtranslation-based quality control. \nExtensive benchmarking of open- and closed-source VLMs reveals key findings: models struggle with basic math and diagram-based reasoning, performance correlates with language Internet presence, Gemini-2.5-Pro leads among closed models. Additionally, direct comparison with student participants shows no significant correlation between VLM and human reasoning patterns, highlighting fundamental differences in problem-solving approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. M3Kang units multilingual, multimodal, and mathematical reasoning, enabling rigorous evaluation of VLMs.\n2. The study benchmarks a diverse set of models, compares text-only vs. diagram-based performance, tests multilingual techniques, and includes human baselines, offering insights into VLM capabilities and limitations.\n3. By leveraging real-world competition data and student performance, the benchmark has direct implications for educational AI development and multilingual model optimization."}, "weaknesses": {"value": "1. The automated translation pipeline may introduce uneven quality across languages, particularly low-resource ones, and human translation (though resource-intensive) is not explored as a refinement.\n2.  Without detailed classification of problem types (e.g., geometry, arithmetic, logical reasoning), it is difficult to pinpoint specific reasoning components where VLMs fail most frequently."}, "questions": {"value": "1. Given the translation quality disparities across resource levels, have you analyzed specific error types in low-resource languages, and how might these errors confound model performance evaluations?\n2. The study finds no significant correlation between VLM and human reasoning—do you hypothesize that this stems from differences in visual processing, mathematical intuition, or other factors, and how might future benchmarks better align with human problem-solving contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tBj9M9tq5q", "forum": "txURffKsPo", "replyto": "txURffKsPo", "signatures": ["ICLR.cc/2026/Conference/Submission17640/Reviewer_9m9j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17640/Reviewer_9m9j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929740323, "cdate": 1761929740323, "tmdate": 1762927498109, "mdate": 1762927498109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents M3Kang, a multilingual and multimodal mathematical reasoning benchmark derived from the Kangaroo Math Competition. The dataset contains 1,789 unique multiple-choice problems across 15 languages, spanning both text-only and figure-based questions. It provides a unified evaluation framework for Vision-Language Models (VLMs) in multilingual mathematical reasoning, with comparisons to over human participants. The authors design an automated three-stage pipeline: (1) extracting and cleaning Catalan problems, (2) translating them to English (M2Kang), and (3) extending to 15 languages with backtranslation-based quality filtering. Experiments benchmark major open and closed models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) M3Kang fills an important gap at the intersection of multilingual, multimodal, and mathematical reasoning. Prior datasets have addressed these dimensions separately; this benchmark enables joint evaluation.\n\n(2) The authors test about 10 VLMs, analyze correlations between accuracy and language Internet presence, compare text-only vs. figure-based questions, and benchmark multilingual reasoning methods. The analysis is thorough and supported by clear figures.\n\n(3) Using performance data from 68,000 students allows a rare and informative human–AI comparison."}, "weaknesses": {"value": "(1) Section 2 omits key multilingual multimodal datasets such as EXAMS-V (ACL 2024), M4U (2024), and M3Exam (NeurIPS 2024), mentioning only M5 (Schneider & Sitaram 2024) without detailed comparison. A table contrasting coverage, modality, and translation strategy would strengthen the contribution claim.\n\n(2) The dataset originates from Catalan, chosen for data availability rather than linguistic suitability. The paper does not analyze potential bias or LLM performance limits in Catalan processing.\n\n(3) Only the Catalan→English stage includes manual correction; multilingual translations rely solely on automatic quality metrics. A small human audit (even 100 samples) would help substantiate reliability."}, "questions": {"value": "In the experiments comparing text-only and figure-based problems, I did not receive the full methodological details. Such comparisons should ideally be conducted on the same set of questions, where the figures in the original problems are converted into equivalent textual descriptions for the text-only version. Please clarify the full details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UkdrHLxoXf", "forum": "txURffKsPo", "replyto": "txURffKsPo", "signatures": ["ICLR.cc/2026/Conference/Submission17640/Reviewer_zmss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17640/Reviewer_zmss"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17640/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983464646, "cdate": 1761983464646, "tmdate": 1762927497600, "mdate": 1762927497600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}