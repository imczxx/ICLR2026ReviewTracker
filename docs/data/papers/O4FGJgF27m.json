{"id": "O4FGJgF27m", "number": 1474, "cdate": 1756885599296, "mdate": 1759898207410, "content": {"title": "VCR: Variance-aware Channel Recalibration Network for Low Light Image with Distribution Alignment", "abstract": "Most sRGB-based LLIE methods suffer from entangled luminance and color, while the HSV color space offers insufficient decoupling at the cost of introducing significant red and black noise artifacts. Recently, the HVI color space has been proposed to address these limitations by enhancing color fidelity through chrominance polarization and intensity compression. However, existing methods could suffer from channel-level inconsistency between luminance and chrominance, and misaligned color distribution may lead to unnatural enhancement results. To address these challenges, we propose the Variance-aware Channel Recalibration Network for Low Light Image with Distribution Alignment (VCR), a novel framework for low-light image enhancement. VCR consists of two main components, including the Channel Adaptive Adjustment (CAA) module, which employs variance-guided feature filtering to enhance the model's focus on regions with high intensity and color distribution. And the Color Distribution Alignment (CDA) module, which enforces distribution alignment in the color feature space. These designs enhance perceptual quality under low-light conditions. Experimental results on several benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance compared with existing methods.", "tldr": "", "keywords": ["Channel Recalibration", "Distribution Alignment", "Image Enhancement"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cc8f8abdbce902ab84e58c792b3565e929233124.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Variance-aware Channel Recalibration Network (VCR), a novel framework for low-light image enhancement. VCR introduces two main components: the Channel Adaptive Adjustment (CAA) module, which applies variance-guided feature filtering to focus on regions with high intensity and color distribution, and the Color Distribution Alignment (CDA) module, which aligns chromatic feature distributions to learn more realistic color distribution. Experimental results on multiple benchmark datasets demonstrate that VCR achieves state-of-the-art performance, delivering visually consistent and perceptually improved results under low-light conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a variance-aware channel filtering approach that selectively masks channels with high-variance values, effectively enhancing luminance–chrominance consistency and perceptual quality under low-light conditions.\n\n\n- The proposed method demonstrates strong generalization performance on both real-world and unpaired datasets, showing its robustness and adaptability across diverse data distributions."}, "weaknesses": {"value": "- According to Figure 2, it appears that multiple stacked CAA modules are followed by a network, but it is unclear which network was used and what baseline model it is built upon.\n\n\n- In the TCE module, the relative importance or contribution of each branch is not well explained. a quantitative or qualitative analysis of branch importance would strengthen the paper.\n\n\n- Although the authors claim that the proposed method addresses misaligned color distributions, there is no supporting quantitative experiment beyond qualitative results. For example, comparing color histograms between ground-truth and enhanced images, as done in [1], would provide stronger evidence.\n\n\n- Line 462: Incorrect reference(Table 4->Table 3)\n\n[1] Learning Color Representations for Low-Light Image Enhancement, WACV 2022"}, "questions": {"value": "Please refer to Weaknesses,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n3Zv1asOA8", "forum": "O4FGJgF27m", "replyto": "O4FGJgF27m", "signatures": ["ICLR.cc/2026/Conference/Submission1474/Reviewer_vKx5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1474/Reviewer_vKx5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761041976444, "cdate": 1761041976444, "tmdate": 1762915778964, "mdate": 1762915778964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes VCR (Variance-Aware Channel Recalibration Network with Distribution Alignment), a novel framework for low-light image enhancement that aims to improve luminance–chrominance consistency and perceptual color fidelity. The method operates in the HVI color space, which decouples brightness and chromaticity more effectively than traditional RGB or HSV spaces. It introduces two key modules: the Channel Adaptive Adjustment (CAA) module, which includes a variance-aware channel filtering mechanism to suppress channels with inconsistent luminance–color variance and a triplet channel enhancement stage to strengthen inter-channel and spatial dependencies; and the Color Distribution Alignment (CDA) module, which enforces consistency between the enhanced image and real-scene references in the color feature distribution via KL divergence. By jointly optimizing reconstruction, variance filtering, and distribution alignment losses, VCR enhances both visual naturalness and structural fidelity under low-light conditions. Extensive experiments on ten benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance in terms of PSNR, SSIM, and perceptual metrics while maintaining reasonable computational efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The proposed framework combines variance-aware channel filtering and color distribution alignment in a clear and technically coherent way. The idea is sensible and easy to follow.\n\n2.Experiments are conducted on multiple benchmark datasets with quantitative and perceptual metrics. The results show consistent improvements over several existing LLIE methods.\n\n3.The paper is generally well written and structured, with clear explanations and informative figures that help readers understand the main ideas."}, "weaknesses": {"value": "1.While the paper introduces variance-aware filtering and color distribution alignment, these components mainly combine existing ideas such as channel attention and distribution matching. The contribution feels incremental.\n\n2.The paper lacks deeper theoretical justification for why variance-based filtering is the optimal approach for channel recalibration. The method is mostly empirical, and there is no ablation or analysis clarifying why variance is a better signal than mean or correlation-based metrics.\n\n3.Although several baselines are included, some recent competitive diffusion-based enhancement approaches are not compared. Including such methods would strengthen the claims of state-of-the-art performance.\n\n4.Some training and implementation aspects (e.g., architecture depth, exact hyperparameters for modules, or computational overhead of variance computation) are insufficiently described, making full reproducibility difficult.\n\n5.The paper does not adequately analyze failure cases or discuss when the proposed method might fail (e.g., extreme noise, mixed lighting). A more explicit discussion would help clarify the method’s boundaries and robustness.\n\n6.The paper does not provide source code or model checkpoints. Without public access to implementation details, it is difficult for others to reproduce the reported results or validate the claimed improvements."}, "questions": {"value": "1.Could the authors better clarify what makes the proposed variance-aware channel recalibration fundamentally different from prior channel attention or feature modulation mechanisms? It would help to explicitly highlight which parts are novel in terms of formulation or learning behavior beyond existing channel-attention frameworks.\n\n2.Please provide more theoretical or intuitive justification for using variance as the key statistic for channel filtering. Why is variance a better indicator of luminance–chrominance consistency than other measures such as covariance, correlation, or entropy? An additional analysis or visualization (e.g., sensitivity plots or variance–performance correlation) would make this design choice more convincing.\n\n3.Have the authors considered comparing VCR against recent diffusion-based or transformer–diffusion hybrid LLIE models? Including such baselines (e.g., diffusion prior–based image enhancement) would make the claimed SOTA results more comprehensive and credible.\n\n4.Could the authors provide more details about the model configuration, such as the number of CAA modules, channel dimensions, and computational overhead of the variance filtering step? A table summarizing complexity and runtime would improve clarity.\n\n5.It would strengthen the paper if the authors could analyze cases where the model fails, such as extremely noisy or mixed lighting scenes, and then discuss possible mitigation strategies. Have they considered integrating explicit noise modeling or adaptive illumination priors?\n\n6.Do the authors plan to release the source code and pretrained models after acceptance? Public access to implementation details and training scripts would significantly enhance the paper’s credibility and facilitate community validation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tj8Ns0I4wH", "forum": "O4FGJgF27m", "replyto": "O4FGJgF27m", "signatures": ["ICLR.cc/2026/Conference/Submission1474/Reviewer_dzLQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1474/Reviewer_dzLQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761462229271, "cdate": 1761462229271, "tmdate": 1762915778777, "mdate": 1762915778777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method for low-light image enhancement, named Variance-aware Channel Recalibration Network (VCR). The method first transforms the input image into the HVI color space to decouple luminance and chrominance. The core of the proposed framework is the Channel Adaptive Adjustment (CAA) module, which consists of a Variance-aware Channel Filtering (VCF) stage to suppress inconsistent features and a Triplet Channel Enhancement (TCE) stage to model cross-dimensional dependencies. Furthermore, a Color Distribution Alignment (CDA) module is introduced to align the chrominance distribution of the enhanced result with that of the ground truth using a Kullback-Leibler divergence loss on softmax-normalized features. The authors conduct experiments on ten benchmark datasets and claim that their method achieves state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The proposed framework is well-structured, and the paper is generally easy to follow.\n\n2. The authors have performed an extensive set of experiments on numerous paired and unpaired datasets to validate their method, which is commendable."}, "weaknesses": {"value": "1.  **Limited Novelty and Unnecessary Elaboration on Existing Concepts:** A significant portion of the methodology section is dedicated to describing pre-existing work.\n    *   **Section 3.1 (HVI Color Space):** This section provides a lengthy explanation of the HVI color space, which was proposed in a prior work (Yan et al., 2025). The authors do not appear to introduce any modifications or improvements to the color space itself. This part could have been significantly condensed and presented as a preliminary, simply citing the original work.\n    *   **Section 3.2.2 (Triplet Channel Enhancement):** The TCE module, which permutes feature dimensions to capture cross-dimensional interactions, lacks clear novelty. This design pattern of using multi-branch structures with rotated feature maps to compute attention is highly reminiscent of existing attention mechanisms, such as Triplet Attention (Misra et al., 2021) and Coordinate Attention (Hou et al., 2021). The paper fails to discuss these related works, making the claimed contribution in this area appear incremental.\n\n2.  **Unclear Motivation for Covariance Constraint:** In Section 3.2.1, the VCF stage proposes to constrain the cross-covariance matrix of intensity (I) and chromaticity (HV) features by penalizing its deviation from the mean of the two covariance matrices. The theoretical motivation behind this design is not well-explained. It is unclear why enforcing the cross-covariance to be close to the average of individual covariances is a principled approach to ensuring \"distributional consistency\" and suppressing artifacts. A more rigorous justification or theoretical analysis is needed to support this core component of the method.\n\n3.  **Questionable Design of the Color Distribution Alignment (CDA) Loss:** The CDA loss aligns feature distributions by minimizing the KL divergence between temperature-scaled softmax outputs of the enhanced and ground-truth features. However, the softmax function is inherently sparse; it amplifies the highest values while suppressing the vast majority of other activations to near zero. Consequently, the gradient signal from this loss would be dominated by a few select spatial locations. This seems counter-intuitive for aligning color, which is predominantly a low-frequency signal that requires a global and dense understanding of the image. The value of such a sparse gradient for enforcing holistic color consistency is questionable.\n\n4.  **Unconvincing Qualitative Results and Flawed Presentation:** The visual results presented in Figure 4 are not compelling and, in some cases, are of poor quality.\n    *   **Visual Artifacts:** In the VV dataset example, the lighting on the person's face is uneven, and the sky appears overexposed. In the DICM example, the underexposure in the shadows of the trees remains a significant issue. Similarly, the crowd in the MEF example is still largely underexposed.\n    *   **Presentation Style:** The presentation format of Figure 4 is highly discouraged as it suggests \"cherry-picking.\" Each example compares the proposed method against only one other, different competitor. This prevents a fair, holistic assessment of the method's performance against the state-of-the-art. Even with this selective comparison, the proposed method fails to show a clear advantage. The visual evidence provided is insufficient to support the claim of superior performance."}, "questions": {"value": "1.  Could the authors clarify the novelty of the TCE module in light of existing works on cross-dimensional attention, such as Triplet Attention? A detailed comparison with these methods would strengthen the paper.\n\n2.  What is the theoretical justification for the covariance constraint in the VCF module? Could the authors provide a more in-depth explanation or ablation study to demonstrate why this specific formulation is effective for improving low-light enhancement?\n\n3.  Regarding the CDA loss, how does the sparse gradient generated by the softmax function effectively guide the alignment of spatially smooth, low-frequency information like color distributions? Have the authors experimented with other distribution metrics (e.g., sliced Wasserstein distance) that might be better suited for this task?\n\n4.  The qualitative comparisons in Figure 4 are presented in a way that makes direct and fair comparison difficult. Could the authors provide a revised figure that compares their method against all key competitors on the same set of images? Furthermore, can they explain the visual artifacts (uneven lighting, over/under-exposure) mentioned in the weaknesses section?\n\n5.  The test images used for comprehensive comparison in the supplementary material appear to be of moderate difficulty. For a more convincing evaluation, I would suggest including results on more challenging cases during the rebuttal phase. Examples could include images `06, 10, 21, 27, 30` from the DICM dataset; `Farmhouse, Venice` from the MEF dataset; and `P1010234, P1010676, P1010880` from the VV dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5Ek0FTYUNI", "forum": "O4FGJgF27m", "replyto": "O4FGJgF27m", "signatures": ["ICLR.cc/2026/Conference/Submission1474/Reviewer_hDZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1474/Reviewer_hDZR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917686470, "cdate": 1761917686470, "tmdate": 1762915778491, "mdate": 1762915778491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a novel framework for low-light image enhancement. This framework consists of a Channel Adaptive Adjustment (CAA) module and a Color Distribution Alignment (CDA) module. These designs enhance perceptual quality under low-light conditions. Experimental results on several benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance compared with existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This manuscript proposes a Variance-aware Channel Recalibration Network for Low-Light Image with Distribution Alignment.\n2. A Channel Adaptive Adjustment (CAA) module is introduced to filter and enhance light and chrominance features at the channel level. \n3. A Color Distribution Alignment (CDA) module is proposed to enforce consistency in the color feature distribution, leading to clearer and more natural results."}, "weaknesses": {"value": "1. The paper exhibits limited innovation, as its core innovative points bear striking similarities to those of CIDNet. In particular, the proposed VCF (Variance-aware Channel Filtering Stage) module closely resembles the LCA (Lighten Cross-Attention) module in CIDNet. \n2. Comparative analysis of enhancement results for SID is lacking.\n3. The ablation experiments are incomplete and thus fail to effectively evaluate the validity of the modules proposed in this paper."}, "questions": {"value": "1. Why are the objective comparison results on the SID dataset provided, while the visual comparison of the enhanced results is not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6iKur8me4c", "forum": "O4FGJgF27m", "replyto": "O4FGJgF27m", "signatures": ["ICLR.cc/2026/Conference/Submission1474/Reviewer_UKY5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1474/Reviewer_UKY5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978549163, "cdate": 1761978549163, "tmdate": 1762915778167, "mdate": 1762915778167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}