{"id": "FlcMckO6x5", "number": 2048, "cdate": 1756981809079, "mdate": 1763530805960, "content": {"title": "Separable Neural Networks: Approximation Theory, NTK Regime, and Preconditioned Gradient Descent", "abstract": "Separable neural networks (SepNNs) are emerging neural architectures that significantly reduce computational costs by factorizing a multivariate function into linear combinations of univariate functions, benefiting downstream applications such as implicit neural representations (INRs) and physics-informed neural networks (PINNs). However, fundamental theoretical analysis for SepNN, including detailed representation capacity and spectral bias characterization \\& alleviation, remains unexplored. This work makes three key contributions to theoretically understanding and improving SepNN. First, using Weierstrass-based approximation and universal approximation theory, we prove that SepNN can approximate any multivariate function with arbitrary precision, confirming its representation completeness. Second, we derive the neural tangent kernel (NTK) regimes for SepNN, showing that the NTK of infinite-width SepNN converges to a deterministic (or random) kernel under infinite (or fixed) decomposition rank, with corresponding convergence and spectral bias characterization. Third, we propose an efficient separable preconditioned gradient descent (SepPGD) for optimizing SepNN, which alleviates the spectral bias of SepNN by provably adjusting its NTK spectrum. The SepPGD enjoys an efficient $\\mathcal{O}(nD)$ complexity for $n^D$ training samples, which is much more efficient than previous neural network PGD methods. Extensive experiments for kernel ridge regression, image and surface representation using INRs, and numerical PDEs using PINNs validate the efficiency of SepNN and the effectiveness of SepPGD for alleviating spectral bias.", "tldr": "We derive theories and propose an efficient preconditioned gradient descent for separable neural networks.", "keywords": ["Separable Neural Networks", "Approximation Theory", "Preconditioned Gradient Descent", "Neural Tangent Kernel"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6644a53f71e220f3f0e5139d84002ab04b1cfb7.pdf", "supplementary_material": "/attachment/e2e9dbb3410ac3854630a5f2c4698a01bd266b73.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies real valued separable neural networks (SepNNs), an alternative architecture which combines some univariate functions linearly to represent multivariate functions. SepNNs combine univariate factor functions $f_{\\theta_i}(x_i)$, with linearity $L$ (equation 1). Authors considered three cases of this $L$ function, CP, TT, Tucker.\nThe paper build up on three claims:\n1. Similar universal approximation guarantees of neural networks in terms of expressivity for SepNNs.\n2. Analyzing neural tangent kernels in the case where rank or width goes to infinity and it establishes that under infinite width and rank, NTK converges to a deterministic kernel and under infinite width and finite fixed rank it converges to a random kernel\n3. Using theoretical analysis of NTK they propose a training algorithm (SepPGD) that reshapes the spectrum with O(nD) preconditioning cost on n grids per axis reaching faster convergence in tasks KRR, INRs, PINNs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The most interesting thing about this paper is how they use their theoretical analysis on optimization of SepNNs and the limitations they found to come up with a prescription as a training algorithm.\n\nTheir universal approximation theorem nicely shows the expressivity power of SepNNs. Also, the proof sketch is nicely and understandably stated in the main body and it’s intuitive enough for readers. (Theorem 1).\n\nThey use NTK to study optimization regimes under asymptotic assumptions (Theorem2, lemma1) and they characterize when training dynamics are kernel-like vs. stochastic.\n\nThey use spectral bias already known results to argue that SepNN suffers from a similar problem, this is not particularly novel but the discussion of spectral bias and how they propose a new training algorithm inspired by similar works for SepNN is nice.\n\nThe training algorithm (SepPGD) is a slightly complicated but good-sounding idea. The authors support this algorithm with nice theoretical analysis showing equivalence to full NTK-PGD step with smoothened eigenvalue spectrum which improves convergence."}, "weaknesses": {"value": "In terms of motivation, I found the introduction to be a bit lacking. The authors don’t clearly discuss the advantages of SepNNs.\nMore thoroughly, the motivation for SepNNs is not well positioned against established efficient architectures. The paper frames SepNNs as a new paradigm but does not convincingly argue why this approach matters beyond reinterpreting classical separability. Without this context, the work risks appearing incremental from the perspective of practical ML.\n\nThe authors mentioned computational complexity: the paper mentions that SepNNs need only $$O(nD) $$ computations (n being number of data and D being dimension). They compare it to computational complexity of NN being $$O(n^D)$. This is true in terms of unique function evaluations but is does not imply faster training since regular models for example a CNN do their computation of all the dimension D in parallel. My impression is that SepNNs’ benefits are limited to high dimensional, low sample problems, with structured scientific problems in terms of learnable parameters and memory efficiency; but the authors don’t discuss that.\n\nTheorem 1, does not say anything about rank, for example there’s no rate describing rank R must grow with target smoothness of dimension or something similar to that. This limits the theorem's relevance for practical rank selection.\n\nThe paper lacks theory for fixed-rank regimes, which is the setting that actually matters in practice. The entire motivation mentioned for using SepNNs is to have low rank structure to achieve parameter efficiency, scalability and interpretability, but the theory for this is limited in terms of convergence guarantees, or bound for generalization errors.\n\nThe random NTK in the fixed rank case is mathematically similar to random features maps. It would be nice for authors to make this connection and use the well-studied random feature maps. This can address the lack between asymptotic assumptions in theorem and finite rank networks used in experiments.\n\nThe preconditioner choices appear to be heuristics and there is no discussion/guarantee on what their condition number might be, or their possible effect on generalization. \n\nThe experiment focuses on grid structured problems; adding other experiments, with non-grid tasks (e.g. tabular inputs) might strengthen things. This is just a suggestion, rather than a requirement.\n\nSome reproducibility details are missing.\n\nSome experiments:\nAblations study on sensitivity to rank, also k in the eigen-leveling and update frequency, choice of activation and width.\nSome baseline are missing specially fourier feature maps.\nSome experiments on robustness to noise and overfitting behaviors of SepPGD \n\nThe claimed efficiency of SepPGD neglects the high upfront cost of constructing the preconditioner. The claim of $O(nD)$ refers to applying the preconditioner while constructing them seems expensive. Also it requires a lot of memory to store these pre-conditioned matrices $O(Dn^2)$ which makes the method not scalable for a higher amount of data.\n\nWhat if the target is not well-separable? It would be nice to have approximation-error curves vs true separation rank of the target (synthetics with controllable TT/Tucker rank). Or some similar attempt to address the real-world misspecification gap."}, "questions": {"value": "I find the definition of TT and Tucker not well placed in the article and equation 2, 3 lacks some notations, since it’s not obvious what are R and the tensor product and what are $f_{\\theta_i}_{i_{i-1}, r_i}$, and $C$, maybe they can be moved to theorem 1 ot a preliminaries section.\n\nThe claim in remark 1 is not obvious to me, how can the results be extended to multi layers?\n\nLine 337 is incomplete, there seems to be a missing sentence.\n\nWhat is the computational cost for calculating the preconditioner?\n\nIn the fixed rank regime is it possible to bound $$||K(t) - K(0)|| over finite time with small step sizes?\n\nHow sensitive is SepPHD to the choice of k?\n\nCould Lemma 2 be extended to non-grid inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zCG7SIsr3X", "forum": "FlcMckO6x5", "replyto": "FlcMckO6x5", "signatures": ["ICLR.cc/2026/Conference/Submission2048/Reviewer_WM7X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2048/Reviewer_WM7X"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761094366086, "cdate": 1761094366086, "tmdate": 1762916005200, "mdate": 1762916005200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work makes contributions to theoretically understanding SepNN. First, using Weierstrass-based approximation and universal approximation theory, they prove that SepNN can approximate any multivariate function with arbitrary precision, confirming its representation completeness. Second, they derive the neural tangent kernel (NTK) regimes for SepNN, showing that the NTK of infinite-width SepNN converges to a deterministic (or random) kernel under infinite (or fixed) decomposition rank, with corresponding convergence and spectral bias characterization. Third, they propose an efficient separable preconditioned gradient descent (SepPGD) for optimizing SepNN, which alleviates the spectral bias of SepNN by provably adjusting its NTK spectrum. The SepPGD enjoys an efficient order of complexity and is much more efficient than previous neural network PGD methods. Extensive experiments for kernel ridge regression, image and surface representation using INRs, and numerical PDEs using PINNs validate the efficiency of SepNN and the effectiveness of SepPGD for alleviating spectral bias."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written. Ideas are well presented."}, "weaknesses": {"value": "No obvious weakness."}, "questions": {"value": "No question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Kiy1zfptz2", "forum": "FlcMckO6x5", "replyto": "FlcMckO6x5", "signatures": ["ICLR.cc/2026/Conference/Submission2048/Reviewer_3E8r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2048/Reviewer_3E8r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524522582, "cdate": 1761524522582, "tmdate": 1762916005015, "mdate": 1762916005015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a comprehensive theoretical and algorithmic framework for Separable Neural Networks (SepNNs), a class of efficient models that factorize a multivariate function into a combination of univariate functions. The main contributions are:\n1. Approximation Theory:Proving that SepNNs are universal approximators, capable of approximating any continuous multivariate function with arbitrary precision.\n2. NTK Analysis:Deriving the Neural Tangent Kernel (NTK) for SepNNs, characterizing their training dynamics and inherent spectral bias.\n3. Efficient Algorithm:Proposing a novel Separable Preconditioned Gradient Descent (SepPGD)method that effectively alleviates spectral bias with significantly lower computational cost than existing preconditioning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors performed an analysis of the sepNN parallel to the fully-connected neural networks. They built the uniform approximation theorem, the NTK regime, and the gradient descent."}, "weaknesses": {"value": "My major concern with this work is that I fail to see the potential impact of developing this theory for separable neural networks (SepNNs). After several years of developments in NTK research, this type of work feels somewhat routine.\n\nIf the author could explain a little bit on the potential application and the near future goal of this theory, I would be happy to increase the score."}, "questions": {"value": "Same to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xqU5wdSOkC", "forum": "FlcMckO6x5", "replyto": "FlcMckO6x5", "signatures": ["ICLR.cc/2026/Conference/Submission2048/Reviewer_A9Bg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2048/Reviewer_A9Bg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991630236, "cdate": 1761991630236, "tmdate": 1762916004771, "mdate": 1762916004771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper begins by establishing the universal approximation theory for three classes of separable neural networks (SepNNs): the CP type, TT type, and Tucker type. The proofs are built on classical results (Stone-Weierstrass, universal approximation theorems) and well-motivated in the context of separable function spaces. The results extend beyond the bivariate approximation in (Cho et al., 2023) to general D-variate functions.\n\nFurther, the paper prove that, under the asymptotic regime of infinite network width and infinite rank, the NTK of a CP type SepNN converges to a deterministic kernel, provide an analogous result to that of a standard MLP. In contrast, under infinite width and fixed rank, the NTK converges to a stochastic kernel defined via Gaussian processes.\n\nFinally, the paper introduces the SepPGD training algorithm that exploits the Kronecker product structure of SepNN NTKs to reduce computational complexity from $\\mathcal{O}(n^D)$ to $\\mathcal{O}(nD)$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. To the best of my knowledge, this is the first work to formally establish the universal approximation theorem for SepNNs, including CP, TT, and Tucker variants. Although the result may not be entirely surprising given known approximation capabilities of neural networks, it fills a gap in the theoretical understanding of SepNNs.\n\n2. I find the introduction of the SepPGD algorithm to be a significant contribution to the application of SepNN, even though no theoretica guarantees is given, its efficiency is demonstrated through various numerical experiments."}, "weaknesses": {"value": "1. While the universal approximation theory for SepNNs is novel, the paper does not provide explicit approximation error rates as a function of network size. In particular, it does not show how the approximation error scales with respect to rank or width.\n2. The analysis of the NTK is restricted to the CP-type SepNN, whereas analogous results for TT and Tucker types are not provided."}, "questions": {"value": "I have some additional questions:\n\n1. The manuscript briefly mentions that SepNN is closely related to the Kolmogorov-Arnold network (KAN). Since KAN is a timely and widely-discussed topic these days, could the authors elaborate more on this connection? Specifically, can KAN be regarded as a special caseof a SepNN under the definitions used in this paper? If so, how do the function composition structures in KAN (i.e., sums of univariate functions composed with multivariate linear transforms) relate to the separable architectures (e.g., CP, TT, Tucker) defined here?\n\n2. I may have missed this in the manuscript, but how does SepPGD perform with varying eigenvalue modulation functions $g(\\lambda)$? Is the choice critical?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rTOLG2A9xM", "forum": "FlcMckO6x5", "replyto": "FlcMckO6x5", "signatures": ["ICLR.cc/2026/Conference/Submission2048/Reviewer_dMQY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2048/Reviewer_dMQY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998322271, "cdate": 1761998322271, "tmdate": 1762916004302, "mdate": 1762916004302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "We have uploaded a revised manuscript"}, "comment": {"value": "Dear reviewers,\n\nIn response to your valuable suggestions, we have revised the manuscript accordingly and provided additional clarifications in the **uploaded revised manuscript**. These include further clarifications of the potential applications of our work, motivations of SepNNs, more complete definitions of notations, and additional computational complexity analysis.\n\nDue to space constraints, we have also included extended discussions in Section A.1 of the manuscript Appendix, including more discussions on the efficiency advantages of SepNNs, discussions on theoretical analyses and future directions, relation to KAN, hyperparameter sensitivity analysis, and experiments on non-grid inputs.\n\nFor easy reference, all revisions have been highlighted in blue in the revised manuscript. Should you need further information, please let us know. We sincerely appreciate your time and valuable efforts!"}}, "id": "WVS1lgyrTC", "forum": "FlcMckO6x5", "replyto": "FlcMckO6x5", "signatures": ["ICLR.cc/2026/Conference/Submission2048/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2048/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission2048/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763538212668, "cdate": 1763538212668, "tmdate": 1763538212668, "mdate": 1763538212668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}