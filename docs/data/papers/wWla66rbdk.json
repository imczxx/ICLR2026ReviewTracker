{"id": "wWla66rbdk", "number": 6280, "cdate": 1757964260832, "mdate": 1762920207784, "content": {"title": "EviDiff: Learning Object-wise Consistency for Text-to-Image Diffusion", "abstract": "Consistency constraint between the text prompts and the image contents is pivotal in text-to-image (T2I) diffusion models for composing multiple object categories. However, such consistency constraint is often underemphasized in the denoising process of diffusion models. Although token supervised diffusion models can mitigate this issue by learning object-wise consistency between the image content and object segmentation maps, it tends to suffer from the problems of segmentation map bias and semantic overlap conflict, especially when involving multiple objects. To address this, we propose EviDiff, a new evidential learning-supervised T2I diffusion model, which leverages the advantages of uncertainty metric and conflict detection to enhance the fault tolerance of unreliable segmentation maps and suppress semantic conflicts, strengthening object-wise consistency learning. Specifically, a pixel evidence loss is proposed to restrain overconfidence in unreliable labels through evidential regularization, and a token conflict loss is designed to weaken the contradiction between semantics through optimizing a measured conflict factor. Extensive experiments show that our EviDiff outperforms state-of-the-art T2I diffusion models in multi-object compositional generation without requiring additional inference-time manipulations. Notably, our EviDiff can be seamlessly extended to the existing training pipeline of T2I diffusion models. The code and the trained EviDiff model are available at https://github.com/anonymity-coder/EviDiff.", "tldr": "An evidential learning driven T2I diffusion model for efficient multi-object composition.", "keywords": ["Text-to-image composition", "Diffusion model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/130fc97acfbe846c7acc7f0731c43e4b9e5ed808.pdf", "supplementary_material": "/attachment/5a833463bf254af48b85888a0b296d8466fb53e3.zip"}, "replies": [{"content": {"summary": {"value": "* The paper introduces EviDiff, a novel T2I model trained with evidential learning for improved object consistency.\n  * The authors propose a pixel evidence loss to manage overconfidence in unreliable segmentation labels by simultaneously considering pixel-level classification prediction and its associated uncertainty estimation.\n  * The authors design a token conflict loss is designed to weaken semantic overlap conflict by using a conflict factor, measured via the DST combination rule, directly as the optimization objective.\n* The authors proved the effectiveness of the proposed EviDiff method by showing it outperforms multiple baseline models and previously proposed training approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed EviDiff method is a novel of Evidential Deep Learning (EDL) and Dempster-Shafer Theory (DST) to a T2I diffusion model, which is a highly original approach for handling compositional conflicts.\n* The proposed pixel evidence loss targes the issue of segmentation map bias by managing overconfidence in unreliable labels through uncertainty estimation.\n* The token conflict loss optimizes against the semantic overlap conflict to tackle this common problem in multi-object compositional generation.\n* Empirical Results show that EviDiff outperform multiple baselines and previous works.\n* EviDiff can be extended to existing T2I diffusion model training pipelines without requiring additional inference-time manipulations."}, "weaknesses": {"value": "* **Empirical Improvements seem trivial.** Improvements on most quantitative metrics seem trivial (e.g. MG3, MG4, CLIP(C), FID(F), FID(C) metrics in Table 2) and sometimes falling behind previous methods (e.g. MG2, MG5 in Table 2). It is hard to justify that the proposed method brings significant enough contributions.\n* **Limited Model Architecture Studied**. Experiments were only conducted on the older generation of T2I models like SD 1.4, SD 2.1, and SD 3. Newer models like SD3.5, FLUX, and Qwen-Image are not explored.\n  * One related point is that, with the stronger generation of T2I models coming out (especially closed-source ones like gpt-image-1 and nano-banana), the object consistency setting studied in this paper might have been fully resolved for newer models. Consistency in more complex generation settings (e.g. multiple objects with different spatial and appearance attributes) might still be challenging, but it is out of the scope of this study, too."}, "questions": {"value": "* How would your method perform when applied to newer models like FLUX, UniWorld and Qwen-Image?\n* How well do more recent T2I models perform on the object consistency task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "boYCqIKCCw", "forum": "wWla66rbdk", "replyto": "wWla66rbdk", "signatures": ["ICLR.cc/2026/Conference/Submission6280/Reviewer_t62f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6280/Reviewer_t62f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825554607, "cdate": 1761825554607, "tmdate": 1762918588044, "mdate": 1762918588044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "77A8YApPwU", "forum": "wWla66rbdk", "replyto": "wWla66rbdk", "signatures": ["ICLR.cc/2026/Conference/Submission6280/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6280/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762920206674, "cdate": 1762920206674, "tmdate": 1762920206674, "mdate": 1762920206674, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes EviDiff, a fine-tuning strategy for text-to-image (T2I) diffusion models to improve multi-object compositionality. It leverages a pixel evidence loss that models cross attention as a Dirichlet distribution to account for noisy segmentation masks, and a token conflict loss to address overlapping token attentions. Authors validated their objectives by training various stable diffusion models on COCO datasets with segmentation masks, and showed consistent gains over baselines e.g., TokenCompose for different models and different metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The objectives are well-motivated e.g., uncertainty on masks and token mask overlaps and are derived from principled methodologies. Empirical evaluation demonstrates their effectiveness on combinations of models and evaluations for instance-centric compositionality.\n- Training is drop-in with the new objective and the method does not rely on additional compute during inference."}, "weaknesses": {"value": "## \n\n- The method mainly integrates evidential learning and overlap/conflict penalties of attention-guidance to the TokenCompose recipe, both of which are existing principles. These are useful empirically, but the contribution feels incremental. Also, the improvements with the TokCon objective appear to be marginal — it would be better to examine whether these numbers are statistically significant by performing multiple training runs and taking account the number of samples used in evaluation into account. Though, my major concern is the selection of models and evaluations being used (see below).\n- Most of results are based on SD 1.4, 2.1 and some are based on SD 3 — but these are no longer representative of current T2I models. It would be great to show if these methods can be adapted to finetune current T2I and unified models (e.g., flux 1, sd 3.5; qwen image, bagel) to achieve a more competitive performance for T2I compositionality.\n- Evaluation set is also outdated. It would be great to show methods’ effectiveness on modern benchmarks for compositionality e.g., T2I-CompBench++, GenEval, GenAI-Bench as they evaluate more than multi-category instance generation; by training newer models and evaluating them on newer benchmarks, the paper can better substantiate its claims beyond dated CLIP/FID-style proxies and small suites."}, "questions": {"value": "- Are there any training instabilities when you add those objectives, and how do you tune all the scalar hyperparameters in these objectives?\n- Because most of gains come from PixEvi objective, I wonder if you could do a fine-grained study on different components of this objective i.e., L_ce, L_ice and L_KL and evaluate their effectiveness individually and when combined as pairs?\n- Do the methods help attribute binding beyond nouns (colors/textures/relations)? i.e., does the probability of having the correct color/texture/relation increase when conditioned on being able to generate a specific instance in multi-category instance generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TdETWpv9uj", "forum": "wWla66rbdk", "replyto": "wWla66rbdk", "signatures": ["ICLR.cc/2026/Conference/Submission6280/Reviewer_H9Tj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6280/Reviewer_H9Tj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953995988, "cdate": 1761953995988, "tmdate": 1762918587479, "mdate": 1762918587479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **EviDiff**, a training framework designed to improve object-wise consistency in text-to-image diffusion models. The authors identify two key issues in prior token-level supervision approaches (e.g., TokenCompose): (1) segmentation mask noise that produces unreliable pixel-token correspondence, and (2) inter-token conflicts when multiple objects overlap semantically or spatially. To address these, EviDiff introduces two complementary losses: a *pixel evidence loss* that interprets attention activations as Dirichlet evidences to down-weight uncertain supervision, and a *token conflict loss* based on Dempster–Shafer Theory (DST) to penalize conflicts between overlapping object regions. The method is applied as a fine-tuning layer to Stable Diffusion 1.4, 2.1, and 3-Medium, improving multi-object composition benchmarks such as VISOR and MULTIGEN, while maintaining comparable inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a clear motivation for addressing segmentation noise and token-level conflict in compositional generation.\n- The pixel evidence loss elegantly integrates uncertainty modeling from evidential deep learning, allowing the model to handle unreliable labels more robustly.\n- The token conflict loss formalizes a DST-based measure to explicitly reduce conflicts between overlapping objects, a conceptually sound and novel approach.\n- Experimental results across SD variants demonstrate consistent improvement in compositional accuracy and FID/CLIP scores, without increasing inference cost.\n- The approach is lightweight and easily integrates into existing diffusion model training pipelines."}, "weaknesses": {"value": "- Algorithm boxes contain a dangling reference (“get LEviDiff from Eq. ??”, Algorithm 1 in Appendix E), suggesting a missing equation label in the main text.\n- The evidential modeling details are under-specified: the class count K for the Dirichlet (foreground/background) and how multi-head/multi-layer attention maps are combined before evidence estimation could be clearer; the choice of layers (mid8, up16, up32, up64) is given but not justified.\n- Reliance on auto-generated segmentation without any human verification raises concerns about confirmation bias in both training and evaluation; the same detector family is used for evaluation (open-vocabulary detector), which may correlate with the supervision signal.\n- The overall writing style, structure, and even visual presentation (e.g., layout, typography, figure design) are very similar to TokenCompose, to the extent that it is difficult to determine whether this is an independent follow-up or an extension."}, "questions": {"value": "Please refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8D8bpmiZBK", "forum": "wWla66rbdk", "replyto": "wWla66rbdk", "signatures": ["ICLR.cc/2026/Conference/Submission6280/Reviewer_fTDj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6280/Reviewer_fTDj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960668481, "cdate": 1761960668481, "tmdate": 1762918586868, "mdate": 1762918586868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors observe that current text-to-image (T2I) diffusion models struggle to reliably compose multiple objects when guided by a full text prompt, due to weak object-wise consistency between textual tokens and image content. Prior token-supervised methods help by aligning each noun token with a segmentation map, but suffer from segmentation map bias (inaccurate masks) and semantic overlap conflicts (when object regions overlap). To address these, EviDiff introduces two key losses: a pixel evidence loss which uses evidential deep learning to model the prediction uncertainty of each pixel (thereby reducing reliance on unreliable segmentation maps), and a token conflict loss based on Dempster-Shafer theory to quantify and suppress conflict between overlapping object token attentions. The approach can be applied as a fine-tuning add-on to existing diffusion models, yields improved multi-object composition, photorealism, and inference efficiency, and outperforms prior methods in various benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. EviDiff clearly identifies two concrete failure modes in text-to-image diffusion — segmentation bias and semantic overlap conflict — and proposes principled solutions using evidential deep learning and Dempster-Shafer theory. The combination of pixel evidence loss and token conflict loss is conceptually sound, theoretically grounded, and effectively targets weaknesses in existing token-supervised methods.\n\n2. The paper demonstrates consistent improvements in multi-object compositionality, object-wise consistency, across several benchmarks and base diffusion models. Its plug-and-play fine-tuning nature allows integration with different pretrained generators without architectural modification, enhancing its practical relevance and reproducibility."}, "weaknesses": {"value": "1. While EviDiff introduces new loss formulations, its core idea—enhancing object-wise consistency through token-level supervision—largely follows the same motivation as TokenCompose. The proposed pixel evidence and token conflict losses refine rather than redefine this framework, leading to improvements that appear incremental rather than conceptually groundbreaking.\n\n2. The paper lacks a deeper theoretical justification or interpretability study explaining why evidential learning and Dempster-Shafer conflict modeling yield better compositional alignment. The improvements are shown empirically, but without clear insights into how uncertainty estimation interacts with diffusion attention dynamics. Additionally, an intuitive example/visualization could help clarify the motivation for introducing uncertainty quantification.\n\n3. Experiments focus primarily on controlled multi-object settings. It remains unclear how EviDiff performs on broader compositional prompts involving spatial relationships and verbs."}, "questions": {"value": "1. Since SAM is not entirely reliable, have the authors considered using a more advanced segmentation model, such as SAM 2, to improve the accuracy of object masks and reduce segmentation bias? Of course, SAM2 + EviDiff may bring additional improvement. But it is interesting to know if the proposed method can still be effective when the segmentation model's capability has been improved a lot. \n\n2. Could the authors clarify the actual computational difference between the proposed intra loss (Eq. 10) and the L_token loss used in TokenCompose? A more detailed comparison would help readers understand the novelty and efficiency of the proposed formulation.\n\n3. Is there any difference in the attention layers used or how the proposed attention mechanism behaves across different architectural designs, such as diffusion models based on U-Net (e.g., SDv2) versus Diffusion Transformers (e.g., SDv3)? It would be helpful to discuss whether architectural differences affect attention consistency or loss of effectiveness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UFGww4SQub", "forum": "wWla66rbdk", "replyto": "wWla66rbdk", "signatures": ["ICLR.cc/2026/Conference/Submission6280/Reviewer_LNzA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6280/Reviewer_LNzA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974153763, "cdate": 1761974153763, "tmdate": 1762918586553, "mdate": 1762918586553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}