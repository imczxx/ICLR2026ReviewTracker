{"id": "HrXFxKTFqk", "number": 23577, "cdate": 1758345793305, "mdate": 1759896806847, "content": {"title": "Geometry-Aware Equivariant Attention for Scalable Nanoelectronic Property Prediction", "abstract": "All advanced nanoelectronic devices, including transistors, image sensors, and LEDs, rely on materials and interfaces scaled down to a few nanometers. At these dimensions, material properties change in nontrivial ways due to quantum confinement and atomic-level variability, creating a multi-scale modeling challenge that requires atomistic simulations for accurate prediction. However, such simulations are often prohibitively slow or intractable, making highly expensive iterative rounds of experimentation the default option. In this work we introduce EBFormer, a geometry-aware equivariant neural network that predicts electronic properties of nanostructures by jointly capturing atomistic interactions and global geometric effects, achieving orders of magnitude speed-up over state-of-the-art physical simulators while preserving high accuracy. This is accomplished through the introduction of a novel boundary cross-attention mechanism, a scalable approach to augment local graph convolution with information of the nanostructure geometry. We validate EBFormer for nanowire and nanosheet transistors - representative of the most advanced nanoelectronic architectures currently in use - and show improved in-distribution and out-of-distribution performance on both material properties and downstream device characteristics. Combined with superior asymptotic scalability and data- and parameter-efficiency, our work paves a pathway to atomistic, automated, high-throughput and predictive nanoscale design that is otherwise not available today.", "tldr": "We introduce a novel boundary attention mechanism for scalable material property prediction of nanoscale structures", "keywords": ["Equivariant Neural Networks", "Material Simulation", "Nanoelectronics"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d394369ed9d69bfdd9e637efeb9208786d0e922.pdf", "supplementary_material": "/attachment/bdc64f691a9fa7972f4cb3f8da36deba662b9128.zip"}, "replies": [{"content": {"summary": {"value": "The primary methodological contribution of the paper is a symmetry-aware message passing scheme between atoms and boundary nodes of finite nanostructures. The work was motivated with the observation that fully-connected attention mechanisms training on Si nanostructures learnt two-fold interactions, one which covered local neighborhood interactions, and another which connected atoms with the boundary. By developing a boundary attention mechanism, the authors include this learned effect as an inductive bias in the model, thus increasing expressivity while avoid the unfavorable scaling which would otherwise result from fully-connected attention mechanisms.\n\nOverall, I think it's an interesting approach to model properties heavily affected by quantum confinement at the mesoscale. However I have a few concerns and questions detailed below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I appreciate the depth the authors went into to generate custom domain-specific dataset, and curate and release it. The overall quality of the work is high and the authors are thorough in gathering results and ablation studies."}, "weaknesses": {"value": "I'm not sure if this work is suitable for a general ML audience. \n\nThe paper is a bit cryptically written, and it is quite difficult to understand the problem that the author are trying to treat. The explanation of quantum confinement effects in nanostructures is understandable, but I had to read halfway through the paper before I understood that the authors designed a tool for DOS prediction in nanostructures which contain both local periodicity and finite overall size. Without some background in device modeling, I don't know if I would have understood the relevance of the problem, and this makes me think it might be better suited to a domain-specific journal.\n\nI'm also not sure if this method can be applied broadly. I can see generalization ability to other mesoscale crystalline structures, but I'm not sure where the results could apply beyond that for the ML materials community.\n\nFinally, looking at Figures 12-14, the results seem to be not yet great. I assume that the goal of this model would be to instantaneously regenerate the DOS/cDOS and re-evaluate the injection velocity upon changes in the atomic structures (such as adding strain, changing the cleavage plane, decreasing the nanosheet thickness, etc) providing a tool for computational design of silicon nanosheets in transistor channels. However I think these errors might be higher than the variations in DOS upon reasonable amounts of strain, for example, which challenges the practical utility. \n\nI would make the paper less cryptic on what is being treated. A schematic illustrating the differences between molecular structures, periodic materials, and mesoscale nanostructures would be nice. I would also include Fig 5, and an example of a silicon nanowire structure in the main text. \n\nI think some more details about the dataset should also be in the main text. Without looking at the appendix, I had no picture of what the model was being trained on."}, "questions": {"value": "-Intuitively, I think there should be a trend in the effect of including the boundary nodes with respect to structure size. It seems however that the results were taken over all structures. Have the authors looked into how including the boundary attention mechanism changes performance when testing on structures with different sizes?\n\n-When evaluating the boundary attention mechanism, the authors compare between a relatively small local cutoff (5A) and a fully-connected graph. Have they looked into instead using a slightly larger cutoff? Presumably, once the nanostructure is large enough, the properties at the 'center' converge to those of the bulk material, the question is only the exact size at which this occurs. I see that the attention learned from the fully-connected graph prioritizes local interactions and boundary connections. However, attention mechanisms are known to learn shortcuts  on the training data in scientific simulations, and maybe this is one of them. I wonder if simply using a cutoff of ~8 A would close the gap between the performance of the local and fully-connected networks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cH3tqLzLZo", "forum": "HrXFxKTFqk", "replyto": "HrXFxKTFqk", "signatures": ["ICLR.cc/2026/Conference/Submission23577/Reviewer_fHnK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23577/Reviewer_fHnK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760831509571, "cdate": 1760831509571, "tmdate": 1762942720975, "mdate": 1762942720975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author invents a method to embed the boundary/cell information and their interaction with the atoms' position into the network and uses this information to guide the prediction of the target properties. It has several benefits, such as computational efficiency, but some shortcomings also exist, which I will comment on later."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The author perform very good experiment include indistribution and out of distribution analysis. The presentation is good, the writing is very clear."}, "weaknesses": {"value": "The method is not very novel; several similar methods that incorporate the boundary/cell information into the system exist. And this method has some intrinsic difficulties in transferring to a similar system."}, "questions": {"value": "1. The box/boundary vector, as initialised, can be shifted by an arbitrary constant in the x/y/z direction without changing the shape. Is this method transferable to all these simulations?\n\n2. How to deal with the supercell of the same system? Does the model, trained on small cells, automatically generalise to its supercell?\n\n3. Can you compare the result with a randomly generated initial bound vector to ensure that the result improvement does not come from the overfitting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LWKi2DSfiZ", "forum": "HrXFxKTFqk", "replyto": "HrXFxKTFqk", "signatures": ["ICLR.cc/2026/Conference/Submission23577/Reviewer_keTe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23577/Reviewer_keTe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879307602, "cdate": 1761879307602, "tmdate": 1762942720712, "mdate": 1762942720712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EBFormer, a geometry-aware equivariant neural network designed to predict electronic properties of nanostructures (nanosheets and nanowires). The key innovation is augmenting local graph convolution (NequIP-style) with boundary cross-attention mechanisms to capture effects arising from global geometry. The authors demonstrate that explicitly encoding boundary interactions helps for accurate property prediction, particularly in out-of-distribution scenarios involving structures with dimensions beyond the training set."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-motivated architecture**: The approach is physically grounded. The authors clearly motivate why boundary information is necessary for capturing quantum confinement effects in nanostructures, and the boundary cross-attention mechanism is an elegant solution that maintains O(N) scaling.\n\n2. **Strong extrapolation performance**: The most impressive result is the model's ability to extrapolate to nanosheets with 46-100 atomic layers when trained only on 15-45 layers. \n\n3. **Thorough experimental design**: The paper includes appropriate ablations (NequIP baselines, parameter-matched comparisons)."}, "weaknesses": {"value": "1. **Limited dataset scope**: The evaluation is restricted to Si and Ge nanosheets/nanowires with specific cleaving orientations. While the paper demonstrates interpolation and size extrapolation, the generalization to:\n   - Other materials systems (III-V semiconductors, 2D materials, etc.)\n   - Different boundary conditions\n   - Novel nanostructure geometries\n   remains unclear.\n\n2. **Insufficient cross-geometry validation**: A critical validation experiment is missing: training on nanosheets and testing on nanowires (and vice versa). This would strongly demonstrate whether the model truly learns general boundary/confinement physics versus memorizing geometry-specific patterns. The current setup trains and tests on each geometry separately.\n\n3. **Transfer learning underexplored**: While Appendix I shows some transfer learning from Ge to Si for thickness extrapolation, this is relatively limited. More systematic evaluation of:\n   - Cross-material transfer (train on Si, test on Ge)\n   - Cross-geometry transfer (nanosheets <-> nanowires)\n   - Few-shot adaptation to new materials\n   would significantly strengthen the claims about learning physical principles.\n\n4. **Computational efficiency concerns**: Table 11 shows EBFormer is 3-6× slower than DOSTransformer and NequIP during inference, and training is even more impacted. While the authors acknowledge this and suggest future optimizations, this limits practical adoption for high-throughput screening."}, "questions": {"value": "1. **Cross-geometry experiments**: Can you add experiments training on nanosheets and testing on nanowires? This would be very valuable for understanding whether boundary attention truly captures generalizable confinement physics.\n\n2. **Boundary representation**: How sensitive is performance to the number and placement of boundary embeddings? What happens with more complex geometries (e.g., core-shell nanowires)?\n\n3. **Comparison scope**: How would simpler approaches perform, such as:\n   - Adding system-level features (thickness, aspect ratio) to a standard GNN?\n   - Using a single global node instead of boundary-specific nodes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hSJOWvElGf", "forum": "HrXFxKTFqk", "replyto": "HrXFxKTFqk", "signatures": ["ICLR.cc/2026/Conference/Submission23577/Reviewer_8WJe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23577/Reviewer_8WJe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916234190, "cdate": 1761916234190, "tmdate": 1762942720393, "mdate": 1762942720393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents EBFormer, an SE(3)-equivariant architecture that augments local graph convolution with a boundary cross-attention mechanism to encode global geometry (e.g., confinement planes). The model is trained to predict DOS/cDOS and derived device metrics (injection velocity, ballistic current) for nanosheets and nanowires, using a custom, simulator-generated dataset (∼12k Si/Ge nanosheets; 380 Si nanowires) computed with tight-binding. EBFormer is compared against NequIP and DOSTransformer (plus an MLP), showing improvements in-distribution and on a thickness OOD split. Speedups vs. physics simulators are claimed (orders of magnitude)."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Physically motivated design: boundary-aware equivariant attention is a plausible way to inject geometry. Clear target quantities: DOS/cDOS → injection velocity/current provide device-adjacent metrics with interpretable units. Some robustness checks: OOD across nanosheet thickness; ablations vs. local-only and parameter-parity settings. Computational efficiency: claimed orders-of-magnitude faster than physics simulators."}, "weaknesses": {"value": "The paper’s evaluation is limited in both scope and rigor. It compares the proposed model only against NequIP, DOSTransformer, and a simple MLP baseline, omitting stronger and more diverse families such as MACE/MACE-MP, Allegro, PaiNN/NequIP-v2, GemNet-T (OC20 baselines), Matformer/COMFormer, and M3GNet. The label space is also narrow, focusing solely on DOS and cDOS while neglecting other physically significant quantities like band structures, effective masses, dielectric and optical responses, phonon spectra, elastic constants, scattering-aware transport, and device-level metrics that would better probe physical generalization. Moreover, the model’s external validity is questionable—no experiments are performed on established public datasets such as Matbench, JARVIS, or OC20/OC22, nor on experimental targets—making it difficult to assess general usefulness beyond the authors’ custom dataset. The claimed novelty of the boundary-attention mechanism is also uncertain, as similar global-context mechanisms could plausibly yield comparable effects, and the paper lacks comparative evidence to isolate what uniquely drives performance gains. Overall, since the work introduces both an in-house dataset and a “novel” model, it should be benchmarked on at least three to five standard datasets and compared with a wider range of five or more well-known models to substantiate its claims and demonstrate genuine methodological advancement."}, "questions": {"value": "Public benchmarks: Can you report on Matbench (DOS/elastic), JARVIS, or OC20/OC22 tasks to test periodic/surface/generalization claims?\n\nParticle count: What are the atom counts in each nanoparticle?\n\nCross-simulator validation: How does EBFormer trained on TB perform on a DFT-labeled subset (no simulator leakage)?\n\nBroader chemistry: Results on III-V (GaAs, InP), wide-bandgap (GaN, SiC), oxides/heterostructures? Any transfer learning experiments?\n\nProperty breadth: Can you extend to band structures/effective masses, phonons/elastic moduli, dielectric spectra to stress different physical regimes?\n\nBaselines: Why the models such as MACE/Allegro/PaiNN/GemNet/Matformer/COMFormer/M3GNet with parameter parity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RzZ7nGNM3O", "forum": "HrXFxKTFqk", "replyto": "HrXFxKTFqk", "signatures": ["ICLR.cc/2026/Conference/Submission23577/Reviewer_GH4c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23577/Reviewer_GH4c"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23577/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942440622, "cdate": 1761942440622, "tmdate": 1762942719835, "mdate": 1762942719835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}