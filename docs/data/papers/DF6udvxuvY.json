{"id": "DF6udvxuvY", "number": 3312, "cdate": 1757399025350, "mdate": 1763744872418, "content": {"title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale", "abstract": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, greatly narrowing the gap with top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLM development, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Code and weights will be publicly available to promote further research.", "tldr": "A novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios.", "keywords": ["Native Vision-Language Models", "Vision-Language Primitive", "Holistic Vision-Language Buffer"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63d5ebb00927eb655ff92eef2abaf83ea0cc82b2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a novel monolithic large vision-language model, called NEO, supported by an improved rotary positional embeddings (RoPE) mechanism and multi-stage training. NEO is also backed by several existing techniques, such as hybrid attention masking, and shared FFN, attention and norm layers for vision and language modalities. With evaluations reported on several well-established benchmarks, the work aims to demonstrate the performance of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The primary strengths of the work could be listed as the following:\n\n- The particular idea of leveraging an improved RoPE mechanism (called \"Native RoPE\" in the work) in the context of monolithic vision-language models is novel.\n- The work includes a very thorough literature review with more than sufficient citations to contemporary works, even those released within the same year, which is much appreciated.\n- Evaluation is performed on a decent number of benchmarks, and there are a number of ablations on the positional encoding strategy, which is also a great to have.\n\nRelatively minor strengths of the work could be listed as the following:\n\n- Overall structure and flow of thought presented in the work is decent.\n- The figures are of very high quality and are visually appealing, though a bit crowded (see the minor weaknesses below)."}, "weaknesses": {"value": "The primary weaknesses of the work could be listed as the following:\n\n**W1: Architectural and Training-time Adjustments Similarities with Existing Works:** The work borrows heavily from two existing works, EVE [A] and EVEv2 [B] in both of its architectural and training-time adjustments. In particular, sharing the norm layers, attention blocks and FFN blocks have been explored in [A], patch embedding and word embedding layers are nearly identical to [B], and the _native multi-modal attention_ is the standard practice in large VLMs [C, D]. Furthermore, the overall training strategy is nearly identical to [B], with the Stage 1 pretraining corresponding to [B]'s Stage 1 & 2.1, Stage 2 mid-training corresponding to [B]'s Stage 2.2. and Stage 3s matching. Normally, having these similarities would not be a major weakness if it was not for the narrative of the work, which, in its current form, appears to present these as novelties of the proposed framework.\n\n**W2: Ambiguities in the Narrative:** There are several ambiguities in the narrative of the work. Most importantly, due to the aforementioned W1 the exact contributions of the work are not very clear to read from the text, as the current narrative renames a few well-established practices in the field in a rather ad-hoc manner. To exemplify, these include renaming the standard hybrid attention masking in the literature to \"Native Multi-modal Attention\" or renaming the monolithic blocks of [A, B] to \"Native VLM primite\" while the only architectural difference from [A, B]'s blocks is the improved RoPE mechanism and the added Q and K parameters that go along with it. Finally, I found it a bit hard to grasp the exact changes introduced over existing works in Section 3.1 in general, with several terms like \"Pre-Buffer\" not being well-defined.\n\n**W3: Fairness of Evaluations:** The work indeed includes a good number of benchmarks and a good number of ablations trying out different RoPE variants within the same framework. However, one critical thing that is lacking is a fair comparison between [A, B] and this work. Notably, NEO utilizes a much better LLM compared to the baselines considered (Qwen 3 versus older Qwen 2.5/Vicuna variants) in Table 1 and it was also trained with much more data than many of them. Given NEO does not add much beyond the improved RoPE mechanism over [A, B] architecturally, a more fairer comparison would demand them trained with similar budgets or at least with similar performing LLMs.\n\nRelatively minor weaknesses of the work could be listed as the following:\n\n- In many parts of the text the usage of \\citep and \\citet commands were used incorrectly. This hinders the readability of the text for the broader audience and fixing them would greatly improve the reading experience.\n\n- Some figures are very crowded with many details, creating potential confusions in grasping their main message. To exemplify, Figure 1 includes many different details regarding the full LVLM pipeline. Although its quality is very high and I can clearly see that much effort went into constructing it, I believe that pruning it greatly would make it easier for the reader to grasp its main message.\n\n*Finally , although I am leaning towards rejection for the work in its current form, I would like to encourage the authors to clarify any potential misunderstandings I might have had.*\n\n---\n[A] Diao, H., Cui, Y., Li, X., Wang, Y., Lu, H., & Wang, X. (2024). Unveiling encoder-free vision-language models. Advances in Neural Information Processing Systems, 37, 52545-52567.\n\n[B] Diao, H., Li, X., Cui, Y., Wang, Y., Deng, H., Pan, T., ... & Wang, X. (2025). Evev2: Improved baselines for encoder-free vision-language models. arXiv preprint arXiv:2502.06788.\n\n[C] Beyer, L., Steiner, A., Pinto, A. S., Kolesnikov, A., Wang, X., Salz, D., ... & Zhai, X. (2024). Paligemma: A versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726.\n\n[D] Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., ... & Dai, J. (2024). Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 24185-24198)."}, "questions": {"value": "- Given the similarities in performance with Video-RoPE [E] and the proposed RoPE mechanism in this work (Table 3), how well the proposed RoPE mechanism compare against it under smaller training budgets, such as those utilized for [A, B]? \n\n- Can you comment on the fairness of evaluations raised from above? How do you think the differences in training and architectural settings could be effecting the evaluation results and how do you think you could address these?\n\n---\n[A] Diao, H., Cui, Y., Li, X., Wang, Y., Lu, H., & Wang, X. (2024). Unveiling encoder-free vision-language models. Advances in Neural Information Processing Systems, 37, 52545-52567.\n\n[B] Diao, H., Li, X., Cui, Y., Wang, Y., Deng, H., Pan, T., ... & Wang, X. (2025). Evev2: Improved baselines for encoder-free vision-language models. arXiv preprint arXiv:2502.06788.\n\n[E] Wei, X., Liu, X., Zang, Y., Dong, X., Zhang, P., Cao, Y., ... & Lin, D. (2025). VideoRoPE: What Makes for Good Video Rotary Position Embedding?. arXiv preprint arXiv:2502.05173."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uGzW0Rn6R7", "forum": "DF6udvxuvY", "replyto": "DF6udvxuvY", "signatures": ["ICLR.cc/2026/Conference/Submission3312/Reviewer_EYZR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3312/Reviewer_EYZR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761305193463, "cdate": 1761305193463, "tmdate": 1762916660689, "mdate": 1762916660689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their careful and constructive feedback. Reviewers consistently recognize our **novel approach** (QS4o, JqhJ, EYZR), **strong performance** (QS4o, JqhJ), **comprehensive comparisons** with existing methods (QS4o, JqhJ, EYZR), **valuable ablation studies** (QS4o, JqhJ, EYZR), and **concise training strategy** (QS4o).\n\nWe have polished the manuscript and improved the illustrations. We hope that these revisions adequately resolve the main issues and kindly invite the reviewers to re-evaluate the paper based on our responses and the updated version.\n\nThe main revisions to the paper are summarized as follows:\n\n- **Clear comparisons with prior native VLMs:** We have added detailed comparisons between NEO and its native counterparts under the same data budgets and LLM backbone (Fig. 7; Lines 476–482).\n\n- **Claims regarding performance against modular models:** We have revised the abstract (Lines 23–24), introduction (Lines 104–107), and main results (Lines 360–368) to clarify and accurately qualify these claims.\n\n- **Clearer highlight of contributions:** We have further polished the statements of key contributions in the introduction (Lines 89–91), related work (Lines 158–161), and methodology (Lines 235–237).\n\n- **Formalization of several concepts:** We have revised and clarified the definitions of primitives (Lines 199–208) and the pre-Buffer and post-LLM components (Lines 244–259).\n\n- **Category in related work:** We have refined the discussion to better distinguish private VLMs from open-source ones in the related work section (Lines 130–133).\n\n- **More details between pre-Buffer and vision encoders:** Additional implementation details have been added in the ablation study section (Lines 467–475).\n\n- **Hyper-parameters of the pre-Buffer:** We have provided further explanations in both the methodology (Lines 246–248) and ablation study (Lines 427–431).\n\n- **Citation style, writing clarity, and figure simplification:** We have standardized citation formatting (\\citep), improved related descriptions, and simplified figures to enhance readability."}}, "id": "gvlkEFGQsc", "forum": "DF6udvxuvY", "replyto": "DF6udvxuvY", "signatures": ["ICLR.cc/2026/Conference/Submission3312/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3312/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3312/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744797020, "cdate": 1763744797020, "tmdate": 1763744797020, "mdate": 1763744797020, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel approach to train autoregressive, monolithic vision-language models that omit domain-specific vision encoders in favor of light-weight encoders and a native multi-modal training resulting in the *NEO* models.  The model consists of a small two-layer convolution encoder, a multimodal pre-buffer, and a pretrained LLM (Qwen3). The attention and position encoding procedure is optimized for multi-modality. The model training comprises of 3 stages with 390M image-text examples. The resulting 2.2B/9B models are thoroughly benchmarked against prior modular and monolithic VLMs, and outperform all prior models in the latter category."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper proposes improvements in native, monolithic multimodal LLM training by introducing a pre-Buffer for better alignment (trained separately in the first stage), native rotary position embeddings with modality-specific base frequencies, native multimodal attention (causal for text, full-bidirectional for vision) with decoupled H, W, T processing. \n- Using 390M image-text examples, NEO reaches a high performance and outperforms all previous native VLMs.\n- NEO is built on top of a modern LLM (Qwen3) and supports flexible resolution\n- The authors provide NEO-2B and 9B intermediate and final checkpoints\n- The models are thoroughly evaluated against other models, including modular and native models in 2B and 8/9B categories\n- Some design choices (number of layers in the pre-Buffer and attention/embedding methods) are ablated"}, "weaknesses": {"value": "- I keep wondering why NEO-9B uses a 50% smaller pre-Buffer than 2.2B. The paper mentions \"mainly due to the good trade-off between performance and efficiency.\" (L431) but does not provide evidence for that. I am not convinced that the results in Fig. 5 extrapolate to a larger post-LLM.\n- Some systematic ablations of design choices are often missing. The number of layers in the pre-Buffer and attention/embedding methods are ablated but nothing else. This left wondering which design choices in NEO actually impact performance: e.g., is it the data (quality/quantity)? The stages? The more modern LLM (related work uses older LLMs)? Or, is it actually the proposed combination of design choices. I understand that providing such controlled ablation experiments might not be economically feasible, but they obfuscate the contribution nonetheless. \n- NEO still significantly lags behind modular VLMs, even older ones like Qwen2-VL (e.g., 16% on InfoVQA). Given the relative improvements in its category and the lower amount of data this is not a big issue itself, however the phrasing in \"Comparison with Modular VLMs.\" (L357ff) is a bit overselling.\n- The scaling improvements between 2B and 9B seem modest compared to modular VLMs, \"casting shadows\" over the scalability of NEO.\n- Some parts of the paper feel LLM-generated by overusing (sometimes nonsensical) synonyms, making it hard to follow the paper. I would encourage the authors to manually revise the paper.\n- Fig. 1/3 are densely packed and hard to comprehend."}, "questions": {"value": "- Are there any insights why the performance on HallusionBench and \"knowledge-heavy\" tasks suffers? Fundamentally, this does not seem like a multi-modal problem to me.\n- Please review the LLM written parts for clarity.\n- Please consider using \\citep to improve legibility"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "03JmK9fhjO", "forum": "DF6udvxuvY", "replyto": "DF6udvxuvY", "signatures": ["ICLR.cc/2026/Conference/Submission3312/Reviewer_JqhJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3312/Reviewer_JqhJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761582971143, "cdate": 1761582971143, "tmdate": 1762916660041, "mdate": 1762916660041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present Neo, a family of native Vision-Language Models built on top of Qwen3-1.7B and Qwen3-8B. The key elements of the Neo architecture are (1) an attention block that decouples H, W and T dimensions for Query and Key computation; (2) RoPE position embeddings that use separate frequencies for H, W and T dimensions; (3) bidirectional attention for images; (4) added transformer layers (\"Pre-Buffer\") to project vision and text embeddings to the same embedding space. The authors train Neo models on 390M image-text examples in a process containing three stages and compare against prior modular VLMs using pretrained vision encoders and native VLMs. The trained models obtain competitive performance on various datasets against modular VLMs, while outperforming prior work on native VLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Each architectural component is motivated with the incorporation of inductive biases that make the processing of images in native VLMs more similar to modular VLMs, which I found to be intuitive.\n- The decoupling of H, W and T dimensions in both the Query and Key computation as well for RoPE computation is novel to the best of my knowledge.\n- The training process does not assume access to any pretrained vision encoders.\n- The various ablation studies in Section 4.3 are very valuable in arguing for the importance of various design choices in the attention blocks, particularly of the suggested adjustment to RoPE. This is especially the case when all native VLM models compared against make use of different datasets or pre-trained LLMs."}, "weaknesses": {"value": "While I believe the contribution is valid, in large part due to the ablation studies, there are various flaws to the paper:\n- Firstly, there are clear errors in the related works section. We do not know the underlying architecture for multimodal GPT models and it is therefore incorrect to claim that they are modular or native. It is also important to note that GPT-4o, being able to both condition on and generate images, is more similar to a model like Chamelon (included in the native model section) than standard modular model counterparts. It is likewise an issue to make claims regarding Claude or Gemini.\n- I would argue that saying Neo \"rivals top-tier modular counterparts\" in the abstract and introduction is overclaiming. For general vision benchmarks, Neo approaches modular model performance, but nonetheless falls short in each case. This is particularly an issue for MMMU and MMVet, where 8B model performance is at least ~20% poorer. This and the bullet point above are the rationale for the soundness score.\n- Likewise, any comparison made to prior native VLMs has the confounder of Neo making use of the newer and stronger Qwen3 backbones. This is mitigated by the ablation studies, which are much welcome.\n- I found the writing to be unclear, particularly for sections dealing with the \"Pre-Buffer\" and \"Post-LLM.\" Some added formalism about what computation each of these components perform and which components are initialized from the Qwen3 backbone and which are not would make understanding easier. Figure 2 sadly is unclear as both the Pre-Buffer and Post-LLM components make use of the same primitives but just differ in color."}, "questions": {"value": "- A majority of the citations should be changed to parentheticals, rather than in-text citations with \\citet.\n- As a follow-up to my point regarding the Pre-Buffer, I also struggled at understanding what exactly was done in the \"Comparison between Pre-Buffer and Vision Encoders\" section. Would it be correct to say that training was repeated here with InternViT/CLIP/SigLIP used in place of the Pre-Buffer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IWfOG1IJtI", "forum": "DF6udvxuvY", "replyto": "DF6udvxuvY", "signatures": ["ICLR.cc/2026/Conference/Submission3312/Reviewer_QS4o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3312/Reviewer_QS4o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987026134, "cdate": 1761987026134, "tmdate": 1762916659813, "mdate": 1762916659813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}