{"id": "Am95bfE207", "number": 12218, "cdate": 1758206401031, "mdate": 1759897524684, "content": {"title": "From Low Intrinsic Dimensionality to Non-Vacuous Generalization Bounds in Deep Multi-Task Learning", "abstract": "Deep learning methods are known to generalize well from training to future data, even in an overparametrized regime, where they could easily overfit. One explanation for this phenomenon is that even when their ambient dimensionality, (i.e. the number of parameters) is large, the models’ intrinsic dimensionality is small; specifically, their learning takes place in a small subspace of all possible weight configurations. In this work, we confirm this phenomenon in the setting of deep multi-task learning. We introduce a method to parametrize multi-task network directly in the low-dimensional space, facilitated by the use of random expansions techniques. We then show that high-accuracy multi-task solutions can be found with much smaller intrinsic dimensionality (fewer free parameters) than what single-task learning requires. Subsequently, we show that the low-dimensional representations in combination with weight compression and PAC-Bayesian reasoning lead to the first non-vacuous generalization bounds for deep multi-task networks.", "tldr": "We prove the first non-vacuous generalization bounds for Deep MTL based on the insight that Deep MTL has much smaller amortized intrinsic dimensionality compared to single task learning.", "keywords": ["Multi-task learning", "non-vacuous generalization bounds", "PAC-Bayes", "intrinsic dimensionality"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a614a04650d78cd78153ab02165ebb69d0f31de5.pdf", "supplementary_material": "/attachment/be487c2f5659d111df5d5e1f22d804ac2722ac9d.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the generalization performance of deep multi-task learning (MTL) through the lens of low intrinsic dimensionality.\nIt introduces the concept of Amortized Intrinsic Dimension (AID) (the effective number of parameters per task when shared representations are learned), and derives the first non-vacuous generalization bounds for deep MTL using compression and PAC-Bayesian analysis.\nEmpirical studies on multiple datasets show that AID is significantly smaller than the single-task intrinsic dimension, and the resulting generalization bounds are numerically non-vacuous."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper extends compression-based and PAC-Bayesian generalization analysis to multi-task learning. Two new encoding-based generalization bounds for multi-task learning that result in non-vacuous guarantees on the multi-task risk for several standard datasets and model architectures are established.\n\n2. Experiments across several benchmarks convincingly demonstrate large reductions in intrinsic dimensionality and non-vacuous bounds."}, "weaknesses": {"value": "1. Although the paper conceptually mentions mutual information as a measure of task relatedness, it remains largely qualitative.\nThere is no empirical estimation of task similarity or dependence, nor a quantitative analysis connecting such measures to the observed reductions in AID or to the resulting generalization gap.\n\n2. Most datasets are relatively small (e.g., MNIST, CIFAR), and the models are relatively shallow compared to modern large-scale MTL or LLM settings.\n\n3. The result is positioned as the first non-vacuous generalization bound for deep MTL, while the proof itself is relatively straightforward and closely follows prior single-task compression/PAC-Bayes analyses, so the technical novelty appears limited."}, "questions": {"value": "1. Are there existing works that explain why low-dimensional shared representations emerge in multi-task learning, and that quantify how mutual information between task distributions influences the compression ratio or the tightness of generalization bounds?\n\n2. Could the authors clarify what specific theoretical or technical challenges arise in this extension and how their analysis addresses them?\n\n3. The paper fixes the threshold parameter $τ = 90$ when computing the intrinsic or amortized intrinsic dimension, yet it does not discuss how sensitive the results are to this choice. Would smaller or larger thresholds (e.g., $τ = 80$ or $95$) substantially change the estimated AID values or the resulting generalization bounds? A brief sensitivity analysis or justification for $τ = 90$ would strengthen the empirical credibility of the results.\n\n4. Minor comment: There is a small grammatical issue in Appendix, “the Theorem 2/3” should be written as “Theorem 2/3”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p59bBQN31X", "forum": "Am95bfE207", "replyto": "Am95bfE207", "signatures": ["ICLR.cc/2026/Conference/Submission12218/Reviewer_kX6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12218/Reviewer_kX6A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951482157, "cdate": 1761951482157, "tmdate": 1762923165444, "mdate": 1762923165444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The submission derives two generalization theorems that yield the first non-vacuous numerical generalization bounds for deep multi-task learning (MTL).\n\nThe submission suggests for any deep learning architecture an alternative lower-dimensional parametrization [Eq. (1) for STL from literature, Eq. (2-3) for MTL, Eq. (7) for TL]. If I understood it correctly, the submission derives computable non-vacuous generalization bounds for models that were trained via these lower-dimensional parametrizations, by using certain compression techniques (such as quantization) for these low-dimensional parameterizations.\n\nThe submission conducts experiments for these low-dimensional parametrizations and computes non-vacuous generalization bounds. Do I understand correctly that no generalization bound was explicitly numerically computed for fully trained models in this submission?\n\nTo be honest, there are some aspects that I did not fully understand (mainly regarding what the overall storyline is). My main confusion is about two sentences on page 2:\n\n“The bounds depend only on quantities that are available at training time and can therefore be evaluated numerically. ” and\n\n“This view, in particular, allows us to avoid apriori parametric assumptions”\n\nTo me, these two statements seem somewhat disconnected. It appears that the paper provides (i) numerically computable bounds under specific a priori parametric assumptions (the proposed low-dimensional parametrizations), and separately (ii) more general theoretical bounds that avoid such assumptions but remain mainly conceptual without a clear numerical instantiation strategy. If I understand it correctly, (i) is kind of an example of applying (ii), but under a specific parametric assumption. So there is quite some uncertainty in my scores, and my evaluations might change based on your answers to my questions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Multi-task Learning, Transfer Learning, and out-of-sample generalization are very important topics, and a better theoretical understanding of those is highly desirable.\n\nMost generalization bounds in the literature are vacuous and not useful in practice. Non-vacuous generalization bounds are therefore appreciated.\n\nWhile most classical learning theory focuses on Single Task Learning (STL), much of the most exciting progress in applied Deep Learning (DL) is connected to Transfer Learning (TL) and Multi-task Learning. More theory in this direction is important.\n\nExplicitly computing non-vacuous generalization bounds on real-world datasets is a valuable contribution to the field."}, "weaknesses": {"value": "The paper’s framing sometimes suggests that the bounds apply to arbitrary deep MTL models, whereas the actual, practically computable results seem to hold specifically for models trained under the proposed low-dimensional parametrizations (Eqs. (1), (2–3), (7)). Clarifying this scope early (perhaps even in the abstract) would prevent misunderstandings. In the current version, I got the feeling that the initial claims oversell the actual results a bit. I think the paper could be improved by communicating earlier, more clearly what the scope, the main results, and the contributions are.\n\nWhy don’t you additionally show the actual (training and) test loss in Tables 2 and 3?\n\nDo your generalization bounds correlate with actual generalization? When you train all your low-dimensional parametrizations for different values of $k$ and $l$ (and varying other hyperparameters), I would be very interested to see the training error, the generalization error bound, and the actual generalization error for all these combinations. Computing these numbers basically comes for free for you, given that you have already trained all of them."}, "questions": {"value": "Q1: Do I correctly understand the intrinsic dimensions of Sections 2 and 3: The definition does not give the intrinsic dimension of a fully trained model f_theta, but the intrinsic dimension only depends on the architecture, the data distribution, the training data, and an accuracy level $\\tilde{A}:=\\tau\\%\\alpha$?\n\nI.e.,  the fully trained model f_theta is conditionally independent of the intrinsic dimension, conditioned on the architecture, the data distribution, the training data, and an accuracy level $\\tilde{A}$,.\n\nFor example a model $f_\\theta$ that learns very high complexity parameters, that totally overfits the training data, and expresses a very complicated high complexity function with poor generalization properties which cannot be compressed, would result in low intrinsic dimension in your experiments: As it only achieves a low test accuracy, it will be to find a low dimensional matrix P which achieves a similar test accuracy.\n\nOn the other hand, a model, $f_\\tilde{\\theta}$ that perfectly compressed the data in a sophisticated way (e.g. is extremely sparse), and therefore achieves a great test accuracy, would therefore correspond to a higher intrinsic dimension, as it is hard to find a low-dimensional matrix P which can match this test accuracy?\n\nSo, in other words, if I have different models $f_\\theta$ with the same architecture for a given dataset, their intrinsic dimension would only vary based on their test accuracy. In a way that better test accuracy corresponds to a higher intrinsic dimension.\n\nI think it would be good to emphasize very clearly that this notion of intrinsic dimension should not be seen as a property of the model $f_\\theta$, but this notion of intrinsic dimension only depends on the architecture, the data distribution, the training data, and an accuracy level.\n\nTo me, the formulation of Definition 1 feels quite misleading: it starts with models $f_i$, but then it uses them only to compute A (which uses the test dataset), but then you don’t even directly use $A$, but only  $\\tilde{A}:=\\tau\\%\\alpha$.\n\nThis formulation could mislead a naive reader into thinking that you are computing “the intrinsic dimension of an arbitrarily trained model $f_\\theta$” (in the spirit of the Local Learning Coefficient (LLC) in Singular Learning Theory) which could mislead a naive reader into thinking that those values of $\\theta$ with lower intrinsic dimension correspond to good generalization (as for the LLC). However, if I understood the submission correctly, this is absolutely not what’s happening here.\n\nI would explicitly formulate Def 1 as a property of the combination of an architecture, a data distribution, a training data set, and an accuracy level $\\tilde{A}$. I don’t see any reason to introduce $\\tau$, nor $A$, nor any fully trained model $f_\\theta$.\n\nQ2: My impression is that you derive generalization bounds for the parametrizations described in Eq. (1), (2-3) and (7))? But not for differently trained models? Technically, Theorems 2 and 3 are very general and could also apply to differently trained models. However, you don’t provide any explicit algorithm on how to numerically compute non-vacuous generalization bounds for fully trained DL models? Do I understand this correctly? Because your explicit compression algorithm assumes that the parameters of the model have the structure form Eq. (1), (2-3), or (7)? I think the paper would profit a lot from being crystal clear about these central questions.\n\nQ3: Line 144: This depends on the random realization of $\\theta_0$ and $P$. Do you average over multiple such realizations, or do you keep one fixed realization? Have you done some sensitivity analysis, if you get similar results for a different random seed, with a different random realization of $\\theta_0$ and $P$? Do the empirical results vary a lot from seed to seed, or are they quite stable? From a mathematical point of view, do you see the ID and AID as random variables that depend on the random variables $\\theta_0$ and $P$?\n\nQ4: Do you resample $\\theta_0$ for every $(k,l)$-gridpoint? Or do you sample $\\theta_0$ once and keep it fixed across the grid? Would this change the results? Intuitively, I would guess that the exact random realization of $\\ theta_0$ and $P$ does not matter a lot for sufficiently large models, but might matter quite a bit for smaller architectures.\n\nQ5: Your current version of Def 1 can only be computed with access to the validation/test set in order to compute $A$?\n\nQ6: minor remark: Line 146; “(2)/(3)” is an unusual formatting of equation numbers. “(2)-(3)”, “(2-3)”, “(2) and (3)” are more common choices, I think.\n\nQ7: Table 1: Why does the number of tasks $n$ depend on the architecture? For example, for split-CIFAR10, two different architectures ConvNet and ViT have different values of $n$ and $m$ while the dataset stays the same.\n\nQ8: Is $\\theta_0$ random for the ViT, or do you use $\\theta_0$ from the pretrained ViT? I would guess that using the pretrained $\\theta_0$ results in a significantly smaller intrinsic dimension than using the random $\\theta_0$. Do you agree? Have you tried that?\n\nQ9: Why is Table 2 not referenced in the text?\n\nQ10: Why don’t you show the actual train and test error in Tables 2 and 3? I would be very interested to see the training error, the numerically computed theoretical test error bound, and the actual test error on the test dataset for all considered combinations of $k$ and $l$.\n\nQ11: Are the models $f_1,\\dots,f_n$ in Section 5.3 trained via the parametrization from Eq. (2-3) or fully trained?\n\nQ 12: Are the models reported in Table 3 trained via the parametrization from Eq. (7) or fully trained?\n\nQ13: Lines 79-81: To me the sentence: “This view, in particular, allows us to avoid a priori parametric assumptions, such as similar model parameters (Evgeniou & Pontil, 2004) or the existence of a common feature space (Caruana, 1997).” feels quite confusing to me, as it seems to me that all the quantities that you can actually compute in practice from your submission also assume very specific non-standard parametrizations, i.e., Eq. (1) for STL, Eq. (2-3) for MTL, Eq. (7) for TL. Did you write this sentence because Theorems 1-3 are more generic? Do you see Theorems 2-3 as your main contribution, or Eq. (2-3) for MTL, Eq. (7) for TL?\n\nMy final score can still change in both directions depending on your answers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "i9VYrlkb1T", "forum": "Am95bfE207", "replyto": "Am95bfE207", "signatures": ["ICLR.cc/2026/Conference/Submission12218/Reviewer_BXs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12218/Reviewer_BXs3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975367138, "cdate": 1761975367138, "tmdate": 1762923165084, "mdate": 1762923165084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies multi-task learning and considers a setting where several tasks can be represented through a shared low-dimensional structure together with small task-specific parameters. The main idea is to describe all tasks jointly using a compact encoder and then apply a compression-style generalization bound so that the generalization error depends on the size of this joint description rather than on the sum of per-task model sizes. The authors argue that this captures the intuition that related tasks should be \"paid for\" only once at the level of the shared representation.\n\nTo support those claims, the authors apply a known generalization bound to this setting, introduce a notion of complexity that aims to reflect the intuition above (\"amortized intrinsic dimension\"), and provide empirical evidence.\n\nTypos\n- In table 1, \"m\" is still undefined (it's only defined later, eg in Figure 1).\n- In definition 4, extra \"its\" in \"whose its base set\".\n- Line 351, a \"s\" is missing in \"Theorem 2 become ...\".\n- A point is missing in: the hypothesis box, Equation 8, Equation 11, Equation 14, Equation 17, Equation 18, and Equation 22."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The discussion seems to address natural questions that one would have when reading the results (i.e., what happens in corner cases when the tasks are very related or completely different), which I appreciate.\n- Experiments in Table 1 seem interesting and support their hypothesis."}, "weaknesses": {"value": "- The statements are close to standard compression arguments (\"if you have an encoder, then the total complexity is the size of the encoder + that of the encoding of the joint tasks\") and seem somewhat straightforward given Theorem 1 from Shalev-Shwartz & Ben-David. The authors argue in lines 399-403 that the advantage is that the bound only depends on the encoding on the tasks all together and not the sum of the individual encodings. But in all the examples given (e.g., line 334 or line 350), it seems it is actually the sum of the individual encodings. Did I miss something? Is there an example where the joint code is shorter or are we just rewriting the sum?\n- In Section 2, line 078, the authors argue \"in this work, we rely on the concept that two tasks are related, if the mutual information between their data distributions is high\", but such information is neither defined nor used throughout the paper. The only vague mention of it I could find is in Appendix A.4, where the authors write \"Given the conceptual connections between compressibility, information, and entropy, the difference between these two quantities can be seen as a computable approximation to the mutual information between the tasks\" and point to a book on Kolmogorov complexity. As currently written, this angle is more confusing than helpful; if it helps, make the link more precise, otherwise remove it.\n- The accuracy used in definition 1 is the uniform average over tasks, regardless of their difficulty. It seems like it would be important to at least comment on what happens when tasks differ in difficulty or size (although I would expect the latter to be a fairly easy extension)."}, "questions": {"value": "- Could you also report the test error in Table 3? It seems like it would be interesting to have?\n- In Table 2, the authors displayed the bounds of Theorems 2 and 3 side by side, as if they were comparable, and argue \"the fast-rate bound offers improved guarantees compared to the more elementary Theorem 2\". How so? One bounds the difference between the population and empirical risks, while the other bounds the KL. Why is it sensible to compare them?\n- In Table 1, the resulting pairs $(l, k)$ are always such that $l > k$, which, if I understand correctly, means that the tasks are (sometimes very) related. Have you tried experimenting with the reverse scenario?\n- Maybe a stupid question, but if I were to think about a naive approach building upon prior work discussed in Section 2, I would try to concatenate the datasets, train a single model with the same low-rank parametrization of Equation 1, and compute the intrinsic dimensionality for this \"aggregated\" task. If this is (much) smaller than $\\text{number of tasks} \\times \\text{ID of a single task}$, then it would suggest it is beneficial to share the representations between the tasks. Why is this wrong?\n\nP.S.: confidence-wise, I am somewhere between 3 and 4; I checked the paper carefully, and I have a theoretical background (not specifically in generalization bounds, although I am familiar with these), but I am not an expert in the multi-task learning literature specifically."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YchNWeASHv", "forum": "Am95bfE207", "replyto": "Am95bfE207", "signatures": ["ICLR.cc/2026/Conference/Submission12218/Reviewer_Rg7L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12218/Reviewer_Rg7L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094719058, "cdate": 1762094719058, "tmdate": 1762923164759, "mdate": 1762923164759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies how multi-task learning can reduce the number of parameters needed to reach high accuracy by sharing structure acrosss tasks. The authors propose the concept of amortized intrinsic dimensionality, which measure the effective number of parameters each task needs when learned jointly with other tasks. The idea is that if tasks are related, they can reuse most of the representational space, so the per-task parameter requirement drops well below that of training them independently. The authors implement this through a shared low-rank parameterization: a small set of shared components captures global structure, and each task learns a few coefficients to adapt them. They empirically show across diverse datasets that as the number of tasks increases, tha amortized parameter cost decreases while the accuracy stays mostly constant. This provides evidence that multi-task learning discovers a shared low-dimensional subspace that explains why such models compress and generalize well, resulting in better generalization bounds."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors clearly demonstrate that the multi-task setting results in higher compressibility and better bounds \"in their particular setting\". The rigorous formulation makes the multi-task efficiency measurable and comparable to the single task setting.\n- The amortized intrinsic dimensionality provides a simple defitition of 'how much sharing happens' and can be evaluated directly from experiments.\n- There is a clear scaling behavior as the number of tasks grows, where the amortized dimension drops continuouslt, which supports the central claim that related tasks occupy a common representation space."}, "weaknesses": {"value": "- The paper’s contribution is mostly conceptual and empirical as it does not show theoretically that if the tasks are related, e.g., measured in some form of distributional distance, the bounds will be tighter compared to the single task setting. It would be great to provide a sufficiency theoretical guarantee where if the task are related given a certain metric, the bounds will be theoretically improved. \n- There is no analysis to determine a priori whether the tasks are related or to identify negative transfer when unrelated tasks are included. The experimental setup is limited to very simple and clearly related tasks. \n- While results are support the main claim that related rasks result in better bound, the paper doesn’t offer prescriptive recipes for how to choose related tasks or how many to choose in order to improve generalization; back to the distance measurement question. \n- The experiments could be extended to more complicated datasets that are harder to perform well on, where the multi-task setting would have clearer empirical benefits to be tested against the bounds.\n- The bounds are not compared against sota techniques to obtain tight bounds: what if sota techniques for single task result in better bounds than the multi-task bounds?"}, "questions": {"value": "- Can you provide a theoretical sufficiency guarantee: for example, proving that if tasks are related under a defined distance metric, the amortized bound will be tighter than the single-tasks one?\n- Is there a quantitative criterion for task relatedness that could predict when multi tak will help or harm performance? Could mutual information or divergence measures serve that role?\n- How does the framework work in the presence of unrelated tasks? Can you empirically test for this case?\n- Can you suggest a practical recipe for selecting related tasks or estimating the right number of tasks to group jointly to improve generalization?\n- Would extending experiments to harder datasets or less correlated task suites change the observed scaling trends?\n- How do your amortized bounds compare numerically to state-of-the-art single-task generalization bounds? is the improvement consistent when using the best available baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bDGiaGPURa", "forum": "Am95bfE207", "replyto": "Am95bfE207", "signatures": ["ICLR.cc/2026/Conference/Submission12218/Reviewer_bsC4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12218/Reviewer_bsC4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196757748, "cdate": 1762196757748, "tmdate": 1762923164148, "mdate": 1762923164148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a parameter-sharing approach for multi-task learning based on a hierarchical parameterization of an intrinsic-dimensional model. Specifically, the method represents a set of shared parameters as (k) (l)-dimensional vectors, each projected into the network parameter space of dimension (D) using a different random matrix (P_i), and models each task as a linear combination of these shared components. The authors leverage this parameter-sharing structure to apply compression-based PAC-Bayesian bounds, leading to non-vacuous generalization guarantees in multi-task learning settings. Empirically, they demonstrate that this parameter-sharing strategy can achieve a pre-set 90% accuracy while operating in a significantly lower \"amortized\"-intrinsic dimensionality, thereby enjoying favorable generalization guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written and well organized.\n* The proposed approach is conceptually sound and well motivated.\n* The computed PAC-Bayesian bounds are non-vacuous, which is a meaningful result for multi-task learning."}, "weaknesses": {"value": "* The technical novelty appears limited.\n* The work largely builds upon the compression-based PAC-Bayesian framework of Lotfi et al. (2022) and extends it in a relatively straightforward manner.\n* The benefit of the proposed hierarchical parameter-sharing approach over existing methods is not clearly demonstrated.\n* It remains unclear how the method compares to simpler parameter-sharing schemes, such as decomposing the parameter (w) into shared and task-specific components as in Li et al. (2018).\n\n**Minor Comments:**\n\n* Reporting test set errors alongside the theoretical guarantees would strengthen the empirical evaluation.\n* For the Transformer-based experiments, it would be useful to show the performance of the pretrained Vision Transformer on CIFAR without fine-tuning.\n* Since Lotfi et al. (2022) evaluated their compression-based PAC-Bayesian framework on larger-scale datasets such as ImageNet, it would be valuable for the authors to test their approach on a comparable scale to better assess its general applicability and scalability."}, "questions": {"value": "See the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qbcYQ5agZ8", "forum": "Am95bfE207", "replyto": "Am95bfE207", "signatures": ["ICLR.cc/2026/Conference/Submission12218/Reviewer_hMHb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12218/Reviewer_hMHb"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12218/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762365844445, "cdate": 1762365844445, "tmdate": 1762923163504, "mdate": 1762923163504, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}