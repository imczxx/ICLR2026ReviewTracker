{"id": "7kb4Otwu72", "number": 4411, "cdate": 1757675799846, "mdate": 1759898033932, "content": {"title": "SpectralFlow: Geometry-Aware Mesh Animation via Spectral Coefficient Diffusion", "abstract": "Generating realistic 3D shape sequences (or 4D shapes) conditioned on actions is challenging due to high-dimensional, non-linear, and temporally coherent deformations across diverse shapes. In this work, we introduce SpectralFlow, a diffusion-based framework for action-conditioned 4D shape generation in the Laplacian spectral domain. Instead of modeling raw vertex trajectories or mesh offsets, we represent each shape using a fixed set of Laplacian eigenbases and a sequence of time-varying spectral coefficients, capturing intrinsic geometry and temporal dynamics compactly. By aligning eigenbases across shapes via sign correction and basis transformation, we establish a shared, topology-agnostic spectral space that supports consistent learning across identities and motion types. A conditional diffusion model is trained to generate spectral trajectories based on the input shape and target action, producing smooth, coherent, and semantically aligned mesh sequences. Our method avoids purely implicit modeling, which typically requires large-scale data, by leveraging lightweight geometric representations for controllable 4D shape generation. Extensive experiments show that SpectralFlow outperforms prior methods in reconstruction quality and motion generalization. Our project page is \\url{https://specflow3d.github.io.}", "tldr": "", "keywords": ["4D generation", "geometry", "mesh"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75b1c7680ba06ec5c05f00caf00bdbb1261f7099.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes SpectralFlow, a diffusion framework for action-conditioned 4D (mesh-sequence) generation that operates in the Laplacian spectral domain instead of raw vertex space. Each mesh is represented by a truncated set of eigenfunctions of the discrete Laplace–Beltrami operator and time-varying spectral coefficients; cross-shape learning is enabled by functional-map–based basis alignment and a sign-correction scheme to resolve eigenfunction ambiguities. Motion is learned as trajectories of spectral-coefficient offsets with a conditional DDPM guided by action labels and shape features (DiffusionNet). For reconstruction, the method solves a regularized least-squares problem with Laplacian and l2 terms, yielding a closed-form solution; a deformation-graph propagation step enhances high-frequency details at inference. Experiments on Image-to-3D and Animal3D report improvements on VBench metrics and user study preferences over Animate3D/AnyMesh."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. For motivation, learning in a shared spectral space with functional-map alignment and sign disambiguation elegantly addresses cross-shape consistency.\n2. For method, the reconstruction energy and closed-form solution are explicit; temporal smoothness and conditioning are straightforward.\n3. Metrics on VBench and user preference across two evaluation sets show some gains."}, "weaknesses": {"value": "1. The alignment depends on functional maps and soft correspondences; failure modes (species with weak correspondences, topological artifacts) and sensitivity to noisy templates are not systematically evaluated.\n2. There are no ablation studies in SpectralFlow, which should be added to illustrate the core component of the method."}, "questions": {"value": "1. Though following the evaluation setting of Animate3D and AnimateAnyMesh, I'm still curious about if authors can test on mesh-space or correspondence-space metric (e.g., per-vertex/part consistency, surface distortion, isometry deviation).\n\nI'm not an expert in this area, so please kindly answer the questions and weakness raised. I'll raise my score for proper answer.-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "wDCaMsCCah", "forum": "7kb4Otwu72", "replyto": "7kb4Otwu72", "signatures": ["ICLR.cc/2026/Conference/Submission4411/Reviewer_k9PP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4411/Reviewer_k9PP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761565146369, "cdate": 1761565146369, "tmdate": 1762917349845, "mdate": 1762917349845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The method proposes a novel method for geometry-aware mesh animation which generates realistic motion sequences for a variety of 3D shapes conditioned on action labels. It can also generate realistic motion sequences for unseen shapes that demonstrates strong generalization in both categories and actions. The key contribution of the method is to use a set of Laplacian eigenbases and a sequence of time-varying spectral coefficients to represent the motion sequence. A conditional diffusion model is trained to generate the time-varying spectral coefficients for geometry-aware mesh animation. Extensive experiments show that the proposed method outperforms prior methods in reconstruction quality and motion generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. The motivation of using time-varying spectral coefficients to represent motion sequence is well illustrated. \n2. The overall framework is clear and efficient. The method trains a conditional diffusion model to predict time-varying spectral coefficients, which is more efficient and generalizable compared to vertex-wise deformation prediction. In order to better model the motion sequence across different shapes, the method proposes spectral basis alignment and sign correction across different shapes based on an existing shape matching method. \n3. The paper also proposes an efficient and robust closed-form solution for the computation of an optimal spectral coefficients given ground-truth target shape, which is important for training a diffusion model.\n4. Compared to baseline methods, the proposed method achieves better text alignment, 3d alignment and motion quality."}, "weaknesses": {"value": "1. The proposed method solely uses spectral coefficients and the eigenbases of the input shape to represent motion sequence, which limits its ability to represent large motions such as large articulations.\n2. As shown in the videos provided in the project website, the proposed method cannot fully decompose the pose-dependent motion and shape-dependent motion. Therefore, it also changes the shape of the object during the animation."}, "questions": {"value": "1. How robust is the method against the shape matching method for spectral basis transformation? If the soft point-wise correspondences from Cao et al. 2023 are wrong, would it impact the final performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pjKk9sSbdi", "forum": "7kb4Otwu72", "replyto": "7kb4Otwu72", "signatures": ["ICLR.cc/2026/Conference/Submission4411/Reviewer_zU3q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4411/Reviewer_zU3q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733132775, "cdate": 1761733132775, "tmdate": 1762917349590, "mdate": 1762917349590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to animate 3d mesh. Instead of directly generating new mesh sequence, it proposes to generate the sequence in spectral space with diffusion models. To ensure this method works well, the paper also proposes a alignment method to establishe a shared spectral space for various topology."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Generate sequence in spectral domain can help to reduce the problems of previous methods such as high dimensionality and reliance on VAE latents. The method is novel.\n\n2. The proposed method provides visualization and analysis of the results and distributions.\n\n3. Instead of naively using the spectral domain, the proposed method also introduces spectral basis alignment and sign correction scheme to improve."}, "weaknesses": {"value": "1. My biggest concern is the ablation part. This paper does not include much ablation experiments. For example, although some information can be found from the method part. The paper does not mention explicitly what is the dimension reduce factor. (e.g. the original length vs. k=32). \n\n2. What if we train the model with naive mesh representation? Is it too high dimensional and can not be used? How is this naive method compared with the proposed method in terms of generation quality and speed.\n\n3. Some of the motion visualization still does not seem very good based on the link provided. For example the cameral in the last video."}, "questions": {"value": "The questions are included in the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gK6i1nOlk0", "forum": "7kb4Otwu72", "replyto": "7kb4Otwu72", "signatures": ["ICLR.cc/2026/Conference/Submission4411/Reviewer_9JJ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4411/Reviewer_9JJ1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903976157, "cdate": 1761903976157, "tmdate": 1762917349375, "mdate": 1762917349375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SpectralFlow, a diffusion-based framework for generating 4D mesh animations conditioned on an input mesh and an action label. The core idea is to operate in the Laplacian spectral domain rather than on raw vertex coordinates. The method first computes a lowfrequency laplacian eigenbasis for the input shape. To enable learning across different shapes and topologies, it aligns this basis to a canonical template using a transformation matrix and a sign correction matrix , both derived from a pretrained functional map based correspondence network. Motion is then represented as a trajectory of spectral coefficients, modeled as an offset from the input's canonical pose. A Transformer based diffusion model is trained to generate these spectral coefficient trajectories, conditioned on the shape's spectral features and the action label. Finally, a deformation graph based method propagates the resulting low frequency motion back to the original high-resolution mesh. The method is evaluated on animal datasets, reporting strong generalization to unseen shapes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1 using a diffusion model to generate trajectories of laplacian spectral coefficients is a novel approach. It leverages a compact, intrinsic, and low dimensional representation of shape, which is different from common vertex-based or implicit field-based methods.\n2. operating in a very low dimensional space (k=32 coefficients per frame) instead of on thousands of vertices, the method significantly reduces the dimensionality of the generation problem."}, "weaknesses": {"value": "1. the method only models the first k=32 low-frequency eigenfunctions. This is a strong low-pass filter on motion, making it fundamentally incapable of generating high-frequency details. The final \"smooth\" look is a limitation of the representation.\n2. the framework is not an end-to-end generator. The pipeline is quite complex and might introduce noises when generalizing to more complex motions.\n3. the visual results have clear artifacts that the entire shape varies in scale durong the motion.\n4. current results only show limited motion and yet not smooth\n5. even the authors claim they can generalize to various unseen shapes but the unseen shapes are all quadrepeds with similar topology."}, "questions": {"value": "1 The authors suggest this low-dimensional space is more data efficient. Have you performed any experiments to validate this claim directly? For example, how does the model's performance degrade when trained on a smaller fraction (50% or 25%) of the training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9aZCthGzZq", "forum": "7kb4Otwu72", "replyto": "7kb4Otwu72", "signatures": ["ICLR.cc/2026/Conference/Submission4411/Reviewer_8WcF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4411/Reviewer_8WcF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4411/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008848236, "cdate": 1762008848236, "tmdate": 1762917348955, "mdate": 1762917348955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}