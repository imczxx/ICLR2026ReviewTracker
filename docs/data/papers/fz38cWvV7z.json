{"id": "fz38cWvV7z", "number": 7420, "cdate": 1758021051430, "mdate": 1759897853898, "content": {"title": "CATFormer: When Continual Learning Meets Spiking Transformers With Dynamic Thresholds", "abstract": "Although deep neural networks perform extremely well in controlled environments, they fail in real-world scenarios where the data isn't available all at once, and the model requires an update to adapt itself to the new data distribution, which might or might not follow the initial distribution. Previously acquired knowledge is lost during such subsequent updates from new data. a phenomenon commonly known as catastrophic forgetting. In contrast, the brain can learn without such catastrophic forgetting, irrespective of the number of tasks it encounters. Existing spiking neural networks (SNNs) for class-incremental learning (CIL) suffer a sharp performance drop as tasks accumulate. We here introduce CATFormer (Context Adaptive Threshold Transformer), a scalable framework that overcomes this limitation. We observe that the key to preventing forgetting in SNNs lies not only in synaptic plasticity, but in modulating neuronal excitability too. At the core of CATFormer is the Dynamic Threshold Leaky Integrate-and-Fire (DTLIF) neuron model, which leverages context-adaptive thresholds as the primary mechanism for knowledge retention. This is paired with a Gated Dynamic Head Selection (G-DHS) mechanism for task-agnostic inference. Extensive evaluation on both static (CIFAR-10/100/Tiny-ImageNet) and neuromorphic (CIFAR10-DVS/SHD) datasets reveals that CATFormer outperforms existing rehearsal-free CIL algorithms across various task splits, establishing it as an ideal architecture for energy-efficient and true class incremental learning.", "tldr": "A spiking transformer with dynamic thresholds for scalable class incremental learning.", "keywords": ["Spiking neural networks", "Lifelong learning", "Transformer-based SNNs", "Neuromorphic chips", "Spiking threshold"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/738860175358440e8bed866cd50f7d257b737e36.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents CATFormer (Context Adaptive Threshold Transformer), a novel framework for class-incremental learning (CIL) in spiking neural networks (SNNs). Unlike conventional SNNs that suffer catastrophic forgetting when learning sequential tasks, CATFormer introduces Dynamic Threshold Leaky Integrate-and-Fire (DTLIF) neurons whose firing thresholds adapt contextually per task. Combined with a Gated Dynamic Head Selection (G-DHS) mechanism, this approach enables rehearsal-free continual learning without storing past data. The model achieves state-of-the-art results on both static datasets and neuromorphic datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n\n2. The proposed task-specific (context adaptive) dynamic neuronal thresholds seem to be an interesting design adaptive to the SNN-based transformer, simple but effective.\n\n3. The proposed method achieves strong performance lead over traditional continual learning baselines on relatively simple datasets."}, "weaknesses": {"value": "1. The compared methods are mainly very traditional continual learning methods (EWC, SI, iCaRL, DER, etc.). Is it possible to include more recent methods?\n\n2. The experiments are mainly performed with CIFAR-100 and Tiny-ImageNet of small image scales. Does the proposed method apply to larger-scale images, such as 224*224 images of ImageNet (subsets)?\n\n3. I’m not sure how the gated dynamic head selection works. Since the tasks are continually introduced, does this mechanisms also suffer catastrophic forgetting?\n\n4. Given that the proposed method targets transformer architectures, and also involves task identity inference, the authors may consider recent prompt-based continual learning methods as the baselines for comparison or implement their idea for larger-scale applications."}, "questions": {"value": "My major concerns lie in the comparison baselines and applicability of the proposed method. Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ebLzrOgghj", "forum": "fz38cWvV7z", "replyto": "fz38cWvV7z", "signatures": ["ICLR.cc/2026/Conference/Submission7420/Reviewer_vi5H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7420/Reviewer_vi5H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760883063549, "cdate": 1760883063549, "tmdate": 1762919541422, "mdate": 1762919541422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CATFormer, a framework for class-incremental learning (CIL) in Spiking Neural Networks (SNNs). The core innovation is the use of dynamic, task-specific firing thresholds (DTLIF) in a Spiking Transformer backbone, coupled with a gating mechanism for task-agnostic inference, all within a rehearsal-free paradigm. Detailed comments are listed as follows."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper brings class-incremental learning to spiking vision transformers by freezing the backbone after the base task and using per-task learnable thresholds in DT-LIF units with a lightweight routing head. This combination is new in the SNN literature and targets a gap called out in surveys. The empirical scope covers both static and neuromorphic datasets with long task sequences, which is rare for SNN-CL. The core idea is easy to follow."}, "weaknesses": {"value": "1. The method is task-conditional at inference due to a learned gate, but the paper’s framing and comparisons read as if it were task-agnostic single-head CIL. This should be stated clearly and evaluated against matched rehearsal-free transformer baselines from the ANN literature that also use routing or adapters; without these, it is hard to attribute gains to threshold learning rather than to routing.\n2. The “reverse forgetting” result lacks controls that separate router calibration from within-task classification; there is no error decomposition, no task-granularity sweeps at fixed total data, and limited class-order or seed variation for long sequences.\n3. Energy and latency claims are not supported for the two-pass inference path (gate pass plus task-threshold pass). The gate’s training procedure suggests accumulation of feature–task pairs; if any past features persist across tasks, that is a form of rehearsal and should be declared, budgeted, and audited.\n4. On Tiny-ImageNet the baselines are not aligned in architecture or rehearsal budget.\n5. Presentation issues further reduce confidence: missing standard deviations for long sequences, incomplete memory accounting for thresholds/heads/gate and any stored features, and a lack of precise taxonomy at the start of the paper."}, "questions": {"value": "1. Can you state unambiguously that the evaluation is multi-head with learned task inference (no oracle) and add a single-head variant to show threshold-only benefits?\n2. Can you report a confusion audit that breaks final accuracy into router errors versus within-task classification errors, and add experiments that keep the total data fixed while varying task granularity and class order across several seeds?\n3. Do you store any past features to train or maintain the gate? If yes, please label this as feature rehearsal and report its memory and privacy cost; if no, explain the schedule that prevents gate drift without past data.\n4. What are the measured syn-ops/MACs and wall-clock for single-pass SpikFormer, your two-pass pipeline, and an SNN-CNN baseline on the same hardware? Please include these numbers to support efficiency claims.\n5. Will you add rehearsal-free transformer CIL baselines from the ANN side that also use routing or adapters, with matched parameter and rehearsal budgets, so the effect of threshold learning can be isolated?\n6. Can you provide gating with fixed thresholds, thresholds with a shared head, and thresholds with a cumulative single-head softmax to isolate where the gains come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YS6bmg2qS7", "forum": "fz38cWvV7z", "replyto": "fz38cWvV7z", "signatures": ["ICLR.cc/2026/Conference/Submission7420/Reviewer_NBLK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7420/Reviewer_NBLK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915171536, "cdate": 1761915171536, "tmdate": 1762919540545, "mdate": 1762919540545, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CATFormer, a spiking vision transformer architecture for rehearsal-free continual learning (CL). It introduces (1) a Dynamic Threshold Leaky Integrate-and-Fire (DTLIF) neuron model, where neuron thresholds are adapted per task, and (2) a Gated Dynamic Head Selection (G-DHS) mechanism for inference. The authors claim that the proposed method mitigates catastrophic forgetting and even achieves a “reverse forgetting” effect, with evaluations on both static and neuromorphic datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic—combining continual learning with spiking transformers—is interesting and timely.\n2. The experimental scope covers both conventional and neuromorphic datasets, with relatively clear implementation details."}, "weaknesses": {"value": "1. The core contribution, dynamic thresholds, has been extensively explored in prior work. The paper does not convincingly show what is new here beyond applying adaptive thresholds to a transformer backbone. The use of a frozen encoder with learnable thresholds is a straightforward modification of known SNN paradigms.\n2. Although the paper repeatedly claims “task-specific thresholds,” it is not clear how thresholds are conditioned on tasks during inference. There is no mechanism for automatic task identification or threshold modulation—only static assignment per task. The conceptual link between “context adaptation” and continual learning is therefore weak.\n3. The G-DHS module is simply a two-layer ANN-based MLP classifier that selects the task head. This design is standard in multi-head continual learning and adds little novelty. Moreover, using an ANN for gating contradicts the claimed biological plausibility.\n4. Evaluation focuses solely on average classification accuracy. There are no metrics for forgetting, stability–plasticity trade-offs, or energy efficiency. Claims such as “reverse forgetting” lack statistical support or ablation analysis to explain the cause.\n5. The baselines are mostly classical (EWC, iCaRL, DER++) and not directly comparable to transformer-based continual learning. Missing comparisons to more relevant transformer or adapter-based CL approaches (e.g., LoRA, adapter fusion, parameter-efficient tuning) further weaken the empirical claims.\n6. Figure 2 is never properly discussed or referenced in the main text, leaving the reader uncertain about its relevance or interpretation.\n7. In Algorithm 1, the variable $\\mathcal{F}_{gate}$ (the buffer for gating) is used but never introduced or explained. Its purpose, scope, and whether it violates the “rehearsal-free” claim are unclear.\n8. In Table 2, the S6MOD baseline is an online continual learning method, not a class-incremental one; this comparison is arguably unfair and weakens the empirical credibility.\n9. The claim that the method is “robust” or achieves “robust continual learning” is not substantiated—no robustness metric, perturbation test, or generalization study is presented."}, "questions": {"value": "1. Could the authors clarify what is meant by the terms “true class incremental learning” and “long class incremental learning”? These phrases are used in the paper but are never formally defined.\n2. What exactly does the $\\mathcal{F}_{gate}$ buffer store, and does it introduce any form of rehearsal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rlbqAzRHAp", "forum": "fz38cWvV7z", "replyto": "fz38cWvV7z", "signatures": ["ICLR.cc/2026/Conference/Submission7420/Reviewer_PeF6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7420/Reviewer_PeF6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975570022, "cdate": 1761975570022, "tmdate": 1762919539998, "mdate": 1762919539998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CATFormer, a spiking transformer-based architecture for continual learning that operates without storing past data. The approach leverages task-specific dynamic threshold patterns, which are learned and saved for inference, effectively allowing the network to adapt to new tasks while retaining prior knowledge.\n\nA particularly interesting finding is the model’s ability to exhibit “reverse forgetting”. Improved performance on prior tasks as new ones are learned, suggesting positive knowledge transfer across tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed task-specific dynamic threshold method for continual learning is an interesting and novel contribution as far I as I'm aware. The authors support this with strong class-incremental learning performance. \n\nNo data replay is required.\n\nTask labels are also not required at inference as a task-prediction step takes care of it. \n\nThe knowledge transfer ability or 'reverse forgetting' is impressive to see."}, "weaknesses": {"value": "All tasks are split-class/CIL style tasks. I would be interested to see some that alternatives, such as a task with a new input distribution each time but the same class labels (permuted MNIST).\n\nsome presentation issues, I think axis labels should be added."}, "questions": {"value": "Why is the performance so much better for CATformer on the zeroth task in figure 1?  It makes me a little worried about the rest of the comparison in this figure as it's hard to know how much of the CIL performance is due to CATformer being a larger, more expressive model.\n\nWould the proposed method with task-specific thresholds extend to other continual learning settings beyond CIL? A simple task would be permuted MNIST."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q6M0U2dvs3", "forum": "fz38cWvV7z", "replyto": "fz38cWvV7z", "signatures": ["ICLR.cc/2026/Conference/Submission7420/Reviewer_p5L3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7420/Reviewer_p5L3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7420/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206719925, "cdate": 1762206719925, "tmdate": 1762919539566, "mdate": 1762919539566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}