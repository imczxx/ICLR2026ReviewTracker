{"id": "uwwo7H3XI1", "number": 8467, "cdate": 1758085077130, "mdate": 1763741336327, "content": {"title": "A Multi-Expert Ensemble Model for Long-Tailed Steel Surface Defect Detection", "abstract": "In the field of industrial steel surface defect detection, defect images often exhibit a pronounced long-tailed distribution, where tail-class—characterized by scarce samples and subtle features—are much harder to recognize than head classes with abundant data. This imbalance typically results in high miss-detection rates and bias toward head classes. To address this challenge, we propose a Multi-Expert ensemble model that integrates classification and detection experts, introducing a Two-Stage strategy into the classification branch. The framework leverages the complementary strengths of various experts, with validation-based joint optimization of confidence thresholds and expert weights, and employs parallelized training and inference to improve computational efficiency. Experimental results show that the method significantly improves the F1-score(0.912)  of tail-classes 2  and achieves state-of-the-art average Accuracy(0.989) on the long-tailed Severstal dataset, while strong performance on the balanced NEU dataset further validates its cross-distribution generalizability and practical applicability for industrial steel surface defect detection.", "tldr": "", "keywords": ["Steel Surface Defect Classification", "Long-Tailed Distribution", "Multi-Expert Ensemble Model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/273975f735e706b4906553470e121df2916bfd59.pdf", "supplementary_material": "/attachment/1a39ac154ba22ff7a725a63bfe2a0cb6d0c2a338.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a recipe for how to train a MoE-style ensemble of methods. The ensemble consists of classification networks such as VGG and ResNet and object detection networks such as YOLO and Faster R-CNN. The method is evaluated on two datasets: Severstal and NEU. The method improves upon previous results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Ensembling methods in a MoE-style network makes sense. \n- Additionally, combining both classification networks and object detection networks makes sense."}, "weaknesses": {"value": "- The paper is hard to follow at times and could use improved presentation. \n- No “newer” methods, such as RF-DETR [1] and RADIO [2], were evaluated putting into question whether such ensembling is even required\n- No qualitative examples are presented in the paper\n- It is unclear which models are used in the final ensemble. All of the proposed (4 classification and 6 detection)?\n- The ablation study does not validate that this is indeed better than a straight-up combination of networks\n\n[1]  Robinson, I., Robicheaux, P., Popov, M., Ramanan, D., & Peri, N.. (2025). RF-DETR. \n\n[2] Heinrich, G., Ranzinger, M., Yin, H., Lu, Y., Kautz, J., Tao, A., ... & Molchanov, P. (2025). Radiov2. 5: Improved baselines for agglomerative vision foundation models. In Proceedings of the Computer Vision and Pattern Recognition Conference (pp. 22487-22497)."}, "questions": {"value": "I have several questions. I have sorted them from most problematic to least problematic.\n\n1. How does the model perform when equal weights are assigned to each model?\n2. What is the performance of newer methods such as RF-DETR and RADIO?\n3. Which models are used in the final ensemble?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jMKUzriJla", "forum": "uwwo7H3XI1", "replyto": "uwwo7H3XI1", "signatures": ["ICLR.cc/2026/Conference/Submission8467/Reviewer_hTzw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8467/Reviewer_hTzw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761511036661, "cdate": 1761511036661, "tmdate": 1762920349760, "mdate": 1762920349760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-expert ensemble framework combining classification and detection models to address the challenge of long-tailed distribution in steel surface defect detection. The method incorporates a Two-Stage classification strategy and validation-based optimization of confidence thresholds and expert weights. The authors demonstrate improved performance on the long-tailed Severstal dataset and competitive results on the balanced NEU dataset, emphasizing computational efficiency and real-time applicability. While the idea of combining experts is promising and the experimental setup is thorough, the paper suffers from significant flaws in novelty, methodological clarity, and evaluation rigor, which undermine its contribution and readiness for publication."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper provides extensive experiments across multiple models and datasets, including detailed ablation studies and efficiency analysis.\n2. The focus on real-time efficiency and industrial applicability, supported by parallel training and inference strategies, could be applied to the field of industrial defect detection."}, "weaknesses": {"value": "1. The core idea of model ensemble is well-established, and the Two-Stage classification strategy is reminiscent of hierarchical or cascaded systems used in prior work. The paper does not sufficiently differentiate its contribution from existing ensemble or multi-stage methods.\n2. The method is largely empirical and engineering-focused, without introducing new theoretical insights or algorithmic innovations. The optimization of thresholds and weights is heuristic and lacks a strong mathematical foundation.\n3. While the paper compares with individual models, it does not adequately benchmark against recent state-of-the-art methods specifically designed for long-tailed recognition or defect detection, such as dynamic routing networks or advanced re-weighting/loss functions.\n4. The criteria for selecting the subset of experts (e.g., based on inference speed) are not thoroughly justified. The impact of expert diversity on ensemble performance is discussed only superficially.\n5. The claim of strong cross-dataset generalization is based on only two datasets (Severstal and NEU), which may not sufficiently represent diverse real-world scenarios. The NEU dataset is relatively small and balanced, limiting the validity of this claim.\n6. The authors acknowledge that their key Two-Stage strategy consistently leads to negligible or even negative improvements in Recall. In imbalanced detection tasks, Recall is often as important as Precision, as missing a rare defect (low Recall) can have severe consequences. The paper fails to adequately justify this trade-off or discuss its practical implications for industrial safety. Besides, it does not use more informative metrics for long-tailed problems, such as the mean Average Precision (mAP) across IoU thresholds, per-class AP, or the \"Few-Shot\" performance breakdown common in long-tailed recognition benchmarks (e.g., AP_tail). This limits a thorough understanding of the method's true performance."}, "questions": {"value": "1. How does the proposed ensemble method fundamentally differ from standard model averaging or weighted voting schemes, and what is the theoretical basis for the chosen optimization procedure?\n2. Why were certain state-of-the-art methods for long-tailed learning (e.g., BBN, BALMS, or logit adjustment techniques) not included in the comparisons?\n3. Can you provide more insight into why the Two-Stage strategy improves tail-class performance only in Precision but not in Recall, as observed in the results?\n4. How does the ensemble performance scale with the number of experts, and is there a risk of overfitting or diminishing returns with more experts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CMj3knFQrP", "forum": "uwwo7H3XI1", "replyto": "uwwo7H3XI1", "signatures": ["ICLR.cc/2026/Conference/Submission8467/Reviewer_pepC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8467/Reviewer_pepC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574655005, "cdate": 1761574655005, "tmdate": 1762920348906, "mdate": 1762920348906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an ensemble framework for steel surface defect detection targeting long-tailed class distributions, specifically addressing the Severstal Steel Defect Dataset where Class 2 represents only 1.97% of samples. The core methodology combines heterogeneous expert ensemble，adaptive joint optimization and two-stage classification strategy."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a practically relevant problem by combining heterogeneous expert types within a unified ensemble framework. The Two-Stage classification strategy, while conceptually simple, demonstrates consistent improvements for tail-class recognition across multiple backbone architectures. The adaptive joint optimization of confidence thresholds and ensemble weights represents a reasonable engineering contribution.\nThe paper is generally well-structured with clear motivation. Algorithm 1 provides reasonable procedural specification, and the extensive appendix offers detailed per-class metrics that facilitate result interpretation."}, "weaknesses": {"value": "1.\tYou repeatedly emphasize the \"significant improvement\" of \"F1-score=0.912\" on tail-class 2, but deliberately avoid several key facts. (1): Class 2 accounts for only 1.97% of the total sample, and even if F1 is raised from 0 to 1, the contribution to overall performance is extremely limited. (2) Your Overall Accuracy (0.989) comes mainly from the performance of the head classes, as seen in Table 2 where F1_0=0.9958. (3) The Precision of your ensemble on Class 2 (Table 9: P_2=0.8966) is much lower than that of the YOLO11 single model (P_2=0.9231), which suggests that your ensemble is instead increasing false positives.\n2.\tYou claim to have solved the long-tail distribution problem, but: your Two-Stage strategy essentially just changes the decision boundary and does not increase the tail-class training signals.\n3.\tThe abstract states that “parallelizing training and inference can improve computational efficiency,” but the results in Section 5.5 show that your method is not significantly faster than the fastest baseline method.\n4.\tYour ensemble requires running seven complete deep learning models (four classification models + three detection models), and even with an RTX 4090 GPU, inference time remains “primarily constrained by Faster R-CNN.”\n5.\tThe paper fundamentally fails to explain why combining these specific experts yields improvements beyond simple heuristic justification. You repeatedly claim experts have \"complementary strengths\" (Abstract, Section 1, Section 3.1) but never define what constitutes complementarity in this context. Is it diversity in learned feature representations? Different inductive biases? Orthogonal error patterns?"}, "questions": {"value": "1.\tIn ablation study, only ensemble size was tested: you only looked at 2-expert to 5-expert performance, but did not ablate individual expert types. which experts are really necessary? Are certain experts actually dragging down ensemble performance?\n2.\tThe Two-Stage approach is standard hierarchical classification used for decades. What specifically is novel about your formulation compared to existing coarse-to-fine methods?\n3.\tYou claim \"real-time\" capability and \"computational efficiency\" (Abstract, Conclusion) while Section 5.5 states inference is \"primarily constrained by the slower Faster R-CNN expert.\" Can you provide actual wall-clock inference time per image, throughput measurements (images/second), GPU memory consumption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cAOCRem4sG", "forum": "uwwo7H3XI1", "replyto": "uwwo7H3XI1", "signatures": ["ICLR.cc/2026/Conference/Submission8467/Reviewer_g9Ke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8467/Reviewer_g9Ke"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672110533, "cdate": 1761672110533, "tmdate": 1762920348246, "mdate": 1762920348246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a novel Mixture-of-Experts model for industrial steel surface defect detection based on optical images, which leverages a two-stage strategy: first, determining whether the image corresponds to a defective piece, and then, in the second stage, classifying the defect type. The authors show that the approach leads to an increased performance when considering the discriminative capabilities of the model, but also when comparing training and inference times achieved by the model w.r.t. models whose discriminative performance comes close to it. The authors have executed the experiments across two real-world datasets achieving SOTA performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The authors propose a novel Mixture-of-Experts model for industrial steel surface defect detection based on optical images. The proposed model achieves SOTA results when compared against many meaningful benchmark models across two datasets. They describe the experimental setup in detail and assess model performance across multiple dimensions (discriminative capability as well as training and inference times). Furthermore, the model is assessed by dimensions that inform production setups, making the results interesting to anyone making decisions about model deployment in an industrial setting. The paper is well-written and clearly articulated. While similar hierarchical approaches have been applied in other domains, we are not aware that it has been applied to defect detection, and certainly not in a Mixture of Experts architecture. The research is likely to have a significant impact, given that the architecture concept can be applied across a wide range of domains, leading to more efficient training and inference while achieving higher-quality outcomes."}, "weaknesses": {"value": "While the paper is solid, we would like to highlight two weaknesses:\n\n - The authors have chosen a set of metrics that require setting some threshold, conditioning the evaluation of the model's discriminative capabilities on the capability to set an appropriate cutoff. \n\n- While the authors identify that the two-stage approach leads to the best performance, they did not assess the model's performance on each stage to understand how much each stage contributes to the overall quality and pinpoint improvement opportunities."}, "questions": {"value": "1- \"The validation metric ValMetric(δ, w) is defined as the sum of Precision and Recall scores.\" -> How do the authors weigh the cases with high Precision and low Recall or high Recall but low Precision?\n\n 2- Metrics: (i)- we encourage the authors to use AUC-ROC and PR-AUC to report results as to avoid metric distortions due to the class imbalance and ensure the metrics are threshold independent. (ii) (a) What is the threshold the authors have considered to compute Accuracy and F1? (b) How did they choose it? (c) Does the threshold consider aspects such as the predictive scores distribution?, (iii) could the authors report a single metric that would summarize (a) model discriminative performance, (b) computational efficiency at model training, and (c) computational efficiency at inference time? Such a metric could be e.g., as the normalized area covered within a radar plot considering these three dimensions. The metric will most likely showcase that the two-fold approach leads to the best performance across all of the dimensions.\n\n 3- (a) How much is the dataset imbalance reduced after the first stage of the two-stage classification (here we do not mean the real class imbalance, but the one that is perceived by the downstream model based on the first stage classification)? (b) How accurate is the first-stage classifier, determining defective vs. non-defective cases? (c) How does the quality of the first-stage classifier impact the overall quality of the model?, (d) Did the authors separately measure the quality of the second-stage classifier, as to understand issues and performance opportunities in this stage?\n\n 4- Figures: ensure the colours are friendly toward color-blind people\n\n 5- Tables: (i) what do bolded results mean?, (ii) ensure the numbers are aligned to the right so that differences in magnitude become evident.\n\n 6- We encourage the authors to test whether the difference in performance noticed among the best models is statistically significant. In particular, it seems that in cases where DINO outperforms the proposed two-stage model, it does so with a very low margin. Is that difference statistically significant?\n\n 7- Figure 5,8: update the colour palette to increase contrast and legibility. Consider using colours that are friendly to color-blind people.\n\n 8- \"In our experiments, we found that head or large-area defects are better addressed by classification models, whereas tail and small-area defects, whose features are difficult to learn, are more effectively handled by detection models for precise localization.\" -> We would appreciate grounding this claim to specific experiments and results.\n\n 9- Figure 6: (i) the information from the Figure could be better conveyed in a single table. (ii) We understand that the authors have reported the averages obtained across training epochs. We encourage them to include the standard deviation among the reported values in the table."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1qtJK4g9O9", "forum": "uwwo7H3XI1", "replyto": "uwwo7H3XI1", "signatures": ["ICLR.cc/2026/Conference/Submission8467/Reviewer_n4v5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8467/Reviewer_n4v5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8467/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988637122, "cdate": 1761988637122, "tmdate": 1762920347690, "mdate": 1762920347690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}