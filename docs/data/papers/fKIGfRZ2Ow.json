{"id": "fKIGfRZ2Ow", "number": 11582, "cdate": 1758202089279, "mdate": 1763725262105, "content": {"title": "DAF: DYNAMIC ADAPTIVE FINE-TUNING OF VISION TRANSFORMERS", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is essential for training large Vision Transformers (ViTs), yet existing methods are fundamentally constrained by a static allocation paradigm, where trainable parameters are fixed before training. We argue this static approach overlooks the evolving optimization priorities of a model during learning, thereby limiting its final performance under a constrained parameter budget. Inspired by the sparse dynamic activation mechanism of neurons in the brain, we introduce a novel dynamic reconfiguration paradigm for PEFT and propose a framework named Dynamic Adaptive Fine-tuning (DAF). The core of DAF lies in its ability to periodically evaluate, select, and reshape its trainable structure during training. It employs our proposed context-aware decoupled sensitivity analysis method to purely assess the backbone network’s potential while preserving the full learning context. Subsequently, it executes the proposed Rebuild-and-Refocus update strategy. This strategy uniquely preserves learned knowledge by freezing outdated fine-tuning modules while decisively reallocating the entire parameter budget to newly identified critical regions. Extensive experiments on several highly challenging vision benchmarks show that the DAF framework not only significantly outperforms mainstream static PEFT methods but also achieves SOTA performance. Our work fundamentally challenges the static nature of the PEFT field and opens a new avenue for adapting large pretrained models more intelligently and efficiently. The code is available at https://anonymous.4open.science/r/DAF-9372.", "tldr": "We challenge the conventional static fine-tuning paradigm by proposing a dynamic adaptive method (DAF) that intelligently reshapes the model's trainable structure during training to adapt to evolving optimization priorities.", "keywords": ["Parameter-Efficient Fine-Tuning", "Vision Transformers", "Dynamic Fine-tuning", "Model Adaptation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e013f1dcc74c40a54962cf13bdc0f74d9ebe99a3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the static parameter allocation limitation of existing ViT PEFT methods by proposing the DAF framework (inspired by brain neurons' sparse dynamic activation), which uses a periodic \"perceive-decide-execute\" cycle (context-aware decoupled sensitivity analysis, budget-based elite selection, Rebuild-and-Refocus). It outperforms static PEFT methods (e.g., Adapter, LoRA) on FGVC and VTAB-1k (76.4% Top-1 on VTAB-1k, tuning only 0.22% params), works for self-supervised backbones (MAE, MoCo v3) with no inference overhead."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of dynamic fine-tuning seems quite interesting. The discussion on the working mechanism of neurons in Lines 77-80 is insightful.  \n\n2. The algorithm design is relatively interesting and appears reasonable.  \n\n3. The experiments are extensive in quantity, with a large number of comparative methods included."}, "weaknesses": {"value": "1. It is suggested to improve the clarity of Figure 2.\n\n2. There are many recent works on PEFT [1-7]; it is recommended to supplement recent studies in the Related Work section.\n\n[1] Dora: Weight-decomposed low-rank adaptation, ICML'24\n\n[2] 5%> 100%: Breaking performance shackles of full fine-tuning on visual recognition tasks, CVPR'25;\n\n[3] Adapters Strike Back. CVPR'24;\n\n[4] 1% vs 100%: Parameter-efficient low rank adapter for dense predictions, CVPR'23;\n\n[5] Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning, ICLR'23;\n\n[6] Gradient-based parameter selection for efficient fine-tuning, CVPR'24;\n\n[7] Parameter-efficient is not sufficient: Exploring parameter, memory, and time efficient adapter tuning for dense predictions, MM'24.\n\n3. Some recent work demonstrate that PEFT can outperform full fine-tuning on complex visual tasks. Given that this paper is submitted to ICLR, the authors should compare the proposed method with more competitive recent approaches.\n\n4. Recent works have begun to conduct comparisons on complex image recognition tasks (such as detection and segmentation); it is suggested that the authors add relevant experiments.\n\n5. The proposed method requires multiple computations during the parameter update process, and the authors should discuss the impact of this process on training time and inference time.\n\n6. What are the essential differences between the method proposed by the authors and the recently emerging series of \"parameter selection\" works."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mrwzskA5vU", "forum": "fKIGfRZ2Ow", "replyto": "fKIGfRZ2Ow", "signatures": ["ICLR.cc/2026/Conference/Submission11582/Reviewer_CQ72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11582/Reviewer_CQ72"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726133245, "cdate": 1761726133245, "tmdate": 1762922667535, "mdate": 1762922667535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new PEFT method called DAF (Dynamic Adaptive Fine-tuning). The main idea is that a model's optimization priorities evolve during training, so the trainable parameters should not be fixed from the start and they should be able to adapt with the training / finetuning. It works by freezing old modules to preserve learned knowledge and reallocates the entire parameter budget to new critical areas adaptively. The main result is that this dynamic approach achieves state-of-the-art (SOTA) performance on benchmarks and demonstrates the superiority of adapting the fine-tuning structure throughout the training process"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and proposes an interesting way to solve the problem of adapting the tuning of relevant parameters during fine-tuning. The idea makes sense at a high-level and the authors do a good job validating their results experimentally. This idea of dynamic parameter adaptation is very compelling and a new approach as far as I'm aware. \n\nThe paper has solid experimental results and their method DAF seems to outperform other static, and more widely used, PEFT methods."}, "weaknesses": {"value": "The DAF method relies on what they call the \"Rebuild-and-Refocus\" strategy, the main objective of which is to \"preserve learned knowledge\" by freezing weights of outdated LoRA modules. Even though those LoRA modules are frozen, arent there are other sources of knowledge loss within the algorithm itself? For example, the optimizer state is re-initialized at every dynamic interval. Depending on the optimizer used (eg momentum or Adam etc) those optimizer statistics would be totally lost which can result in unstable training and slow convergence. Was this effect measured in a meaningful way and is there any way to avoid this behavior? Or did you find that this disruption was actually not that detrimental in practice?"}, "questions": {"value": "Overall the dynamic reconfiguration and the Rebuild-and-Refocus strategy make intuitive sense. The paper describes a three stage perceive-decide and execute process that occurs in each dynamic cycle and this helps to solve the signal to noise problem. However, the sensitivity score is computed in relatively few batches of data which can be noisy and this is score is what's used to reallocate resources determining the next set of top-k parameters. Wouldn't this also introduce its own form of noise? Or is it negligible in practice? How robust is DAF to the noise and variance that is introduced in this step of the algorithm? The paper does explore sensitivity of DAF to other hyperparams in the ablation studies of section 4 which is nice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l2kmqNlZhA", "forum": "fKIGfRZ2Ow", "replyto": "fKIGfRZ2Ow", "signatures": ["ICLR.cc/2026/Conference/Submission11582/Reviewer_rjZT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11582/Reviewer_rjZT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928105777, "cdate": 1761928105777, "tmdate": 1762922666968, "mdate": 1762922666968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Dynamic Adaptive Fine-tuning (DAF), a dynamic Parameter-Efficient Fine-Tuning (PEFT) framework that challenges the static allocation paradigm prevalent in existing methods. Unlike conventional approaches that fix trainable parameters before training, DAF periodically reconfigures its trainable structure during training. The method achieves state-of-the-art results on VTAB-1k and FGVC benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is easy to follow.\n2. The main results evaluation on different benchmarks with different pretrained models is comprehensive."}, "weaknesses": {"value": "**1. The motivation is unclear.** The authors motivate on reducing the PEFT parameter budget through gradually reducing learnable parameters. However, all the changed parameters need to be saved using extra storage, including early-stage ones. The motivation and the method are not matched.\n\n**2. The results lack a training efficiency comparison.** The elite selection and Rebuild-and-Refocus Update in each dynamic analysis point may result in extra overhead. A runtime comparison (e.g., GPU hours or wall-clock time) against static PEFT baselines would be valuable for assessing the trade-off between performance gains and computational overhead.\n\n**3. The rationale of DAF outperforming Static DAF is not well explained.** Both DAF and its static counterpart did one round of elite selection. The multiple rounds in DAF only progressively freeze more modules. Is it caused by training instability, overfitting, or an inferior hyperparameter? Does this mean that the learning rate in Static DAF is too high or the learnable parameter is too many? This would benefit from a comprehensive comparison of DAF vs. a spectrum of Static DAF (using different numbers of learnable parameters, different lr)"}, "questions": {"value": "1. Are the reported tuned parameters the average number of tuning parameters during training, or all the tuned parameters in every stage?\n\n2. Does Static DAF select the same group of parameters for training compared to DAF in the first stage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "n2rHbxXMbl", "forum": "fKIGfRZ2Ow", "replyto": "fKIGfRZ2Ow", "signatures": ["ICLR.cc/2026/Conference/Submission11582/Reviewer_hR8X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11582/Reviewer_hR8X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943046143, "cdate": 1761943046143, "tmdate": 1762922666632, "mdate": 1762922666632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DAF (Dynamic Adaptive Fine-tuning), a method that adjusts layer-wise learning rates and regularization coefficients dynamically during fine-tuning based on gradient statistics. The goal is to achieve a balance between frozen and fully fine-tuned strategies by adapting each layer’s update strength in real time. The authors claim that DAF improves adaptation efficiency and stability while maintaining competitive performance. Experiments are conducted on two datasets (FGVC and VTAB-1k) using ViT-B as the backbone, and the paper reports small performance gains compared to standard fine-tuning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method is simple to implement and can be easily integrated into existing fine-tuning pipelines.\n\nThe overall paper is well-structured, with intuitive explanations and clear algorithmic descriptions."}, "weaknesses": {"value": "The experimental validation is quite limited. Only two datasets (FGVC and VTAB-1k) are used, and there are no results on large-scale or multimodal settings to justify the claim of “general applicability.” \n\n\nThe backbone choice is also narrow, the paper only tests ViT-B, without any analysis on deeper or larger architectures or comparisons to CNN-based backbones such as ConvNeXt.\n\nThe pre-training weights are still based on ImageNet, while current practice has shifted toward stronger foundations such as CLIP, EVA, or even VLM-based checkpoints. Evaluating DAF on both conventional and modern pretrained models would be necessary to demonstrate relevance in today’s context.\n\nThe comparison baselines are outdated, and the authors do not cite the papers corresponding to each method in the result tables. Many of the compared approaches are from 2022 or earlier, which weakens the experimental credibility. \n\nFinally, the reported performance gains are marginal, most improvements are within 0.5%, which likely falls within statistical noise and does not convincingly support the claimed advantages of DAF."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sWmoJFxUnt", "forum": "fKIGfRZ2Ow", "replyto": "fKIGfRZ2Ow", "signatures": ["ICLR.cc/2026/Conference/Submission11582/Reviewer_TK3t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11582/Reviewer_TK3t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11582/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991315742, "cdate": 1761991315742, "tmdate": 1762922665855, "mdate": 1762922665855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}