{"id": "7kLNGaAHaw", "number": 9055, "cdate": 1758108773587, "mdate": 1759897746076, "content": {"title": "Architecture-Agnostic Test-Time Adaptation via Backprop-Free Embedding Alignment", "abstract": "Test-Time Adaptation (TTA) adapts a deployed model during online inference to mitigate the impact of domain shift. While achieving strong accuracy, most existing methods rely on backpropagation, which is memory and computation intensive, making them unsuitable for resource-constrained devices. Recent attempts to reduce this overhead often suffer from high latency or are tied to specific architectures such as ViT-only or CNN-only.\nIn this work, we revisit domain shift from an embedding perspective. Our analysis reveals that domain shift induces three distinct structural changes in the embedding space: translation (mean shift), scaling (variance shift), and rotation (covariance shift). Based on this insight, we propose Progressive Embedding Alignment (PEA), a backpropagation-free and architecture-agnostic TTA approach. By applying a novel covariance alignment procedure at each intermediate layer, PEA efficiently corrects the embedding distortions with only two forward passes. Extensive experiments demonstrate that PEA achieves state-of-the-art performance in both accuracy and efficiency, while also proving versatile across different architectures including ViTs and CNNs.", "tldr": "This paper propose a lightweight forward-only test-time adaptation approach using covariance alignment in embedding space.", "keywords": ["Test-time adaptation; efficiency; feature space; embedding alignment"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc6009cb21a3615dd34fa4ec97e38a3d3f5822ea.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a new backpropagation-free test-time adaptation method to calibrate the pre-trained model to the target domains using two forward passes per sample. The authors observe that the distribution shift can cause three distinct structural changes in the features of intermediate layers. Based on this observation, they design a Progressive Embedding Alignment (PEA) to correct the feature distortions. One forward is used to estimate the level of distribution shift, and the other is used to recompute the intermediate features to tackle the distribution shift. Moreover, they also use the exponential moving average and data argumentation to enable stable and robust adaptation. Experimental results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors empirically reveal that the distribution shift can cause the three distinct structural changes in the feature space, including translation (mean shift), scaling (variance shift), and rotation (covariance shift).\n2. The authors propose a Progressive Embedding Alignment (PEA) to calibrate the feature without updating the parameters of the model, which can originally mitigate catastrophic forgetting. \n3. The results of the ablation study demonstrate the necessity of each component in the proposed method."}, "weaknesses": {"value": "1. The authors propose a feature re-computation method to calibrate the pre-trained model. However, a similar idea that updates the intermediate features has been proposed by the previous method[A]. What is the essential difference and advantage of the proposed method? \n2. The authors show the distinct structural changes of the ViT model in Figure 1. However, it is unclear whether this observation is related to this specific architecture. More discussion or experimental results on CNN models or models with different normalization layers should be provided.\n3. The proposed method requires computing the statistics of a batch of test samples, so the distribution of different domains in the same batch will greatly affect the accuracy of the statistical distribution. When the test samples within the same batch come from different domains (mixed data domains), the proposed method may suffer from unreliable distribution estimation and cannot guarantee a stable performance.\n4. The Spike detection is proposed to detect the domain changes and then reset the estimated statistical information for the new domain. However, it is unclear how to decide the threshold \\theta_{ent} for different domains, especially under challenging test scenarios such as imbalanced label shifts per SAR.\n5. The data enrichment is designed to enhance the estimation of the target batch distribution by generating K augmented views for each test sample. However, it is unclear how the hyperparameter K affects the adaptation performance and the computational cost.\n6. More BP-free test-time adaptation methods should be discussed and compared in the experiment, including but not limited to [B-D].\n\n[A] LeanTTA: A Backpropagation-Free and Stateless Approach to Quantized Test-Time Adaptation on Edge Devices, arXiv 2025.\n\n[B] Test-Time Classifier Adjustment Module for Model-Agnostic Domain Generalization, NeurIPS 2021.\n\n[C] Test-Time Model Adaptation for Quantized Neural Networks, ACM MM 2025.\n\n[D] Backpropagation-Free Test-Time Adaptation via  Probabilistic Gaussian Alignment, NeurIPS 2025."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PzJtCN65lJ", "forum": "7kLNGaAHaw", "replyto": "7kLNGaAHaw", "signatures": ["ICLR.cc/2026/Conference/Submission9055/Reviewer_Lsas"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9055/Reviewer_Lsas"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761721586581, "cdate": 1761721586581, "tmdate": 1762944859250, "mdate": 1762944859250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes PEA, a backpropagation-free and architecture-agnostic test-time adaptation (TTA) approach that aims to enhance adaptation while reducing computational bottlenecks. Unlike most TTA methods that rely on backpropagation—which is computationally expensive at test time—or are architecture-specific, PEA provides a general alternative. The authors observe that the representations of shifted domain samples across layers are geometric transformations of the source representations, primarily translation, scaling, and rotation. Building on this insight, PEA performs two forward passes: the first collects batch statistics per layer, and the second applies a correction based on source statistics. Experimental evaluations on ResNet-50 and ViT-B across corrupted datasets (CIFAR-10-C, CIFAR-100-C, and ImageNet-C) demonstrate accuracy gains when combined with test-time augmentation. Additional experiments on low-resource devices further highlight the method’s memory efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well written and easy to follow.\n\n* The paper proposes a method with two interesting properties: backpropagation-free and architecture-agnostic, which increases the applicability of TTA across different models and devices.\n\n* The evaluation on low-resource devices is a valuable contribution, highlighting the method’s efficiency in practical settings."}, "weaknesses": {"value": "* The paper emphasizes the existence of discrepancies in representations between source and target distributions and their variation across layers. However, this adds limited new understanding. By definition, distribution shifts alter centroids and other statistical descriptors, and much of the domain adaptation literature already focuses on aligning source and target distributions.\n\n* The hypothesis that shifts reduce to simple geometric transformations in embedding space is overly strong, and the visualizations in Figure 1 and Appendix A do not provide convincing evidence. For example, as long as representations differ, a translation between centroids is always possible and therefore not surprising. Moreover, these are linear operations, and evidence from only two transformations is insufficient to claim generality. Non-linear or more complex transformations may well occur in intermediate representations, and the current evidence does not rule this out.\n\n* A critical assumption is that source statistics for all layers are accessible at test time. This may not hold in many realistic scenarios and sets the method apart from approaches that do not make such an assumption. This should be acknowledged explicitly as a limitation.\n\n* The conclusions in lines 160–163 and 139-141 overlook prior fine-grained analyses, such as Sahoo et al. and Maharana et al., which emphasize that not all layers are equally affected and advocate for layer-wise adaptation. \n\n\n* The detection mechanism based on the entropy of predictions is limited. Entropy is a weak OOD detection score compared to alternatives available in the literature (e.g., OpenOOD benchmark).\n\nReferences\n\nSahoo, Sabyasachi, et al. \"A layer selection approach to test time adaptation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 19. 2025.\n\nMaharana, Sarthak Kumar, Baoming Zhang, and Yunhui Guo. \"Palm: Pushing adaptive learning rate mechanisms for continual test-time adaptation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 18. 2025.\n\nOpenOOD benchmark: https://github.com/Jingkang50/OpenOOD"}, "questions": {"value": "* Class representations appear already well separable after only 3 blocks (out of 9). How do you explain this observation?\n\n* Line 167: “This systematic shift does not lead to a random feature distortion; rather, it manifests as a geometric rotation and shearing of the entire feature cloud.” Why would we expect a random feature distortion in the first place?\n\n* In pass1 paragraph of line 290: How are K views of multiple augmentations obtained within a single forward pass? Please clarify this point.\n\n* For clarity, in the main text, please specify what a “block” corresponds to in the ViT architecture, and explicitly indicate that the referenced block 3 is the third out of nine blocks.\n\n* Please cite prior work using test-time augmentation (e.g., Simonyan and Zisserman, 2014).\n\nReference\n\nSimonyan, Karen, and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OmZVEQ4TOd", "forum": "7kLNGaAHaw", "replyto": "7kLNGaAHaw", "signatures": ["ICLR.cc/2026/Conference/Submission9055/Reviewer_9W85"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9055/Reviewer_9W85"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810141687, "cdate": 1761810141687, "tmdate": 1762920766274, "mdate": 1762920766274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Progressive Embedding Alignment (PEA), a backpropagation-free and architecture-agnostic Test-Time Adaptation (TTA) method. The key idea is to interpret domain shifts as three geometric transformations in the embedding space—translation (mean shift), scaling (variance shift), and rotation (covariance shift)—and correct them progressively via Whitening-Coloring Transform (WCT)–based covariance alignment. PEA performs only two forward passes per test batch, without modifying model weights, and can be applied to both CNNs and ViTs. Experiments on CIFAR10-C/100-C and ImageNet-C show that PEA achieves competitive accuracy with minimal memory and latency, demonstrating its suitability for real-time or edge deployment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The embedding-space interpretation of domain shift (translation, scaling, rotation) is elegant and intuitive. It provides a clear geometric motivation for the proposed covariance alignment strategy.\n\nPEA is architecture-agnostic for both CNNs and ViTs.\n\nThe fully forward-only, memory-light, and backprop-free natures make PEA ideal for edge devices. It achieves low latency (~0.3s) and small memory footprint (<1GB) while maintaining strong accuracy.\n\nDevice-level tests on Jetson Orin Nano enhance practical relevance."}, "weaknesses": {"value": "As the proposed method relies on statistical estimation, its performance degrades under small batch sizes. And, I am wondering how does the method performs when the batch size is as small as 2? Moreover, with augmentation enabled, could PEA still operate effectively with a batch size of 1?"}, "questions": {"value": "How to decide the threshold for detecting domain changes (Eq. 7)  across different models and domains during online testing? An ablation study (with and without Eq.7) and sensitivity analysis on this threshold would be helpful.\n\nHow does the number of augmented views in PEA affect performance? More detailed sensitivity analyses on this factor are preferred.\n\nHow does PEA perform under mixed domain shifts? It would be helpful to see results for both CNN and ViT backbones in such scenarios.\n\nThe reported results of SPA appear to be lower than those in the original paper. Could the authors clarify this discrepancy? Since SPA is a strong backpropagation-based method, it is understandable that PEA (focus on forward-only) has lower performance than SPA, but ensuring consistent and accurate SPA results would make the comparison more convincing.\n\nSuggestion:\n\nFOA also includes an alignment component, namely “activation shifting,” which appears conceptually related to the proposed PEA. However, PEA performs alignment in a more fine-grained, layer-wise, and comprehensive manner. It would be much better to discuss this relationship in the paper.\n\nI would like to consider increasing my score if these are well addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qe6gZvicHa", "forum": "7kLNGaAHaw", "replyto": "7kLNGaAHaw", "signatures": ["ICLR.cc/2026/Conference/Submission9055/Reviewer_bwXx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9055/Reviewer_bwXx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896753882, "cdate": 1761896753882, "tmdate": 1762920765823, "mdate": 1762920765823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **Progressive Embedding Alignment (PEA)**, a novel Test-Time Adaptation (TTA) method that is **backpropagation-free**, **architecture-agnostic**, and highly efficient. The authors aim to overcome two major limitations of existing TTA work: 1) the high computational and memory overhead of backpropagation-based methods (e.g., Tent, EATA), which makes them unsuitable for edge devices , and 2) the architecture-specificity of recent efficient methods, which are often tailored for either CNNs or ViTs, but not both.\n\nThe core idea is motivated by a principled analysis of domain shift. The authors empirically demonstrate that domain shifts induce a combination of three consistent geometric distortions in the intermediate embedding space:\n1.  **Translation** (mean shift) \n2.  **Scaling** (variance shift) \n3.  **Rotation** (channel-wise covariance shift) \n\nPEA works by explicitly correcting these distortions. In an offline step, it pre-computes and stores the source-domain statistics (mean $\\mu_{s,l}$ and covariance $\\Sigma_{s,l}$) for each block $l$. Then, at test time, it uses a two-pass, forward-only procedure:\n* **Pass 1:** Estimates the statistical distance between the current batch and the source to compute a layer-wise alignment weight $w_l$.\n* **Pass 2:** Applies a Whitening-Coloring Transform (WCT) to realign the batch features to the source statistics. The final, corrected feature $F_l'$ is a weighted blend of the original and aligned features, $F_{l}^{\\prime}=(1-w_{l})F_{l}+w_{l}Y_{l}$, which prevents over-correction ."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Clear Motivation and Novelty: The analysis of domain shift as a geometric transformation (translation, scaling, rotation) in the embedding space is intuitive and provides a strong foundation for the proposed method. The idea of reversing these distortions via a weighted WCT, without any gradient updates, is a novel and elegant approach to TTA.\n\n- Exceptional Efficiency (Memory and Latency): The paper's primary contribution is its practicality. By being backprop-free and requiring only two forward passes , PEA is significantly more efficient than SOTA methods like CMF, SPA, or EATA. This is powerfully demonstrated in the edge device evaluation (Table 4), where PEA runs successfully on a Jetson Orin Nano, while numerous backprop-based baselines (Tent, EATA, CMF, etc.) fail due to OOM errors.\n\n- Strong Empirical Results: PEA (especially with augmentation) achieves state-of-the-art or highly competitive accuracy on all three benchmarks (ImageNet-C, CIFAR10-C, CIFAR100-C). \n\n- Generality (Architecture-Agnostic): This is a key advantage. The method works \"out-of-the-box\" for both CNNs (ResNet-50) and Transformers (ViT-Base) using the identical procedure"}, "weaknesses": {"value": "- Source Data Requirement: The method's primary limitation, which is acknowledged by the authors26, is that it is not \"source-free.\" It requires access to the original source training dataset in the offline stage to pre-compute the source statistics ($\\mu_{s,l}$, $\\Sigma_{s,l}$)27. While this is a valid setup for TTA, it is a stronger assumption than methods that only require the pre-trained model.\n\n-  Storage Overhead: While the inference compute/memory is low, PEA introduces a storage cost by requiring the mean vector and covariance matrix for each aligned block to be stored. The paper mentions this is minimal (~30MB for ViT-Base), but this cost should be explicitly reported for all models in the main tables to provide a complete picture of the resource trade-offs.\n\n- Statistics Estimation: The method's effectiveness hinges on the quality of the estimated test-time statistics ($\\mu_{t,l}$, $\\Sigma_{t,l}$)28. While the EMA helps, the authors note that performance could degrade with extremely small batches (e.g., BS=1) or high class imbalance29. A formal evaluation at BS=1, a true \"online\" setting, would be valuable to fully test this boundary condition."}, "questions": {"value": "- Are recent methods used to reduce the overhead of backprop only tied to vit or cnn or have high latency? if so can you share related work to that?\n\n-  In equation 1 why you didn't use the mahalanobis distance between each input in the batch and the source distribution instead of relying on the batch statistics and in that case even for bs 1 the method shall work preoperly out of the box without having to think about the scale difference between mean and variance as the current distance can be overwhelmed by one measure over the other (if we have more mean shift or more variance). \n\n- Can you ablate the choice of distance and what happens if we simply use the shift in mean ?\n\n- how do you select the fixed threshold used to flag spikes in feature alignment\n\n- Can you ablate the choice of k views augmentaion to see its effect on the main results ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FF7TRncUaF", "forum": "7kLNGaAHaw", "replyto": "7kLNGaAHaw", "signatures": ["ICLR.cc/2026/Conference/Submission9055/Reviewer_mDae"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9055/Reviewer_mDae"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9055/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923675231, "cdate": 1761923675231, "tmdate": 1762920765359, "mdate": 1762920765359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}