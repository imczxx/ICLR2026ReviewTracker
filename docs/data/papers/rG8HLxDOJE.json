{"id": "rG8HLxDOJE", "number": 16053, "cdate": 1758259211320, "mdate": 1759897265345, "content": {"title": "Graph Autoencoder-based Motif Extraction Algorithm for Molecular Representation Learning", "abstract": "Molecular representation learning using graph neural networks(GNNs) has become a research hotspot in the fields of chemistry and biology in recent years. The pretraining-finetuning paradigm has been widely used to address the issue of limited labeled molecular datasets, achieving great success due to its ability to leverage large amounts of unlabeled data. Additionally, frequently occurring molecular substructures, known as motifs, can often capture the local information and higher-order connectivity  of molecules more effectively, providing a better paradigm for pretraining. However, existing motif extraction methods face the issues of relying on domain-specific knowledge and neglecting the local structural information of atoms. To address these problems, we propose a motif self-extraction method based on a graph autoencoder. This method utilizes the graph autoencoder for structural reconstruction, allowing the model to automatically identify frequently occurring local patterns. Furthermore, we also propose a motif-based pretraining method that simultaneously captures the local information and higher-order connections of both the molecular graph and the motif graph. We pretrain on the 250K Zinc15 dataset and conduct downstream performance prediction on eight commonly used molecular property prediction datasets. Experimental results demonstrate the effectiveness of our method.", "tldr": "", "keywords": ["Molecular Representation Learning", "Graph Neural Network"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d6f2a4af61ec2e40c806baced3c974c977f666d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This study tackle the molecular representation learning problem and proposes a motif self-extraction method using graph autoencoder, which automatically identify frequently occurring local patterns. In addition, a motif-based multi-level molecular pretraining method is proposed to capture the local information and higher-order connections. Experiments were done on the 250K Zinc15 dataset to predict eight typical molecular properties."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Motif-based contrast learning."}, "weaknesses": {"value": "* There is considerable room for improving the writing to make the manuscript clearer and easier to understand.\n* Section 3.2.2 is unclear. Please explain \"permutation and concatenation\". \n* Section 3.2.3, what is the definition of a \"motif class\"?\n* Table 2: The advantage of the proposed method over the baselines is not statistically significant in more than half of the prediction tasks."}, "questions": {"value": "* Table 1: What does \"Local-Struc\" mean? Aren't motifs local structures? Methods for extracting motifs should consider local structure information.\n* Line 85: Why need a graph autoencoder to evaluate the connection frequencies between atom pairs? Can this be simply done by counting? There are plenty of unsupervised data mining methods for discovering graph motifs. The author does not appear to be aware of them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hs6A3wfgic", "forum": "rG8HLxDOJE", "replyto": "rG8HLxDOJE", "signatures": ["ICLR.cc/2026/Conference/Submission16053/Reviewer_hpJk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16053/Reviewer_hpJk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710232688, "cdate": 1761710232688, "tmdate": 1762926247832, "mdate": 1762926247832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GAME, a motif extraction algorithm that trains an autoencoder on structural reconstruction, which groups atoms with high-frequency connection patterns. The authors then introduce MBMP, a multi-level pretraining framework that uses these extracted motifs to capture both local and higher-order molecular information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Motif extraction method is a crucial topic that needs more study in the community.\n2. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. My concern is that if the reconstruction task perfectly learns the identity function (making $F(\\tilde{A})$ equal $A$, especially if  $\\tilde{A}=A$, how can this same process also function as a filter to remove low-frequency edges and identify motifs?\n2. What is a graph encoder used in the framework? A GNN or MLP?\n3. I'm trying to understand the impact of the additional filtering step for low-frequency motifs. What proportion of motifs does this step remove, and how sensitive are the final results to this filtering? This is essential for accurately judging the effectiveness of the core motif extraction method.\n4. The experimental results are not promising. The proposed method does not outperform other motif extraction methods.\n5. Some motif-based molecular representation learning related works need to be discussed."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HRJ6E7E2Rl", "forum": "rG8HLxDOJE", "replyto": "rG8HLxDOJE", "signatures": ["ICLR.cc/2026/Conference/Submission16053/Reviewer_qKPK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16053/Reviewer_qKPK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761850821978, "cdate": 1761850821978, "tmdate": 1762926247237, "mdate": 1762926247237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors proposes GAME, a graph autoencoder-based motif extraction method that is trained to  perform structural reconstruction on molecular data. The authors claim that this is the first such GNN-based motif extraction method. They also introduce MBMP, a motif-based multi-level molecular pertaining framework for leveraging structural motif information in molecules via contrastive learning, cross-level matching learning, and prototype prediction learning. The paper further includes experiments on molecular property prediction datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- I like the approach of incorporating several different losses in training to learn a more robust representation for a given molecule\n- The experimental ablations for the various components of MBMP and the effect of different bond-breaking thresholds k and minimum motif frequencies N provides interesting additional insights into the method presented."}, "weaknesses": {"value": "Major Weaknesses:\n- There seem to be significant parts of the method that are missing or not presented properly in this work. A few examples are as follows:\n    -  Line 194: “which uses its aggregation mechanism to capture the local structural pattern” - what is the aggregation mechanism? Depending on what aggregation is used, this has significant impacts on the expressive power of a given model\n    - Line 197: “we use a decoder to calculate the probability of an edge” - again, what decoder do you use here?\n    - Line 281: “MAP(·) is a mapping function that establishes the relationship between motifs and atoms” - this MAP function is not formally defined anywhere in the paper. What kind of mapping are you using?\n- Why did the authors not compare against state of the art motif/hierarchical-based GNNs or graph transformer based architectures [1-6]? If one of the arguments is that this method can compete with domain-knowledge informed approaches, this is an essential comparison to show.\n- All experiments are on the MoleculeNet dataset, which is known to have several flaws. Did the authors evaluate on any other molecular datasets, such as from ADME [7], OGB [8], LRGB [9], etc.? While all benchmarks have some issues to them, the experimental section of this paper would be greatly strengthened if success is shown across multiple datasets.\n\nMinor Weaknesses:\n- While the motivation of reducing reliance on chemical domain knowledge is somewhat sound, there are many cases where including it is greatly beneficial. Is there a way to incorporate this additional knowledge if desired?\n- Although the paper is generally well laid out, it's somewhat sloppily written - there are several typos and misformated references present throughout the work that should be fixed and cleaned up.\n\nOverall I believe this paper still requires significant work, and given the fact that there is no appendix or supplementary material, this submission seems too preliminary to be a full conference paper. Therefore, I recommend it for rejection.\n\nReferences:\n1. Bouritas et. al. Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. In ICLR, 2021.\n2. Barcelo et. al. Graph Neural Networks with Local Graph Parameters. In NeurIPS, 2021.\n3. Bodar et. al. Weisfeiler and Lehman Go Cellular: CW Networks. In NeurIPS, 2021.\n4. Jin et al. Homomorphism Counts for Graph Neural Networks. In ICML, 2024.\n5. Luo et al. Enhancing Graph Transformers with Hierarchical Distance Structural Encoding. In NeurIPS, 2024.\n6. Bao et al. Homomorphism Counts as Structural Encodings for Graph Learning. In ICLR, 2025.\n7. Fang et al. Prospective validation of machine learning algorithms for absorption, distribution, metabolism, and excretion prediction: An industrial perspective. JCIM 2023. \n8. Hu et al. Open graph benchmark: Datasets for machine learning on graphs. In NeurIPS 2020.\n9. Dwivedi et al. Long Range Graph Benchmark. In NeurIPS 2022."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gQRLBeAoc9", "forum": "rG8HLxDOJE", "replyto": "rG8HLxDOJE", "signatures": ["ICLR.cc/2026/Conference/Submission16053/Reviewer_nN47"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16053/Reviewer_nN47"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921454328, "cdate": 1761921454328, "tmdate": 1762926246785, "mdate": 1762926246785, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a motif self-extraction method based on a graph autoencoder, as well as a motif-based pretraining method for molecular graph representation learning. First, assuming that frequently occurring substructural motifs in molecular tasks can be used in pretraining, the paper introduces a simple graph autoencoder to extract these motifs. By overlaying the adjacency matrices of the encoder-decoder output and the input graph and applying thresholding to remove low-value entries, the method fragments the molecular graph and builds a motif vocabulary. Based on this motif self-extraction, the paper proposes a pretraining strategy that combines three types of loss: motif-based contrastive learning, motif-to-atom level matching, and prototype learning of motif types. Experimental results show that this pretraining–fine-tuning approach outperforms benchmark methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents a simple but quite effective \"self-extraction\" method for motif vocabulary building. The idea of leveraging the output of a graph autoencoder to effectively fragment molecular structures for motif discovery is itself a very interesting concept.\n- Building on this motif extraction approach, the paper proposes a motif-aware pretraining strategy for learning molecular representation, and demonstrates its effectiveness through experimental comparisons. This reflects recent insights suggesting that incorporating motif-level knowledge is quite effective for molecular representation learning, making it a noteworthy contribution."}, "weaknesses": {"value": "- While the main focus of the paper is on pretraining, it's unfortunate that there's no analysis comparing the extracted motifs to those obtained using conventional methods such as BRICS since BRICS is also based on fragmentation of the molecular graphs. Understanding how similar or different these motifs are would have added valuable insight.\n- The paper proposes both a motif extraction method and a corresponding pretraining strategy, but an ablation study is essential to verify the effectiveness of each component individually.\n- For instance, MGSSL (Zhang et al., 2021), which is cited in the paper, uses BRICS as its motif vocabulary. Therefore, it’s important to investigate whether simply replacing BRICS with the proposed self-extracted motifs could lead to improved performance. Similarly, for other motif-aware approaches that rely on explicit motif vocabularies, it would be necessary to test whether swapping in the proposed method yields benefits.\n- Conversely, the proposed pretraining strategy should, in principle, be applicable to existing motifs as well. This raises the question of how effective it would be when used with BRICS motifs, for example. As I understand it, Eq. (15) does not incorporate the loss used for vocabulary construction in Eq. (3), meaning that vocabulary building is treated separately from pretraining and is not part of an end-to-end framework.\n- The individual components of the pretraining strategy are conceptually sound, but they appear incremental and lack novelty. A clearer explanation of how this approach differs from existing methods such as MGSSL is essential."}, "questions": {"value": "- Have you investigated how the motifs extracted by your proposed graph autoencoder-based method compare to those obtained through established motif extraction approaches, such as BRICS in terms of similarity or differences?\n- In existing motif-based pretraining approaches, such as MGSSL (Zhang et al., 2021), have you investigated how replacing the BRICS motif vocabulary with the one obtained through your proposed graph autoencoder-based self-extraction affects prediction accuracy?\n- In your proposed pretraining strategy, have you examined how using existing vocabulary building strategies such as BRICS affects prediction accuracy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "71evskdH7E", "forum": "rG8HLxDOJE", "replyto": "rG8HLxDOJE", "signatures": ["ICLR.cc/2026/Conference/Submission16053/Reviewer_GKrJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16053/Reviewer_GKrJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934565647, "cdate": 1761934565647, "tmdate": 1762926246306, "mdate": 1762926246306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}