{"id": "kpwGlSY1He", "number": 14499, "cdate": 1758237367964, "mdate": 1759897366740, "content": {"title": "A Diffusive Classification Loss for Learning Energy-based Generative Models", "abstract": "Score-based generative models have recently achieved remarkable success. While they are usually parameterized by the score, an alternative way is to use a series of time-dependent energy-based models (EBMs), where the score is obtained from the negative input-gradient of the energy. Crucially, EBMs can be leveraged not only for generation, but also for tasks such as compositional sampling or model recalibration via Monte Carlo methods. However, training EBMs remains challenging. Direct maximum likelihood is computationally prohibitive due to the need for nested sampling, while score matching, though efficient, suffers from mode blindness. To address these issues, we introduce the *Diffusive Classification* (DiffCLF) objective, a simple method that avoids blindness while remaining computationally efficient. DiffCLF reframes EBM learning as a supervised classification problem across noise levels, and can be seamlessly combined with standard score-based objectives. We validate the effectiveness of DiffCLF by comparing the estimated energies against ground truth in analytical Gaussian mixture cases, and by applying the trained models to tasks such as model composition and recalibration. Our results show that DiffCLF enables EBMs with higher fidelity and broader applicability than existing approaches.", "tldr": "We introduce a Diffusive Classification (DiffCLF) loss to learn energy-based generative models.", "keywords": ["Energy-based Models", "Diffusion Models", "Stochastic Interpolants"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b4f78f4b2fd10579dd5ad86f017baef4da57767.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Score matching is a common approach to train Energy-Based Models (EBMs), leveraging the fact that the intractable normalizing constant does not affect the score function. This property has made score matching a central tool in score-based generative modeling. However, while these methods effectively estimate the score, recovering the underlying energy function itself, along with the associated log-normalizing constant, remains challenging, which has limitations for various applications.\n\nIn this paper, the authors address this limitation by jointly modeling both the energy function and the log-normalizing constant, and by introducing a new training objective referred to as Diffusive Classification (DiffCLF). This proposed objective admits the ground-truth distribution as a minimizer, thereby providing a principled foundation to train energy-based models.\n\nEmpirical evaluations compare DiffCLF with Conditional Time Score Matching and a classical score-based training. Experiments on Gaussian mixture models show that the proposed approach achieves comparable performance according to standard evaluation metrics while yielding superior classification accuracy. Furthermore, DiffCLF outperforms baseline methods on two toy composition tasks and a recalibration task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method allows to simultaneously learn the energy function and the normalizing constant, which is known to be a challenging task.\n\n- The authors provide a rich literature overview and connect their works to many related works in generative modelling, in particular with score-based models.\n\n- The proposed approach provides consistenly better classification performance than the alternatives."}, "weaknesses": {"value": "- The proposed method provides an interesting framework to train simultaneously the energy function and the lognormalizing constant. However, the alternative considered in the paper, such as score-based training, have been widely studied these past few years. Wasserstein and KL upper bounds have been proposed in the strong log concave case and under weaker assuptions on the data distribution. \nAs the method highlights better performance in classification loss, it would be very interesting to obtain upper bounds for this classification loss, supporting the empirical results and the interest of the method with respect to score matching.\n\n- The experimental section focuses on mixture of Gaussian distributions. I believe this is an important setting where most computations and the ground truth are available explicitly, however, providing another most complex example would support the fact that the conclusions can be extended to more realistic settings."}, "questions": {"value": "- In simple settings (Gaussian, mixture of Gaussian distributions, strongly log concave distributions) have you investigated the theoretical properties of your estimator ?\n\n- Since the method is not provided with theoretical guarantees, the empirical results could be richer. Have you obtained results in other settings than Gaussian mixtures ?\n\n- Score-based models have also been extended to posterior sampling. Can this method be extended to posterior sampling ? Can a pretrained model with DiffCLF be used to solve inverse problems ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hYypWVdPOY", "forum": "kpwGlSY1He", "replyto": "kpwGlSY1He", "signatures": ["ICLR.cc/2026/Conference/Submission14499/Reviewer_RJKZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14499/Reviewer_RJKZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761349869494, "cdate": 1761349869494, "tmdate": 1762924895717, "mdate": 1762924895717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Diffusive Classification (DiffCLF), a method for training diffusion-based energy models that reformulates score learning as a classification-style objective across diffusion times. The approach aims to learn explicit time-dependent energies $U_\\theta^t$ rather than only scores $\\nabla_x \\log p_t(x)$, in order to address the mode-blindness problem—where score-matching diffusions can reproduce local shapes of the data distribution but fail to assign correct relative probabilities to different modes. The paper presents synthetic experiments on mixtures of Gaussians and stochastic interpolants suggesting that DiffCLF can better capture multi-modal structure than Diffusion Score Matching (DSM) or CtSM.\n\nThe paper presents an intriguing idea that may help alleviate mode blindness in diffusion models, but the exposition is confusing and the theoretical justification incomplete. Important definitions, model details, and experimental explanations are missing, and empirical validation remains limited to toy settings. I recommend rejection in its current form, though the core idea could merit reconsideration if rewritten clearly and supported by more rigorous analysis and experiments."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* Addresses a real limitation of current diffusion models (mode blindness) by attempting to model energies, not only scores.\n* The idea of connecting diffusion training with a classification/NCE-type loss is novel and potentially useful.\n* Theoretical motivation and experiments are at least qualitatively consistent with the intended effect."}, "weaknesses": {"value": "* The paper is very difficult to follow. Section 2 in particular is chaotic: notation such as $X_t$, $Y_t$, $p_t$, $q_t$, $S(t)$, $\\sigma(t)$ is introduced with little intuition or connection, and the relationship between the data distribution and the time-evolving process is unclear.\n* The stated objective “to estimate the densities $(p_t)_t$” is conceptually confusing—the goal should be to model the data distribution $p_0$, not all intermediate marginals.\n* Equations (6)–(7) appear without sufficient context; the link between them and the proposed loss is missing.\n* Example 1 (“Diffusion Models’’) introduces $S(t)$, $\\sigma(t)$, and $\\gamma(t)=S(t)\\sigma(t)$ without citing their origin or explaining how this parametrization relates to the standard variance-preserving diffusion SDE $(dX_t=-\\tfrac12\\beta(t)X_t,dt+\\sqrt{\\beta(t)},dW_t)$.\n* Proposition 1 (Appendix D.1) is not valid as stated. The normalization constraint used in the proof makes sense for discrete class probabilities but not for probability densities. Different EBMs could yield identical normalized ratios $p_\\theta^{t_i}(y)/\\sum_j p_\\theta^{t_j}(y)$ and thus the same $L_{\\text{clf}}$ loss. The proposition therefore only guarantees matching of relative posteriors across times, not recovery of the true marginals  $p_t$.\n* It is unclear how the network architecture jointly estimates score and energy for each $t$, or how $F_\\theta^t$ (“a learnable parameter’’) is parameterized and optimized.\n* The “recalibration’’ section is not well introduced: the paper mixes discussion of Monte-Carlo expectations with experiments that actually measure distributional distances ($W_2$, KS). The reader is left unsure what is being estimated or why these metrics demonstrate recalibration ability.\n* In general, the text contains unnecessary and poorly motivated detail (e.g., extended references to SMC/AIS) while omitting key implementation and experimental information.\n* Evaluation is limited to toy examples. For the Diffusion Model experiment, the method mainly improves the $L_{\\text{clf}}$ metric—which is unsurprising since that is the optimized loss—while DSM performs better on MMD. No log-likelihood or quantitative likelihood comparison is reported, even though ground-truth densities are available. Scalability to realistic domains is untested."}, "questions": {"value": "1. How exactly is the model parameterized to output both energy and score for each $t$?\n2. How is $F_\\theta^t$ modeled and optimized in practice?\n3. What assumption ensures identifiability of $p_\\theta^{t_i}=p_{t_i}$ in Proposition 1?\n4. Can the authors clarify what quantity of interest is estimated in the “recalibration’’ experiment—expectations or only sample distributions?\n5. Why not report log-likelihoods, given that ground-truth densities are available?\n6. Are there any results beyond toy data that would show the method’s behavior at scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7qex3bTsVO", "forum": "kpwGlSY1He", "replyto": "kpwGlSY1He", "signatures": ["ICLR.cc/2026/Conference/Submission14499/Reviewer_pymV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14499/Reviewer_pymV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762220174257, "cdate": 1762220174257, "tmdate": 1762924895229, "mdate": 1762924895229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Diffusive Classification (DiffCLF), a training objective for energy-based generative models (EBMs) that reformulate energy estimation as a multi-class classification problem across noise levels. Instead of learning score functions, the proposed approach is based on learning a time dependent family of energy functions together with the log-normalizing constants, using the DiffCLF objective."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed loss function for training time-dependent energy functions seem to be novel and interesting."}, "weaknesses": {"value": "Overall the paper is rather poorly written. While the manuscript spends several pages introducing the background, the key part of the proposed approach (section 3) is rather brief and needs further elaboration.  In its current form, the objective function is not clearly explained, in particular why such objective is used. \n\nMoreover, the statement of the theoretical result is not precise and seems to miss assumptions. For example, the authors claimed that the score-matching methods suffer from the mode blindness issue when the distributions have disjoint supports, however in the proof of Proposition 1, it is assumed that the supports are the same, and such assumption is not stated in the Proposition. The theoretical result also tells little information about what happens when the minimum of the objective function is not exactly achieved. Does one have error bounds from the optimization gap? \n\nThe numerical validation of the proposed method is also rather weak, as it is only tested on synthetic Gaussian mixtures; which is far from practical setting of generative models. \n\nOverall, the reviewer finds that while the paper contains some interesting ideas, it lacks in presentation, theoretical results and empirical validation."}, "questions": {"value": "See \"weakness\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dz4omGpMz3", "forum": "kpwGlSY1He", "replyto": "kpwGlSY1He", "signatures": ["ICLR.cc/2026/Conference/Submission14499/Reviewer_8TVA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14499/Reviewer_8TVA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14499/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762913770906, "cdate": 1762913770906, "tmdate": 1762924894645, "mdate": 1762924894645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}