{"id": "iGDv1e4rFY", "number": 14471, "cdate": 1758236629233, "mdate": 1759897368150, "content": {"title": "UniDoc-Bench: A Unified Benchmark for Document-Centric Multimodal RAG", "abstract": "Multimodal retrieval-augmented Generation (MM-RAG) is a key approach for applying large language models (LLMs) and agents to real-world knowledge bases, yet current evaluations are fragmented—focusing on either text or images in isolation, or simplified multimodal setup, failing to capture document-centric multimodal use cases.\nIn this paper, we introduce UNIDOC-BENCH, the first large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across 8 domains.\nOur pipeline extracts and links evidence from text, tables, and figures, then generates 1,600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries. \nTo ensure reliability, 20% of QA pairs are validated by multiple annotators and expert adjudication.\nUNIDOC-BENCH supports apples-to-apples comparison across four paradigms --- 1) text-only, 2) image-only, 3) multimodal text–image fusion and 4) multimodal joint retrieval --- under a unified protocol with standardized candidate pools, prompts, and evaluation metrics. \nOur experiments show that multimodal text–image fusion RAG systems consistently outperform both unimodal and jointly multimodal embedding–based retrieval, indicating that neither text nor images alone are sufficient and that current multimodal embeddings remain inadequate. \nBeyond benchmarking, our analysis reveals when and how visual context complements textual evidence, uncovers systematic failure modes, and offers actionable guidance for developing more robust MM-RAG pipelines.", "tldr": "UNIDOC-BENCH is a large-scale MM-RAG benchmark with 70k real-world PDF pages across eight domains, generating 1,600 multimodal QA pairs from text, tables, and figures.", "keywords": ["RAG", "evaluation", "multimodal", "benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1ec5c2b61d0a8c31524d945fbd3c68835b2d07c4.pdf", "supplementary_material": "/attachment/1081f031e9288fd93715b4950c0af214ce7467a9.zip"}, "replies": [{"content": {"summary": {"value": "This introduce UniDoc-Bench, a large-scale, realistic benchmark for MM-RAG built from 70k real-world PDF pages across 8 domains. It supports 4 different types of retrieval paradigms: text-only, image-only,  multimodal text–image fusion and multimodal joint retrieval. The paper further offers analysis comparing visual and textual context usage in sota multimodal LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduce a novel benchmark UniDoc-Bench which contains 1, 600 multimodal QA pairs spanning factual retrieval, comparison, summarization, and logical reasoning queries.\n2. The paper introduce a fair and reproducible evaluation framework by fixing candidate pools across modalities , and measuring retrieval effectiveness, answer faithfulness, and completeness end-to-end across different RAG systems.\n3. This paper conducts a systematic comparison of text-retrieval, image-retrieval, text–image fusion, multimodal joint retrieval pipelines, analyzing which retrieval strategy performs best under different question types, evidence modalities."}, "weaknesses": {"value": "1. The significant contribution that I take away from this paper vs the previous work is unified evaluation and multiple reference. Therefore, \nWhat are the benefits of unified evaluation?\n2. How is multiple reference used  during evaluation to enhance the groundness?"}, "questions": {"value": "1. See the in weakness.\n2. Will this paper opensource their evaluation framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UrTgaXukuA", "forum": "iGDv1e4rFY", "replyto": "iGDv1e4rFY", "signatures": ["ICLR.cc/2026/Conference/Submission14471/Reviewer_7MPR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14471/Reviewer_7MPR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463060326, "cdate": 1761463060326, "tmdate": 1762924871694, "mdate": 1762924871694, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniDoc-Bench, a benchmark for document-centric multimodal retrieval-augmented generation (RAG). The dataset includes 1,600 synthetically generated multimodal QA pairs, of which 20% are validated by multiple annotators and expert adjudication to ensure quality. The benchmark standardizes evaluation across four paradigms: (1) text-only, (2) image-only, (3) multimodal text–image fusion, and (4) multimodal joint retrieval, enabling fair, apples-to-apples comparisons. Experimental results show that multimodal text–image fusion RAG systems perform best."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Provides a systematic and unified comparison of text retrieval, image retrieval, text–image fusion, and multimodal joint retrieval pipelines under consistent settings.\n2. Ensures data reliability by validating 20% of the QA pairs through multiple annotators and expert adjudication."}, "weaknesses": {"value": "The QA data are synthetically generated (GPT-4.1 and Gemini-Pro-2.5), which may introduce bias toward the LLMs used. This limits the benchmark’s ability to fully assess model generalization beyond this generation and templates distribution."}, "questions": {"value": "Have you compared LLM-generated QA performance against fully human-written QA pairs to confirm generalizability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sguInAUuAU", "forum": "iGDv1e4rFY", "replyto": "iGDv1e4rFY", "signatures": ["ICLR.cc/2026/Conference/Submission14471/Reviewer_7bma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14471/Reviewer_7bma"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865288437, "cdate": 1761865288437, "tmdate": 1762924871317, "mdate": 1762924871317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper UniDoc-Bench introduces a large-scale benchmark for multimodal retrieval-augmented generation (RAG) across text, image, and table modalities, providing standardized evaluation for both open-source and commercial models.\nIt demonstrates that multimodal RAG improves performance and cost efficiency compared to text-only RAG, particularly in domains rich in visual content such as finance and construction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Covers diverse modalities (text, image, multimodal, table-required) and question types, providing a unified evaluation framework.\n2. Offers granular insights into the relative strengths of text and image retrieval and the role of domain-specific content richness.\n3. Highlights cost and latency trade-offs in multimodal RAG, providing actionable insights for system optimization."}, "weaknesses": {"value": "1. It would be beneficial to add more SoTA models' results such as Gemini and Claude with and without retrieved instances. Also, it'd be great to perform a more detailed comparison between different SoTA models.\n2. Overrepresentation of finance and construction may bias generalization to less visually complex domains."}, "questions": {"value": "How would different SoTA models (e.g., GPT, Gemini, Claude) perform on this benchmark and what types of errors do these models make?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iEerLPIyRA", "forum": "iGDv1e4rFY", "replyto": "iGDv1e4rFY", "signatures": ["ICLR.cc/2026/Conference/Submission14471/Reviewer_wtt8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14471/Reviewer_wtt8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953001304, "cdate": 1761953001304, "tmdate": 1762924870942, "mdate": 1762924870942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UniDoc, a unified benchmark designed to evaluate multimodal document understanding across diverse input forms, e.g., scanned pages, forms, and charts. UniDoc aggregates and harmonizes multiple existing datasets into a standardized schema, supporting different types of tasks. The benchmark construction is illustrated, and relevant experiments are executed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark is inclusive. As shown in Table 1, UniDoc contains the tasks from multiple domains, and it has the largest number of queries and pages of documents.\n- The dataset curation section is clear to some extent, and the further release and deployment seem promising."}, "weaknesses": {"value": "- The paper mostly reuses and reprocesses existing datasets, offering engineering unification rather than new data collection or annotation.\n- 20% generated QA pairs are validated by annotators or experts. More can increase the credibility of the benchmark.\n- A clear task definition should be formally expressed along with the proposed benchmark.\n- The tested baseline model scope should be enlarged to increase the credibility of the proposed benchmark. So far, only 4 main models are included in the paper. Although tables 4 and 5 involve 6 models, this is still not adequate."}, "questions": {"value": "In addition to the weakness, other questions are listed\n- Is it necessary to measure the difficulty of tasks when unifying different datasets?\n- Is there any plan for including reasoning chain annotations to assess step-by-step interpretability?"}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Different source datasets may have distinct licensing terms. Clarity is needed for redistribution under a unified release."}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "51bdYBJb2y", "forum": "iGDv1e4rFY", "replyto": "iGDv1e4rFY", "signatures": ["ICLR.cc/2026/Conference/Submission14471/Reviewer_jCxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14471/Reviewer_jCxW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14471/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991843406, "cdate": 1761991843406, "tmdate": 1762924870522, "mdate": 1762924870522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}