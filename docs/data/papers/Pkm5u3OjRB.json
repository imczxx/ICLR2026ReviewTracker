{"id": "Pkm5u3OjRB", "number": 16859, "cdate": 1758269557955, "mdate": 1759897215205, "content": {"title": "Training-Free Generative Modeling by Kernelizing Pretrained Diffusion Models", "abstract": "Generative diffusion models, including stochastic interpolants and score-based approaches, require learning time-dependent drift or score functions through expensive neural network training. Here we avoid these computations by representing the drift in a reproducing kernel Hilbert space, reducing the learning problem to solving linear systems. The key challenge becomes selecting kernels with sufficient expressiveness for the drift learning task. We address this by constructing kernels from pretrained drift or score functions, leveraging the fact that our linear systems depend only on gradients of kernel features---not the features themselves. Since pretrained drifts provide these gradients directly, we can build expressive kernels without access to the underlying feature representations. This enables seamless combination of multiple pretrained models at inference time and cross-domain enhancement through the same framework. Experiments demonstrate competitive sample quality with significantly reduced computation, consistent ensemble improvements, and successful cross-domain enhancement---even cheap, low-quality models can match expensive high-quality models when combined through our framework.", "tldr": "Training-free generative modeling by building kernels from pretrained diffusion models, enabling model combination and domain adaptation through linear solves instead of neural network training.", "keywords": ["generative models", "diffusion models", "stochastic interpolants", "kernel methods", "RKHS", "training-free", "model ensembling", "domain adaptation", "pretrained models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0e8953ca8642148a51d5cc71d0c349647b1ca2d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for ensembling pretrained diffusion models that is inspired by kernel learning. From a kernel learning perspective, the key insight is to solve the score-matching problem within the class of functions that are representable in the form $\\hat{b}_t(x) = \\langle \\nabla \\Phi(x), \\eta_t \\rangle$, where $\\Phi$ is a feature map and the inner product is in the induced RKHS. The authors show that the ground truth score is representable in such a form if the RKHS has a characteristic kernel and that the weights $\\eta_t$ can be learned by solving a system of linear equations. In practice, the authors replace the gradients $\\nabla \\Phi(x)$ with the velocity fields or score functions of a set of pretrained diffusion models, which reduces the learning problem to learning a set of global weights for each model and then sampling using linear combinations of the pretrained velocities/scores. The authors show that their method can be used to ensemble collections of weak diffusion models and briefly experiment with cross-domain enhancement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written, and I was able to follow the key ideas without much trouble. I appreciate the use of colored boxes to signpost important results.\n- Both model ensembling and kernel learning for diffusion models are interesting problems, and the authors propose a sensible algorithm for learning a diffusion model's score or velocity in an RKHS\n- Using this method to ensemble weak diffusion models improves on the sample quality that one would obtain from any of the individual models."}, "weaknesses": {"value": "My primary critique of this paper is that while the theory is interesting, in practice, the method seems to boil down to learning a set of global weights (i.e. independent of $x$) for each pretrained model and sampling using a linear combination of the pretrained models' velocity fields. As the authors note, there is no guarantee that these velocity fields are in fact gradients in practice and that the resulting kernel is characteristic. This makes the theory seem somewhat superfluous: Can one use this theory to make meaningful predictions about the behavior of the method's practical implementation? Does it add much value relative to beginning with the (admittedly heuristic) ansatz that one should ensemble a set of diffusion models by taking a linear combination of their velocity fields and simply deriving the solution to the learning problem under that ansatz (i.e. lines 218-236)?\n\nIt's also unclear to me whether this in fact is a sensible strategy. For instance, wouldn't it make more sense to learn space- and time-dependent weights $\\eta^i_t(x)$? It seems to me that at sampling time, one should weight each pretrained model according to some measure of their \"confidence\" in their prediction at $x$. For instance, if each model has learned a unimodal distribution and one wishes to sample from a multimodal distribution, then $\\eta^i_t(x)$ should be large for models whose mode is close to x.\n\nBuilding on the previous point, the authors might consider adding a simple baseline for their method: Simply fix uniform weights $\\eta^i_t = \\frac 1 P$ and otherwise sample as in Algorithm 1. I wonder if the weak models in Section 3.1 can be interpreted as noisy estimates of the score/velocity field near convergence, which can be \"denoised\" by simply averaging the estimates."}, "questions": {"value": "- I would appreciate if the authors would comment on the novelty of their theoretical results for learning a diffusion model's score or velocity field in an RKHS -- particularly Theorem 2.5. I am not intimately familiar with the intersection of kernel learning and diffusion models, but the authors state in the related work that \"RKHS reformulations of diffusion models have been considered e.g. in Maurais & Marzouk (2024); Yi et al. (2024): in contrast with these approaches that use standard kernels (Gaussian/RBF, Laplacian, etc.) we build the kernel using pre-trained score or drifts...\" Do results similar to Theorem 2.5 appear in any of these prior works? If so, does this work's novel contribution boil down to using pretrained diffusion models as the source of the $\\nabla \\Phi$?\n\n- Are there any notable theoretical obstacles to learning space- and time-dependent weights $\\eta^i_t(x)$ in the proposed kernel learning framework?\n\n- In Section 3.1 and Figure 3, the weak models are trained on Fashion-MNIST, EMNIST, MNIST, and Kuzushiji-MNIST, but the ensembled model's samples appear to be drawn exclusively from MNIST. Why is that? Is there any theoretical reason why we wouldn't expect the ensembled model to sample from any of the other data distributions? Is there any way to control which of the data distributions the ensembled model samples from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zt4wpzbuEt", "forum": "Pkm5u3OjRB", "replyto": "Pkm5u3OjRB", "signatures": ["ICLR.cc/2026/Conference/Submission16859/Reviewer_Z1LV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16859/Reviewer_Z1LV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761081929991, "cdate": 1761081929991, "tmdate": 1762926877455, "mdate": 1762926877455, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes kernelization theories of diffusion models for training-free generative modeling. By leveraging Maximum Mean Discrepancy (MMD) and stochastic interpolants, the authors develop a kernelization framework that enables efficient sampling from pretrained diffusion models without additional training. The paper discusses two application scenarios: mixture of (weak) experts and cross-domain enhancement. Experimental results on standard datasets demonstrate the effectiveness of the proposed kernelization approach in generating high-quality samples."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The background of kernelization and MMD is well explained, making the paper accessible.\n- The kernelization framework is theoretically well-founded, providing novel perspectives on diffusion models.\n- The discussed application scenarios of mixture of (weak) experts and cross-domain enhancement are sound and promising."}, "weaknesses": {"value": "- The theoretical contributions, while solid, may lack depth in terms of novelty.\n- The application scope of the kernelization framework is limited.\n- The experimental validation is somewhat preliminary, requiring more extensive empirical evidence."}, "questions": {"value": "1. **Theoretical novelty.** While the kernelization framework is well-founded, the kernalization theories, MMD definitions and stochastic interpolants have been extensively studied in prior works. The contribution of this paper seems to be an application or extension of these existing theories to diffusion models. Could the authors clarify the novel theoretical contributions of this work compared to prior literature?\n\n2. **Application scope.** The proposed kernelization framework is applied to mixture of (weak) experts and cross-domain enhancement. However, these applications seem somewhat limited in scope. Could the authors discuss potential extensions or broader applications of the kernelization framework in diffusion models?\n\n3. **Insufficient experimental validation.** While the authors discuss two potential applications, the experiments are performed on relatively simple datasets (e.g., MNIST). Could the authors provide more extensive experimental validation on larger and more complex datasets to better demonstrate the effectiveness of the proposed framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R83zMeEg33", "forum": "Pkm5u3OjRB", "replyto": "Pkm5u3OjRB", "signatures": ["ICLR.cc/2026/Conference/Submission16859/Reviewer_WxFt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16859/Reviewer_WxFt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921305578, "cdate": 1761921305578, "tmdate": 1762926876855, "mdate": 1762926876855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a novel and interesting training-free generative modeling approach by kernelizing pre-trained diffusion models. The authors construct a kernel directly on the gradients of kernel features, leveraging pre-trained drift or score functions. The experiments demonstrate that the proposed approach can produce better generations by ensembling weak models. The method also shows successful cross-domain enhancement"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper proposes an interesting training-free generative modeling method by kernelizing pre-trained drift or score functions, which is novel. The derivations are overall correct.\n* The paper provides empirical evidence showing that the proposed method improves generation quality on CelebA and MNIST, and it displays interesting cross-domain enhancement within MNIST-related settings."}, "weaknesses": {"value": "* The main weakness lies in the simplicity and roughness of the evaluation. The experiments are conducted only on toy scenarios, which weakens confidence in extending the method to larger or more practical settings.\n* Even for MNIST and CelebA, the paper provides insufficient information and analysis of the method.\n\nMinor issue that does not directly affect my rating:\n\n* The right-hand side of Equation 6 (representing velocity by score) is incorrect."}, "questions": {"value": "* The expressiveness of the kernel is crucial for practical performance. Even on MNIST, the authors should design experiments to show how the kernel choice or the chosen weak models influence the final results (currently, only Figure 1 Right shows how the number of weak models influences performance).\n* How do domain gaps influence cross-domain enhancement? How can pre-trained models on MNIST, SVHN, CIFAR, or ImageNet cooperate to produce better results on other domains such as CelebA?\n* As a training-free approach that claims to save computational budgets, can the authors extend experiments to ImageNet with pre-trained models?\n* In lines 254â€“255, we need to compute $b_t(X_t)$ with all $\\sum^P_{i=1}b^i_t(X_t)\\eta_t^i$. Does this mean we still need to forward all weak components $b_t^i$?\n* We need a batch size of $N$ data points in the target domain; how does $N$ influence the results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DvYFcw1fjb", "forum": "Pkm5u3OjRB", "replyto": "Pkm5u3OjRB", "signatures": ["ICLR.cc/2026/Conference/Submission16859/Reviewer_gAia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16859/Reviewer_gAia"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923379482, "cdate": 1761923379482, "tmdate": 1762926876414, "mdate": 1762926876414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a method to construct a generative diffusion model in the (allegedly) *training-free* way, building upon existing pretrained, possibly weak, diffusion models. Theoretically, based on the stochastic interpolant model, the authors reduce the generative modeling task into finding an appropriate drift $b_t$ to be used in a certain SDE, and then suggest building this $b_t$ from predefined feature maps (or more precisely, their derivatives) using a kernel regression type method. Relying on the freedom to select which features are to be used, the paper chooses them to be the log-density functions (whose derivatives are the score functions) learned by pretrained diffusion models. Experiments are demonstrated, as evidence to support the validity of the claim."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The idea of leveraging Hilbert space kernel regression into finding the score function is interesting and noteworthy. The theoretical developments are sound, giving us concrete training recipes. The experiments, while preliminary, offer encouraging visual evidence supporting the approach."}, "weaknesses": {"value": "Above all, the paper would benefit from articulating a well-defined objective, ideally with a clear mathematical formulation, in the introduction and maintaining a consistent focus on it throughout. For me, it was not entirely clear whether the ultimate aim of this paper is to propose a new fine-tuning strategy, a novel method for obtaining score functions, an approach to distributed training, or some combination of these.\n\nAfter reading the paper in full, my understanding is that the main goal of this paper aligns with what I described in the **Summary** section. It is a promising direction, and the core idea is a very interesting one. However, the theoretical contributions are incremental, being no more than a special case of well-known facts in kernel regression theory. From the practitioner's point of view, it seems several challenges still need to be addressed. Please see the **Questions** section below. I would be more than happy to have a constructive discussion with the authors, and I am willing to increase my score accordingly if my concerns are adequately addressed."}, "questions": {"value": "1.  Theorem 2.5, which is central to this work, relies on a relatively strong assumption that the kernel is *characteristic*. This is not that of a big problem when we have full freedom to choose which kernel, or equivalently the features, to use. However, once we get into the \"weak-to-strong\" tasks (building the drift function $b_t$ using weak pretrained models), we no longer can guarantee that the kernel is characteristic, and being able to recover $b_t$ only makes sense if $b_t$ is in the span, or at least very close to the span, of $\\nabla \\Phi_i$s. The only scenarios I can think of this happening is either (1) we use a massive amount of pretrained models so that the span gets sufficiently large, (2) when the \"weak\" models are actually already expressive enough (although this seems somewhat contrary to the set-up under consideration), or (3) the dataset lies in a very low intrinsic dimension, enough to guarantee that the true drift varies primarily in a low-dimensional subspace, so that a small ensemble of weak model suffices. Otherwise, the learned drift will be the projection of $b_t$ onto the span of $\\nabla \\Phi_i$, which can be very different from what we want to learn. This seems to be a critical gap between theoretical explanation and the actual proposed method, and I would like to hear how authors think about this. \n\n2. I can see where the naming *\"training-free\"* comes from; the \"training\" part, in practice, reduces to solving a system of linear equations. However, it seems like the training cost is just transferred to the inference part; now during inference, we have to get inference results from all of the weak models, in order to take the linear combination of them. Can we say for sure that this approach has a clear advantage over paying the cost once in the training phase? \n\n3. I am quite skeptical about the validity of the experimental results for a few reasons: (1) there are no quantitative results regarding the sample quality and the report only relies on the visual aspects, (2) only a handful of samples are shown for each experiments, not fully convincing that the model is performing well, and (3) the experiments are only on \"relatively easy\" datasets such as MNIST and Celeb-A. Testing on easier datasets is not that of a big problem, as they can serve as a proof-of-concept for a theoretical result, but the first two points are much more critical and should be discussed carefully when doing so, which the paper currently lacks. Could the authors explain why these decisions were made when presenting the experimental results, and are there any plans to provide further elaboration or quantitative evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wW1TTgDMS8", "forum": "Pkm5u3OjRB", "replyto": "Pkm5u3OjRB", "signatures": ["ICLR.cc/2026/Conference/Submission16859/Reviewer_33k8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16859/Reviewer_33k8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16859/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983221107, "cdate": 1761983221107, "tmdate": 1762926876041, "mdate": 1762926876041, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}