{"id": "3jDSqfTSrn", "number": 14871, "cdate": 1758244915498, "mdate": 1759897344282, "content": {"title": "Is Finer Better? The Limits of Microscaling Formats in Large Language Models", "abstract": "Microscaling data formats leverage per-block tensor quantization to enable aggressive model compression with limited loss in accuracy. Unlocking their potential for efficient training and inference necessitates hardware-friendly implementations that handle matrix multiplications in a native format and adopt efficient error-mitigation strategies. Herein, we reported the emergence of a surprising behavior associated with microscaling quantization, whereas the output of a quantized model degrades as block size is decreased below a given threshold. This behavior clashes with the expectation that a smaller block size should allow for a better representation of the tensor elements. We investigate this phenomenon both experimentally and theoretically, decoupling the sources of quantization error behind it. Experimentally, we analyze the distributions of several Large Language Models and identify the conditions driving the anomalous behavior. Theoretically, we lay down a framework showing remarkable agreement with experimental data from pretrained model distributions and ideal ones. Overall, we show that the anomaly is driven by the interplay between narrow tensor distributions and the limited dynamic range of the quantized scales. Based on these insights, we propose the use of FP8 unsigned E5M3 as a novel hardware-friendly format for the scales in FP4 microscaling data types. We demonstrate that UE5M3 achieves comparable performance to the conventional FP8 unsigned E4M3 scales while obviating the need of global scaling operations on weights and activations.", "tldr": "Naive microscaling formats hit their limits when block size is too small", "keywords": ["microscaling", "fine-grained", "FP4", "quantization", "low-precision", "llm"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6628051736a5a78f2b9ddd54424863600ea9ded4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the counterintuitive behavior in microscaling quantization where smaller block sizes can lead to higher quantization error. The authors provide both experimental evidence across several LLMs and a theoretical framework explaining the phenomenon, attributing it to scale quantization effects. They further propose a hardware-friendly fix using FP8-UE5M3 scale representation, demonstrating improved performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The discovery of the “perplexity inversion” phenomenon is novel and well-motivated.\n\nThe theoretical modeling is rigorous and matches experimental data convincingly.\n\nThe proposed UE5M3 solution is simple, practical, and hardware-friendly.\n\nWriting and figures are clear; experiments cover multiple models and tasks."}, "weaknesses": {"value": "The experiments mainly focus on inference; it would strengthen the paper to evaluate whether the same anomaly occurs during training.\n\nWhile hardware feasibility is discussed qualitatively, more quantitative data (e.g., area, latency, or energy cost of adding one exponent bit) would clarify the trade-offs.\n\nThe proposed format is only tested on sub-10B models. Given the claim of generality, evaluating larger-scale LLMs (e.g., 30B–70B) would enhance credibility.\n\nSome connections to existing FP8 and mixed-precision deployment standards (e.g., NVIDIA MXFP4, OCP spec) could be more explicitly compared."}, "questions": {"value": "How does the anomaly behave with integer quantization (INT4) in practical LLM inference, not only synthetic distributions?\n\nWould dynamic or learned scale clipping alleviate the same issue without new hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LQYQy8ldp7", "forum": "3jDSqfTSrn", "replyto": "3jDSqfTSrn", "signatures": ["ICLR.cc/2026/Conference/Submission14871/Reviewer_UtsX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14871/Reviewer_UtsX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547796880, "cdate": 1761547796880, "tmdate": 1762925222947, "mdate": 1762925222947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uncovers a surprise in LLM quantization: making quantization blocks too small (\"finer\") can paradoxically hurt performance. The authors trace this \"perplexity inversion\" anomaly to the standard FP8 UE4M3 format used for the per-block scales, which fails to accurately represent tensors with very small values (narrow distributions). They provide a rigorous theoretical model to prove this and propose a simple, hardware-friendly fix: FP8 UE5M3, a new scale format that uses a spare bit to add a 5th exponent bit. This new format solves the anomaly and achieves high accuracy without requiring expensive per-tensor scaling operations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Identifies the counter-intuitive \"finer is worse\" quantization anomaly.\n\nDevelops a mathematical framework that perfectly explains the why behind the anomaly, which is a significant step beyond just observing it."}, "weaknesses": {"value": "The theory is heavily based on weight distributions (modeled as Normal), with less focus on how the anomaly impacts different and often asymmetric activation distributions.\n\nThe claim of \"minimal\" hardware cost for UE5M3 is asserted but not analyzed in-depth (e.g., no area or latency estimates)."}, "questions": {"value": "How does this anomaly, and the UE5M3 fix, perform with the different, often-asymmetric distributions of activations?\n\nDid you investigate adding a mantissa bit (i.e., UE4M4) for precision instead of an exponent bit (UE5M3) for range?\n\nTable 1 shows that for several models (e.g., llama-3.1-8b, bamba-9b-v2), your UE5M3 proposal achieves nearly identical performance to the UE4M3-S (with scaling), not a clear gain. Given this, what is the primary motivation for a hardware change when a software mitigation performs comparably?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Xtenp8KkNV", "forum": "3jDSqfTSrn", "replyto": "3jDSqfTSrn", "signatures": ["ICLR.cc/2026/Conference/Submission14871/Reviewer_bRVj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14871/Reviewer_bRVj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905952703, "cdate": 1761905952703, "tmdate": 1762925222459, "mdate": 1762925222459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the unexpected behavior of microscaling FP4 quantization when FP8 scale quantization is used. The authors find that reducing block size does not always reduce error and in fact can worsen accuracy for narrow weight distributions. They develop a theoretical framework that decouples sources of quantization error and show excellent agreement between theory and empirical results across multiple models. Finally, they propose an FP8 UE5M3 scale format that mitigates the anomaly without requiring additional hardware cost, and demonstrate improved model accuracy compared to UE4M3 or per tensor scaling."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "I appreciate the solid empirical observation and thorough investigation of a subtle but important anomaly in microscaling quantization. \n\nThe paper formulates a clear theoretical framework that generalizes the understanding of error behavior and matches experiments well. \n\nThe analysis in figures such as Fig 2b and Fig 3c is especially compelling as it isolates the dependence on distribution width and scale quantization. \n\nThe proposed UE5M3 solution is simple, hardware friendly, and demonstrates practical effectiveness."}, "weaknesses": {"value": "The anomaly is a surprising phenomenon for readers and it may help to offer a concise intuitive explanation earlier in the introduction, rather than waiting until later sections, so that readers understand the high level mechanism before diving into the detailed framework. For example, a short statement that quantization of scales interacts with narrow distributions and reduces effective representable range could improve clarity.\n\nIt would also be valuable to expand the discussion to other scale precisions. The paper focuses on FP8 scales versus FP16 and the new UE5M3 format. Discussion on whether the same anomaly is expected for future lower precision formats such as FP4 scales or mixed mantissa exponent configurations would help generalize the insight."}, "questions": {"value": "Why does FP16 scaling not suffer from this anomaly if the cause is related to the deviation of the maximum weight in a block and scale resolution?\n\nCould you briefly comment on expected behavior for future lower precision scale formats such as FP4 or hybrid exponent mantissa configurations. For example, if microscaling continues to push toward fewer bits for scale, should we expect similar inversion behaviors and would your theoretical framework still apply."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gLFfF1YBuI", "forum": "3jDSqfTSrn", "replyto": "3jDSqfTSrn", "signatures": ["ICLR.cc/2026/Conference/Submission14871/Reviewer_fiMv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14871/Reviewer_fiMv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762063345528, "cdate": 1762063345528, "tmdate": 1762925221986, "mdate": 1762925221986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates an unexpected limitation of microscaling quantization—a fine-grained, per-block quantization method increasingly used for efficient training and inference of large language models (LLMs). While smaller block sizes are generally assumed to improve quantization accuracy, the authors discover a quantization anomaly whereby further reducing block size below a threshold increases model perplexity, a phenomenon they term perplexity inversion. They diagnose this anomaly through extensive experimentation across various LLMs and develop a robust theoretical framework that decomposes the Mean Squared Error (MSE) into three distinct contributions, revealing that the quantization of scaling factors and the effect of zero-rounding are the primary drivers of the inversion, especially for narrow tensor distributions. To address this, the paper proposes FP8 unsigned E5M3 scales, demonstrating that this hardware-friendly solution effectively mitigates perplexity inversion by offering an increased dynamic range without the need for global scaling operations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper makes a highly original and significant contribution by identifying \"perplexity inversion,\" a counter-intuitive phenomenon where smaller block sizes unexpectedly increase quantization error in microscaling for LLMs. This challenges common assumptions and reveals a critical pitfall for future low-bit quantization efforts.\n2. The FP8 UE5M3 solution stands out as a key strength due to its practical and well-reasoned approach to mitigating the identified perplexity inversion. By repurposing an unused bit to extend the exponent range, UE5M3 significantly increases dynamic range, enabling better representation of small-magnitude elements crucial for narrow tensor distributions. This design is not only hardware-friendly, requiring minimal modifications to existing infrastructure, but also achieves comparable or superior performance to more complex per-tensor scaling methods, effectively simplifying the quantization pipeline while preserving or enhancing model accuracy. Its foundation in the paper's theoretical insights ensures it is a targeted and robust solution to the core problem."}, "weaknesses": {"value": "1. The paper effectively shows perplexity inversion with FP4 elements and FP8 UE4M3 scales. Do the authors observe similar inversion with other low-bit formats (e.g., INT4, INT8, other FP formats) and quantized scales? Clarifying if this mechanism is universally applicable or specific to the studied configuration would define the discovery's scope.\n2. The paper emphasizes the hardware-friendly nature of UE5M3, particularly for inference. However, the practical implications of integrating UE5M3 during the training phase, especially for quantization-aware training (QAT), are not fully elaborated. The discussion mainly focuses on FP8 UE4M3 for existing hardware. Clarifying how the extended exponent range of UE5M3 impacts gradient calculations, potential numerical stability issues during training, or if it primarily targets post-training quantization (PTQ) scenarios would be beneficial.\n3. While the proposed FP8-UE5M3 format effectively extends the dynamic range of fixed scales, the paper lacks a comparison with adaptive scaling methods such as VS-Quant (Per-vector Scaled Quantization for Accurate Low-Precision Neural Network Inference) and GWQ (Gradient-Aware Weight Quantization for Large Language Models). These approaches also refine scale granularity through per-vector or group-wise learnable scaling. Without an empirical or qualitative comparison, it remains unclear whether UE5M3 offers distinct advantages over these adaptive strategies or if their benefits overlap."}, "questions": {"value": "1. The paper effectively demonstrates perplexity inversion using FP4 elements and FP8 UE4M3 scales. It would be valuable to understand if this phenomenon is specific to this combination or broadly applicable. Have the authors observed similar inversion when using other common low-bit quantization formats (e.g., INT4, INT8, or different FP variants) with their corresponding scales also quantized? Clarifying whether the identified mechanism (interplay of narrow distributions and limited scale dynamic range) is a universal challenge or uniquely pronounced with the studied configuration would better define the scope and novelty of this important discovery.\n2. The benefits of UE5M3 are predominantly highlighted for inference. However, its role in the training phase, particularly within a Quantization-Aware Training (QAT) framework, needs further clarification. Could the authors elaborate on how the extended exponent range of UE5M3 impacts gradient calculations, numerical stability, or convergence during QAT compared to UE4M3? Understanding whether UE5M3 is primarily geared towards Post-Training Quantization (PTQ) or if it seamlessly integrates with QAT, potentially requiring specific modifications or offering distinct advantages, would provide a more complete view of its practical utility.\n3. While the UE5M3 format improves scale representation through a fixed hardware-defined design, it would be valuable to compare it with adaptive scaling methods such as VS-Quant (Dai et al., 2021) and GWQ (Yang et al., 2024), which optimize scales per vector or group. These methods share the goal of mitigating quantization errors via finer-grained scaling. An empirical or qualitative comparison discussing complexity, overhead, and accuracy trade-offs would help clarify UE5M3’s unique advantages and positioning within the current quantization landscape."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YSG1UTHWdP", "forum": "3jDSqfTSrn", "replyto": "3jDSqfTSrn", "signatures": ["ICLR.cc/2026/Conference/Submission14871/Reviewer_qJkV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14871/Reviewer_qJkV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14871/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762086374887, "cdate": 1762086374887, "tmdate": 1762925221399, "mdate": 1762925221399, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}