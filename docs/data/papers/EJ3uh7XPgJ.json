{"id": "EJ3uh7XPgJ", "number": 24899, "cdate": 1758361716210, "mdate": 1759896743160, "content": {"title": "On Non-interactive Evaluation of Animal Communication Translators", "abstract": "If you had an AI Whale-to-English translator, how could you validate whether or not it is working?\nDoes one need to interact with the animals or rely on grounded observations such as temperature? We provide theoretical and proof-of-concept experimental evidence suggesting that interaction and even observations may not be necessary for sufficiently complex languages. One may be able to evaluate translators solely by their English outputs, offering potential advantages in terms of safety, ethics, and cost. This is an instance of machine translation quality evaluation (MTQE) without any reference translations available. A key challenge is identifying \"hallucinations,\" false translations which may appear fluent and plausible. We propose using segment-by-segment translation together with the classic NLP shuffle test to evaluate translators. The idea is to translate animal communication, turn by turn, and evaluate how often the resulting translations make more sense in order than permuted. Proof-of-concept experiments on data-scarce human languages and constructed languages demonstrate the potential utility of this evaluation methodology. These human-language experiments serve solely to validate our reference-free metric under data scarcity.\nIt is found to correlate highly with a standard evaluation based on reference translations, which are available in our experiments. We also perform a theoretical analysis suggesting that interaction may not be necessary nor efficient in the early stages of learning to translate.", "tldr": "If you had an AI Whale-to-English translator, how could you validate whether or not it is working?", "keywords": ["Machine translation", "Reference-free evaluation", "Semantic Order", "Low-resource learning", "Active learning theory", "Animal communication"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/54192c81bfd1cc6cb9de8faa75d42d8233ce60ad.pdf", "supplementary_material": "/attachment/d7368bb8501e0861807d2814cd0c578761987dc2.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes and evaluates a method on how to rate the translation quality of translations which don’t contain any reference.\nThe proposed method is to split the given source into segments (using a LLM) and translate each segment independently (using a LLM). Afterwards a LLM is used to judge if the ordering of the target sequences makes sense.\n\nIt’s a inneressing approach to solve an unusual problem. The main weakness seems to be that it’s overly relying on wikipedia and models from the same LLM company."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method allows to give some insights of whether the translation makes any sense or not when no reference is available.\n\nFor animal communication it allows one to get some impression if the method works or not without requiring additional communication, observing the animals is enough.\n\nThey confirmed that the approach works on low resource languages and on constructed languages."}, "weaknesses": {"value": "I think the results shown in Figure 5 are most likely exaggerated. While it’s very plausible that later models simply perform better because they are strong, it is also very likely that later models saw the given wikipedia articles in its latest form during training. These models would know the correct ordering which in turn would inflate their results.\n\nSince the parallel text were extracted from wikipedia articles in different languages there is a high likelihood that the first paragraphs are not really translations of each other. The authors acknowledge this, but not having proper translations still adds a lot of noise into the evaluation.\n\nWe can assume that all LLMs saw the latest wikipedia version available at training time. It’s not unlikely that they learned the correct paragraph ordering across languages. Also since the approach to treat the first paragraphs as parallel data seems to work, we can assume that the ordering is similar across languages meaning the LLMs saw the right ordering during training.\n\nEvery step in this work involves LLMs from OpenAI. This increases the likelihood that the LLM actually knows the correct sentence since it was created by a similarly trained LLM, again inflating the results. It would be good to see the results of at least one other LLM as well. I don’t expect a different result (thanks to wikipedia), but it would be more valuable information than having everything from related LLMs.\n\nThe constructed languages were created using LLMs. Given that LLMs are trained on human languages, I’m not really convinced that these languages are really so different to human languages as claimed. For the given purpose I think the described method should work well enough."}, "questions": {"value": "End of Line 255: I think you forgot the “=0” and wanted to write “p(T, T’) = 0 indicates T is more plausible than T’”.\n\nWas the cut off date June 1st, 2024 only checked on the non English side of the wikipedia article, or also on the English side?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nvbmM5k38q", "forum": "EJ3uh7XPgJ", "replyto": "EJ3uh7XPgJ", "signatures": ["ICLR.cc/2026/Conference/Submission24899/Reviewer_FqGT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24899/Reviewer_FqGT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494229070, "cdate": 1761494229070, "tmdate": 1762943238120, "mdate": 1762943238120, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This submission derives some theoretical results on learning based on interactions vs. (less invasive and easier) observations. This is provided in the context of reference-free evaluation of low-resource or no-resource translation, with the particular use case of the translation of animal communication. Experiments show that the proposed shuffling-based metric is potentially useful to evaluate translation, without requiring costly or impossible to acquire references."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The core problem of evaluating \"translations\" in the context where references are not available is highly relevant. The application of evaluating animal communication is interesting and exciting. The proposed technique, relying on comparing the coherence of a translation with a shuffled version of itself, is novel (in this context). Experiments on both low-resource languages and constructed languages show that the proposed shuffle test yields significant correlation with a reference-based evaluation, which is promising."}, "weaknesses": {"value": "A large part of the paper is occupied by a theoretical analysis. The theory it presents derives from well-established learning theory results. More importantly, the setup of interactive vs. observational learning is very loosely connected to the actual, highly interesting application to animal communication. In fact, there is essentially no reference to the results in Section 2 in the rest of the paper. A much tighter and clearer connection between that theory and the shuffle test would be highly appreciated. Otherwise, more methodological and experimental details on ShuffleEval and its evaluation (now exiled in the appendix) would be welcome.\n\nExperiments are limited to about 100 test examples in each language/conlang. This may reflect operational constraints of animal communications studies, but is fairly low by machine translation standards. In that context, error bars or uncertainty evaluation would greatly help qualify how variable actual results and assess confidence.\n\nAlthough experiments involve many \"translators\", they are all flavours of OpenAI's GTP. In addition, all evaluation is done using GPT. MT metrics (esp. reference-free) is a lively field of research, it is surprising that none of these metrics was included as reference. Minimally, the use of GPT5/4 are references for coherence & MT quality could be manually validated on a sample of examples."}, "questions": {"value": "Recent work on MT metrics suggest that novel LLM-based metrics have strength for high-performing (high-resource) languages, but struggle to estimate mid-to-low performance systems (such as typically the case for low-resource languages, and one would assume conlangs).\n\n(l.177) Why is the empirical risk minimization infeasible? Do you mean because of practical (e.g. multiple minima) or theoretical reasons?\n\n(l.201) How is the translator family growing with the number of interactive experiments? Would it not be fixed for a choice of parameterization?\n\n(l.230-238) Paragraph is not super convincing as the argument relies on ad hoc parameter choices (c, eps).\n\n(l.249) Why would translators be different for paragraph-level (f) and segment-level (\\phi)? Esp. with LLMs one would expect that they are the same.\n\n(l.366) Why is the date of June 1, 2024 chosen?\n\n(l.343) \"prior work has validated\": a reference would be nice here. Presumably you mean the refs. in l.052.\n\n(l.357) How is 99.9% estimated from 100 examples?\n\n(l.481-483) Could you expand?\n\nTypos:\n\nl.091: Set 'F' is only introduced later (l.144)\n\nl.140: the the\n\nl.222: Is opt_n the same as {f}^*_n?\n\nl.255: missing value before \"indicates T\"?\n\nl.343: we highly -> were highly\n\nl.352: missing 'of'?\n\nl.354: while model the -> while the?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jKrtpV0nxg", "forum": "EJ3uh7XPgJ", "replyto": "EJ3uh7XPgJ", "signatures": ["ICLR.cc/2026/Conference/Submission24899/Reviewer_guKa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24899/Reviewer_guKa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925755807, "cdate": 1761925755807, "tmdate": 1762943237732, "mdate": 1762943237732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an unsupervised evaluation for machine translation in a relatively high error stage. The motivation is to extend MT and its evaluation to animal language, where `interactive testing` as they call is, is more expensive or sometimes even infeasible with current resources.\nThe core contribution is a formal motivation and simulated experiments where ShufflEval is deployed as unsupervised MT metric."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Unusual topic, adds diversity.\n- Formal derivations and proof of the approach."}, "weaknesses": {"value": "- Title is misleading. The motivation stems from animal sound translation, but the paper does not actually perform any experiments with whales.\n- Artificial language setups might not be appropriately mimicking working with animals.\n- Novelty: ShufflEval is not new and this evaluation doesn't add much to the adoption or success of it.\n- Interesting discussion on the trade-off between cost and interactivity of feedback."}, "questions": {"value": "- Why did you not use actual parallel, sentence-aligned data for the simulated? It would have removed some of the confounding factors/challenges.\n- The \"whalebreak\" term is fun, but I'm not entirely sure if I understand it correctly? What does it entail?\n- Can you motivate your data generation protocol? \n- What other simple MTQE could you apply?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H3mv0T58bn", "forum": "EJ3uh7XPgJ", "replyto": "EJ3uh7XPgJ", "signatures": ["ICLR.cc/2026/Conference/Submission24899/Reviewer_JJun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24899/Reviewer_JJun"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976089584, "cdate": 1761976089584, "tmdate": 1762943237437, "mdate": 1762943237437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a translation quality metric that is purely candidate-based, meaning it has no access to the source or any reference translations. Their metric, which they call ShufflEval, compares the coherence of translating the source as-is versus cutting the source into smaller segments and translating the permuted segments. Naturally, one would expect the original source to have much higher coherence. \n\nThe authors justify their proposed metric by appealing to basic statistical learning theory. They argue that although active learning/interactive systems can learn better models from less data than systems trained purely on observational data, this advantage essentially vanishes when collecting observational data is much cheaper than active learning, as one can just train systems on a lot more observational data. This is good news for the author's metric, as it relies solely on observational data.\n\nFinally, the authors conduct experiments on low-resource human languages as well as so-called conlangs, which are artificial languages that they fabricated via prompting some powerful language model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, I found the original premise of the paper really intriguing and refreshing. I commend the authors on the first sentence of the abstract; it immediately drew me in and made me want to know everything about their work. I'm also a big fan of the terminology, e.g., the \"whalebreak\" model.\n\nThough the applications of the authors' method seem a bit hypothetical or far-fetched at present, I found it relevant and interesting even as a purely thought-experiment."}, "weaknesses": {"value": "I have two main issues with the paper: one regarding the theory/theoretical framing and one regarding the methodology.\n\nFirst, the authors appeal to a fairly basic PAC bound (Eq 3) to argue that the expected risk of a system trained via empirical risk minimisation with respect to some loss $\\ell$ will be close to the expected risk of a system trained via active learning. The result itself is fine, but using it to justify the use of the ShufflEval loss (defined in the equation in Section 2.4) is unsound for two reasons:\n 1. It is easy to construct a system that minimises $\\ell_{ShufflEval}$, which I call the \"independent natural translator\": it either ignores its input completely and outputs a piece of fluent English text, or (to make things more interesting) hashes the input and uses the hash as a seed to a random number generator to sample a piece of fluent English text from some dataset (e.g. English Wikipedia). As such, Eq 3 is vacuous for $\\ell_{ShufflEval}$, and I cannot immediately see how the function class can be restricted to exclude these examples.\n 2. This framing doesn't address the real problem: that we don't have reference translations. Given the story the authors tell in the rest of the paper, I would have expected Eq. 3 to connect the shufflEval loss to a loss that incorporates reference translations. As such, the authors should at least make it explicit that this is not what Eq. 3 represents.\n\nAs a more minor point, I would prefer the authors model translators as conditional distributions rather than functions, since in almost all cases sources have multiple valid translations. (Though this should not change the theory much)\n\nMy methodological issue has two parts also. First, I was disappointed that, despite the paper's incredible opening sentence and the careful ethics discussion in the introduction and at the end of the paper, there are no experiments on animal-to-English translation. Given this situation, I would either reduce the emphasis on animal translation in the main text (it occupies over one page!) or include some actual animal translation experiments. \n\nSecond, regarding the conlangs examples, the authors state: \"As a result, one might expect our conlangs to be less human-like, which serves the purpose of stress-testing ShuffleEval beyond human languages.\" However, I randomly spot-checked the translations generated by language models in the supplementary material and found that most seem to produce excellent translations. As such, this calls into question whether these experiments are meaningful in the first place. At any rate, the author's statement above certainly is not borne out by this observation.\n\nIf the authors can elucidate if and how my reasoning is incorrect and address my concerns, I'll be happy to raise my score. However, if they find my concerns valid, then I'm afraid that I cannot recommend acceptance without significant modifications to the paper. \n\nMiscellaneous:\n - page 5: $\\rho(T, T')$ should be $\\rho(T, T') = 0$\n - \"We use LMs for several purposes, increasingly common practices in MT (Bavaresco et al., 2025), including\" -- needs fixing"}, "questions": {"value": "How do the authors propose to pronounce their method?\nShuff - LEE - val, Shuffle - Eval or ShuffL - EE - val or some other way?\n\nDoes the authors' method have some connection with minimum Bayes risk decoding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AsmatBotUX", "forum": "EJ3uh7XPgJ", "replyto": "EJ3uh7XPgJ", "signatures": ["ICLR.cc/2026/Conference/Submission24899/Reviewer_wFLY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24899/Reviewer_wFLY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24899/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762272723463, "cdate": 1762272723463, "tmdate": 1762943237172, "mdate": 1762943237172, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}