{"id": "4VzVWXUkhf", "number": 7702, "cdate": 1758032751220, "mdate": 1763475314311, "content": {"title": "Zero-shot Synthetic Video Realism Enhancement via Structure-aware Denoising", "abstract": "We propose an approach to enhancing synthetic video realism, which can re-render synthetic videos from a simulator in photorealistic fashion. \nOur realism enhancement approach is a zero-shot framework that focuses on preserving the multi-level structures from synthetic videos into the enhanced one in both spatial and temporal domains, built upon a diffusion video foundational model without further fine-tuning. Specifically, we incorporate an effective modification to have the generation/denoising process conditioned on estimated structure-aware information from the synthetic video, such as depth maps, semantic maps, and edge maps, by an auxiliary model, rather than extracting the information from a simulator. This guidance ensures that the enhanced videos are consistent with the original synthetic video at both the structural and semantic levels. Our approach is a simple yet general and powerful approach to enhancing synthetic video realism: we show that our approach outperforms existing  baselines in structural consistency with the original video while maintaining state-of-the-art photorealism quality in our experiments.", "tldr": "", "keywords": ["Video Enhancement", "Controllable Video Synthesis"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cbe9865a7ae1742bbe182f08d143fe138669bf35.pdf", "supplementary_material": "/attachment/28a729d15ec51947df7b96d15613c43b300b00cd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a zero-shot, training-free framework that converts synthetic videos generated by simulators into realistic videos. The method introduces a novel inversion-and-generation paradigm, which applies a reverse DDIM process to add noise to the input synthetic video and then denoises it, ensuring that the output video preserves consistent scene structure information with the input. Experimental results demonstrate that the proposed method achieves superior realism enhancement in autonomous driving scenarios compared to baseline methods, while maintaining consistency in key scene elements such as traffic lights between the input and output videos."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method introduces a zero-shot, training-free realism enhancement framework that effectively reduces the training cost of the model.\n- Experimental results show that the proposed method maintains consistency between the input and output videos for key objects such as traffic signs and traffic lights."}, "weaknesses": {"value": "1. The contributions of this paper are somewhat limited. The idea of using an inversion DDIM process has already been adopted by many existing methods, such as AnyV2V[1] and WAVE[2]. The authors should emphasize, from a methodological perspective, the theoretical advantages of the proposed inversion DDIM compared with other existing inversion schemes. In addition, experimental comparisons between the proposed baseline and other inversion DDIM baselines are needed to demonstrate the superiority and rationality of the proposed method both theoretically and experimentally.\n2. The ablation study in this paper is not well designed. The paper emphasizes that the noised images obtained through the inversion DDIM framework contain more structural information from the input video compared to pure noise. However, this claim is not experimentally validated. The authors should include two additional ablation experiments: one comparing the results when replacing the inverted latent with pure noise, followed by denoising through the DiT ControlNet, and another where T-step noise is directly added to the input video before denoising. These two ablation studies would better demonstrate that the proposed inversion DDIM method effectively preserves the structural information of the input video.\n3. The proposed method is based on the Cosmos-Transfer model, which is capable of handling multiple application scenarios such as indoor environments and robotic data. However, this paper only presents results in autonomous driving scenarios. It is recommended that the authors include experiments on additional scenarios to demonstrate the robustness of the proposed realism enhancement model.\n\n[1] AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks. TMLR 2024.\n\n[2] Wave: Warping ddim inversion features for zero-shot text-to-video editing. ECCV 2024."}, "questions": {"value": "Please see the weekness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QsMcsc4P22", "forum": "4VzVWXUkhf", "replyto": "4VzVWXUkhf", "signatures": ["ICLR.cc/2026/Conference/Submission7702/Reviewer_Yyh4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7702/Reviewer_Yyh4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904881462, "cdate": 1761904881462, "tmdate": 1762919761428, "mdate": 1762919761428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a zero-shot framework for enhancing the realism of synthetic videos using a structure-aware denoising process built on top of a pre-trained video diffusion model. \nThe paper proposes a method for enhancing the realism of synthetic videos by integrating several existing techniques: Classifier-Free Guidance (CFG), latent inversion generation, ControlNet with structure-aware guidance, and the EDM scheduler. The authors apply these components to the video enhancement domain, aiming to improve visual fidelity and structural consistency in generated video frames."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical Engineering: The paper presents a well-integrated pipeline that combines several state-of-the-art techniques, demonstrating solid engineering and implementation.\n2. Clarity: The methodology is clearly described, and the paper is easy to follow for readers familiar with diffusion models and video synthesis.\n3. Significance: The task of synthetic video realism enhancement is important for applications in virtual production, simulation, and content creation."}, "weaknesses": {"value": "1. Lack of Novelty: The core components, i.e. CFG, latent inversion, ControlNet, and EDM, are not new, and the paper does not offer significant innovation in how they are applied. The techniques used are well-established and widely applied in similar contexts, and the paper does not introduce novel algorithms or insights beyond their combination.\n2. Missing Ablation Study: There is no ablation analysis to isolate the contribution of each component, which makes it difficult to assess the effectiveness of the proposed pipeline.\n3. No Comparative Evaluation of Structure Guidance: The paper does not explore or compare different types of structure-aware guidance, which could have strengthened the evaluation and provided deeper insights."}, "questions": {"value": "1. Can you provide a discussion on individual impact of CFG, latent inversion, ControlNet, and EDM on the final video quality?\n2. Have you considered evaluating different types of structure-aware guidance (e.g., pose, depth, edge maps)? A comparative analysis could help clarify the strengths and limitations of your approach.\n3. Is there any novel insight or adaptation in how these components are combined for video enhancement, beyond straightforward integration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3RVeVaxJwR", "forum": "4VzVWXUkhf", "replyto": "4VzVWXUkhf", "signatures": ["ICLR.cc/2026/Conference/Submission7702/Reviewer_JCw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7702/Reviewer_JCw1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905941118, "cdate": 1761905941118, "tmdate": 1762919760624, "mdate": 1762919760624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their insightful comments. Our work targets the important problem of enhancing the realism of synthetic driving videos to reduce the sim-to-real gap, which is critical for autonomous driving research (7wuF, JCw1). The proposed method is practical to deploy and achieves strong empirical performance (JCw1，Yyh4). Below we address concerns regarding the limited novelty of our method and the insufficiency of our ablation study.\n\n**Novelty**\n\nWe position our work as a method for enhancing realism in synthetic videos. The primary objective is to improve style and texture realism while preserving object identity and scene semantics, rather than altering object appearance or performing large stylistic transformations (e.g., cartoonization).\n\nWithin this task, we treat the combination of ControlNet and the base model as a unified video generation model and perform inversion-based editing. ControlNet supplies strong, explicit structural constraints (via depth/segmentation/edges), which allow substantial geometric changes (e.g., object scaling, rotations) without semantic drift, simply by modifying the text prompt and using classifier-free guidance (CFG). In this way, we can achieve video realism enhancement with a simple pipeline.\n\nOur method offers several advantages over existing video editing approaches. Firstly, compared to FateZero-style attention-based methods, which enforce consistency with the original video’s structure through cross-attention, our approach explicitly delegates structural control to ControlNet while allowing the generator’s learned real-world appearance prior to dominate texture synthesis. Also, compared to frame-editing–based video editing methods such as AnyV2V, which typically operate by editing frames conditioned on one key frame, they often struggle with complex motion patterns and appearing objects. \n\nour method proves highly effective in practice, though it is simple to implement. Without CFG, our pipeline reconstructs the simulator video. Increasing CFG to 3–10 activates the base model’s realism prior: textures become photorealistic while identity and layout remain stable due to ControlNet constraints and condition-consistent inversion. We chose Cosmos as the backbone because it is trained on real videos and is more responsive to “photorealistic” cues than WAN/VACE, making it better suited to our objective. \n\n\nOur contribution lies in (1) the explicit formulation of this synthetic-to-real realism enhancement task, (2) the principled recipe—inversion with ControlNet plus CFG steering—that reliably produces realistic textures without altering appearance or semantics in simulator videos and (3) we introduce an evaluation protocol for this task that measures the consistency between the realism-enhanced video and the original input video.\n\n**Ablation Study for inversion**\n\nWe appreciate the reviewer’s suggestion and agree that explicitly isolating the role of inversion is important for validating our “inversion‑and‑generation” paradigm.\nIn our current submission, the effect of removing inversion is implicitly reflected by the Cosmos‑Transfer1 baseline in Table 1:\n__Cosmos‑Transfer1 corresponds exactly to the “w/o inversion” setting__: it starts from random noise, uses the same Cosmos backbone, and is conditioned on the same spatial controls (depth/segmentation/edges) and photorealistic prompts.\nOur method differs only by adding condition-consistent DDIM inversion before denoising.\nUnder these matched conditions, our method improves both perceptual similarity and object consistency while maintaining comparable photorealism. This comparison directly reflects the impact of inversion: under the same ControlNet conditions and prompts, initializing from an inverted latent (ours) rather than pure noise (Cosmos‑Transfer1) yields strictly better structural/semantic fidelity to the input video, especially on small, safety‑critical objects, without sacrificing photorealism. We also illustrate this effect through qualitative comparisons between our results and those of the base model in the main paper."}}, "id": "YF1WIk1McK", "forum": "4VzVWXUkhf", "replyto": "4VzVWXUkhf", "signatures": ["ICLR.cc/2026/Conference/Submission7702/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7702/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7702/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763457800237, "cdate": 1763457800237, "tmdate": 1763457800237, "mdate": 1763457800237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a zero-shot pipeline for making synthetic driving videos look more photorealistic. The stated goal is to bridge the sim-to-real gap for autonomous driving training data, especially for rare/long-tail scenes. The method is based on taking an off-the-shelf controllable video diffusion model (Cosmos-Transfer, built on a DiT backbone with a video ControlNet), performing DDIM inversion on the simulator video to obtain an initial latent that is “structurally tied” to the source content, and then generating the video again by denoising from that latent while conditioning on multiple spatial control signals, like depth, semantic segmentation, Canny edges, and using classifier-free guidance (CFG) with a “photorealistic” text prompt. The claim is that this preserves layout, object identity, and temporal coherence while removing “rendered or  “game-like” textures. The paper also proposes an evaluation protocol for “small, safety-critical objects,” in particular traffic lights and traffic signs. They detect those objects in both source and generated frames using a Grounding DINO+SAM2 pipeline, extract DINOv2 or CLIP features, and compute an average cosine-similarity-like score over the mask regions across time. They also report LPIPS to the source video, GPT-4o win rates for photorealism, and VBench metrics. Experiments are on a custom CARLA benchmark of 900$\\times$121-frame clips spanning different times of day and weather, such as day/night, rain, fog. There are also qualitative GTA samples. Baselines include: Cosmos-Transfer itself, WAN2.1-VACE, and a frame-by-frame FLUX multi-controlnet pipeline. The paper claims improved structural consistency of small objects, comparable photorealism, and lower LPIPS. No downstream task (e.g., driving perception/planning performance) is evaluated. The method is described as zero-shot, and the paper argues this is simple, general, and practically useful."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a practically relevant problem of improving the realism of synthetic driving videos to reduce the sim-to-real gap, which is important for autonomous driving research. The motivation is clear, particularly the focus on preserving small, safety-critical objects such as traffic lights and signs while enhancing overall visual fidelity.\n2. The proposed pipeline is conceptually simple and easy to follow, combining well-known diffusion techniques like DDIM inversion and ControlNet conditioning into a coherent workflow. \n3. The paper is generally well-structured, with visual examples that clearly illustrate the qualitative differences between the proposed method and baselines."}, "weaknesses": {"value": "1. Novelty is limited / largely engineering of known pieces. DDIM inversion, CFG steering, ControlNet conditioning on depth/seg/edges, and video diffusion backbones are all established. FateZero-style zero-shot editing already uses inversion to preserve structure. The paper does not convincingly argue for a fundamentally new algorithm beyond “we combine these for CARLA videos.”\n2. No downstream AV task evaluation. The primary stated motivation is improving autonomous driving models trained on synthetic data. The paper never measures whether perception/planning/forecasting improves when trained on the enhanced videos versus raw simulator output. Without this, the work does not seem to prove to be impactful.\n3. Evaluation methodology is not convincing enough. Photorealism is judged by GPT-4o pairwise votes where “our method is fixed at 50% reference,” which bakes in a comparison framing and does not allow absolute quality judgments. Also, GPT-4o is a proprietary black-box; no human preference study is provided. Also, the strongest metric (small object consistency) is designed by the authors and may favor their method. VBench scores are actually worse (authors admit deformation and temporal instability), which contradicts the claim that the method improves temporal coherence.\n4. Claims of temporal consistency are overstated. The paper repeatedly says it “preserves temporal coherence,” yet later concedes that objects deform, structure drifts, and VBench penalizes “Dynamic Degree” / “Imaging Quality.” The CARLA videos are only 121 frames, and longer clips require chunking that introduces discontinuities. This undermines one of the headline claims. The resulting videos in supporting materials clearly show temporal instability in some regions.\n5. Inversion/latents not ablated. The central technical claim is that deterministic DDIM inversion to $x_T$ preserves global layout/content which then guides the denoising. But there is no ablation “w/o inversion, with same spatial controls” to quantify how much inversion matters. The ablations in Tables 2–3 instead vary CFG or remove certain control inputs. We therefore cannot tell if the main advertised trick is actually necessary.\n6. Over-claiming of “state-of-the-art photorealism”. Table 1 actually shows that Cosmos-Transfer has roughly comparable photorealism per GPT-4o, and WAN2.1-VACE is competitive. Meanwhile, the proposed model hurts temporal stability per VBench. So the “state-of-the-art photorealism with structural consistency” claim, including in the abstract and Figure 1 caption, is not convincingly demonstrated."}, "questions": {"value": "1. Downstream utility. Can you provide any quantitative downstream result, even preliminary, showing that training an AV perception model (e.g., traffic light state classifier, sign detector, lane segmentation, etc.) on your enhanced videos improves performance on real-world validation data compared to training on raw CARLA? Without this, the “sim-to-real for autonomy” claim is unsubstantiated.\n2. Role of inversion. Please, provide an ablation where you do not perform DDIM inversion. Instead, start denoising from random noise but condition on exactly the same spatial controls (depth/seg/edge) and the same positive prompt. How much do LPIPS, small object alignment, and VBench change? This is critical to prove the necessity of your “inversion-and-generation” paradigm.\n3. Temporal stitching details. For sequences longer than 121 frames, you mention a chunk-based approach that creates discontinuities at boundaries. How exactly are chunks overlapped / blended? Are the control maps continuous through the boundary? How severe are these artifacts quantitatively?\n4. Prompt generation / editing details. How are $c_{inv text}$ and $c_{real text}$ produced in practice? Are they hand-authored, heuristically edited, or automatically generated (e.g., via VideoLLaMA3 captioning + manual style adjectives like “masterpiece, best quality, realistic lighting, photorealistic:1.2…”)? Please clarify reproducibility here. \n5. Failure cases. Please show negative examples where the pipeline hallucinates unsafe content (e.g., changes a red light to green, deletes a pedestrian, invents a new vehicle). Do such errors occur? How often? This matters for safety.\n6. Why VBench gets worse. You acknowledge deformation / shape drift and low “Dynamic Degree / Imaging Quality” scores. How do you reconcile this with the strong claim that your method improves temporal coherence? Can you provide a metric (besides LPIPS-to-source) that actually improves temporal stability over Cosmos-Transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z2HXz5Qzui", "forum": "4VzVWXUkhf", "replyto": "4VzVWXUkhf", "signatures": ["ICLR.cc/2026/Conference/Submission7702/Reviewer_7wuF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7702/Reviewer_7wuF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7702/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922357731, "cdate": 1761922357731, "tmdate": 1762919759946, "mdate": 1762919759946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}