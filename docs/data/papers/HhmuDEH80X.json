{"id": "HhmuDEH80X", "number": 15405, "cdate": 1758251017012, "mdate": 1759897309076, "content": {"title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Synergizing-Oriented Multi-Turn Reinforcement Learning", "abstract": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is an emerging strategy to improve the reasoning efficiency of LLMs. Despite its promising benefits, existing studies equally compress all thoughts within a long CoT, hindering more concise and effective reasoning. To this end, we first investigate the importance of different thoughts by examining their effectiveness and efficiency in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. Building upon the insights, we propose a theoretically bounded metric to jointly measure the effectiveness and efficiency of different thoughts. We then propose Long$\\otimes$Short, an efficient reasoning framework that enables two LLMs to collaboratively solve the problem: a long-thought LLM for more effectively generating important thoughts, while a short-thought LLM for efficiently generating remaining thoughts. Specifically, we begin by synthesizing a small amount of cold-start data to fine-tune LLMs for long-thought and short-thought reasoning styles, respectively. Furthermore, we propose a synergizing-oriented multi-turn reinforcement learning, focusing on the model self-evolution and collaboration between long-thought and short-thought LLMs. Experimental results show that our method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and GPQA Diamond benchmarks.", "tldr": "This paper stablish a thought analysis framework and then propose an efficient reasoning framework that enables two LLMs to collaboratively solve the problem.", "keywords": ["Large language models", "chain-of-thought reasoning", "efficient reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/556e76c6d6ce10ba6e36426e52d56a9bb16939eb.pdf", "supplementary_material": "/attachment/4238a632724b21028f241f314189c23e9adac08b.zip"}, "replies": [{"content": {"summary": {"value": "This paper explores reasoning compression for large language models (LLMs) by analyzing the importance of individual thoughts within long Chain-of-Thought (CoT) traces. The authors propose **Long⊗Short**, a two-model framework where a \"long-thought\" LLM generates critical reasoning steps and a \"short-thought\" LLM handles less important segments. The method combines cold-start supervised fine-tuning and multi-turn reinforcement learning (RL) between the two models. Experiments on multiple reasoning benchmarks (MATH500, AIME24/25, AMC23, GPQA) show moderate accuracy gains and over 80% reduction in token length compared to long-CoT baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important and active topic: improving reasoning efficiency of LLMs through CoT compression.  \n- The experimental section is comprehensive, including diverse datasets and ablation studies.  \n- The asynchronous multi-turn RL design is well-documented and empirically stable."}, "weaknesses": {"value": "- **Limited novelty**: The core contribution is mainly a pipeline combining existing techniques (CoT chunking, two-model SFT, RL fine-tuning), without a clear new theoretical or algorithmic insight.  \n- **Two-model framework**: Requires training and maintaining two separate LLMs, which increases complexity and weakens industrial practicality.  \n- **Unfair comparison**: The baselines (e.g., UPFT, DAST, C3oT) use single-model setups, while this approach leverages two coordinated models.  \n- **Marginal improvements**: Performance gains are small (≈1–2 points) and become saturated for larger models (>8B), suggesting that scaling benefits are not fully realized.  \n- **Weak ablation**: The paper does not sufficiently isolate the contribution of the “Long⊗Short synergy” beyond generic SFT and RL benefits."}, "questions": {"value": "1. How sensitive are the results to the specific choice of long vs. short thought partitioning rules?  \n2. Could the same compression effect be achieved with a single model using controlled CoT length or token-level regularization?  \n3. How does the method perform under real inference-time constraints (e.g., limited decoding budget)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0HLQ27K1J5", "forum": "HhmuDEH80X", "replyto": "HhmuDEH80X", "signatures": ["ICLR.cc/2026/Conference/Submission15405/Reviewer_epch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15405/Reviewer_epch"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867190990, "cdate": 1761867190990, "tmdate": 1762925683377, "mdate": 1762925683377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to improve the reasoning process of large language models by distinguishing between “genuine” and “non-genuine” thoughts within the chain-of-thought framework. The authors introduce a filtering mechanism designed to identify productive intermediate reasoning steps and suppress unhelpful or misleading ones, with the goal of enhancing both reasoning quality and interpretability. Experiments are conducted on a range of reasoning benchmarks to evaluate the method’s effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of treating reasoning traces as heterogeneous and selectively weighting or pruning them is interesting to me.\n2. The experimental section is well-organized, with comparisons across standard benchmarks such as GSM8K and StrategyQA. The paper is generally well written."}, "weaknesses": {"value": "1. The paper’s central notion of “genuine thought” is not well-defined in operational or mathematical terms. The method appears to rely on subjective or post-hoc labeling of reasoning steps, rather than any verifiable criterion. This makes the approach unscientific and difficult to reproduce.\n2. The proposed filtering algorithm seems to be an ad hoc combination of existing techniques such as CoT pruning or confidence-based re-ranking. There is no theoretical justification or ablation showing how each component contributes to performance.\n3. The evaluation is not taht unconvincing. The experiments are limited to small-scale reasoning benchmarks with unclear experimental settings. It is still questionable that if the method can generalize beyond the chosen datasets.\n4. The manuscript contains exaggerated statements about “redefining thought” or “unifying reasoning and cognition,” which are not supported by any empirical or theoretical results. Such philosophical or speculative claims weaken the credibility of the technical content."}, "questions": {"value": "1. What is the formal definition or measurable criterion for “genuine thoughts” in operational or mathematical formulation?\n2. Is there any ablation studies that report performance with different filtering thresholds or criteria?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mvw5l8DK3u", "forum": "HhmuDEH80X", "replyto": "HhmuDEH80X", "signatures": ["ICLR.cc/2026/Conference/Submission15405/Reviewer_95kP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15405/Reviewer_95kP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991811506, "cdate": 1761991811506, "tmdate": 1762925683023, "mdate": 1762925683023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how to compress long CoTs in LLMs to improve reasoning efficiency.\nIt uses a three-step approach.\nThe authors analyze the importance of different thoughts using automatic CoT chunking and Monte Carlo rollouts.\nThey introduce a metric that jointly measures thought effectiveness and efficiency.\nBased on this, they propose Long&&Short, a collaborative reasoning framework involving two LLMs: one focusing on key long thoughts, the other on concise short thoughts.\nBoth models are fine-tuned with cold-start data and further optimized via a multi-turn reinforcement learning approach encouraging synergy."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.  The efficiency of reasoning CoT is indeed one of the key challenges for large-scale LLM applications.\n2.  The formulas and figures in this paper are presented clearly, and the authors carefully highlight key points with different colors.\n3.  The paper first explores how different parts of the CoT affect the results, then proposes a comprehensive metric, and finally uses these insights for training — this overall approach is very well-grounded and makes perfect sense."}, "weaknesses": {"value": "There is still much to explore regarding the experiments; please refer to the questions section below."}, "questions": {"value": "1.  The authors use LLMs to chunk the CoT, but since different problems may have CoTs of varying lengths and formats, have the authors considered the potential influence of CoT length and structure?\n2.  Table 1 is not presented clearly enough—at first glance, it’s hard to intuitively understand how the two LLMs collaborate and what benefits this brings, which makes the paper more difficult to follow.\n3.  Could the authors further elaborate on the conclusions drawn from Figure 2(b)?\n4.  Why did the authors choose to compare their method with distilled model versions in the experiments?\n5.  The paper seems to lack more analytical experiments, such as testing different training settings or algorithms to directly quantify the improvement in reasoning efficiency. If I understand correctly, the two LLMs are used to generate training data (for long-thought and short-thought reasoning), and this dataset is then used to train a single LLM. Could the authors provide evidence that the trained LLM has indeed acquired this native reasoning style?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wtILVW3MHk", "forum": "HhmuDEH80X", "replyto": "HhmuDEH80X", "signatures": ["ICLR.cc/2026/Conference/Submission15405/Reviewer_vX4E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15405/Reviewer_vX4E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997480997, "cdate": 1761997480997, "tmdate": 1762925682570, "mdate": 1762925682570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes \"Long & Short,\" a novel framework for efficient CoT reasoning that bypasses the inefficiency of uniform compression by leveraging two specialized LLMs. The method first quantifies the importance of individual thoughts via Monte Carlo rollouts, and then trains a long-thought and a short-thought LLM to collaboratively solve problems using synergizing-oriented multi-turn reinforcement learning, achieving over 80% token reduction with comparable accuracy to full CoT models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The Monte-Carlo rollout study shows front thoughts contribute the most to accuracy while often inflating length, motivating selective preservation.\n2. The cold-start SFT builds long and short datasets directly from scored thoughts.\n3. The paper achieves a token length reduction of over 80% while maintaining performance on challenging, multi-step benchmarks."}, "weaknesses": {"value": "1. In Figure 2, the general trend should show accuracy improving as more thinking steps are added, followed by a slight decrease. However, the highest accuracy for Math500 occurs at 0 thought chunks, indicating that the best performance is achieved without any reasoning process. The curve declines too early. Could the authors clarify why this happens?\n2. In Lines 200–201, the phrase “assign higher scores to thoughts with shorter context length” seems inaccurate. It would be more precise to state “assign higher scores to earlier thoughts.”\n3. Figure 3 seems to have a similar pattern to Figure 2, where the score decreases as the number of thoughts increases. Does this imply that responses without thoughts achieve the best performance?\n4. In Lines 214–215, the authors mention that high scores are predominantly associated with front thoughts. Given this, why not employ a training-free approach such as early stopping instead? What is the benefit of combining SFT and RL under this setting?\n5. It is better to include an ablation study on the decay penalty term $\\delta (y_i)$."}, "questions": {"value": "1. The paper is not well-written, and several parts require further clarification.\n2. In line 157, the authors mention performing rollouts at each thought. Does this mean that the model is prompted with the question and the accumulated thoughts as the instruction, and then asked to generate the final answer directly without additional reasoning?\n3. The use of $N_i^{sum}$ and $N_i^{right}$ is confusing. Since each rollout contains multiple thoughts, and $N_i^{sum}$ represents the total number of rollouts, it should not be specific to a particular thought $i$. Could the authors clarify this notation?\n4. Could the authors elaborate on how the SFT training data is constructed? If my understanding is correct, the process starts from long-thought data, and for less important thoughts, the model switches from long to short thoughts. Then, how are $D_{long}$ and $D_{short}$ created? How are tags such as <rethink> generated or assigned during this process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E62Q1Wjacg", "forum": "HhmuDEH80X", "replyto": "HhmuDEH80X", "signatures": ["ICLR.cc/2026/Conference/Submission15405/Reviewer_kdZZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15405/Reviewer_kdZZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194827958, "cdate": 1762194827958, "tmdate": 1762925681863, "mdate": 1762925681863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}