{"id": "gufRimweSQ", "number": 21909, "cdate": 1758323451055, "mdate": 1759896897330, "content": {"title": "STEM: SCALING TRANSFORMERS WITH EMBEDDING MODULES", "abstract": "Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce \\textbf{STEM} (\\emph{Scaling Transformers with Embedding Modules}), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances it knowledge storage capacity. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to $\\sim$3--4\\% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while remaining simpler to train and deploy than existing fine-grained sparse models.", "tldr": "STEM replaces each FFN up-projection with a per-layer embedding lookup to scale parametric capacity without increasing per-token compute or cross-device communication, yielding FLOP-efficient performance gains.", "keywords": ["Sparse Transformer", "Parametric scaling", "Embedding Layers", "Foundation Models", "Pre-training", "Model Architecture"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f926ae295b79d144ef88a6c0ae443a9cc1c041b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes STEM, a sparse transformer architecture that replaces the FFN up-projection with token-indexed embeddings from a layer-local lookup table. The authors claim improved training stability, better downstream performance, and enhanced long-context capabilities compared to dense baselines and MoE alternatives."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies real issues with fine-grained MoE models (training instability, load balancing, communication overhead) and proposes a simpler alternative.\n\n2. The paper provides evaluation across multiple scales (350M, 1B), training regimes (pretraining, midtraining, context extension), and benchmarks, as well as systematic ablations on layer count, placement (gate vs. up-projection), and hybrid variants.\n\n3. Analysis of embedding space properties (angular spread) is valuable."}, "weaknesses": {"value": "1. There is a parameter mismatch in the experiments, in particular, STEM uses 1.14B parameters vs. 0.37B dense baseline at 350M scale.\n\n2. The results are incocsistent, as table 3 shows STEM often underperforms baseline: 350M: ARC-C (32.68 vs 30.55), SIQA (39.76 vs 41.10), 1B: BoolQ (61.66 vs 64.21), PIQA (75.00 vs 73.44 favors STEM but margin is small). Abstract claims \"up to 3-4% improvement\" but average gains are ~1% or less.\n\n3.  The long-context analysis is limited, as NIAH is a synthetic task."}, "questions": {"value": "1. Can you provide results with parameter-matched dense baselines?\n\n2. What are actual wall-clock training/inference times and memory usage?\n\n3. How does this compare to DeepSeekMoE, Mixtral, or other modern MoE methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fs7Cq2svky", "forum": "gufRimweSQ", "replyto": "gufRimweSQ", "signatures": ["ICLR.cc/2026/Conference/Submission21909/Reviewer_eAWT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21909/Reviewer_eAWT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760911563984, "cdate": 1760911563984, "tmdate": 1762941977396, "mdate": 1762941977396, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces STEM, which is a static, token-indexed design that replaces the FFN up-projection with a layer-local embedding lookup.This decouples parametric capacity from per-token compute and cross-device communication, yielding lower per-token FLOPs and fewer parameter accesses, and enabling CPU offload with asynchronous prefetch. STEM is a static, token-indexed, fine-grained mechanism that replaces only the up-projection in gated FFNs with a token-specific vector retrieved from a layer-local embedding table."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "STEM simplifies operation compared to MOE by dense up-projection in the SwiGLU FFN with a token-indexed vector from a per-layer table lookup. \n\nSTEM improves both computation by reducing per-layer FLOPs during training and memory access by lowering parameter traffic relative to a dense up-projection.\n\nExperiments are informative, with sufficient ablations. The paper does a good job at describing the various aspects of the STEM technique and in pointing out the benefits of the approach relative to other methods such as MOE. Design choices are clearly (if tersely) motivated."}, "weaknesses": {"value": "Paper is dense and focused. It uses a lot of jargon and will not be so accessible to readers not familiar with the ideas and issues specific to this narrow research area. This is not necessarily a weakness, but readers that are not in this area may not appreciate the paper's purpose or contributions. The paper would be better with some diagrams to illustrate the architectural differences between STEM and other approaches, including MOE techniques. Figure 1.c is too small and unclear for this purpose.\n\nIn section 4.4.1 the term \"ROI\" is used in reference to table 3. However, table 3 does not include ROI in the numbers. What does the acronym ROI stand for? \"Return on Investment\"? That doesn't really make sense in this context (what is the \"investment\"? What is the \"return\"?). The text only vaguely defines what is meant by ROI, as \"performance over training flops\". I assume that \"over\" in this context means divided by, and performance means the average of the scores. This should be expressed more clearly and precisely, e.g. as an equation \"ROI = average accuracy/#training flops\". \n\nTable 3 (and other tables in the paper) is lacking an informative caption. There is no definition of the numbers, just the column headings. The column headings just state the problem task (presumably). The caption should say that these numbers represent percent correct on the task test sets.\n\nIt is not clear how figure 3 expresses anything about the \"Geometry\" (which is misspelled here) of the STEM Embeddings. All that it shows are histograms of cosine distances."}, "questions": {"value": "What are the limitations of the STEM method compared to the competing approaches such as MOE? Presumably something is lost due to the increased sparsity compared to MOE."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3Af3PQR8xA", "forum": "gufRimweSQ", "replyto": "gufRimweSQ", "signatures": ["ICLR.cc/2026/Conference/Submission21909/Reviewer_7Z8s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21909/Reviewer_7Z8s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851049642, "cdate": 1761851049642, "tmdate": 1762941976978, "mdate": 1762941976978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces STEM, a new transformer architecture that replaces the FFN up-projection with a token-indexed, layer-local embedding lookup. This approach offers static sparsity, removing runtime routing and enabling CPU offload, while keeping gate and down-projection layers dense. Experiments on 350M and 1B parameter models show consistent improvements compared with dense and Hash-MoE baselines, with no observed instability. Theoretical analysis, ablations, and interpretability experiments (e.g., token embedding swapping) support the design’s efficiency and explainability advantages."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Architecture Design. A creative and simple static-sparsity alternative to MoE models that avoids routing overhead and load-balancing complications.\n2. Strong Empirical Validation. Comprehensive experiments across scales (350M, 1B) and tasks demonstrate consistent benefits in both efficiency and accuracy.\n3. Training Stability. Unlike many fine-grained sparse models, STEM avoids loss spikes and under-trained experts."}, "weaknesses": {"value": "1. Potential Memory Overhead. Each layer’s token-indexed embedding table may become impractical for large vocabularies, despite CPU offload?\n2. Lack of Ablation on Embedding Dimensionality. The effect of embedding size or vocabulary size on performance and stability is unexplored."}, "questions": {"value": "1. How does STEM scale beyond 1B parameters? Are there any observed communication or memory bottlenecks at larger scales (e.g., 7B+)?\n2. What is the exact CPU-GPU data transfer overhead in practical deployments? Are there latency trade-offs during inference?\n3. Could we have some analysis on inference and training efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7NjXSJhr0e", "forum": "gufRimweSQ", "replyto": "gufRimweSQ", "signatures": ["ICLR.cc/2026/Conference/Submission21909/Reviewer_8jLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21909/Reviewer_8jLf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979214051, "cdate": 1761979214051, "tmdate": 1762941976716, "mdate": 1762941976716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new modification to the Transformer architecture to enhance fine-grained sparsity. The most popular way to achieve this today is via MoE architectures which are known to the tricky to train\nin a stable manner and introduce significant communication overheads during training and inference. In addition MoE training also has to deal with issues such as experts getting under-trained. Load-balancing objectives are commonly used to alleviate this but they come at a certain quality cost typically. \nSTEM proposes instead looking at the standard SwiGLU FFN architecture and replacing the up-projection layer with a token indexed embedding lookup that is specific to each layer. That is a depth L model will have L+1 embedding tables in total. This is a way of introducing static fine-grained sparsity into deeper layers of the model. The benefits of such a change include minimizing communication overhead, enabling CPU offload with asynchronous prefetch while achieving fine-grained sparsity.\n\nThe authors perform pre-training experiments on a 350M parameter model for 100B tokens and a 1B parameter model for 1T tokens. They also perform mid-training and context extension experiments on a 1B model with 100B and 20B token budgets respectively.\n\nThe authors observe better training stability with STEM training. They see consistently improved performance on knowledge-intensive tasks.\nThey also see improved long-context inference performance as STEM activates more distinct parameters as the sequence length grows. STEM also is more efficient in terms of train time and test time FLOPs.\nA key concern with approaches like STEM is the gain in shallow parametric capacity comes with losses in deep contextual reasoning abilities. However, the experiments of the paper show some initial promising results to potentially mitigate this concern."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes an interesting idea to complement the transformer architecture. The authors are aware of the many practical challenges surrounding today’s transformer architecture and take them into account when designing STEM.\n\n- The proposed method can be applied in stages by deciding to only apply it to some portion of the layers of a transformer. This offers for a smoother transition away from the standard architecture and for greater flexibility in architecture design.\n\n- The paper performs an analysis of the pairwise cosine similarity of STEM embeddings vs original FFN up projection’s embeddings and finds that there is less redundancy in STEM embeddings. While this is an interesting finding, it is unclear whether reduced redundancy might negatively affect robustness."}, "weaknesses": {"value": "- Although the paper correctly accounts for reduced FLOPs, it can often be quite challenging to realize the full benefit of a FLOP reduction in practice. The paper offers a theoretical discussion of how the embedding table reads can be pre-fetched and offloaded onto CPU which makes sense but it would have made a stronger case if wall-clock training, prefill and generation time comaprisons were provided along with HBM usage.\n\n- The concern about the hit to contextual reasoning abilities (since we replace processing of later layers’ highly enriched residual streams with a token indexed embedding) isn’t fully alleviated by the downstream numbers shown. It is nice that we see noticeable improvements in MMLU and GSM8k which are more reasoning heavy tasks but given the scale of the model, the improvement could be coming from the greater parametric knowledge capacity provided by STEM."}, "questions": {"value": "- Can you provide some details on what the wall-clock time or GPU hour impact of STEM is in practice? I believe the proposed benefits can indeed be realized but it might be less than the theoretical maximum of 33% reported.\n\n- Can you provide more details on how you perform the mid-training? How are the STEM embeddings initialized? How long does the loss spike (due to sudden architecture change) take to recover?\n\n- Can you provide more downstream evaluations on more contextual reasoning heavy benchmarks? MMLU at the 1B scale seems barely above random guessing (25%) and moreover MMLU is also quite knowledge intensive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gBtoGcolW0", "forum": "gufRimweSQ", "replyto": "gufRimweSQ", "signatures": ["ICLR.cc/2026/Conference/Submission21909/Reviewer_YuJd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21909/Reviewer_YuJd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21909/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762017944303, "cdate": 1762017944303, "tmdate": 1762941976435, "mdate": 1762941976435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}