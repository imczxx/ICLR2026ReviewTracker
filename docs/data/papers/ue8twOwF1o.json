{"id": "ue8twOwF1o", "number": 6637, "cdate": 1757991055262, "mdate": 1762943466016, "content": {"title": "Motion-Guided Prior Support and Polarity Interaction for Event Stream Super-Resolution", "abstract": "In this paper, we aim to enhance the representation of spatio-temporal semantics during the Event stream Super-resolution (ESR) reconstruction by leveraging inter-frame motion information.\nTo this end, we propose a Motion-Guided Prior Support and Polarity Interaction Network (MPS-PI Net). The MPS-PI Net takes event frames as the primary input, while incorporating positive and negative event streams as auxiliary inputs. The MPS-PI Net contains two novel designs: Motion-Guided Semantic Prior (MGSP) Module and Bipolar Semantic Interaction and Fusion (B-SIF) Module. In the MGSP module, we capitalize on inter-frame optical flow information to seamlessly integrate semantic cues derived from previously reconstructed frames into the super-resolution reconstruction process of the current frame. This integration provides valuable prior support for reconstructing the content of the current frame with greater accuracy. Building upon the prior semantic information introduced by the MGSP module, within the B-SIF Module, we initially undertake self-representational enhancement for both positive and negative polarity semantics independently. Following this, we conduct an interactive fusion of these two polarity semantics to fully harness their unique advantages. Experimental results unequivocally demonstrate that our proposed MPS-PI Net achieves competitive performance on many ESR datasets.", "tldr": "", "keywords": ["Event stream Super-resolution;  Semantic Interaction and Fusion;   Event Camera"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9058fd7af1657da6dacc62e4699ca3f17616b6f8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the task of event stream super-resolution (ESR) and proposes a Motion-Guided Prior Support and Polarity Interaction Network (MPS-PI Net). The method consists of two main components: a Motion-Guided Semantic Prior (MGSP) module and a Bipolar Semantic Interaction and Fusion (B-SIF) module. Following prior ESR approaches such as BMCNet-ESR (CVPR’24), the authors evaluate their method on the same benchmark datasets and report state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The proposed method demonstrates strong quantitative results, achieving state-of-the-art MSE scores across all datasets and upscaling factors presented in Table 1."}, "weaknesses": {"value": "1.\t**Unclear Motivation**: The motivation for the proposed method is not clearly articulated in the Introduction. The paper does not provide a convincing justification for why the MGSP and B-SIF modules are necessary, as the stated limitations of previous ESR methods are not thoroughly established.\n2.\t**Unrealistic Qualitative Examples**: The qualitative examples shown appear to be based on unrealistic inputs. The low-resolution (LR) event frames in Fig. 4 and Fig. 5 seem to have a far lower quality than what would be captured even by a standard event camera (e.g., a DAVIS346). This raises questions about whether the synthetic data is representative of a real-world scenario. It is suggested to conduct experiments on data more similar to real event streams. Additionally, the qualitative results on real data (from EventNFS) should be presented in the main paper, not relegated to the appendix (Fig. 5).\n3.\t**Insufficient Validation of Practical Utility**: The paper does not sufficiently validate the practical utility of the ESR task itself. The MSE improvements should be validated on downstream tasks, such as object detection or segmentation, to verify that the super-resolved event streams can indeed improve accuracy in real-world applications. While prior work (e.g., BMCNet) has included such validation, this paper lacks a similar analysis, making the significance of the MSE gains unclear.\n4.\t**Marginal Visual Improvements**: The visualized improvements against baselines in Fig. 4 and Fig. 6 appear marginal in terms of semantic content. While the method may produce slightly sharper or more numerous event edges, it is not immediately clear how this translates to a more meaningful or semantically rich representation of the scene. The practical benefit of these visual enhancements is not well-demonstrated."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i4fgXH35Kp", "forum": "ue8twOwF1o", "replyto": "ue8twOwF1o", "signatures": ["ICLR.cc/2026/Conference/Submission6637/Reviewer_35qN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6637/Reviewer_35qN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572208538, "cdate": 1761572208538, "tmdate": 1762918954146, "mdate": 1762918954146, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "9shXoF3gZ2", "forum": "ue8twOwF1o", "replyto": "ue8twOwF1o", "signatures": ["ICLR.cc/2026/Conference/Submission6637/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6637/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762943464231, "cdate": 1762943464231, "tmdate": 1762943464231, "mdate": 1762943464231, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MPS-PI Net, a method that leverages inter-frame motion information to enhance event-based super-resolution. The approach includes a Motion Guided Semantic Prior (MGSP) module and a Bipolar Semantic Interaction and Fusion (B-SIF) module. The MGSP module extracts motion and semantic information from previous frames to guide the super-resolution of the current frame, while the B-SIF module performs self-representation enhancement and fusion of positive and negative events. Experimental results demonstrate the effectiveness of these modules."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method achieves state-of-the-art performance on event-based super-resolution tasks while maintaining a relatively small number of parameters.\n\n2. The method introduces two novel modules and effectively leverages motion information between event frames."}, "weaknesses": {"value": "1. Limited novelty and contribution. The main contribution of this work lies in leveraging motion information between event frames to guide event-based super-resolution. However, similar approaches have been extensively explored in video-based super-resolution, and the authors do not provide targeted improvements that exploit the unique characteristics of event data. For example, event cameras have high temporal resolution, and the interval between event frames is much smaller than between video frames. Could motion information be extracted across multiple frames or adaptively, rather than simply using adjacent frames?\n\n2. Potential lack of practical applicability. With the development of hardware, the resolution of event cameras has increased significantly—for example, the EVK4 reaches 1280×720 pixels. In contrast, this study focuses on super-resolution for very low-resolution inputs (55×31 and 80×45 pixels), which may not match real-world scenarios. It would be more practical to target high-resolution event cameras capturing small objects.\n\n3. Missing important baselines. The proposed method uses a flow network for inter-frame motion estimation, which has been widely applied in video-based super-resolution [1,2]. The authors should clarify how their method differs from video-based approaches and include comparisons. Additionally, a key event-based super-resolution method, EventZoom, is not compared.\n\n4. Unclear role of semantic information. From my understanding, the method primarily leverages temporal consistency and motion information between event frames, and the semantic component is not clearly demonstrated.\n\n5. The line 65 “the structured semantic meanings embedded in positive and negative polarity events are distinct” is unclear. Positive and negative events typically occur together, reflecting object motion and shape, and do not carry substantially different semantic content.\n\n6. Why does the method rely on the RGB-based SpyNet for optical flow estimation? In fact, many event-based optical flow methods have already been proposed [3–5], which may be more suitable for this context.\n\n**Reference**\n\n[1] Tu Z, Li H, Xie W, Liu Y, Zhang S, Li B, Yuan J. Optical flow for video super-resolution: A survey. Artificial Intelligence Review. 2022 Dec;55(8):6505-46.\n\n[2] Chan KC, Zhou S, Xu X, Loy CC. Basicvsr++: Improving video super-resolution with enhanced propagation and alignment. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition 2022 (pp. 5972-5981).\n\n[3] Wan Z, Dai Y, Mao Y. Learning dense and continuous optical flow from an event camera. IEEE Transactions on Image Processing. 2022 Nov 14;31:7237-51.\n\n[4] Gallego G, Rebecq H, Scaramuzza D. A unifying contrast maximization framework for event cameras, with applications to motion, depth, and optical flow estimation. InProceedings of the IEEE conference on computer vision and pattern recognition 2018 (pp. 3867-3876).\n\n[5] Gehrig M, Millhäusler M, Gehrig D, Scaramuzza D. E-raft: Dense optical flow from event cameras. In2021 International Conference on 3D Vision (3DV) 2021 Dec 1 (pp. 197-206). IEEE."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XMRIqHJ2YU", "forum": "ue8twOwF1o", "replyto": "ue8twOwF1o", "signatures": ["ICLR.cc/2026/Conference/Submission6637/Reviewer_P3ZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6637/Reviewer_P3ZD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573062410, "cdate": 1761573062410, "tmdate": 1762918953417, "mdate": 1762918953417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles event stream super-resolution by leveraging motion cues and polarity-aware modeling. It proposes MPS-PI Net, a framework that treats event frames as the main input while using positive and negative event streams as auxiliaries. A Motion-Guided Semantic Prior (MGSP) module aligns and injects semantic cues from previously reconstructed frames via optical flow to support the current frame, aiming to improve temporal coherence and reduce reconstruction difficulty. A Bipolar Semantic Interaction and Fusion (B-SIF) module first strengthens intra-polarity representations through parallel spatial-channel attention, then performs cross-polarity interaction to exploit complementarities without early fusion interference. The approach positions itself as a principled way to preserve event-specific dynamics, enhance spatio-temporal consistency, and provide a practical ESR pipeline validated on both synthetic and real data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper identifies two key issues in event-stream ESR: insufficient spatial–motion semantic consistency and underutilization of the complementary patterns between positive and negative polarities.\n2.\tThis paper uses optical flow to align the historical hidden state and injects semantic cues from the previously reconstructed frame into the current reconstruction, simultaneously reducing current reconstruction difficulty and enhancing cross-frame semantic consistency."}, "weaknesses": {"value": "1.\tThe effectiveness of B-SIF is mainly supported by empirical results, but lacks deeper analysis of cross-polarity interaction. The proposed cross-attention appears to be a plug-and-play Transformer application; the paper should further articulate the motivation for the design. Why is cross-polarity interaction reasonable? Why is it effective?\n2.\tThe framework relies on optical flow to align the previous hidden state. Under large motion, non-rigid deformation, occlusion, or flow estimation errors, could error propagation introduce significant negative effects? This requires further discussion.\n3.\tWhat concrete benefits does Event Stream SR bring to event-based vision? A deeper discussion is recommended; otherwise, the motivation of the paper may appear unclear.\n4.\tThe comparative methods in Table 4 should include citations."}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yfqCeyWgWk", "forum": "ue8twOwF1o", "replyto": "ue8twOwF1o", "signatures": ["ICLR.cc/2026/Conference/Submission6637/Reviewer_MHVF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6637/Reviewer_MHVF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620196382, "cdate": 1761620196382, "tmdate": 1762918953077, "mdate": 1762918953077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Motion-Guided Prior Support and Polarity Interaction Network for event stream super-resolution.\n Reported results on synthetic and real datasets show quantitative gains over selected baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Leverages inter-frame motion and polarity-specific streams to enhance ESR features.\n\nClear modular design with ablations indicating component contributions.\n\nEvaluations provided on both synthetic and real event datasets."}, "weaknesses": {"value": "1 Mis-specified motivation. \n\nModern event sensors (e.g., Prophesee Gen4 at 1280×720; CeleX-V) already achieve high spatial resolution, rendering \"low resolution\" an unconvincing driver for ESR. The paper should (i) delineate specific ESR application scenarios and (ii) rigorously justify its advantages over higher-resolution sensors or alternative preprocessing techniques.\n\n2 Llimited novelty. \n\nThe assertion that prior ESR methods merge polarities and discard information is incorrect; several works explicitly preserve polarity separation (e.g., Li et al., ICCV 2021; BMCNet, CVPR 2024). The proposed B-SIF thus appears incremental—demonstrate quantifiable benefits under controlled, matched comparisons to establish novelty.\n\n3. Gaps in metrics and task validation. \n\nWhile prior ESR literature commonly reports MSE, this work uses only RMSE, hindering direct benchmarking. More critically, validate utility on downstream tasks (e.g., event-based video reconstruction, tracking, optical flow, or detection) using standard pipelines and established metrics.\n\n4. Limited contribution relative to added complexity. \n\nThe approach integrates existing motion-guided priors with polarity-separated attention. Reported gains are modest compared to increased complexity and latency; strengthen evidence through cross-dataset generalization, robustness to sensor noise and illumination variations, and ablation studies on flow quality, polarity imbalance, and noise levels."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dec8bFUwh7", "forum": "ue8twOwF1o", "replyto": "ue8twOwF1o", "signatures": ["ICLR.cc/2026/Conference/Submission6637/Reviewer_vRZF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6637/Reviewer_vRZF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887991206, "cdate": 1761887991206, "tmdate": 1762918952484, "mdate": 1762918952484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}