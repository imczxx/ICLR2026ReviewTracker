{"id": "ebe9uW2yQA", "number": 16717, "cdate": 1758268030717, "mdate": 1759897223108, "content": {"title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "abstract": "Effective coordination among artificial agents in dynamic and uncertain environments remains a significant challenge in multi-agent systems. Existing approaches, such as self-play and population-based methods, either generalize poorly to unseen partners or require impractically extensive fine-tuning. To overcome these limitations, we propose Coordination Transformers (CooT), a novel in-context coordination framework that uses recent interaction histories to rapidly adapt to unseen partners. Unlike prior approaches that primarily aim to diversify training partners, CooT explicitly focuses on adapting to new partner behaviors by predicting actions aligned with observed interactions. Trained on trajectories collected from diverse pairs of agents with complementary preferences, CooT quickly learns effective coordination strategies without explicit supervision or parameter updates. Across diverse coordination tasks in Overcooked, CooT consistently outperforms baselines including population-based approaches, gradient-based fine-tuning, and a Meta-RL-inspired contextual adaptation method. Notably, fine-tuning proves unstable and ineffective, while Meta-RL struggles to achieve reliable coordination. By contrast, CooT achieves stable, rapid in-context adaptation and is consistently ranked the most effective collaborator in human evaluations.", "tldr": "We present Coordination Transformers (CooT), an in-context learning framework that can efficiently and adaptively coordinate with unseen partners.", "keywords": ["In-Context Learning", "In-Context Reinforcement Learning", "Multi-Agent Coordination", "Human-AI Collaboration", "Transformers for Decision Making"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6cd61b49e2e7c669b4d3b0bcc1235d1402de57a9.pdf", "supplementary_material": "/attachment/9bfec178b2cdd593ac1eeb936571222e0f5dbed0.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces COOT, a novel in-context learning framework designed to enable agents to coordinate with previously unseen partners by conditioning their actions on recent interaction histories. COOT is trained on trajectories from diverse pairs of agents whose behavior is driven by hidden reward functions, framing coordination as a Hidden-Utility Markov Game (HU-MG) problem. In experiments using Overcooked, COOT consistently outperforms strong baselines (including population-based and gradient-based methods) across diverse coordination tasks, achieving stable and rapid adaptation without any parameter updates. Human evaluations also ranked COOT as the most effective and preferred collaborator, demonstrating its robustness against the variability of human partners."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- COOT consistently outperforms strong population-based baselines (HSP, MEP) across diverse, coordination-heavy Overcooked layouts, including the challenging multi-recipe variant.\n- The user study provides strong evidence that COOT's advantage extends to real-world variability.\n- The controlled partner-swap experiment was a creative way of demonstrating COOT’s capability to recalibrate its strategy to an abruptly changing partner"}, "weaknesses": {"value": "- If the partner you’re interacting with has a different hidden reward function then this is effectively a different MDP since they’ll take different actions making the ego agent’s transition dynamics different. Generating many diverse trajectories and training on them then seems exactly similar to [RUSP](https://arxiv.org/abs/2011.05373) or [CEC](https://arxiv.org/abs/2504.12714). The main novelty seems to come from using a transformer compared to LSTMs which that work uses, but even here that doesn’t seem like too big of an architectural contribution, and there’s no comparison to these models.\n- It is unclear whether this method addresses the zero shot or few shot coordination case, it seems to frame the method in the latter but the details of how it evaluates it in the former for both AI-AI comparison and Human-AI comparisons are not clear\n- For the qualitative metrics, did participants say anything about the other models that were similar to what they said about Coot? If so then this isn’t a strong signal about peoples’ experience with the models.\n- The computational costs of this method are not well addressed, despite efficiency being a core claim of this method\n- Only Overcooked is used as a baseline"}, "questions": {"value": "- The training methodology—generating diverse trajectories from agents with varying hidden reward functions and training a context-conditioned policy—appears functionally similar to approaches like RUSP and CEC. Given that the main architectural difference is the use of a Transformer over an LSTM, can the authors perform a direct quantitative comparison to those methods which also claimed SOTA?\n- Can the authors clarify whether this is a few-shot or zero-shot method and what the evaluation settings were done in?\n- Can the authors please provide a more granular breakdown of the qualitative feedback (e.g., coding and quantifying the frequency of terms like \"adapts\" vs. \"blocks\") to demonstrate that COOT’s perceived superiority is statistically or subjectively unique, rather than just the result of general human positivity towards the best-performing agent.\n- Given the complexity of the Transformer architecture, can the authors please provide a comprehensive analysis of the full computational cost, including the total memory footprint and the amortized cost of pre-training the expert best-response policies used to generate the large offline dataset? Does the initial cost of generating this dataset outweigh the fine-tuning cost of the baselines?\n- The entire evaluation is confined to the Overcooked environment. Given that coordination challenges are fundamentally domain-dependent, how do the authors think about the general applicability of COOT without demonstrating performance on a second, structurally different multi-agent coordination benchmark (e.g., a capture-the-flag setting, or a mixed-motive social dilemma)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IMZU8Egq6l", "forum": "ebe9uW2yQA", "replyto": "ebe9uW2yQA", "signatures": ["ICLR.cc/2026/Conference/Submission16717/Reviewer_Krp4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16717/Reviewer_Krp4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606149638, "cdate": 1761606149638, "tmdate": 1762926769198, "mdate": 1762926769198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Coordination Transformers (CooT), a framework for in-context coordination in multi-agent systems. CooT leverages interaction histories to infer and adapt to new partner behaviors on the fly. The model is trained on trajectories collected from diverse agents with different preferences, enabling it to learn implicit coordination strategies without explicit supervision or gradient updates at test time. Experiments in the Overcooked benchmark demonstrate that CooT achieves superior performance compared to population-based methods, gradient-based fine-tuning, and Meta-RL-style contextual adaptation, particularly in zero-shot coordination with unseen partners. The method also shows robustness and strong human compatibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is generally well written and easy to follow. The motivation and the problem formulation are clearly stated. The figures are clear, visually consistent, and effectively convey both the architecture and the experimental results.\n\n2. The paper provides an extensive and carefully designed ablation study. The authors include experiments on non-stationary or changing partners, adaptation over multiple interaction episodes. The ablation studies are detailed, demonstrating CooT's capability in partner adaptation.\n\n3. The Human–AI collaboration study assesses how well CooT collaborates with human partners compared to baseline methods. This shows the proposed framework is not only effective in simulated agent settings but also aligns well with human coordination patterns."}, "weaknesses": {"value": "1. **Insufficient literature review**:\nThe paper overlooks several highly relevant works in adaptive coordination and partner modeling, such as PACE [1], GSCU [2], and LIAM [3]. These works similarly address how to infer a partner’s latent policy or behavioral intent from interaction history and adapt one’s own policy accordingly. In particular, PACE directly studies peer adaptation with context-aware exploration, which is conceptually very close to the proposed “in-context coordination” problem formulation. The omission of these works weakens the paper’s positioning in the broader literature.\n\n2. **Limited contribution.** The core method of CooT, using a transformer-based policy to autoregressively predict next actions from recent interaction histories, follows a standard paradigm in decision-transformer and in-context RL literature. While the implementation appears solid, the conceptual innovation is limited. Moreover, the problem formulation and methodological scope of CooT appear to be a subset of PACE [1]. PACE not only addresses partner adaptation but also generalizes to opponent and mixed-motive adaptation, and introduces the notion of context acquisition via online exploration when informative histories are unavailable. None of these extensions are discussed or contrasted in this paper. Given that PACE explicitly frames and solves a broader problem under the same high-level theme of context-based coordination, the novelty of CooT remains unclear.\n\n3. **Narrow experimental scope**:\nThe evaluation is restricted to two-player coordination tasks in Overcooked, which does not fully demonstrate diversity. The method’s scalability to more complex multi-agent settings, including partially observable or mixed-motive environments, remains untested. For comparison, PACE evaluates on PO-Overcooked, which explicitly requires context acquisition through exploration, and also considers multi-agent competition and mixed-motive games involving more than two players. Without experiments in such diverse or challenging settings, it is difficult to judge how well CooT generalizes beyond cooperative, fully observable domains.\n\n4. **Missing comparison with LLM-based baselines**:\nThe paper does not include comparisons to LLM-based partner coordination methods, such as ProAgent [4], which leverage large language models’ reasoning and common-sense capabilities to infer partner intentions and generate cooperative responses. Given their strong zero-shot coordination abilities, they should serve as an important baseline for CooT. The lack of such comparisons limits the paper’s relevance to the broader research trend of foundation models for interactive decision-making.\n\n[1] Fast Peer Adaptation with Context-aware Exploration\n\n[2] Greedy when Sure and Conservative when Uncertain about the Opponents\n\n[3] Agent Modelling under Partial Observability for Deep Reinforcement Learning\n\n[4] ProAgent: Building Proactive Cooperative Agents with Large Language Models"}, "questions": {"value": "1. Could the authors clarify the specific conceptual or methodological contribution of CooT beyond existing works such as PACE, which already investigates in-context partner adaptation using interaction histories? Both frameworks appear to share a similar formulation—treating partner adaptation as a contextual adaptation problem where the policy conditions on recent interactions. From the current description, the main distinction seems to be the use of a transformer-based backbone, while PACE also explores transformer encoders in its appendix.\n\n2. Could the authors consider extending the experimental evaluation beyond the two-player cooperative Overcooked domain? Broader tests in partially observable, mixed-motive, or multi-agent (>2 player) environments would better demonstrate the generality and robustness of the proposed framework. \n\n3. Could the authors consider including comparisons with LLM-driven online adaptation methods, such as ProAgent? These approaches represent an increasingly relevant direction in partner modeling and human–AI coordination."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gpSyefPxEL", "forum": "ebe9uW2yQA", "replyto": "ebe9uW2yQA", "signatures": ["ICLR.cc/2026/Conference/Submission16717/Reviewer_MpAU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16717/Reviewer_MpAU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937936315, "cdate": 1761937936315, "tmdate": 1762926768752, "mdate": 1762926768752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method, Coot, for in-context adaptation with previously unseen teammates. The method first collects trajectories and then queries a BR \"expert\" to match the policy of the expert under a context. Experiments show that the method outperforms Meta-RL and gradient-based finetuning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The concept of in-context adaptation for multi-agent generalization is interesting. The method's relation to LLM is very interesting.\n\n(2) The way the experiments are set up makes sense to me"}, "weaknesses": {"value": "(1) First of all, ZSC (zero-shot coordination, by Treutlein et al., 2021) and AHT (ad-hoc teamwork, by Stone et al., 2010) are two different things. The paper falsely relates itself to ZSC, which refers to training the same algorithm to always converge to the same convention, rather than to AHT, which generalizes to previously unseen teammates.\n\n- Treutlein, J., Dennis, M., Oesterheld, C., & Foerster, J. (2021, July). A new formalism, method and open issues for zero-shot coordination. In International Conference on Machine Learning (pp. 10413-10423). PMLR.\n\n- Hu, H., Lerer, A., Peysakhovich, A., & Foerster, J. (2020, November). “other-play” for zero-shot coordination. In International Conference on Machine Learning (pp. 4399-4410). PMLR.\n\n(2) No repeated experimentation. The mean ± std is only over 50 rollouts of the same training trial, instead of being repeated multiple times.\n\n(3) Miss meta learning baselines like RL^2 and missing discussion Ad Hoc Teamwork works that use such meta-learning methods. Meta-learning methods like RL^2 using SSM or Transformers usually perform well in prior works on Ad Hoc Teamwork in 2019 Overcooked."}, "questions": {"value": "(1)(2)(3) see weakness\n\n(4) The BR is just the BR to the current unknown teammate, right? Then it may or may not be an optimal adaptation to teammates given the context. Can you prove it is optimal in a rigorous way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vOxgJEOreo", "forum": "ebe9uW2yQA", "replyto": "ebe9uW2yQA", "signatures": ["ICLR.cc/2026/Conference/Submission16717/Reviewer_79kx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16717/Reviewer_79kx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065313755, "cdate": 1762065313755, "tmdate": 1762926768393, "mdate": 1762926768393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CooT, a framework that learns context-driven coordination strategies aligned with a partner’s behavioral policy to maximize collaboration effectiveness. CooT is trained on trajectory pairs collected from interactions between behavior-preferring agents with hidden reward biases and their best-response counterparts. Through this data, CooT learns to infer hidden partner preferences from interaction context and to adapt its coordination policy within a few episodes, even when paired with previously unseen partners. The model maintains a fixed-length FIFO context buffer, ensuring that only recent trajectories are retained for adaptation. CooT is trained using a cross-entropy loss to imitate the best-response agent’s actions given the context of partner behaviors, without relying on gradient-based updates during deployment. Evaluation is conducted in the Overcooked environment across five layouts using the ZSC-Eval benchmark, which measures zero-shot coordination with unseen partners. Results show that CooT generally outperforms all baselines, including Behavior Cloning, MEP, HSP, fine-tuned HSP, and Meta-RL variants, particularly in complex coordination tasks. Furthermore, human-AI collaboration studies demonstrate that CooT achieves the highest mean score and top subjective ratings for adaptability and collaboration quality compared to all other methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper tackles an important and underexplored problem of partner-centric coordination rather than task-oriented learning. This focus makes the approach more realistic for real-world multi-agent and human-AI interaction settings, where uncertainty often arises from the partner’s changing behavior rather than from the task itself.\n2.\tCooT achieves few-shot adaptation without gradient updates, leveraging contextual information from recent interactions to adjust its policy online. This enables the agent to adapt efficiently to unseen partners at test time, making it practical for dynamic coordination scenarios.\n3.\tFine-tuning baselines underperform compared to CooT, highlighting that gradient-based adaptation methods are unstable and less effective when partner behavior shifts. This demonstrates the stability advantage of in-context adaptation.\n4.\tExperimental results show that CooT adapts quickly and robustly to partner behavior changes, typically recovering effective coordination within only a few (≈6) episodes after a new partner or strategy switch."}, "weaknesses": {"value": "1.\tWhile the paper covers most technical aspects, several implementation and evaluation details are not sufficiently explained in either the main text or supplementary material, requiring to refer to prior work. For example, the ZSC-Eval-based evaluation pipeline is only briefly described, with the similarity metric computation and partner selection process largely assumed from the original ZSC-Eval paper.\n2.\tThe paper lacks deeper analysis of the evaluation results in Table 1. For instance, it is unclear why MEP performs slightly better than CooT in the Coordination Ring layout but not in others. More discussion is needed on whether these differences arise from task complexity, coordination demand, or model design characteristics.\n3.\tSimilarly, it does not analyze why HSP and HSP-ft outperform CooT on Coordination Ring and Asymmetric Advantages layouts but not on others. Since HSP uses reward shaping, it should theoretically provide an upper bound on coordination performance. Why is this not observed across all tasks? Is this due to HSP’s reward design?\n4.\tProviding explicit details of the reward functions used for each Overcooked layout would improve clarity and help interpret the quantitative results. \n5.\tThe paper provides limited information about the specific event-based reward components used to train the behavior-preferring agents. Including this information would improve understanding of the underlying working of CooT and the baseline methods. \n6.\tThe paper lacks qualitative or visual demonstrations showing how CooT’s coordination behavior evolves over episodes as it adapts to partner policies. Without these, it is difficult to interpret how the model’s adaptation works temporally. \n7.\tIt is unclear whether the CooT agent dynamically switches between behaviors learned from different behavior-preferring agents within an episode or maintains one behavior per partner throughout. Qualitative analysis or visualizations could clarify this temporal aspect of adaptation.\n8.\tThe paper evaluates COOT only in the 2 agent Overcooked environment. Demonstrating its performance across different environments, especially those involving longer episodes or varied coordination structures, would show the context-based adaptation framework’s generalization limits and scalability beyond short, discrete cooking tasks. Additionally, it would be valuable to discuss how context-based adaptation might scale beyond dyadic coordination, outlining the potential challenges of extending the current framework to multi-agent settings, even if this is planned as future work."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TeUgit0ay3", "forum": "ebe9uW2yQA", "replyto": "ebe9uW2yQA", "signatures": ["ICLR.cc/2026/Conference/Submission16717/Reviewer_y8QZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16717/Reviewer_y8QZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission16717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762397702216, "cdate": 1762397702216, "tmdate": 1762926767904, "mdate": 1762926767904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}