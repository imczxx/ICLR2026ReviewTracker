{"id": "ym7L1by6iO", "number": 2515, "cdate": 1757130775971, "mdate": 1763039727641, "content": {"title": "EgoQuestions: Crafting Egocentric Questions for Egocentric Video Question Answering", "abstract": "A thorough understanding of models’ egocentric capabilities is crucial for robotics, autonomous driving, smart glasses, etc.\nEgocentric VideoQA aims to assess models’ understanding of first‑person videos, \nbut existing benchmarks often include questions that do not reliably probe recorder‑centric reasoning. \nUsing these datasets to train and evaluate models can obscure true model capabilities and reduce the value of curated egocentric data.\nTo address this, we define egocentric questions and propose three clear principles: a question should focus on the video recorder and their activities;\nit must avoid shortcut cues that allow answers via generic scene or action recognition (e.g., simultaneously naming an action and its object); while intentions and attributes may serve as shortcuts for actions and objects, those that require understanding of the recorder’s perspective will not. \nGuided by these principles, we build a checking pipeline to filter existing QA pairs and a crafting pipeline to generate valid egocentric questions. We release EgoQuestions, a benchmark of 2,500 curated egocentric QA instances created with our pipeline, and evaluate several proprietary and open‑source VLMs. \nResults reveal substantial room for improvement in current models’ egocentric capabilities and a clear performance gap  (about 10%) between egocentric questions that adhere to our principles and flawed alternatives, \ndemonstrating existing egocentric benchmarks tend to overrate models’ first-person capabilities,\nand the need for rigorously designed egocentric benchmarks\nto more accurately assess models’ first-person vision capabilities.", "tldr": "", "keywords": ["Egocentric Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/da1642200236a767ca2d63afab12b01a87b546b9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to reflect the challenges of egocentric VideoQA from the question side. It argues that existing egocentric VideoQA datasets are not 100% egocentric, and emphasizes a subset of questions focusing on the camera wearers which cannot be answered with generic visual understanding. The authors design three principles to curate more challenging egocentric questions, and conduct experiments to show that there is a huge gap between existing VLMs in answering generic questions about egocentric visual scenes and answering true egocentric questions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper delves into existing egocentric VideoQA datasets and resolves non-egocentric short-cuts, thus exposing the true challenge of answering egocentric questions.\n2.\tIt designs 3 principles to generate true egocentric questions, and constructs a related dataset EgoQuestion.\n3.\tThe analyses show that EgoQuestion is harder to answer than generic questions about egocentric visual scenes."}, "weaknesses": {"value": "1.\tWhile the paper identifies an interesting problem, I do not quite agree that egocentric understanding should be restricted to such narrowly defined questions. The core of egocentric understanding lies in its visual perspective. Therefore, any question grounded in an egocentric visual scene should be considered relevant to egocentric understanding.\n\n2.\tThe three principles are also problematic. For the first principle (P1), it is hard to justify what questions are related to the recorder themselves. In my opinion, it depends on specific situations and user intentions. For example, all questions seeking for egocentric assistance should be considered pertaining to the video recorders. As for the second principle (P2), it seems more focused on reducing contextual cues to increase task difficulty, rather than reflecting the essence of egocentric understanding. Principle 3 (P3) is rather vague, I feel hard to grasp how it helps to shape the question in EgoQuestion. \n\n3.\tSome recent egocentric QA datasets (EgoLife-QA, EgoBlind) that target at egocentric visual assistance have been ignored. According to the 3 principles, I feel hard to be convinced the distinct contribution of EgoQuestion for egocentric understanding compared with these datasets\n\n4.\tEgoQuestion’s QAs are automatically generated using LLM and code template. Without human participation, the quality and naturalness of the generated QAs could be questioned. It would be better to attach the annotations or provide some example QAs for understanding. Also, the dataset is relatively small (260 videos and 2500 questions) compared with other egocentric VideoQA datasets.\n\n5.\tThe experiments and analyses so far are insufficient. The current experiments only demonstrate that the curated questions are more challenging to answer, but do not investigate or suggest directions for solving the challenge. For example, is the performance degradation because of a data shift between model developing and testing? Could crafting similar questions for training improve model performance?"}, "questions": {"value": "1.\tWhat do SOAP and SPAO mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hOlbco9DQk", "forum": "ym7L1by6iO", "replyto": "ym7L1by6iO", "signatures": ["ICLR.cc/2026/Conference/Submission2515/Reviewer_prCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2515/Reviewer_prCC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761377783224, "cdate": 1761377783224, "tmdate": 1762916263594, "mdate": 1762916263594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Thank you for the reviews"}, "comment": {"value": "We sincerely thank all the reviewers for the insightful feedback. We will carefully address your comments and improve the quality of our work accordingly.\n\nWe wish you all the best in your future research."}}, "id": "xjbBSXl9XM", "forum": "ym7L1by6iO", "replyto": "ym7L1by6iO", "signatures": ["ICLR.cc/2026/Conference/Submission2515/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2515/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2515/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763039647585, "cdate": 1763039647585, "tmdate": 1763039647585, "mdate": 1763039647585, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "9trzAMFImy", "forum": "ym7L1by6iO", "replyto": "ym7L1by6iO", "signatures": ["ICLR.cc/2026/Conference/Submission2515/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2515/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763039726784, "cdate": 1763039726784, "tmdate": 1763039726784, "mdate": 1763039726784, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper articulates three principles that define what constitutes an egocentric question, presents a semi-automatic pipeline for constructing egocentric QA pairs, and releases EgoQuestions, a curated question set derived from Ego4D and HD-Epic. The authors evaluate a range of off-the-shelf VLMs on this benchmark and demonstrate a consistent performance gap that underscores how question formulation can materially affect assessments of egocentric understanding. However, because the dataset primarily rephrases existing video content rather than adding new data, and no fine-tuning experiments are provided to demonstrate training utility, its contribution remains largely diagnostic and does not clearly advance model capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed pipeline is a pragmatic compromise between rule-based control and LLM flexibility. The design is reproducible and the figures/flowcharts in the paper make the procedure clear.\n2. By holding video and answer fixed and only varying question wording/structure, the paired design metric (i.e. T1 vs F2) show that question formulation can materially inflate model scores."}, "weaknesses": {"value": "1. Table 3 reports off-the-shelf model inference performance on two question formats (T1 vs F2). This demonstrates that evaluation is sensitive to question phrasing, but it does not establish that EgoQuestions as training data improves model performance. \n\n\n2. EgoQuestions is derived from Ego4D and HD-Epic. Several evaluated large models may have been pre-trained on data overlapping with Ego4D/ HD-Epic text or video metadata. The paper does not document whether evaluated models’ training/pretraining corpora could include these sources, nor does it provide a decontamination strategy.\n\n3. The authors use a capable LLM to score answers, but do not report how well this automated scoring correlates with human judgments."}, "questions": {"value": "1. Is EgoQuestions valuable beyond evaluation/diagnostics? Please provide training/fine-tuning experiments or clearly position the dataset as an evaluation/diagnostic resource only.\n\n2. Were the evaluated large models pretrained on Ego4D or HD-EPIC, the sources from which EgoQuestions was drawn?\n\n\n3. What is the agreement between the LLM judge and human annotators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8RnKzXPmB6", "forum": "ym7L1by6iO", "replyto": "ym7L1by6iO", "signatures": ["ICLR.cc/2026/Conference/Submission2515/Reviewer_8u9x"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2515/Reviewer_8u9x"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615359272, "cdate": 1761615359272, "tmdate": 1762916263192, "mdate": 1762916263192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an Egocentric Video QA benchmark. The authors say that many questions in current datasets don’t really test whether a model understands the video from the recorder’s point of view. Instead, models can often guess the answer using easy “shortcuts,” like recognizing the general scene or common actions. This gives a false impression that Vision-Language Models (VLMs) are better at first-person understanding than they actually are.\n\nTo fix this, the authors make three main contributions: Defining good egocentric questions, creating a question-generation pipeline, and building the EgoQuestions Benchmark. Using this pipeline, they produce EgoQuestions, a benchmark containing 2,500 carefully made question–answer pairs.\n\nThey then test several Vision-Language Models on both the new and existing datasets. The results show that current models perform much worse on the new, stricter questions. There’s about a 10% performance drop compared to the older, easier benchmarks. This confirms that existing benchmarks overestimate how well current models truly understand first-person videos."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides insights about how to better build egocentric video understanding benchmarks by eliminating the shortcut in questions."}, "weaknesses": {"value": "- Limited Question Scope: The benchmark does not explicitly assess more complex reasoning abilities, such as those involving long-term intentions, social dynamics, or the causal consequences of the recorder’s actions.\n\n- Reliance on LLM-as-Judge: While using an LLM to evaluate open-ended responses is a common practice, it remains imperfect. The observed 10% $\\Delta_{acc}$ performance gap might partly result from the judge model itself finding the F2 questions (which include shortcuts) easier to assess. Incorporating more reliable multiple-choice questions into the benchmark is recommended.\n\n- Writing Quality: The paper’s writing requires improvement, as several key concepts—such as the SPO, SOAP, and SPAO subsets—are insufficiently defined.\n\n- Insufficient Model Coverage: The number of evaluated models is too limited for a benchmark paper, which reduces the generality and robustness of the findings.\n\n- Missing qualitative study: The qualitative analysis of benchmark data and wrong answers by different models is missing."}, "questions": {"value": "- The definitions of the data subsets—“SPO (because we provide the Subject and Predicate in the question while inquiring about the Object), SOAP, and SPAO”—are unclear. The distinctions among these subsets are not well explained, and it is not evident why they differ. This section requires significant clarification and improvement in writing quality.\n\n- Did you perform any human validation on a subset of the LLM-as-judge's scores? How can we be confident that the judge itself isn't systematically scoring the \"flawed\" F2 questions more leniently, thereby contributing to the inflated gap? \n\n- The current question templates (SPO, SOAP, SPAO) are focused on grounding actions and objects. Do you believe your three principles and crafting pipeline could be extended to generate more complex, abstract egocentric questions, such as those reasoning about the recorder's intentions (\"Why am I lifting the pan?\") or beliefs?\n\n- Could you provide significantly more qualitative examples of your benchmark data?\n\n- How do the SOTA models fail on your T1 questions? Could you analyze some examples? For instance, when asked an SPO question (\"What am I picking up?\"), do models tend to name a plausible but incorrect object from the scene, hallucinate an object, or simply state they don't know?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7rNkgreBAe", "forum": "ym7L1by6iO", "replyto": "ym7L1by6iO", "signatures": ["ICLR.cc/2026/Conference/Submission2515/Reviewer_3o3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2515/Reviewer_3o3B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761627997729, "cdate": 1761627997729, "tmdate": 1762916262835, "mdate": 1762916262835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper critiques existing egocentric Video Question Answering (VideoQA) benchmarks, arguing that many of their questions are not genuinely \"egocentric\" and can be answered using simple scene or action recognition shortcuts. The authors propose three principles to define what constitutes a valid egocentric question: it must concern the video recorder, avoid obvious shortcuts, and can involve the recorder's intentions.\nBased on these principles, the authors develop a complex, multi-stage pipeline that uses Large Language Models (LLMs) to parse narrations from existing video datasets (Ego4D, HD-Epic) into grammatical components. These components are then reassembled into new questions using code templates. The result is a new benchmark of 2,500 QA pairs, named EgoQuestions. The paper evaluates several Vision-Language Models (VLMs) on this new benchmark and on a set of \"flawed\" counterpart questions, demonstrating that model performance is substantially lower on their rigorously crafted questions. This suggests that previous benchmarks may overestimate the true egocentric reasoning capabilities of current models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Important Problem Identification: The paper correctly identifies and thoroughly diagnoses a subtle but important flaw in existing egocentric VideoQA benchmarks. The analysis of how shortcuts can lead to an overestimation of model capabilities is a valuable contribution to the community.\n\n2. Clear Conceptual Framework: The three principles proposed for defining an egocentric question are intuitive, well-reasoned, and provide a useful conceptual vocabulary for future benchmark design. This formalization is a key strength of the paper.\n\n3. Analysis of Existing Benchmarks: The quantitative analysis in Table 1, showing that a very low percentage of questions in popular benchmarks adhere to their principles (e.g., ~10-11% for EgoTempo and EgoTextVQA), provides strong empirical evidence for the paper's central claims."}, "weaknesses": {"value": "1. Limited Scope of Contribution: The paper is primarily a dataset paper. While dataset contributions are valuable, this one is accompanied by neither a novel model nor a new learning technique. Its main finding is that \"harder questions are harder,\" which, while important to demonstrate, may not be seen as a sufficiently impactful research finding on its own.\n\n2. The evaluation of open-ended answers relies on an LLM-as-a-judge. Could the authors provide a more extensive comparison between their LLM judge and human annotators (beyond a simple accuracy metric) to better understand its biases and reliability? For instance, what is the inter-rater reliability (e.g., Cohen's Kappa) between the LLM and human judges on a representative subset of the data? This would add much-needed credibility to the results in Table 3. \n\n3. Given the multi-step LLM pipeline for parsing grammar and refining questions, what is the estimated end-to-end success rate of the generation process? How many initial narrations result in flawed or discarded questions due to cascading errors in the pipeline, and what measures are in place to ensure the final 2,500 questions are free of subtle artifacts introduced by this process? \n\n4. Principle 2 aims to avoid shortcuts by, for example, not simultaneously naming an action and an object. This is handled by parsing grammar. However, shortcuts can be semantic rather than purely grammatical. How do the authors ensure their automated pipeline robustly distinguishes between a helpful contextual clue and an undesirable shortcut that a model could exploit?"}, "questions": {"value": "This paper has a strong motivation and a thoughtful conceptual framework for improving egocentric VideoQA. The diagnosis of the problem is excellent. However, the work is let down by significant methodological concerns in both its data generation and, most critically, its evaluation protocol. The reliance on an LLM-as-a-judge to score open-ended answers is a major weakness that undermines the paper's goal of bringing more rigor to the field.  Hope the authors could address the questions in the above sections."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wT9C50zeHl", "forum": "ym7L1by6iO", "replyto": "ym7L1by6iO", "signatures": ["ICLR.cc/2026/Conference/Submission2515/Reviewer_cYqD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2515/Reviewer_cYqD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2515/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148767571, "cdate": 1762148767571, "tmdate": 1762916262577, "mdate": 1762916262577, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}