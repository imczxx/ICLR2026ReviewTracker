{"id": "zPv46YKv3w", "number": 3514, "cdate": 1757455284148, "mdate": 1759898083634, "content": {"title": "OWL : Geometry-Aware Spatial Reasoning for Audio Large Language Models", "abstract": "Spatial reasoning is fundamental to auditory perception, yet current audio large\nlanguage models (ALLMs) largely rely on unstructured binaural cues and single-\nstep inference. This limits both perceptual accuracy in direction and distance\nestimation and the capacity for interpretable reasoning. Recent work such as BAT\ndemonstrates spatial QA with binaural audio, but its reliance on coarse categorical\nlabels (left, right, up, down) and the absence of explicit geometric supervision\nconstrain resolution and robustness. We introduce the $\\textbf{Spatial-Acoustic Geometry\nEncoder (SAGE}$), a geometry-aware audio encoder that aligns binaural acoustic\nfeatures with 3D spatial structure using panoramic depth images and room-impulse\nresponses at training time, while requiring only audio at inference. Building on this\nrepresentation, we present $\\textbf{OWL}$, an ALLM that integrates $\\textbf{SAGE}$ with a spatially\ngrounded chain-of-thought to rationalize over direction-of-arrivals (DoA) and\ndistance estimates. Through curriculum learning from perceptual QA to multi-step\nreasoning, $\\textbf{OWL}$ supports o’clock-level azimuth and DoA\nestimation. To enable large-scale training and evaluation, we construct and release $\\textbf{BiDepth}$,\na dataset of over one million QA pairs combining binaural audio with panoramic\ndepth images and room impulse responses across both in-room and out-of-room scenarios. Across two benchmark datasets, our new $\\textbf{BiDepth}$ and the public SpatialSoundQA, $\\textbf{OWL}$ reduces mean DoA error by $\\textbf{11$^{\\circ}$}$ through $\\textbf{SAGE}$\nand improves spatial reasoning QA accuracy by up to $\\textbf{25}$% over BAT. Our dataset and code are available at: https://anonymous.4open.science/r/OWL-ICLR-26/", "tldr": "", "keywords": ["Chain of Thoughts", "Spatial Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a425a423f70a9ac88cca0effd6c65765b4d14cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel framework for spatial audio understanding, marked by three core contributions. First, it presents BiDepth, a new large-scale synthetic dataset for spatial audio tasks. Second, it proposes SAGE, a novel spatial audio encoder designed to align binaural acoustic features with 3D spatial structure. Finally, it introduces OWL, an audio LLM that utilizes the SAGE encoder to extract and process acoustic and spatial features. The integrated OWL model is shown to achieve superior performance on both the proposed BiDepth dataset and the existing SpatialSound-QA benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The development of the BiDepth dataset is a significant contribution to the field. A large-scale, well-structured dataset for spatial audio understanding can facilitate further research.\n- The SAGE encoder is also a good design."}, "weaknesses": {"value": "- Lack of Evaluation on Real-World Data. The model's performance is exclusively validated on simulated data. While the results are encouraging, models trained solely on synthetic data often fail to generalize to real-world scenarios due to the domain gap. The paper would be significantly strengthened by testing the model's robustness and transferability on real-world datasets, such as those from the DCASE Challenge on spatial audio tasks."}, "questions": {"value": "- What is the model's performance on real-world data? For example, on the DCASE Challenge?\n- In the results presented in Table 2, the model using Chain-of-Thought (CoT) reasoning shows improved performance on Type I and Type II tasks. This is counterintuitive, as these tasks do not seem to require complex reasoning. Why CoT is beneficial for these tasks?\n- What is the performance of the SAGE encoder for general audio perception tasks? Besides, what are the advantages of a unified encoder like SAGE, which models audio and spatial information jointly, compared to a multi-encoder architecture that models them separately? For instance, [1] demonstrates success with separate modeling. Have you experimented with combining a state-of-the-art audio encoder with the SAGE encoder?\n\n[1] Tang, C.,et al. Can Large Language Models Understand Spatial Audio? Proc. Interspeech 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y4Tf0j2BjL", "forum": "zPv46YKv3w", "replyto": "zPv46YKv3w", "signatures": ["ICLR.cc/2026/Conference/Submission3514/Reviewer_V2cc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3514/Reviewer_V2cc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918481397, "cdate": 1761918481397, "tmdate": 1762916777986, "mdate": 1762916777986, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SAGE (Spatial-Acoustic Geometry Encoder), a geometry-aware audio encoder, and OWL, an audio large language model (ALLM) that integrates SAGE with a chain-of-thought (CoT) spatial reasoning mechanism for auditory tasks. To support this approach, the authors curate BiDepth, a large-scale simulated dataset of over 1.1 million question-answer (QA) pairs, combining binaural audio, room impulse responses (RIR), panoramic depth images, and spatially grounded QA annotations. SAGE is supervised with paired geometric and acoustic data during training (but requires only audio at inference), and OWL builds on SAGE to enable structured spatial reasoning with interpretable rationales. Experiments across both new (BiDepth) and existing (SpatialSoundQA) datasets show consistent improvements in direction-of-arrival (DoA) estimation, spatial QA, and multi-step reasoning compared to prior work."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The introduction of SAGE, trained with auxiliary geometric supervision (via depth/RIR) but requiring only audio at inference, effectively bridges a key gap in current multimodal audio-language models (ALLMs), which often lack geometric grounding.\n\n2. BiDepth represents a substantial, balanced, and diverse resource for training and evaluating geometry-aware audio-language systems.\n\n3. Quantitative results demonstrate that SAGE and OWL outperform strong baselines (such as BAT and Spatial-AST) on both event detection and fine-grained DoA and spatial reasoning accuracy."}, "weaknesses": {"value": "1. Reliance on synthetic data limits real-world generalizability.\n2. The BiDepth dataset’s elevation coverage is heavily skewed toward the horizontal plane (Figure 5); this is realistic for indoor sources but may result in models struggling with uncommon vertical positions."}, "questions": {"value": "1. Given the elevation bias displayed in Figure 5, how severe is OWL/SAGE’s performance degradation for sources at highly non-horizontal elevations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BXVmb0fh0c", "forum": "zPv46YKv3w", "replyto": "zPv46YKv3w", "signatures": ["ICLR.cc/2026/Conference/Submission3514/Reviewer_tQDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3514/Reviewer_tQDg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955055787, "cdate": 1761955055787, "tmdate": 1762916776236, "mdate": 1762916776236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces OWL, an ALLM for geometry-aware spatial reasoning. It integrates an audio encoder with a spatially grounded CoT, enabling it to perform multi-step reasoning over direction and distance estimates. A large-scale dataset, coined BiDepth, is constructed to power this framework.\n- The Spatial-Acoustic Geometry Encoder is trained to align binaural acoustic features with 3D spatial structure using panoramic depth images and simulated RIRs.\n- Evaluated on BiDepth and the public SpatialSoundQA dataset, OWL demonstrates state-of-the-art SELD performance and spatial reasoning QA accuracy compared to major baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This work provides a comprehensive end-to-end framework for geometry-aware spatial reasoning, addressing all key components from the SAGE encoder and the OWL LLM to the specialized BiDepth training data.\n- The paper introduces BiDepth, the first large-scale dataset of its kind, and its construction is sufficiently detailed in the appendix.\n- The framework demonstrates significantly superior performance against both open-source and closed-source baselines, supported by extensive experimental validation and ablation studies."}, "weaknesses": {"value": "- My main concern lies in the complexity and potential over-fitting of the proposed training pipeline. The framework requires a two-stage SAGE encoder pre-training, followed by a three-stage curriculum for the OWL model. While the ablation studies confirm this complex, multi-step process is necessary for the reported performance, it gives a strong impression of being tightly tailored to the custom-built BiDepth dataset. This raises significant questions about whether the approach is generally applicable or a highly specific solution.\n- The contribution of using panoramic depth maps feels overstated, as this is a standard modality provided by the underlying SoundSpaces and Matterport simulation platforms. This is further confused by diagrams (Fig. 2) that imply the presence of specific sounding objects in depth maps (e.g., a cat), but the methodology does not discuss augmenting the 3D scenes with such new visual assets, only placing sound sources at coordinates.\n- The design choice of dividing azimuth into 12 \"o'clock\" sectors feels ad-hoc and is not justified against other discretization schemes or as an established practice."}, "questions": {"value": "Please refer to the weaknesses for detail. Almost all citations in the main draft are ill-formatted, making it quite uncomfortable to read."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KxOoB1JTW4", "forum": "zPv46YKv3w", "replyto": "zPv46YKv3w", "signatures": ["ICLR.cc/2026/Conference/Submission3514/Reviewer_2wCv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3514/Reviewer_2wCv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762271740262, "cdate": 1762271740262, "tmdate": 1762916775912, "mdate": 1762916775912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}