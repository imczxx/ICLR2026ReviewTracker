{"id": "z9HUMon2Br", "number": 705, "cdate": 1756774650324, "mdate": 1759898246241, "content": {"title": "Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large Language Models", "abstract": "Large language models (LLMs) have demonstrated impressive performance across various language tasks. However, existing LLM reasoning strategies mainly rely on the LLM itself with fast or slow mode (like o1 thinking) and thus struggle to balance reasoning efficiency and accuracy across queries of varying difficulties. In this paper, we propose Cognitive-Inspired Elastic Reasoning (CogER), a framework inspired by human hierarchical reasoning that dynamically selects the most suitable reasoning strategy for each query. Specifically, CogER first assesses the complexity of incoming queries and assigns them to one of several predefined levels, each corresponding to a tailored processing strategy, thereby addressing the challenge of unobservable query difficulty. To achieve automatic strategy selection, we model the process as a Markov Decision Process and train a CogER-Agent using reinforcement learning. The agent is guided by a reward function that balances solution quality and computational cost, ensuring resource-efficient reasoning. Moreover, for queries requiring external tools, we introduce Cognitive Tool-Assisted Reasoning, which enables the LLM to autonomously invoke external tools within its chain-of-thought. Extensive experiments demonstrate that CogER outperforms state‑of‑the‑art Test‑Time scaling methods, achieving at least a 13% relative improvement in average exact match on In‑Domain tasks and an 8% relative gain on Out‑of‑Domain tasks.", "tldr": "We propose the  Cognitive-Inspired Elastic Reasoning framework for efficient scaling of language model reasoning, which dynamically selects the most appropriate processing mode for each query.", "keywords": ["Large language models; Reinforcement Learning; Markov decision process"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3cd4b2bfacd97ce407486ddea7cde57f66818a79.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a dynamic reasoning framework called CogER (Cognitive-Inspired Elastic Reasoning), which aims to address the inefficiency in LLM reasoning caused by the one-size-fits-all strategy. The paper divides query tasks into four levels (L1-L4) based on cognitive complexity. Subsequently, the paper uses RL to train a \"CogER-Agent.\" For effective training, the paper designs a composite reward function that includes a \"Hierarchical-Aware Reward,\" which penalizes \"overthinking\" (i.e., using a more complex strategy than the required level). The experimental results show that CogER significantly reduces latency while achieving higher accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a novel classification perspective and a new method for understanding problem difficulty.\n\n2. Experiments were conducted on multiple In-domain and Out-of-Domain datasets, and detailed ablation studies were also included."}, "weaknesses": {"value": "1. Regarding \"the minimal level required for a given query,\" the paper does not provide a reasonable explanation. How is this obtained? If the question-solving ability is not stable (e.g., L2 might occasionally be able to solve it correctly), how to handle this? I believe this is a crucial point of the paper, but it is not discussed.\n\n2. There is confusion between the cognitive hierarchy and tool requirements. Problem complexity and the need for tools are two orthogonal attributes. A very complex mathematical calculation (e.g., 9-digit multiplication) might be solvable with \"L1+tool,\" so I do not fully understand the rationale for L4 and designing the CoTool component. Its contribution to the main theme does not seem sufficient."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u5ewQKGIiv", "forum": "z9HUMon2Br", "replyto": "z9HUMon2Br", "signatures": ["ICLR.cc/2026/Conference/Submission705/Reviewer_Bhkw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission705/Reviewer_Bhkw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775191656, "cdate": 1761775191656, "tmdate": 1762915585557, "mdate": 1762915585557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CogER, a framework to address the inefficiency of uniform LLM reasoning. It dynamically allocates computation by classifying queries into four complexity levels (L1-L4), inspired by cognitive science. An RL-trained \"CogER-Agent\" routes queries to one of four corresponding strategies. A composite reward function is designed to balance accuracy and computational cost. Experiments show CogER achieves SOTA accuracy on various reasoning benchmarks while dramatically reducing end-to-end latency compared to strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  Clear Motivation & Principled Design: The paper clearly targets the \"one-size-fits-all\" inefficiency. The 4-level hierarchy derived from cognitive science (Bloom's Taxonomy) provides a logical and well-founded structure.\n2.  Strong Experimental Results: CogER achieves SOTA accuracy on both ID and OOD tasks. The efficiency gains are significant, demonstrating, for example, over 4x lower latency than the top-performing DeepSeek-R1 baseline.\n3.  Effective Reward Function: The composite reward is well-designed. The $\\mathcal{R}_{hierarchy}$ component, which penalizes \"overthinking,\" is proven critical by the ablation study (Table 4) for preventing the agent from defaulting to the most expensive strategy."}, "weaknesses": {"value": "1.  Lack of Controlled Routing Overhead Analysis: The paper does not quantify the specific latency overhead in the controlled environment. This makes it difficult to ascertain the precise efficiency trade-off, especially for simple L1 queries where the router's cost may be non-trivial.\n\n2.  Absence of Error Analysis: There is no breakdown analysis of the framework's failure cases. It is unclear whether errors stem from (1) the agent's incorrect routing or (2) the execution module's failure despite correct routing. This analysis is needed to understand the model's limitations."}, "questions": {"value": "1.  What is the CogER-Agent's \"routing accuracy\" in terms of predicting the minimal sufficient level ($L_{min}$)? The classifier might be wrong. Relatedly, what is the \"oracle\" accuracy of the CogER framework, assuming a 100% perfect router?\n\n2.  Could the authors provide the accuracy within each dynamically routed level? (i.e., for the set of all queries the agent routed to L1, what was the accuracy? And similarly for L2, L3, and L4?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5UIWnhP76", "forum": "z9HUMon2Br", "replyto": "z9HUMon2Br", "signatures": ["ICLR.cc/2026/Conference/Submission705/Reviewer_Y9xi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission705/Reviewer_Y9xi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839636693, "cdate": 1761839636693, "tmdate": 1762915585259, "mdate": 1762915585259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses inefficiencies in using fixed reasoning strategies in LLMs. It introduces CogER, a cognitive-inspired framework that dynamically matches query complexity with adaptive reasoning modes, ranging from direct answers to tool-assisted reasoning. Experimental results on various datasets demonstrate that CogER improves accuracy, reduces latency, and decreases token generation compared to fixed strategies and scaling-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes categorizing queries by complexity into different reasoning modes, including direct answer, concise CoT, extended CoT, and tool-assisted reasoning. This method provides a novel mechanism to balance accuracy and computational efficiency.\n\n- The approach demonstrates significant improvements across multiple benchmarks, achieving notable gains in accuracy, efficiency, and reduced latency compared to standard fixed or scaling-based strategies.\n\n- By drawing inspiration from cognitive models, the paper introduces hierarchical reasoning strategies that align with human reasoning complexity. This innovative approach improves interpretability and realism of model reasoning."}, "weaknesses": {"value": "- The reward depends on $L_{\\min}(\\mathcal{S})$, the minimal sufficient level, but the paper does not explain how this unobservable quantity is obtained during training or evaluation.\n\n- It is not clear how to handle tool errors and prompt injection, and how to avoid gaming of the format reward by printing tags without real gains.\n\n- The MDP action space mixes high-level actions with the token vocabulary $\\mathcal{V}$. It would be beneficial if the authors could further explain how actions are masked or factorised."}, "questions": {"value": "Please check the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4SnFgzFAYz", "forum": "z9HUMon2Br", "replyto": "z9HUMon2Br", "signatures": ["ICLR.cc/2026/Conference/Submission705/Reviewer_EgK3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission705/Reviewer_EgK3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922896352, "cdate": 1761922896352, "tmdate": 1762915585042, "mdate": 1762915585042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cognitive-Inspired Elastic Reasoning (CogER), a framework for dynamically allocating computational resources in LLM reasoning based on query complexity. The authors classify queries into four complexity levels (L1-L4) inspired by Bloom's Taxonomy, train a 7B CogER-Agent using reinforcement learning to route queries to appropriate processing modes (ranging from direct answering to tool-augmented reasoning), and introduce Cognitive Tool-Assisted Reasoning (CoTool) for L4 queries. Experiments on mathematical reasoning and commonsense QA tasks show improvements over test-time scaling baselines with reduced computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Well-Motivated Problem**: Dynamic resource allocation in test-time compute is an important practical challenge. The paper clearly articulates the inefficiency of one-size-fits-all reasoning strategies.\n\n2. **Comprehensive System Design**: The framework includes multiple well-integrated components: complexity classification, MDP formulation, specialized reward functions (particularly R_hierarchy for cost-awareness), and CoTool for tool integration. Algorithm 1 provides clear implementation guidance.\n\n3. **Solid Experimental Results**: Achieves competitive accuracy (89.28% on ID tasks) with significant efficiency gains (118.53s average latency vs. 506.19s for DeepSeek-R1). The OOD evaluation on MAWPS and CollegeMath demonstrates some generalization capability.\n\n4. **Detailed Reproducibility Information**: Appendix B provides complete prompts, Appendix D specifies RSTKit tools, and implementation details (hyperparameters, training procedures) are thoroughly documented.\n\n5. **Ablation Studies**: Tables 2-5 examine the contribution of different components (routing levels, reward terms, CoTool), providing insights into what drives performance."}, "weaknesses": {"value": "## 1. Inadequate Baseline Selection\n\n**Missing critical routing baselines**: The paper omits comparisons to directly relevant work:\n- **RouteLLM (ICLR 2025)**: Uses preference-based training for LLM routing with similar objectives\n\n**Unfair comparisons**:\n- DeepSeek-R1 is a closed-source 671B model tested under unknown conditions; should compare against open DeepSeek-R1-Distill (7B/14B/32B)\n- No iso-compute baseline: should compare \"always QwQ-32B\" with same average compute budget as CogER\n\n## 2. Narrow Dataset Selection\n\n**Limited task coverage**: Evaluation restricted to math reasoning and QA. Missing:\n- Code generation (HumanEval, MBPP)\n- Long-context reasoning (QuALITY, NarrativeQA)\n- Multi-turn dialogue (MT-Bench)\n- Factual QA with retrieval (Natural Questions, TriviaQA)\n- Multimodal tasks (ScienceQA, MMMU)\n\n**Train-test contamination risk**: Training uses samples from GSM8K/MATH/CommonsenseQA, then evaluates on same benchmark test sets. Router may learn dataset artifacts rather than complexity.\n\n**Weak OOD evaluation**: Only 2 OOD datasets (MAWPS, CollegeMath), both still mathematical. Needs different task types.\n\n**Small scale**: Only 8K training samples total; no ablation showing this suffices.\n\n## 3. The L_min Determination Black Box\n\n**Core methodology undefined**: Equation (6) depends on L_min(S) but never explains how these labels are obtained for 8,000 training samples. Possible methods, both problematic:\n\n- **Human annotation**: No guidelines, inter-annotator agreement, or subjectivity handling reported\n\n- **Empirical testing**: How is stochasticity handled? (Same query may succeed/fail across runs). What success threshold? (50%? 80%? 100%?). Circular dependency: need models to determine L_min for training those models.\n\n## 4. Router Capability Paradox\n\n**Weak-model-judging-strong-model**: 7B router must assess whether queries require 32B models or tools—but how can it judge capabilities beyond its own? Missing:\n- Analysis of how 7B learns 32B/QwQ capability boundaries from binary success signals alone\n- Routing accuracy stratified by true complexity (likely degrades for hard queries where accurate routing matters most)\n- Router size ablation (1.8B/7B/14B/32B)\n\n**Table 4 evidence**: Without R_hierarchy, 88.46% of queries route to L4. This reveals risk-aversion (\"when uncertain, pick strongest\") rather than genuine complexity understanding. R_hierarchy forces cheaper selections via penalty, not learned understanding.\n\n**Training signal insufficiency**: GRPO provides only correct/incorrect feedback, never showing how stronger models reason. Compare to RouteLLM's preference-based training that explicitly teaches capability differences.\n\n**No routing accuracy validation**: Table 4 shows level distributions but never reports:\n- Confusion matrix vs. ground truth L_min\n- Per-level precision/recall\n- Error decomposition (routing mistakes vs. execution failures)\n\nThis is the fundamental metric for any routing system—its absence is critical."}, "questions": {"value": "## Q1: L_min Label Generation Methodology\n\n Please provide a complete, step-by-step specification of how L_min labels are generated for the 8,000 training samples:\n\n- What is the exact procedure? (Human annotation / Empirical testing / Heuristic rules?)\n\nL_min is the core supervision signal and the method cannot be reproduced without this information.\n\n## Q2: Routing Accuracy Validation\n\nTable 4 reports level assignment distributions (2% / 28.17% / 21.90% / 47.93%) but never validates routing correctness. Please provide:\n\n- **Confusion matrix**: Predicted level vs. ground truth L_min for test set\n- **Per-level metrics**: Precision, recall, and F1 for L1/L2/L3/L4 classification\n\nWithout these metrics, we cannot assess whether the system truly learns complexity classification or succeeds through other factors (ensemble effects, tool usage).\n\n## Q3: Router Capability Boundary Learning\n\nThe 7B router must predict whether queries require 32B/QwQ models, yet only receives binary success signals. Please address:\n\n- **Learning mechanism**: How does the 7B router learn the capability boundaries of stronger models it never observes reasoning? GRPO provides only correct/incorrect feedback—no reasoning traces, no intermediate steps.\n\n- **Table 4 explanation**: Without R_hierarchy, 88.46% of queries route to L4. Does this indicate:\n  - The router genuinely assesses most queries as requiring tools?\n  - Or risk-aversion (\"when uncertain, pick the strongest option\")?\n  \n  If the latter, how can we trust that R_hierarchy teaches genuine complexity understanding rather than merely forcing cheaper selections through penalty?\n\n- **Router size ablation**: Have you tested routing with different model sizes (1.8B / 7B / 14B / 32B)?\n  - If larger routers improve accuracy → confirms stronger models make better judges (but undermines cost savings)\n  - If accuracy plateaus at 7B → important empirical finding worth reporting\n  - If 1.8B performs comparably → suggests learned heuristics rather than deep understanding\n\n- **Comparison to RouteLLM**: RouteLLM uses preference data to teach routers about capability differences. Why is GRPO's binary feedback sufficient when RouteLLM requires richer training signals?\n\n## Q4: Baseline Comparisons and Fairness\n\n- **Missing routing baselines**: Why not compare to:\n  - RouteLLM (ICLR 2025) - directly comparable routing framework\n\n- Can you compare against DeepSeek-R1-Distill models (7B/14B/32B) for fairer scale comparison?\n\n## Q5: Generalization Beyond Training Distribution\n\n- **Out-of-distribution tasks**: Training uses only math + commonsense + medical QA. How does CogER perform on:\n  - Code generation (HumanEval, MBPP)\n  - Long-context reasoning (QuALITY, NarrativeQA)\n  - Multi-turn dialogue (MT-Bench)\n  - Factual QA with retrieval (Natural Questions)\n  \n  The current 2 OOD datasets (MAWPS, CollegeMath) are still mathematical—not true distribution shift.\n\n- **Training data scale**: You use 8K samples. Have you:\n  - Ablated training data size to show performance saturates at 8K?\n  - Tested whether more diverse training data improves routing generalization?\n\n- **Dataset artifact learning**: Training on GSM8K/MATH then testing on same benchmarks risks learning dataset-specific patterns (e.g., \"MATH → L3\" heuristic). How do you ensure the router learns generalizable complexity assessment rather than dataset shortcuts?\n\n## Q6: Model Upgrade and Long-term Viability\n\nThe 7B router is trained on specific model capabilities (Qwen2.5-7B/32B, QwQ-32B):\n\n- **Model upgrades**: If Qwen3.0-7B surpasses Qwen2.5-32B in capability, how should the system adapt?\n  - Does the router require complete retraining?\n  - Can L_min labels transfer across model versions?\n\n- **Cross-model generalization**: Can the trained router generalize to different model families (e.g., Llama, Mistral)?\n\n- **Failure modes**: When does CogER perform worse than baselines? Please provide:\n  - Case studies of routing failures\n  - Characterization of query types most prone to misclassification"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "M93xVmPWSg", "forum": "z9HUMon2Br", "replyto": "z9HUMon2Br", "signatures": ["ICLR.cc/2026/Conference/Submission705/Reviewer_opSX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission705/Reviewer_opSX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission705/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923843848, "cdate": 1761923843848, "tmdate": 1762915584849, "mdate": 1762915584849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}