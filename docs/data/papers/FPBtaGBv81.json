{"id": "FPBtaGBv81", "number": 25440, "cdate": 1758368114413, "mdate": 1759896721048, "content": {"title": "Dynamic Trust Region Adaptation for \\\\ Human-in-the-Loop Reinforcement Learning \\\\ in Code Refinement", "abstract": "We propose a dynamic trust region adaptation framework for Human-in-the-Loop Reinforcement Learning (HITL-RL) in code refinement to address the challenge of incorporating unskilled human feedback into policy updates. Conventional methods handle all feedback in the same way, and this may result in poor convergence because not all feedback is of the same quality. The proposed system presents a Bayesian-driven Feedback Confidence Estimator, which geometrizes the faith in the human as the sole reliable agent as a dynamically updated score of confidence, and an Adaptive Trust Region Controller to modulate the policy updates based on a dynamically changing confidence score. High confidence feedback works to enlarge the trust region so they will explore, while low confidence feedback works to shrink the trust region so they will avoid overfitting. Furthermore, the framework has a confidence weighting reward shaping mechanism and a gated policy network to selectively favor reliable feedback during the training process. Implemented with transformative architectures including Codex-style policy network and DeBertA-v3 feedback encoder, closed-looped adaptation to feedback uncertainty.", "tldr": "", "keywords": ["Code Refinement"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5e672f3ea95a9e58fe41dc6e69e40c23a3003aa5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a novel dynamic trust region adaptation framework with Bayesian-driven Feedback Confidence Estimator and Adaptive Trust Region Controller. The method combines dynamic trust region strategies with adaptive mechanisms, aiming to address shortcomings in current TRPO algorithms for human-in-the-loop Reinforcement Learning in code refinement tasks. The central idea is to address the variability in human feedback quality by modeling feedback confidence using Bayesian methods and the framework dynamically adjusts policy update bounds and reward shaping based on estimated feedback reliability. The approach is validated through empirical studies in the code refinement environment, demonstrating improvements in convergence stability and final performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated. The new trust region strategy and its integration into existing adaptive optimizers show clear contributions.\n\n2. The empirical evaluation is comprehensive and the method performs well on all three metrics when compared with state-of-the-art optimizers."}, "weaknesses": {"value": "1. The accuracy of confidence estimation depends heavily on the initial feature extraction from feedback text. The system's robustness might be compromised when facing adversarial feedback.\n\n2. While the framework claims easy generalization, empirical validation outside code refinement is not provided.\n\n3. The setup for handling conflicting feedback is interesting. However, it lacks a bit of justification."}, "questions": {"value": "1. In the contribution part (line 58), filtering noisy input was mentioned but I could not find this anywhere else. Is this claim well-supported?\n\n2. Referring to W2, are there any adjustments necessary for cross-domain transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WSwaGiLOnt", "forum": "FPBtaGBv81", "replyto": "FPBtaGBv81", "signatures": ["ICLR.cc/2026/Conference/Submission25440/Reviewer_EYRa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25440/Reviewer_EYRa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760522861947, "cdate": 1760522861947, "tmdate": 1762943432953, "mdate": 1762943432953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a Dynamic Trust Region Adaptation framework for Human-in-the-Loop Reinforcement Learning in the context of code refinement. The key idea is to dynamically adjust the trust region used in policy optimization based on the estimated confidence of human feedback, which is modeled using a Bayesian Beta-Bernoulli estimator. The system is evaluated on a dataset of 10,000 Python/Java programs with synthetic bugs and style issues, showing consistent gains in code correctness, style improvement, and feedback utilization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Handling noisy, variable-quality human feedback is a major open challenge in HITL-RL, especially in code refinement where feedback can be vague, asynchronous, or inconsistent. The use of a Bayesian confidence model to dynamically modulate trust region size is principled and well-motivated."}, "weaknesses": {"value": "1. Dynamic trust regions based on uncertainty have been explored before (e.g., UA-TRPO), and Bayesian confidence estimation for human feedback is common in preference-based RL. The paper’s combination is sensible but incremental.\n2. Human feedback is simulated by injecting noise into labels or scores. No real human annotators are involved in the main experiments.\n3. No comparison with recent LLM-based code refinement systems (e.g., Codex), which are strong contenders in this space."}, "questions": {"value": "1. How does DTRA compare against recent LLM-based refinement systems?\n2. Can the method scale to real-world pull-request reviews with thousands of lines and multi-file changes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rRtVR5rrDG", "forum": "FPBtaGBv81", "replyto": "FPBtaGBv81", "signatures": ["ICLR.cc/2026/Conference/Submission25440/Reviewer_AENV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25440/Reviewer_AENV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548020214, "cdate": 1761548020214, "tmdate": 1762943432752, "mdate": 1762943432752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a dynamic trust region framework for Human-in-the-Loop RL to improve code refinement, addressing inconsistent human feedback quality. It uses a Bayesian estimator to score feedback confidence and an adaptive controller to adjust the policy's trust region, using high-confidence feedback to explore and low-confidence to prevent overfitting."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Adaptive trust region and Bayesian confidence modeling is a creative solution\n- The method achieves significant improvements in Code Correctness Score, Style Improvement Ratio, and Human Feedback Utilization.\n- The authors explicitly discuss ethical implications (bias, transparency, and privacy in feedback tracking)"}, "weaknesses": {"value": "- Generalization Across Domains: While the paper motivates general applicability (e.g., robotics, creative design), all evaluations are in code refinement.\n- Limited Exploration of Alternative Alignment Paradigms and Limited Scope of Experimental Baselines\n- Dependence on Textual Feature Extraction Quality. If the natural-language feedback is ambiguous, inconsistent, or noisy, the confidence estimates (α, β updates) may be unreliable, leading to suboptimal trust region scaling. \n- High Computational Overhead: Maintaining and updating Bayesian parameters for every feedback instance adds significant computational and memory cost.\n- Parameter choice and design are not well-explained or are missing supporting evidence to convince that the proposed approach is sound."}, "questions": {"value": "- Why does the paper focus exclusively on PPO-based RLHF when newer, RL-free methods like Direct Preference Optimization have shown improved stability and simplicity? Why not compare with DPO or its recent variants for code refinement?\n- The proposed method passively reweights feedback based on confidence. Have the authors considered active feedback selection approaches that query humans only when model uncertainty is high? \n- How sensitive is the model to the initialization of the Beta distribution parameters ($\\alpha_0, \\beta_0$)?\n- The clipping parameter $\\delta_t$ adapts via tanh($\\gamma \\theta_t$). How was γ selected, and how sensitive is the training stability to this hyperparameter? (PPO and trust-region methods are highly sensitive to clipping behavior, so understanding how $\\gamma$ affects convergence is crucial. The paper provides a default value of 3.0 but does not justify it or analyze sensitivity empirically.)\n- How do confidence-weighted rewards (Eq. 13) interact with environmental rewards during early training when $\\theta_t$ is unreliable?\n- Can the dynamic trust region concept be extended to multi-human settings where different users have independent confidence profiles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "ezK0IKdIEu", "forum": "FPBtaGBv81", "replyto": "FPBtaGBv81", "signatures": ["ICLR.cc/2026/Conference/Submission25440/Reviewer_Nu87"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25440/Reviewer_Nu87"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761701805379, "cdate": 1761701805379, "tmdate": 1762943432350, "mdate": 1762943432350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Reinforcement learning (RL) from human feedback is a common approach to enhancing code refinement tasks. However, it suffers from the inconsistent quality of human feedback. To address this, the authors propose a dynamic trust region–based RL method that incorporates the estimated confidence of human feedback. The approach consists of three main components:\n\n1. A Bayesian estimator to approximate the importance of human feedback,\n2. A trust region controller to adjust policy updates dynamically, and\n3. A gated module that selectively incorporates human feedback based on its confidence."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "\\+ Optimizing models with a dynamic trust region based on human feedback confidence sounds like an intriguing idea to explore.\n\n\\+ This paper attempts to address a well-known issue in a structured manner."}, "weaknesses": {"value": "Cons:\n\n\\- There are too many vague descriptions, which make it difficult to understand the detailed methods proposed, as well as the evaluation results. For instance, it is not explained what \"feedback $f_{t+k}$  confirms the original suggestion\" means (line 189).\n\n\\- Related work is limited and seems not to capture the state-of-the-art developments in RL for coding or RLfH.\n\n\\- Numerous grammar errors, format issues, and typos, e.g.,\" følben\" (line 49), having a paragraph with only one sentence (line 104),  \"rLO\" without definition or explanation (line 117), which largely impacts the readability of this paper. The authors used too many bullet points and enumerated lists to stretch this paper to 9 pages.\n\n\\- The paper is addressing a meaningful aspect of model adaptation for coding refinement, but the overall quality is not to the standards of ICLR. The submission reads more like a student project than a polished research paper."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UnqM2JXUqh", "forum": "FPBtaGBv81", "replyto": "FPBtaGBv81", "signatures": ["ICLR.cc/2026/Conference/Submission25440/Reviewer_3QSA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25440/Reviewer_3QSA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25440/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704812334, "cdate": 1761704812334, "tmdate": 1762943431913, "mdate": 1762943431913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}