{"id": "KjlLwRsiUE", "number": 2220, "cdate": 1757033902143, "mdate": 1763692637537, "content": {"title": "MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation", "abstract": "Character image animation, which synthesizes videos of reference characters driven by pose sequences, has advanced rapidly but remains largely limited to single-human settings. Existing methods struggle to generalize to multi-humanoid scenarios, which involve diverse humanoid forms, complex interactions, and frequent occlusions. We address this gap with two key innovations. First, we introduce unified motion representations that extract identity-agnostic 4D motions and explicitly bind them to corresponding characters, enabling generalization across diverse humanoid forms and seamless extension to multi-humanoid scenarios. Second, we propose a holistic 4D-anchored paradigm that constructs a shared 4D space to fuse motion representations with video latents, and further reinforces this process with hierarchical 4D-level supervision to better handle interactions and occlusions. We instantiate these ideas in MotionWeaver, an end-to-end framework for multi-humanoid image animation. To support this setting, we curate a 46-hour dataset of multi-human videos with rich interactions, and construct a 300-video benchmark featuring paired humanoid characters. Quantitative and qualitative experiments demonstrate that MotionWeaver not only achieves state-of-the-art results on our benchmark but also generalizes effectively across diverse humanoid forms, complex interactions, and challenging multi-humanoid scenarios.", "tldr": "MotionWeaver is a thoroughly 4D-anchored framework for multi-humanoid animation, enabling robust synthesis across diverse body forms, complex interactions, and severe occlusions.", "keywords": ["Character Animation", "Diffusion Model", "Video Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ee063e2eb2de7a85d6bd573ec01bf24de6b9832.pdf", "supplementary_material": "/attachment/c3bd1322ae619af123937e86e128cbee71e70eb0.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Motion Weaver, a novel and holistic 4D-Anchored Framework designed for multi-humanoid image animation. It specifically tackles the challenges of inter-character occlusion, complex interactions, and diverse humanoid forms—issues that often cause existing single-person animation models to fail. The method utilizes a Unified Choreography Core (UCC), a Hyper-Scene Integrator (HSI), and a Hierarchical 4D Supervision (H4S) strategy, alongside contributing the MultiHuman46 dataset and DualDynamics benchmark. The results are qualitatively impressive in multi-character settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe paper tackles the crucial and challenging problem of multi-character image animation, which current state-of-the-art methods typically cannot handle robustly due to their focus on single-person scenarios.\n2.\tThe introduction of the MultiHuman46 dataset and the DualDynamics benchmark provides essential resources for advancing research in the multi-humanoid animation field."}, "weaknesses": {"value": "1.\tThe proposed Motion Weaver is trained on the dedicated, multi-humanoid focused MultiHuman46 dataset. The methods chosen for comparison (e.g., RealisDance-DiT, UniAnimate-DiT, etc.) are predominantly designed and trained on single-person or general video datasets. Comparing Motion Weaver (trained on multi-person data) against baselines (trained on single-person data) on a multi-person benchmark (DualDynamics) creates a massive advantage for the proposed method. The baseline models are being tested significantly Out-of-Distribution (OOD) for the task of multi-person interaction and occlusion handling. The reported performance gain is likely inflated due to this training data mismatch.\n2.\tThe authors must perform a fair comparison by either: a) Re-training/Fine-tuning all comparable baselines on the MultiHuman46 training set and reporting the results. This would isolate the performance difference due to the architectural design. b) Alternatively, the authors should present an ablation study where Motion Weaver is only trained on a standard, single-person dataset and then tested on the multi-person benchmark, to demonstrate the robustness of its 4D framework even without specific multi-person training data.\n3.\tThe paper focuses almost exclusively on multi-humanoid scenarios. It is critical to demonstrate that the novel 4D-anchored components and mechanisms designed for multiple characters do not degrade performance or introduce unnecessary complexity/overhead when applied to standard, single-person animation tasks."}, "questions": {"value": "Please refer to Weaknesses for more details. If my concerns are solved, I am glad to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1CW8tTjPES", "forum": "KjlLwRsiUE", "replyto": "KjlLwRsiUE", "signatures": ["ICLR.cc/2026/Conference/Submission2220/Reviewer_dGPY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2220/Reviewer_dGPY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638774806, "cdate": 1761638774806, "tmdate": 1762916149695, "mdate": 1762916149695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MotionWeaver for multi-humanoid image animation. MotionWeaver introduces unified motion representations to extract identity-agnostic 4D motions and bind them to corresponding characters, enabling generalization across diverse humanoid forms. It fuses motion representations with video latents to control the generated motion. The paper also introduces training datasets and benchmarks to support this task. Results show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The multi-humanoid image animation is an important, practical and interesting task.\n2. The proposed method designs several components to extract motion to improve generalization, which is well motivated and reasonable.\n3. The results show that the proposed method outperforms previous methods."}, "weaknesses": {"value": "1. The training setting of comparing methods seems to be different from the proposed method. Are the comparing methods trained on the same data?\n2. The proposed method consists several steps. What is the inference time comparison to previous methods? How would the error propagation affect the final results?\n3. The evaluation is only conducted on the proposed benchmark. It is unclear whether the proposed method outperforms comparing methods on existing benchmarks."}, "questions": {"value": "1. What is the training settings of the comparing methods? Are they trained on the same data? Could the improvements come from different training data?\n2. What is the inference time comparison to previous methods? How would the error propagation affect the final results?\n3. What is performance of the proposed method on existing benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PIDkAESx4h", "forum": "KjlLwRsiUE", "replyto": "KjlLwRsiUE", "signatures": ["ICLR.cc/2026/Conference/Submission2220/Reviewer_9egx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2220/Reviewer_9egx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668820376, "cdate": 1761668820376, "tmdate": 1762916149273, "mdate": 1762916149273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MotionWeaver for multi-humanoid image animation. MotionWeaver introduces unified motion representations to extract identity-agnostic 4D motions and bind them to corresponding characters, enabling generalization across diverse humanoid forms. It fuses motion representations with video latents to control the generated motion. The paper also introduces training datasets and benchmarks to support this task. Results show the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The multi-humanoid image animation is an important, practical and interesting task.\n2. The proposed method designs several components to extract motion to improve generalization, which is well motivated and reasonable.\n3. The results show that the proposed method outperforms previous methods."}, "weaknesses": {"value": "1. The training setting of comparing methods seems to be different from the proposed method. Are the comparing methods trained on the same data?\n2. The proposed method consists several steps. What is the inference time comparison to previous methods? How would the error propagation affect the final results?\n3. The evaluation is only conducted on the proposed benchmark. It is unclear whether the proposed method outperforms comparing methods on existing benchmarks."}, "questions": {"value": "1. What is the training settings of the comparing methods? Are they trained on the same data? Could the improvements come from different training data?\n2. What is the inference time comparison to previous methods? How would the error propagation affect the final results?\n3. What is performance of the proposed method on existing benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PIDkAESx4h", "forum": "KjlLwRsiUE", "replyto": "KjlLwRsiUE", "signatures": ["ICLR.cc/2026/Conference/Submission2220/Reviewer_9egx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2220/Reviewer_9egx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761668820376, "cdate": 1761668820376, "tmdate": 1763741024021, "mdate": 1763741024021, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presented MotionWeaver which synthesizes multi-character video given the reference character images and pose sequences. The proposed MotionWeaver includes these modules:\n\n1)\tUnified-Choreopgraphy Core (UCC), which extracts 4D motion representations of characters that exclude their appearances. UCC uses SMPL to represent 3D human body as their joint coordinates and normalizes to a standardized skeleton. Individual character is segmented from the reference image to bind with the motion representations by a pre-trained I2V model.\n\n2)\tHyper-Scene Integrator (HIS), which estimates the depth from the occlusion-loss supervision and encodes with a dynamic C-RoPE. This representation allows a Hierachical-4DSupervision (H4S) to train the model with occlusion loss and motion-level loss, etc. \n\nThe MotionWeaver model is trained on a new 46-hour multi-human video dataset and evaluated on a new DualDynamics benchmark including 300 videos with complicated interactions between two humanoid characters. MotionWeaver outperforms 7 recent character animation models on this new DualDynamics benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This work is well motivated. Character video generation including two characters remains very challenging. The paper presented a clear analysis of the issues of motion representation and 4D modeling. \n\nThe proposed technical approaches, i.e., UCC, HIS, H4S, are technical novel and make sense to improve the performance of multi-character video generation.\n\nThe experiments on the new DualDynamics benchmark show clear improvement, which outperform recent related works substantially."}, "weaknesses": {"value": "The paper only reported the performance comparison on the new DualDynamics benchmark. Why only part of the constructed benchmark will be released? Then how do subsequent works compare with MotionWeaver?\n\nAll the technical approaches, i.e., UCC, HIS, H4S, are backward applicable to the single character case. So please show the performance of MotionWeaver on Fashion and TikTok, thus, the readers have a clear understanding of the advantages of these modules."}, "questions": {"value": "Throughout the paper, it seems all approaches and the training/benchmark datasets focused on two-humanoid interactions. So perhaps “two humanoid image animation” is a more proper claim than “multi-humanoid image animation”?\n\nll.251: “and augment them with the latent timestep t”, could you elaborate how to do this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h5cl5cQsMR", "forum": "KjlLwRsiUE", "replyto": "KjlLwRsiUE", "signatures": ["ICLR.cc/2026/Conference/Submission2220/Reviewer_FnGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2220/Reviewer_FnGY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714899411, "cdate": 1761714899411, "tmdate": 1762916148784, "mdate": 1762916148784, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MotionWeaver, a multi-humanoid image-to-video framework that (1) learns identity-agnostic 4D motion tokens via the Unified-Choreography Core (UCC), (2) fuses motion and video latents in a shared 4D space using the Hyper-Scene Integrator (HSI) with depth-aware attention and Dynamic Cross-RoPE, and (3) trains with Hierarchical-4D Supervision (H4S) that adds occlusion supervision at high noise and motion-level supervision at low noise. The authors also curate MultiHuman46 (46 hours of multi-human video) and release DualDynamics (300 two-character clips) to stress-test interactions/occlusions, reporting consistent SOTA on that benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly argues that 2D control entangles appearance and motion and lacks explicit depth reasoning in multi-person scenes, and responds with a fully 4D-anchored pipeline (UCC, HSI and H4S) that separates motion from morphology, injects depth ordering, and supervises motion explicitly.\n\n2. UCC standardizes SMPL joints to strip appearance cues and binds per-person motion with group attention. HSI adds depth-aware cross-attention plus Dynamic Cross-RoPE to align (t, x, y) between camera and image planes. H4S schedules occlusion vs. motion-level supervision by noise step. These pieces are concrete and novel in combination.\n\n3. This paper provides extensive experiments to support its claim. On DualDynamics, MotionWeaver tops all baselines on every metric, supporting the claim that explicit depth/occlusion handling and identity-motion binding help in multi-humanoid settings. Module-wise ablations and attention visualizations show each component (motion normalization, group attention, depth-aware attention, Dynamic Cross-RoPE, timestep-aware training) matters. Qualitative figures highlight fewer identity swaps and cleaner occlusion ordering than baselines."}, "weaknesses": {"value": "My major concern about this paper is the detailed comparison over existing methods to show the contribution clearly. The multi-person interaction and 4D motion tokens are adopted by previous methods already. I hope the authors could clearly clarify the difference with existing methods.\n\n1. MTVCrafter (Ding et al., 2025) also models raw 4D motion via a tokenizer (4DMoT) and conditions a DiT with 4D positional encodings, reporting large gains on open-world human animation. MotionWeaver likewise trains a 4D tokenizer, uses 4D positional cues (Dynamic Cross-RoPE), and claims generalization beyond humans, so the novelty boundary feels blurred. The paper would benefit from a direct experimental comparison (same control signals/base model), or at least an ablation contrasting quantized motion tokens (MTVCrafter) versus the authors’ standardized-skeleton motion units and group-attention binding.\n\n2. DanceTogether (Chen et al., 2025) introduces a MaskPoseAdapter that fuses tracking masks with noisy pose heatmaps at every denoising step to suppress identity drift when actors swap positions and interact over long horizons. MotionWeaver’s identity binding relies on group attention and occlusion-aware depth cues but evaluates on 49-frame clips. It doesn’t stress identity under frequent cross-overs or long sequences. A head-to-head on DanceTogether’s scenarios/metrics, or adding long-horizon tests with frequent position exchanges, would strengthen claims. Besides, Structural Video Diffusion [a] in ICCV 2025 proposes identity-specific embeddings plus structural learning with depth + surface normals. MotionWeaver also models depth (via depth-aware attention and occlusion loss). A comparison on the [a]'s setup or cross-evaluation across datasets would clarify when explicit identity embeddings vs. identity-agnostic binding are preferable.\n\n[a] Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Yuwei Guo, Dahua Lin, Tianfan Xue, Bo Dai. Multi-identity Human Image Animation with Structural Video Diffusion, ICCV 2025. https://openaccess.thecvf.com/content/ICCV2025/papers/Wang_Multi-identity_Human_Image_Animation_with_Structural_Video_Diffusion_ICCV_2025_paper.pdf\n\n3. DualDynamics emphasizes two-character, 49-frame interactions crafted by an animation team. MultiHuman46 includes AI-generated material and web-crawled clips. While appropriate for controlled studies, this may underestimate long-range identity drift and real-world messiness. Evaluating on longer, real-capture sequences (or adopting external multi-human sets) would improve external validity."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GHKNw7Zz94", "forum": "KjlLwRsiUE", "replyto": "KjlLwRsiUE", "signatures": ["ICLR.cc/2026/Conference/Submission2220/Reviewer_Yd3q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2220/Reviewer_Yd3q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847034462, "cdate": 1761847034462, "tmdate": 1762916148588, "mdate": 1762916148588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global reply"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful comments and constructive feedback.\n\nWe are pleased that the feedback consistently highlights strengths of our work:\n\n- **All reviewers** acknowledged the **significance** of our work, such as reviewer dGPY writing: \"The paper tackles the crucial and challenging problem of multi-character image animation.\"\n- **Three reviewers** pointed out our strong analysis and **reasonable motivation**, such as reviewer 9egx stating that it is \"well motivated and reasonable.\"\n- **All reviewers** agreed that our proposed method is **novel**, such as reviewer Yd3q noting: \"These pieces are concrete and novel in combination.\"\n- **All reviewers** recognized the **compelling performance** of our method, such as reviewer FnGY commenting: \"which outperform recent related works substantially.\"\n\nIn the following, we provide clear and detailed replies to all questions and concerns, and include additional experiments on more datasets to further validate our method. \n\nWhile we commit to incorporating the additional experimental results into the final version of the paper, we have temporarily refrained from modifying the main text of the paper at this stage. We plan to first engage in a productive discussion with the reviewers and will subsequently integrate the full set of supplementary experiments into Page 10 of the revised paper."}}, "id": "Yi10PmZrWV", "forum": "KjlLwRsiUE", "replyto": "KjlLwRsiUE", "signatures": ["ICLR.cc/2026/Conference/Submission2220/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2220/Authors"], "number": 14, "invitations": ["ICLR.cc/2026/Conference/Submission2220/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763692435680, "cdate": 1763692435680, "tmdate": 1763692435680, "mdate": 1763692435680, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}