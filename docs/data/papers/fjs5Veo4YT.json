{"id": "fjs5Veo4YT", "number": 2990, "cdate": 1757312841824, "mdate": 1759898115158, "content": {"title": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models", "abstract": "The Needle-in-a-Haystack (NIAH) benchmark is commonly utilized to evaluate the capacity of large language models (LLMs) to manage long contexts by determining whether a model can identify query-relevant information amidst a vast amount of irrelevant text. This paradigm is increasingly regarded as a standard method for quantifying the effective context length of LLMs. However, we find that the context length measured in this manner does not accurately reflect the genuine context understanding capabilities of LLMs. Specifically, even advanced models like GPT-4o face challenges when the context includes only a few query-relevant sentences without irrelevant distractors. To address this, we introduce NeedleChain, a new benchmark designed to evaluate the range of context lengths that allow intact understanding by LLMs. Unlike NIAH, NeedleChain requires models to integrate and reason over the entire input to arrive at the correct answer. This benchmark is adaptable, enabling researchers to adjust both context length and reasoning order for a more thorough analysis of long-context performance. Experiments with various state-of-the-art LLMs reveal a notable gap between their ability to process long inputs and their capacity for full understanding, underscoring the need for benchmarks and methodologies beyond the NIAH paradigm. Additionally, we propose a straightforward yet effective strategy, ROPE Contraction, which directly enhances long-context reasoning without altering the architecture. Throughout this paper, we argue that instead of rapidly extending context length, improving comprehension within a limited range could be more advantageous.", "tldr": "", "keywords": ["Large Language Model", "Natural Language Model", "Natural Language Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3eaf3e19595b179526f8654857a1738f71698de.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work highlights that existing Needle-in-a-Haystack benchmarks fail to faithfully evaluate the long-context understanding ability of large language models (LLMs). To address this, the authors propose the NeedleChain dataset, which manually constructs contexts involving complex multi-hop reasoning. Specifically, it introduces two data modes—Independent Needle and Dependent Needle—and three logical formats—Forward Chain, Backward Chain, and Mixed Chain—to test LLMs’ comprehension of three mathematical relations: halving, doubling, and retraining. Experimental results show that advanced models such as Qwen2.5-32B, Llama3.3-70B, and GPT-4o exhibit performance degradation on NeedleChain as the token length increases, suggesting that longer context windows do not necessarily imply better contextual reasoning. The authors therefore call for future research to focus more on genuine long-context understanding rather than merely expanding context length. In addition, they design a novel ROPE strategy to further enhance LLMs’ reasoning capability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of this study is important. The limitations of the Needle-in-a-Haystack evaluation scheme have gradually become common sense in the community. Exploring improved methodologies for assessing LLMs’ long-context understanding is therefore highly valuable.\n\n2. The construction of the dataset is clear and flexible. The results are reproducible, and the context length can be easily extended.\n\n3. The experimental findings convincingly demonstrate that current LLMs still lack genuine long-context comprehension ability."}, "weaknesses": {"value": "1. The evaluation setting deviates from the stated goal. Although the paper emphasizes long-context reasoning, the proposed NeedleChain dataset focuses too heavily on reasoning itself, while the maximum context length is only around 2k tokens, which is far below the 32k+ scale typically considered in long-context evaluation. Moreover, the task setting is overly narrow, involving only simple proportional arithmetic relations (0.5×, 1×, 2×) about character salaries. In contrast, benchmarks like LongBench adopt more diverse scenarios to comprehensively evaluate long-context understanding.\n\n2. The proposed ROPE Contraction method is unclear. The description lacks sufficient detail. Since modifying RoPE is a common strategy for extending context windows, it is not clear how the proposed approach differs from prior work or why it theoretically improves reasoning, especially when the evaluated context length is relatively short.\n\n3. Numerous typos and formatting issues. For example, the content of Fig. 2 does not match its caption; the abbreviation should be ROPE rather than RoPE; and citation commands should follow LaTeX conventions, using \\citep{} for parenthetical citation and \\citet{} when used as the grammatical subject."}, "questions": {"value": "Please see the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cnpQtPX9X1", "forum": "fjs5Veo4YT", "replyto": "fjs5Veo4YT", "signatures": ["ICLR.cc/2026/Conference/Submission2990/Reviewer_wQTb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2990/Reviewer_wQTb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723564821, "cdate": 1761723564821, "tmdate": 1762916483907, "mdate": 1762916483907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark for evaluating the long-context performance of LLMs using fact chaining, or as they call it, NeedleChain. Unlike the well-known Needle-in-a-Haystack benchmark, NeedleChain provides a sequence of interrelated statements, where answering the final query requires reasoning over all the given facts. Experimental results across various models show that as the number of statements within the context window increases, the task becomes increasingly challenging. The paper also introduces a ROPE contraction mechanism, claiming it contributes to performance gains."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "While the experiments are not conducted in a fully controlled setup, the paper does include a fair amount of analysis—for example, examining error types and positional effects."}, "weaknesses": {"value": "Novelty:\nAlthough the paper references some relevant works, it does not adequately compare its contributions with prior research. For instance, RULER includes a variable tracking (VT) task where multiple statements are chained together. Due to its controlled setup—where the number of statements is fixed regardless of haystack length—it provides a more consistent measure of task complexity (see next point). Moreover, Li et al. (2024b) introduce multi-needle reasoning, which is highly similar to NeedleChain.\n\n\nTask complexity varies with length:\nIncreasing the number of statements simultaneously increases both the context length and the task’s inherent complexity. This makes it difficult to determine whether model failures are due to limitations in length generalization or in problem complexity. In RULER, for example, a VT task could be defined with eight interconnected facts while filling the remaining context with haystack text—allowing evaluation of long-context performance at a fixed difficulty level.\n\n\nUsing arithmetic:\nArithmetic is already a challenging task for LLMs, independent of context length (Yuan et al., 2023). Moreover, in cases with k=200, results can be as high as 2**200, introducing severe numerical precision and overflow issues. Even with tool use, such decimal computation errors would remain.\n\n\nUnclear and insufficient evaluation of the ROPE contraction method:\nFigure 6 is unclear, and the explanation fails to properly describe how ROPE contraction works. Does it apply across all frequencies? In contrast, YARN uses an NTK-by-parts approach to preserve high-frequency (short-distance) relationships, as noted by Peng et al. Therefore, the observed gap at k=50 between YARN and the original model in Figure 7 is questionable, since the context at that length is relatively short (~2K tokens).\nYuan et al., 2023: How Well Do Large Language Models Perform in Arithmetic Tasks?\n\n\nOther Issues:\n- The original NIAH task is cited incorrectly (lines 052, 440, 443):\nKamradt, G. (2023). Needle in a Haystack – Pressure Testing LLMs. GitHub Repository, p.28.\n- Line 253 mentions that examples for each error type are discussed in the appendix, but they are not included.\n- In the main experiment, the models exhibit some level of in-context reasoning. Based on the prompt shown in the appendix, the models produce a brief explanation followed by a final answer, essentially resembling chain-of-thought (CoT) prompting.\n- The error analysis relies on the limited explanations output by the model, which may not accurately reflect its internal (implicit) reasoning processes.\n- Several experimental details remain unclear. For example:\n Which model was used for Figure 7?\n What is the YARN scaling factor?\n- The needle templates are questionable. For example:\n \t\t\t\t{A} received X last week.\nThe query then asks about salary (How much salary did {Z} get?), which may refer to a different timeframe and thus be ambiguous for the model.\n\n\n\nMissing References:\n- Random-Access Infinite Context Length for Transformers (Amirkeivan Mohtashami, Martin Jaggi, 2023).\n- Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long-Context NLP – addresses the issue described in Weakness No. 2.\n- NOLIMA: Long-Context Evaluation Beyond Literal Matching – discusses the backward reasoning challenge\n- Michelangelo: Long-Context Evaluations Beyond Haystacks via Latent Structure Queries."}, "questions": {"value": "(no questions)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "c1jerTWp9E", "forum": "fjs5Veo4YT", "replyto": "fjs5Veo4YT", "signatures": ["ICLR.cc/2026/Conference/Submission2990/Reviewer_jZFm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2990/Reviewer_jZFm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844472331, "cdate": 1761844472331, "tmdate": 1762916483600, "mdate": 1762916483600, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper critiques the popular \"Needle-in-a-Haystack\" benchmark, arguing it fails to measure true long-context understanding in LLMs, focusing instead on simple information retrieval. The authors introduce NeedleChain, a new benchmark requiring models to integrate and reason over the entire input. Experiments reveal a significant gap between LLMs' ability to process long texts and their capacity for genuine comprehension."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work provides a detailed error analysis and case study of the proposed benchmark, uncovering some interesting experimental findings.\n2. This work introduces NeedleChain, a novel task that challenges even advanced LLMs, thus offering a new angle for a more complete assessment of their abilities."}, "weaknesses": {"value": "1. Lacking experimental results from more powerful closed-source models such as Claude Sonnet 4.5, Gemini 2.5 Pro, and GPT-5 to prove the necessity of NeedleChain.\n2. In Table 1, the longest context length in the main experiment is only 2k. This is actually evaluating the model's logical reasoning ability rather than its long-context modeling capability. The authors could include additional experimental results using reasoning models to examine whether stronger reasoning capabilities can improve accuracy.\n3. The NeedleChain task deviates from real-world long context application scenarios. Even achieving strong performance on this task does not demonstrate true long context modeling abilities.\n4. ROPE Contraction is not actually a new method, as similar attempts have already been made in prior work [3]. \n5. Missing citations for some long-context modeling benchmark works [1][2].\n \n\n[1] BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. (COLING'24)\n\n[2] Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA. (EMNLP'24)\n\n[3] Scaling Laws of RoPE-based Extrapolation. (ICLR'24)"}, "questions": {"value": "1. Regarding the Error analysis in Figure 3, it is unclear how to handle cases when both Needle Omission and Calculation Error occur simultaneously."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cDyTIBVNB8", "forum": "fjs5Veo4YT", "replyto": "fjs5Veo4YT", "signatures": ["ICLR.cc/2026/Conference/Submission2990/Reviewer_BuXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2990/Reviewer_BuXb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851667004, "cdate": 1761851667004, "tmdate": 1762916483444, "mdate": 1762916483444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NeeleChain, a benchmark designed to evaluate the intact context comprehension of LLMs. Three chain variants (forward, backward, and mixed) are designed to analyze performance variations based on the reasoning order. Experimental results reveal the gap between LLMs’ ability to process long inputs and their capacity for full understanding the context."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ a new benchmark using chained inputs to evaluate the intact context comprehension of LLMs\n+ experimental findings to identify the limitations of LLMs’ long-context understanding capabilities"}, "weaknesses": {"value": "- the benchmark focuses on numerical calculation only and thus is limited\n- the findings are kind of expected, making the research contributions weak"}, "questions": {"value": "Is the poor performance of the LLMs over NeedleChain because their weak numerical calculation ability rather than the intact context comprehension of the LLMs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SFfoX2SXyu", "forum": "fjs5Veo4YT", "replyto": "fjs5Veo4YT", "signatures": ["ICLR.cc/2026/Conference/Submission2990/Reviewer_Pupg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2990/Reviewer_Pupg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2990/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997640915, "cdate": 1761997640915, "tmdate": 1762916483273, "mdate": 1762916483273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}