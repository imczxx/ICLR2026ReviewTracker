{"id": "AwowReRWXI", "number": 23742, "cdate": 1758347833534, "mdate": 1763739946972, "content": {"title": "Beyond Ensembles: Simulating All-Atom Protein Dynamics in a Learned Latent Space", "abstract": "Simulating the long-timescale dynamics of biomolecules is a central challenge in computational science. While enhanced sampling methods can accelerate these simulations, they rely on pre-defined collective variables that are often difficult to identify, restricting their ability to model complex switching mechanisms between metastable states. A recent generative model, LD-FPG, demonstrated that this problem could be bypassed by learning to sample the static equilibrium ensemble as all-atom deformations from a reference structure, establishing a powerful method for all-atom ensemble generation. However, while this approach successfully captures a system's probable conformations, it does not model the temporal evolution between them. We introduce the Graph Latent Dynamics Propagator (GLDP), a modular component for simulating dynamics within the learned latent space of LD-FPG. We then compare three classes of propagators: (i) score-guided Langevin dynamics, (ii) Koopman-based linear operators, and (iii) autoregressive neural networks. Within a unified encoder–propagator–decoder framework, we evaluate long-horizon stability, backbone and side-chain ensemble fidelity, and temporal kinetics via TICA. Benchmarks on systems ranging from small peptides to mixed-topology proteins and large GPCRs reveal that autoregressive neural networks deliver the most robust long rollouts and coherent physical timescales; score-guided Langevin best recovers side-chain thermodynamics when the score is well learned; and Koopman provides an interpretable, lightweight baseline that tends to damp fluctuations. These results clarify the trade-offs among propagators and offer practical guidance for latent-space simulators of all-atom protein dynamics.", "tldr": "To massively speed up simulations, this AI model compresses a protein's complex all-atom structure into a simple latent space, simulates its movement quickly there, and then reconstructs the full-atom trajectory.", "keywords": ["representation learning", "generative models", "latent space dynamics", "score-based generative models", "dynamical systems", "GNNs", "autoregressive models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e865effeabbde09049ef75699f870c3180f49669.pdf", "supplementary_material": "/attachment/63c0132b545c53e862fe159bc8e1f3944118464d.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Graph Latent Dynamics Propagator (GLDP), a framework for simulating protein dynamics in the learned latent space of LD-FPG. The approach uses a frozen encoder-decoder from LD-FPG and compares three propagator classes: (i) score-guided Langevin dynamics, (ii) Koopman-based linear operators, and (iii) autoregressive neural networks. The work evaluates these propagators on systems of increasing complexity (alanine dipeptide, 7JFL, A1AR, A2AR)"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written, the concept and results are well presented\n- The paper proposes novel method which uses LD-FPG for encoding and decoding, and performs simulation in the latent space\n- Different methods of propagator strategies are compared\n- The authors have performed benchmarks on different scales of systems"}, "weaknesses": {"value": "Generalization:\n- The model is trained only on one time interval (frame stride), there's no evidence of generalizing to different time intervals\n- All the models are trained and tested on the same system\n- From Table 4,6,9, it seems that different systems use different hyperparameters. It's then questionable how to select proper hyperparameters when having a new system.\n\nUnclear Advantage Over Equilibrium Models:\n- All evaluations (RMSF, dihedrals, FES) measure equilibrium properties, not dynamics.\n- No transition timescales, autocorrelation functions, or mean first passage times.\n- The benchmarks and applications make the reviewer think that time-independent samplers such as AlphaFlow, BioEmu, might achieve same results without modeling dynamics."}, "questions": {"value": "- Even training and testing on the same system, the model is trained on long simulation trajectory. But that does not make this model useful if it always needs a long simulation over a certain time scale to be able to forecast well. The authors should investigate how the model performance depends on the training trajectory length\n- Regarding Langevin dynamics, as the authors mention that the score at t near 0 is used. Notice that the denominator is close to 0, then numerically there could be instability, which might explain the dynamics instability"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FDQx7MKPIf", "forum": "AwowReRWXI", "replyto": "AwowReRWXI", "signatures": ["ICLR.cc/2026/Conference/Submission23742/Reviewer_5p11"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23742/Reviewer_5p11"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692787088, "cdate": 1761692787088, "tmdate": 1762942788214, "mdate": 1762942788214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Graph Latent Dynamics Propagator (GLDP), a modular framework for simulating all-atom protein dynamics from learned space from LP-FPG. It compares three latent-space propagators: (i) score-guided Langevin dynamics, (ii) linear Koopman operators, and (iii) a nonlinear autoregressive neural network. Latent propagators are evaluated across several systems, from alanine dipeptide to A2AR, showing the trade-offs of propagators."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Systematic comparison of latent propagator (quality)\n\nThe latent propagators are compared under a shared encoder, decoder, and latent space. \n\n2. Clearly written and organized (clarity)"}, "weaknesses": {"value": "1. Detailed comparison with baselines\n\nThe authors only report the JSD of dihedral and coordinates to the ground truth. As in Figure 2 of MDGen, it would be more convincing to also show each dihedral distribution against the baselines.\n\n2. Validness of decoded molecules\n\nI could not find any content on the validity of the decoded full atom resolution. For the 3th and 7th molecule in Figure 5 (when going from left to right and up to down), the structure seems a bit odd. Simply plotting the energy distribution would make the paper more convincing. Additionally, in minor, plotting more qualitative plots in Figure 5 for each latent propagator and molecule, would also be a good case to see whether the latent dynamics succeed.\n\nMinor\n\n- Line 316 - Full results on Pearson correlation is missing, a wrap table for it would be good\n- Figure 3 - the ordering or propagators is different for the left one\n- Figure 4 - four trajectories with the background downgrades visibility a bit, plotting four plots separately seems also good. Also, the visibility of  inactive and active site are hindered."}, "questions": {"value": "1. Residual prediction with an autoregressive neural network\n\nJust a suggestion, perhaps learning the $f_\\theta(z_t)$ to approximate $z_{t+1} - z_{t}$ would improve the performance even more.\n\n2. Long horizon stability (section 4.2)\n\nI am a little confused about the conclusion for section 4.2. I understand the task, but since molecular dynamics trajectories contain randomness from the Brownian motion, maybe a propagator that did understand the molecular dynamics could result in sending the structure totally different from the ground truth data? Does 1DDT threshold include distinct local energy minima?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hauLaSXliY", "forum": "AwowReRWXI", "replyto": "AwowReRWXI", "signatures": ["ICLR.cc/2026/Conference/Submission23742/Reviewer_NVTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23742/Reviewer_NVTg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921923180, "cdate": 1761921923180, "tmdate": 1762942787755, "mdate": 1762942787755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Graph Latent Dynamic Propagator (GLDP), an approach for modeling molecular dynamics in the latent space of an encoder–decoder framework (LD-FPG). The encoder and decoder are kept fixed, while latent-space propagators are trained on the temporal sequences of latent representations. Three types of propagators are proposed: score-based Langevin dynamics, Koopman-based linear operators, and autoregressive neural networks.\n\nTo evaluate the flexibility and distributional fidelity of the dynamics generated by GLDP, several metrics are computed—such as the Jensen–Shannon Divergence (JSD) with respect to the ground-truth ensemble and the average RMSF across the sequence—and compared with baseline approaches including LSS, GeoTDM, and MD-Gen. The results indicate superior performance in recovering the ground-truth distribution and achieving flexibility values closer to the reference.\n\nIn the long-horizon modeling scenario, the three propagators are compared, with the autoregressive neural network demonstrating the greatest stability. Free-energy surfaces (FES) are computed in the space of two variables to measure fidelity to the equilibrium ensemble. Finally, GLDP is shown to successfully reproduce the inactive-to-active transition of $\\mathrm{A}_{2A}$R, where the score-guided Langevin dynamics covers most of the FES valley as well as the corridor connecting the inactive and active regions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper conducts a thorough evaluation across multiple dimensions, including quantitative metrics for stability, flexibility, and distributional fidelity. It also verifies that GLDP recovers the two metastable states of $\\mathrm{A}_{2A}$R (active and inactive), demonstrating consistency with real biological processes.\n2. The proposed method is encoder/decoder agnostic, as the encoder and decoder remain frozen. This design makes the framework easily adaptable to different latent spaces.\n3. The paper is clearly written and well presented."}, "weaknesses": {"value": "Overall, this is a solid paper with well-designed experiments and sound conclusions. However, it could be further improved in the following aspects:\n1. The evaluation is conducted on only three proteins. Although these systems cover increasing complexity from ADP to A1AR GPCR, experiments on additional systems would strengthen the conclusions regarding the relative performance of the propagators.\n2. It would be interesting to examine whether other baselines can also recover the active–inactive transition of $\\mathrm{A}_{2A}$R.\n3. In addition to performance metrics, it would be valuable to include efficiency comparisons between different propagators, which are particularly important for long-horizon molecular dynamics.\n4. The necessity or advantage of modeling dynamics in latent space, rather than Cartesian space, is not clearly articulated in the paper."}, "questions": {"value": "1. There are space formatting issues in lines 260, 264, and 265.\n2. There are also space formatting issues in lines 60 and 61.\n3. In Figure 4, the corridor and the regions representing inactive and active states are not clearly visible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j4zR39fF4L", "forum": "AwowReRWXI", "replyto": "AwowReRWXI", "signatures": ["ICLR.cc/2026/Conference/Submission23742/Reviewer_XsXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23742/Reviewer_XsXh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989762346, "cdate": 1761989762346, "tmdate": 1762942787491, "mdate": 1762942787491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates generating protein trajectories in the latent space of pretrained encoder-decoder models, rather than directly in Cartesian coordinates. The authors present GLDP, a plug-in module for the graph-based conformation generation model LD-FPG, enabling trajectory propagation in latent conformational space, followed by decoding to atomic coordinates. \nWithin this framework, the paper systematically compares three latent-space propagation strategies: Score-based Langevin dynamics (similar to Two-for-One, https://arxiv.org/abs/2302.00600); Koopman operator-based linear propagation, and Neural network-based nonlinear propagation, for autoregressive generation.\nThese approaches are evaluated on three protein systems of varying sizes. The neural autoregressive propagator is found to be the most stable and best at capturing ensemble-level statistics, while Langevin dynamics can perform better in recovering in side-chain torsional distributions.\nOverall, the work offers an interesting perspective on biomolecular dynamics by exploring trajectory generation in latent space and systematically comparing reasonable propagation strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Exploring protein dynamics in latent space as a potential way to accelerate MD simulations is an interesting direction, and this work provides a controlled comparison of three propagation strategies.\nThis study evaluates methods across protein systems of different sizes, offering some insight into applicability across system sizes"}, "weaknesses": {"value": "1. The idea of modeling dynamics in latent space is interesting, but the overall architecture (e.g., LD-FPG encoder-decoder) feels dated, and the evaluation is limited to three systems in non-transferable settings.\n\n2. Some evaluation choices are not fully convincing, and it is unclear whether certain results are statistically significant or lead to conclusive insights on this latent dynamic problem.\n\n3. The paper would benefit from stronger organization, clearer presentation of results, and inclusion of key experimental details that are currently missing. \n\nSee Questions for details."}, "questions": {"value": "[Model]\n\n1. What data are used to train the LD-FPG encoder–decoder? Is single encoder/decoder modules shared model used for different proteins, or separately for each protein system?\n2. Why choose LD-FPG (ChebNet + MLP) instead of more modern transformer-based architectures (e.g., as in AlphaFold3, https://www.nature.com/articles/s41586-024-07487-w)? This design also requires frame alignment and offset prediction, which limits transferability.\n3. The use of \"pooling’\" and \"decoder\" commonly appear together (e.g., in Table 3). Is my understanding right that pooling happens after encoding and before propagation, and is not part of the decoder?\n4. For score-based Langevin dynamics, score estimation near $t\\approx 0$ is known to be unstable due to very low noise level in the denominator. Is this a problem in practice?\n5. For baseline models (e.g., MD Gen), were their pretrained weights used, or were all models retrained for each system?\n\n[Data]\n\n6. How are the trajectory splits defined for training, validation, and testing? Has any time-based or conformation-based split applied? Are models always trained and evaluated on the same protein system?\n7. 7JFL_C is a small and fully helical protein (47 residues). Have larger or systems with other secondary structures (e.g., from ATLAS) been tested?\n\n[Results]\n\n8. Is lDDT alone sufficient to assess long-horizon physical stability, given it does not capture energetics or steric quality? The choice of failure threshold (lDDT < 0.65) also appears arbitrary - how was it determined?\n9. Figure 2a shows stable rollouts beyond 10,000 steps for the autoregressive model, but line 357 states failure at 3,176 steps. \n10. The claim that Langevin fails earlier on alanine due to step sizes tuned for GPCRs seems inconsistent with the earlier statement that step sizes are tuned per system. Can you clarify?\n11. In Table 2 (A1AR side-chain torsions), results are shown for only one protein and from single test, and differences between AR and Langevin are small. Are these statistically significant?\n12. For the A2AR case study, how does this system differ from A1AR, and what are the main takeaways?\n13. How does the runtime of GLDP compare to classical MD simulations? It seems that this method requires system-specific MD trajectories exist for training before it can generate new trajectories - would it become a problem in practical use?\n\n[Other comments]\n\nThe related work section is somewhat long and loosely organized. GPCR datasets appear alongside general method discussions, while other relevant datasets are not covered. This section could be better organized by theme and shortened."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vEyZk5k1Kb", "forum": "AwowReRWXI", "replyto": "AwowReRWXI", "signatures": ["ICLR.cc/2026/Conference/Submission23742/Reviewer_zVyy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23742/Reviewer_zVyy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992927442, "cdate": 1761992927442, "tmdate": 1762942787279, "mdate": 1762942787279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their constructive feedback and high-quality engagement. Based on your suggestions, we have significantly expanded the manuscript to include a new protein system, rigorous kinetic benchmarks, and computational efficiency analysis.\n\n## Key Updates in the Revision\n\n### 1. New Test System: HIV-1 Protease (1R6W)\n\nTo address concerns regarding system diversity and scalability (Reviewers zVyy, XsXh), we extended our benchmark to include HIV-1 Protease (1R6W).\n\n**Complexity:** This 320-residue system features a mixed $\\alpha/\\beta$ topology, testing the model's ability to handle non-local $\\beta$-sheet dependencies beyond the helical structures of 7JFL/A1AR.\n\n**Result:** GLDP generalizes successfully, achieving the highest contact map correlation ($r=0.972$) and maintaining stable long-horizon kinetics ($t_{\\text{decorr}} \\approx 1650$ steps), reinforcing the robustness of the latent propagator.\n\n### 2. Kinetic Analysis: Dynamics vs. Sampling\n\nTo clarify the advantage of GLDP over time-independent equilibrium samplers (Reviewer 5p11), we added a kinetic analysis using TICA and Autocorrelation (Table 1, Appendix A.7).\n\n**Physical Memory (Autocorrelation):** Equilibrium samplers produce independent, identically distributed (i.i.d.) samples, meaning the current state does not predict the next ($t_{\\text{decorr}} \\approx 0$). In contrast, GLDP trajectories exhibit deep physical memory ($t_{\\text{decorr}} \\approx 1650$ steps for 1R6W), confirming it integrates a continuous physical path rather than randomly sampling valid structures.\n\n**Energy Barriers (TICA):** GLDP recovers stable, positive implied timescales ($t_1$) across all systems. This proves the model has learned the metastability of the energy landscape. More specifically, it respects the height of free-energy barriers that govern slow conformational transitions, which memoryless samplers cannot capture.\n\n### 3. Computational Efficiency Benchmarks\n\nWe added a Wall-Clock Time analysis (Appendix A.9.4, Table 13) to quantify the utility of the method (Reviewer XsXh).\n\n**Result:** The Autoregressive NN generates 100k simulation steps in $\\approx 12$ seconds on a single GPU, offering orders-of-magnitude acceleration over classical integration.\n\n### 4. Expanded Ablations (Architecture & Data)\n\nWe performed new ablations to validate design choices (Reviewers NVTg, 5p11):\n\n**Residual vs. Direct Prediction:** We trained a residual variant ($\\Delta z$) and found statistically indistinguishable performance from our direct prediction baseline (Appendix A.9.3), validating the robustness of the deformation-based latent space.\n\n**Temporal Stride:** We analyzed performance across strides $k \\in \\{1..10\\}$. Results confirm that finer strides ($k=1$) are necessary to preserve physical energy barriers (Appendix A.9.2).\n\n**Data Efficiency:** We demonstrated that GLDP recovers accurate structural ensembles using as little as 25% of the training trajectory (Appendix A.9.1).\n\n### 5. Expanded Visualizations & Restructuring\n\n**Visual Baselines:** Added 2D Free Energy Surface comparisons against LSS, GeoTDM, and MD-Gen (Appendix A.10) to visually demonstrate GLDP's superior resolution of metastable basins (Reviewer NVTg).\n\n**Stability Calibration:** Clarified the empirical calibration of the lDDT $<0.65$ failure threshold and its correlation with steric validity (Reviewer zVyy).\n\n**Related Work:** Reorganized Section 2 to better categorize approaches by methodology (Latent Simulators vs. Coordinate Generative Models), as requested by Reviewer zVyy.\n\n---\n\nWe believe these additions address the core concerns regarding scope, validity, and utility. We invite the reviewers to examine the updated PDF."}}, "id": "3XWxBA1Cna", "forum": "AwowReRWXI", "replyto": "AwowReRWXI", "signatures": ["ICLR.cc/2026/Conference/Submission23742/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23742/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23742/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763739967244, "cdate": 1763739967244, "tmdate": 1763739967244, "mdate": 1763739967244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}