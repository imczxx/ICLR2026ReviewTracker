{"id": "hZy6YG2Ij8", "number": 13212, "cdate": 1758215144771, "mdate": 1759897456172, "content": {"title": "YuE: Scaling Open Foundation Models for Long-Form Music Generation", "abstract": "We tackle the task of long-form music generation, particularly the challenging \\textbf{lyrics-to-song} problem, by introducing \\textbf{YuE (乐)}, a family of open-source music generation foundation models. Specifically,\nYuE scales to trillions of tokens and generates up to five minutes of music while maintaining lyrical alignment, coherent musical structure, and engaging vocal melodies with appropriate accompaniment. It achieves this through \\textbf{track-decoupled next-token prediction} to overcome dense mixture signals, and \\textbf{structural progressive conditioning} for long-context lyrical alignment. In addition, we redesign the \\textbf{in-context learning} technique for music generation, enabling bidirectional content creation, style cloning, and improving musicality. Through extensive evaluation, we demonstrate that YuE matches or even surpasses some of the proprietary systems in musicality and vocal agility (as of 2025-01). We strongly encourage readers to \\textbf{listen to our demo}\\footnote{\\url{https://yue-anonymous.github.io}}.", "tldr": "We scale up an open LM-based song generation model to match the performance of proprietary systems.", "keywords": ["lyrics2song", "song generation", "long-form", "foundation model", "music generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ceb01c3e8c2264e426ad7c4c9fb7e471c28da00.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a generative model for lyrics to full song generation.\nThe authors observe that generating mixed tracks (voice + accompaniment) directly leads to a loss of intelligibility, even more pronounced for some musical genres. The proposed approach to overcome this issue is to rely on source separation to decompose tracks into vocals + accompaniment and to encode each component separately.\nThe vocals and accompaniment tokens are then interleaved so that the authors can rely on a standard autoregressive (based on Llama 2) next token prediction. In practice, only the first codebook of the RVQ representations is modelled in this stage, and a second stage consists in predicting the remaining RVQ codes.\n\nThere's an accompanying website as well as an open source + weights release.\nEvaluations are thorough and go beyond simple audio-quality measures, with a real emphasis on vocals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Modelling choices are coherent and well motivated: e.g. generating first interleaved vocals-accompaniment RVQ tokens, splitting full-song generation into multiple segments.\n- Open weights + open source is a plus, as song genAI models with vocals are rare. Authors also discuss the ethical issue and explain their scraping process and the questions this raises.\n- Extensive experiments\n- Many details and discussions about dataset, observations, evaluation."}, "weaknesses": {"value": "Some parts could be better explained like MUSIC IN-CONTEXT LEARNING, this introduces many notations that are not useful.\nStructural progressive conditioning is also evoked too quickly.\nMaybe being clearer in the main text that instead of lyrics to song, the problem is in fact casted as a \"lyrics with structure\" to song. This is clear in appendix I.\n- The way ICL is used (i.e. by conditioning on an existing song chorus) highly conditions the type of possible applications."}, "questions": {"value": "- Is the dataset available?\n- For the 30s context given for in-context learning, do you make sure to consider a part that is different (like not providing Verse 2 to generate Verse 1) as these are expected to be extremely similar for the instrumental parts?\n- How is it possible to do condition on a 30s context and ask to generate the full song?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EWDf25o2Fg", "forum": "hZy6YG2Ij8", "replyto": "hZy6YG2Ij8", "signatures": ["ICLR.cc/2026/Conference/Submission13212/Reviewer_szRj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13212/Reviewer_szRj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939141043, "cdate": 1761939141043, "tmdate": 1762923903457, "mdate": 1762923903457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents YuE for lyrics to song generation. It aims to generate full songs (up to five minutes) with vocals, accompaniment, and makes sure that the lyrics would align with the melody/companion.\n\nThe model is based on a two-stage autoregressive framework using discrete audio codec. The first stage models lyrics to first layer audio tokens, and then the second stages models first layer tokens to all residual tokens. \n\nThe paper introduces three key techniques:\n\nDual-NTP: Each timestep in the first stage LM generates one vocal token and one companion token, instead of directly modeling the mixture.\n\nSPC: A new way to organize training data. Songs are segmented into sections (intro, verse, chorus, etc.), and their lyrics and audio tokens are interleaved section by section. This helps the model maintain long-range lyrical alignment.\n\nMusic ICL (In-Context Learning): A modified in-context learning setup where a 30-second reference audio is prepended to SPC data.\nFor evaluation, YuE performs comparably to Tiangong and Udio, better than Hailuo, but below Suno V4. Objective metrics (KL divergence, FAD, alignment scores) also show strong performance.\n\nFor evaluation, YuE performs comparably to Tiangong and Udio, outperforming Hailuo but falling short of Suno V4. Objective metrics (KL divergence, FAD, alignment scores) also show strong performance.\n\nOverall, YuE is positioned as the first open-source full-song lyrics-to-song model with quality approaching commercial systems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis work presents the first open-sourced song generation model that achieves performance close to commercial systems.\n2.\tImplementing and training a full-length audio LM of this scale is a major technical achievement.\n3.\tDual-NTP and SPC integrate domain-specific musical structure into the generative process, representing a meaningful methodological contribution.\n4.\tDespite limited access to proprietary data, YuE achieves results comparable to those of closed-source commercial models"}, "weaknesses": {"value": "1.\tThe descriptions of data preprocessing are somewhat brief. It would help to clarify how vocal–accompaniment tracks were separated and how Creative Commons data were curated.\n\n2.\tThe “all-in-one method” used in the paper could be elaborated.\n\n3.\tThe paper could provide more analysis of sample diversity and potential copyright overlap (e.g., unintentional melodic copying). Clarifying whether such issues were observed would strengthen credibility.\n\n4.\tPrior research on multi-track or separated-source generation [1–3] should be acknowledged to better contextualize the contribution.\n[1] Mariani, Giorgio, et al. \"Multi-Source Diffusion Models for Simultaneous Music Generation and Separation.\" The Twelfth International Conference on Learning Representations.\n[2] Xu, Zhongweiyang, et al. \"Multi-Source Music Generation with Latent Diffusion.\" Audio Imagination: NeurIPS 2024 Workshop AI-Driven Speech, Music, and Sound Generation.\n[3] Karchkhadze, Tornike, et al. \"Multi-track musicldm: Towards versatile music generation with latent diffusion model.\" International Conference on ArtsIT, Interactivity and Game Creation. Cham: Springer Nature Switzerland, 2024."}, "questions": {"value": "1.\tHow are the tracks separated from the mixture? Does the separation process introduce perceivable artifacts that could impact performance? What separation model is being used?\n\n2.\tWhat is the dataset used in detail? Is there any preprocessing to clean the dataset?\n\n3.\tFor the dual-track tokens, the voice should have much lower entropy than the accompaniment. Have you considered using a low-bitrate codec for voice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CCdA8fJSRO", "forum": "hZy6YG2Ij8", "replyto": "hZy6YG2Ij8", "signatures": ["ICLR.cc/2026/Conference/Submission13212/Reviewer_wdiX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13212/Reviewer_wdiX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948166805, "cdate": 1761948166805, "tmdate": 1762923903202, "mdate": 1762923903202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces YuE, a family of open-source foundation models designed for long-form music generation, and more specifically, the lyrics-to-song problem. The model is trained on trillions of tokens and can generate coherent, lyrically-aligned songs of up to five minutes in length. YuE achieves its performance through several key technical contributions:\n\n- Structural Progressive Conditioning (SPC): This approach uses the inherent musical structure (like verses and choruses) to enable long-context lyrical alignment and overall song structure control, which is essential for full-song generation.\n- Redesigned In-Context Learning (ICL): A novel framework for music that allows for advanced use cases such as style transfer, voice cloning, and bidirectional content creation.\n- Track-Decoupled Next-Token Prediction (Dual-NTP): A variation of existing multi-track/multi-stem music generation methods here adapted for jointly generating the vocal and accompaniment tracks. The method is reported to improve lyrical intelligibility and to overcome problems related to dense signal mixtures.\n\nThrough extensive human evaluation, the model is shown to be competitive with, and in some aspects, surpasses widely used proprietary music-generation systems (like Tiangong and Udio) in musicality and vocal agility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Strength\n\n- innovative SPC training strategy taking into account musical structure\n- successful evaluation demonstrating competitive performance with state-of-the-art music generation models \n- open source availability"}, "weaknesses": {"value": "### References are outdated, discussion of relevant existing approaches is incomplete. \n\n- paper states in the abstract to refer to the state when the model was initially published in January 2025.\n- The list of references appears to be limited when it comes to covering more recent approaches. However, relevant approaches have appeared early in 2025. Here we consider prioublciation in 2026. Therefore appacohes from early 205 should be included into the discussion \n\n- NTP decoupled next token prediction. Rather similar approaches have been proposed here:\n\nLiu, 2025. \"SongGen: A Single Stage Auto-regressive Transformer for Text-to-Song Generation\", https://arxiv.org/pdf/2502.13128\nRouard, 2025. \"MusicGen-Stem: Multi-stem music generation and edition through autoregressive modeling\" https://arxiv.org/pdf/2501.01757\n\nI think these papers should be discussed under related works, and they should be compared to the proposed NTP strategy. \n\n- concerning SPC a related approach has been published in\n\nLam, 2025. \"MusiCoT: Analyzable Chain-of-Musical-Thought Prompting for High-Fidelity Music Generation\", https://arxiv.org/pdf/2503.19611\n\nThis alternative approach for including musical structure into music generation should at least be mentioned. I note that the paper features a comparison with YuE.\n\n- ICL\n\nThe paper makes the point of comparing with speech/TTS ICL methods. Given today, there exist a few approaches supporting lyricx conditioning for music generation, it appears a bit outdated to compare with these methods from a different problem domain."}, "questions": {"value": "Please see under weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z0bIXDLr4Y", "forum": "hZy6YG2Ij8", "replyto": "hZy6YG2Ij8", "signatures": ["ICLR.cc/2026/Conference/Submission13212/Reviewer_ytHG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13212/Reviewer_ytHG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009160597, "cdate": 1762009160597, "tmdate": 1762923902829, "mdate": 1762923902829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents YuE, a lyrics-to-song generation model that aims to produce long-form music generation with vocals and accompaniment. The authors propose a two-stage autoregressive architecture operating on discrete music tokens (X-Codec). The first stage predicts the first codebook of the music tokens by givens the semantic conditions (lyric, and others). and the second stage makes up the token predictions of the higher codebooks. To support long-form generation, the authors introduce Structural Progressive Conditioning (SPC), which uses section-level structure tokens to progressively condition the LM. Experimental results demonstrates that YuE performs competitively with commercial models (Suno, Udio, Hailuo)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper has two strengths.\n\n1.The usage of long-form structure conditioning (SPC) is novel. Using section metadata to segment data and guide generation is a practical strategy for improving macro-level structure, and the staged conditioning improves continuity.\n\n2.The experimental design is reasonable and comprehensive. The paper includes both objective metrics and human preference studies among YuE and four baselines. The results (and the demo website) show multi-minute outputs with acceptable alignment and structure."}, "weaknesses": {"value": "The paper has several critical weaknesses that hinders its acceptance.\n\n1.The overall novelty of this paper is limited, especially compared to the same period and already published work (SongBloom [1]) in NeurIPS 2025. Specifically, the two-stage autoregressive LM with Dual-NTP and SPC is incremental relative to SongBloom. And SongBloom even proposed more modern formulation: \n\n1 (a). SongBloom applies a unified autoregressive-diffusion architecture that interleaves semantic sketching and acoustic refinement, enabling significantly stronger acoustic quality with comparable modeling complexity. \n\n1 (b). SongBloom also covers the voice and accompaniment tokens by leveraging the Demucs separation model to extract the music data.\n\n1 (c). SongBloom covers essentially all YuE capabilities and more, including lyric-to-music generation, structured multi-section music, reference audio prompting, diffusion decoder for refinement, disentanglement of semantic and acoustic tokens, and also open-source release.\n\n1 (d). Despite having very similar high-level goals and tokenization methods, YuE does not compare to SongBloom, although SongBloom explicitly compares against YuE and reports better performance.\n\nCompared to SongBloom, YuE’s design is behind state-of-the-art, which lacks diffusion-based refinement, Interleaving of semantic + acoustic token, and robust modeling of reference-prompt conditions. Therefore, YuE does not demonstrate novelty or superiority over SongBloom. At present, it is difficult to justify acceptance when a clearly more advanced method has already been published.\n\n2.The experimental design is comprehensive but the baseline design is problematic. Apparently, YuE does not compare to the most relevant works, including SongBloom and ACE-Step [2]. Although YuE includes industry comparisons (Suno, Udio, Tiangong), the lack of comparison to academic/open models with similar architectures weakens the empirical claims. Also there is no ablation studies to isolate the contributions of Dual-NTP (decoupled vs non-decoupled), SPC, and Data-scale effects. And such ablation studies are very imperative to fully demonstrate YuE's contribution and its improvement to prior works. From the experimental results, YuE does not demonstrate superiority. It does not outperform Suno V4, and remain slightly behind Udio and Tiangong. It is acceptable that the model remains competitive but not superior to industrial models. But to fully demonstrate the proposed method, more ablation studies and comparisons to academical or open-source models should be considered. At the same time, SongBloom reports stronger performance over YuE. Thereofore, YuE does not establish SOTA status for either research or production settings.\n\n3.The data sourcing transparency and licensing concerns remain vague in the paper. The paper claims to train on around 650k hours of creative common (CC) music. However, there are several critical concerns. The actual data sources are not specified. The platforms, artists, catalogs remain unknown, and the licenses are not enumerated (CC-BY and CC-BY-NC are completely different). It is unclear whether scraping is permitted or whether data came from commercial music platforms (plus that, whether the authors of YuE own the copyrights of these data for AI training). And there is no dataset preview or download interface provided. So reproducibility, legality, and downstream use constraints remain ambiguous. Given active litigation around music AI, such as Suno and Udio to three major labels (Sony, UMG, and Warner), this raises concerns about copyright and reproducibility. Till now, only Udio and UMG come to an agreement of data for music AI training. Open-source models must satisfy a higher legal/ethical bar, and YuE’s data description is too vague to be evaluated.\n\n4.The claim and the vibe of novelty overstates the contribution. The paper claims that “As of its release on Jan. 28, 2025, YuE familiy is the first publicly available, open-source lyrics-to-song model capable of full-song generation with quality on par with commercial systems\". As a reviewer, this makes an excuse to prevent it from comparing to latest models after that. This is negative and questionable, especially given SongBloom’s public release and NeurIPS-publication timeline. The lack of acknowledgment or direct comparison reduces credibility.\n\n\n[1] SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement. NeurIPS 2025.\n\n[2] ACE-Step: A Step Towards Music Generation Foundation Model"}, "questions": {"value": "1. Why is SongBloom not included in your comparisons? Can you add some comparisons to it (and ACE-Step) and also give the evidence on the difference between your potential results to the results reported in their papers?\n\n2. Can you provide detailed documentation of the 650k hours of CC-licensed music? Which platforms were used and under what specific license? How many tracks / hours from each source? Are licenses compatible with redistribution + model training? Do you own the copyright of using these data? If not, since you release the model, how can external users ensure they are free from copyright exposure? How can the community safely adopt YuE?\n\n3. Please provide ablation studies including: Dual-NTP vs. single (combine voice and accompaniment) token prediction; SPC vs. no-SPC; and Data-scaling sensitivity (e.g., 10k/100k hours). For the data-scaling, it is encouraged to use the open-available data for training, even the results are not good (e.g., FMA, Jamendo, MusDB, MoisesDB, MedleyDB). Other data contains the lyric annotation might better to be used. \n\n4. For reproducibility, it is encouraged to have example data from your training set, and will you release the training script with data loader and processing, or just a simple inference code and model checkpoint?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper states the training on around  650k hours of creative common music mined from the web. In the Ethics Appendix details filtering for CC and string-match opt-outs. This acknowledges a difficulty of item-level verification. It does not enumerate datasets, licenses, or per-item sources/links; thus reproducibility, legality (CC-BY vs CC-BY-NC), and downstream use constraints remain ambiguous. The paper also plans watermark/fingerprinting but indicates it’s not yet ready."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9d9cjpEu0J", "forum": "hZy6YG2Ij8", "replyto": "hZy6YG2Ij8", "signatures": ["ICLR.cc/2026/Conference/Submission13212/Reviewer_aT19"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13212/Reviewer_aT19"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13212/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762316710281, "cdate": 1762316710281, "tmdate": 1762923902526, "mdate": 1762923902526, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}