{"id": "8c2SbG5PLj", "number": 5826, "cdate": 1757937838536, "mdate": 1759897951110, "content": {"title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection", "abstract": "Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: ***Rank-One Safety Injection (ROSI)***, a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. **ROSI** operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that **ROSI** consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that **ROSI** can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.", "tldr": "This paper introduces ROSI, a lightweight training-free method that amplifies safety in LLMs without training. The method can also be used realign uncensored LLMs.", "keywords": ["Large Language Models", "Alignment", "Safety", "Refusal"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad25b605a79ca642fbfd95c4ec6093b53cf5b914.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "ROSI enhances model security by guiding model activations to a rejection-related subspace, eliminating the need to fine-tune the original model and modifying weights with the help of residual flow. Experiments on llama guard3 and general tasks demonstrate that the proposed method improves the rejection rate of harmful requests while maintaining practicality."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "ROSI uses rank-one updates instead of multi-vector linear combinations, making it more lightweight. It tested various models and security evaluation datasets, and the results showed that it can improve the rejection rate. The paper is well-structured and the results are intuitive, making it a medium-quality empirical paper."}, "weaknesses": {"value": "**1. Lack of in-depth follow-up and reference to cutting-edge work such as AlphaEdit/AlphaSteer:**\n\n(1) The core idea of ​​ROSI is consistent with that of AlphaEdit [1], the best ICLR paper. However, there is no relevant citation and only a brief mention of \"our method is inspired by interpretability-based steering\". Both rely on extracting a direction vector in the model activation space and achieving good behavior control through linear intervention. In comparison, this paper has limited room for innovation and lacks in-depth discussion of the theoretical mechanism, assumptions and differences of activation guidance. \n\n\n(2) The discussion of Beyond Steering is very interesting. It focuses on research related to finetuning and red teaming outside of editing. It is recommended to add more supplements to highlight the focus of the work. \n\n\n**2. Lack of theoretical depth and mechanism analysis:**\n\nThere is no explanation of why rank-one injection can effectively capture or amplify the rejection subspace signal, nor is its theoretical advantage over multi-direction steering explained. There is no analysis of the stability of ROSI at different layers and different model sizes. The current results are only empirical observations and lack theoretical support.\n\n\n\nRef:\n\n[1] Fang J, Jiang H, Wang K, et al. Alphaedit: Null-space constrained knowledge editing for language models. ICLR'25"}, "questions": {"value": "1. Is there any inter-layer interference or redundancy when ROSI performs multi-layer intervention?\n\n2. Why haven't the experiments been replicated on the same security benchmark (such as the harmful-pairs dataset used by AlphaEdit) for direct comparison?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "raBfisubZr", "forum": "8c2SbG5PLj", "replyto": "8c2SbG5PLj", "signatures": ["ICLR.cc/2026/Conference/Submission5826/Reviewer_KkPq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5826/Reviewer_KkPq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036976322, "cdate": 1761036976322, "tmdate": 1762918289212, "mdate": 1762918289212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fine-tuning-free safety enhancement method, ROSI (Rank-One Safety Injection), which permanently strengthens LLM safety by injecting a single “safety direction” as a rank-one modification into the residual stream weights.\nThe method derives the safety direction from a small set of harmful/harmless instruction pairs and significantly improves refusal rates and robustness against jailbreak attacks, without impairing model capability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed method is conceptually simple yet effective, introducing only a lightweight rank-one modification that achieves substantial safety improvements across diverse models and benchmarks.\n\n2. The paper is well structured and clearly written, making the motivation, methodology, and experimental design easy to understand and follow even for readers outside the safety alignment community.\n\n3. The rank-one update is easy to implement, requires no retraining, and facilitates reproducibility and deployment."}, "weaknesses": {"value": "1. Could the learning of the safety direction be extended to a multi-dimensional subspace rather than a single vector?\n\nThis assumption may oversimplify the underlying representation of safety-related behaviors, which could be inherently multi-dimensional.\n\n2. The stability of the safety vector is not analyzed — is it highly sensitive to the specific set of harmful and harmless prompts used?\n\n3. Safety evaluation mainly relies on LLAMA GUARD 3; have the authors tested with multiple evaluators or different safety assessment models?"}, "questions": {"value": "See Weakness Section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FJGolN4w25", "forum": "8c2SbG5PLj", "replyto": "8c2SbG5PLj", "signatures": ["ICLR.cc/2026/Conference/Submission5826/Reviewer_1AT3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5826/Reviewer_1AT3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894121060, "cdate": 1761894121060, "tmdate": 1762918288806, "mdate": 1762918288806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Rank-One Safety Injection (ROSI), a lightweight method for enhancing safety alignment. The approach computes a safety vector in the representation space of LLMs and injects a rank-one matrix update along this direction into the model’s weights, achieving safety alignment enhancement without fine-tuning. The method is simple, interpretable, and demonstrates consistent effectiveness across multiple models and benchmark evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The ROSI method is simple and interpretable, with a clear mathematical formulation. It requires only a single rank-one weight modification, needs no additional training, and offers transparent and controllable operation.\nThe ROSI method enhances model safety while exerting minimal impact on general performance. Moreover, it introduces no additional runtime overhead, demonstrating strong practical value."}, "weaknesses": {"value": "The injection strength hyperparameter $\\alpha$ and the number of injection layers in ROSI may affect model stability. It is recommended to include additional ablation studies to clarify their potential impact on model safety and general performance.\nThe paper lacks adversarial evaluations against several classic jailbreak attacks methods, such as GCG [1], PAIR [2], RandomSearch [3], etc. Adding such attack–defense experiments would help further validate the method's effectiveness in enhancing model safety.\nThe paper lacks a direct comparison with other defense methods, such as SmoothLLM [4], Safe LoRA [5], Jailbreak Antidote [6], etc.\n[1] Zou, Andy, et al. \"Universal and transferable adversarial attacks on aligned language models.\" _arXiv preprint arXiv:2307.15043_ (2023).\n\n[2] Chao, Patrick, et al. \"Jailbreaking black box large language models in twenty queries.\" _2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)_. IEEE, 2025.\n\n[3] Andriushchenko, Maksym, Francesco Croce, and Nicolas Flammarion. \"Jailbreaking leading safety-aligned llms with simple adaptive attacks.\" _arXiv preprint arXiv:2404.02151_ (2024).\n\n[4] Robey, Alexander, et al. \"Smoothllm: Defending large language models against jailbreaking attacks.\" _arXiv preprint arXiv:2310.03684_ (2023).\n\n[5] Hsu, Chia-Yi, et al. \"Safe lora: The silver lining of reducing safety risks when finetuning large language models.\" _Advances in Neural Information Processing Systems_ 37 (2024): 65072-65094.\n\n[6] Shen, Guobin, et al. \"Jailbreak antidote: Runtime safety-utility balance via sparse representation adjustment in large language models.\" _arXiv preprint arXiv:2410.02298_ (2024)."}, "questions": {"value": "- Besides using the mean to extract safety vectors, could other mathematical approaches, such as principal component analysis (PCA), be employed? How would safety vectors extracted using different methods affect the performance of the approach?  \n- Could ROSI be extended to other value dimensions, such as honesty, to help mitigate hallucinations in large models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ygh3dohHxx", "forum": "8c2SbG5PLj", "replyto": "8c2SbG5PLj", "signatures": ["ICLR.cc/2026/Conference/Submission5826/Reviewer_N7TL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5826/Reviewer_N7TL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922688263, "cdate": 1761922688263, "tmdate": 1762918288418, "mdate": 1762918288418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ROSI (Rank-One Safety Injection), a white-box method for enhancing safety alignment in LLMs through permanent rank-one weight modifications. The approach extracts a \"safety direction\" from harmful/harmless instruction pairs using difference-in-means, then injects this direction into residual stream write matrices via the update rule W'_out ← W_out + α·ŝ·w̄^T. Experiments across aligned models (LLAMA, QWEN, GEMMA, YI) and uncensored models (DOLPHIN series) demonstrate improved harm refusal rates and jailbreak robustness with minimal utility degradation on standard benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. ROSI provides a lightweight alternative to expensive fine-tuning, requiring only 50 instruction pairs and simple weight modifications\n\n2. The paper tests across 13 models, multiple safety benchmarks (CATQA, HARMBENCH, WILDJAILBREAK), utility benchmarks (MMLU, HELLASWAG, ARC, etc.), and attack scenarios\n\n3. Demonstrating effectiveness on both aligned and uncensored models broadens the method's utility\n\n4. Tables 3 and 6 show remarkably stable performance across capability benchmarks (typically <0.5% average change)\n\n5. The method maintains transparency about what is being modified and why, unlike black-box fine-tuning approaches"}, "weaknesses": {"value": "1. Why is w̄·\\hat{s}^T the right rank-one update? The paper doesn't justify this choice over alternatives like random projections or learned directions. An ablation comparing different rank-one formulations would strengthen the claims.\n\n2. The paper states l* is \"selected based on a validation set\" but provides no details about this validation procedure, what metrics were optimized, or how many layers were tested.\n\n3. Only 50 harmful/harmless pairs seems quite small. What's the variance across different samples?\n\n4. The safety system prompt approach (Figure 2, Appendix A) seems somewhat circular—you're using a prompt to elicit safety behavior, then trying to make that permanent. How robust is this to variations in the prompt? The ❢ ablations suggest this is fragile for smaller models."}, "questions": {"value": "1. Which layers benefit most from ROSI? Did you try layer-specific \\alpha values or applying ROSI to only a subset of layers?\n2. Does a safety direction extracted from one model transfer to architecturally similar models? This could have interesting implications for safety."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rfurpkqjcs", "forum": "8c2SbG5PLj", "replyto": "8c2SbG5PLj", "signatures": ["ICLR.cc/2026/Conference/Submission5826/Reviewer_jJC3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5826/Reviewer_jJC3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5826/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988655924, "cdate": 1761988655924, "tmdate": 1762918288045, "mdate": 1762918288045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}