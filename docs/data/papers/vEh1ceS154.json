{"id": "vEh1ceS154", "number": 7931, "cdate": 1758044192033, "mdate": 1759897821501, "content": {"title": "Partition Generative Modeling: Masked Modeling Without Masks", "abstract": "Masked generative models (MGMs) are widely used to capture complex data and enable faster generation than autoregressive models (AR) through parallel decoding.\nHowever, MGMs typically operate on fixed-length inputs, which can be inefficient: early in sampling, most tokens are masked and carry little information, leading to wasted computation. In contrast, AR models process only tokens generated previously, making early iterations faster. \nIn this work, we introduce the ``Partition Generative Model'' (PGM), a novel approach that combines the strengths of AR and MGMs. Rather than masking, PGM partitions tokens into two groups and employs sparse attention to block information flow between them.\nSince there is no information flow between partitions, the model can process the previously-generated tokens only during sampling, while retaining the ability to generate tokens in parallel and in any order.\nOn OpenWebText, PGMs offer at least $5\\times$ improvements in sampling latency and throughput, while producing samples with superior generative perplexity, compared to Masked Diffusion Language Models. In the ImageNet dataset, PGMs achieve up to $7\\times$ better throughput compared to MaskGIT with only a small change in FID. Finally, we show that PGMs are compatible with distillation methods for MGMs, enabling further inference speedups.", "tldr": "We show that it is possible to train masked generative models without using MASK tokens, resulting in efficiency gains at inference.", "keywords": ["masked generative modeling", "discrete diffusion", "masked diffusion language modeling", "diffusion language modeling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ddad910ca151d55a960a04fd4375d8fc10c25d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the PGM, a new framework with core architecture innovation that combines the strengths of AR and MGM. PGM partitions tokens into two disjoint groups and constrains attention such that each group predicts the other. This removes the need for explicit MASK tokens while preserving parallel decoding and arbitrary generation order and address the training inefficiency of MGM.\nExperiments on LM1B, OpenWebText, and ImageNet show that PGMs achieve up to 5–7× faster inference throughput than MDLM and MaskGIT, with similar or better metrics including perplexity and FID. PGMs also support distillation for additional speedups.\nThis work is overall sound to me, but I am not an expert in architecture design."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Good novelty: replacing masking with partitioning is a simple yet powerful idea that effectively unifies the efficiency of AR models with the flexibility of MGMs.\n2. Solid architectural design: The GroupSwap mechanism and partition-wise attention are well-motivated and carefully engineered to achieve the partition mechanism.\n3. The experimental results are strong, with improved performance for both text and image generation."}, "weaknesses": {"value": "1. While the PGM is motivated be the inefficiency of MDM training, the authors are encouraged to provide evidence to show faster learning/convergence of PGM than MDLM. This probably relates to the training stability."}, "questions": {"value": "1. Does \"PGM 8 / 8\"  mean 8 layers of encoder and 8 layers of decoder?\n2. The origin of the training instability. Do authors still observe this when PGM is trained without complementary masking."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LP1ol2z26W", "forum": "vEh1ceS154", "replyto": "vEh1ceS154", "signatures": ["ICLR.cc/2026/Conference/Submission7931/Reviewer_m818"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7931/Reviewer_m818"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760893608669, "cdate": 1760893608669, "tmdate": 1762919955174, "mdate": 1762919955174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes avoiding the repetitive computation of the `MASK` token in masked generative modeling (MGM), switching from the decoder-only MGM to an encoder-decoder architecture. The inference model is defined as:\n1. self-attention **only within known indices**.\n2. cross-attention swapping to unknown indices (with opposite-group masking to prevent leakage).\n3. cross-attention **only within unknown indices** (projecting to the embeddings of stage one).\n\nThe complexity is still O(L^2) (L = sequence length) but with a significantly smaller coefficient (encoder $(L-k)^2$ + decoder $k(L−k)$ per step when remaining sequence length = k). \nThe practical speedup is about 5x and is scalable, with comparable generation quality against MGM.\nDistillation-accelerated models maintains the acceleration against MGM."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The empirical benefit is strong: 5x faster than MGM (4.6x faster with nucleus sampling).\n\n- Complementary masking is a smart and original trick to let one training step effectively count as two steps.\n\n- Section 5.3: fair comparison against MDLM (MGM) by isolating the complementary masking trick.\n\n- The down-stream tasks spreads across image and language, and the evaluation is solid. Distillation is also explored, which improves the practical significance of the paper."}, "weaknesses": {"value": "- The fairness of Table 2's comparison is not immediately visible—I believe the fairness should outweigh matching performance. Since the paper switches from decoder-only to encoder-decoder architecture, controlling hyperparameters (width, head, depth and MLP width multipliers) seems crucial to get a fair comparison.\nIn LM1B, it is a good idea controlling parameter counts and comparing with PGM(6/6)\\~170M, but in OWT, that model is missing in the main text (only the dim. 1024 model is shown). I don't understand why it only appears in the appendix.\n\n- I don't understand the labels (5.3) (5.4) (5.5) in Figure 4 (right).\n\n- Minor: \"sparse attention\" is used to describe the masking mechanism, but I believe it is an overuse of the term, as the mask is not actually sparse—perhaps group-wise attention is more suitable."}, "questions": {"value": "- Except for the top-k/nucleus confident tokens, the computations are wasted. I wonder if it is possible to reuse these noisy states instead of re-initializing decoder queries at each denoising step?\n\n- The current decoder architecture is cross-attention-only—which makes it easy to control parameter count c.f. MDLM, but lacks the standard self-attention component. Have you thought about this variant?\n\n- The information exchange from known to unknown indices entirely relies on the swap xattention layer. I wonder if it is possible to do the exchange in each decoder layer instead? (Of course this will make the complimentary masking trick not possible.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MyKtVVbFCT", "forum": "vEh1ceS154", "replyto": "vEh1ceS154", "signatures": ["ICLR.cc/2026/Conference/Submission7931/Reviewer_DktD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7931/Reviewer_DktD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587835129, "cdate": 1761587835129, "tmdate": 1762919954645, "mdate": 1762919954645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new generative modeling framework for language modeling, termed Partition Generative Models (PGM), aimed at improving inference efficiency. Unlike Masked Generative Models (MGM), PGM avoids applying the forward process to masked tokens, thereby reducing computational cost. The authors present tailored architectural modifications, along with corresponding training and inference strategies, to enable efficient generation within this framework. Experimental results indicate that PGM achieves faster inference than existing MGM approaches while maintaining comparable generation quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of avoiding computation on masked tokens during inference, along with the corresponding training strategy, is interesting and effectively targets a key inefficiency in existing masked generative models.\n\n2. The empirical results demonstrate that PGM can significantly accelerate inference while maintaining generation quality comparable to other state-of-the-art generative models, supporting the practical value of the proposed approach.\n\n3. The paper is clearly written, well-structured, and easy to follow, making the technical contributions accessible to the reader."}, "weaknesses": {"value": "I did not identify any major weaknesses in this paper. I do, however, have one question for clarification:\n\nThe proposed training pipeline includes two prediction components that operate on the same batch of data, which suggests that training efficiency could potentially be better than MDLM. Could the authors provide quantitative results or analysis regarding training efficiency, such as training speed, computational cost, or resource usage compared to MDLM?"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ljcr4gb12z", "forum": "vEh1ceS154", "replyto": "vEh1ceS154", "signatures": ["ICLR.cc/2026/Conference/Submission7931/Reviewer_pTqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7931/Reviewer_pTqP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950544143, "cdate": 1761950544143, "tmdate": 1762919954195, "mdate": 1762919954195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce Partition Generative Models (PGMs), based on the observation that masked generative models (MGMs) waste compute on masked tokens, which contain no information. \nInstead of masking tokens, PGMs partition the input tokens into two disjoint groups and train the model to predict one group from the other.\nThis approach allows the model to process only unmasked tokens which eliminating the need for explicit masking and leads to significantly faster sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The GroupSwap layer and partition-aware transformer structure are well-motivated\n- Includes analyses of perplexity, latency, throughput, and ablations on masking vs. partitioning.\n- Strong empirical results across both text and image generation tasks, PGMs deliver substantial inference speedups (up to 7×) with little to no degradation in output quality."}, "weaknesses": {"value": "- The architectural details (e.g., data-dependent vs. data-independent queries) are dense and could be clarified or simplified, the paper is a bit difficult to follow.\n- The largest experiments are modest in size (268M parameters). It remains unclear if PGMs scale favorably compared to state-of-the-art large AR or diffusion model\n- No comparison against recent SOTA model non-autoregressive language models beyond MDLM."}, "questions": {"value": "- How does the choice of partition ratio (t) affect convergence and quality? Is it dynamically sampled or fixed?\n- why cant you use KVcache that would reduce the time complexity from sampling in MGM? Would PGM will be still faster?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LabNEsk09h", "forum": "vEh1ceS154", "replyto": "vEh1ceS154", "signatures": ["ICLR.cc/2026/Conference/Submission7931/Reviewer_YDst"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7931/Reviewer_YDst"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993670284, "cdate": 1761993670284, "tmdate": 1762919953840, "mdate": 1762919953840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}