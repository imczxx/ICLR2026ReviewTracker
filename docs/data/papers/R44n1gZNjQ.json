{"id": "R44n1gZNjQ", "number": 19337, "cdate": 1758295455347, "mdate": 1763722622713, "content": {"title": "Statistical Advantage of Softmax Attention: Insights from Single-Location Regression", "abstract": "Large language models rely on attention mechanisms with a softmax activation. Yet the dominance of softmax over alternatives (e.g., component-wise or linear) remains poorly understood, and many theoretical works have focused on the easier-to-analyze linearized attention. In this work, we address this gap through a principled study of the single-location regression task, where the output depends on a linear transformation of a single input token at a random location. Building on ideas from statistical physics, we develop an analysis of attention-based predictors in the high-dimensional limit, where generalization performance is captured by a small set of order parameters. At the population level, we show that softmax achieves the Bayes risk, whereas linear attention fundamentally falls short. We then examine other activation functions to identify which properties are necessary for optimal performance. Finally, we analyze the finite-sample regime: we provide an asymptotic characterization of the test error and show that, while softmax is no longer Bayes-optimal, it consistently outperforms linear attention. We discuss the connection with optimization by gradient-based algorithms.", "tldr": "We demonstrate advantages of softmax-based attention over alternatives in a high-dimensional information retrieval regression task.", "keywords": ["attention", "softmax attention", "deep learning theory", "sparse information retrieval", "high-dimensional limit", "high-dimensional statistics"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d05ddef05e2d53e56fa62e41e85d41f9913d4821.pdf", "supplementary_material": "/attachment/44b82548cba3a05ec9eec92f65ad15b138b9ae2f.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies why softmax attention often outperforms linear attention by analyzing a stylized but mathematically tractable setting called Single-Location Regression (SLR). In this setup, only one randomly chosen token in a sequence carries useful information for predicting the target. The authors derive population-risk expressions showing that softmax attention achieves the Bayes risk (i.e., the minimum possible expected loss), while linear attention is inherently suboptimal. They further extend the analysis to finite-sample training using replica methods and confirm that gradient-based optimization empirically follows the predicted risk curves. The work provides a clear theoretical explanation for the statistical advantage of softmax normalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a principled and analytically clean setup (SLR) that isolates the retrieval aspect of attention mechanisms.\n\n- Derives explicit population-risk expressions demonstrating when and why softmax outperforms linear attention.\n\n- Extends the analysis to the finite-sample regime using replica theory, connecting asymptotic analysis with realistic training outcomes.\n\n- Numerical simulations show strong alignment with theoretical predictions, suggesting the model captures key phenomena of attention mechanisms.\n\n- Offers valuable theoretical insight into the long-standing question of softmax’s statistical advantage, which is of interest to both theoretical and applied communities."}, "weaknesses": {"value": "1. Limited task scope. The analysis focuses solely on the single-location regression (SLR) task, which captures a narrow form of retrieval behavior and does not generalize to multi-token dependencies or compositional reasoning.\n\n2. Restricted model comparison. The study only contrasts softmax and linear attention, omitting other relevant variants such as kernelized softmax approximations and state-space models (SSMs), which prevents a broader understanding of where softmax’s advantage truly lies.\n\n3. Reliance on idealized assumptions. The theoretical results depend on Gaussian i.i.d. token embeddings and replica-symmetry assumptions; robustness to more realistic distributions and correlated features is not explored.\n\n4. Absence of realistic experiments. All evaluations are conducted on synthetic data, with no validation on real-world or language-based datasets, leaving uncertain whether the observed statistical gap translates into practical performance gains."}, "questions": {"value": "Please refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "80ptLRimSU", "forum": "R44n1gZNjQ", "replyto": "R44n1gZNjQ", "signatures": ["ICLR.cc/2026/Conference/Submission19337/Reviewer_J4sf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19337/Reviewer_J4sf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958161144, "cdate": 1761958161144, "tmdate": 1762931278835, "mdate": 1762931278835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the rational behind why softmax attention always outperform linear attention. They use an analysis called Single Location Regression, denoted as SLR. In that setting, the randomly chosen token in a sequence carries useful information for predicting the final target. The author also derive the population risk expression to show that softmax can achieve the Bayes risk. But that cannot be achieve by linear attention. \n\nThen, they extend to the training paradigm with finite samples. Overall, this work investigate the rational and advantage of softmax normalization from a theoretical perspective, which is quite interesting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 This paper provide a clean math framework SLR to analyze the attention mechanism. \n\n2 The author derive from population-risk and showing that softmax attention can better perform linear attention from a Bayes risk perspective, which quite interesting and intuitive. \n\n3 The author extend the analyze to finite sample using replica theory, which connects their anlaysis to possible real world setting. \n\n4 Numerical experiments supports the theoretical analysis, which supports the analyze framework captures the key nature of the attention mechanism. \n\n5 This paper offers valuable theoretical insight into the long-standing question of softmax’s statistical advantage, which is of interest to both theoretical and applied communities."}, "weaknesses": {"value": "1 Limited task scope. The analysis focuses solely on the single-location regression (SLR) task, which captures a narrow form of retrieval behavior and does not generalize to multi-token dependencies or compositional reasoning.\n\n2 Experiments only contrast softmax and linear attention, lack of studying other relevant variants such as kernelized softmax approximations and state-space models, which prevents a broader understanding of where softmax’s advantage truly lies.\n\n3 Assumptions are too idealize. The theoretical results depend on Gaussian i.i.d. token embeddings and replica-symmetry assumptions. \n\n4 Absence of realistic experiments. All evaluations are conducted on synthetic data, with no validation on real-world or language-based datasets, leaving uncertain whether the observed statistical gap translates into practical performance gains."}, "questions": {"value": "Please refer to the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "80ptLRimSU", "forum": "R44n1gZNjQ", "replyto": "R44n1gZNjQ", "signatures": ["ICLR.cc/2026/Conference/Submission19337/Reviewer_J4sf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19337/Reviewer_J4sf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958161144, "cdate": 1761958161144, "tmdate": 1763693512371, "mdate": 1763693512371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the theoretical foundations of softmax attention by introducing a principled single-location regression (SLR) model. The authors derive analytical and asymptotic results using statistical physics tools (notably order parameters and replica analysis) to show that softmax attention achieves Bayes-optimal performance in high-dimensional limits, while linear attention and other alternatives (e.g., kernelized or element-wise nonlinearities) fundamentally fall short. The paper further provides a finite-sample characterization of test risk, confirming the statistical and computational advantages of softmax. Overall, it offers a clean theoretical framework that bridges information retrieval toy models and high-dimensional generalization analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Clear and original theoretical contribution:\nThe paper introduces a new analytical framework — the Single-Location Regression (SLR) model — to study the statistical behavior of attention mechanisms. This formulation unifies previous “needle-in-a-haystack”-type setups under a mathematically tractable regime, enabling a principled comparison between softmax and linear attention.\n\nMethodological depth:\nThe work combines sequence multi-index models with replica-based high-dimensional analysis, bringing together tools from statistical physics and modern learning theory. This cross-disciplinary approach is technically sophisticated and extends recent progress in the theoretical understanding of attention networks."}, "weaknesses": {"value": "Idealized task setting:\nThe SLR model assumes the label depends on a single token’s linear transformation, which, while analytically convenient, is far from the multi-head, multi-layer structure of real Transformers. Hence, the practical relevance is limited.\n\nLimited empirical grounding:\nThe validation is restricted to synthetic setups, without experiments on realistic retrieval or sequence tasks. It remains unclear whether the predicted statistical advantage of softmax can be observed in real neural models."}, "questions": {"value": "Can the proposed Single-Location Regression framework be extended to multi-location or multi-head attention, where multiple tokens jointly determine the output? Would the Bayes-optimality of softmax still hold in those cases?\n\nThe analysis assumes Gaussian and independent token embeddings. How sensitive are the results to these assumptions? Would correlated or structured embeddings affect the theoretical conclusions?\n\n\nHave the authors tested whether the predicted statistical gap between softmax and linear attention appears in small-scale Transformer experiments or synthetic retrieval tasks (e.g., Needle-in-a-Haystack)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "elY0l3pGcm", "forum": "R44n1gZNjQ", "replyto": "R44n1gZNjQ", "signatures": ["ICLR.cc/2026/Conference/Submission19337/Reviewer_hz39"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19337/Reviewer_hz39"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966912062, "cdate": 1761966912062, "tmdate": 1762931278305, "mdate": 1762931278305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common answer"}, "comment": {"value": "We thank the reviewers for their comments and for highlighting the quality, originality, and anticipated importance of our article. We uploaded a revised version of the manuscript implementing the suggestions, and below we answer the common remarks or questions the reviewers raised. \n\n**Idealized task setting, limited task scope:** we agree that understanding compositional reasoning, multi-token dependencies or more complex query mechanisms are major goals. Yet, the choice of our model, the SLR task, is not a weakness, for the following reasons:\n* On a theoretical point of view, the analysis we carried is challenging enough so that it has never been carried before. According to reviewer *hz39*, it is a clear and original theoretical contribution. When aiming to understand multi-token models, it is a clear prerequisite to have a detailed understanding of a single-token dependency model first. As reviewers *J4sf* and *tjje* highlight, the SLR provides a principled and analytically clean setup. Our analysis can be extended to a multi-token setting using the same tools and the same framework, with a considerable aditional work and some new technical elements; as such, the present paper is the first necessary step toward analyzing more complicated tasks and architectures.\n* The SLR task gives a clear explanation of the better performances of the softmax, over linear or kernalized attention, which is a crucial topic for transformers and LLMs. As such, it is of interest to both theoretical and applied communities, as highlighted by reviewer *J4sf*. We believe that presenting a minimal setting where an advantage of a widely used ingredient -- such as softmax -- can be  clearly demonstrated is scientifically valuable. Our opinion is that considering other more complex settings in a single paper would dilute the message and the presented intuition. But we agree that those are all interesting questions and we certainly hope that our paper will motivate ample follow-up work in these directions.  \n* As we show, even a single-token dependency presents a rich phenomenology with multiple minima of the risk, that was not previously described.\n\n**Extension to more realistic distributions and correlated features:** our work can definitively be adapted to embeddings that have a richer structure. One can assume a covariance with a well-defined asymptotic spectral density, or a hidden random manifold on which the features are embedded, and adapt the equations of https://journals.aps.org/prx/abstract/10.1103/PhysRevX.10.041044 for one-pass GD to the attention on the SLR. We expect that the phenomenology would be different and dependent on the structure of the correlations. For instance, if all the tokens have a larger covariance in a direction aligned with the signals $k^\\*$ and $v^\\*$, we can expect a mismatch where the values $v$ align with $k^\\*$ and the keys $k$ with $v^\\*$.\n\n**Validation on realistic experiments:** we appreciate the reviewers' emphasis on empirical validation in realistic settings. In the case at hand, a substantial body of prior work has already documented the empirical advantages of softmax attention over several alternatives, including kernelized-attention methods and state-space models. Because these results are well established, we focused our contribution on the theoretical aspects rather than attempting to replicate findings that are already consistent across the literature. That said, we agree that explicitly summarizing this evidence strengthens the motivation for our study. In the revised manuscript, we provide a more comprehensive overview of these empirical results, including a detailed discussion of the experiments  by Shen et al. (2024). In brief, their findings indicate that while optimally scaled SSMs and kernelized-attention models achieve competitive performance on linguistic proficiency benchmarks, they consistently underperform softmax attention on retrieval-oriented tasks. Again, we would like to stress that the fact that SLR-type tasks have been widely benchmarked in the literature attests its relevance to practice."}}, "id": "FKEYuiKAev", "forum": "R44n1gZNjQ", "replyto": "R44n1gZNjQ", "signatures": ["ICLR.cc/2026/Conference/Submission19337/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19337/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19337/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763722812061, "cdate": 1763722812061, "tmdate": 1763722812061, "mdate": 1763722812061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper theoretically explains why softmax attention outperforms linear attention by analyzing a single-location regression task  (Marion et al., 2025), where output depends on one hidden token, proving softmax achieves Bayes-optimal risk while linear attention fundamentally fails. \n\nBuilding on sequence multi-index models (Cui et al. 2024, Troiani et al. 2025), the authors extend these techniques to variable-length sequences and characterize both population and finite-sample risks via replica methods, showing that exponential nonlinearity and normalization are both critical. \n\nExperiments validate theoretical predictions with agreement between predicted and observed test error curves."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors provide theoretical results establishing provable performance gaps between softmax and linear attention ( by analyzing a single-location regression task ) at the population level (Propositions 4.1-4.4, Corollaries 4.2-4.3).\n\n\nThe paper successfully extends sequence multi-index model techniques to handle the challenging softmax nonlinearity through careful renormalization and order parameter analysis. \n\nThe authors provide strong empirical validation. Code was shared but I didn't check.  The experiments provide quantitative agreement between theoretical predictions and gradient-based optimization experiments across multiple settings (different signal strengths ν, sequence lengths L, regularizations)."}, "weaknesses": {"value": "- No optimization theory/landscape analysis is provided, and this is indeed a limitation as the paper proves softmax's empirical risk minimizer has superior statistical properties but provides no characterization of the optimization landscape or guarantees connecting estimation to optimization. The empirical risk (15) is non-convex with multiple local minima (Appendix A.2.6), yet the paper offers no analysis of landscape geometry, convergence guarantees for gradient descent, or the estimation-optimization gap. \n\n- The main finite-sample characterization relies on the replica symmetry assumption from statistical physics, which lacks rigorous justification. This should be stated more prominently in the theorem statement itself, not buried in discussion. \n\n\n- Expectation notation: $E[\\cdot]$ vs. $\\mathbb{E}[\\cdot]$ inconsistently used  \n- Risk terminology: switches between \"test error,\" \"test risk,\" \"Bayes risk,\" \"Bayes error,\" and \"Bayes-optimal risk\". \n\n- \"Symmetric activation function\" in Corollary 4.1 needs precise definition.  \n\n\n- Equation (3) notation confusion: The notation $\\chi^* = (1/\\sqrt{D}) x \\otimes k^*$ uses $\\otimes$ which typically denotes tensor product, but context suggests matrix multiplication. Should be clarified or corrected.  \n\n- Proposition 4.2, condition (6): States \"for all $L > 0$\" but $L$ is sequence length (positive integer), should be \"for all $L \\ge 1$\" or \"for all $L \\in \\mathbb{N}$\".  \n\n- Asymptotic notation in eq. (12): The notation $E_{\\text{lin}} = (L/(L-1))(1/\\nu) + o_{\\nu\\to\\infty}(1)$ places subscript on little-o, which is non-standard. Should be $E_{\\text{lin}} = (L/(L-1))(1/\\nu) + o(1)$ as $\\nu \\to \\infty$.  \n\n- High-dimensional limit not precisely stated: The relationship between $N$, $D$, and $\\alpha = N/D$ in the limit is ambiguous. Is it $D \\to \\infty$ then $N \\to \\infty$? Or jointly? The order and dependence should be explicit early in the paper.  \n\n- Manifold assumption justification: The manifold $\\mathcal{M}$ is introduced abruptly on page 5. While Section A.2.2 shows it's invariant under gradient descent, more intuitive geometric/statistical interpretation would help readers understand why this restriction is natural.\n\n\n-  Equations (18)–(25)  present complex fixed-point equations with no intuitive explanation. What do these order parameters represent physically? Why these specific equations?  \n\n- Figure 1 misleading markers: Caption states \"markers on the lines are for readability only.\" If markers don't represent actual data points, they should be removed as they're potentially misleading.  \n\n- Figure 2 incomplete experimental details: Caption mentions regularizations \"tuned by grid search\" but doesn't specify the grid values in the experiments section. \n\n\n- Some figure legends are too small to read comfortably (especially Figure 3)."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rzV9UOB96F", "forum": "R44n1gZNjQ", "replyto": "R44n1gZNjQ", "signatures": ["ICLR.cc/2026/Conference/Submission19337/Reviewer_Uy5X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19337/Reviewer_Uy5X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050815968, "cdate": 1762050815968, "tmdate": 1762931277699, "mdate": 1762931277699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors do a study to show why softmax-based attention mechanism outperforms alternative attention variants (such as linear attention). To do so, they use a simplified “single-location regression” task in a theoretical framework. They show that in the infinite-data population limit, a softmax attention model achieves the Bayes-optimal prediction error, which cannot be reached by linear attention models. Furthermore, they also show, that even in the finite case, where no model can reach the Bayes risk, softmax attention still outperforms linear attention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper has the following strengths:\n\n1) The paper introduces a clear mathematical framework for studying attention mechanisms by formalizing a single-location regression model, in which the output depends on a single informative token in the input sequence. In this way, this setup generalized earlier theoretical studies by allowing random sequence lengths and mirrorying “needle-in-a-haystack” retrieval tasks.\n\n2) The authors develop a tractable high-dimensional analysis of attention layers using techniques from sequence mulit-index models that the softmax nonlinearity can be handles with a small set of parameters. What is quite exciting is that they show that compared to linear attention, only the softmax attention can achieve the Bayes-optimal error, while the linear attention is suboptimal.\n\n3) The study also examines the finite-sample regime (the real-world case) adding practical value to the theoretical analysis in (2). They show that the softmax model still holds an advantage compared to linear attention in the finite-sample setting."}, "weaknesses": {"value": "I think the paper can be further improved in these parts:\n\n1) The analysis in limited to the simplified single-location retrieval scenario. While this abstraction (which corresponds to using a fixed query token like [CLS]) is mathematically convenient and appropriate for the task, it does not cover cases where multiple tokens or more complex query mechanisms are involved. Real-world Transformers often deal with interactions among many relevant tokens and use learned queries, so I am not sure how the softmax advantage might play out in these scenarios.\n\n2) The paper’s findings are supported primarily by theory and synthetic experiments. All experiments are on simplified tasks defined by the authors (e.g. variants of the single-location regression or synthetic retrieval scenarios), which raises the question of how well the insights transfer to practical large-scale NLP tasks. As a result, the work does not empirically confirming that softmax’s advantage in more complex or realistic conditions."}, "questions": {"value": "Slightly off the topic, but recently, there has been an interest in softmax-1 activation [A, B, C], which is a version of softmax designed to prevent attending in the first (few tokens). Considering the similarities between it and the standard softmax, I wonder if the findings would hold there.\n\n[A] Miller, Attention is off by one, 2023\n\n[B]  Bondarenko et al., Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing, NeurIPS, 2023\n\n[C] Kaul et al., From Attention to Activation: Unravelling the Enigmas of Large Language Models, ICLR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "t4pAyCgg6M", "forum": "R44n1gZNjQ", "replyto": "R44n1gZNjQ", "signatures": ["ICLR.cc/2026/Conference/Submission19337/Reviewer_tjje"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19337/Reviewer_tjje"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762166478369, "cdate": 1762166478369, "tmdate": 1762947857816, "mdate": 1762947857816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}