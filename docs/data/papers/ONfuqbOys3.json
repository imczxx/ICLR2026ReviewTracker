{"id": "ONfuqbOys3", "number": 16869, "cdate": 1758269705621, "mdate": 1763128732852, "content": {"title": "DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems", "abstract": "Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce \\dejaq, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of \\dejaq, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.", "tldr": "We evolve training data tailored to a model’s current skill level using LLM-guided mutations, enabling more efficient RL-based training of reasoning models.", "keywords": ["LLM", "Reinforcement Learning", "Auto-Curricula", "Reasoning", "Quality-Diversity"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a721f51dd23323ea68ac15349c083b3f52b9978c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DéjàQ, a novel framework for improving the mathematical reasoning abilities of Large Language Models (LLMs) through dynamic dataset evolution. Instead of relying on static datasets, DéjàQ co-evolves a diverse set of synthetic problems alongside model training.\n\nThe core of the contribution lies in three LLM-guided mutation operators that generate new problems: a \"setting mutator\" that alters the problem's narrative context, a \"distractor mutator\" that adds irrelevant information, and a \"symbolic mutator\" that modifies the underlying mathematical structure.\n\nCrucially, the same model being trained is used to perform these mutations, creating an efficient, self-bootstrapping system. The paper also provides a thoughtful analysis of the validity of generated problems and the computational overhead of the system."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Novel Framework**: The framework is a novel and well-designed synthesis of evolutionary algorithms, curriculum learning, and self-improvement for LLM post-training.\n\n- **Strong Empirical Performance**: The method demonstrates significant and consistent improvements over well-chosen baselines on a variety of mathematical reasoning benchmarks, including both in-distribution and out-of-distribution tasks.\n\n- **Efficient Bootstrapping**: The use of the same model for both training and data generation is a key strength, eliminating the common reliance on more powerful external \"teacher\" models and making the system more self-contained and efficient.\n\n- **Thorough Analysis**: The paper includes a high-quality analysis of the method's potential failure modes, including the verifiability of generated problems over time (Section 5.2) and the practical resource implications (Section 5.3).\n\n- **Clarity of Presentation**: The paper is written with clarity, and the figures, particularly the system overview in Figure 1, are highly effective at conveying the core ideas."}, "weaknesses": {"value": "- **Unresolved Teacher-Student Lag**: The paper identifies a key limitation where the \"teacher\" does not improve with the \"student\", leading to a higher proportion of invalid problems among high-learnability candidates after training. While identifying this is a strength of the analysis, the paper frames it as future work and does not propose or test a mechanism to resolve it. This might limit the truly \"open-ended\" nature of the evolution in the long run without further modification.\n\n- **Scalability Questions**: The experiments are conducted on a 7B model. While the results are excellent, it remains an open question how the dynamics of this tightly coupled system would scale to much larger models (e.g., 100B+ parameters). For instance, the quality of mutations might improve, but the rate of student improvement might also accelerate, potentially exacerbating the teacher-student lag.\n\n- **Dependence on Initial Seed Data**: The evolutionary process begins from a seed set of problem templates (GSM-Symbolic). While the mutations, especially the symbolic one, introduce significant novelty, the framework may still be fundamentally constrained by the mathematical concepts present in the initial seed data. It is unclear if the system could evolve problems requiring entirely new types of reasoning not represented in the seed set."}, "questions": {"value": "- The symbolic mutator is arguably the most powerful operator. Could you provide a more detailed breakdown of its typical failure modes? For instance, beyond producing an incorrect final answer, how often does it generate problems that are logically inconsistent, ambiguous, or unsolvable?\n\n- The analysis in Appendix A.3 mentions the model spontaneously generating problems in Spanish, which highlights the open-ended nature of the system. Did you observe any other surprising emergent behaviors? Specifically, did the symbolic mutator ever introduce mathematical operations or concepts that were not present in the original GSM-Symbolic templates, thereby increasing the conceptual complexity of the archive?\n\n- In your related work, you cite Rainbow Teaming for its use of MAP-Elites and an archive to generate diverse problems. Have you considered citing OMNI-EPIC? It seems highly relevant and arguably closer to your work compared to Rainbow Teaming, as it also describes an open-ended evolutionary process that maintains an archive of generated tasks to create a curriculum of increasing difficulty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gwvGj9rIAS", "forum": "ONfuqbOys3", "replyto": "ONfuqbOys3", "signatures": ["ICLR.cc/2026/Conference/Submission16869/Reviewer_oZJP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16869/Reviewer_oZJP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760609614160, "cdate": 1760609614160, "tmdate": 1762926888533, "mdate": 1762926888533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DÉJÀQ, a self-bootstrapping framework that simultaneously evolves a curriculum of synthetic math problems and trains a 7B LLM via RL with verifiable rewards. Using MAP-Elites, it maintains a diverse archive of problem-answer pairs indexed by human-defined settings; three in-model mutators (setting, distractor, symbolic) continually rewrite problems, while an estimated learnability score filters offspring for neither-too-easy-nor-impossible instances. The same 7B model serves as both rollout model and mutation \"teacher\", yielding gains on GSM-Symbolic and MATH-500 without external labels or larger teachers."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The paper introduces a fully self-contained framework that co-evolves a curriculum of synthetic math problems and improves a solver without any external oracle or larger teacher. By uniting MAP-Elites, learnability-based filtering, and three LLM-driven mutators inside a single RLVR loop, it removes the usual dependency on hand-written templates or proprietary generators, which is promising for community adoption.\n\n* Experiments show consistent gains over benchmarks: +6–7 % accuracy on GSM-Symbolic subsets and +1.6 % on MATH-500, together with better tail robustness (CVaR).\n\n* Resource measurements prove the extra inference calls fit within the idle time of the existing rollout server, validating practical deployability."}, "weaknesses": {"value": "A substantive assessment of the weaknesses of the paper. Focus on constructive and actionable insights on how the work could improve towards its stated goals. Be specific, avoid generic remarks. For example, if you believe the contribution lacks novelty, provide references and an explanation as evidence; if you believe experiments are insufficient, explain why and exactly what is missing, etc.\n\n* After post-training, high-learnability pairs being increasingly likely to be invalid (Fig. 4), indicating the teacher can no longer invent genuinely new, correct problems as the student becomes stronger.\n\n* About \"Verifiable\": symbolic mutator lets the same model rewrite the question and supply the new chain-of-thought + ground-truth answer; there seems no adequate method to guarantee the validity or correctness."}, "questions": {"value": "1. In Section 5.2, the paper mentioned: \"Since our RLVR process optimises only the student's performance and leaves the teacher static, this mismatch likely exacerbates the problem.\" What exactly are the student and teacher in RLVR? Is it not the same model that trains and generates data? Why is the teacher static?\n\n2. What reward is used in RLVR? During data generation, is format checking required and should a format reward be used to guide the process?\n\n3. Section 4.4 lists many tricks without detailed explanations-could additional operational descriptions be added? After learnability scores decay, are high-learnability problems replenished during training? Otherwise, how is it ensured that problems are not selected repetitively while still keeping high-learnability problems for training?\n\n4. The paper mentioned: \"We do not evaluate the distractor or symbolic mutators in isolation, as they cannot produce cross-category mutations.\" In fact, only the symbolic mutator changes the problem answer and requires altering the computation steps in the output-why is the comparison focused only on whether the category changes?\n\n5. Why do the experimental results show improvement on symbolic tasks, yet on GPT-Eval-ID the performance of DÉJÀQ-A worsens while only DÉJÀQ-S improves, even though the invalidity of new problems generated by DÉJÀQ-S increases with training?\n\n6. The GSM dataset contains relatively simple problems. Could scalability and generality be issues for this method? The paper only conducts experiments on a 7B model; would smaller models struggle to generate high-quality questions and answers, while larger models can already solve most problems in the GSM dataset? Moreover, among the three mutation methods, none is designed to alter the difficulty level of the problems.\n\n7. The experimental baselines are too few; there is no comparison with performances of other same-scale open-source models.\n\n8. The classification prompt is NOT found in Appendix E."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EPP0kO1msy", "forum": "ONfuqbOys3", "replyto": "ONfuqbOys3", "signatures": ["ICLR.cc/2026/Conference/Submission16869/Reviewer_XXEE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16869/Reviewer_XXEE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876719787, "cdate": 1761876719787, "tmdate": 1762926887770, "mdate": 1762926887770, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Coment"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback and for recognising the potential of DéjàQ. We are encouraged that every reviewer independently highlighted the **practicality**, **robustness improvements**, and **clarity of presentation** of our work. For ease of review, all **revisions and newly added text are highlighted in blue** throughout the manuscript.\n\nIn particular:\n- All reviewers agree that our approach offers a **promising direction for scaling RLVR in LLMs**, effectively leveraging the *same inference infrastructure*.\n- All acknowledge that the paper is **well written and well motivated**, with remaining clarifications now addressed in the revised manuscript.\n- There is broad consensus that **DéjàQ substantially improves robustness and out-of-distribution generalisation**, confirming its value beyond standard RLVR post-training.\n\n**Alternative models** \n--\nWe acknowledge the request for experiments across multiple model families and sizes. However, such experiments would require rerunning the entire RLVR and curriculum-evolution pipeline, including all ablations, which exceeds our current resource budget. We therefore focused on a single strong open model (Qwen2.5-7B-Instruct) to ensure representativeness and reproducibility. Our goal is to analyse the algorithmic contributions such as learnability-based sampling, LLM-guided mutation, and evolutionary curriculum design, rather than architectural or scaling effects, which we view as complementary future work.\n\n**Summary of main changes** \n--\n- Added a subsection to the related-work section discussing prior work on methods for data curation (Section 3).  \n- Introduced Section 4.1 to provide a clearer overview of the training process of DéjàQ and to explain how the learnability-based scoring function and shared inference infrastructure are integrated.  \n- Updated Section 4.4 to provide additional context on the stabilisation techniques applied to the evolutionary process.\n- Included a detailed summary of all baselines in Appendix C.1.\n\n\nWe hope the clarifications and improvements provided in this revision will help reviewers converge towards a positive consensus."}}, "id": "89suASmImB", "forum": "ONfuqbOys3", "replyto": "ONfuqbOys3", "signatures": ["ICLR.cc/2026/Conference/Submission16869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16869/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16869/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763128839718, "cdate": 1763128839718, "tmdate": 1763128839718, "mdate": 1763128839718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comment"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful feedback and for recognising the potential of DéjàQ. We are encouraged that every reviewer independently highlighted the **practicality**, **robustness improvements**, and **clarity of presentation** of our work. For ease of review, all **revisions and newly added text are highlighted in blue** throughout the manuscript.\n\nIn particular:\n- All reviewers agree that our approach offers a **promising direction for scaling RLVR in LLMs**, effectively leveraging the *same inference infrastructure*.\n- All acknowledge that the paper is **well written and well motivated**, with remaining clarifications now addressed in the revised manuscript.\n- There is broad consensus that **DéjàQ substantially improves robustness and out-of-distribution generalisation**, confirming its value beyond standard RLVR post-training.\n\n**Alternative models** \n--\nWe acknowledge the request for experiments across multiple model families and sizes. However, such experiments would require rerunning the entire RLVR and curriculum-evolution pipeline, including all ablations, which exceeds our current resource budget. We therefore focused on a single strong open model (Qwen2.5-7B-Instruct) to ensure representativeness and reproducibility. Our goal is to analyse the algorithmic contributions such as learnability-based sampling, LLM-guided mutation, and evolutionary curriculum design, rather than architectural or scaling effects, which we view as complementary future work.\n\n**Summary of main changes** \n--\n- Added a subsection to the related-work section discussing prior work on methods for data curation (Section 3).  \n- Introduced Section 4.1 to provide a clearer overview of the training process of DéjàQ and to explain how the learnability-based scoring function and shared inference infrastructure are integrated.  \n- Updated Section 4.4 to provide additional context on the stabilisation techniques applied to the evolutionary process.\n- Included a detailed summary of all baselines in Appendix C.1.\n\n\nWe hope the clarifications and improvements provided in this revision will help reviewers converge towards a positive consensus."}}, "id": "89suASmImB", "forum": "ONfuqbOys3", "replyto": "ONfuqbOys3", "signatures": ["ICLR.cc/2026/Conference/Submission16869/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16869/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16869/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763128839718, "cdate": 1763128839718, "tmdate": 1763471073817, "mdate": 1763471073817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DejaQ, a framework for open-ended evolution of synthetic training data in reasoning domains, particularly mathematical problem solving. Instead of relying on static datasets, DejaQ co-evolves problem–answer pairs alongside model training using LLM-driven mutations (setting, distractor, symbolic). These mutations aim to increase dataset diversity and adapt difficulty to the model’s current capability. The authors integrate this approach with RLVR and MAP-Elites to manage diversity and learnability. Experiments with QWEN2.5-7B-INSTRUCT show performance gains over standard RL baselines and domain randomisation, especially in robustness and out-of-distribution generalisation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting integration of evolutionary search and RL-based fine-tuning for dataset generation.\n- Demonstrates a novel use of LLM-guided mutations that preserve verifiability while diversifying data.\n- Evidence that DejaQ improves robustness and OOD performance."}, "weaknesses": {"value": "- Should also show results on a different family of LLMs (e.g., llama) instead of just Qwen. Different families of LLMs might have different behaviours.\n- Another ablation of mutating the samples but not having the learning progress sampling would be useful to see which components contribute to the algorithm's overall performance.\n- The authors assume that GPT-5-mini is a \"reasonable\" oracle. A better scientific practice would be to show on a dataset or human annotations on how good GPT-5-mini is as a judge.\n- For the result of post-training and invalidity base rate, it would be interesting to see the same plot (fig 4) for each mutation separately. Since \"post-training raises the invalidity base rate for the setting mutator but lowers it for the all mutator\", seeing the changes for each type of mutation could give more insight into the differences between DejaQ-A and DejaQ-S.\n\n- Since defining learning progress is a key part of the paper, it is missing a lot literature on learning progress. \n\nThe concept of learning progress in prediction or curiosity-driven networks originates from Schmidhuber’s early work on artificial curiosity in 1991 (see historical overview in https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5508364). It was later formalized by Oudeyer and Kaplan in 2007 as a computational mechanism for intrinsic motivation (https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141061, https://pmc.ncbi.nlm.nih.gov/articles/PMC2533589/). Around 2013, Oudeyer’s group introduced the notion of competence progress, measuring improvement in goal achievement or task completion to drive exploration and skill acquisition (https://www.sciencedirect.com/science/article/pii/S0921889012000644 and related works from 2013–2014). Since 2018, this principle has been integrated into intrinsically motivated deep reinforcement learning frameworks (https://arxiv.org/abs/1810.06284, https://arxiv.org/abs/1906.08190). More recently, similar approaches have been applied in complex environments such as Minecraft (https://arxiv.org/pdf/2106.14876) and in LLM-guided data generation settings https://arxiv.org/abs/2306.01711)."}, "questions": {"value": "- The behaviour descriptors are handcrafted (i.e., manually inspected by the authors to come up with the templates). Could this part be potentially automated? e.g., approaches like QDAIF (https://arxiv.org/abs/2310.13032) or ACES (https://arxiv.org/abs/2310.10692)\n- As with Goodhart's law, when a measure becomes a target, it ceases to be a good target. Did they authors see any pathologies happening when optimizing for the proposed learning progress metric? Discussion on how this issue could be solved would be useful.\n- It is said that the \"initial learnability become stale as the model improves\", and so the \"learnability scores are decayed over time\". It would be useful to see an ablation whereby the learnability scores are recalculated, to see how much of this is a problem.\n- The authors show an ablation of keeping the same evolutionary process but resampling from the initial dataset. Is the resampling based on the learning progress metric in this baseline?\n- How do the authors know quantitatively/ qualitatively if a task set is out-of-distribution?\n- What is the \"risk parameter alpha\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "70k0ZR8SPw", "forum": "ONfuqbOys3", "replyto": "ONfuqbOys3", "signatures": ["ICLR.cc/2026/Conference/Submission16869/Reviewer_ZWDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16869/Reviewer_ZWDX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974664300, "cdate": 1761974664300, "tmdate": 1762926887085, "mdate": 1762926887085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces DéjàQ, a system for RLVR post-training where the dataset is evolved by a curriculum as well as LLM mutation. Evaluation results with Qwen2.5-7B-Instruct show the merits of this approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a novel application of LLM mutators (to RLVR post-training)\n2. The fact that the dataset evolution process can reuse the same inference infrastructure makes the approach more practical.\n3. There are detailed analyses on robustness, maintaining verifiability, and resource requirements and hardware utilization.\n4. The paper is well-written."}, "weaknesses": {"value": "1. The paper would benefit from a clearer or more detailed description of the training process of DéjàQ and baselines. See Questions 2 and 3 below. (For example, the paper currently reads as though the training set and test set could’ve been identical (or at least the set of GSM-Symbolic templates used is identical across the training set and test set), and clarification from the authors would be appreciated.)\n\n2. The fact that the RLVR baselines result in *worse* performance than the base model suggests that they were not implemented/engineered/tuned properly, since properly implemented RLVR should not result in worse performance. If that is indeed the case, then comparison with these baselines is not meaningful."}, "questions": {"value": "1. Line 300 says “We do not evaluate the distractor or symbolic mutators in isolation, as they cannot produce cross-category mutations.” Why does the inability to produce cross-category mutations justify not evaluating distractor/symbolic mutators in isolation?\n\n2. Could you explain more about how the seed training set is generated? Lines 192-193 say that the seed training set is GSM-Symbolic, but I’m not aware of an explicit training split for GSM-Symbolic. (https://huggingface.co/datasets/apple/GSM-Symbolic only contains a test split.)\n\n3. Can you describe the “resampling” baseline in more detail? The current explanation (line 297) is not very clear to me.\n\n4. In Algorithm 1, how often is the dataset evolved? In other words, how many iterations of (2) occur for every iteration of (1)?\n\n5. Some of the benchmarks (GPT-Eval-ID, GPT-Eval-OOD) were generated by an LLM. How was it ensured that these synthetically generated benchmarks are of high quality (e.g., are error-free)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zAUZJRNCgQ", "forum": "ONfuqbOys3", "replyto": "ONfuqbOys3", "signatures": ["ICLR.cc/2026/Conference/Submission16869/Reviewer_gWP3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16869/Reviewer_gWP3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16869/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762639835181, "cdate": 1762639835181, "tmdate": 1762926886385, "mdate": 1762926886385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}