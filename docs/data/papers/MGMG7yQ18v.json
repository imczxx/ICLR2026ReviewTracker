{"id": "MGMG7yQ18v", "number": 14811, "cdate": 1758244274593, "mdate": 1759897347839, "content": {"title": "Ice Cream Doesn’t Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference", "abstract": "Reliable causal inference is essential for making decisions in high-stakes areas like medicine, economics, and public policy. However, it remains unclear whether large language models (LLMs) can handle rigorous and trustworthy \\textit{statistical causal inference}. Current benchmarks usually involve simplified tasks. For example, these tasks might only ask LLMs to identify semantic causal relationships or draw conclusions directly from raw data. As a result, models may overlook important statistical pitfalls, such as Simpson’s paradox or selection bias. This oversight limits the applicability of LLMs in the real world.\nTo address these limitations, we propose \\textbf{CausalPitfalls}, a comprehensive benchmark designed to rigorously evaluate the capability of LLMs in overcoming common causal inference pitfalls. Our benchmark features structured challenges across multiple difficulty levels, each paired with grading rubrics. This approach allows us to quantitatively measure both causal reasoning capabilities and the reliability of LLMs' responses. We evaluate models using two protocols: (1) direct prompting, which assesses intrinsic causal reasoning, and (2) code-assisted prompting, where models generate executable code for explicit statistical analysis. Additionally, we validate the effectiveness of this judge by comparing its scoring with assessments from human experts.\nOur results reveal significant limitations in current LLMs when performing statistical causal inference. The CausalPitfalls benchmark provides essential guidance and quantitative metrics to advance the development of trustworthy causal reasoning systems.", "tldr": "We present CausalPitfalls, a benchmark that systematically evaluates LLMs' reliability in causal inference, revealing how models can fall into statistical traps like Simpson’s or Berkson's paradoxes, even when their answers appear accurate.", "keywords": ["Large Language Model", "Causal inference"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebb0d2f9a72735a985d2ced3c140064f3f4a9150.pdf", "supplementary_material": "/attachment/773b5f77db81b3bb55f6193f53d9ccd59638cb82.zip"}, "replies": [{"content": {"summary": {"value": "LLMs have commonly been assessed for causal reasoning and inference tasks; however typically only a subset of these tasks - or plain commonsense reasoning - is evaluated in any benchmark. This paper proposes an exhaustive summary of pitfalls in causal inference to be verified, and proposes an automated pipeline with a LLM to rate answers - this LLM is shown to be in agreement with a large pool of human graders."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Original approach, and first paper I see that extensively reviews all causal inference skills in LLMs. The list of tasks looks exhaustive, and the taxonomy by difficulty looks relevant and consistent with results.\n\n- The automatic grading with gpt-4o is a welcome innovation and seems validated by external graders"}, "weaknesses": {"value": "- It is not detailed where exactly the previous evaluations in the literature are missing wrt the challenges in Table 1. A table detailing this would be welcome.\n\n- The process to generate DAGs in l.245-253 is a bit unclear. No precise equations are given. In the sentences, \"For every statistical pitfall, we select causal graphs that capture its unique complexities and characteristics. Each challenge is accompanied by five distinct datasets, each containing over 500 samples for comprehensive evaluation.\", it is unclear whether five daatsets are generated by graph, or five graphs are chosen by task? Also, the sentence \"For every statistical pitfall, we select causal graphs that capture its unique complexities and characteristics. Each challenge is accompanied by five distinct datasets, each containing over 500 samples for comprehensive evaluation.\" suggests that only linear SCMs are evaluated, which would be restrictive.\n\n- It is unclear how scores are computed exactlty. I understand that points are assigned to questions, but what's the exact assignment? And why assign more than 1 point to each question (scores could then be normalized on a 0-100% scale)? It is also unclear how gpt-4o is prompted to evaluate models, or is evaluation is done exactly with it. Does it choose points itself? Or is it Correct/Incorrect, with Correct giving all points and Incorrect giving none?"}, "questions": {"value": "- Can you address/answer the above weaknesses?\n\n- To double check that the discrepancy of 0.11 between gpt-4o and human scores is low, can you also compute it in these cases: a) scores are all zero, b) scores are all maximal, c) scores are uniformly sampled between zero and the maximal possible value (averaged over several seeds).\n\n- Did/Can you test the evaluation pipeline on statistical or causal libraries (eg DoWhy), or on humans?\n\nNote: I am ready to increase my score up to Accept levels if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "k7QFSzJjoF", "forum": "MGMG7yQ18v", "replyto": "MGMG7yQ18v", "signatures": ["ICLR.cc/2026/Conference/Submission14811/Reviewer_gZXP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14811/Reviewer_gZXP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761234582810, "cdate": 1761234582810, "tmdate": 1762925159762, "mdate": 1762925159762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CausalPitfalls, a benchmark to test whether LLMs can do statistical causal inference without falling into classic traps like Simpson’s paradox, selection/collider bias, et.c. It shows that current models often give confident but statistically wrong causal conclusions that are highly sensitive to phrasing, and that letting them run code helps but doesn’t solve the problem overall.\n\nContributions:\n\n* 6 causal-pitfall families, 15 challenges, 75 questions + 75 SCM-based datasets, each with 5 difficulty levels, targeting: confounding/spurious association, interventions vs observation, counterfactuals, mediation, causal discovery, and external validity/transport.\n* (1) Direct prompting to measure intrinsic causal reasoning from raw data; (2) Code-assisted prompting where the model generates and runs statistical code before answering. This separates questions of “can the model spot the pitfall?” and “can the model reason its way out of it?”\n* A rubric-based, normalized metric for causal reliability validated against human statisticians to compare models on trustworthiness, not just accuracy.\n* Illustrative failure cases including models flipping answers based on branding (“HealthPlus” vs “UltraSugar”) on identical data; falsely detecting gender bias / Simpson’s paradox with age stratification in synthetic medical data, falsely treating Berkson’s/collider bias in hospital data as a real effect, over-attributing effects in mediator–outcome confounding setups, fail to detect non-identifiability/Markov equivalence in causal discovery.\n* Empirically findings: the best model reaches only ~40–43% causal reliability, mediation and external validity are especially difficult"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "In my view, the main strength of the paper is demonstrating cases where the LLM fails to resolve common pitfalls in causal inference across a broad set of causal inference problems, using generative causal graphical model ground truth. The paper then extends this analysis into a formal benchmark with a useful metric focused on evaluating LLMs for how reliable they are in answering these types of questions."}, "weaknesses": {"value": "The benchmark relies heavily of the LLMs ability to parse raw data."}, "questions": {"value": "Could you have converted the raw data into sufficient statistics sufficient to resolve the causal inference problems (e.g., counts, proportions, rates, descriptions that imply monotonicity in case of probabilities of causation, etc.), then construct prompts based on these sufficient stats? One could do this without giving cues to the LLM about the right way to solve a problem (e.g., providing additional summary statistics that not needed to correctly do the inference). This would have better evaluated a models' ability to \"talk\" its way through the inference problem and less dependent on its ability to parse raw data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6p9t5VIjHo", "forum": "MGMG7yQ18v", "replyto": "MGMG7yQ18v", "signatures": ["ICLR.cc/2026/Conference/Submission14811/Reviewer_iHLz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14811/Reviewer_iHLz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938902880, "cdate": 1761938902880, "tmdate": 1762925159330, "mdate": 1762925159330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CausalPitfalls, a benchmark designed to evaluate the capability of LLMs in recognizing and thus overcoming common causal inference pitfalls. The authors claim this will improve LLMs' capability of handling statistical causal inference. They show this by getting LLM generated text and code evaluated by human experts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Advancing causal inference research is a welcome topic in the field of LLMs\n2. The structure of the benchmark and the design aspect of it seems well-thought out and illustrated\n3. Having human evaluation for LLM generated content is key"}, "weaknesses": {"value": "1. The datasets are at the end of the day still synthetic and not drawn from real observational/experimental studies\n2. It is not clear how well this will generalize to real world scenarios\n3. It would be helpful if the authors expanded a bit more on what is an acceptable success rate at these tasks. How well do humans do?"}, "questions": {"value": "1. Do the authors fear that benchmarks for LLMs could be a losing cause? For example, is it possible that this benchmark becomes \"training data\" - in other words can LLMs be gamed to do well on benchmarks but actually still be bad at causal inference?\n2. Having said that, do the authors have any plans to extend their work to include more types of pitfalls in the future?\n3. What if you tell LLMs that they were wrong and they should try again. Do you expect these second chance answers to improve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "egbXaM8ytg", "forum": "MGMG7yQ18v", "replyto": "MGMG7yQ18v", "signatures": ["ICLR.cc/2026/Conference/Submission14811/Reviewer_Z3sS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14811/Reviewer_Z3sS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978187388, "cdate": 1761978187388, "tmdate": 1762925158769, "mdate": 1762925158769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CausalPitfalls, a benchmark designed to evaluate large language models (LLMs) on statistical causal inference tasks while specifically testing their susceptibility to classical causal reasoning errors. The benchmark comprises 75 datasets and 15 challenges across six categories, including confounding, interventions, counterfactual reasoning, mediation, causal discovery, and external validity. It assesses two protocols, direct prompting and code-assisted prompting, and proposes a quantitative metric called causal reliability to standardize comparison across models. Results show that all evaluated LLMs exhibit systematic reliability gaps, particularly on tasks involving mediation and generalization, with moderate improvements observed when code execution is permitted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a timely and significant contribution by systematically targeting the reliability rather than accuracy of LLMs in causal inference, an aspect underexplored in previous benchmarks such as CausalBench and CLADDER. The structured coverage of classical statistical pitfalls like Simpson’s and Berkson’s paradoxes, mediation confounding, and counterfactual reasoning reflects a solid understanding of the causal inference literature and provides a rigorous stress test for LLMs. The benchmark’s dual evaluation protocol is well-motivated and demonstrates a clear methodological insight: executable statistical reasoning through code-assisted prompting substantially mitigates causal errors that stem from linguistic biases. The proposed causal reliability metric is also a valuable addition that enables consistent quantitative comparisons across diverse causal reasoning challenges."}, "weaknesses": {"value": "Although overall a good contribution, The benchmark lacks a bit in scope and interpretation. Although the benchmark convincingly demonstrates single LLM limitations, the current coverage only includes very basic prompting stratagies. Current causal inference methods don't use LLMs directly anyways, benchmarking more architectures like finetuned models for causal inference, or better prompting stratagies like React would be more helpful. Moreover, while the paper positions code-assisted prompting as a key improvement, it does not sufficiently analyze why certain models benefit more than others or what kinds of causal reasoning errors persist even after code execution. Without this diagnostic breakdown, the benchmark feels more descriptive than explanatory."}, "questions": {"value": "Refer to the weaknesses section above, additionally, How prevalent are the main pitfalls highlighted in the benchmark within real-world datasets, and to what extent do they typically affect practical causal analyses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fshEOBE4VF", "forum": "MGMG7yQ18v", "replyto": "MGMG7yQ18v", "signatures": ["ICLR.cc/2026/Conference/Submission14811/Reviewer_fhn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14811/Reviewer_fhn1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14811/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996297394, "cdate": 1761996297394, "tmdate": 1762925158319, "mdate": 1762925158319, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}