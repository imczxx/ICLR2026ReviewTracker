{"id": "PyPqhQSJKz", "number": 19261, "cdate": 1758294881419, "mdate": 1763643705563, "content": {"title": "Forward-only Diffusion Probabilistic Models", "abstract": "This work presents a forward-only diffusion (FoD) approach for generative modelling. In contrast to traditional diffusion models that rely on a coupled forward-backward diffusion scheme, FoD directly learns data generation through a single forward diffusion process, yielding a simple yet efficient generative framework. The core of FoD is a state-dependent stochastic differential equation that involves a mean-reverting term in both the drift and diffusion functions. This mean-reversion property guarantees the convergence to clean data, naturally simulating a stochastic interpolation between source and target distributions. More importantly, FoD is analytically tractable and is trained using a simple stochastic flow matching objective, enabling a few-step non-Markov chain sampling during inference. The proposed FoD model—despite its simplicity—achieves state-of-the-art performance on various image restoration tasks. Its general applicability on image-conditioned generation is also demonstrated on diverse image-to-image translation tasks.", "tldr": "This paper proposes a simple and effective forward-only diffusion model for image generation.", "keywords": ["Diffusion models", "image generation", "image restoration", "computer vision application"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cb230700d6448370cb9fded1e489108972a494e.pdf", "supplementary_material": "/attachment/ce8c44a4cae2de8725835415698204f4265c5a87.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a probabilistic forward-only diffusion model (FoD) that incorporates Geometric Brownian Motion (GBM) to transform an image into another that is in two distributions separately. The authors claimed that by introducing a mean-reversion term $\\mu - x_t$​ into both the drift and diffusion coefficients of the SDE, they defined an analytically solvable forward-only process, eliminating the need to approximate or learn a reverse SDE. However, I have several concerned. First, I believe this claim of FoD is actually a backward diffusion (as I explained in Weakness section).  Second, in conventional diffusion models, the process involves diffusing an image  to noise (forward), and then reconstructing from noise to an image that closely remember the original image (reverse) although one can always  transform between two deterministic images. The setup in this paper actually resembles the GOUB., more recently; UNIDB. Although the authors proposed to  add the state dependency into the diffusion coefficient, there is no proof of the effect of this diffusion. Indeed the author almost has not discussion on the coefficients. Instead, the authors used extremely small diffusion coefficients, this made it actually more close to deterministic method, reducing the diffusion effects, which I believe making this paper relatively weak contribution to the field."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well written. The authors effort to consider diffusion coefficients as state dependent although the implementation is weak."}, "weaknesses": {"value": "1. It seems the authors misinterpretation of forward diffusion as backward. This can be observed from they actually started from $x_T$ ∼ $p_{data}$ to  $x_0$ ∼$ p_{prior}$. The training step stated in the paper actually resembles the backward diffusion diffusion step in the conventional diffusion. There is not much different between this method in terms forward/backward as the diffusion itself is not a reason to stay on forward only as what we can see from GOUB.\n\n2. P4. Line 178, as the author mentioned, \"The subtractive form of the logarithm reflects that\nthe flow field decays multiplicatively from its initial value with a stochastic exponential scaling\", this translate to the original image difference, the variance of the noise can explore, I wonder how the authors address this issue. The only solution in the paper is that they set the $ e^{-\\int_0^t (\\theta_s+\\frac{1}{2} \\sigma_s^2) ds}= 0.001$. The authors should have discussed the impact of selecting such small diffusion coefficients. This small value seems to make it almost a deterministic process. \n\n3. As I mentioned in the summary and item 2, the authors used extremely small diffusion coefficients, this made it actually more close to deterministic method, reducing the diffusion effects. This can be observed in several sequences of images. For example, the second row images (over various time steps) in Figure 1 seemed so deterministic.\n\n4. The following paper should be referred:\n1). K Zhu et. al. UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control\n2). G Kim et. al. Diffusion-based generative model for financial time series via geometric Brownian motion\n\n5.  PROPOSITION 3.1 seems like an existing result and no need to prove again.\n6. Although the paper compared their method to GOUB in terms of metrics such as FID, they did not provide the GOUB results in the figures of images that compares the results. It will be good to seem their comparisons in images as they are similar diffusions between two deterministic images."}, "questions": {"value": "1. The authors need to discuss more how selection of the coefficients impact the behavior of the diffusion. \n2. Choice of $\\theta$ and $\\sigma$ affects the performance? \n3. In P4, line 146, the authors stated that As $t \\rightarrow \\infty$, the SDE converges to a stationary state $x_T ∼ N(x_t | \\mu, \\lambda^2)$. This formula does not make sense to me. Please check if t is T."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TMiO4zfyhA", "forum": "PyPqhQSJKz", "replyto": "PyPqhQSJKz", "signatures": ["ICLR.cc/2026/Conference/Submission19261/Reviewer_nRYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19261/Reviewer_nRYJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761688710753, "cdate": 1761688710753, "tmdate": 1762931228734, "mdate": 1762931228734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript proposes a new forward diffusion process for generative modeling. Specifically, the authors leverage a mean-reverting style SDE and apply the mean-reversion terms to both the drift and diffusion coefficients. The designed SDE has a closed-form solution, and the only unknown term is the simulation target. Following the spirit of denoising score matching, the author proposes a simple regression objective to learn it and thus enable simulating the designed SDE. For the sampling option, the author explored both the Euler method and the first-order discretization. Experimental results demonstrate that the proposed technique is applicable on image-to-image tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The writing is easy to follow.\n* Using the proposed mean-reverting-style SDE for generative modeling looks novel to me."}, "weaknesses": {"value": "* The claim about \"simpler, single\" diffusion process is somewhat unconvincing to me. According to the training algorithm and sampling procedure, the effort is almost the same as that of the diffusion models, and the training objective itself needs approximation.\n* Another main claim of the paper is that the proposed method can be viewed as a stochastic counterpart to flow matching. However, to me, it would be necessary to compare the established stochastic counterpart, known as diffusion bridges or bridge matching [1, 2], of the flow matching model, both conceptually and empirically (on image-to-image benchmarks).\n* For empirical results, it would be better to add Gassuain-to-image generation tasks to demonstrate the effectiveness of the proposed framework.\n\n\n\n## References\n[1] Peluchetti, Stefano. ‘Non-Denoising Forward-Time Diffusions’. (2023)\n\n[2] Shi, Yuyang, et al. ‘Diffusion Schrödinger Bridge Matching’. (NeurIPS 2023)"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p6Y1JvHCwy", "forum": "PyPqhQSJKz", "replyto": "PyPqhQSJKz", "signatures": ["ICLR.cc/2026/Conference/Submission19261/Reviewer_owVT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19261/Reviewer_owVT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027811309, "cdate": 1762027811309, "tmdate": 1762931228135, "mdate": 1762931228135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of changes in revision"}, "comment": {"value": "We appreciate all valuable feedback and comments from the reviewers. The revised paper has now been uploaded. In this version, we added several new experiments, including **extensive evaluations on image-to-image translation**, **comparisons with diffusion bridges**, **schedule sensitivity** and **log-normal prior** ablation studies, and additional **visualizations of the FoD sampling process**. A more detailed summary of updates is provided below for the convenience of all reviewers and the AC.\n\n1. Added more discussion about diffusion bridges in Section 2.1. Added the comparison and discussion of the sampling process between FoD and diffusion bridges in *Figure 6 (left), Figure 8, and Figure 9*.\n2. Updated *Table 1* with a diffusion bridge baseline, UniDB[1], which is a state-of-the-art method for diffusion-based image restoration.\n3. Added *Table 3* and *Table 8* for an extensive evaluation on image-to-image translation benchmarks (with MSE, LPIPS, and FID metrics). The comparisons include a flow matching method and two diffusion bridge models. A qualitative comparison is also added in *Figure 3*.\n4. Added the discussion about the choice of $\\theta$ and $\\sigma$ schedules in Section 5 (*right of Figure 6*) and in Appendix C.2 (*Figure 10 and Table 6*).\n5. Added *Figure 7* in Appendix C.1 to analyze the effect of different time interval coefficient values.\n6. Added *Table 7* to analyze different priors (normal and log-normal) for unconditional generation.\n7. We have further revised the paper to include all reviewer comments and improve the clarity. In addition, we added more experimental discussion and qualitative comparisons in the Appendix.\n\n---\n\nReferences:\n\n[1] Zhu et al. UniDB: A Unified Diffusion Bridge Framework via Stochastic Optimal Control. ICML 2025."}}, "id": "MIGy3gGq7m", "forum": "PyPqhQSJKz", "replyto": "PyPqhQSJKz", "signatures": ["ICLR.cc/2026/Conference/Submission19261/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19261/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19261/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763643923035, "cdate": 1763643923035, "tmdate": 1763645499158, "mdate": 1763645499158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces forward-only diffusion, which replaces the standard diffusion SDE with a mean-reversion term in both the drift and diffusion terms. They derive a tractable solution to the SDE that determines the conditional distribution $p(x_{t+1}|x_t, \\mu)$ and parameterize their neural network to learn the flow $\\hat{\\mu}_{\\phi} - x_t$. This enables a tractable loss to minimize the KL between the ground truth conditional and the model's estimate. Experimental results on image restoration tasks are provided, demonstrating improvements over baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "**(S1)**: Elegant formulation. A single, forward-only SDE for diffusion with the mean-reversion term is a neat formulation for image-restoration and conditional generation tasks. The mean-reversion term enables a state-dependent denoising process that dynamically adjusts to different corruption levels within an image. To me, this makes a lot of sense for conditional generation.\n\n**(S2)**: Tractability and flexibility. The SDE with mean-reversion in the diffusion term still yields a unique, tractable solution and enables a simple loss function that is stable to train. This is a strong point in favor of the method. Providing both Markovian and non-Markovian sampling strategies for this forward-only SDE further improves the flexibility of this approach.\n\n**(S3)**: Good ablations. Nullifying the diffusion term and reverting the SDE back to the flow-matching ODE clearly demonstrates worse performance on image restoration tasks, in terms of structural similarity metrics. This demonstrates the need for stochasticity in the mean-reverting SDE. Another ablation on fast-sampling is helpful. \n\nOverall, I think the idea and application of this paper is novel and interesting, and so I would recommend it for acceptance. If some of my concerns outlined below are addressed, I would be happy to raise my score."}, "weaknesses": {"value": "**(W1)**: Poor unconditional generation. The FID scores on CIFAR-10 (7.89 for FoD-SDE, 5.01 for FoD-ODE) are not competitive with standard forward-backward diffusion models (e.g., Score SDE @ 2.38) or even other forward-only ODE models like Rectified Flow (2.58). While noted as a limitation, this positions FoD as more a specialized method for conditional image generation tasks than as a general generative model. \n\n**(W2)**: Limited exploration of conditional image generation tasks. The paper mainly focuses on image restoration, which is a low-entropy task (i.e. the source is already close to the target image). Some qualitative examples for image-to-image translation are provided, but a more extensive evaluation on translation tasks would be useful to support the generality of this approach. Additional experiments on text-to-image or latent-diffusion architectures would be further welcome for completeness.\n\n**(W3)**: Missing comparisons. Some recent work on diffusion bridges (denoising diffusion bridge models) tackle similar problems as this paper. Comparison and a more detailed discussion around bridge models is missing. While FoD is an instantiation of stochastic interpolant methods, the paper could be strengthened by a more detailed comparison to other recent SI-based methods that have also been applied to image restoration."}, "questions": {"value": "**(Q1)**: Given the primary weakness is unconditional generation, have the authors experimented with modifying the prior distribution to better match the model's log-normal structure (e.g., starting from a log-normal prior)\n\n**(Q2)**: Do the authors have explanations for the behavior between the MC and non-MC samplers in Figure 4? Why are structural metrics better here compared to generation quality metrics? How should one decide between the two samplers?\n\n**(Q3)**: What is $x_s$ and $\\mu$ in the tasks outlined in Figure 3? How does the SDE behave when the tasks represents bridging two very semantically different distributions?\n\n**(Q4)**: How sensitive is the model to the choice of the $\\sigma_t$ schedule given that it now controls mean-reversion / a state dependent term in the diffusion term?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5wuFP3e5EN", "forum": "PyPqhQSJKz", "replyto": "PyPqhQSJKz", "signatures": ["ICLR.cc/2026/Conference/Submission19261/Reviewer_jcwr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19261/Reviewer_jcwr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762397835152, "cdate": 1762397835152, "tmdate": 1762931227766, "mdate": 1762931227766, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}