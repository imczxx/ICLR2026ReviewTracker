{"id": "Eix7AEtV48", "number": 6300, "cdate": 1757965664787, "mdate": 1759897923881, "content": {"title": "AMDCP: Adaptive Mixture Density for Conformal Prediction", "abstract": "We present Adaptive Mixture Density for Conformal Prediction (AMDCP), a glass-box framework that co-designs the non-conformity scoring framework with a mixture-density head. Our weight-aware component score yields analytic non-contiguous unions of ellipsoids with constant-time membership tests, preserving classical coverage guarantees while improving efficiency. A comprehensive empirical evaluation on real-world datasets from weather forecasting to protein property prediction demonstrates that AMDCP's regions are sharper than state-of-the-art generative methods, while also being an order of magnitude faster at inference time. We complement these results with theory (finite-sample marginal validity; an asymptotic efficiency guarantee; and approximate group-conditional coverage) and extensive ablations under extreme distributions, model misspecification, various model architectures, and more. AMDCP turns CP into a practical tool for real-world implementation: it is valid by construction, produces shape-adaptive sharp predictive sets, and is systems-efficient for modern pipelines.", "tldr": "Our \"glass-box\" method uses a novel score on a model's internal structure for analytic, non-contiguous regions. This robust approach yields sharper sets with 10x faster inference than SOTA generative methods, as confirmed by extensive ablations.", "keywords": ["mixture density networks", "adaptive non-conformity score", "uncertainty quantification", "prediction intervals", "multimodal distributions", "Mahalanobis distance", "coverage efficiency", "regression calibration", "heteroskedasticity", "multimodal distributions", "coverage guarantees"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1168dbdbcf5576dbd4aded7e769a17c8e61f694a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose AMDCP, a conformal prediction method based on mixture density networks. The method is invariant with respect to the underlying model architecture and easily allows for generating non-continuous prediction sets because score evaluation is only performed with the maximum density Gaussian component. Empirical results show that AMDCP works effictively and generates interpretable prediction sets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Overall, I think the work is somewhat interesting.\n\n1. The method is simple and architecture-invariant.\n\n2. The method allows for easily generating discontinuous prediction sets.\n\n3. The experimental evaluation is strong."}, "weaknesses": {"value": "While the method is interesting, I also do not think it is particularly stellar. The following two aspects, especially the first one, are keeping me from giving a \"borderline accept\" score. If the authors successfully resolve 1. and improve on 2., I am willing to increase my score to \"borderline accept\".\n\n1. The paper has a fundamental flaw in its theoretical contribution, namely Theorem 4.2: First of all, the proof is not very rigorous and it seems there are some implicit assumptions that may actually not hold for AMDCP. I am specifically worried about step 3, because AMDCP does not construct the non-conformity score as a function of the full density estimate, but does this max Gaussian approximation. The second problem is that the theorem statement is implied by the assumptions alone and is entirely disconnected from the proposed AMDCP. Any method that fulfills assumption (i) (in addition to some more assumptions that I believe are missing), should achieve the property. Assumption (ii), on the other hand, is trivially the case for any consistent estimate of the quantile (like the one used in conformal prediction). Perhaps there is a great miss-understanding from my side: In that case, I would be happy if the authors could clarify.\n\n2. The style of writing is clear, but overall too grandiose. I suggest to adopt a more scientific style of writing. Just one example: *\"Crucially, our framework retains exceptional modularity:\"*. Words like *\"exceptional\"* can be safely dropped. On the same note, I believe that the appendix should be greatly reduced. I believe that the paper should only cover points that are either contributions or improve clarity, including the appendix. I therefore suggest referencing the original works for lengthy proofs. This will improve readability and clarify the contributions of the manuscript.\n\n3. The method requires training a mixture density network head on top of an existing model, which may be prohibitive."}, "questions": {"value": "* What is a \"glass-box framework\"? I have never heard this term before.\n\n* Theorem 4.1: Would it be possible to stress more clearly that this result is well-known and comes from another work? For instance, I recommend *Theorem 4.1 (Marginal coverage guarantee; INSERT CITATION)*. This way, it is more clear that this theorem is not a contribution of this work. Personally, I am generally not a great fan of stating existing theorems like that because it makes it hard to see what the contribution is. I am aware that doing so is common these days, but I believe we should be clear about what our contributions are.\n\n* 6 Discussion: The authors use the word \"SOTA\" a number of times. I believe this term is unscientific and should be avoided. I suggest a more balanced perspective that highlights upsides and downsides of the proposed method. It would be great if the authors could move Appendix F.1 (which has such a very useful analysis) to the main text and remove these uninformative SOTA claims.\n\n* There are many well-known results and trivialities in the appendix. For instance, the proof for B.1 is well-known and I do not think it needs to be repeated in that length. Also, B.2.2 is just an existing result without any addition and I do not see why it has anything to do with AMDCP. Of course, it is all just in the appendix and the original works are properly cited, so it is not a great issue. Nevertheless, I have doubts whether we need to extend a paper to 30 pages just by listing well-known and uninformative results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x3ieMd56WS", "forum": "Eix7AEtV48", "replyto": "Eix7AEtV48", "signatures": ["ICLR.cc/2026/Conference/Submission6300/Reviewer_fHY2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6300/Reviewer_fHY2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760690231360, "cdate": 1760690231360, "tmdate": 1762918603984, "mdate": 1762918603984, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adaptive Mixture Density for Conformal Prediction (AMDCP), a new framework that enhances the efficiency of conformal prediction by integrating Mixture Density Networks (MDNs). Unlike traditional CP methods that often yield overly conservative prediction regions, AMDCP models the conditional distribution as a mixture of Gaussians with parameters learned from the input, allowing it to flexibly capture complex, multimodal, and non-contiguous patterns in data. The proposed novel conformity score combines Mahalanobis distance with mixture weights to generate prediction regions that adapt to local data structures. The authors provide theoretical guarantees for coverage and asymptotic efficiency, and extensive experiments show that AMDCP achieves significantly narrower prediction intervals while improving coverage. The method also demonstrates robustness to hyperparameter settings, making it suitable for real-world applications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a novel non-conformity score specifically designed to handle multimodal distributions with Mahalanobis distance and mixture weights.\n2. The improvements in prediction efficiency across both synthetic and real-world datasets are significant.\n3. AMDCP is robust to different estimation backbones, model misspecification and extreme distributions."}, "weaknesses": {"value": "1. AMDCP performs well on complex data distributions but might tend to underperform on simple cases, raising concerns about robustness and limiting its practical scope.\n2. The idea of post-processing non-conformity scores is not entirely novel. Prior works have explored similar strategies such as using quantile regression or normalizing flows to approximate conditional distributions, or rescaling non-conformity scores (e.g., via temperature scaling) to improve efficiency."}, "questions": {"value": "1. Please empirically validate the stated assumptions in the experiments.\n2. Can AMDCP be applied to classification? If so, please specify the nonconformity score, treatment of multiclass/multilabel settings, calibration protocol, and the thresholding procedure.\n3. Please explain why DCP and SPCI exhibit significant under-coverage and why PCP over-covers. If possible, please include ablations to isolate these factors and I think this is another avenue to help highlight the contributions.\n4. Why did AMDCP intervals tend to be wider than SPCI’s on synthetic datasets? Will AMDCP outperform SPCI with more extreme synthetic distributions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7UkL3kr08y", "forum": "Eix7AEtV48", "replyto": "Eix7AEtV48", "signatures": ["ICLR.cc/2026/Conference/Submission6300/Reviewer_mFqt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6300/Reviewer_mFqt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525225530, "cdate": 1761525225530, "tmdate": 1762918603278, "mdate": 1762918603278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides a new method called AMDCP for conformal prediction (for one and multi-dimensional outcome y) that can produce non-contiguous preidction intervals prediction sets. This is useful in situations where the conditional distribution of y given predictors x is multimodal for some x. The backbone of AMDCP is fitting a mixture of Gaussian (using maximum likelihood method), where mixture probabilities and components (mean vector and covariance matrix of each Gaussian) can depend on input x via some neural network. A key idea is to define the non-conformity score using some distance between the observed outcome and the mixture component closest to it, adjusted to prefer components that had higher estimated weights. (This score is given in (3), which is simply the smallest of all -2log[\\pi_j \\phi(y; mu_j, \\Sigma_j)], that is a change from more commonly used \"full\" -2log likelihood.)\n\nEmpirical results were good over extensive studies. However, the authors describe their AMDCP as a “glass-box” and other existing methods as “black-box”, a distinction I find somewhat unjustified."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The literature review contains good information and is easy to follow.\n\nThe method uses mixture model in a novel way for conformal prediction.\n\nThe framework can produce prediciton set for multi-dimensional outcome, at least in principle.\n\nThe empirical results favors the proposed methods in many cases.\n\nExtensive details, examples and studies were provided in the Appendix, to demonstrate the robust performance of the proposed method to various data distributions, model misspecifications (e.g. over or under-specified number of components, K), choice of underlying prediction methods, and different size of training data etc. There were also practical guidelines for users."}, "weaknesses": {"value": "Why putting the main Algorithm (alg. 1) in the appendix?\n\nIt's not clear to me why is the proposed non-conformity score a better choice than other possibilities, or in what situations does it heursitically work better than alternatives like the negative log likelihood. I do see that the empirical experiments presented tend to show they lead to smaller sized prediction regions. Some heuristics, discussions or comparison to other possiblities are welcome.\n\nAny choice of non-conformity score is legit in the sense that they lead to marignal coverage guarantee. I am not against the proposed method, but I don’t see how the authors choice is more different/advantageious/interpretable than others that theirs is called a \"glass-box\" while their competitors are called \"black box\". This is especially questionable in cases where the (finite) Gaussian mixture is just an approximation, not a true reflection of the real data generating process. For instance, when modeling a skewed or heavy-tailed distribution, the fitted Gaussian components will likely have no inherent interpretive meaning. And given much uncertainty in fitting the mixture model itself (not one mixture, but one mixture at every x), I find it difficult to attach substantive meaning to the component associated with a given data point or the distance between them. In short, I agree there is novelty and benefits in certain cases, but I do not think the level of novelty matches the advertisement about moving from \"black-box\" to \"glass-box\".\nQuote:\"The inclusion of the data-dependent mixing weight is what allows us to \"open the black box.\" While the weights\nare mathematically present in the total log-likelihood score used by \"black box\" methods like DCP,\nthe crucial information about which mode is most probable for a given point is effectively \"averaged\nout\" and lost. In contrast, our score directly leverages the mixing weights to actively shrink the\nprediction regions around the most probable mixture components—a selective, adaptive mechanism\nimpossible with black-box scores. This direct inclusion enables our ’glass-box’ innovation...\"\n\nAlthough there are many experiment setups and methods, within each setup and method, there were usually only 5 trials. That is a fairly small number. And it was not clear how the repetions are done, and whether they reflect re-generation of data, and if the evaluation is on a single x is not clear. (I do see Appendix E where different values/regions of x were examined. My question was on experiments outside the Appendix E examples. I am not asking for extensive study over many x in each example, I am asking for clarification of the definition of \"repetititions\" and how the assessment are done.)"}, "questions": {"value": "What happens if the number of components, K, varies according to x and should really be K(x)? (That said, I am not suggesting trying to model K in practice, as data could have limited information for it unless x is very low dimensional and there are plenty of data points in every neighborhood.)\n\nIn most tables (1, 3, 10 and many others), I just want to confirm if you are displaying one standard deviation, \\sigma (sd) after the plus minus sign or the standard error (se) of estimating the mean size, which is 1/sqrt(no. replications) times sd? (I am a bit surprised because the reported sd values seem pretty small in general, many less than 5% of the mean.) Also, how did you choose the x value being conditioned upon when you report these interval sizes and coverage? \nRelated to the questions above about variation: when you say 5 trials or repeated experiements, exactly how are they done? In the synthetic data case, do you re-generate new training and calibration data and redo all the steps or some other way? For real data, do you randomly re-split the data etc? Again, I am surprised that the variations are so small over the current replications.\n\nI am still not sure what is the key difference between a so-called a \"glass-box\" and a \"traditional black-box\"... Is any of your competitors method clean enough to be called a \"glass-box\"? And if someone has a new method, how would you decide if it's black or glass?\n\nCan we have a concise summary of the dimension of y and x, training and calibration size etc so users can better see the range of applicability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bknjfLIltF", "forum": "Eix7AEtV48", "replyto": "Eix7AEtV48", "signatures": ["ICLR.cc/2026/Conference/Submission6300/Reviewer_ugTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6300/Reviewer_ugTf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840052033, "cdate": 1761840052033, "tmdate": 1762918602912, "mdate": 1762918602912, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AMDCP (Adaptive Mixture Density for Conformal Prediction), a novel \"glass-box\" framework that co-designs the non-conformity scoring mechanism with a Mixture Density Network (MDN) head. Rather than treating the MDN as a black-box oracle that outputs only aggregate log-likelihood (as previous methods like DCP do), AMDCP leverages the granular information within the mixture model to construct prediction regions that are unions of ellipsoids. This approach enables non-contiguous prediction regions that can capture multimodal distributions while maintaining constant-time membership tests. The authors provide theoretical guarantees for marginal coverage and asymptotic efficiency, along with extensive empirical validation across synthetic and real-world datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The \"glass-box\" approach to MDN-based conformal prediction is interesting. The component-aware non-conformity score (Equation 3) that leverages mixture weights to \"shrink prediction regions around the most probable mixture components\" appears to be an interesting idea that distinguishes AMDCP from prior work.\n\nThe paper provides solid theoretical foundations with proofs for marginal coverage (Theorem 4.1) and an asymptotic efficiency bound (Theorem 4.2). The extension to group-conditional coverage (G-AMDCP) with formal guarantees (Theorems E.1 and E.2) adds significant value."}, "weaknesses": {"value": "Most experiments focus on low-dimensional outputs (typically 1D). More evaluation on higher-dimensional outputs would better demonstrate the method's scalability."}, "questions": {"value": "Most experiments focus on low-dimensional outputs (typically scalar). How does AMDCP scale to higher-dimensional outputs (e.g., multivariate response problems)? \nHow would AMDCP perform with alternative density estimators beyond Gaussian mixtures (e.g., kernel density estimation, normalizing flows)? Are there theoretical advantages to using MDNs specifically?\nFor practitioners implementing AMDCP, what are your recommendations for selecting the number of mixture components K?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qusO8XdYPF", "forum": "Eix7AEtV48", "replyto": "Eix7AEtV48", "signatures": ["ICLR.cc/2026/Conference/Submission6300/Reviewer_Ez9V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6300/Reviewer_Ez9V"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6300/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902330179, "cdate": 1761902330179, "tmdate": 1762918602514, "mdate": 1762918602514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}