{"id": "t3FSGlOcsG", "number": 24915, "cdate": 1758361867354, "mdate": 1759896742511, "content": {"title": "FedHyMoe: Hypernetwork-Driven Mixture-of-Experts for Federated Domain Generalization", "abstract": "Federated Learning (FL) enables collaborative model training without sharing raw data, but most existing solutions implicitly assume that each client’s data originate from a single homogeneous domain. In practice, domain shift is pervasive: clients gather data from diverse sources, domains are heterogeneously distributed across clients, and only a subset of clients participate in each round. These factors cause substantial degradation on unseen target domains. Prior Federated Domain Generalization (FedDG) methods often assume complete single-domain datasets per client and sometimes rely on sharing domain-level information, raising privacy concerns and limiting applicability in real-world federations. In this paper, we introduce FedHyMoe, a Hypernetwork-Driven Mixture-of-Experts framework that addresses these challenges by shifting from parameter-space fusion to embedding-space parameter synthesis. Each client is represented by a compact domain embedding, and a shared hypernetwork generates its Mixture-of-Experts (MoE) adapter parameters. At test time, unseen domains are handled by attending over source client embeddings to form a test-domain embedding, which the hypernetwork uses to synthesize a specialized adapter. This enables non-linear interpolation and extrapolation beyond convex averages of stored parameters, while reducing communication and storage overhead and mitigating privacy risks by exchanging only low-dimensional embeddings. FedHyMoe consistently achieves higher generalization accuracy and improved calibration compared to baselines under domain heterogeneity and partial participation highlighting embedding-driven hypernetwork synthesis as a powerful inductive bias for robust, efficient, and privacy-conscious Federated Domain Generalization.", "tldr": "TL;DR: **FedHyMoe uses hypernetworks with client embeddings to synthesize Mixture-of-Experts adapters, enabling robust, efficient, and privacy-preserving domain generalization in federated learning under heterogeneity and partial participation.**", "keywords": ["Federated Learning", "Domain Generalization", "Hypernetworks", "Mixture-of-Experts", "Privacy-Preserving Learning", "Cross-Domain Adaptation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f2243f77eacb3fb6ff575638a8a73e87426f75ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes FedHyMoe, a framework for Federated Domain Generalization that uses hypernetwork-driven mixture-of-experts adapters. Instead of traditional parameter-space averaging in federated learning, the method represents each client with a compact domain embedding and uses a shared hypernetwork to generate MoE adapter parameters. At test time, the system composes embeddings from source clients to synthesize specialized adapters for unseen domains, enabling non-linear interpolation beyond convex parameter fusion."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The shift from parameter-space fusion to embedding-space composition with hypernetwork synthesis is creative and well-motivated for the FDG problem.\n2. The paper evaluates on three standard benchmarks (OfficeHome, PACS, VLCS) and compares against numerous baselines across different categories."}, "weaknesses": {"value": "1. The novelty is quite limited. The core components (hypernetworks, MoE, Kronecker factorization) are all existing techniques. The combination, while reasonable, doesn't represent a significant technical advance. The paper reads more like an engineering contribution than a research breakthrough.\n2. There is significant formatting issues and wasted space throughout. The paper only uses ~8 pages of content despite having 9 pages available (excluding references). \n3. Emprical improvements are quite marginal. On PACS: only 0.3% improvement over FedDG-MoE baseline. The improvements don't strongly justify the added complexity.\n4. No computational cost analysis or timing comparisons.\n5. No ablation studies on key design choices (embedding dimension, number of experts, etc.).\n6. Uses CLIP-pretrained models but doesn't properly compare to other CLIP-based methods\n7. No analysis of partial client participation effects despite claiming this as a contribution."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aEQ38ynJbe", "forum": "t3FSGlOcsG", "replyto": "t3FSGlOcsG", "signatures": ["ICLR.cc/2026/Conference/Submission24915/Reviewer_69if"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24915/Reviewer_69if"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761591985916, "cdate": 1761591985916, "tmdate": 1762943243294, "mdate": 1762943243294, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedHyMoe, a federated domain generalization framework that replaces traditional parameter averaging with a hypernetwork-driven mechanism. Each client is represented by a compact domain embedding, and a shared hypernetwork generates domain-specific Mixture-of-Experts (MoE) adapter parameters from these embeddings. During test time, the system synthesizes an adapter for an unseen domain by computing a weighted combination of stored client embeddings. Experimental results on standard domain generalization benchmarks demonstrate improved out-of-domain performance compared to existing federated learning and domain generalization methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The use of compact domain embeddings allows for lightweight communication and privacy-preserving domain adaptation without direct access to client-specific data."}, "weaknesses": {"value": "1. The paper lacks theoretical analysis or guarantees on how well the hypernetwork generalizes to unseen domains, especially when domain embeddings are noisy or insufficiently representative.\n2. The effectiveness of the proposed domain embedding and interpolation strategy is underexplored; more ablations or visualizations could clarify its contribution and limitations."}, "questions": {"value": "1. How sensitive is the hypernetwork's performance to the quality or dimensionality of the domain embeddings? Have the authors evaluated whether simple or noisy embeddings degrade generalization to unseen domains?\n2. While the interpolation of domain embeddings is proposed for synthesizing unseen domain adapters, what guarantees or empirical support exist that this interpolation results in meaningful or performant representations under severe domain shifts?\n3. As the number of clients or domain embeddings increases, how does the computational and memory overhead of the hypernetwork scale? Would the architecture remain effective with hundreds of client domains?\n4. What is the specific benefit of using a MoE-style adapter architecture in this context compared to simpler domain-conditioned parameter generators? Are all experts utilized effectively during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Gknh2S0IuR", "forum": "t3FSGlOcsG", "replyto": "t3FSGlOcsG", "signatures": ["ICLR.cc/2026/Conference/Submission24915/Reviewer_Wjuv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24915/Reviewer_Wjuv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761697440118, "cdate": 1761697440118, "tmdate": 1762943242980, "mdate": 1762943242980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FedHyMoe, an FDG method that (i) represents each client with a compact domain embedding, (ii) uses a shared hypernetwork to generate Mixture-of-Experts (MoE) adapters with Kronecker/low-rank structure, and (iii) at test time attends over stored source embeddings to build a target embedding which conditions the hypernetwork to synthesize an adapter for the unseen domain. The method is positioned as a shift from parameter-space fusion to embedding-space composition + non-linear parameter synthesis, with claimed benefits in generalization, communication efficiency, privacy alignment, and calibration."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tCompelling high-level idea. Moving fusion from parameter space to embedding space with a generator that can synthesize adapters non-linearly is a clean and well-motivated inductive bias for FDG; the core mechanism is clearly presented in Eqs. (1)–(7) and Fig. 2. \n2.\tArchitectural efficiency. The Mixture of Kronecker-Product Experts balances expressivity and storage by sharing slow factors A_i and generating fast low-rank factors u_{i,k}v_{i,k} via the hypernetwork. \n3.\tFederated protocol clarity. The training and inference procedures address partial participation, secure aggregation, and server-side storage of embeddings (Algs. 1–2), which clarifies the intended deployment."}, "weaknesses": {"value": "1. Claimed advantages in privacy and communication are not empirically supported\n\n> The abstract and method sections repeatedly claim reduced communication and improved privacy exposure (e.g., exchanging low-dimensional embeddings, SecAgg of hypernetwork gradients, “narrowed attack surface”) but the experiments do not include any communication-cost accounting or privacy evaluations (e.g., gradient inversion/membership inference).  \nMorover, only accuracy numbers are reported (Table 1), without bandwidth/MB per round, parameter counts transmitted, or server storage overheads for the registry of embeddings; likewise, there are no privacy attack results. \n\n2. Key numerical claims in the text conflict with Table 1:\n\n>\t•\tOfficeHome: The text claims “gain of at least 4.6%,” but the best baseline average is 88.60 (FedDG-MoE Avg/AM) and the best FedHyMoe is 89.63 (Prox): +1.03 pts, not +4.6.  \n\t•\tPACS: The text claims improvement “by at least 0.7%,” but 97.19 (FedHyMoe Avg) is below 97.35 (FedDG-MoE TTF). Only FedHyMoe Scaffold hits 97.67, which is +0.32 over 97.35, not ≥0.7.  \n\t•\tVLCS: The text claims surpassing “by at least 0.6%,” but the best baseline is 82.66 (FedDG-MoE Prox), while the best FedHyMoe variant is 82.62 (TTF): −0.04.  \n\n\n3. No ablations or sensitivity analyses on core design choices\n\n> There are no ablations for: (i) embedding dimension d, (ii) #experts and rank, (iii) hypernetwork non-linearity (linear vs non-linear), (iv) adapter placement within ViT, (v) test-batch size used to form $e_{\\text{test}}$. The experiments section lists protocol and results but no ablation studies.  \n\n4. Reliance on a test-time batch; unknown behavior for single-sample queries\n\n> Algorithm 2 constructs $e_{\\text{test}} = g_{\\text{map}}(\\text{mean}(Z))$ from a batch of test features, then attends over stored $e_k$ and synthesizes parameters. Realistic deployments often require one-sample-at-a-time predictions. The paper does not study sensitivity to batch size (noise vs. stability) or performance in the single-sample regime. \n\n\n5. Equation and citation issues / clarity gaps\n\n>\t•\tA dangling “(?)” after “mixture-of-experts” in Eq. (3) paragraph (p. 5).  \n\t•\tEq. (6): It is unclear why $\\bar e_{\\text{test}}$ (the pooled embedding) is used to generate $\\theta$ rather than $e_{\\text{test}}$ directly; please justify.  \n\t•\tSection 2.4, 2nd paragraph lacks citations despite referring to specific methods (PLAN, MaPLe, FedCLIP/PromptFL) in prose. Please add the appropriate citations inline.  \n\n6. Choice of pre-trained backbone in a federated context\n\n>All experiments use a frozen CLIP ViT-B/16. In FL/FDG there is no access to local data centrally, so there is no guarantee the pre-training distribution matches local client distributions. The paper does not analyze performance when the pre-trained encoder is mismatched to clients (e.g., deliberately choosing a pre-trained model with minimal overlap to client domains)."}, "questions": {"value": "Please addressed the mentioned issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gZ6eoOtrmt", "forum": "t3FSGlOcsG", "replyto": "t3FSGlOcsG", "signatures": ["ICLR.cc/2026/Conference/Submission24915/Reviewer_LKg4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24915/Reviewer_LKg4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971172511, "cdate": 1761971172511, "tmdate": 1762943242762, "mdate": 1762943242762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}