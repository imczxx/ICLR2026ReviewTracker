{"id": "1jlci3sF4Q", "number": 12654, "cdate": 1758209309310, "mdate": 1759897495838, "content": {"title": "Self-adaptive Retrieval-Augmented Reinforcement Learning for Time Series Forecasting", "abstract": "Deep learning models for time series forecasting, typically optimized with Mean Squared Error (MSE), often exhibit spectral bias. This phenomenon arises because MSE prioritizes minimizing errors in high-energy, typically low-frequency components, leading to an underfitting of crucial, lower-energy high-frequency dynamics and resulting in overly smooth predictions. To address this, we propose Self-adaptive Retrieval-augmented Reinforcement learning for time series Forecasting (SRRF), a novel plug-and-play training enhancement. SRRF uniquely internalizes high-frequency modeling capabilities into base models during training, ensuring no additional inference costs or architectural changes for the base model. The framework operates by first employing Retrieval-Augmented Generation (RAG) to provide contextual grounding via relevant historical exemplars. Subsequently, building on this contextual guidance, a Reinforcement Learning (RL) agent learns an adaptive policy to correct and enhance initial forecasts, optimized via a reward function that promotes both overall predictive accuracy and fidelity to high-frequency details. Comprehensive evaluations on diverse benchmarks demonstrate that models trained with the SRRF methodology substantially improve upon their original versions and other state-of-the-art techniques, especially in accurately predicting volatile series and fine-grained temporal patterns. Qualitative and spectral analyses further confirm SRRF's effectiveness in mitigating spectral bias and enhancing high-frequency representation.", "tldr": "", "keywords": ["Time Series", "Reinforcement Learning", "Retrieval Augment Generation"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5700f3b7646863120bbcbf3219b2d435089b4b8b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Self-adaptive Retrieval-augmented Reinforcement Learning for Time Series Forecasting, a novel enhancement framework designed to address spectral bias in time series models trained with MSE. The framework integrates Retrieval-Augmented Generation to provide contextual grounding through historical exemplars and employs RL for adaptive correction of base model predictions. The paper demonstrates SRRF’s effectiveness through comprehensive experiments, showing significant improvements in forecasting performance across diverse datasets and models, especially in terms of spectral fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Innovative framework for spectral bias correction.\nSRRF’s integration of RAG and RL to counteract spectral bias is novel and effectively addresses a long-standing challenge in time series forecasting. The clear focus on mitigating spectral bias enhances its applicability to volatile data where high-frequency dynamics are critical.\n\n2. Plug-and-play design with no inference overhead.\nThe SRRF framework is a training-time enhancement that operates without changing the base model architecture. This approach ensures no added computational burden during inference, making SRRF a practical solution for enhancing existing models without requiring significant infrastructure changes."}, "weaknesses": {"value": "1. Lack of clarity in RL correction mechanism.\nThe mathematical details of the RL correction process (Sec. 2.4) are not fully elaborated, leaving some uncertainty about the exact mechanism through which the RL agent refines predictions. While the qualitative results suggest success, the absence of a more detailed theoretical explanation or formal derivation of the RL correction step limits the depth of understanding regarding how SRRF resolves issues with traditional gradient-based optimization (Sec. 2).\n\n2. Sensitivity to hyperparameters.\nThe performance of SRRF is sensitive to key hyperparameters, including the retrieval count (k) and the RL sample count, with noticeable degradation when too many exemplars are retrieved. While the paper discusses these sensitivities, it lacks a deeper exploration of how these parameters interact across different datasets, which could further help users tailor the framework to their needs.\n\n3. Increased computational costs during training.\nAlthough SRRF improves model accuracy, the additional computational cost from RAG and RL sampling may limit its scalability, particularly for large datasets or more complex base models (Sec. 5). The paper acknowledges these costs but does not provide a detailed breakdown of the time or memory overhead incurred by the retrieval and reinforcement learning steps during training.\n\n4. While SRRF significantly enhances performance in high-frequency components, its impact on low-frequency components is sometimes inconsistent. In certain cases, SRRF leads to an increase in error energy in the lowest frequency band, suggesting that the framework may unintentionally sacrifice some predictive accuracy for smooth trends in favor of capturing finer details."}, "questions": {"value": "1. A more explicit description or pseudocode would clarify how the reinforcement signal modifies predictions and how stability is ensured during joint optimization.\n\n2. What is the empirical trade-off between retrieval depth and performance?\nHow sensitive are results to the number of retrieved exemplars and their selection strategy? \n\n3. What portion of the total training time is attributable to retrieval and RL sampling? Would a lighter retrieval or policy model achieve similar gains? Including GPU hours or per-epoch runtime comparisons to baselines would strengthen the practicality argument."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "F3d737Rl8m", "forum": "1jlci3sF4Q", "replyto": "1jlci3sF4Q", "signatures": ["ICLR.cc/2026/Conference/Submission12654/Reviewer_5Trs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12654/Reviewer_5Trs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724880407, "cdate": 1761724880407, "tmdate": 1762923494638, "mdate": 1762923494638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the problems of MSE loss function commonly used in the regression problem and thus proposes Retrieval-augmented Reinforcement learning for time series Forecasting (SRRF). The idea is to provide compensations via a policy network to the predicted outputs. The main model is trained via a joint loss function while the policy network is trained to minimize the RL loss via a policy gradient method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) the idea of RL for compensations of predicted output is novel;\n2) the presentation of this paper is easy to follow;\n3) Numerical results are convincing."}, "weaknesses": {"value": "1) Code information should be placed in the abstract to gain better visibility;\n2) Comparisons with those published in 2025 onward should be added;\n3) The performance difference is not big such that the statistical tests are necessary;\n4) Complexity analysis should be provided."}, "questions": {"value": "1) Code information should be placed in the abstract to gain better visibility;\n2) Comparisons with those published in 2025 onward should be added;\n3) The performance difference is not big such that the statistical tests are necessary;\n4) Complexity analysis should be provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FLmqL8j29s", "forum": "1jlci3sF4Q", "replyto": "1jlci3sF4Q", "signatures": ["ICLR.cc/2026/Conference/Submission12654/Reviewer_7A3k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12654/Reviewer_7A3k"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804672185, "cdate": 1761804672185, "tmdate": 1762923494185, "mdate": 1762923494185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the issue of overly smooth predictions caused by MSE-based training, the paper proposes a Self-adaptive Retrieval-augmented Reinforcement learning framework (SRRF) for time series forecasting. SRRF employs Retrieval-Augmented Generation to provide contextual grounding and reinforcement learning to correct initial forecasts. By integrating the SRRF module into various forecasting models, the approach achieves good prediction performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a plug-and-play that can be applied to all time series forecasting models.\n\n2. The paper adopts reinforcement learning to learn a policy network that corrects the model’s initial forecasting results.\n\n3. As a plug-and-play, SRRF achieves promising forecasting performance across different models."}, "weaknesses": {"value": "W1. Method description is unclear:  1. The paper does not clearly explain how the policy network learns the mean (**μ**) and standard deviation (**σ**) from the reference prediction and the initial prediction. 2. It is also unclear whether the policy network generates the correction term (**α**) for each individual time step or for the entire time series sample.\n\nW2. Experiment results: 1. The SRRF results in Table 1 are reported on top of the iTransformer, but they are inconsistent with the iTransformer+SRRF results shown in Table 2.   2. Some baseline results in Table 2 are significantly worse than those reported in the original papers — for example, PatchTST on the Traffic and Weather datasets shows noticeably lower prediction performance.\n\nW3. Hyperparameter sensitivity experiment: The authors did not specify which dataset was used for this experiment.  Moreover, the RL sample count parameter has a major impact on prediction performance. The authors should conduct additional experiments across multiple datasets to verify whether the optimal choice of this parameter varies between datasets.\n\nW4. Missing baselines: The authors should include more recent baselines, such as TimeMixer and TimeMixer++, for a more comprehensive comparison."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ifkisUzkk7", "forum": "1jlci3sF4Q", "replyto": "1jlci3sF4Q", "signatures": ["ICLR.cc/2026/Conference/Submission12654/Reviewer_Ah7n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12654/Reviewer_Ah7n"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909239610, "cdate": 1761909239610, "tmdate": 1762923493676, "mdate": 1762923493676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to integrate RAG and RL into the learning process of a deep learning models for time series forecasting."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- original idea\n- reasonably well written paper"}, "weaknesses": {"value": "- unclear integration of RAG and RL into model training\n- no detailed analysis of additional training costs introduced by RAG and RL"}, "questions": {"value": "Given that RAG has high costs, how do you integrate RAG into the iterations used during the optimization process?\nHow can a subset of plausible historical examples be used to train the model, and how exactly are these examples selected?\nHow much extra training costs are caused by your RAG and RL sampling procedure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eFn8nKsrQg", "forum": "1jlci3sF4Q", "replyto": "1jlci3sF4Q", "signatures": ["ICLR.cc/2026/Conference/Submission12654/Reviewer_ci19"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12654/Reviewer_ci19"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12654/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023253581, "cdate": 1762023253581, "tmdate": 1762923493416, "mdate": 1762923493416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}