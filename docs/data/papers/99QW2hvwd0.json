{"id": "99QW2hvwd0", "number": 20140, "cdate": 1758302967792, "mdate": 1759896999469, "content": {"title": "Morality is Contextual: Learning Interpretable Moral Contexts from Human Data with Probabilistic Clustering and Large Language Models", "abstract": "Moral actions are judged not only by their outcomes but by the context in which they occur. We present \\textsc{COMETH} (Contextual Organization of Moral Evaluation from Textual Human inputs), a framework that integrates a probabilistic context learner with LLM-based semantic abstraction and human moral evaluations to model how context shapes the acceptability of ambiguous actions. We curate an empirically grounded dataset of 300 scenarios across six core actions (violating \\emph{Do not kill}, \\emph{Do not deceive}, and \\emph{Do not break the law}) and collect ternary judgments (Blame/Neutral/Support) from $N{=}101$ participants. A preprocessing pipeline standardizes actions via an LLM filter and MiniLM embeddings with K-means, producing robust, reproducible core-action clusters.\\textsc{COMETH} then learns action-specific \\emph{moral contexts} by clustering scenarios online from human judgment distributions using principled divergence criteria. To generalize and explain predictions, a Generalization module extracts concise, non-evaluative binary contextual features and learns feature weights in a transparent likelihood-based model. Empirically,\\textsc{COMETH} roughly doubles alignment with majority human judgments relative to end-to-end LLM prompting ($\\approx 60\\%$ vs.\\ $\\approx 30\\%$ on average), while revealing which contextual features drive its predictions. The contributions are: (i) an empirically grounded moral-context dataset, (ii) a reproducible pipeline combining human judgments with model-based context learning and LLM semantics, and (iii) an interpretable alternative to end-to-end LLMs for context-sensitive moral prediction and explanation.", "tldr": "COMETH-RL learns moral contexts from human judgments using probabilistic clustering and LLM semantics, then predicts and explains new cases with interpretable features, achieving about 2x higher alignment than end-to-end prompting.", "keywords": ["Moral Dilemmas", "Context Learning", "Large Language Models", "Probabilistic Modeling", "Explainable AI", "AI Alignment", "Computational Ethics"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e063f9582b3488c34489c27c8c77bd538be997d4.pdf", "supplementary_material": "/attachment/568ba9209bfaaf1a1c690780e9a66f992a833160.pdf"}, "replies": [{"content": {"summary": {"value": "In my understanding, the paper introduces COMETH, a three-stage framework for modeling how context shapes human moral judgments of ambiguous actions. First, the authors standardize scenario descriptions into a “core action” (e.g., to steal, to lie by interest) using LLM filtering plus MiniLM embeddings with K-means; this pre-processing is evaluated via clustering quality (e.g., ARI) across prompts and models (Table 1). Second, a probabilistic context learner groups scenarios with the same action into \"moral contexts\" by clustering their human judgment distributions (Blame/Neutral/Support) with online add/merge steps (thresholded by KL/JS divergence). These clusters are visualized in the Support/Blame plane with Neutral implied (Figure 2). Third, a generalization module uses LLM-extracted binary contextual features and learned feature weights to predict which cluster (and thus which majority human judgment) a new scenario belongs to, providing feature-level interpretability (Figure 5). Empirically, the pipeline reports roughly doubling alignment with majority human judgments relative to end-to-end LLM prompting on the same scenarios (~60% vs ~30%)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Clear motivation: Focuses on context-sensitive moral evaluation and the limitations of end-to-end prompting for alignment.\n\nHuman-grounded dataset: 300 scenarios across six actions with N=101 participants; the work builds on common-morality rules and measures full distributions over Blame/Neutral/Support.\n\nMethodological separation of concerns: \n- reproducible pre-processing that standardizes actions and shows robustness across LLMs/prompts (Table 1), \n- probabilistic clustering over human judgment distributions\n- a feature-based generalization step with interpretability.\n\nReported gains: Substantial improvements in alignment rate vs end-to-end LLM prompting, plus a discussion of variance sources (feature extraction quality and cluster diversity)."}, "weaknesses": {"value": "Clarity on the two kinds of clustering. The paper interleaves K-means over MiniLM embeddings for core action grouping in pre-processing and probabilistic clustering of human judgment distributions into contexts. I recommend a schematic/table early in Section 2 that explicitly contrasts these two steps (inputs, objective, outputs, evaluation) and moves essential definitions into the main text (a short \"prompt glossary\" for Infinitive, OneWord, MainAct and one short worked example).\n\nEvaluation transparency and ablations. The claimed ~2× improvement is compelling, but the paper would be much stronger with:\n\nModule ablations: \n- no LLM pre-processing (use rule-based extraction)\n- replace the probabilistic add/merge with a static clustering baseline\n- remove/replace LLM-extracted features (e.g., human-coded features, bag-of-words or topic features)\n- no feature-weight learning.\n\nSensitivity analyses: thresholds \"a\" and \"m\", feature binarization heuristics, and prompt choice.\n\nHuman-data reliability: inter-annotator agreement and test-retest stability for judgment distributions.\nSummarizing the parameter search (currently in the appendix) in the main text would help readers interpret a=0.12 and m=0.03.\n\nMetric definition and placement. The alignment rate is defined in Section 3.3 as majority-match accuracy, but the definition is easy to miss. I suggest introducing it earlier, adding a compact formula/box, and reporting additional metrics (e.g., expected calibration error, selective abstention/coverage) given the inherent ambiguity in moral judgments.\n\nInterpretability grounding. Figure 5 shows feature weights for Practice Euthanasia, but the feature extraction pipeline still feels under-specified for reproducibility. Please include: a verbatim prompt template, 2-3 concrete scenario -> feature examples (raw text -> candidate features -> binarization), and a small human-coded feature check to validate LLM extraction quality.\n\nFigure and presentation issues:\n\nFigure 1 font is hard to read. Please enlarge labels and ensure contrast.\n\nFigure 3 currently lacks explicit axis labels; add \"P(Support)\" and \"P(Blame)\" (Neutral = 1 − P(S) − P(B)). Consider an inset reminding readers that Neutral is implied.\n\nFor Figure 4, reiterate the definition of alignment rate in the caption or a footnote and report per-action counts to contextualize variance.\n\nPositioning among related work. The paper motivates interpretable, context-sensitive moral prediction, but it would benefit from sharper comparisons to purely statistical clustering over judgment distributions (no LLM features), instruction-tuned moral reasoning models, and rule-based or social-choice-based aggregation approaches (even small baselines would anchor the contribution)."}, "questions": {"value": "Notes for the authors:\n\nPlease add an in-text citation for the Adjusted Rand Index and consider citing Steinley (2004) for properties of the adjusted index, as it’s commonly referenced in clustering evaluation.\n\nDefine Infinitive, OneWord, and MainAct prompts the first time they’re mentioned and add one example per prompt in the main text (not only the appendix).\n\nConsider releasing data, prompts, and code for the feature extraction and generalization modules to facilitate replication.\n\nWhat would raise my score:\n\nAblation and sensitivity results that isolate the contribution of each pipeline component (pre-processing, probabilistic clustering, feature extraction/weighting), plus baselines without LLM-derived features.\n\nClear, early definitions of the main constructs (two clustering stages, alignment rate), with a worked example that walks a single scenario through all stages of COMETH.\n\nReproducibility additions: released prompts, feature binarization rules, and a small human-coded feature study; summary table of the a/m search; and improved figure legibility and axis labeling."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "eVtPbYhNK3", "forum": "99QW2hvwd0", "replyto": "99QW2hvwd0", "signatures": ["ICLR.cc/2026/Conference/Submission20140/Reviewer_7E5C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20140/Reviewer_7E5C"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608757699, "cdate": 1761608757699, "tmdate": 1762933169418, "mdate": 1762933169418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework that integrates empirical moral judgment data with a probabilistic RL architecture designed to infer context-specific reward models from ternary human moral evaluations (blame, neutral, support), to ensure that AI systems consistently comply with human ethical standards across diverse contexts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "A dataset with different scenarios and actions is created.\nThe proposed framework achieves better performance w.r.t. alignment rates, compared to baseline end-to-end methods.\nThe idea of contextual moral evaluation and alignment is novel."}, "weaknesses": {"value": "The experimental evaluations are weak.\nThe evaluations on clustering are hard to tell whether the resultant clusters are corresponding to different contexts. \n\nThe claimed interpretability is also lacking support. Figure 5 has two columns with the same feature of \"approved directive(s)\", but the contributions to assigning scenarios to contexts (C1 & C2) are different. According to the assigning weights, the contexts C1 and C2 are quite different. However, L366 mentioned \"scenarios mentioning an “approved directive” tend to be assigned to the second cluster\", which contradicts with the figure.\n\nThe inherent connection between different scenarios corresponding to a certain action is not analyzed. However, this is one key motivation for clustering scenarios of one action."}, "questions": {"value": "L215: What is the ideal clustering? How to obtain these clusters? \nWhat does it mean by the ARIs at about 80%. It is unclear how good is the clustering. \n\nIt may be helpful to show the performance of action parsing, and examine the effect of the parsing performance on the consequent context clustering and moral prediction. How to determine the number of actions is also important to investigate.\n\nHow is the generalizability for new actions. The authors may provide experiments with one action held out during context clustering, and test moral prediction on the contexts with the held-out action.\n\nTo show the inherent connection between scenarios, the LLM-extracted descriptive contextual features for a cluster can be demonstrated together with representative scenarios to show the connections.\n\nIn some parts, 'scenarios' are also referred to as 'states' (L293), which confuses the audience. It would be helpful to use one term consistently.\n\nFigure 1, data gathering is confusing, why there are three groups of responses? What do the response distributions mean?\n\nFigure 3 shows the reasonable clustering of new scenarios, how about the effect of the small cluster on the consequent moral prediction? Will the small newly-created cluster that contains the new scenarios cause inferior results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P0ma2L8h0s", "forum": "99QW2hvwd0", "replyto": "99QW2hvwd0", "signatures": ["ICLR.cc/2026/Conference/Submission20140/Reviewer_WLw9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20140/Reviewer_WLw9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734078165, "cdate": 1761734078165, "tmdate": 1762933168782, "mdate": 1762933168782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes COMETH, a pipeline that learns moral contexts of a core action by grouping moral scenarios having human judgment distributions (Blame/Neutral/Support). The goal is to model and explain how context changes the acceptability of ambiguous actions. First, an LLM‑driven pre‑processing step normalizes each scenario into a core action and clusters actions via MiniLM embeddings + K‑means. Then, for each action, a probabilistic context learner clusters scenarios online based on their ternary human‑judgment distributions, adding a new context based on KL divergence and merging redundant contexts using a semi‑weighted Jensen–Shannon criterion. To generalize and provide interpretability, a generalization model extracts binary descriptive features and  learns feature weights that assign new scenarios to contexts and predict judgment distributions. Built on 300 scenarios across six actions with 101 participants, COMETH yields feature‑level explanations and roughly doubles alignment with majority human judgments versus end‑to‑end prompting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper offers an original reframing of context‑sensitive moral evaluation. Rather than predicting a single label from text, it learns action‑specific moral “context models” by clustering scenarios according to the empirical distributions of human judgments, with the number of contexts per action emerging from the data. It then generalizes and explains these contexts via an interpretable module that uses LLM‑derived, non‑evaluative binary features and learns feature weights to assign new scenarios to contexts and predict their judgment distributions. \n\nThe technical quality is solid and reproducibility‑minded. The appendix provides sufficient technical information for all modules (i.e., formulas, procedures and threshold search of adding and merging modules, prompts of the pre-processing and feature extraction steps, experimental settings of the probabilistic context learner and the generalization module). Extended tables report ARI for pre‑processing and alignment/error breakdowns across models and prompts, corroborating the x2 gain over end‑to‑end models.\n\nAnother strength is that COMETH naturally accommodates growth: when new human-labeled scenarios are added, the representation is updated by assigning them to existing contexts when they fit or creating new contexts when they do not.\n\nIn terms of clarity, the manuscript is easy to follow endtoend. Figure 1 provides a clear roadmap of the pipeline. Terminology stays consistent throughout, and technical details (prompts, optimization, parameter search) are deferred to the appendix, supporting reproducibility without interrupting the flow."}, "weaknesses": {"value": "The study pools 101 raters split into six groups (50 scenarios each; so about 16–17 raters per scenario) with items presented in multiple languages and with demographic information collected. Yet the modeling and evaluation are fully pooled, with no stratification by language or demographics as provided in the survey materials (Appendix A.1). In a setting where moral judgments are culturally sensitive, as the annotator pool per scenario increases, the empirical blame/neutral/support distributions may broaden or become multimodal since they reflect genuine plurarism of point of views; so, majority‑label alignment may then mask subgroup structure, yielding context models that depend on the mix of annotators rather than stable moral regularities. It would be significant to learn/evaluate contexts separately by language, country and basic demographics (e.g., age, gender), then compare number/locations of contexts, assignment overlap, and feature‑weight patterns across subgroups. To better situate choices when Blame/Neutral/Support distributions stay broad, it would be helpful to ask raters for a one-sentence rationale (with a feature extraction phase) and/or to indicate the moral driver they prioritized (a possibility would be to choose a moral value from the Moral Foundation Theory), so that disagreement becomes a leading signal. These steps would test whether COMETH’s contexts are robust to annotator variance and reveal where genuinely different moral frames are at play.\n\nThe corpus comprises 300 impersonal scenarios (6 actions × 50), with many variants produced by GPT‑4 and then manually rewritten to ensuring that participants evaluated the morality of others’ actions rather than their own decisions. This narrow topical breadth and single‑generator provenance risk introducing stylistic and topical regularities that can inflate downstream performance and reduce ecological validity - especially because the generalization module depends on LLM‑extracted binary features, which may latch onto generator‑specific phrasing rather than genuinely contextual factors. It would be beneficial to add human‑authored scenarios or vary the generators to diversify style, e.g., create additional variants with different LLMs and provide cross‑source generalization. These controls would clarify whether COMETH’s gains reflect context understanding rather than generator‑induced regularities.\n\nCore‑action extraction achieves high ARI against an “ideal” 6‑way partition, but because the corpus itself was constructed from exactly those six actions (50 each), this in‑domain score likely overstates robustness. This is a closed-world setting: the proposal evaluates the separation only between those six canonical classes, on the same domain from which the examples come. This does not guarantee that preprocessing remains reliable when unexpected actions appear (distractors) or the same scenario contains multiple actions or intertwined actions. In this regard, the pipeline extracts a single core action per scenario, yet real narratives can contain multiple actions. Accordingly, the evaluation should include these scenarios (distractors and multiple actions) since they reflect more realistic cases and would better characterize robustness beyond the current setting.\n\nMinor issues and typos:\nIn Section 2, the acronyms COMETH, MBRL, and LLMs are reintroduced even though they were already defined just above.\nAt the end of Section 2.2, the sentence “The final survey (N = 101, mean age = 35.2, 48 women) asked participants to judge each action as Blame, Support, or Neutral” should probably read “each scenario” instead of “each action.”\nIn Section 3.2, the meaning of “state” in (state, action) is not made explicit.\nIn Appendix A.2, the authors write that “the full set of 300 scenarios (50 per action) is provided in the appendix”, but these scenarios are not actually included.\nAppendices A.2 and A.3 contain missing cross-references (“Section ??”).\nIn Appendix A.6, it is unclear why the prompts refer to five clusters instead of six."}, "questions": {"value": "Main questions and suggestions are detailed in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YRLNk86gaM", "forum": "99QW2hvwd0", "replyto": "99QW2hvwd0", "signatures": ["ICLR.cc/2026/Conference/Submission20140/Reviewer_XNyg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20140/Reviewer_XNyg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840657073, "cdate": 1761840657073, "tmdate": 1762933167412, "mdate": 1762933167412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ChatGPT said:Moral judgment depends not only on an action’s outcome but also on its surrounding context. This paper introduces COMETH (Contextual Organization of Moral Evaluation from Textual Human inputs), a framework that models how contextual cues shape the moral acceptability of ambiguous actions. Using 300 empirically curated scenarios across six core moral actions (e.g., killing, deception, law-breaking), COMETH integrates probabilistic context learning with LLM-based abstraction and human moral judgments. It constructs reproducible context clusters from human responses and learns interpretable contextual features through a transparent likelihood model. Empirically, COMETH doubles alignment with human majority judgments compared to end-to-end LLMs, offering an interpretable and empirically grounded approach to context-sensitive moral reasoning."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "This paper exploits the conference submission format by substantially shrinking the page margins. Hence, I will desk reject the paper for the severe format violation."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kyGwXfl5ts", "forum": "99QW2hvwd0", "replyto": "99QW2hvwd0", "signatures": ["ICLR.cc/2026/Conference/Submission20140/Reviewer_2C2Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20140/Reviewer_2C2Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987288348, "cdate": 1761987288348, "tmdate": 1762933166783, "mdate": 1762933166783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}