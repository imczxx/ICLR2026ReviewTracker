{"id": "nYpOd7Eiq8", "number": 18403, "cdate": 1758287301438, "mdate": 1759897105655, "content": {"title": "Turn-Level Trajectory Optimization for Robust Multi-Turn LLM Reasoning", "abstract": "Large Reasoner Models (LRMs) excel at single-turn reasoning but often degrade in multi-turn settings due to insufficient alignment at the dialogue level. We propose Turn-level Trajectory-Clipping with Back-Propagation Optimization (TTPO), a critic-free Reinforcement Learning from Verifiable Rewards (RLVR) algorithm that extends GRPO to robust multi-turn reasoning. TTPO introduces three components: (i) a turn-level policy ratio with PPO-style clipping, treating each turn as a unified action; (ii) trajectory clipping, which prunes low-reward branches to mitigate exponential forking; and (iii) reward back-propagation, which propagates discounted terminal rewards to earlier turns for stable credit assignment. Experiments across six representative multi-turn tasks—Code, Database, Math, Actions, Data-to-Text, and Summarization—show that TTPO substantially improves mean performance while sharply reducing run-to-run volatility (U90–10) without sacrificing high-percentile quality (A90). Ablations confirm contributions from all three components, with trajectory clipping and reward back-propagation yielding the largest reliability gains. These results demonstrate that turn-level alignment offers a simple and general recipe for robust long-horizon dialogue reasoning.", "tldr": "", "keywords": ["LLM; RL; Post-training;multi-turn conversation; Reasoning LLM; LRM;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/860247b0367f38e2bb7109118c295f1a6a5a03b0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes TTPO, a RL method that optimizes large language models at the turn level instead of the token level to improve multi-turn reasoning stability.\nTTPO introduces turn-level policy ratios, trajectory clipping, and reward back-propagation to address instability, sparse rewards, and poor credit assignment in long dialogues.\nExperiments across six reasoning tasks show significant gains in mean performance and reliability, reducing training variance by up to 40% compared to GRPO while maintaining peak quality ."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Six representative multi-turn tasks provide broad coverage.\n- Improvements in both mean and stability (∼40% reduction in run variance) are significant, especially under SHARDED settings that mimic real-world multi-turn interactions."}, "weaknesses": {"value": "- The paper introduces 3 tricks: turn-level policy ratios, trajectory clipping, and reward back-propagation, but provide no referneces for discussion\n- The implementation details in Algorithm 1 (line 12) are unclear."}, "questions": {"value": "- It would strengthen the empirical claims of stability if the paper included training curves (e.g., reward, KL divergence, or loss over steps) comparing TTPO and GRPO.\n- The logic of Algorithm 1, particularly the loop starting at line 5, is confusing. The description suggests that TTPO divides each trajectory into multiple turn-level data points, but it is unclear how these turns are processed sequentially or in parallel.\n- In line 8, it is not clear how to compute the advantages if the rollout samples has different number of turns. \n- After line 10, when additional outputs are sampled to maintain batch size, should the steps in lines 6–9 (reward computation, propagation, advantage estimation, and clipping) be recomputed for the new samples, or are the previous statistics reused? Please clarify this iterative process.\n- In line 5, does this loop refer to iterating over turns within a single trajectory or across turns treated as independent sub-trajectories for optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8GHzYUUAfO", "forum": "nYpOd7Eiq8", "replyto": "nYpOd7Eiq8", "signatures": ["ICLR.cc/2026/Conference/Submission18403/Reviewer_KCHs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18403/Reviewer_KCHs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772518446, "cdate": 1761772518446, "tmdate": 1762928108107, "mdate": 1762928108107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Turn‑level Trajectory‑Clipping with Back‑Propagation Optimization (TTPO), which is a critic‑free RLVR extension of GRPO for robust multi‑turn LLM reasoning. TTPO has three parts: (i) turn‑level policy ratios (treat a whole turn as one “action”); (ii) trajectory clipping (prune low‑reward branches); and (iii) reward back‑propagation across turns via discounting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper treats turns (not tokens) as actions is a natural and useful abstraction for multi‑turn reasoning. \n2. The three components (turn‑level ratio, clipping, reward propagation) are easy to implement in GRPO/PPO style loops. \n3. “premature submission” and “answer bloat” analyses (p. 9, §5.5) provide plausible behavioral interpretations."}, "weaknesses": {"value": "1. Turn‑level ratio definition (Equation 3) is non‑standard and likely biased. In PPO/GRPO, if you aggregate multiple atomic actions into one macro‑action, the correct ratio for that macro‑action is the product of token conditionals (or equivalently, the exp of the sum of log‑ratios, i.e., a geometric aggregation). An arithmetic mean of ratios does not equal the probability ratio of the whole turn, introduces bias, and can mis‑scale clipping.  \n2. Equation. 1 defines group‑relative advantage at the response level; later, the text alternates between $\\hat A_{i,j}$ and $\\hat A_{i,t,j}$ without formal definitions. Equation 6 mixes a turn‑level ratio $r_{i, t}$ with token‑level advantages $\\hat A_{i,t,j}$ but never specifies how $\\hat A$ is computed per token/turn (mean‑std normalization over which set?). \n3. Table 3 says \"– no reward back‑prop ($\\gamma$ = 1.0)\" (p. 8). But with the Equation, 5 means no discounting (the reward is propagated equally to all turns), which is back‑propagation, just undiscounted. No back‑prop” should set rewards only at the terminal turn (e.g., $R_{i, t}=0$ for $t<T_i$). Please fix this ablation and re‑report the results.  \n4. The paper alternates among:\n- \"Turn‑level Trajectory‑Clipping with Back‑Propagation Optimization\" (title/abstract)\n- \"Turn–Trajectory Propagation Optimization\" (Sec. 3.2 heading)\n- \"Turn‑level Trajectory Back‑Propagation Optimization\" \n- “TTBPO\" in Fig. 3 (p. 18) \nThese should be unified as one name and acronym throughout. \n5. Fig. 2 asserts that TTPO‑Qwen3‑32B is \"competitive with GPT‑4o\" and \"surpasses Claude‑3.7 Sonnet and Gemini‑2.5‑Pro\" on SHARDED metrics, but exact numbers, variance bars, and evaluation parity (token limits, reasoning tokens, temperature) are not shown; Table 1 does not list business models.  \n6. he sharding pipeline drops items that fail verification or yield <3 shards and requires CONCAT/SHUFFLE‑CONCAT to hit ≥80%. This can change the task distribution in ways that favor methods tailored to sharding. Please report how many items were filtered, per task, and provide results on the full unfiltered sets and standard public suites (e.g., GSM8K, Spider, etc.) in non‑simulated settings to validate generality."}, "questions": {"value": "Suggestions: \n1. Because Eq. (3) is central, add an ablation comparing the arithmetic mean vs geometric mean vs exact sequence ratio of token probabilities within turns. This will directly test the hypothesis that your ratio choice stabilizes learning. \n2. This paper states ~1.2× per‑step overhead and 30% fewer steps, yielding net efficiency. Provide wall‑clock, steps‑to‑target, GPU type and count, and batch and G values for each method and model size. \n3. Sections 5.1–5.2 read as marketing (\"quantum leap,\" \"paradigmatic breakthrough\". Adopt a neutral, scientific tone. \n4. Provide a brief derivation showing that your chosen turn‑level ratio yields an unbiased (or practically low‑bias) estimator of the sequence‑level PPO objective, or explain why a geometric aggregator approximates the true ratio better than an arithmetic mean.  \n5. Fix Typo issues in weakness 4. \n6. What is POMDP? (P. 3, line 188)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TmB6RkHGDK", "forum": "nYpOd7Eiq8", "replyto": "nYpOd7Eiq8", "signatures": ["ICLR.cc/2026/Conference/Submission18403/Reviewer_FtZG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18403/Reviewer_FtZG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938796158, "cdate": 1761938796158, "tmdate": 1762928107609, "mdate": 1762928107609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a reinforcement learning framework called TTPO (Turn-Level Trajectory Back-propagation Optimization), which extends GRPO and aims to enhance the stability of multi-turn LLM inference. TTPO performs turn-level optimization, treating each dialogue turn as an action and employing a PPO-style pruning method; it applies trajectory pruning to discard low-reward or unstable unfolds; and introduces reward backpropagation with a discount factor γ to improve credit assignment across turns. As a criticless RLVR algorithm, TTPO significantly improves stability, average performance, and reliability across six inference domains: code, database, mathematics, action, data-to-text, and summarization, reducing inter-run variance by approximately 40% and achieving 7-9 percentage point performance improvements on Qwen3-8B and Qwen3-32B models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "TTPO is optimized at the round-level to align the learned signal with the actual operation of multi-round agents. This round-level objective reduces the proportional noise present at the token level and stabilizes advantage estimation across messages. It also supports round-by-round reward adjustment while avoiding overfitting to surface-length content.\n\nThe method prunes low-reward branches, thereby suppressing variance introduced by degenerate unfolding and limiting credit leakage to out-of-policy detours. As TTPO applies pruning at the trajectory/round level, it preserves coherent inference chains rather than cutting sentences during thinking. This design allows batch statistics to perform well under group relative normalization.\n\nEvaluation prioritizes reliability over ex-post consideration. In addition to average accuracy, the authors report U90-10 and A90, which directly reflect inter-run variability and significant tail failures during deployment. This emphasis encourages the method to sacrifice a small amount of raw score for greater stability where appropriate."}, "weaknesses": {"value": "Inconsistencies in terminology and specification exist across different sections of this paper, as method names and algorithm labels vary, and index notation sometimes mixes round numbers and labels. These inconsistencies increase the risk of implementation mismatches; therefore, this paper should enforce a standardized set of notations, thresholds, and index ranges throughout the text, figures, and pseudocode.\n\nThe ablation experiment evidence is insufficient to support causal claims because the method incorporates multiple controls (round number proportioning, trajectory pruning, and reward backpropagation) but only reports placeholder tables and omits the results of cost-matching runs. This paper should replace placeholders with final values, add confidence intervals, and ensure strict computational equivalence to separate the contributions of each component.\n\nThe semantic definition of reward propagation controlled by γ is ambiguous because this paper does not precisely distinguish between \"no backpropagation\" and \"full backpropagation\", nor does it describe which round numbers yield non-zero integrals in each setting. The paper should map each γ choice to a specific reward allocation within a round and explain how discounts interact with variable dialogue lengths.\n\nThe pruning rules and their thresholding procedures are inconsistent because the text alternates between percentile-based thresholds and indicator functions with different scopes (per round, per trajectory, and per group); the paper should define a single, auditable pruning predicate, clearly specify the computational domain, and explain the rationale for the choice so that readers can understand bias and sample efficiency.\n\nThe evaluator and simulator settings lack sufficient transparency because reported win rates or consistency with robust closed models depend on internal slicing, hints, and reward judges; the paper should expose these components, or at least provide cross-judge robustness checks to support external validity and fair comparisons."}, "questions": {"value": "What exactly is “turn” in mixed tool-use settings, does a tool call within an assistant message count as a separate action for ratios and rewards?\n\nHow is the reference policy $\\pi_{ref}$ chosen per step (line 1 in Algorithm 1)? Do you use a moving copy (“online-to-target”) or the initial model? How sensitive are results to β schedules?\n\nwhat is the variance-reduction vs cost trade-off across G$\\in${2,4,8,16}? Any instability for small G when clipping removes many samples?\n\ndoes pruning low-reward branches bias the policy toward conservative behaviors (e.g., fewer exploratory questions)? Any evidence of mode collapse in later turns?\n\nCompatibility with token-level shaping: can turn-level ratios be combined with token-level process rewards (e.g., “verify-step-by-step”) without double counting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AznhwahVb3", "forum": "nYpOd7Eiq8", "replyto": "nYpOd7Eiq8", "signatures": ["ICLR.cc/2026/Conference/Submission18403/Reviewer_kdh3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18403/Reviewer_kdh3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002060584, "cdate": 1762002060584, "tmdate": 1762928107238, "mdate": 1762928107238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Turn-Level Trajectory-Clipping with Back-Propagation Optimization (TTPO) designed to enhance the robustness and stability of multi-turn reasoning. TTPO extends GRPO by applying turn-level policy ratios, trajectory clipping, and cross-turn reward propagation to better assign credit across conversational steps. Experiments on diverse reasoning domains show that TTPO improves both mean task performance and training stability compared to GRPO baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "TTPO consistently improves mean performance and especially reliability across multiple tasks and scales. \nThe results are thorough and supported by ablations. The ablation study isolates the contribution of each TTPO element."}, "weaknesses": {"value": "The novelty of TTPO over GRPO is limited, as its main ideas—turn-level policy clipping, reward propagation, and trajectory pruning—are standard RL adaptations rather than fundamentally new techniques.\nThe experimental comparison is weak, relying mainly on GRPO; stronger baselines such as GiGPO, SPO, or other process-level RL methods should be included to validate the claimed improvements.\nThe algorithm description lacks clarity and completeness\n[1] Group-in-Group Policy Optimization for LLM Agent Training\n[2] Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models"}, "questions": {"value": "Could the authors provide detailed step-by-step descriptions of TTPO, particularly clarifying line 6-12\nWhy were methods such as GiGPO, SPO, or other process- or group-based RL baselines not included in the comparison? How might TTPO differ from these in principle or behavior? It is better to do some experiments or have some discussion\nHow well does TTPO scale with longer trajectories (e.g., 10–20 turns) or more complex environments beyond the current six reasoning domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w9Lq81vmWd", "forum": "nYpOd7Eiq8", "replyto": "nYpOd7Eiq8", "signatures": ["ICLR.cc/2026/Conference/Submission18403/Reviewer_2vzD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18403/Reviewer_2vzD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18403/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003888750, "cdate": 1762003888750, "tmdate": 1762928106675, "mdate": 1762928106675, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}