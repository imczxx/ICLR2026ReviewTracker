{"id": "BpBW4gJofo", "number": 12616, "cdate": 1758209014267, "mdate": 1759897498204, "content": {"title": "Canonical cortical circuits: A unified sampling machine for static and dynamic inference", "abstract": "The brain lives in an ever-changing world and needs to infer the dynamic evolution of latent states from noisy sensory inputs. Exploring how canonical recurrent neural circuits in the brain realize dynamic inference is a fundamental question in neuroscience. Nearly all existing studies on dynamic inference focus on deterministic algorithms, whereas cortical circuits are intrinsically stochastic, with accumulating evidence suggesting that they employ stochastic Bayesian sampling algorithms. Nevertheless, nearly all circuit sampling studies focused on static inference with fixed posterior over time instead of dynamic inference, leaving a gap between circuit sampling and dynamic inference. To bridge this gap, we study the sampling-based dynamic inference in a canonical recurrent circuit model with excitatory (E) neurons and two types of inhibitory interneurons: parvalbumin (PV) and somatostatin (SOM) neurons. We find that the canonical circuit unifies Langevin and Hamiltonian sampling to infer either static or dynamic latent states with various moving speeds. Remarkably, switching sampling algorithms and adjusting model's internal latent moving speed can be realized by modulating the gain of SOM neurons without changing synaptic weights. Moreover, when the circuit employs Hamiltonian sampling, its sampling trajectories oscillate around the true latent moving state, resembling the decoded spatial trajectories from hippocampal theta sequences. Our work provides overarching connections between the canonical circuit with diverse interneurons and sampling-based dynamic inference, deepening our understanding of the circuit implementation of Bayesian sampling.", "tldr": "The canonical recurrent neural circuit dynamics employes sampling algorithms to flexibly compute static and dynamic posteriors", "keywords": ["Recurrent neural circuit", "Inhibitory interneurons", "hidden Markov model", "Bayesian inference", "sequential sampling"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1b617810d9adc15dacc9eb89f340993671b89061.pdf", "supplementary_material": "/attachment/1662cc73a979f30240a71bc63ad6480d628430c6.zip"}, "replies": [{"content": {"summary": {"value": "The paper adapts a bump attractor network with two kinds of inhibition and intrinsic receiving stimulus-tuned poisson input (originally developed in Sale&Zhang2024 and preceding); it uses it as a way to implement different forms of sampling from a distribution over a one dimensional circular variable.  Mechanistically, the attractor dynamics make for a stable bump of activity which is interpreted as a parametric \n\nThe dynamic inference takes the form of linear gaussian dynamics whose solution is equivalent to a Kalman filter."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The recurrent circuit matches coarse anatomy of cortical circuitry and can be mathematically traced back to long history of mathematically tractable attractor dynamics.\n\nThe parametric nature of the encoding allows for the circuit to process inputs with varying degrees of uncertainty without having to adjust synaptic strengths, which is particularly interest for dynamic stimuli  (this was true about old versions of the model for static inference but is also inherited to this variant).\n\nAlthough the text is dense in places, it is generally clear what was done and how (bar the explanation of the mapping between sampling dynamics and circuit elements which could need clarification and additional justification)."}, "weaknesses": {"value": "The conceptualization of the problem as a whole: The general logic of using sampling-based inference for computations that are fundamentally parametric using a network that explicitly encodes the parametric form with sampling on top lacks the even the most basic normative justification. It's sampling for sampling's sake rather than in service of actually achieving representational gains for complex high-dimensional posteriors, which was the original logic of neural sampling as introduced by Hoyer & Hyvärinen, 2003 and Fiser et al 2010 and the way it has been used to relate to visual neural activity in later works like those from Orban and Echeveste. Computationally, I fail to see the point of why do things this way. A lot of the logic feels backward to me as it forces an assumption of world statistics needing to match the constraint of the circuit dynamics rather than the typical other way around. \n\nComputational limits: The going from well established attractor dynamics to an induced model is also extremely restrictive and allows little to no variation outside of a very simple generative model form. This means the ability of such a model to generalize across natural world statistics in different modalities or even outside the simple low-d linear gaussian set difficult if not impossible. \n\nValidation of the quality of sampling: given the very coarse approximation which replaces the full prior form one step back with a single sample from it, it is not clear neither mathematically nor numerically that the resulting system is actually sampling from the same posterior density as the ground truth kalman filter. at the very minimum the quality of the approximation is expected to decrease with posterior or sensory uncertainty which i have not seen analyzed in any systematic way in the text but seems critical for demonstrating the circuit actually achieves the claimed computational function. \n\nComparison to previous model alternatives: i would have liked to see a more precise discussion about predictions that the model makes which are actually distinguishable from previous proposals. usually with sampling-based representations the mean responses do not distinguish form a parametric solution such as that from Deneve and Pouget 2007 (although in this instance the fact that the alternative does exact close to exact kalman whereas this relies on a coarse approximation for the propagation of uncertainty over time may already have signatures in readout behavior or neural mean responses, not sure); distinguishable features in this space would usually take the form of predictions about response variability but i did not see anything of the form in the text.\n\nNovelty and significance: at the technical level this seems like an incremental variation of the previous models in Zhang et al, Sale et al, with complete equivalence for the static case and minimal change for the kalman addition."}, "questions": {"value": "Description of dynamics in Eq 1a-e is missing details. Can you please make sure you list and explain each variable appearing there.\n\nWhat is the computational purpose of Hamiltonian sampling for a 1d density? The use of Hamiltonian dynamics (Aitchison), or nonnormal dynamics (Hennequin) or other forms of annealed sampling (Savin) have historically been invoked in the service of sampling fast from complex distributions which are either heavily correlated or multimodal. I fail to see the logic of doing the same here. Please explain. \n\nMissing literature: several Kalman filtering implementations have been proposed in the lit. that do not seem referred to, e.g. Deneve et al 2007 \"Optimal Sensorimotor Integration in Recurrent Cortical Networks: A Neural Implementation of Kalman Filters\" which do not require the very coarse computational approximations due 1 sample per step updates. \n\nThe math in the sampling section makes some implicit assumptions w.r.t. the map between the inferred latent variable z_t and the population responses of the neurons in the circuit u or r_e. Please clarify this map explicitly in the main text. More generally, the exact mapping between the neural dynamics elements and the dynamic inference implementation could be generally more explicit and better explained. \n\nCan you comment on the biophysical nature of the mechanism that controls the gain of the SOM subpopulation and how is that supposed to be calibrated to the true speed of the moving sensory stimulus?\n\nthe faster sampling with faster speed is misleading as a mathematical statement\" sampling speed refers to the autocorrelation of MCMC samples drawn from the same density whereas here we are talking about changes across samples driven by temporal changes in the mean of the density itself. this seems formally incorrect as a statement. \n\nHamiltonian dynamics: can you please comment on the exact map between the variables of the sampler and the neural activity, in particular how is the momentum variable encoded in neural activity? that was not clear in either the static nor the dynamic case. The way i understand the approach as a whole it is a matter of establishing a direct map between dynamic variables and then work out by structural equation identification the expression for various pieces of the connectivity so i don't quite get how one can do that without having some set of neural variables explicitly implementing the momentum auxiliary variable. Or if there are such variables how do they fit into the canonical circuit definition presented at the start.\n\nIs the momentum variable also estimated via single samples? how do you ensure that the dynamics as a whole remain volume preserving which such a coarse approximation ? what guarantees that the resulting circuit samples from the right distribution and does not accummulate errors over time? is there a formal proof for that statement or something you observe empirically and if the latter then under what kind of conditions?\n\nWhat is the size of the networks numerically simulated in Fig2? \nis this the prediction of the asymptotic limit or the dynamics of a finite simulated network? \nSame question for Fig3\n\nWhat makes the oscillations of hamiltonian dynamics (well documented in past work including Aitchison) have anything to do specifically with hippocampal activity? how is a simple kalman filter in 1d related to hippocampal computations? can you please expand on the nature of the analogy you are trying to build between model and data and at what qualitative level should one evaluate that comparison\nWhat is unique about it to your model as opposed to more explicit internal models of hippocampus based on sampling proposed before, e.g. by Ujfalussy and Orban?\n\nDiscussion: based on the textbook definition of the term HMMs assume discrete latent variables whereas the speed of stimulus is continuous, why talk about alpha-beta type of inference and smoothing in HMMs specifically when 1) filtering is all you do for kalman inference and 2) a hierarchical kalman seems much more suited to the continuous nature of the latent variable in question. Please explain or correct."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "UoMK6wL40K", "forum": "BpBW4gJofo", "replyto": "BpBW4gJofo", "signatures": ["ICLR.cc/2026/Conference/Submission12616/Reviewer_7SmZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12616/Reviewer_7SmZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761685878114, "cdate": 1761685878114, "tmdate": 1762923463310, "mdate": 1762923463310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a canonical circuit that unifies Langevin and Hamiltonian sampling to infer either static or dynamic latent states\nwith various moving speeds. \n\nIt is shown that switching sampling algorithms and adjusting internal latent moving speed can be realized by modulating the gain of SOM neurons without changing synaptic weights. \n\nWhen the circuit employs Hamiltonian sampling, trajectories resemble the decoded spatial trajectories from hippocampal theta sequences.\n\nWhile the approach is elegant, it is hard to determine what the novel contributions are beyond previous work and to what extent the approach scales to more challenging real-world inference problems."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper provides an elegant theoretical framework with which Bayesian inference is related to processing in canonical biological circuits. The results show effective Langevin and Hamiltonian sampling in static and dynamic settings."}, "weaknesses": {"value": "I find it hard to assess the novelty of this paper compared to the following earlier related work that is cited in the paper: \n\nEryn Sale and Wenhao Zhang. The bayesian sampling in a canonical recurrent circuit with a diversity of inhibitory interneurons. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\n\nThe current presentation is quite dense and it remains unclear if the current paper is an incremental improvement over this previous work. It would help if the authors make more clear in the text which contributions go beyond previous work. It would also help if previous work is used as a quantitative baseline to compare with to show where the current model provides significant improvements.\n\nThe approach is tested in simple toy settings. How does this generalize to more complex real-world Bayesian inference problems which may be high-dimensional and multimodal? It would help if the authors provide more extensive analyses to demonstrate the importance of their work for machine learning. Scaling results will also provide more certainty that the developed theory is used in biological systems.\n\nFigure 2: it remains unclear what the quality of the static and dynamic inference is. Which metrics are used? Can baselines be introduced that demonstrate improvement over established approaches? Fig 2C suggests that the tracking of the true z is completely off. I may interpret this wrongly but more explanation/interpretation would be useful. The same for Figure 3. Panels D and F suggest that the sampling is far off from what would be ideal behaviour.\n\nThe authors make a connection between sampling and decoded spatial trajectories during hippocampal theta sequences. It remains unclear if it is valid to make a direct comparison between these two processes."}, "questions": {"value": "The authors refer to canonical *cortical* circuits in their title and text. However, they make a link to biology by referring to hippocampal theta wave. However, hippocampus is a *subcortical* structure. This requires some thought.\n\nIn general text should be checked for typos and grammatical errors. There are quite a few sentences that are unintelligible. For example:\n\nLine 101: fix Fig. A1B\n\nLine 134: sentence needs some initial words.\n\nLine 147: It has established theoretical approach  => fix\n\nLine 208: Although the RBF with Gaussian case exists exact inference via Kalman filter\n\nLine 264: remove ’t’ in this sentence\n\nLine 448: Eq,\n\nLine 460: Missing period\n\n\\xi_t in eqn 12 non-bold (consistency wrt other eqns)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EOaYHEPnv3", "forum": "BpBW4gJofo", "replyto": "BpBW4gJofo", "signatures": ["ICLR.cc/2026/Conference/Submission12616/Reviewer_tmYd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12616/Reviewer_tmYd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904559659, "cdate": 1761904559659, "tmdate": 1762923462918, "mdate": 1762923462918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces neural circuit implementations of dynamical Bayesian inference (recursive Bayesian filtering) based on a sampling-based code. It explores Langevin and Hamiltonian non-equilibrium dynamics in E/I attractor networks with two types of inhibitory neurons, putatively corresponding to PV and SOM neurons. It suggests interesting and computationally novel roles for SOM neurons and theta oscillations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper brings together sophisticated analyses of E/I attractor network dynamics with sampling theory, including advanced forms of sampling (Hamiltonian and Riemannian manifold), and makes novel links to biology."}, "weaknesses": {"value": "- The main (and undiscussed) limitation of the model that it is restricted to a generative model in which the observations are the firings of a Gaussian-Poisson neural population encoding the relevant latent variable directly (and so the likelihood is a linear PPC). This prevents inference under any generative model of practical interest in which observations may be related to latent variables in more interesting way (e.g. inferring head direction from noisy self motion inputs and visual landmarks). \n\n- The proposed algorithm essentially performs particle filtering with a single particle. The authors advocate for this essentially on biological plausibility grounds — but give the obvious concerns about computational accuracy short shrift by suggesting that a sufficient separation of time scales between latent and neural network dynamics will solve this problem automatically. I don't see how that is possible — there is potentially catastrophic information loss the moment the posterior is collapsed onto a single particle, and this information cannot be recovered no matter how soon the particle is updated. More broadly, I saw no calibration of the performance of the network (e.g. comparisons to standard recursive Bayesian filtering algorithms — particle-based or otherwise). Figures 2C & 3D do not allay my concerns about computational performance: the neural trajectories are far from the latent, and there is no attempt to extract error bars from neural activities to see if at least the latent is within error bars (or vice versa, if the neural trajectories are within the error bars of the exact posterior — see above comment about lack of calibration wrt e.g. exact inference). Figure 2I looks better, but it's dominated by prior knowledge baked into the network about latent variable drift, so it's unclear how online inference by the circuit actually contributes here.\n\n- The links to biology are interesting but somewhat superficial. In particular, the first-order effect of theta oscillations on hippocampal activity is a strong modulation of overall firing rates of both E and I cells (with different characteristic profiles and preferred phases of firing for E, PV, and SOM cells [the latter aka O-LM cells in the hippocampus]). Also highly relevant for the current model is that theta frequency shows robust modulation by movement speed. Are any of these effects borne out in the model?\n\n- The presentation is somewhat confusing. For example, based on section 3.2, in the static case, p(z_{t+1} | \\tilde{z}_t) is a delta on the previous location of the particle (from Eq.5), which can thus only stay at the same place in the next time step (based on Eq.8). Then, in section 3.3, for the Langevin sampling variant (also shown in Fig.2A), it is stated that tau_L \\propto \\Omega_t is used (from Eq.13). However, from Eq.9 \\Omega_t→\\infty in the static case (because \\Lambda_z→\\infty) which would again suggest infinitely slowed down dynamics. Despite all this, in Fig.2A this is clearly not the case.\n\n- There is ambiguity as to whether the fact that the proposed \"canonical circuit unifies Langevin and Hamiltonian sampling\" is itself novel or not. The abstract makes it sounds it is novel, but later it seems that the static case has already been covered by Sale & Zhang, 2024. \n\n- There are a number of grammatically incorrect sentences. E.g. \"Although the RBF with Gaussian case exists exact inference via Kalman filter\""}, "questions": {"value": "- Is there a meaningful way to quantify the performance of the circuit models and compare it to relevant baselines (as well as to one another)? \n\n- Can the model \"predict\" theta modulation, and different theta phase preferences, of different E and I cell types, or the modulation of theta frequency by running speed?\n\n- Could you discuss more explicitly novelty wrt Sale & Zhang, 2024?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WRl1qCA6tL", "forum": "BpBW4gJofo", "replyto": "BpBW4gJofo", "signatures": ["ICLR.cc/2026/Conference/Submission12616/Reviewer_ougJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12616/Reviewer_ougJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762125222776, "cdate": 1762125222776, "tmdate": 1762923461985, "mdate": 1762923461985, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the work of Sale & Zhang (2024) on Bayesian sampling in canonical cortical circuits to the problem of dynamic inference. The authors propose that the same circuit, consisting of excitatory neurons, and parvalbumin/somatostatin interneurons, can implement both Langevin and Hamiltonian sampling to infer either static or dynamic latents. The key mechanism enabling this flexibility is the modulation of SOM neuron gain ($g_{S}$​), which the authors decompose into two components: a speed-dependent gain that encodes the stimulus velocity, and a “switching” gain that controls the ratio between Langevin and Hamiltonian sampling. The authors demonstrate through theoretical analysis of the nonlinear circuit dynamics that the circuit can track moving stimuli without modifying synaptic weights, and show that Hamiltonian sampling trajectories produce oscillations resembling hippocampal theta sequences."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: the paper makes a theoretical contribution from static to dynamic inference within the Sale & Zhang (2024) sampling framework. While the foundational circuit architecture is borrowed from Sale & Zhang (2024), the application of this model to dynamic inference represents conceptual progress. They provide a novel functional role for the gain of the SOM neurons in the context of dynamic stimuli, and offers an original perspective on the functional role of oscillations in hippocampal theta sequences.\n\nQuality: the mathematical analysis is rigorous and follows established methods for analyzing continuous attractor networks. The authors provide detailed perturbative analysis, eigenmode decomposition, and explicit mappings between circuit dynamics and Langevin and Hamiltonian sampling algorithms. The derivations connect circuit parameters to posterior distributions in a principled way.\n\nClarity: the paper is generally well-structured. The progression from static to dynamic inference is well explained, and the figures are effective in illustrating the main concepts and results. The supplementary materials provide thorough derivations.\n\nSignificance: the potential unification of different sampling algorithms within a single circuit architecture is conceptually appealing. The model makes concrete predictions about the relationship between SOM gain, stimulus speed, and neural oscillations, and if these are validated experimentally, this work could offer insights into how cortical circuits implement probabilistic inference in dynamic environments."}, "weaknesses": {"value": "1. The most fundamental weakness in the paper is in what constitutes “dynamic inference”. The circuit already receives the velocity $v$ as input via SOM gain modulation (Eq. 16, Fig. 2G). This undermines the claim of performing dynamic inference, since the primary goal of dynamic inference is typically to *infer* the latent dynamics from noisy observations. If the velocity is provided a priori, the circuit is merely tracking a stimulus with known dynamics rather than performing inference over hidden states. Similarly, the transition precision $\\Lambda_z$​ is hardcoded into the recurrent weights $w_{EE}$​ (Eq. 14b). The authors argue this is necessary to implement adaptive step sizes, but it means the circuit cannot adapt to varying environmental statistics without synaptic plasticity. This is at odds with the authors’ argument that gain modulation is preferred over synaptic weight changes because it operates at faster timescales. This rigidity limits the generality of the approach compared to more flexible inference algorithms.\n2. Moreover, restrictive assumptions limit the generality of this approach. The framework relies heavily on three assumptions. First, the model assumes that feedforward inputs have Gaussian tuning curves (Eq. 1e) leading to Gaussian likelihoods (Eq. 6). However, real sensory likelihoods are often non-Gaussian and multimodal, and one of the purported strengths of sampling-based approaches is that they can represent arbitrary distributions. The authors do not address how the circuit would handle non-Gaussian inference problems. Second, the model assumes a 1D ring attractor, and its specific eigenmode structure is essential for the perturbation analysis (Eqs. D15-D16). It's unclear how the approach would scale to higher-dimensional feature spaces without this structure. While the authors show a 2D extension (Fig. A4) that couples two ring attractors, this is still a rather restrictive latent structure which presumably not all canonical circuits possess. Finally, the analysis uses uniform priors throughout, which means the posterior is essentially a scaled likelihood. This eliminates one of the key computational challenges of Bayesian inference, which is to show that the prior can reflect the statistics of its inputs. The authors do not demonstrate that the circuit can implement informative non-uniform priors or that it can flexibly switch between different prior distributions.\n3. I am not sure about the necessity of sampling in your circuit model if not to encode posterior uncertainty. The noise in the circuit dynamics is framed as enabling exploration, but the posterior uncertainty is entirely determined by the feedforward input rate $R_F$​ (which controls likelihood precision $\\Lambda_F$​). The neural variability does not represent posterior uncertainty in the way that sampling-based models typically propose. The authors state that \"a single snapshot of $\\mathbf{r}_F(t)$ ​parametrically conveys the whole stimulus likelihood\" (Section 3.1), meaning a population vector readout is sufficient. In this case, it is unclear what computational advantage sampling provides over simpler population coding schemes like probabilistic population codes (PPC), which can also perform Bayesian inference with linear readouts but without the complexity of maintaining sampling dynamics. \n4. I believe there is insufficient comparisons with alternative approaches. The authors briefly mention that deterministic inference circuits require \"complicated nonlinear functions to implement marginalization,\" which their sampling approach avoids. However, this advantage is specific to the problem structure (Gaussian distributions, linear-Gaussian dynamics). Moreover, the authors do not compare computational costs, convergence speed, or accuracy with deterministic approaches like Kalman filters or variational inference. Overall, the claim that their sampling framework provides advantages is not substantiated with quantitative comparisons.\n5. There is limited biological justification for key mechanisms in the model. While the authors mention that VIP neurons might modulate SOM gain to convey self-motion signals, this remains speculative. The assumption that the motor system provides precise, instantaneous velocity information to sensory cortex requires substantial justification. Moreover, the paper assumes that SOM neurons do not receive feedforward input (necessary for Hamiltonian sampling), but this constraint may not hold across all cortical areas and contradicts some anatomical evidence."}, "questions": {"value": "- How would the circuit handle time-varying or unknown velocities? How does the circuit adapt when transition statistics change?\n- How would the circuit handle non-Gaussian likelihoods, which are common in real sensory processing? Can you provide numerical experiments or extensions of your framework to cases where the Gaussian assumption breaks down?\n- Given that the likelihood can be read out with a population vector (linear decoder), what specific computational advantages does sampling provide over probabilistic population codes (PPC) in your framework? Can you provide quantitative comparisons (e.g. in terms of inference accuracy or speed)?\n- If the transition probabilities are Gaussian and the likelihood is a Gaussian function, as in your setup, then the recursive Bayesian filtering algorithm reduces to the Kalman filter, where the instantaneous posterior is also Gaussian. In this case, what is the point in approximating this quantity with sampling when an exact solution is attainable? \n- How sensitive is the circuit to mismatches between the assumed velocity $v$ (encoded in SOM gain) and the true stimulus velocity? What happens when there are errors/noise in gain modulation?\n- How would the circuit acquire the precise weight configurations required for sampling (Eqs. 14, 19)? Do you think these synaptic weights are learned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7kMSMrk0tC", "forum": "BpBW4gJofo", "replyto": "BpBW4gJofo", "signatures": ["ICLR.cc/2026/Conference/Submission12616/Reviewer_mCaV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12616/Reviewer_mCaV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762374490750, "cdate": 1762374490750, "tmdate": 1762923461708, "mdate": 1762923461708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends the work of Sale & Zhang (2024) on Bayesian sampling in canonical cortical circuits to the problem of dynamic inference. The authors propose that the same circuit, consisting of excitatory neurons, and parvalbumin/somatostatin interneurons, can implement both Langevin and Hamiltonian sampling to infer either static or dynamic latents. The key mechanism enabling this flexibility is the modulation of SOM neuron gain ($g_{S}$​), which the authors decompose into two components: a speed-dependent gain that encodes the stimulus velocity, and a “switching” gain that controls the ratio between Langevin and Hamiltonian sampling. The authors demonstrate through theoretical analysis of the nonlinear circuit dynamics that the circuit can track moving stimuli without modifying synaptic weights, and show that Hamiltonian sampling trajectories produce oscillations resembling hippocampal theta sequences.\n\nAs a side note, I wish to disclose that I used a local LLM to help edit my initial draft of this review for clarity and to catch grammar mistakes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality: the paper makes a theoretical contribution from static to dynamic inference within the Sale & Zhang (2024) sampling framework. While the foundational circuit architecture is borrowed from Sale & Zhang (2024), the application of this model to dynamic inference represents conceptual progress. They provide a novel functional role for the gain of the SOM neurons in the context of dynamic stimuli, and offers an original perspective on the functional role of oscillations in hippocampal theta sequences.\n\nQuality: the mathematical analysis is rigorous and follows established methods for analyzing continuous attractor networks. The authors provide detailed perturbative analysis, eigenmode decomposition, and explicit mappings between circuit dynamics and Langevin and Hamiltonian sampling algorithms. The derivations connect circuit parameters to posterior distributions in a principled way.\n\nClarity: the paper is generally well-structured. The progression from static to dynamic inference is well explained, and the figures are effective in illustrating the main concepts and results. The supplementary materials provide thorough derivations.\n\nSignificance: the potential unification of different sampling algorithms within a single circuit architecture is conceptually appealing. The model makes concrete predictions about the relationship between SOM gain, stimulus speed, and neural oscillations, and if these are validated experimentally, this work could offer insights into how cortical circuits implement probabilistic inference in dynamic environments."}, "weaknesses": {"value": "1. The most fundamental weakness in the paper is in what constitutes “dynamic inference”. The circuit already receives the velocity $v$ as input via SOM gain modulation (Eq. 16, Fig. 2G). This undermines the claim of performing dynamic inference, since the primary goal of dynamic inference is typically to *infer* the latent dynamics from noisy observations. If the velocity is provided a priori, the circuit is merely tracking a stimulus with known dynamics rather than performing inference over hidden states. Similarly, the transition precision $\\Lambda_z$​ is hardcoded into the recurrent weights $w_{EE}$​ (Eq. 14b). The authors argue this is necessary to implement adaptive step sizes, but it means the circuit cannot adapt to varying environmental statistics without synaptic plasticity. This is at odds with the authors’ argument that gain modulation is preferred over synaptic weight changes because it operates at faster timescales. This rigidity limits the generality of the approach compared to more flexible inference algorithms.\n2. Moreover, restrictive assumptions limit the generality of this approach. The framework relies heavily on three assumptions. First, the model assumes that feedforward inputs have Gaussian tuning curves (Eq. 1e) leading to Gaussian likelihoods (Eq. 6). However, real sensory likelihoods are often non-Gaussian and multimodal, and one of the purported strengths of sampling-based approaches is that they can represent arbitrary distributions. The authors do not address how the circuit would handle non-Gaussian inference problems. Second, the model assumes a 1D ring attractor, and its specific eigenmode structure is essential for the perturbation analysis (Eqs. D15-D16). It's unclear how the approach would scale to higher-dimensional feature spaces without this structure. While the authors show a 2D extension (Fig. A4) that couples two ring attractors, this is still a rather restrictive latent structure which presumably not all canonical circuits possess. Finally, the analysis uses uniform priors throughout, which means the posterior is essentially a scaled likelihood. This eliminates one of the key computational challenges of Bayesian inference, which is to show that the prior can reflect the statistics of its inputs. The authors do not demonstrate that the circuit can implement informative non-uniform priors or that it can flexibly switch between different prior distributions.\n3. I am not sure about the necessity of sampling in your circuit model if not to encode posterior uncertainty. The noise in the circuit dynamics is framed as enabling exploration, but the posterior uncertainty is entirely determined by the feedforward input rate $R_F$​ (which controls likelihood precision $\\Lambda_F$​). The neural variability does not represent posterior uncertainty in the way that sampling-based models typically propose. The authors state that \"a single snapshot of $\\mathbf{r}_F(t)$ ​parametrically conveys the whole stimulus likelihood\" (Section 3.1), meaning a population vector readout is sufficient. In this case, it is unclear what computational advantage sampling provides over simpler population coding schemes like probabilistic population codes (PPC), which can also perform Bayesian inference with linear readouts but without the complexity of maintaining sampling dynamics. \n4. I believe there is insufficient comparisons with alternative approaches. The authors briefly mention that deterministic inference circuits require \"complicated nonlinear functions to implement marginalization,\" which their sampling approach avoids. However, this advantage is specific to the problem structure (Gaussian distributions, linear-Gaussian dynamics). Moreover, the authors do not compare computational costs, convergence speed, or accuracy with deterministic approaches like Kalman filters or variational inference. Overall, the claim that their sampling framework provides advantages is not substantiated with quantitative comparisons.\n5. There is limited biological justification for key mechanisms in the model. While the authors mention that VIP neurons might modulate SOM gain to convey self-motion signals, this remains speculative. The assumption that the motor system provides precise, instantaneous velocity information to sensory cortex requires substantial justification. Moreover, the paper assumes that SOM neurons do not receive feedforward input (necessary for Hamiltonian sampling), but this constraint may not hold across all cortical areas and contradicts some anatomical evidence."}, "questions": {"value": "- How would the circuit handle time-varying or unknown velocities? How does the circuit adapt when transition statistics change?\n- How would the circuit handle non-Gaussian likelihoods, which are common in real sensory processing? Can you provide numerical experiments or extensions of your framework to cases where the Gaussian assumption breaks down?\n- Given that the likelihood can be read out with a population vector (linear decoder), what specific computational advantages does sampling provide over probabilistic population codes (PPC) in your framework? Can you provide quantitative comparisons (e.g. in terms of inference accuracy or speed)?\n- If the transition probabilities are Gaussian and the likelihood is a Gaussian function, as in your setup, then the recursive Bayesian filtering algorithm reduces to the Kalman filter, where the instantaneous posterior is also Gaussian. In this case, what is the point in approximating this quantity with sampling when an exact solution is attainable? \n- How sensitive is the circuit to mismatches between the assumed velocity $v$ (encoded in SOM gain) and the true stimulus velocity? What happens when there are errors/noise in gain modulation?\n- How would the circuit acquire the precise weight configurations required for sampling (Eqs. 14, 19)? Do you think these synaptic weights are learned?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7kMSMrk0tC", "forum": "BpBW4gJofo", "replyto": "BpBW4gJofo", "signatures": ["ICLR.cc/2026/Conference/Submission12616/Reviewer_mCaV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12616/Reviewer_mCaV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762374490750, "cdate": 1762374490750, "tmdate": 1763399097407, "mdate": 1763399097407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}