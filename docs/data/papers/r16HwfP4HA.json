{"id": "r16HwfP4HA", "number": 24616, "cdate": 1758358639193, "mdate": 1759896758311, "content": {"title": "Kernel Binary Optimizer (KBOP): Latent-free Optimizer for Binary Neural Networks", "abstract": "Binary Neural Networks (BNNs) are receiving growing interest for enabling energy-intensive deep learning on resource-limited edge devices. Traditionally, the training methods of such models rely on minimizing the quantization error in forward propagation and approximating the sign function in full-precision models. However, such methods do not leverage the nature of training in the space of binary weights. To address this issue, we propose a latent-free method called Kernel Binary Optimizer (KBOP) to enable binary deep learning. The proposal is largely based on three major technical innovations: a sign-changing rule that reverses the sign of a binary weight according to the value of the gradient calculated at the binary point, where the rigidity of this rule is regulated by the learning rate (LR); a learnable scaling factor that partially integrates the full-precision nature of the input into the binary optimization space, using a different LR from the sign-changing rule; and a BNN Initialization (BNN Init) procedure that stabilizes the beginning of binary learning. The novelty of KBOP lies not only in its binary optimization approach but also in its seamless integration into existing architectures, such as convolutional or transformer networks. To demonstrate this, we compare our approach to state-of-the-art (SoTA) BNN training methods for the super-resolution task and large language models, achieving a significant inference performance increase. Experimental results indicate that our method closes, on average, 36.83% of the gap between existing SoTA BNN model results and full-precision model performance. We also provide a theoretical proof of the convergence of KBOP and a theoretical justification for BNN Init.", "tldr": "", "keywords": ["Binary neural network", "Binary-specific initialization", "Momentum-based weight update", "Learnable scaling factors"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee2b40d4517383b93a82892e73e98d68f4c6ac67.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel training method for Binary Neural Networks (BNNs), named Kernel Binary Optimizer (KBOP), aiming to enhance training stability and performance. The authors claim that KBOP achieves improvements in tasks such as image super-resolution, text classification, and text generation without architectural modifications. They also provide theoretical convergence analysis. While the experiments cover diverse architectures and datasets, demonstrating performance gains, the paper has notable issues in experimental design, theoretical rigor, and result interpretation that require further clarification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) Proposes KBOP, a new BNN training method that allegedly improves stability and quality without modifying the architecture.  \n(2) Designs a BNN-specific initialization method (BNN Init) combined with a tailored learning rate scheduling strategy.  \n(3) Provides theoretical convergence guarantees and validates KBOP through experiments on multiple tasks and datasets."}, "weaknesses": {"value": "(1) Insufficient experimental comparison: The claim that KBOP is \"36.83% closer to FP (full-precision) than SoTA BNN baselines\" lacks a clear definition of the calculation method. Full comparison results with all baseline methods are missing, weakening the credibility of this assertion.  \n(2) Overly simplified theoretical analysis: Although convergence theorems (Theorem 3 and 4) are presented, the paper does not explain why KBOP’s learning rate scheduling ensures convergence. The proofs lack critical technical details, making it hard to verify the theoretical validity.  \n(3) Questionable result comparability: Table 6 reports a 0.857 accuracy on CIFAR-10, but no comparison with state-of-the-art methods is provided. The statistical significance of the performance gap over baselines is also unaddressed.  \n(4) Missing implementation details: While \"KBOP with BNN Init\" outperforms variants, the paper fails to explain how BNN Init is implemented or why it was chosen, hindering reproducibility.  \n(5) Unexplained hyperparameter settings: Tables 10–14 list hyperparameter combinations but do not justify their selection or analyze their impact on performance, reducing the interpretability of results."}, "questions": {"value": "(1) How does KBOP specifically improve over existing BNN training methods (e.g., IR-Net, LAB, BBCU)? The paper claims \"KBOP demonstrates lower average quality difference across architectures, being 36.83% closer to FP than SoTA baselines,\" but no concrete comparison data or calculation methodology is provided.  \n(2) The paper states that \"KBOP (without BNN Init) achieves 0.838\" compared to the full KBOP’s 0.857. Why is BNN Init critical? What are the specific implementation details of BNN Init, and why does it significantly boost performance?  \n(3) The paper claims KBOP \"improves PSNR by an average of 2.157,\" but it does not specify which baseline methods this comparison refers to or provide per-dataset results. This undermines the credibility of the reported performance gain."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fG6VsNKwe1", "forum": "r16HwfP4HA", "replyto": "r16HwfP4HA", "signatures": ["ICLR.cc/2026/Conference/Submission24616/Reviewer_jS4k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24616/Reviewer_jS4k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744248926, "cdate": 1761744248926, "tmdate": 1762943137233, "mdate": 1762943137233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an optimizer for binary networks called KBOP which flips binary weights based on an adaptive and statistic based threshold derived from gradient distributions. KBOP also introduces a specialized initialization scheme for binary networks. KBOP is designed to enable stable and direct training of the value of binary networks without using latent real valued weights."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and problem statement\n2. Novel idea advanced from existing works:\n- Kernel rule: While binary optimizer uses a simple constant threshold, KBOP introduces the effect of dynamic and statistical threshold using the mean and standard deviation of the gradient magnitudes across a layer instead of a fixed number.\n- BNN initialization: Inspired by the fact that binary weights are fixed at +1 or -1 with real valued scaling factor for each layer, KBOP derives optimal initial value for this scaling factor using Kaiming analysis which used to derive Kaiming initialization.\n\nWhile the proposed methods are novel, I have a lot of concerns about the results."}, "weaknesses": {"value": "I integrated this section to Questions section."}, "questions": {"value": "1. The amount of quality improvement with KBOP is shown small. In some cases, it doesn’t show improvement.\n2. In the comparison with Reactnet ResNet-18 Imagenet case, why don’t you compare it on MobileNet? How did you construct the block structure in this case? As you may know the base structure of Reactnet is designed from MobileNet. I think this comparison is important since Reactnet is sota.\n2. Are you sure that the perplexity of wikitext on GPT-2 FP is 169.93?\n3. As in Table 6, when KBOP isn’t conducted with BNN init, the result is inferior than STE. Does it mean that the kernel rule and other proposed dynamic scalings have no improvement?\n4. In Table 6, what happens if BNN init is applied on STE?\n\nMinor\n- Typo of superscript ‘4’ in Table5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vGvIdR7vV4", "forum": "r16HwfP4HA", "replyto": "r16HwfP4HA", "signatures": ["ICLR.cc/2026/Conference/Submission24616/Reviewer_3F7Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24616/Reviewer_3F7Y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960254565, "cdate": 1761960254565, "tmdate": 1762943136861, "mdate": 1762943136861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a novel method to train Binary Neural Networks (BNN) with binar weights and activations. It is called kernel binary optimizer (KBOP) and uses a heuristic based on the first and second order moments of the exponential moving average computed over the gradients to control which weights should flip sign during training. The proposed selection rule has two parameters. $\\lambda$ can be interpreted as the learning rate and $\\beta$ is the parameter of the exponential moving average over the weight gradients."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper gives a decent overview of the performance of recent and old methods to train BNNs in section 2."}, "weaknesses": {"value": "Several aspects limit its clarity of the paper:\n\n1) Key aspects of the algorithm that are required to understand and reproduce it are missing or scattered in the paper. For example, the criterion proposed to select which parameters flip sign is based on statistics of $v$, the exponential moving average of the gradient. However, this one is only defined in the pseudo code. It is not clear how the moments are calculated.\n\n2) Important information about the practical behavior of the algorithm are missing. First, there is little information about how to chose the parameters $\\lambda$ and $\\beta$. Choices for the experiments are not documented or at least hidden to me. Second, the complexity of the algorithm is not discussed?\n\n3) Inconsistent performance results are given in tables what diminishes my trust in the results (see questions)"}, "questions": {"value": "1) How is $l=E[|v_i|]$ estimated? Is it $l=\\frac{1}{d} 1^Tv$? If yes, the followup question would be if it is computed across all parameters or for each layer separately, because I am a bit concerned about selection bias across layers.\n\n2) What is the rationale behind the proposed selection rule, because the choice feels a bit arbitrary. I can think of many other selection methods, e.g. we could just flip the top-k weights receiving largest gradients and schedule k to decrease. Because the selection method is the core proposal, I miss some theory or experiments why it should be optimal.\n\n3) In Table 1, the best SOTA method is ReActNet wit $69.4%$ top-1 accuracy. However, in Table 3, it only achieves $65.5%$. I guess this is a typo.\n\n4) I looked into the ReActNet paper and actually they even report $71.4%$ with their ReActNet-C, which is much higher then your reported $65.8%$ with KBOP. I guess there is a reason you did not compare to ReActNet-C, but I would like to know to trust the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "20ici3kjgB", "forum": "r16HwfP4HA", "replyto": "r16HwfP4HA", "signatures": ["ICLR.cc/2026/Conference/Submission24616/Reviewer_xoDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24616/Reviewer_xoDg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762128108968, "cdate": 1762128108968, "tmdate": 1762943136657, "mdate": 1762943136657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a latent-free optimization (called Kernel Binary Optimizer) for training binary neural networks. To address the mismatch of the non-contiguity nature of BNNs and the contiguous latency weights used by gradient based optimizers, three major changes are made to the weight updates rule\nRather than frequently flipping the weight signs, they proposed a sign flipping rule based on the accumulated binary gradients. For the binary weight updates, it uses a separate learning rate\nThe learnable layer-wise scaling factor is using a separate LR to reduce quantization error.\nThey proposed a theoretically derived initialization (BNN Init) ensuring variance stability for binary weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Good demonstration of effectiveness on multiple applications.\n2. Simple kernel rule is for weight update"}, "weaknesses": {"value": "The accuracy drop for imagenet (65.8% vs. 69.6%) seems large."}, "questions": {"value": "How are different LRs (initial learning rate) tuned for different tasks? \nAccuracy comparison of SoTA BNN training methods for image classification task and super resolution tasks is shown. How about  language tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lAe33iTlmh", "forum": "r16HwfP4HA", "replyto": "r16HwfP4HA", "signatures": ["ICLR.cc/2026/Conference/Submission24616/Reviewer_xoRM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24616/Reviewer_xoRM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24616/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762285736883, "cdate": 1762285736883, "tmdate": 1762943136284, "mdate": 1762943136284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}