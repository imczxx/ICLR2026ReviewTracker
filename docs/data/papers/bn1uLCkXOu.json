{"id": "bn1uLCkXOu", "number": 11433, "cdate": 1758198856774, "mdate": 1759897575896, "content": {"title": "Don’t Forget the Context: A Multitask Transformer for Intracortical Speech Decoding", "abstract": "We present a transformer-based sequence-to-sequence model for human speech decoding from intracortical neural recordings. Unlike prior framewise recurrent approaches trained with connectionist temporal classification, our approach jointly models neural and linguistic dynamics and generates open-vocabulary word sequences directly from the neural signal. To address the limited-data regime of human brain–computer interface datasets, we adopt a multitask framework that combines phoneme and word decoding with auxiliary supervision from Mel-frequency cepstral coefficients, and we introduce Neural Hammer \\& Scalpel day-specific transformation to mitigate cross-day nonstationarity. The model establishes a new benchmark in phoneme decoding on the Willett et al. dataset and improves over previous end-to-end systems in word decoding. Attention visualizations reveal interpretable temporal chunking aligned with speech segments, shedding light on emergent neural dynamics. Finally, a scaling analysis shows favorable power-law trends, suggesting that continued data growth could yield substantial gains and positioning transformers as strong candidates for future brain-to-text", "tldr": "We present a multitask seq2seq Transformer with a day-adaptive Neural Hammer & Scalpel that decodes open-vocabulary text from intracortical signals, sets a new phoneme benchmark, and shows interpretable attention and favorable scaling", "keywords": ["neuroAI", "speech decoding", "neuroscience", "transformer", "seq2seq", "MAE", "scaling", "attention"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9de120d9c341ad896c70c1c5e66df55b9539604d.pdf", "supplementary_material": "/attachment/a60909a60ae44cb2993dec0474b4c950dcbabc55.zip"}, "replies": [{"content": {"summary": {"value": "The authors present a sequence-to-sequence (seq2seq) Transformer model for decoding open-vocabulary speech from intracortical neural signals. To address data scarcity, the model is trained in a multitask framework that includes phoneme decoding, word decoding (using a pretrained BART head), and an auxiliary regression task on MFCCs. The paper also introduces a novel day-specific transform, the \"Neural Hammer & Scalpel\" (NHS), to mitigate cross-session non-stationarity. The authors claim their model sets a new state-of-the-art in phoneme decoding and improves upon a previous end-to-end baseline for word decoding."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a strong rationale for moving beyond RNN+CTC models, clearly outlining their limitations, such as the conditional-independence assumption and the separation of the neural encoder from the language model.\n- The model claims to achieve a new state-of-the-art (SOTA) Phoneme Error Rate (PER) of 14.3%. This demonstrates that the seq2seq architecture and multitask setup are effective for learning robust phoneme-level representations.\n- The scaling analysis (Fig. 2) provides good evidence that the model architecture can improve with more data. The attention visualizations (Fig. 3) offer valuable qualitative insights into the model's learned alignments."}, "weaknesses": {"value": "- The primary weakness is the model's performance on the main task. The paper's best-reported WER is 25.6%. This is significantly worse than the established 17.8% WER from the hybrid RNN-CTC + LM baseline (Willett et al., 2023b) reported in the same table. An absolute performance gap of 7.8% on the primary metric is too large to overlook, especially for a clinically-motivated application.\n- The paper's methodological contributions are largely combinations of existing techniques.\n  - The use of seq2seq Transformers is a standard, established practice in automatic speech recognition (ASR). Applying it to neural signals is a logical, but incremental, step.\n  - The \"Neural Hammer & Scalpel\" (NHS) transform is a novel combination of a per-day affine transform (which the authors note is similar to a prior baseline) and a FiLM-style modulation. This is a good engineering contribution but not a fundamental new method for adaptation.\n  - Multitask learning is a common regularization technique.\n- The paper claims to improve \"over previous end-to-end systems in word decoding\" by comparing its 25.6% WER to the 26.3% WER of Feng et al. (2024). While true, this comparison obscures the fact that both end-to-end systems perform substantially worse than the existing, simpler hybrid-model baseline (17.8% WER).\n\nIn my opinion, the paper's strongest result is its SOTA phoneme decoding. The work might be better received if it were reframed to focus on this achievement. By framing the paper as a word decoder (as in the title and abstract), it invites a direct comparison to the SOTA word decoder, a comparison it does not win. Focusing on the value of seq2seq and multitask learning for phoneme-level representation learning would be a more defensible claim."}, "questions": {"value": "- The authors have shown the model is a superior phoneme decoder (14.3% PER vs. 17.4% for the RNN-CTC). What happens if the authors use their Transformer model as just a phoneme generator and feed its output into the same WFST + LM rescoring pipeline used by the Willett et al. (17.8% WER) baseline? This would provide a direct, apples-to-apples comparison of the neural encoder quality and test if the improved PER can actually lead to a better WER.\n- NHS transform introduces 24 separate sets of \"hammer\" and \"scalpel\" parameters for the 24 days. How does this model generalize to a hypothetical 25th day not seen in the training set? Does this per-day parameterization risk overfitting and limit the model's ability to generalize to new, unseen sessions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h3o0oSd0Uv", "forum": "bn1uLCkXOu", "replyto": "bn1uLCkXOu", "signatures": ["ICLR.cc/2026/Conference/Submission11433/Reviewer_1HWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11433/Reviewer_1HWu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761708620168, "cdate": 1761708620168, "tmdate": 1762922547616, "mdate": 1762922547616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multitask seq2seq Transformer for intracortical speech decoding, directly mapping neural activity to phoneme and word sequences. It introduces Neural Hammer & Scalpel (NHS), a day-specific calibration module that mitigates cross-day drift, and adopts multitask training with auxiliary MFCC prediction to improve data efficiency. Experiments on the Willett et al. dataset show state-of-the-art phoneme decoding and competitive word-level results. Analyses of attention patterns and scaling laws further reveal interpretable temporal structure and consistent data–performance trends. Overall, the paper demonstrates that Transformer-based decoding can effectively capture contextual and linguistic information from intracortical recordings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) Originality\n- The paper presents a meaningful step forward in intracortical speech decoding by introducing a unified seq2seq Transformer architecture that jointly models neural and linguistic dynamics. While Transformers themselves are not new, their application in this specific domain, particularly with multitask supervision and explicit day-wise calibration—is novel and well-motivated. The proposed Neural Hammer & Scalpel (NHS) module is an way to address day-to-day nonstationarity, a major challenge often glossed over in prior work.\n\n(2) Quality\n- The experiments are carefully executed, with fair baselines and ablations that isolate the impact of each design component (e.g., NHS, MFCC supervision, BART conditioning). The inclusion of scaling law analysis and attention visualization adds depth and scientific rigor beyond simple performance reporting. The model achieves state-of-the-art phoneme decoding accuracy and competitive word-level results despite limited data.\n\n(3) Clarity:\n- The paper is generally clear and well-structured. Each architectural choice is motivated by a concrete empirical or neuroscientific problem (e.g., temporal drift, weak audio supervision). Figures and tables effectively support the arguments, and the explanations of the day-adaptation mechanism are particularly easy to follow.\n\n(4) Significance:\n- This work bridges a methodological gap between modern sequence modeling and practical neural prosthetic applications. By showing that Transformer-based decoding can capture contextual and linguistic dependencies directly from intracortical activity, the paper paves the way for future foundation-style “brain-to-text” models. It provides both a technical contribution and a broader conceptual shift for the speech BCI field."}, "weaknesses": {"value": "(1) Limited novelty in model design\n- Most components like Transformer backbone, multitask setup, FiLM modulation are adaptations of existing techniques rather than fundamentally new inventions. The originality lies mainly in integration and application. The paper could be strengthened by articulating why this particular combination works better than other possible architectures (e.g., conformer-based, latent-alignment models).\n\n(2) Insufficient evaluation diversity\n- Experiments focus heavily on a single dataset (Willett et al.), with no cross-subject or cross-task validation. This limits claims of generalization and practical robustness. A small-scale transfer or held-out-day test would better demonstrate NHS’s effectiveness beyond memorizing per-day patterns.\n\n(3) Ablation depth and analysis scope\n- While ablations exist, some design choices (e.g., the gating function or MFCC weight) lack sensitivity analysis. It would help to quantify how each auxiliary loss contributes to performance and stability over time. Similarly, results are mostly quantitative; additional qualitative error analyses (e.g., semantic vs. phonetic errors) would provide richer insight."}, "questions": {"value": "(1) The proposed NHS module effectively addresses day-to-day nonstationarity, but it appears to rely on per-day embeddings learned jointly with the training set.\n- Have you tested the model’s ability to generalize to unseen days (e.g., a held-out-day split)?\n- If not, could you comment on whether NHS can handle new sessions without retraining, or how a continuous-time version might perform?\n  \n(2) The paper shows that adding MFCC and BART supervision improves performance, but the mechanism remains somewhat unclear.\n- Do you have any analysis (e.g., layer probing, representation similarity) indicating how MFCC or BART signals influence encoder representations?\n- Would the same benefit persist if MFCCs were randomly shuffled or misaligned?\n  \n(3) The scaling law results are interesting but extrapolated from relatively small data fractions.\n- How sensitive are these fits to the chosen data fractions (0.1–1.0)?\n- Have you validated that the power-law trend holds when adding or removing entire recording days rather than random subsets?\n  \n(4) The model uses a BART decoder with partially frozen layers.\n- Did you explore alternative strategies, such as training from scratch or using a smaller LM head?\n- How crucial is the BART initialization compared to a randomly initialized decoder for achieving good WER?\n  \nOverall, the experimental setup is comprehensive, with ablations and analyses. However, the work feels more like a domain-focused integration study rather than a conceptual or algorithmic innovation typical of ICLR. Its strength lies in methodological rigor and neuroscientific relevance, which might make it a better fit for a specialized journal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9XnduDWsEm", "forum": "bn1uLCkXOu", "replyto": "bn1uLCkXOu", "signatures": ["ICLR.cc/2026/Conference/Submission11433/Reviewer_Sx4q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11433/Reviewer_Sx4q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712913331, "cdate": 1761712913331, "tmdate": 1762922547115, "mdate": 1762922547115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a transformer-based sequence-to-sequence model for decoding speech from intracortical neural recordings in a patient with ALS and anarthria. The authors propose a multitask framework that jointly learns phoneme and word decoding with auxiliary MFCC supervision, and introduce a \"Neural Hammer & Scalpel\" (NHS) day-specific transformation to handle cross-day nonstationarity. the model achieves 14.3% phoneme error rate (PER) and 25.6% word error rate (WER), improving over previous end-to-end approaches. The authors demonstrate favorable power-law scaling trends and provide attention visualizations showing interpretable temporal chunking aligned with speech structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong ablation studies, Table 1 systematically evaluates architectural choices (Transformer vs. RNN), auxiliary tasks (MFCC, BART), and day transformations (NHS vs. Linear vs. None)\n2. Strong phoneme performance and clear improvements over framewise CTC approaches\n3. Excellent presentation and contextualization\n4. Good details for facilitating reproducibility"}, "weaknesses": {"value": "1. Scaling law extrapolations (Section 3.4) are based on single seeds; quantitative uncertainty is lacking.\n2. The paper’s power-law scaling analysis (Section 3.4; Appendix B; Figure 2) extrapolates phoneme and word error rates from ∼10 k to ∼100 k training trials under the assumption that the neural data distribution remains stationary. This assumption is unlikely to hold for intracortical recordings. Therefore, the statement of the projected “low single-digit PER and WER” estimates (page 6, line 319) may be overly optimistic.\n3. While Figure 3 presents intriguing qualitative patterns, the attention interpretation (Section 3.6) is based on single representative trial; needs statistical validation across dataset. The authors should quantify: (a) what fraction of trials exhibit clean \"box\" structure in Layer 6, (b) how cross-attention entropy varies across decoder types, (c) whether attention peaks align with envelope features statistically across the test set\n4. The authors state \"training stability and performance to be highly sensitive to hyperparameters\" but provide no ablation over critical choices like model dimension, number of layers, dropout rate, or warmup schedule."}, "questions": {"value": "1. Can you provide confidence intervals or multiple-seed averages for the power-law fits in Figure 2? How sensitive are the extrapolations to the chosen functional form?\n2. Why freeze the first 3 BART decoder layers? What is the impact of different freezing strategies or fine-tuning more/fewer layers?\n3. Your power-law fits project substantial gains at 100k trials. How do you reconcile this with known long-term nonstationarities (electrode drift, scarring, etc.) that would violate the stationary-distribution assumption?\n4. In the NHS module, how sensitive are results to the FiLM modulation strength?\n5. Could you quantify attention alignment, for example, by correlating peak cross-attention with speech envelope events?\n6. Given that inference is much faster than two-stage systems, have you considered beam search or other techniques to generate multiple hypotheses for rescoring? This might close the WER gap in Table1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kCb5kutUrc", "forum": "bn1uLCkXOu", "replyto": "bn1uLCkXOu", "signatures": ["ICLR.cc/2026/Conference/Submission11433/Reviewer_cjht"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11433/Reviewer_cjht"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070827294, "cdate": 1762070827294, "tmdate": 1762922546797, "mdate": 1762922546797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a seq2seq Transformer that decodes intracortical signals directly into phonemes and words. It uses (i) a multitask setup with an auxiliary MFCC head, (ii) a frozen-part BART word decoder for language priors, and (iii) a day-specific “Neural Hammer & Scalpel (NHS)” transform (global affine + FiLM) to address across-day drift."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1) The paper tackles a relevant and challenging problem: the limited-data issue of human brain–computer interface datasets.\n2) The paper includes clear, well-documented architecture and training details which helps reproducibility."}, "weaknesses": {"value": "1) The novelty of the work is limited. The core contribution is a conventional seq2seq Transformer with (a) an auxiliary MFCC prediction head, (b) a partially frozen BART decoder, and (c) a day-wise affine+FiLM calibration (NHS). None of these ingredients are algorithmically new in ML; NHS is essentially per-day affine re-mix + FiLM gating. The work reads as careful engineering, not a conceptual advance.\n\n2) All core results are reported on one intracortical participant from Willett et al. Moreover, the paper discards one of the implanted areas and uses only area 6v (128 channels), further narrowing scope. There is no cross-subject or cross-implant generalization. Therefore, conclusions about general utility are not supported.\n\n3) The evaluation scope of the paper is narrow. It primarily contrasts variants of the authors’ own model and omits comparison with other baselines for intracortical or speech decoding  in Table 1 (e.g., transducer-based, CTC-Transformer, or hybrid pipelines). As a result, the contribution cannot be properly contextualized within the existing literature on brain signal-based speech decoding."}, "questions": {"value": "Were the WFST + LM results recomputed under your preprocessing pipeline, or were they taken directly from Willett et al. (2023)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VpyZNG4v2o", "forum": "bn1uLCkXOu", "replyto": "bn1uLCkXOu", "signatures": ["ICLR.cc/2026/Conference/Submission11433/Reviewer_E9Q6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11433/Reviewer_E9Q6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11433/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762480017218, "cdate": 1762480017218, "tmdate": 1762922546424, "mdate": 1762922546424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}