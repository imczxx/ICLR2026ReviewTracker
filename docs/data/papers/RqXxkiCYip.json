{"id": "RqXxkiCYip", "number": 17620, "cdate": 1758278417881, "mdate": 1759897164339, "content": {"title": "Locally Subspace-Informed Neural Operators for Efficient Multiscale PDE Solving", "abstract": "We propose GMsFEM-NO, a novel hybrid framework that combines the robustness of the Generalized Multiscale Finite Element Method (GMsFEM) with the computational speed of neural operators (NOs) to  create an efficient method for solving heterogeneous partial differential equations (PDEs). GMsFEM builds localized spectral basis functions on coarse grids, allowing it to capture important multiscale features and solve PDEs accurately with less computational effort. However, computing these basis functions is costly. While NOs offer a fast alternative by learning the solution operator directly from data, they can lack robustness. Our approach trains a NO to instantly predict the GMsFEM basis by using a novel subspace-informed loss that learns the entire relevant subspace, not just individual functions. This strategy significantly accelerates the costly offline stage of GMsFEM while retaining its foundation in rigorous numerical analysis, resulting in a solution that is both fast and reliable. On standard multiscale benchmarks‚Äîincluding a linear elliptic diffusion problem and the nonlinear, steady-state Richards equation‚Äîour GMsFEM-NO method achieves a reduction in solution error compared to standalone NOs and other hybrid methods. The framework demonstrates effective performance for both 2D and 3D problems. A key advantage is its discretization flexibility: the NO can be trained on a small computational grid and evaluated on a larger one with minimal loss of accuracy, ensuring easy scalability. Furthermore, the resulting solver remains independent of forcing terms, preserving the generalization capabilities of the original GMsFEM approach. Our results prove that combining NO with  GMsFEM creates a powerful new type of solver that is both fast and accurate.", "tldr": "", "keywords": ["Neural Operators", "heterogeneous PDEs", "scientific machine learning", "Generalized Multiscale Finite Element Method", "localized spectral basis functions"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4d539840a3b41ed47d33432e23c497999503f893.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The manuscript introduces the GMsFEM-NO framework, which integrates the Generalized Multiscale Finite Element Method with NOs to improve the efficiency of existing NO frameworks when solving multiscale and high-contrast PDEs. Legacy GMsFEM requires eigen solvers, which are computationally expensive. GMsFEM-NO legacy eigen solvers with a NO trained to efficiently predict the local multiscale basis subspaces. A Subspace Alignment Loss, with a regularized variant, is also proposed to enhance global reception and ensure geometric and physical consistency. Experiments on 2D and 3D benchmarks show significant speedup over legacy GMsFEM with a comparable accuracy. Other technical merits include better generalization to OOD enforcing terms (v.s. NOs) and resolution invariance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Mathematical grounds**: Links subspace alignment to Grassmann manifold distance, enhancing theoretical credibility. The new SAL and SAL-PR losses are well-motivated and mathematically tied to Grassmannian geometry. \n- **Comprehensive experiments**: Includes linear/nonlinear PDEs, 2D/3D cases, various forcing terms, and extensive comparisons vs. multiple baselines.\n- **Performance boost**: 60x speedup v.s. legacy GMsFEM with comparable accuracy; better OOD generalization vs. standalone NOs; resolution invariance."}, "weaknesses": {"value": "- **Limited scope**: The experiments are limited to elliptic problems. The study does not include time-dependent PDEs, limiting the demonstrated generality. FEMs and NOs are known to be robust to hard problems, e.g., Navier-Stokes. This limits the scope of the proposed framework. \n- **Assumption of regular grids**: Current implementation works only for regular grids due to the use of F-FNO; performance on irregular geometries or adaptive meshes remains unexplored.\n- **Dependence on GMsFEM setup**: The framework still relies on a coarse grid and precomputed spectral problems for training data. It does not remove the expensive stage, but only accelerates it."}, "questions": {"value": "- Could the same SAL losses be extended to time-dependent GMsFEM formulations or to non-elliptic PDEs?\n- Can GMsFEM-NO handle non-rectangular or unstructured meshes if implemented with GNOs or other NO variants, e.g., GNOT, Transolver, etc.?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A7lhSbP3fa", "forum": "RqXxkiCYip", "replyto": "RqXxkiCYip", "signatures": ["ICLR.cc/2026/Conference/Submission17620/Reviewer_E9D5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17620/Reviewer_E9D5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760931119649, "cdate": 1760931119649, "tmdate": 1762927480967, "mdate": 1762927480967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper extends the GMSFEM by incorporating a neural operator to accelerate the process. The work proposes a subspace alignment loss function to learn coherent subspaces, and introduces projection regularization terms to enforce consistent projections. Experiments compare the proposed loss function with the conventional loss function, GMSFEM-NO with GMSFEM and F-FFO, for evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The combination of GMSFEM with NO is an interesting idea.\n\nThe proposed SAL and PR losses can train the NO effectively."}, "weaknesses": {"value": "The method is only tested with m moderate-scale data in rectangular domains. Is it able to solve larger-scale problems in non-rectangle domains?\n\nThe generalization ability of the learned NO is not evaluated sufficiently. The usability of the method is unclear.\n\nOnly zero Dirichlet boundary condition equations are tested? Can it solve Dirichlet boundary condition equations with other values?"}, "questions": {"value": "Are there failed cases that don't converge to the solution?\n\nIs it able to extend the method to time-dependent equations?\n\nIs it possible to further reduce the error if more basis functions are used? What's the limits of the error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tRgAi8ieBb", "forum": "RqXxkiCYip", "replyto": "RqXxkiCYip", "signatures": ["ICLR.cc/2026/Conference/Submission17620/Reviewer_EehQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17620/Reviewer_EehQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460569129, "cdate": 1761460569129, "tmdate": 1762927480477, "mdate": 1762927480477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes **GMsFEM-NO**, a hybrid framework that uses a neural operator to accelerate the **offline stage** of the Generalized Multiscale Finite Element Method (GMsFEM). Instead of solving many local eigenvalue problems to construct multiscale basis functions, the method trains an NO to map a heterogeneous coefficient field (\\kappa(x)) directly to the **subspace** spanned by the GMsFEM bases. The key idea is a **Subspace Alignment Loss** ((\\mathcal{L}_{\\text{SAL}})) that aligns predicted and true multiscale subspaces on a Grassmannian, avoiding the sign/permutation ambiguity of individual eigenvectors. The approach preserves two important GMsFEM properties: (i) independence from the forcing term (f(x)), so it generalizes to OOD right-hand sides, and (ii) **resolution invariance** (train on coarse, use on fine). Experiments on 2D/3D high-contrast diffusion and steady-state Richards equations show **>60√ó speedup** in basis generation while matching GMsFEM-level accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Targets the real bottleneck.** It identifies the most expensive part of GMsFEM (local spectral problems in the offline stage) and replaces it with an NO, which is a practical and impactful acceleration.\n2. **Subspace-level supervision.** The proposed $(\\mathcal{L}_{\\text{SAL}})$ is technically meaningful: learning the *space* of local bases is more robust than learning each basis function, and it naturally resolves sign ambiguity; this is the main methodological contribution.\n3. **Strong empirical claim: fast but still GMsFEM.** The method demonstrates >60√ó speedup in basis construction while retaining essentially the same $(L_2/H^1)$ errors as classical GMsFEM on multiscale benchmarks, which makes it attractive for large runs or parameter sweeps. \n4. **OOD and resolution generalization.** Because the solver structure of GMsFEM is kept, the approach remains independent of $(f(x))$ and supports train‚Äìcoarse / test‚Äìfine usage, which standard NOs typically fail to do."}, "weaknesses": {"value": "1. **Marginal gain from $(\\mathcal{L}_{\\text{SAL-PR}})$.** The paper introduces a more complex variant with projection regularization, but the improvement over plain $(\\mathcal{L}_{\\text{SAL}})$ is small (e.g. 1.82% ‚Üí 1.72% on 250√ó250), and only on a single setting; the extra term is not clearly justified. \n2. **Systematically ‚Äúbetter than‚Äù the target.** In several tables, GMsFEM-NO slightly outperforms the original GMsFEM it is approximating; calling this ‚Äústatistical variation‚Äù is not entirely convincing because it appears repeatedly, suggesting either measurement noise, data leakage, or a smoother inductive bias from the NO. This needs a clearer explanation. \n3. **Baselines not fully up to date.** Most comparisons are to F-FNO and classical reduced-order/ POD-style methods; stronger modern neural operators for multiscale/high-contrast settings are missing, so it‚Äôs hard to tell where GMsFEM-NO sits against the current SOTA."}, "questions": {"value": "1. Several tables show GMsFEM-NO outperforming the classical GMsFEM it is meant to approximate. Can you provide a more concrete explanation than ‚Äústatistical variation‚Äù (e.g. averaging over multiple local domains, extra smoothing from the NO, or differences in quadrature/projection)?\n\n2. For $(\\mathcal{L}_{\\text{SAL-PR}})$, how sensitive is the reported improvement to the choice and number of random test vectors \n$v^{i}$? \n\nWould the simpler $\\mathcal{L}_{\\text{SAL}}$ suffice in most practical cases?\n\n3. Can you add comparisons to more recent neural operators (beyond F-FNO / POD-style baselines) to better justify that GMsFEM-NO is competitive not only with classical GMsFEM but also with current NO-based multiscale solvers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concers."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tc5TDohzEn", "forum": "RqXxkiCYip", "replyto": "RqXxkiCYip", "signatures": ["ICLR.cc/2026/Conference/Submission17620/Reviewer_9cmw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17620/Reviewer_9cmw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550648261, "cdate": 1761550648261, "tmdate": 1762927479808, "mdate": 1762927479808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles multiscale, high-contrast PDEs by replacing the expensive local eigenvalue problems of GMsFEM with a neural operator that predicts the local coarse spaces directly. Instead of regressing individual basis functions, the method learns the subspace they span and trains with a Grassmann-geometry‚Äìaware subspace alignment loss (with an optional projection regularizer). The learned spaces assemble into a restriction operator that preserves the classical GMsFEM solve, yielding GMsFEM-level accuracy while reducing offline basis construction by more than an order of magnitude. Experiments on diffusion and steady Richards equations in 2D and 3D show strong accuracy, data efficiency, right-hand-side robustness, and resolution transfer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The conceptual shift from matching basis functions to matching their span is clear and well motivated, aligning the learning objective with what GMsFEM truly needs. The subspace alignment loss directly optimizes principal angles on the Grassmann manifold, removing sign and ordering ambiguities and leading to stable training. The framework preserves the classical coarse-solve pipeline, so accuracy remains comparable to GMsFEM while offline cost is drastically reduced; the empirical speedups are practically meaningful. Results demonstrate robustness to right-hand-side changes and cross-resolution transfer, where direct solution-predicting NNs typically struggle. The paper is clearly structured, with a clean train‚Üíassemble‚Üísolve story that is easy to reuse in other PDEs."}, "weaknesses": {"value": "Theoretical guarantees connect subspace error to final solution error only implicitly; a formal upper bound from principal angles to \n$ùêª^1$error would strengthen soundness. Evaluation focuses on structured grids with Dirichlet boundaries and steady problems; non-structured meshes, mixed or Robin boundaries, and time-dependent or strongly nonlinear systems remain open. Comparisons omit graph- or mesh-based neural operators and learning-augmented multigrid with learned prolongation/restriction, which are natural baselines here. The model zoo created by per-region-type networks increases operational burden at scale; an ablation on parameter sharing or conditional modulation would be helpful. Wall-clock reporting includes basis generation speedup but a comprehensive end-to-end budget (training time, energy, peak memory, coarse-solve cost) would improve practical transparency."}, "questions": {"value": "Could you provide a bound that maps principal angles between true and predicted coarse spaces to a bound on the $ùêª^1$ relative error of the final solution, at least under simplifying assumptions?\nFor 3D large-scale runs, what are the peak memory, wall-clock breakdown, and parallelization strategy during inference and coarse solves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Discrimination / bias / fairness concerns", "Yes, Privacy, security and safety", "Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)", "Yes, Potentially harmful insights, methodologies and applications", "Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)", "Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3SVBrL48Tk", "forum": "RqXxkiCYip", "replyto": "RqXxkiCYip", "signatures": ["ICLR.cc/2026/Conference/Submission17620/Reviewer_zYEo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17620/Reviewer_zYEo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17620/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981692388, "cdate": 1761981692388, "tmdate": 1762927479121, "mdate": 1762927479121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}