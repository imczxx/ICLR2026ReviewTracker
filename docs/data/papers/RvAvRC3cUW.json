{"id": "RvAvRC3cUW", "number": 16565, "cdate": 1758266176799, "mdate": 1759897232285, "content": {"title": "Federated Hierarchical Anti-Forgetting Framework for Class-Incremental Learning with Large Pre-Trained Models", "abstract": "Large pre-trained models, such as BERT, have demonstrated strong performance across various tasks. However, they are vulnerable to catastrophic forgetting in incremental learning, particularly in federated settings with non-IID data. Existing approaches, such as knowledge distillation and exemplar replay, partially address this issue but often incur high memory overhead, increase privacy risks, and introduce additional computational burden. To overcome these challenges, we propose FedHAF, a modular framework for federated class-incremental learning with large pre-trained models. FedHAF consists of three key components: a frozen feature extractor, a feature adjustment module, and a task-specific head. This structure enables efficient adaptation to new tasks while preserving knowledge from previous ones. We further introduce a two-stage training strategy that separates classifier learning from feature alignment. This strategy combines feature-level distillation with balance regularization, improving knowledge retention without requiring extensive parameter tuning or compromising privacy. Extensive experiments on benchmark datasets, including CIFAR-100, TinyImageNet, ImageNet, and Shakespeare, demonstrate that FedHAF consistently outperforms existing methods.", "tldr": "", "keywords": ["Pre-trained Model", "Forgetting", "Incremental Learning", "Federated Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9c2e790bb5c9eb482d4e8889279a0086f2c5d9b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed the novel methods for federated class incremental learning based on tuning large pretrained model to improve the performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The task is important but still presents many challenges to be addressed.\n3. The use of pretrained models for this task is practical in real-world applications and clearly explains why it can improve performance."}, "weaknesses": {"value": "1. Since the paper mentions the use of pretrained models, it is unclear whether the base models for CIFAR-100, Tiny-ImageNet, and ImageNet are pretrained, and if so, on which datasets they were pretrained.\n\n2. If pretrained models are used, the performance improvement is somewhat expected. It would be better if the authors could demonstrate that their method still outperforms others when all methods use the same pretrained model.\n\nMinor error:\n1. Reference for Tran et al, 2024b does not existed."}, "questions": {"value": "Please address Problems 1 and 2 in the weakness section first. I will provide further comments after reviewing your response and will consider raising my score if all of my concerns are properly addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yOgJ7zGSnP", "forum": "RvAvRC3cUW", "replyto": "RvAvRC3cUW", "signatures": ["ICLR.cc/2026/Conference/Submission16565/Reviewer_SrRk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16565/Reviewer_SrRk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761884599139, "cdate": 1761884599139, "tmdate": 1762926646288, "mdate": 1762926646288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers the problem of federated class-incremental learning. The proposed framework consists of two modules: the feature adjustment module refines the intermediate features before classification, and the task-specific head module accommodates new classes while retaining previous knowledge. Experimental results show the effectiveness of the proposed methods on several image and language datasets."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ Federated class-incremental learning is a timely topic.\n\n+ The proposed method demonstrates consistent improvements over baselines."}, "weaknesses": {"value": "- The proposed method is overall not tailored for \"federated\" class-incremental learning. The proposed four losses are all for general knowledge distillation and balanced learning. Hence, the proposed method appears to be a direct adaptation of existing losses. To avoid this criticism, for example, as stated in L148, the authors could address the issue of federated learning by introducing some \"mechanisms to detect task shifts across clients.\"\n\n- The math is overall not understandable, as the definition of terms is unclear and they are often overriden. For example, it is unclear what F' or adjusted feature representations are. F_t in Eq. (2) is the current model, F_i in Eq. (3) is the i-th class feature vector, and F_t in Eq. (4--5) is an output probability vector.\n\n- Table 5 implies that the proposed method is highly sensitive to the choice of hyperparameters, while the validation process is never explained. A more thorough hyperparameter analysis is required.\n\n- Sever -> Server"}, "questions": {"value": "Please address concerns in Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8cRLOiEIEf", "forum": "RvAvRC3cUW", "replyto": "RvAvRC3cUW", "signatures": ["ICLR.cc/2026/Conference/Submission16565/Reviewer_HUK4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16565/Reviewer_HUK4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932703421, "cdate": 1761932703421, "tmdate": 1762926645953, "mdate": 1762926645953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of catastrophic forgetting in Federated Class-Incremental Learning settings, particularly when using large pre-trained models. The core idea is to freeze the large pre-trained backbone (feature extractor) and only train two lightweight components: a Feature Adjustment Module (FAM) and a Task-Specific Head (TSH). This modularity is combined with a two-stage training strategy:\n\n- Stage 1 (TSH Training): The TSH is trained (while FAM is frozen) using a standard CE and KD loss on a local buffer of \"herding exemplars\" from old tasks.\n- Stage 2 (FAM Training): The FAM is fine-tuned (while TSH is frozen) using two losses: a Feature Distillation Loss ($L_{FD}$) to maintain feature consistency with the previous model and a Feature Balance Loss ($L_{FB}$) to equalize feature norms between old and new classes.\n\nThe authors conduct experiments on CIFAR-100, TinyImageNet, ImageNet, and Shakespeare to show the efficacy of their method compared to different baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1- The authors did a good job in motivating and explaining the problem. \n\n2- The final method is considerably more efficient than consistently communicating the whole model. \n\n3- The experimental evaluation shows the applicability of the method on multiple vision benchmarks and an NLP task (Shakespeare). \n\n4- The paper has a thorough Ablation Study section. Table 2 provides a detailed ablation study that methodically validates the contribution of each component of the proposed framework (the four loss terms, the FAM module, and the two-stage training). This strengthens the paper's claims about its design."}, "weaknesses": {"value": "1- The paper positions itself as overcoming the \"high memory overhead\" and \"privacy risks\" of exemplar replay (Abstract, Introduction). However, the proposed method relies on exemplar replay, which it terms \"local herding sample.\"\n\n* The memory costs of storing the herding examples, previous models and other components of the paper is not clear. \n\n2- Comparing against LANDER (Tran et al., 2024b), a data-free continual learning method, is not completely fair. FEDHAF is an exemplar-based method that uses a 20% data buffer. It is an established fact that exemplar-based methods almost always outperform data-free methods. To make a credible SOTA claim, the paper must compare against other state-of-the-art exemplar-based FCL methods.\n\n3- The theory section fails to provide a deeper understanding of why the FEDHAF architecture works.\n* The proof for Theorem 2 explicitly relies on \"Assumption 3\" which is never defined in the main paper.\n* The theorems themselves provide little new insight. Theorem 1 is a standard FedAvg convergence proof. Theorem 2 essentially states \"if feature drift is low, forgetting is low,\". It does not prove why or how the proposed $L_{FD}$ and $L_{FB}$ losses actually succeed in bounding this drift. \n\n4- Limited Novelty in Components: The paper's components are largely a novel combination of existing techniques rather than a fundamental innovation.\n* Freezing the backbone and training small adapters/heads is a standard practice in Parameter-Efficient Fine-Tuning (PEFT) and continual learning.\n* Using logit-level knowledge distillation ($L_{KD}$) is a cornerstone of CIL.\n* Using feature-level distillation ($L_{FD}$) is also a well-known technique.\n* Using exemplar replay (\"herding\") is arguably the most common CIL technique."}, "questions": {"value": "1- It is not clear why authors decided to use herding examples vs the exemplars (is there any difference between them?)\n\n2- What is the point of theoretical analysis?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "p3nqjANPEh", "forum": "RvAvRC3cUW", "replyto": "RvAvRC3cUW", "signatures": ["ICLR.cc/2026/Conference/Submission16565/Reviewer_mjCH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16565/Reviewer_mjCH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16565/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953005715, "cdate": 1761953005715, "tmdate": 1762926645494, "mdate": 1762926645494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}