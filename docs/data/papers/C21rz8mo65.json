{"id": "C21rz8mo65", "number": 23513, "cdate": 1758344813466, "mdate": 1763656745382, "content": {"title": "MergOPT: A Merge-Aware Optimizer for Robust Model Merging", "abstract": "Model merging aims to integrate multiple independently fine-tuned expert models into a single model while preserving the knowledge of all experts. However, existing approaches mainly address parameter conflicts at the merging stage and overlook the role of the fine-tuning process, which often leads to significant post-merge performance degradation. To address this limitation, we propose a novel merging-aware optimizer (abbreviated as MergOPT) that injects principled merge-induced parameter shifts into the weight update steps so that the fine-tuned model exhibits a more stable loss landscape under subsequent merging operations. Specifically, we first formulate model merging as a distributionally robust optimization problem in the weight space: the parameters of other experts to be merged are viewed as adversarial merge-offsets, and fine-tuning adapts to the worst-case merging scenario. Building on this formulation, we analyze the distribution of parameter updates and the effects of merging hyperparameters, from which we derive a merging-guided feasible region for weight shifts. Finally, extensive experiments across four large language models (LLMs) show that our approach consistently outperforms standard fine-tuning, yielding an average relative gain of 3.5\\% and a maximum gain of 9.5\\% across four merging strategies when merging seven experts.", "tldr": "This paper proposes a novel merging-aware optimizer (called MergOPT) that injects principled parameter perturbations into the weight update steps so that the fine-tuned model exhibits a more stable loss landscape under subsequent merging operations.", "keywords": ["Model Merging", "Fine-tuning", "Multi-task Learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/39d1b1dd0c5f6a397f853108701889f563743479.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MergOPT, a merge-aware optimization method that enhances the compatibility and robustness of fine-tuned models for subsequent model merging. The method reformulates fine-tuning as a distributionally robust optimization problem in the weight space, where parameters from other expert models are regarded as adversarial perturbations. By introducing controlled perturbations during the fine-tuning stage, MergOPT guides models toward flatter and more stable loss landscapes, thereby improving their adaptability under potential merging operations. In particular, the article designs a fine-tuning optimization method that enables the fine-tuned model to remain more compatible with potential future model-merging operations, thereby achieving better performance when merging occurs. Overall, this work aims to establish a more merge-friendly fine-tuning ecosystem for future model merging, providing a feasible approach to improve merge performance through enhanced finetuned model compatibility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a new perspective by shifting the focus of model merging from a post-merging adjustment problem to a fine-tuning optimization problem. This ''merge-aware fine-tunin`` view is conceptually novel and provides a promising new direction for improving model compatibility before merging.\n\n2. The article aims to build a more friendly model ecosystem for future parameter-level merging, allowing users to obtain more compatible expert models during the fine-tuning stage. It is essentially an infrastructure method for constructing a merge-friendly ecosystem, establishing the foundation for future collaborative fine-tuning and model merging workflows.\n\n3. The paper includes extensive analysis on perturbation parameters (e.g., Laplace scale, number of tasks, and merge coefficient) and provides insights into how these factors influence the merge robustness and final model quality.\n\n4. The study demonstrates strong generality, showing that merge-aware fine-tuning can benefit multiple architectures and merging algorithms.\""}, "weaknesses": {"value": "1. The method assumes that robustness to future, unseen tasks can be achieved by simulating “potential merging perturbations” using only data from single-task fine-tuning. This assumption is overly strong because it depends on the empirical task-vector distribution estimated from the current benchmark and may not generalize to unseen tasks. The diversity of real-world merging scenarios can far exceed what a single parameter distribution can approximate. Moreover, the Laplace assumption mainly captures sparsity or concentration in parameter differences but fails to represent semantic conflicts or gradient-direction contradictions between tasks. Consequently, the assumed perturbation distribution may deviate substantially in cases of stronger conflicting or different modal tasks.\n\n2. Although Eq. (6) defines a robustness objective against worst-case perturbations, the implementation (Eq. 10) adopts the single-step approximation. This procedure perturbs only within a local noise neighborhood instead of solving the true inner maximization. Hence, the method effectively behaves as a heuristic regularization rather than a strict distributionally robust optimization. No theoretical justification or upper/lower bounds are provided to show that the sampled perturbation covers the genuine worst-case scenario. The robustness may thus fail when the future merging tasks deviate from the training-time task-vector distribution.\n\n3. Since MergOPT requires access to fine-tuning data, its applicability in privacy-restricted or cross-organizational settings remains unclear. For example, if user A fine-tunes a model using MergOPT but user B fine-tunes without it, how would the merging behave? Does the method require all participants to adopt MergOPT to realize its benefits?\n\n4. The paper cites recent works such as AdaMerging but does not experimentally compare with them. Including comparisons with newer or optimization-driven merging strategies such as WUDI Merging or RegMean would make the evaluation more convincing.\n\n5. All experiments were conducted on text-based tasks. Extending the evaluation to other modalities, such as vision, would better demonstrate the generality and impact of the proposed method across different modalities.\n\n6. It is recommended that the “Related Work” and “Introduction” sections include more discussion on current fine-tuning methods. For example, an analysis of why previous fine-tuning methods result in poor compatibility during model merging would be valuable. Furthermore, it would be beneficial to review any existing work that is similar to the proposed method and to detail their approaches. This would better align with the positioning of this research.\n\n7. Although SAM results are reported, the discussion could be more prominently integrated into the main text rather than deferred to the appendix, as SAM represents the most closely related optimization paradigm to MergOPT.\""}, "questions": {"value": "Q1. The Laplace distribution used for perturbation sampling is fitted from a specific set of task vectors. How should this distribution be re-estimated or adapted for new domains, modalities, or highly conflicting task collections? Is there any online or adaptive estimation strategy?\nQ2. In real-world collaboration, contributors may fine-tune their models with different strategies. Must all participants adopt MergOPT to benefit from improved mergeability, or can MergOPT-trained and standard fine-tuned models be merged effectively?\nQ3. Beyond the parameters explicitly discussed in the paper, does MergOPT exhibit sensitivity to factors such as batch size, learning-rate schedule, or training steps, similar to the SAM?\nQ4. What specific experimental device or environment were the results reported in the paper based on? This is crucial for promoting reproducibility.\nQ5. What are the significant differences in the experimental settings between Tables 8-9 and Tables 10-11 in the appendix? Why is the latter considered a fairer comparison? Did Tables 10-11 adjust the hyperparameters of SAM to align with those of MergOPT in order to match the computational costs?\nQ6. As shown in Tables 8-11 in the appendix, MergOPT and the SAM method actually have their own advantages and disadvantages in terms of performance when combined with different merging methods and in terms of runtime under different configurations.What are the significant differences between these two optimization methods in terms of optimization objectives and perturbation modeling? What inspirations did MergOPT draw from SAM?\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical issues identified."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "r8r4ZwbpZY", "forum": "C21rz8mo65", "replyto": "C21rz8mo65", "signatures": ["ICLR.cc/2026/Conference/Submission23513/Reviewer_emci"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23513/Reviewer_emci"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761963503054, "cdate": 1761963503054, "tmdate": 1762942693285, "mdate": 1762942693285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles model merging, particularly during the fine-tuning stage, similar to few previous works. The paper takes a similar perspective with a previous work that views merging process as parameter perturbation process. From this perspective, the paper proposes a new optimizer, named MergOPT that employs a distributionally robust optimization (DRO). The experiments on three LLM demonstrate the effectiveness of the proposed method, in comparison to using standard fine-tuning in terms of the performance of merged model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is simple yet practical, avoiding the double-forward/backward passes required by SAM and thereby achieving the efficiency.\n- The proposed method demonstrates strong performance, validated on three LLM models.\n- The paper is well-organized and easy to follow.\n- The paper shows that a task vector can be well-approximated by a Laplacian distribution, which provides a new insight."}, "weaknesses": {"value": "- The paper claims that one of core ideas of the proposed method is viewing merging process as perturbations. However, to the best of the reviewer's knowledge, the view of seeing other experts as adversarial perturbation seems to be similar to the view introduced by SAFT-Merge, a work mentioned in the paper. The mathematical formulation in Eq. (5) seems to be similar to the one in SAFT-Merge as well. However, the paper does not provide credit to the previous work for the perspective and formulation.\n- The introduction section lacks discussions on previous works that also focus on fine-tuning stage for merging, including but not limited to SAFT-Merge and [A,B,C].\n- The related work section lacks discussions on similar previous works [A,B,C].\n- The experimental results lack comprehensive comparisons against previous works [A,B,C].\n- The experiments are mainly performed only on seven language tasks, in contrast to previous model merging works that also show experiments on vision tasks to demonstrate the applicability and generalizability .\n\n[A] Tang et al., Parameter Efficient Multi-task Model Fusion with Partial Linearization. ICLR 2024.  \n[B] Ortiz-Jimenez et al.,Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. NeurIPS 2023.  \n[C] Jin et al., Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic. ICLR 2025."}, "questions": {"value": "- The datasets used in the experiment seem to be different from standard datasets used in the field of model merging. What's the justification for such setting? How does the performance compare against previous works on fine-tuning methods on standard datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pci1sFhqgT", "forum": "C21rz8mo65", "replyto": "C21rz8mo65", "signatures": ["ICLR.cc/2026/Conference/Submission23513/Reviewer_VdQC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23513/Reviewer_VdQC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985851700, "cdate": 1761985851700, "tmdate": 1762942693059, "mdate": 1762942693059, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MergOPT, a fine-tuning-time optimizer that models downstream parameter-level merging as weight-space perturbations and trains each expert to be robust to those perturbations. Experiments on three LLM bases (Llama-3.2-1B/3B, Qwen-2.5-1.5B) and four merging schemes (WA, Task Arithmetic, TIES, DARE) show consistent average gains when merging seven experts, visualizations suggest flatter joint-loss landscapes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Shifts attention from *merging-time* heuristics to *fine-tuning-time* preparation, the weight-space DRO view is intuitive and connects to robustness.\n2. The single-step perturbation keeps cost close to standard fine-tuning and is optimizer-agnostic. \n3. Tables 1–3 show improvements for WA/TA/TIES/DARE; 3.5% average, up to 9.5% in a seven-expert setting. \n4. Heatmaps indicate flatter, wider low-loss regions after MergOPT, supporting the robustness claim."}, "weaknesses": {"value": "1. While Figure 1 and Figure 3 empirically show \"reasonable\",  Laplace fits for task vectors, the choice is ultimately justified mostly visually and via mean/scale parameter tuning. There is no mathematical analysis or statistical test quantifying goodness-of-fit (e.g., K–S distance, log-likelihood comparison to alternatives), nor discussion of the method’s sensitivity to non-Laplacian deviations, skew, or multi-modal effects. As this modeling choice affects the entire robust optimization, more rigor is needed.\n2. The practical DRO surrogate in Algorithm 1 replaces the inner maximization over merging perturbations with a simple Monte Carlo sampling from the approximated feasible region. There is little analysis of the gap between this heuristic and true minimax-optimality, particularly for non-Laplacian/real-world merging scenarios. How often does the sampled perturbation land in truly adversarial directions (i.e., the “tails”)? Is there a risk of systematically missing impactful merging failures?\n3. The choice of the Laplace distribution, the settings $\\mu=0$, $b$ values, and the discrete set $\\mathcal{A}$ for $\\alpha$ are justified by empirical distributions and prior work, but ablation results (see Table 6, Table 7) could be improved with more diagnostic measures: e.g., what happens if a different parametric family is used (Gaussian, mixture, etc.)? How robust are improvements if empirical task vector statistics deviate (e.g., outlier tasks, long tails, multi-modal structure)?"}, "questions": {"value": "Refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K7uFWz4QhL", "forum": "C21rz8mo65", "replyto": "C21rz8mo65", "signatures": ["ICLR.cc/2026/Conference/Submission23513/Reviewer_wjmi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23513/Reviewer_wjmi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987213642, "cdate": 1761987213642, "tmdate": 1762942692655, "mdate": 1762942692655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MergOPT, a merge-aware optimizer designed to improve model robustness to parameter perturbations during model merging. The key idea is to introduce merge-aware regularization during fine-tuning, formulated under a distributionally robust optimization (DRO) framework. By injecting Laplace-distributed weight perturbations during training, the model becomes more resilient to merging-induced parameter shifts. Experiments on multiple large language models (Llama3, Qwen2.5, etc.) and merging baselines (Weight Averaging, Task Arithmetic, TIES, DARE) show consistent performance gains — typically around +3.5% average and up to +9.5% on certain benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel training perspective: The work is among the first to explicitly incorporate merge robustness into the optimization objective, shifting the focus from “how to merge” to “how to train for merging.”\n2. Theoretical motivation: The DRO-based formulation provides a principled justification for the merge-aware objective.\n3. Empirical breadth: The experiments cover several LLM architectures and multiple merging algorithms, showing consistent performance improvements."}, "weaknesses": {"value": "1. Insufficient Baseline Comparisons: The paper does not compare against optimization-driven merging methods, such as [1] and later variants using evolutionary or gradient-based recipe search. This omission weakens the claim of superiority.\n2. Scalability and Efficiency Concerns: While MergOPT is shown to be more efficient than SAM, it still adds a non-trivial 17% computational overhead to the fine-tuning process (Table 9). This cost is incurred for every single expert model that is fine-tuned. The paper does not adequately discuss the scalability of this approach in a real-world scenario where dozens of expert models might need to be trained before merging.\n3. Scalability is not verified: Experiments focus on 1B–3B models; results for larger (≥7B) or multimodal models are missing, leaving questions about scalability.\n\n[1] Akiba T, Shing M, Tang Y, et al. Evolutionary optimization of model merging recipes[J]. Nature Machine Intelligence, 2025, 7(2): 195-204."}, "questions": {"value": "1. Have the authors tested combining MergOPT with evolutionary or gradient-based merging recipe optimizers?\n2. When merging more than 10 models, does the Laplace noise assumption still approximate the merge perturbation distribution well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4y8kQ21Bq6", "forum": "C21rz8mo65", "replyto": "C21rz8mo65", "signatures": ["ICLR.cc/2026/Conference/Submission23513/Reviewer_5xzh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23513/Reviewer_5xzh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993896422, "cdate": 1761993896422, "tmdate": 1762942692202, "mdate": 1762942692202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of model merging with LLMs. The key idea to propose a new optimization method is that, usually, despite a good merging procedure, fine-tuning processes don't address parameter conflicts between models in a robust way, which decreases performance severely. To solve this problem, the work introduces a merging-aware optimizer inspired by the recent *distributional robust optimization* (DRO) method (Lin et al. 2022), which sets an objective based on training data coming from a perturbed distribution around a divergence constraint (let's say these distributions are perturbed versions of the empirical data density). The authors adapt this idea for the weight-space, where perturbations are now caused by the addition of new models to the merging objective. Despite computational and privacy issues, the paper proposed a manageable objective and tested it on several LLM-oriented datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The manuscript is in good shape, and I can see the research quality of the work at first sight. The paper is excellently written, extremely clear, does a very good review of the needed background and SOTA methods, and derives the solution in a thorough way. One of the key points I enjoyed more while reading is the clarity and transparency on the challenges and how these are addressed (i.e. basically the approach and discussion introduced in section 3.2.3).\n- The DRO techniques are quite interesting, and it makes sense to me that authors develop their solution in this direction. Although I missed a bit of an extra teaser-figure or empirical evidence in pp.4 on how DRO works (for the sake of being a self-contained work or better comprehension), the way it works is well explained and connected to the solution.\n- The way the method is proposed, with the perturbation-based approach and later the Laplace-fit and sampling, is very proximal to some probabilistic merging approaches which have been mostly ignored due to they have not yet been applied to LLMs, etc. I don't think this is precisely a big weakness, but I just wanted to mention it. The way objectives are presented and discussed is insightful."}, "weaknesses": {"value": "To me, the paper doesn't have very big critical weaknesses that make it difficult to communicate a scientific contribution. However, what I detect are several big gaps of information, derivations, avoidance of certain clarity on tricky issues and some details that should be corrected:\n\n- I can guess how the last terms in Eq. 5 are obtained, but I would like to have/see the entire derivation of it. Where is the baseline/initial parameter set $\\theta_{0}$?\n- The paper abuses a bit of the word \"perturbation\". In some way it confuses the reader, as what we have is different arithmetic combinations of parameters. Someone this naming and recursive mentioning decouples the line-of-explanations from the usual model merging clarity and intuition.\n- Not a lot of references to the maximum number of K models to merge or the size. These two are key limitations, and I think they should be present in the discussion and development since the beginning of the manuscript.\n- The *alternating optimization strategy* mentioned in pp.5 looks a lot like something we have already considered in the optimization literature with focus on ML/AI (i.e. coordinate ascent/descent, min/max or expectation-maximization). I am sure the authors can find and add some extra details in that direction, so it provides more insights to the reader.\n- Already in section 3.2 the combinatorial problem of having a lot of different perturbations as K increases (without even considering the size of models), can be deduced easily. This is somehow mentioned lated in section 3.2.3 in a way that is not very precise for this reviewer. Having a combinatorial problem of this sort is a big problem, despite doing sampling later solves it a bit.\n- The comments on the innacessibility of models and tasks in pp. 5 should have been considered from the beginning as a big assumption. What about alignment of logits/labels? what are the main features of the models we are considering?\n- First paragraph in pp.6: the empirical distribution of weight perturbations doesn't seem like a good fit to a Laplace distribution, but more of an approximation which underestimates a lot the probability mass on the tails."}, "questions": {"value": "- Results are very confusing without explaining what the performance metrics are, not bold numbers and not having insights on its properties (what is better, being higher or lower). Leaving all this little effort to the reader is not the best idea for communicating precisely the empirical results.\n- How many times and how is the Laplace distribution fitted to the empirical changes? is the empirical distribution computed per-model or taking into account all K models together?\n- Could the authors add some info on the computational/combinatorial cost of the methodology?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jBd5Iv0k5J", "forum": "C21rz8mo65", "replyto": "C21rz8mo65", "signatures": ["ICLR.cc/2026/Conference/Submission23513/Reviewer_yTnV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23513/Reviewer_yTnV"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission23513/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762335048683, "cdate": 1762335048683, "tmdate": 1762942691757, "mdate": 1762942691757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}