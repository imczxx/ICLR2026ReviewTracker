{"id": "2g8vgmyXgQ", "number": 4501, "cdate": 1757690448178, "mdate": 1759898029518, "content": {"title": "Provable In-Context Learning of Nonlinear Regression with Transformers", "abstract": "The transformer architecture has revolutionized machine learning by processing input sequences into outputs. A defining feature is in-context learning (ICL)—the ability to perform unseen tasks from prompts without updating model parameters. Early theoretical work focused on linear tasks, and recent studies have begun exploring nonlinear functions. Yet a rigorous analysis of the training dynamics—how transformers learn such complex tasks—remains elusive. This paper presents the first formal analysis of ICL training dynamics for a broad class of nonlinear regression functions. We analyze the stage-wise dynamics of attention during training: attention scores between a query token and its target features rise rapidly at first, then gradually converge to one, while attention to irrelevant features decays more slowly and can oscillate. Our analysis explicitly characterizes how general non-degenerate $L$-Lipschitz task functions shape attention weights, identifying the Lipschitz constant $L$ as the key factor governing the convergence dynamics. Leveraging these insights, for two distinct regimes depending on whether $L$ is below or above a threshold, we derive different time bounds to guarantee near-zero prediction error.\nDespite convergence time depending on the task, we prove query tokens ultimately focus on highly relevant prompt tokens, demonstrating transformers’ robust ICL capability for unseen functions.", "tldr": "", "keywords": ["In-Context Learning", "Transformers", "Nonlinear Regression", "Learning Dynamics"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8040b68cf389f2f96992366f7159e43b1753471.pdf", "supplementary_material": "/attachment/ac5ad31fa313bfc1cf5adbf2254237a3a344f823.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the convergence of nonlinear Transformers on nonlinear regression tasks using ICL. The authors consider non-degenerate L-Lipschitz task functions and establish the analysis on flat and sharp curvature regimes, respectively. The results show that the learning phases and rates are different in the two cases based on the Lipschitz constant."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of nonlinear regression with nonlinear Transformers using ICL is important and interesting. \n\n2. The theoretical analysis is solid and impressive."}, "weaknesses": {"value": "1. The contribution is not significant enough. The key reason is that the studied feature embedding is too simple by considering well-separated data with tiny or even no noise. This makes the mechanism of self-attention be to find context examples that share the same feature as the query. Such a mechanism is already discovered in previous works. Extending this result to nonlinear regression is not impressive enough since I believe this can be proved given well-separated data. A more challenging and interesting problem for nonlinear regression is what if there is no context input that is very close to the query. \n\n2. The experiment results seem not aligned with the theory. The bound of $T\\_s^\\*$ in Theorem 2 shall be larger than $T_f^\\*$ in Theorem 1. The authors also claim that \"flat regime may enjoy faster convergence\" in lines 305-306. However, the convergence of sharp regime in Figure 1(b) is faster than that of flat regime in Figure 1(a). \n\n3. I am not sure how the results are related with the existing linear case analysis. Can the results be redueced to the linear case? Here is a key question, i.e., for the linear case, $L$ could be any arbitrary value $<\\infty$. However, it seems that previous works do not divide the discussion into two cases like yours. Is it because your analysis is more fine-grained than theirs or anything else? I believe there should be some discussions here."}, "questions": {"value": "1. In Eqn. 4, there is no $P$ in the right-hand side. Why not delete $P$?\n\n2. Can you theoretically study the case where (1) $L$ is much larger than the case studied in Theorem 2, and/or (2) the noise is not $o(1)$ in the feature, and/or (3) $p_k$ is not uniform? \n\n3. Are there any practical insights from the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sAetZW3EjD", "forum": "2g8vgmyXgQ", "replyto": "2g8vgmyXgQ", "signatures": ["ICLR.cc/2026/Conference/Submission4501/Reviewer_b5AQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4501/Reviewer_b5AQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761438647337, "cdate": 1761438647337, "tmdate": 1762917404117, "mdate": 1762917404117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the optimization dynamics of Transformers in the setting of in-context learning. Unlike previous work focusing on linear mappings, the authors consider tasks given by $L$-Lipschitz functions, where each input vector in the context is a non-degenerate feature vector perturbed by noise. They show that by optimizing a simplified Transformer’s population risk via gradient descent, the loss can be reduced close to zero. Furthermore, they prove that the convergence rate is governed by the Lipschitz constant $L$, and that two distinct convergence regimes arise depending on the magnitude of $L$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper removes strong assumptions made in prior work (such as linearity or orthogonal feature bases) and establishes theory for a broader class of problems.\n- The comparison with previous studies is clear. Existing results on optimization in in-context learning are well summarized, and the paper clearly explains how its contributions differ from them."}, "weaknesses": {"value": "1. The experiments in Section 6 are conducted on a simple case. It would be desirable to evaluate the theory in more practical settings to confirm whether the assumptions and results hold for real-world problems.\n2. In addition to data realism, empirical validation on more realistic architectures (e.g., deeper Transformers) would also strengthen the paper.\n3. The paper focuses on optimizing population risk, without discussing how the finite-sample training loss landscape might differ or how generalization behaves with respect to sample size."}, "questions": {"value": "1. How would the results change if we considered training the empirical (finite-sample) loss instead of the population risk?\n2. The paper’s analysis provides upper bounds parameterized by $L$. While it claims that differences in $L$ lead to distinct convergence behaviors, the tightness of these bounds remains unclear. Can the authors comment on how tight these bounds are?\n3. Do the results recover known behaviors in the linear case? Since a linear function $x \\mapsto w^\\top x$ is also $||w||_2$-Lipschitz continuous, does the Lipschitz constant $||w||_2$- similarly govern convergence?\n4. In Figures 1 and 4, the authors claim that different ranges of $L$ lead to differences in convergence speed. While it is indeed observable that larger $L$ tends to result in faster convergence, there does not appear to be a clear separation between the two regimes.\nCould the authors provide a more detailed explanation of this point?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ARxE3B6xTB", "forum": "2g8vgmyXgQ", "replyto": "2g8vgmyXgQ", "signatures": ["ICLR.cc/2026/Conference/Submission4501/Reviewer_DMZQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4501/Reviewer_DMZQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652549558, "cdate": 1761652549558, "tmdate": 1762917403764, "mdate": 1762917403764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the training dynamics of attention weights during in-context learning training. It is shown that depending on the Lipschitz constant of the target function class, convergence is either exponential or polynomial. This is confirmed with some empirical evidence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This is a training dynamics analysis, as opposed to characterization of global OPT. The paper is clear to read."}, "weaknesses": {"value": "Features come from a discrete set? The optimal attention pattern is to attend to only exactly identical vectors."}, "questions": {"value": "Take some u such that half of the v_k, those in a set S, satisfy v_k^\\top u < 0 (while the other half satisfy v_k^\\top u > 0). Consider only functions satisfying f(v_k) = -L for k\\in S and f(v_k) = L for k \\not\\in S. It seems like in this case, there is no reason to attend to only the same feature, and I think you can get this to satisfy Assumption (2). How is his consistent with Attn_k ~ 1?\n\nWhy is Attn_k not just |V_k|attn_k?\n\nShould most of these theorems have a “for all k” at the end?\n\nTheorem 1,2 shouldnt there be an upper bound on eta? Usually inverse with smoothness or something.\n\nIsnt Attn_k between 0, 1? So arent both (10) and (11) always positive?\n\nHow would this change if you looked at gradient descent on the actual WK, WQ matrices (not Q)?\n\nIn the experiments, I think it would be better to have log-scale for the y-axis. This might highlight difference in convergence better (since the difference is something like logarithmic vs polynomial in 1/eps)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Am9Sbx4ZOn", "forum": "2g8vgmyXgQ", "replyto": "2g8vgmyXgQ", "signatures": ["ICLR.cc/2026/Conference/Submission4501/Reviewer_EakK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4501/Reviewer_EakK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906158197, "cdate": 1761906158197, "tmdate": 1762917403518, "mdate": 1762917403518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the convergence dynamics of a single-layer Transformer trained under the in-context learning (ICL) framework. It establishes convergence guarantees for nonlinear functions in both flat and sharp regimes and reveals a two-phase transition in the evolution of both the loss and the attention scores throughout training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper studies the convergence of transformers during the pretraining stage, which is an interesting topic.\n\n2. The discovery of the sharp and flat regimes, along with the two-phase transition observed in both the training loss and the attention scores during training, is particularly intriguing.\n\n3. Experimental results show the two-phase transition of the pretraining stage, which coincides with the theory."}, "weaknesses": {"value": "1. The paper focuses only on a single-layer Transformer, and extending the analysis to a multi-layer setting would provide valuable insights.\n\n2. The technical approach appears to overlap with that of [1], which somewhat diminishes the paper’s original contribution.\n\n[1] Huang, Y., Cheng, Y., & Liang, Y. (2023). In-context convergence of transformers. arXiv preprint arXiv:2310.05249."}, "questions": {"value": "1. What are the main technical challenges involved in extending the current framework to a multi-head, multi-layer Transformer architecture?\n\n2. What is the major techinical novelty of extending the linear regression in [1] to the nonlinear regression problem?\n\n[1] Huang, Y., Cheng, Y., & Liang, Y. (2023). In-context convergence of transformers. arXiv preprint arXiv:2310.05249."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PpNCiqy7LF", "forum": "2g8vgmyXgQ", "replyto": "2g8vgmyXgQ", "signatures": ["ICLR.cc/2026/Conference/Submission4501/Reviewer_FbmK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4501/Reviewer_FbmK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4501/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983775597, "cdate": 1761983775597, "tmdate": 1762917403222, "mdate": 1762917403222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}