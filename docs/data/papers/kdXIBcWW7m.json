{"id": "kdXIBcWW7m", "number": 7204, "cdate": 1758011546196, "mdate": 1759897866705, "content": {"title": "The Ever-Evolving Science Exam", "abstract": "As foundation models grow rapidly in capability and deployment, evaluating their scientific understanding becomes increasingly critical. Existing science benchmarks have made progress towards broad **Range**, wide **Reach**, and high **Rigor**, yet they often face two major challenges: **data leakage risks** that compromise benchmarking validity, and **evaluation inefficiency** due to large-scale testing. To address these issues, we introduce the **Ever-Evolving Science Exam (EESE)**, a dynamic benchmark designed to reliably assess scientific capabilities in foundation models. Our approach consists of two components: 1) a non-public **EESE-Pool** with over 100K expertly constructed science instances (question-answer pairs) across 5 disciplines and 500+ subfields, built through a multi-stage pipeline ensuring Range, Reach, and Rigor, 2) a periodically updated 500-instance subset **EESE**, sampled and validated to enable leakage-resilient, low-overhead evaluations. Experiments on 32 open- and closed-source models demonstrate that EESE effectively differentiates the strengths and weaknesses of models in scientific fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and forward-compatible solution for science benchmark design, offering a realistic measure of how well foundation models handle science questions.", "tldr": "", "keywords": ["Science Benchmark", "LLM Evaluation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/53e2ca1fdb2212eeb92e88abeb0887eec7d97181.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "**Motivation**: The paper aims to provide a science benchmark that is robust to data leakage and cheaper to run than very large static suites. The authors argue that public benchmarks risk leakage and that exhaustive evaluation is costly. \n\n**Approach**: They create a large non-public pool of more than 100k curated question answer items across five disciplines and 500 plus subfields, then periodically resample a 500 item subset named EESE for evaluation. The pool is built with a three stage pipeline and a three branch refinement process to increase difficulty. The subset is refreshed over time to reduce leakage and compute cost.\n\n**Key Results**: They evaluate 32 models and report that thinking style models tend to outperform non thinking ones, that humans remain well ahead, that discipline specific gaps are large, and that the small resampled EESE tracks the larger pool by rank correlation while keeping cost lower. They also show that refinement lowers model accuracy, which they present as evidence that difficulty increased."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Usefulness**: A periodically refreshed 500 item suite can lower evaluation cost while still surfacing rank differences among models. The reported rank correlations between the 500 item subset and the full pool suggest the subset is a reasonable proxy for model ordering. This can help labs iterate faster on science capabilities while avoiding full scale runs. \n\n**Human curated difficulty**: The multi stage refinement, including expert driven edits and targeted distractor design, plausibly increases difficulty and reduces trivial items. The authors show consistent drops in accuracy after refinement, which supports the claim that questions became harder. Expert involvement likely improves clarity and reduces labeling errors, which is valuable for fair evaluation.\n\n**Coverage**: The pool spans many subfields and formats, which can capture different skills such as recall, selection with distractors, and limited open ended reasoning. This breadth improves the chance that weaknesses in particular scientific areas will be exposed."}, "weaknesses": {"value": "**Comparability over time**: Because each release is a fresh resample, it is hard to compare new model results to older baselines or to chart model progress across time stamps. Without anchors or versioned fixed sets, changes in scores may reflect sampling noise as much as model improvement. This weakens the utility for longitudinal tracking and for comparing to external baselines that were reported on a different draw.\n\n**Novelty**: There are many science benchmarks and pools that use transcription, expert curation, and difficulty control. The main novelty claimed here is the resampling based leakage mitigation. That choice also introduces cost and comparability challenges since many baselines must be re run whenever the subset changes. The net novelty feels modest relative to prior benchmark design patterns.\n\n**Reproducibility**: The paper calls the pool non public while the reproducibility section states that code and datasets will be made publicly available. This needs to be reconciled. If the 100k plus pool stays private, then full reproduction and community baseline reuse are limited. If it becomes public, the leakage claim becomes weaker. Clarifying what will be released and when is important.\n\n**Presentation quality**: Table text is small and hard to read in places such as Table 1. Figure 5 appears to lack some labeling details and the visual style of several figures feels closer to promotional animations than archival scientific figures. These choices hurt readability.\n\n**Terminology misuse**: Using the word “evolving” suggests regeneration or distributional shift over time. The method is resampling from a fixed non public pool, not generating new problems. “Resampled” or “rotating” would be more precise and would avoid confusion."}, "questions": {"value": "1. Do I have to rerun every baseline each time the benchmark is resampled. Is there any efficient workaround?\n2. Can the authors quantify the magnitude of sampling variance in EESE and set reporting rules?\n3. Can sampling 500 samples capture enough coverage of diverse scientific questions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TuZCYAYjSa", "forum": "kdXIBcWW7m", "replyto": "kdXIBcWW7m", "signatures": ["ICLR.cc/2026/Conference/Submission7204/Reviewer_oX8M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7204/Reviewer_oX8M"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761225633271, "cdate": 1761225633271, "tmdate": 1762919356000, "mdate": 1762919356000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents the Ever-Evolving Science Exam (EESE), a benchmark consisting of over 100 000 questions and answer pairs from five science disciplines. The benchmark is designed so that it will sample 500 questions (EESE) from the total dataset (EESE-Pool) and benchmark LLMs on this subset. The subset of 500 questions will be resampled from time to time to ensure that there is little data-leakage from the datasets to the LLMs to ensure that the benchmark stays relevant over time. \n\nThe dataset has a broad range (100k questions), wide reach (5 disciplines, 500 subfields, wide range of question formats) and high rigor (systematic and principled process to ensure quality).\n\nFinally, the dataset is used for a comprehensive evaluation of LLMS, benchmarking 32 different models, including thinking models one both EESE-Pool and EESE. The benchmark reveals that human experts outperform the best LLMs and shows that the benchmark is demanding for current LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "* The paper is easy to read and understand; it is well-written and structured well.\n* Comprehensive dataset of 100k question answer pairs with a nice evaluation set of 500 samples (stratified sampling across difficulty levels) that will be resampled from time to time. \n* Rigorous process for quality checking and human involvement. \n* Huge evaluating on 32 different LMMs models. \n* The results are clear and the discussion nice."}, "weaknesses": {"value": "* The paper leaves a lot of detail out, so my understanding of it is only partial. \n* The description of the dataset and what is measures is not clear from time to time. I disagree that the benchmark assesses scientific capabilities, as is stated in the abstract. My understanding is that this benchmark is a test of knowledge about science and not scientific capabilities. Similar examples of imprecise language about what the benchmark tests exist elsewhere in the text as well.\n* I am not really a fan of Figure 2. I find its “the artistic form” to reduce the clarity, and I would prefer that the process was illustrated with boxes and icons only. Figure 1 is better, as the artistic illustration does not hamper the readability. \n* Limitations are not discussed. Please do. \n\nOverall, this could be a very good addition to the literature. By clarifying the questions I have below, the presentation and soundness could improve, and my recommendation could improve with them."}, "questions": {"value": "* Is the exam ever evolving? Resampling 500 question answering pairs from a set of 100 000 is not ever evolving. After some time (depending on resampling rate) the whole dataset will have been exposed. This could of course take a long time if the evaluation set changes every month, but it is not ever evolving …\n* I wonder if data leakage has happened already. All 100k questions are asked to the best performing LLMs 1) during Categorization and 2) when the EESE-Pool performance was calculated. Given that the top-performing commercial models gather data on questions and answers, they have seen all questions already. This means that it is possible for them to improve their answers to these questions. Am I wrong?\n* I do not understand the sentence that starts on line 183: “EESE is then randomly …”. Is this an ongoing process or does it happen as part of something else?\n* Line 242: What is meant by course-grained control?\n* It is not clear how the predefined thresholds for the three difficulty levels are decided. Please specify. \n* Also, are outlier cases not used for setting the thresholds?\n* It is not clear how the stratified sampling of difficulty levels is done, nor how many questions of each level a 500 sample consists of. Could you please explain? \n* Is the Three-Branch Refinement Framework parallel? Do questions go through all three branches at the same time, or will they only go through one branch? The text indicates the latter, but then I would not state that it is a parallel framework. \n* Could you please be explicit about what “resilient against data leakage” means?\n* It is not clear to me how the evaluation set EESE is shared with the world. Is it through an API?\n* In the caption of Table 1, it is stated that third best is underlined as well as the second best. Is this correct?\n* How are human experts found? Who are they? An what characteristics make them experts in this regard?\n* Could you please explain SSH, AS, MS and so on. Please make them correspond to the terms in Figure 1.  \n* How is performance calculated? Number of correct answers divided by total of questions asked? This is not mentioned explicitly only implied. How is free text answers evaluated?\n* In table 2, is not Performance a better term than Overall? \n* On line 418: Is the word “cost” missing before $4.5\\times$?\n* Table 2 caption: Could you please state average of best models? Make it explicit. \n* Given the results in Table 2, is not DeepSeek R1 the best compromise between cost and performance? Should this be mentioned?\n* How are the results illustrated in Figure 6 b) made? Is it made from one sampling of the EESE-Pool or many?\n* How can all relevant data and code be made publicly available, as stated in the reproducibility statement, without enabling the LLMs to optimize on it? It is also explicitly stated that the datasets used in the paper are publicly available. How can this be?\n* Are model configurations and hardware details described in detail in the paper as stated in the reproducibility statement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2ZKEaBcqPy", "forum": "kdXIBcWW7m", "replyto": "kdXIBcWW7m", "signatures": ["ICLR.cc/2026/Conference/Submission7204/Reviewer_JFgV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7204/Reviewer_JFgV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910970588, "cdate": 1761910970588, "tmdate": 1762919355591, "mdate": 1762919355591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents The Ever-Evolving Science Exam (EESE), a dynamic benchmark designed to evaluate the scientific reasoning capabilities of large foundation models while addressing two key issues with existing science benchmarks: data leakage and evaluation inefficiency. EESE consists of a large, non-public EESE-Pool (100K+ science question-answer pairs across 500+ subfields and five major disciplines) and a smaller, dynamic subset of 500 questions that are periodically resampled for evaluation. The benchmark emphasizes three principles—Range, Reach, and Rigor—supported by a multi-stage data construction process (Transcription, Expansion, Categorization) and a three-branch refinement pipeline aimed at increasing question difficulty and diversity. The paper evaluates 32 large models, both open and proprietary, and shows that EESE can differentiate scientific strengths and weaknesses across domains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Ambitious and large-scale effort to build a benchmark addressing key issues of leakage and scalability.\n\n2. Strong organization with clear design principles and methodology.\n\n3. Inclusion of both natural and social science disciplines broadens scope beyond typical benchmarks.\n\n4. Demonstrates clear model differentiation and human-model performance gaps, suggesting benchmark difficulty is appropriate.\n\n5. Introduces a refinement process that systematically increases question complexity."}, "weaknesses": {"value": "1. The dynamic “ever-evolving” aspect is underspecified. It is unclear how sustainable or reproducible the periodic updates are in practice.\n\n2. Heavy reliance on proprietary models for evaluation and quality control (e.g., “thinking models”) makes reproducibility questionable. This also raises a question about how users can evaluate it without access to such models. \n\n3. The use of LLMs to label and refine data raises circularity concerns and potential bias—models are used to create and test themselves.\n\n4. The writing is verbose and could be more critical and analytical. The paper reads like an extensive benchmark manual rather than a scientific paper.\n\n5. The evaluation mainly reports aggregate performance; there’s little qualitative analysis of what kinds of reasoning or question types models fail at.\n\n6. No discussion of potential biases or limitations in expert selection, data sources, or field balance."}, "questions": {"value": "1. How frequently will EESE be updated in practice, and what infrastructure or governance ensures that these updates are sustainable?\n\n2. How do you ensure that using LLMs to annotate or refine instances does not bias the evaluation toward their own reasoning styles?\n\n3. Could you provide more transparency on the cost and human involvement behind EESE’s refinement stages?\n\n4. Is there a plan to make the dataset partially accessible (e.g., for academic verification) without compromising leakage resistance?\n\n5. How does EESE compare in calibration stability to recent leakage-aware benchmarks such as AntiLeakBench or LessLeakBench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gy23GB1cpz", "forum": "kdXIBcWW7m", "replyto": "kdXIBcWW7m", "signatures": ["ICLR.cc/2026/Conference/Submission7204/Reviewer_fMyi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7204/Reviewer_fMyi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919024705, "cdate": 1761919024705, "tmdate": 1762919355247, "mdate": 1762919355247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EESE, a benchmark for scientific reasoning designed to be \"leakage-resilient\" by keeping 99.5% of its 100K-item dataset private (the \"EESE-Pool\") and releasing small, 500-item \"evolving\" subsets.\n\nThis review recommends a clear rejection.\n\nThe paper's \"private benchmark\" model is not useful to the scientific principles of reproducibility, verification, and community progress. It is fatally undermined by a direct internal contradiction, promising in its reproducibility statement to make \"all relevant... datasets... publicly available\" while basing its entire premise on being \"non-public\". Finally, it suffers from critical methodological opacity, eg, failing to provide concrete details on its human-intensive and expert-driven data creation pipeline, which makes its claim of a sustainable, \"ever-evolving\" benchmark untrustworthy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper correctly identifies a critical and widely recognized challenge in LLM evaluation: data leakage and benchmark contamination, which can invalidate results.  \n\nThe stated goal of creating a large-scale, high-quality (\"Rigor\") benchmark covering a wide range of scientific disciplines (500+ subfields) is ambitious and, if executed transparently, would be valuable to the community."}, "weaknesses": {"value": "1. The paper is fatally flawed by an irreconcilable contradiction. It states its 100K-item EESE-Pool is \"non-public\" (Abstract, Sec 1) , yet its Reproducibility Statement (Sec 7) explicitly \"guarantee[s] that all relevant... datasets will be made publicly available\". This core incoherence makes the paper's premise impossible to evaluate.\n\n2. The paper's core proposal of a non-public dataset is useless to the purpose of a scientific benchmark, which is to provide a standardized, verifiable, and public artifact for reliable comparison. This model makes replication of the paper's own results (e.g., Table 1) impossible, prevents external bias audits, and blocks community-led error correction. It functions as a private evaluation service, not a scientific contribution.\n\n2. The paper substitutes buzzwords (\"Rigor,\" \"Data Engine\") for methodology. It provides zero actionable details on its \"Parallel Three-Branch Refinement Framework,\" (especially the later two refinement methods) failing to meet minimum reporting standards by omitting:  \n\n    - The technical or manual routing process for assigning/labeling instances for the Medium/High HI branches.\n    - The recruitment, qualifications, compensation, or instructions for the \"600+ experts\" involved in data creation.\n\n3. The claim of a sustainable, \"ever-evolving\" benchmark is unsubstantiated and appears false.  \n\n    - The process is not \"low-cost.\" The paper's \"refinement\" examples (Appendix B) reveal an extremely high-expertise, human-intensive process of the problem creation, not simple editing.   \n\n   - The 100K-item pool itself is static. The paper provides no mechanism for refreshing the pool, meaning \"evolving\" is just bootstrap from a fixed set. This does not solve long-term benchmark staleness."}, "questions": {"value": "- How in details are the experts instructed to label the required refinements in the middle/high HI;\n- How much cost in labor/time/money is this process;\n- How do you plan to make this sustainable so that you can catch up with your claim to \"regularly update\" the subset;\n- Is there any verification mechanisms on the problems within this variety of the subjects?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper's Ethics Statement (Section 6) contains a verifiably false claim: it states that \"no human subjects... was involved\". This is directly contradicted by the paper's own methodology, which relies on the intensive labor of \"600+ experts\" to transcribe, expand, and refine the dataset. These experts are, by definition, human participants in a large-scale data-creation experiment. The paper fails to provide any of the required ethical disclosures for human-subjects research, such as IRB approval, participant recruitment methods, or compensation details, which is a significant violation of standard reporting practices."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l2XdxWinvn", "forum": "kdXIBcWW7m", "replyto": "kdXIBcWW7m", "signatures": ["ICLR.cc/2026/Conference/Submission7204/Reviewer_2rAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7204/Reviewer_2rAW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7204/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985927778, "cdate": 1761985927778, "tmdate": 1762919354797, "mdate": 1762919354797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}