{"id": "QqvQ3iAdpC", "number": 18657, "cdate": 1758289796492, "mdate": 1759897089446, "content": {"title": "It's All Just Vectorization: einx, a Universal Notation for Tensor Operations", "abstract": "Tensor operations represent a cornerstone of modern scientific computing. However, the Numpy-like notation adopted by predominant tensor frameworks is often difficult to read and write and prone to so-called shape errors, i.a., due to following inconsistent rules across a large, complex collection of operations. Alternatives like einsum and einops have gained popularity, but are inherently restricted to few operations and lack the generality required for a universal model of tensor programming.\n\nTo derive a better paradigm, we revisit vectorization as a function for transforming tensor operations, and use it to both lift lower-order operations to higher-order operations, and conceptually decompose higher-order operations to lower-order operations and their vectorization.\n\nBuilding on the universal nature of vectorization, we introduce einx, a universal notation for tensor operations. It uses declarative, pointful expressions that are defined by analogy with loop notation and represent the vectorization of tensor operations. The notation reduces the large APIs of existing frameworks to a small set of elementary operations, applies consistent rules across all operations, and enables a clean, readable and writable representation in code. We provide an implementation of einx that is embedded in Python and integrates seamlessly with existing tensor frameworks: https://github.com/REMOVED_FOR_REVIEW", "tldr": "We introduce einx, a universal notation for tensor operations, and provide a Python implementation.", "keywords": ["Tensor notation", "tensor programming", "einx", "einsum", "einops"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/421034fa4f88e80fc870a13cda0117d39bab32d3.pdf", "supplementary_material": "/attachment/66e0e267d53121595a6c2bf663ed38f7aa97d406.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces einx, a notation and library for expressing tensor operations as vectorizations of elementary functions. The authors define a consistent syntax based on loop notation and implement it in Python for NumPy, PyTorch, and JAX backends. The goal is to replace the heterogeneous and sometimes confusing APIs of current tensor frameworks with a small, uniform set of composable operations. The paper is clearly written and technically sound."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well organized and easy to follow.\n- The notation is clearly defined and internally consistent.\n- The implementation is described in sufficient technical detail and appears complete.\n- The examples and tables illustrate coverage across many tensor operations.\n- The case study on multi-head attention demonstrates syntactic conciseness."}, "weaknesses": {"value": "- The contribution is about notation and implementation, not about new ML algorithms or empirical insights.\n- The main problem addressed (API inconsistency) is largely syntactic and not shown to have a measurable impact on ML research or practice.\n- No experiments, user studies, or adoption data support the claim that einx improves model development or reduces errors.\n- The approach generalizes existing ideas from einsum and einops rather than introducing a new paradigm.\n- The motivation section frames the issue as readability and conceptual elegance, but provides no evidence that current tools are a significant bottleneck.\n- The comparison to other libraries is descriptive only, without performance or usability analysis."}, "questions": {"value": "- Are there examples where einx allows expressing a model or computation that cannot be easily written with existing tools?\n- Can you provide any data, even informal, on user adoption or code simplification in real ML workflows?\n- How does einx handle contraction path optimization? Can it automatically determine and apply efficient evaluation orders for chained tensor products (e.g., einx.dot(\"m [a], [a b], [b] -> m\", M, A, v)), or does it always execute operations in the explicit order implied by the expression?\n- einx accepts sparse tensors for simple elementwise operations (e.g., scalar multiplication) but fails for contractions, additions, or reshapes because the PyTorch backend invokes dense operations (einsum, reshape, add(sparse, dense)). Could the authors clarify whether sparse interoperability is within the design scope of einx? Are there plans to support dispatching contractions to sparse-aware kernels (e.g., torch.sparse.mm), or is einx currently intended primarily for dense tensors?\n- How much runtime overhead does the einx compilation and caching mechanism add compared to equivalent direct calls in PyTorch or NumPy, and are there cases where it limits backend optimizations (e.g., for dynamic shapes or JIT execution)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "26SowPsori", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Reviewer_juN8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Reviewer_juN8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557834774, "cdate": 1761557834774, "tmdate": 1762928361261, "mdate": 1762928361261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new extended grammar for describing tensor operations that generalizes those common in `einsum` and `einops`. The authors cast vectorization as a general transformation that lifts lower-order operations to higher-order operations with a pointful (instead of pointless) style that is more declarative. It can naturally subsume all kinds of indexing (gather, scatter, index_select); tensor contraction (multiply, inner, outer, kronecker product, etc.); and all kinds of reshaping operations (broadcast, repeat, squeeze, etc.). Additionally, the authors also included a `(a+b)` notation that covers stack and concat. \n\nThe case study with multi-head attention is pretty illuminating. This library would be beneficial to all machine learning and scientific computation practitioners."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- A good generalization of `einsum` and `einops` that would be beneficial to all ML practitioners.\n- Paper is clearly written, with lots of examples to showcase the semantics the proposed grammar."}, "weaknesses": {"value": "- The semantics of `[x]` is not clear enough: at sometimes it is for axes to be contracted; sometimes it can be used in a `get_at` expression whose semantics is a bit vague."}, "questions": {"value": "- In the abstract, please cite `einsum` and `einops`.\n- L165 Named tensors: Missing citations to include:\n  - T Chen (2017): Typesafe abstractions for tensor operations. https://dl.acm.org/doi/10.1145/3136000.3136001\n  - D Chiang, A M Rush, B Borak (2021): Named Tensor Notation. https://arxiv.org/abs/2102.13196\n- L309: The semantics of `[ ]` is vague: it seems to indicate matched axes for tensor contraction, but sometimes it has other uses: please clarify"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FhQNFGjR0O", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Reviewer_bcTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Reviewer_bcTx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717387092, "cdate": 1761717387092, "tmdate": 1762928360768, "mdate": 1762928360768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes einx, a “universal” notation for tensor operations that extends einsum. It also claims other features, including declarative and Interpretable. The paper compares einx with einops using multi-head attention as a case study."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Brings many NumPy-like operations under a unified function signature.\n+ The declarative nature of the notation is advantageous; for instance, Line 374 demonstrates how the necessary permutation is inferred and performed implicitly."}, "weaknesses": {"value": "- Section 3 discusses tensor ops and vectorization, but lacks formal definitions of lower-order vs higher-order vectorization; this makes the argument hard to follow\n- The core conclusion in Section 3 —“It’s all just vectorization”—reflects a well-known aspect of tensor computation (same computation applied across slices). The paper does not clearly present how this finding leads to the specific design choices in the einx signature.\n- It’s unclear, in notation and capability, how the proposed {vectorization}/bracket syntax differs in practice from PyTorch einsum (or its common extensions).\n- Table 1 categorizes Numpy-like notations into 4 groups. It might be interesting to discuss the intuition of such a classification."}, "questions": {"value": "1. How does the finding about vectorization lead to the design of einx?\n2. What problems do brackets solve that einsum cannot, and how often do those cases arise in practice?\nFor example, this notation seems unnecessary in many cases in the paper.\nLine 358, einx.sum(\"a [b]\", x) seems equivalent to torch.einsum(“a b -> a”, x).\nLine 308, einx.dot(\"a [b], [b] c -> a c\", x, y)  seems equivalent to torch.einsum(\"a b, b c\", x, y)\nLine 472, einx.softmax(\"[k]\", A) seems equivalent to torch.softmax(a, axis=1). \n3. What are the key differences between einx and PyTorch einsum (or other einsum extensions) in terms of notation design? Which operations can einx express that others cannot, beyond the softmax example above?\n4. What is the scope? Is it really universal?\na. Convolution operators are important; why are they not covered or discussed in this paper? There are works extending einsum to convolution operations [1][2].\nb. How does einx represent pixel shuffle?\nc. How does einx represent jacobi-2D? \n5. Table 1 classifies Numpy-like operations into four categories. What is the intuition for such classification? \n6. How can the proposed classification (Table 1) be extended to cover custom operations? Can it serve as a framework or guideline for defining new operations within einx?\n7. Section 5.2 presents multi-head attention as an example; however, similar high-level fused attention layers already exist. Please include additional examples where einx clearly shortens code, improves readability, or demonstrates other unique advantages.\n\n[1] Dangel, Felix. \"Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods.\" Advances in Neural Information Processing Systems 37 (2024): 96671-96727.\n\n[2] Rabbani, Tahseen, et al. \"conv_einsum: A Framework for Representation and Fast Evaluation of Multilinear Operations in Convolutional Tensorial Neural Networks.\" arXiv preprint arXiv:2401.03384 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AtZg408C5F", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Reviewer_GyJP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Reviewer_GyJP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961649470, "cdate": 1761961649470, "tmdate": 1762928359484, "mdate": 1762928359484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new notation for tensor operations called einx. Previous attempts, such as einsum and its variants, have been proposed and widely used in the machine learning community. However, these notations have limitations in expressive power by definition, and their semantics are not always straightforward to understand. In this paper, the authors propose einx, a new notation focusing on vectorization. In the proposed notation, an einx expression can be derived by rewriting an implementation originally written using for-loops. The proposed notation enables concise descriptions of a wide range of tensor operations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper are as follows:\n\n## Convenience of a unified notation\nAs the authors claim, the proposed notation enables a unified expression of various tensor operations. In particular, the ability to represent concatenated axes, such as in `einx.id`, is interesting. Specifically, in the expression of attention in Sec 5.2. 5.2, the use of `einx` indeed allows the attention mechanism to be written in a small number of lines.\n\n## Beginner-friendly for elementary examples\nIn the elementary procedure of this method, one first considers a standard loop-based formulation and then converts it into `einx` notation by arranging the corresponding ids. This step-by-step approach seems intuitive and accessible even to beginners.\n\n## Introduction and related work\nI appreciate Sections 1 and 2 of this paper. The authors provide a summary of the history of past `ein*` notations, which should serve as valuable material for future researchers considering similar notational systems."}, "weaknesses": {"value": "On the other hand, the weaknesses of this paper are as follows:\n\n##  Mismatch with the venue due to a lack of quantitative evaluation\nWhile the proposed notation is interesting, there is no quantitative evaluation demonstrating its actual effectiveness. Therefore, it isn't easy to judge whether the proposed method is scientifically sound as a research contribution. Since quantitative evaluation is not necessarily required for notation proposals, this work would be more suitable for venues such as MLOSS or the ACMMM Open Source Competition rather than the ICLR main conference.\n\n## Still complex for complex examples\nAlthough the proposed notation is easy to understand in elementary examples, it remains complex for complicated cases. For instance, an expression like `z = einx.multiply(\"a b c, b -> c b a\", x, y)` is understandable by reasoning from the corresponding loops. Still, the attention example requires considerable familiarity to interpret. Therefore, the effectiveness of this notation (i.e., whether it truly makes complex operations easier to understand) depends heavily on the user's level of expertise with the notation."}, "questions": {"value": "In Sec. 3.22, the phrase \"The sum-reduction operation `np.sum(x, axis=1)` ... is decomposable\" appears, but this wording may be somewhat misleading and might benefit from rephrasing. When I first read it, I interpreted it to mean that the `sum` computation can be \"decomposed\" into two operations, namely `y[i] = sum(x[i, :])` and `y[i] += x[i, j]`. However, that is not the intended meaning here; instead, it means that the sum operation can be expressed in two distinct ways, correct?\n\nThis is more of a comment than a question, but for complex notations (such as `einx.dot(\"b q (h [c]), b k (h [c]) -> ...` in Sec. 5.2), I believe understanding would be deepened by a visualization that explains them visually. For example, a simple HTML page that can be opened in a browser, where entering an `einx` expression produces a real-time visualization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SjvUkbZeuh", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Reviewer_PoRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Reviewer_PoRD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106380005, "cdate": 1762106380005, "tmdate": 1762928358823, "mdate": 1762928358823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}