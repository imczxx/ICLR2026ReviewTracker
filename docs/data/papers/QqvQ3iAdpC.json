{"id": "QqvQ3iAdpC", "number": 18657, "cdate": 1758289796492, "mdate": 1759897089446, "content": {"title": "It's All Just Vectorization: einx, a Universal Notation for Tensor Operations", "abstract": "Tensor operations represent a cornerstone of modern scientific computing. However, the Numpy-like notation adopted by predominant tensor frameworks is often difficult to read and write and prone to so-called shape errors, i.a., due to following inconsistent rules across a large, complex collection of operations. Alternatives like einsum and einops have gained popularity, but are inherently restricted to few operations and lack the generality required for a universal model of tensor programming.\n\nTo derive a better paradigm, we revisit vectorization as a function for transforming tensor operations, and use it to both lift lower-order operations to higher-order operations, and conceptually decompose higher-order operations to lower-order operations and their vectorization.\n\nBuilding on the universal nature of vectorization, we introduce einx, a universal notation for tensor operations. It uses declarative, pointful expressions that are defined by analogy with loop notation and represent the vectorization of tensor operations. The notation reduces the large APIs of existing frameworks to a small set of elementary operations, applies consistent rules across all operations, and enables a clean, readable and writable representation in code. We provide an implementation of einx that is embedded in Python and integrates seamlessly with existing tensor frameworks: https://github.com/REMOVED_FOR_REVIEW", "tldr": "We introduce einx, a universal notation for tensor operations, and provide a Python implementation.", "keywords": ["Tensor notation", "tensor programming", "einx", "einsum", "einops"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/421034fa4f88e80fc870a13cda0117d39bab32d3.pdf", "supplementary_material": "/attachment/66e0e267d53121595a6c2bf663ed38f7aa97d406.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces einx, a notation and library for expressing tensor operations as vectorizations of elementary functions. The authors define a consistent syntax based on loop notation and implement it in Python for NumPy, PyTorch, and JAX backends. The goal is to replace the heterogeneous and sometimes confusing APIs of current tensor frameworks with a small, uniform set of composable operations. The paper is clearly written and technically sound."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well organized and easy to follow.\n- The notation is clearly defined and internally consistent.\n- The implementation is described in sufficient technical detail and appears complete.\n- The examples and tables illustrate coverage across many tensor operations.\n- The case study on multi-head attention demonstrates syntactic conciseness."}, "weaknesses": {"value": "- The contribution is about notation and implementation, not about new ML algorithms or empirical insights.\n- The main problem addressed (API inconsistency) is largely syntactic and not shown to have a measurable impact on ML research or practice.\n- No experiments, user studies, or adoption data support the claim that einx improves model development or reduces errors.\n- The approach generalizes existing ideas from einsum and einops rather than introducing a new paradigm.\n- The motivation section frames the issue as readability and conceptual elegance, but provides no evidence that current tools are a significant bottleneck.\n- The comparison to other libraries is descriptive only, without performance or usability analysis."}, "questions": {"value": "- Are there examples where einx allows expressing a model or computation that cannot be easily written with existing tools?\n- Can you provide any data, even informal, on user adoption or code simplification in real ML workflows?\n- How does einx handle contraction path optimization? Can it automatically determine and apply efficient evaluation orders for chained tensor products (e.g., einx.dot(\"m [a], [a b], [b] -> m\", M, A, v)), or does it always execute operations in the explicit order implied by the expression?\n- einx accepts sparse tensors for simple elementwise operations (e.g., scalar multiplication) but fails for contractions, additions, or reshapes because the PyTorch backend invokes dense operations (einsum, reshape, add(sparse, dense)). Could the authors clarify whether sparse interoperability is within the design scope of einx? Are there plans to support dispatching contractions to sparse-aware kernels (e.g., torch.sparse.mm), or is einx currently intended primarily for dense tensors?\n- How much runtime overhead does the einx compilation and caching mechanism add compared to equivalent direct calls in PyTorch or NumPy, and are there cases where it limits backend optimizations (e.g., for dynamic shapes or JIT execution)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "26SowPsori", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Reviewer_juN8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Reviewer_juN8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761557834774, "cdate": 1761557834774, "tmdate": 1762928361261, "mdate": 1762928361261, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new extended grammar for describing tensor operations that generalizes those common in `einsum` and `einops`. The authors cast vectorization as a general transformation that lifts lower-order operations to higher-order operations with a pointful (instead of pointless) style that is more declarative. It can naturally subsume all kinds of indexing (gather, scatter, index_select); tensor contraction (multiply, inner, outer, kronecker product, etc.); and all kinds of reshaping operations (broadcast, repeat, squeeze, etc.). Additionally, the authors also included a `(a+b)` notation that covers stack and concat. \n\nThe case study with multi-head attention is pretty illuminating. This library would be beneficial to all machine learning and scientific computation practitioners."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- A good generalization of `einsum` and `einops` that would be beneficial to all ML practitioners.\n- Paper is clearly written, with lots of examples to showcase the semantics the proposed grammar."}, "weaknesses": {"value": "- The semantics of `[x]` is not clear enough: at sometimes it is for axes to be contracted; sometimes it can be used in a `get_at` expression whose semantics is a bit vague."}, "questions": {"value": "- In the abstract, please cite `einsum` and `einops`.\n- L165 Named tensors: Missing citations to include:\n  - T Chen (2017): Typesafe abstractions for tensor operations. https://dl.acm.org/doi/10.1145/3136000.3136001\n  - D Chiang, A M Rush, B Borak (2021): Named Tensor Notation. https://arxiv.org/abs/2102.13196\n- L309: The semantics of `[ ]` is vague: it seems to indicate matched axes for tensor contraction, but sometimes it has other uses: please clarify"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FhQNFGjR0O", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Reviewer_bcTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Reviewer_bcTx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717387092, "cdate": 1761717387092, "tmdate": 1762928360768, "mdate": 1762928360768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Answer to all reviewers"}, "comment": {"value": "We thank all reviewers for their constructive feedback. We will address the four main points raised by the reviews in this comment and answer individual questions in the comments to each review.\n# Novelty\nSeveral reviews raised questions about the novelty of our proposed notation w.r.t. einsum/einops and our observations on the role of vectorization in tensor operations. We emphasize that einx is not an extension of einsum/einops, but follows a fundamentally different paradigm of tensor programming.\n\nOur observations on vectorization (which the design of einx is based on) are not recognized by or incorporated into existing Numpy-like or ein* notations:\n* Numpy (Harris et al.) refers only to some element-wise operations (e.g., np.add) as \"vectorized\", but does not recognize that most other operations (e.g., np.sum, np.tensordot, np.linalg.solve) also inherently perform vectorization. This is also evident by the fact that these operations follow different rules for how their inherent vectorization is expressed (see Appendix B). In contrast, we subsume the different interfaces under a single notation, demonstrating that \"it's all just vectorization\".\n* einsum/einops do not recognize the role of vectorization in tensor operations and contain design choices that distinctly contradict these observations (see Sec. 5.1):\n\t- Lack of distinction between vectorized axes and sub-tensor argument axes (i.e., no brackets).\n\t- No analogy of expressions with loop notation.\n\t- Functions `rearrange` and `repeat` compute the same elementary operation.\n\t- The naming of functions is not related to the underlying elementary operations: `einsum` is not called `dot`, `rearrange` and `repeat` are not called `identity`. It instead follows an unrelated principle: \"[W]e made an explicit choice to separate scenarios of “adding dimensions” (`repeat`), “removing dimensions” (`reduce`) and “keeping number of elements the same” (`rearrange`)\" (Rogozhnikov et al. 2022a)\n\t- `repeat` and `reduce` are framed as symmetrical, despite being not.\n\nWhile einsum/einops support few, particular tensor operations (see Tab. 2), einx is a universal notation and allows invoking *any* tensor operation.\n# einx notation is universal\nSeveral reviews raised questions about whether einx notation is truly universal. We want to emphasize that *any* operation can be invoked with einx syntax, even custom ones, and demonstrate this below. As an example, the einx call\n```\nz = einx.SOME_OPERATION(\"a [b], [b] c -> a c\", x, y)\n```\nwill yield the same output as\n```\nfor a in range(...): for c in range(...):\n    z[a, c] = SOME_OPERATION(x[a, :], y[:, c])\n```\nregardless of what is computed in `SOME_OPERATION`. The notation is universal because it applies to and supports *any* tensor operation (i.e., any `SOME_OPERATION`) and *any* vectorization by analogy with loop notation.\n\neinx contains functions for commonly used elementary operations (e.g., einx.sum) and adapters that allow invoking custom elementary operations. For instance, the code\n```\ndef myfunc(x, y):\n    return 2 * x + torch.sum(y)\neinmyfunc = einx.torch.adapt_with_vmap(myfunc)\n```\ndefines a new einx operation which can be invoked with einx notation\n```\nz = einmyfunc(\"a [c], b [c] -> a b [c]\", x, y)\n```\nyielding the same output as\n```\nfor a in range(...): for b in range(...):\n    z[a, b, :] = myfunc(x[a, :], y[b, :])\n```\n# Adoption of einx\nSeveral reviews raise questions about the usefulness, impact and soundness of einx. While it is difficult to quantify the impact directly, the existing adoption of einx in the community (it has been publicly available for ~2 years) does provide evidence of its usefulness and impact on machine learning research. Due to the double blind review process we cannot provide references, but only state the numbers here:\n- Over 900 repositories on Github list einx as a direct or indirect dependency.\n- einx has been downloaded from PyPI over 10,000 times per day over the last weeks (although this includes automatic downloads, e.g., from CI).\n# Venue placement\nWe believe that ICLR is the best fit for our paper:\n- einx fundamentally changes how machine learning models are mentalized and code is written, and has already found adoption across many machine learning projects. We believe most practitioners are familiar with the problems in tensor notation that einx addresses (see Appendix B). We aim specifically at machine learning use cases by supporting common deep learning frameworks (e.g., PyTorch, Jax) and operations (e.g., softmax).\n- The paper introducing einops was published at ICLR in 2022. It is similar to ours in that it provides (1) notation and (2) software. The final acceptance decision for its review (https://openreview.net/forum?id=oapKSVM2bcj) supports our view and states that the paper is \"about design, not about models or algorithms\" and that ICLR \"expose[s] researchers and practitioners in machine learning to ideas and techniques that may advance their research and practice\"."}}, "id": "brHSSrpTHY", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763682972521, "cdate": 1763682972521, "tmdate": 1763682972521, "mdate": 1763682972521, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes einx, a “universal” notation for tensor operations that extends einsum. It also claims other features, including declarative and Interpretable. The paper compares einx with einops using multi-head attention as a case study."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Brings many NumPy-like operations under a unified function signature.\n+ The declarative nature of the notation is advantageous; for instance, Line 374 demonstrates how the necessary permutation is inferred and performed implicitly."}, "weaknesses": {"value": "- Section 3 discusses tensor ops and vectorization, but lacks formal definitions of lower-order vs higher-order vectorization; this makes the argument hard to follow\n- The core conclusion in Section 3 —“It’s all just vectorization”—reflects a well-known aspect of tensor computation (same computation applied across slices). The paper does not clearly present how this finding leads to the specific design choices in the einx signature.\n- It’s unclear, in notation and capability, how the proposed {vectorization}/bracket syntax differs in practice from PyTorch einsum (or its common extensions).\n- Table 1 categorizes Numpy-like notations into 4 groups. It might be interesting to discuss the intuition of such a classification."}, "questions": {"value": "1. How does the finding about vectorization lead to the design of einx?\n2. What problems do brackets solve that einsum cannot, and how often do those cases arise in practice?\nFor example, this notation seems unnecessary in many cases in the paper.\nLine 358, einx.sum(\"a [b]\", x) seems equivalent to torch.einsum(“a b -> a”, x).\nLine 308, einx.dot(\"a [b], [b] c -> a c\", x, y)  seems equivalent to torch.einsum(\"a b, b c\", x, y)\nLine 472, einx.softmax(\"[k]\", A) seems equivalent to torch.softmax(a, axis=1). \n3. What are the key differences between einx and PyTorch einsum (or other einsum extensions) in terms of notation design? Which operations can einx express that others cannot, beyond the softmax example above?\n4. What is the scope? Is it really universal?\na. Convolution operators are important; why are they not covered or discussed in this paper? There are works extending einsum to convolution operations [1][2].\nb. How does einx represent pixel shuffle?\nc. How does einx represent jacobi-2D? \n5. Table 1 classifies Numpy-like operations into four categories. What is the intuition for such classification? \n6. How can the proposed classification (Table 1) be extended to cover custom operations? Can it serve as a framework or guideline for defining new operations within einx?\n7. Section 5.2 presents multi-head attention as an example; however, similar high-level fused attention layers already exist. Please include additional examples where einx clearly shortens code, improves readability, or demonstrates other unique advantages.\n\n[1] Dangel, Felix. \"Convolutions and More as Einsum: A Tensor Network Perspective with Advances for Second-Order Methods.\" Advances in Neural Information Processing Systems 37 (2024): 96671-96727.\n\n[2] Rabbani, Tahseen, et al. \"conv_einsum: A Framework for Representation and Fast Evaluation of Multilinear Operations in Convolutional Tensorial Neural Networks.\" arXiv preprint arXiv:2401.03384 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AtZg408C5F", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Reviewer_GyJP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Reviewer_GyJP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961649470, "cdate": 1761961649470, "tmdate": 1762928359484, "mdate": 1762928359484, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new notation for tensor operations called einx. Previous attempts, such as einsum and its variants, have been proposed and widely used in the machine learning community. However, these notations have limitations in expressive power by definition, and their semantics are not always straightforward to understand. In this paper, the authors propose einx, a new notation focusing on vectorization. In the proposed notation, an einx expression can be derived by rewriting an implementation originally written using for-loops. The proposed notation enables concise descriptions of a wide range of tensor operations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The strengths of this paper are as follows:\n\n## Convenience of a unified notation\nAs the authors claim, the proposed notation enables a unified expression of various tensor operations. In particular, the ability to represent concatenated axes, such as in `einx.id`, is interesting. Specifically, in the expression of attention in Sec 5.2. 5.2, the use of `einx` indeed allows the attention mechanism to be written in a small number of lines.\n\n## Beginner-friendly for elementary examples\nIn the elementary procedure of this method, one first considers a standard loop-based formulation and then converts it into `einx` notation by arranging the corresponding ids. This step-by-step approach seems intuitive and accessible even to beginners.\n\n## Introduction and related work\nI appreciate Sections 1 and 2 of this paper. The authors provide a summary of the history of past `ein*` notations, which should serve as valuable material for future researchers considering similar notational systems."}, "weaknesses": {"value": "On the other hand, the weaknesses of this paper are as follows:\n\n##  Mismatch with the venue due to a lack of quantitative evaluation\nWhile the proposed notation is interesting, there is no quantitative evaluation demonstrating its actual effectiveness. Therefore, it isn't easy to judge whether the proposed method is scientifically sound as a research contribution. Since quantitative evaluation is not necessarily required for notation proposals, this work would be more suitable for venues such as MLOSS or the ACMMM Open Source Competition rather than the ICLR main conference.\n\n## Still complex for complex examples\nAlthough the proposed notation is easy to understand in elementary examples, it remains complex for complicated cases. For instance, an expression like `z = einx.multiply(\"a b c, b -> c b a\", x, y)` is understandable by reasoning from the corresponding loops. Still, the attention example requires considerable familiarity to interpret. Therefore, the effectiveness of this notation (i.e., whether it truly makes complex operations easier to understand) depends heavily on the user's level of expertise with the notation."}, "questions": {"value": "In Sec. 3.22, the phrase \"The sum-reduction operation `np.sum(x, axis=1)` ... is decomposable\" appears, but this wording may be somewhat misleading and might benefit from rephrasing. When I first read it, I interpreted it to mean that the `sum` computation can be \"decomposed\" into two operations, namely `y[i] = sum(x[i, :])` and `y[i] += x[i, j]`. However, that is not the intended meaning here; instead, it means that the sum operation can be expressed in two distinct ways, correct?\n\nThis is more of a comment than a question, but for complex notations (such as `einx.dot(\"b q (h [c]), b k (h [c]) -> ...` in Sec. 5.2), I believe understanding would be deepened by a visualization that explains them visually. For example, a simple HTML page that can be opened in a browser, where entering an `einx` expression produces a real-time visualization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SjvUkbZeuh", "forum": "QqvQ3iAdpC", "replyto": "QqvQ3iAdpC", "signatures": ["ICLR.cc/2026/Conference/Submission18657/Reviewer_PoRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18657/Reviewer_PoRD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762106380005, "cdate": 1762106380005, "tmdate": 1762928358823, "mdate": 1762928358823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}