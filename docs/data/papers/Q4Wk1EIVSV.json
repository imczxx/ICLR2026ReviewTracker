{"id": "Q4Wk1EIVSV", "number": 24809, "cdate": 1758360576884, "mdate": 1763732608424, "content": {"title": "Addressing Exogenous Variability in Cooperative Multi-Agent Reinforcement Learning", "abstract": "Multi-agent reinforcement learning (MARL) has advanced control of many cooperative multi-agent systems. However, most approaches are trained against a single fixed adversarial strategy, leaving teams fragile to adversarial strategy shifts at test time. To handle such limitations, in this paper, we recast cooperative MARL from a new perspective into an Exogenous Dec-POMDP, separating agent-controllable endogenous and environment-driven exogenous dynamics in order to learn policies that adapt to exogenous shifts while preserving coordination. Our framework is composed of two main components: (i) learning exogenous dynamics and (ii) updating policy with two complementary goals - coordination to achieve high team return and causal influence on future exogenous evolution.\nWe implement the framework under centralized training with decentralized execution into a practical algorithm, named Learning Exogenous Influence for Coordination and Adaptation (LEICA), and evaluate it on SMAX with distinct train/test adversarial strategies. Experimental results show that our approach drastically improves performance in test time with unseen opponents' strategies while achieving high training-time performance, demonstrating its ability to handle exogenous shift and improve training stability.", "tldr": "LEICA: learn exogenous influence, weight counterfactual shaping by sensitivity, and get robust, well-coordinated MARL under exogenous shifts.", "keywords": ["Multi-Agent Reinforcement Learning", "Exogenous Dec-POMDP", "Influence-based Coordination"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/40e5c2d48455716c5a668470f947d7dda7921a8c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses generalization in multi-agent reinforcement learning (MARL) and proposes Exogenous Decentralized POMDPs (ED-POMDP) to model the separate endogenous influences of the agents' actions and the exogenous influences by the environment (transition). Based on a modeled causal chain, Learning Exogenous Influence for Coordination and Adaptation (LEICA) is proposed to shape rewards with respect to the separated influences. An endogenous and exogenous state predictor based on variational autoencoders are trained to estimate causal influences via a Jacobi matrix of the predictors. These influence estimates are transformed into counterfactual intrinsic rewards and added to the extrinsic reward for standard MARL algorithms like MAPPO to train on. LEICA is evaluated on a Jax-based variant of StartCraft Multi-Agent Challenge (SMAX) and shown to generalize to randomized variations of the original micromanagement tasks, where initial states are varied to different degrees."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper focuses on an important and interesting topic. Generalization in MARL, especially regarding distribution shifts, are open challenges that need to be addressed.\n\nThe paper is well-written and mostly easy to follow."}, "weaknesses": {"value": "**Novelty**\n\nWhile the addressed problem is well-motivated, the approach is merely a mixture of existing techniques that are already known/established:\n1. Multi-task reinforcement learning [1,2,3]\n2. Initial state variation [4,5,6]\n3. Reward shaping with counterfactuals/causal inference [7,8,9]\n\nWhile 3. seems to be a theoretically sound approach to solving the addressed problem, 1. and 2. are known to merely shift the actual problem without sufficiently solving it in a causal manner [10], as the learning algorithms depend on i.i.d. samples (either data point-wise or problem-wise).\n\nTo improve the paper, a discussion and experimental comparison to these works is required to justify the proposed approach.\n\nThe paper also ignores prior work on robust MARL, which addresses adversarial behavior of separate agents [11,12,13].\n\n**Soundness**\n\nThe paper focuses on a causal chain that determines the effect of the endogenous context on the next exogenous context. However, according to Fig. 1, both contexts form a colliding node in the causal graph, which could interfere with the general concept. Since the paper does not provide any theoretical analysis, e.g., regarding identifiability, I cannot confirm the validity of the proposed approach from a causal perspective.\n\nThe paper proposes to consider \"baseline actions\" for the counterfactual rewards, which resemble the \"default actions\" for the difference rewards or aristrocat utilities introduced in [7].\n\n**Quality**\n\nWhile the paper is generally well-written and presented, the proposed approach introduces a list of hyperparameters, such as $\\alpha$ (trade-off exogenous influence and value-sensitivity), $\\tau$ for sharpness of the weighting vector, and $\\beta$ for decaying the weighting vector. $\\lambda$ for the influence-based reward, indicating a tuning-intensive approach.\n\nOf these hyperparameters, only $\\alpha$ is ablated. For revision, I recommend:\n1. A more thorough ablation study on all of these hyperparameters, e.g., for the appendix.\n2. An intuition, how to set these parameters, e.g, regarding domains different from SMAX\n3. Ideally, finding an (adaptive) approach that can set these hyperparameters automatically.\n\n**Significance**\n\nThe experiments are conducted on some well-known SMAC maps and compared to standard MARL baselines, such as MAPPO and QMIX. The only variation tested in the paper is the initial state, which has been investigated for SMAC in [4,5,6], where MAPPO and QMIX have already been demonstrated to perform poorly.\n\nTo improve the significance of the work, especially regarding generalization, I suggest the following:\n- Vary the rewards and other functions, as defined in Section 3.1\n- Compare with the methods introduced in [4,6]\n- Compare with adversarial (test) methods introduced in [11,12,13]\n- Test other domains beyond StarCraft II, such as multi-agent MuJoCo and Google Research Football\n\n**Literature**\n\n[1] Omidshafiei et al., \"Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability\", ICML-17\n\n[2] Li et al., \"Multi-task Reinforcement Learning in Partially Observable Stochastic Environments\", JMLR-09\n\n[3] Hessel et al., \"Multi-Task Deep Reinforcement Learning with PopArt\", AAAI-19\n\n[4] Lyu et al., \"On Centralized Critics in Multi-Agent Reinforcement Learning\", JAIR-23\n\n[5] Ellis et al., \"SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning\", NeurIPS-23 Benchmarks\n\n[6] Phan et al., \"Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability\", ICML-23\n\n[7] Wolpert et al., \"Optimal Payoff Functions for Members of Collectives\", Advances in Complex Systems 2001\n\n[8] Li et al., \"Automatic Reward Shaping from Confounded Offline Data\", ICML-25\n\n[9] Jaques et al., \"Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning\", ICML-19\n\n[10] Schölkopf, \"Causality for Machine Learning\", 2019\n\n[11] Li et al., \"Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient\", AAAI-19\n\n[12] Phan et al., \"Resilient Multi-Agent Reinforcement Learning with Adversarial Value Decomposition\", AAAI-21\n\n[13] Li et al., \"Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game\", ICLR-24"}, "questions": {"value": "1. According to the problem statement in Section 3.1: What is the difference to multi-task learning, given that the reward function can vary as well?\n2. In Definition 1: What does the letter $I$ in the definition of $S^{e}_{t}$ stand for? Is it the same $I$ (mutual information) as in Equations 1 and 2?\n3. Why is the test considered to be adversarial? The experimental setting description implies that the initial states are merely randomized (without any value-minimizing intentions)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ND1vJ2zePb", "forum": "Q4Wk1EIVSV", "replyto": "Q4Wk1EIVSV", "signatures": ["ICLR.cc/2026/Conference/Submission24809/Reviewer_i8ij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24809/Reviewer_i8ij"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760782229178, "cdate": 1760782229178, "tmdate": 1762943203457, "mdate": 1762943203457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response to Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. Key concerns repeatedly raised across multiple reviews included, in particular:\n(1) the principled definition and generality of the endogenous-exogenous state partition, and \n(2) the relationship of our approach to robust MARL methods.\nWe have substantially revised and strengthened the paper with new explanations, additional experiments, and refined terminology. Below we summarize the key clarifications and improvements.\n\n## 1. Endogenous–Exogenous State Partition\n>Many reviewers questioned whether the endogenous/exogenous split is hand-crafted or environment-specific. We fully agree that the original explanation was too brief. We significantly expanded and clarified this part.\n>- ED-POMDP does not require raw-state partitioning. The decomposition can be defined or learned via latent-space masking. We now state explicitly that learning the partition is a major and intended future direction.\n>- SMAX provides a uniquely transparent ally/enemy separation, which we used solely to validate the ED-POMDP formulation. We now provide a clear, mechanistic justification in Appendix B.1 of the revised paper.\n>- We emphasize that our contribution lies in ED-POMDP formulation and LEICA, not in the particular SMAX split. The SMAX partition is simply a transparent validation case where controllable vs. uncontrollable transitions are objectively defined.\n>\n>We have added a detailed explanation of the above discussion to the paper, and we hope this will help resolve confusion regarding the manual structure and clarify the generality of the proposed formalization.\n\n## 2. Comparison to Robust MARL Baselines\n>We fully agree that comparison with robust MARL methods is crucial. Following the reviewers' suggestions, we implemented M3DDPG and a PPO-based minimax algorithm designed analogously to M3DDPG.\n> ### Why robust MARL is fundamentally insufficient in this setting\n>While these baselines are valuable, their failure in this experiment is unsurprising for structural reasons. Robust MARL methodologies assume adversarial action perturbations (adversarial co-learners, policy perturbations). In cooperative CTDE environments like SMAX, opponents' actions are neither controlled nor perturbed, and adversarial attacks are absent by definition. Our setting deals with exogenous MDP perturbations, not behavioral-level adversarial optimization. The environment (opponent scripts, map scaling, distance fluctuations) induces state transition-level volatility, which robust MARL algorithms are not designed to model.\n>\n>This structural mismatch prevents robust MARL methods from effectively handling exogenous variability, and our empirical results confirm this theoretical expectation:\nLEICA consistently outperforms robust MARL baselines for both observed and unobserved exogenous strategies.\n\n## 3. Additional Experiment: Influence-Weighted Intrinsic Reward Visualization\n>To improve interpretability and address reviewers’ requests for deeper intuition behind our intrinsic reward design, we added a new visualization to the revised paper (Section 6.3 and Figure 3). This experiment analyzes how the influence weights $w_j$ evolve throughout training in the 3s\\_vs\\_6z scenario.\n>\n>The result verifies that LEICA’s intrinsic reward operates exactly as intended. Early in training, influence concentrates on ally positional coordinates, when the value function is unstable and the agent must first learn how its controllable dynamics affect exogenous transitions. Later in training, influence shifts toward enemy-health dimensions, consistent with the exponentially decaying intrinsic coefficient and the growing reliability of the extrinsic value function.\n>\n>This demonstrates that LEICA first uses intrinsic shaping to learn the structure of exogenous variability, and then naturally transitions to value-driven exploitation.\n## 4. Additional Revisions of the Paper\n>We marked major revised part of the paper with red texts to easily find out. \n>- Figure 6, 7, 8, 9 and Table 1 are updated with M3DDPG/M3PPO results.\n>- The draft used the terms \"adversarial\" and \"adversary\" to refer to external opponent scenario variations, which differs from how these terms denote adversarial attacks in the robust RL literature. To avoid ambiguity, we replace these terms with \"exogenous\" or \"opponent variation\". This prevents confusion with “adversarial attacks” used in prior research.\n> - Line 115: $I$ is the identity matrix, not mutual information.\n> - Line 150: The explanation of blue arrow in Fig 1. \n> - Line 527: Limitation part.\n> - Appendix B.4: More detail information of computational cost.\n> - Appendix D.3: Guidelines to select hyperparameters of LEICA.\n\nWe thank the reviewers once again for their thoughtful comments. If any additional concerns arise, we would be more than happy to address them."}}, "id": "LhdhaXZrC8", "forum": "Q4Wk1EIVSV", "replyto": "Q4Wk1EIVSV", "signatures": ["ICLR.cc/2026/Conference/Submission24809/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24809/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24809/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728325815, "cdate": 1763728325815, "tmdate": 1763728325815, "mdate": 1763728325815, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response to Reviewers"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback. Key concerns repeatedly raised across multiple reviews included, in particular:\n(1) the principled definition and generality of the endogenous-exogenous state partition, and \n(2) the relationship of our approach to robust MARL methods.\nWe have substantially revised and strengthened the paper with new explanations, additional experiments, and refined terminology. The revised paper is now available at openreview as PDF with new or modified portion in red color.\n\nBelow we summarize the key clarifications and improvements.\n\n## 1. Endogenous–Exogenous State Partition\n>Many reviewers questioned whether the endogenous/exogenous split is hand-crafted or environment-specific. We fully agree that the original explanation was too brief. We significantly expanded and clarified this part.\n>- ED-POMDP does not require raw-state partitioning. The decomposition can be defined or learned via latent-space masking. We now state explicitly that learning the partition is a major and intended future direction.\n>- SMAX provides a uniquely transparent ally/enemy separation, which we used solely to validate the ED-POMDP formulation. We now provide a clear, mechanistic justification in Appendix B.1 of the revised paper.\n>- We emphasize that our contribution lies in ED-POMDP formulation and LEICA, not in the particular SMAX split. The SMAX partition is simply a transparent validation case where controllable vs. uncontrollable transitions are objectively defined.\n>\n>We have added a detailed explanation of the above discussion to the paper, and we hope this will help resolve confusion regarding the manual structure and clarify the generality of the proposed formalization.\n\n## 2. Comparison to Robust MARL Baselines\n>We fully agree that comparison with robust MARL methods is crucial. Following the reviewers' suggestions, we implemented M3DDPG and a PPO-based minimax algorithm designed analogously to M3DDPG.\n> ### Why robust MARL is fundamentally insufficient in this setting\n>While these baselines are valuable, their failure in this experiment is unsurprising for structural reasons. Robust MARL methodologies assume adversarial action perturbations (adversarial co-learners, policy perturbations). In cooperative CTDE environments like SMAX, opponents' actions are neither controlled nor perturbed, and adversarial attacks are absent by definition. Our setting deals with exogenous MDP perturbations, not behavioral-level adversarial optimization. The environment (opponent scripts, map scaling, distance fluctuations) induces state transition-level volatility, which robust MARL algorithms are not designed to model.\n>\n>This structural mismatch prevents robust MARL methods from effectively handling exogenous variability, and our empirical results confirm this theoretical expectation:\nLEICA consistently outperforms robust MARL baselines for both observed and unobserved exogenous strategies.\n\n## 3. Additional Experiment: Influence-Weighted Intrinsic Reward Visualization\n>To improve interpretability and address reviewers’ requests for deeper intuition behind our intrinsic reward design, we added a new visualization to the revised paper (Section 6.3 and Figure 3). This experiment analyzes how the influence weights $w_j$ evolve throughout training in the 3s\\_vs\\_6z scenario.\n>\n>The result verifies that LEICA’s intrinsic reward operates exactly as intended. Early in training, influence concentrates on ally positional coordinates, when the value function is unstable and the agent must first learn how its controllable dynamics affect exogenous transitions. Later in training, influence shifts toward enemy-health dimensions, consistent with the exponentially decaying intrinsic coefficient and the growing reliability of the extrinsic value function.\n>\n>This demonstrates that LEICA first uses intrinsic shaping to learn the structure of exogenous variability, and then naturally transitions to value-driven exploitation.\n## 4. Additional Revisions of the Paper\n>We marked major revised part of the paper with red texts to easily find out. \n>- Figure 6, 7, 8, 9 and Table 1 are updated with M3DDPG/M3PPO results.\n>- The draft used the terms \"adversarial\" and \"adversary\" to refer to external opponent scenario variations, which differs from how these terms denote adversarial attacks in the robust RL literature. To avoid ambiguity, we replace these terms with \"exogenous\" or \"opponent variation\". This prevents confusion with “adversarial attacks” used in prior research.\n> - Line 115: $I$ is the identity matrix, not mutual information.\n> - Line 150: The explanation of blue arrow in Fig 1. \n> - Line 527: Limitation part.\n> - Appendix B.4: More detail information of computational cost.\n> - Appendix D.3: Guidelines to select hyperparameters of LEICA.\n\nWe thank the reviewers once again for their thoughtful comments. If any additional concerns arise, we would be more than happy to address them."}}, "id": "LhdhaXZrC8", "forum": "Q4Wk1EIVSV", "replyto": "Q4Wk1EIVSV", "signatures": ["ICLR.cc/2026/Conference/Submission24809/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24809/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24809/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728325815, "cdate": 1763728325815, "tmdate": 1763766460652, "mdate": 1763766460652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Most MARL approaches are trained against a single fixed adversarial strategy, leaving teams vulnerable to adversarial strategy shifts at test time. In this work, the authors recast cooperative MARL from a new perspective into an Exogenous Dec-POMDP. It separates agent-controllable endogenous and environment-driven exogenous dynamics in order to learn policies. It consists of VAEs to learn endogenous and exogenous dynamics and influence-based reward design. Through experiments in a modified SMAX, the authors demonstrate the effectiveness of their proposed approach."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper views MARL cooperation from a somewhat new perspective.\n2. The experimental results demonstrate the effectiveness of the LEICA in a modified SMAX (more challenging with changing opponent strategies) \n3. The paper is well-written, especially the introduction."}, "weaknesses": {"value": "1. It seems that decoupling the controllable and non-controllable idea is similar to the idea of DRIMA, which considers environmental risk and cooperation risk. \n2. It seems that dividing the state/observation into endogenous and exogenous parts depends on the environment. In SMAX/SMAC, the state/observation can be decomposed in such a way due to the data structure design. However, it is unclear whether it is suitable for other environments.\n3. The reward design is similar to the design of COMA. Moreover, formulas (5)-(8) do not provide much insight regarding endogenous and exogenous parts.\n\nREFERENCE \n\n[1] Disentangling Sources of Risk for Distributional Multi-Agent Reinforcement Learning, ICML 22."}, "questions": {"value": "1. line 66, why \"a new scalable SMAX\"? Do you show that your new SMAX is more scalable through experiments?\n2. Figure 1, please describe the blue line and the black line in detail.\n3. line 165-166, is the training process divided into multiple stages? The first stage learns the VAEs?\n4. line 173, “decomposed as S_t = (...”. Does this rely on the data structure of environments? \n5. What are the MARL insights regarding (5)-(8)?\n6. line 278-280, There are many MARL value-based approaches published after 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GRJ3odT25j", "forum": "Q4Wk1EIVSV", "replyto": "Q4Wk1EIVSV", "signatures": ["ICLR.cc/2026/Conference/Submission24809/Reviewer_xiHW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24809/Reviewer_xiHW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724429739, "cdate": 1761724429739, "tmdate": 1762943203183, "mdate": 1762943203183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets cooperative multi-agent reinforcement learning under exogenous variability—e.g., unseen opponent strategies at test time. The authors propose an Exogenous Dec-POMDP (ED-POMDP) that factorises the global state into an endogenous component (controllable by the team) and an exogenous component (driven by the environment/opponent). Under this formalism they derive LEICA, a CTDE algorithm that (i) learns separate variational predictors for endogenous and exogenous transitions, and (ii) shapes a counterfactual, influence-weighted intrinsic reward which encourages actions that simultaneously improve team return and shift future exogenous states in a favourable direction. Extensive experiments on a new SMAX benchmark with 63 opponent strategies show large gains over MAPPO, QMIX, LAIES and SHAQ in both training and zero-shot generalisation regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel conceptual framing: ED-POMDP explicitly disentangles controllable vs. uncontrollable dynamics, giving a principled way to reason about robustness to non-stationary, non-learning opponents.\nPractical algorithm: LEICA retains the scalability of CTDE actor-critic methods; the additional VAE predictors and Jacobian-based reward are cheap to compute and easy to plug into MAPPO.\nStrong empirical results: Across 15 train/test splits (Small / Medium / Large) LEICA consistently outperforms strong baselines, often doubling the win-rate on unseen strategies while maintaining highest training performance."}, "weaknesses": {"value": "Manual state partition: The endogenous/exogenous split is hand-crafted using domain knowledge of SMAX; the method could fail if the partition is misspecified or unavailable in new domains.\nSingle-step influence: The reward uses only the one-step Jacobian ∂sˆx_{t+2}/∂s^e_{t+1}; long-horizon influence or multi-step planning is not considered."}, "questions": {"value": "1. Could the partition be learned from data using, e.g., conditional independence tests, sparsity priors, or causal discovery?\n2. Why restrict influence to one-step? Would a multi-step rollout (even short) improve the credit assignment, especially in sparse-reward tasks?\n3. What is the relation between the world model in model-based RL and the proposed exogenous dynamic model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wt5YMS6g3V", "forum": "Q4Wk1EIVSV", "replyto": "Q4Wk1EIVSV", "signatures": ["ICLR.cc/2026/Conference/Submission24809/Reviewer_YKj2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24809/Reviewer_YKj2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901550323, "cdate": 1761901550323, "tmdate": 1762943202899, "mdate": 1762943202899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for cooperative MARL under exogenous variability. The authors reformulate MARL as an Exogenous Dec-POMDP that explicitly separates team-controllable and environment related state components. They propose a CTDE-based algorithm, called LEICA, that combines two variational inference for exogenous/endogenous factors with an influence-weighted intrinsic reward that encourages both coordination and adaptability. Experiments on the SMAX benchmark demonstrate gains over standard MARL baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Authors is proposing an interesting approach to separate the learning of agent controlled states and environment controlled states to improve the robustness of MARL\n- The paper is well written and easy to follow."}, "weaknesses": {"value": "- No comparison to robust MARL baselines: my main concern with this paper is that it omits comparison to established minimax or distributionally-robust algorithms (e.g.,M3DDPG, “Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning”). This limits the strength of the robustness claim.\n- Hand-crafted state partition: The endogenous/exogenous split is assumed given; learning this decomposition automatically would improve generality.\n- Computational complexity: Two VAEs per agent and Jacobian-based influence estimation introduce significant training overhead compared to MAPPO/QMIX."}, "questions": {"value": "- how do you apply this type of method to environments where it is hard to separate the exogenous and indigenous states (e.g. image-based inputs)\n- Can the intrinsic reward be generalized to other environment other than SMAX, such as the ones that has on opponents. It makes sense for SMAX as we are trying to reduce the opponent’s health, so influence is desired. What happens to the other environment (e.g.cooperative navigation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "llN6QyGtyS", "forum": "Q4Wk1EIVSV", "replyto": "Q4Wk1EIVSV", "signatures": ["ICLR.cc/2026/Conference/Submission24809/Reviewer_1dpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24809/Reviewer_1dpp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24809/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762329157187, "cdate": 1762329157187, "tmdate": 1762943202688, "mdate": 1762943202688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}