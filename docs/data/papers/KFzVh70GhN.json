{"id": "KFzVh70GhN", "number": 651, "cdate": 1756763132681, "mdate": 1759898248565, "content": {"title": "Multiple Images Distract Large Multimodal Models via Attention Fragmentation", "abstract": "Many everyday tasks involve integrating information across multiple images, such as comparing photos and reading social media posts.\nRecent large multimodal models (LMMs) therefore accept multiple images, yet open-source systems remain far from reliable in multi-image understanding, with accuracies often falling below 50\\% on recent evaluations.\nWe analyse how these models allocate attention across images when visual tokens are processed in a single autoregressive, causally masked sequence. \nOur study uncovers a joint failure mode: the same background positions in each image repeatedly attract high attention while contributing little to prediction, and this effect is stronger for earlier images due to one-way attention under causal masking. \nWe term this phenomenon attention fragmentation, as attention is split across non-informative tokens instead of binding evidence between images. \nThese high-attention, low-utility tokens correspond to attention sinks previously observed in LLMs.\nTo address attention fragmentation, we introduce Attention Remasking (AR), a zero-parameter, post-training edit that operates on attention scores where the causal mask is enforced.\nAR masks sink tokens column-wise to prevent any query from attending to them, and selectively unmasks cross-image visual tokens deemed relevant by a grounded patch relevance score.\nThe attention scores freed from the masked sinks are reassigned to these unmasked links, creating forward connections from earlier to later images while preserving text autoregression.\nAR reduces attention fragmentation and improves accuracy over post-training baselines on recent multi-image benchmarks, delivering more effective cross-image integration without additional training.", "tldr": "", "keywords": ["Multi-image Understanding", "Large Multimodal Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6060812e678f5aaa51268c229342a954559d38fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the limitations of current Large Multimodal Models (LMMs) in understanding multiple images and introduces a novel concept called Attention Fragmentation.\nThrough empirical analysis, the authors reveal that when multiple images are processed in a single autoregressive sequence, attention tends to concentrate on similar background regions (so-called attention sinks) across images, especially for earlier ones due to causal masking. This leads to dispersed, low-utility attention and weak cross-image integration.\n\nTo address this, the authors propose Attention Remasking (AR)  to the pre-softmax attention scores. AR masks out sink tokens column-wise and selectively un-masks task-relevant cross-image tokens based on patch-level CLIP relevance guided by GroundingDINO. The freed attention is redistributed to these semantically meaningful links, preserving text autoregression.\n\nExperiments demonstrate consistent improvements across various open-source LMMs. Analyses further show that AR reduces attention fragmentation, mitigates recency bias, and enhances cross-image reasoning without retraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies attention fragmentation as a fundamental limitation in multi-image LMMs â€” connecting visual attention sinks with causal masking and positional bias. This provides a clear and interpretable explanation for why models struggle with multi-image reasoning.\n\n- The proposed Attention Remasking is a lightweight, training-free approach that can be universally applied to existing LMMs. It effectively reuses the modelâ€™s existing structure without introducing new parameters or requiring fine-tuning.\n\n- Evaluations on diverse benchmarks and models show consistent performance gains.\n\n- The paper is well-structured and easy to follow, with clear figures and thorough theoretical reasoning."}, "weaknesses": {"value": "- AR relies on GroundingDINO and CLIP for patch-level relevance estimation, which introduces external biases and limits reproducibility. The performance may degrade if the grounding detector is weak or misaligned with the modelâ€™s visual encoder.\n\n- Editing pre-softmax attention at inference could increase computational cost, but the paper does not report runtime or memory overhead.\n\n- The paper focuses training-free methods. Comparisons with fine-tuned or retrained multi-image models would better contextualize its practical competitiveness."}, "questions": {"value": "- How sensitive is AR to the accuracy or granularity of the grounding model (e.g., using weaker detectors or fewer regions)?\n\n- Could the CLIP-based relevance prior be replaced by an internal attention signal from the model itself (to reduce reliance on external models)?\n\n- Would fine-tuning a model with AR-modified attention patterns produce further gains, or does AR mainly serve as a post-hoc patch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OqxBWFgaNt", "forum": "KFzVh70GhN", "replyto": "KFzVh70GhN", "signatures": ["ICLR.cc/2026/Conference/Submission651/Reviewer_AqZJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission651/Reviewer_AqZJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900074240, "cdate": 1761900074240, "tmdate": 1762915575560, "mdate": 1762915575560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that large multimodal models that accept several images at once get stuck attending to the same background patches in every image. Because attention is causal, earlier images attract even more wasteful attention, so the model fails to connect evidence across images. They call this attention fragmentation. The fix is attention remasking. They edit the pre softmax attention scores only in the visual to visual block, hard mask columns that correspond to sink tokens, then selectively open forward links from earlier queries to later image tokens when a text grounded patch relevance prior says those links matter. The freed attention budget is reassigned to those links. They claim this lowers attention dispersion across images and reduces order bias, improving accuracy on recent multi image benchmarks without any retraining."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper ties order sensitivity and flaky multi-image reasoning to a concrete mechanism (repeated sinks + causal masking), not just vibes. The Chamfer/entropy analyses and sink-share skew give a coherent story. \n2. AR edits scores, not weights; preserves text autoregression; and uses grounded, instruction-conditioned priors to decide which forward links to unmask. Thatâ€™s practical and model-agnostic. \n3.They explain why VAR/SoFA fail conceptually (no new forward links; proportional redistribution preserves dispersion; untrained logits everywhere) and then show AR actually lowers late-layer entropy and reduces order bias. \n4.Mask-only and unmask-only both underperform full AR; grounded-CLIP scoring outperforms uniform / naÃ¯ve variants. Thatâ€™s the kind of sanity check many papers skip."}, "weaknesses": {"value": "1. The whole approach leans on a separate detector and CLIP to decide what is relevant. When grounding is brittle or the instruction is abstract, the method inherits those mistakes. There is no robustness story beyond trusting the detector. This is an external crutch, not an internal fix. \n2. The routing policy is hand crafted. You reclaim a fixed sink budget and pour it into a sparse set of keys according to a similarity prior. There is no guarantee those queries want that mass and no stability or sensitivity analysis to show the edit cannot go sideways. It is a clever hack, but still a hack. \n3.Selective unmasking mixes trained and never trained pathways. The paper itself points out that SoFA opens links that were never trained and performs unevenly. Here you also open untrained links, just fewer of them. There is no stress test for cases where CLIP overconfidently highlights the wrong region or where instructions do not map cleanly to patches. \n4. The evaluation story avoids cost and failure analyses. We see accuracy bumps and nicer entropy curves, but no latency or memory breakdown from running a detector plus CLIP plus mask rewrites, no adversarial counter cases, and no tasks where pushing attention forward is the wrong move. The claims are broad and the tests are narrow"}, "questions": {"value": "1. What happens when grounding is wrong or the instruction is abstract or aesthetic. Does the method confidently shove attention into bad regions and lock in errors. Please quantify failure rates under degraded grounding. \n2. How far can a patch level similarity prior carry genuine multi image reasoning like counts, geometry, and relational chains instead of just saliency chasing. Where does it break. \n3. Does the edit ever interfere with long mixed prompts where text needs steady internal routing. Show text only and text heavy mixed tasks to prove there is no collateral damage. \n4. Could a later image with high CLIP like regions hijack the freed budget and force order flips in the opposite direction. Any robustness checks against adversarial sinks and spurious relevance. \n5. What are the true compute and latency costs at realistic batch sizes and image counts. Do the gains hold under tight inference budgets or are we trading accuracy for throughput."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Svc8z6SXwP", "forum": "KFzVh70GhN", "replyto": "KFzVh70GhN", "signatures": ["ICLR.cc/2026/Conference/Submission651/Reviewer_42zK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission651/Reviewer_42zK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907008539, "cdate": 1761907008539, "tmdate": 1762915575417, "mdate": 1762915575417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates a crucial issue in Large Multimodal Models (LMMs) that handle multiple images, where the models suffer from attention fragmentation. To solve this, the authors propose a method called Attention Remasking (AR), which is a post-training technique that modifies the attention score matrix. This method masks \"sink\" tokens and unmasks task-relevant cross-image tokens, improving multi-image understanding without requiring retraining or hyperparameter tuning. The results show that AR mitigates attention fragmentation and improves performance on multiple image benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The identification of attention fragmentation in multi-image LMMs and the introduction of AR to address this issue is a novel contribution. \n2. The experimental methodology is sound, using well-defined benchmarks and appropriate metrics. The empirical evidence supports the claims of the paper, showing that AR reduces attention fragmentation and improves model performance.\n3. The method presented has the potential to impact the way multimodal models are designed, improving their accuracy and efficiency in tasks that require multi-image understanding."}, "weaknesses": {"value": "1. While the paper performs extensive testing on well-established multi-image benchmarks like MMIU, MuirBench, and MIRB, the results remain largely confined to these curated datasets. These benchmarks are useful, but they may not represent the full range of challenges present in real-world multimodal tasks. Real-world images often come with more noise, more complex interactions between images, and varied content that may require a different approach to attention allocation. For instance, highly dynamic images (e.g., images involving fast-moving objects or complex scenes) or images with low resolution might not behave the same way as those in the current benchmarks. Additionally, considering adversarially trained datasets such as the GTSRB dataset [1] is also an option.\n2. One of the strengths of AR is its post-training application, but the computational complexity of the technique remains unclear. The paper does not discuss the scalability of AR, especially in the context of large models or very large datasets. As LMMs continue to grow in size (think of models with billions of parameters), the overhead introduced by manipulating attention matrices post-training could become significant.\n3. While the paper shows that AR improves multi-image understanding, it doesnâ€™t provide a comprehensive ablation study. The authors mention DINO-grounded CLIP score/Masking & Unmasking visual tokens, but they do not break down the impact of each individual component on model performance. Is the core improvement due to DINO-grounded CLIP score, or masking attention sinks & unmasking cross-image links? Given that AR is a post-training edit, understanding which component has the most significant effect would give future researchers a clearer path for improvement.\n4. The paperâ€™s method, AR, focuses on the manipulation of attention weights based on a grounded patch relevance score. While this appears to be effective for the benchmarks, it raises concerns about overfitting the model to certain types of images. Specifically, the reliance on CLIP-based grounding may make the model too dependent on image content that aligns with the predefined semantic concepts of CLIP. When the images do not fit well into these predefined concepts, or when the grounded relevance does not accurately represent task-relevant information, AR's performance will drop.\n\n[1] Stallkamp, J., Schlipsing, M., Salmen, J., & Igel, C. (2011, July). The German traffic sign recognition benchmark: a multi-class classification competition. In The 2011 international joint conference on neural networks (pp. 1453-1460). IEEE."}, "questions": {"value": "1. The current benchmarks used in the paper do not reflect the diversity of real-world images, especially those with poor quality or significant noise. In practical applications like surveillance or social media analysis, the model will often have to process images that may be blurry, low-resolution, or distorted. Would AR still be effective in such cases, or does the technique rely too heavily on high-quality, well-structured images for successful attention manipulation?\n2. One of the key benefits of the AR method is that it does not require retraining the model. However, what impact does this post-processing step have on real-time performance? In applications where speed is crucial, such as autonomous driving or real-time video analysis, could the time spent remasking attention and recalculating attention scores become a bottleneck?\n3. The current results show that AR improves multi-image accuracy and reduces recency bias. However, over the long term, does it cause any catastrophic forgetting of previously learned behaviors when the model encounters unseen data distributions [2]? \n\n[2] Yang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., ... & Liu, Z. (2022). Openood: Benchmarking generalized out-of-distribution detection. Advances in Neural Information Processing Systems, 35, 32598-32611."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WHUEhrhvS0", "forum": "KFzVh70GhN", "replyto": "KFzVh70GhN", "signatures": ["ICLR.cc/2026/Conference/Submission651/Reviewer_1bgt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission651/Reviewer_1bgt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972088870, "cdate": 1761972088870, "tmdate": 1762915575297, "mdate": 1762915575297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies multi-image LMMs, diagnosing attention â€œfragmentationâ€ and recency bias, and proposes a post-training Attention Remasking (AR) that masks visual sink tokens and selectively un-masks cross-image links to re-allocate attention. It reports consistent gains across several open-source LMMs with simple plug-in changes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clearly formulates an important failure mode (fragmentation/recency) with intuitive metrics and diagnostics.\n2. Method is simple, training-free, and targeted (operates on visualâ€“visual attention only), with informative ablations.\n3. Broad empirical coverage across multiple LMMs/datasets showing consistent improvements."}, "weaknesses": {"value": "1. â€œLarge Multmodal Model (LMM)â€ (missing â€œiâ€) in Preliminaries.\n2. Eq. (8) is â€œdefined when w(i,â„“)>0â€, but the manuscript does not state how entropy and subsequent statistics are handled when ğ‘¤(ğ‘–,â„“)=0 (e.g., skip, impute, or add ğœ–). This affects fragmentation estimates and hypothesis tests.\n3. The method â€œrelaxes the visual parts of \"ğ‘€_causalâ€ based on relevance, but there is no formal constraint to prevent unintended information flow (e.g., unmasking transitive chains across later images) nor analysis of complexity/sparsity guarantees for the edited mask. Provide a clear rule or bound for which visual links can be unmasked and why leakage cannot occur.\n4. You report detector thresholds and a 0.1â€“0.9 threshold sweep; whatâ€™s missing is end-to-end runtime/throughput and memory overhead of AR per model/dataset, a proposal-count ablation (e.g., top-K), complexity stats for the edited mask (avg nonzeros per row), and a brief detector/backbone swap study. Include cost-vs-accuracy plots to support practical adoption.\n5. Beyond Wilcoxon+Holm and the Hodgesâ€“Lehmann median shift, please add per-task effect sizes with 95% CIs, seed variance (meanÂ±std over runs), and per-dataset tables of absolute accuracy deltas. This will clarify practical significance and guard against over-generalizing pooled results.\n6. Experiments focus on accuracy/entropy and order sensitivity; missing are robustness tests under noisy/over-segmented proposals, occlusion, or multi-turn prompts where mask edits persist across turns. Add these to demonstrate stability beyond single-turn benchmarks."}, "questions": {"value": "1. â€œLarge Multmodal Model (LMM)â€ (missing â€œiâ€) in Preliminaries.\n2. Eq. (8) is â€œdefined when w(i,â„“)>0â€, but the manuscript does not state how entropy and subsequent statistics are handled when ğ‘¤(ğ‘–,â„“)=0 (e.g., skip, impute, or add ğœ–). This affects fragmentation estimates and hypothesis tests.\n3. The method â€œrelaxes the visual parts of \"ğ‘€_causalâ€ based on relevance, but there is no formal constraint to prevent unintended information flow (e.g., unmasking transitive chains across later images) nor analysis of complexity/sparsity guarantees for the edited mask. Provide a clear rule or bound for which visual links can be unmasked and why leakage cannot occur.\n4. You report detector thresholds and a 0.1â€“0.9 threshold sweep; whatâ€™s missing is end-to-end runtime/throughput and memory overhead of AR per model/dataset, a proposal-count ablation (e.g., top-K), complexity stats for the edited mask (avg nonzeros per row), and a brief detector/backbone swap study. Include cost-vs-accuracy plots to support practical adoption.\n5. Beyond Wilcoxon+Holm and the Hodgesâ€“Lehmann median shift, please add per-task effect sizes with 95% CIs, seed variance (meanÂ±std over runs), and per-dataset tables of absolute accuracy deltas. This will clarify practical significance and guard against over-generalizing pooled results.\n6. Experiments focus on accuracy/entropy and order sensitivity; missing are robustness tests under noisy/over-segmented proposals, occlusion, or multi-turn prompts where mask edits persist across turns. Add these to demonstrate stability beyond single-turn benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9N4HNMci8e", "forum": "KFzVh70GhN", "replyto": "KFzVh70GhN", "signatures": ["ICLR.cc/2026/Conference/Submission651/Reviewer_FiPB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission651/Reviewer_FiPB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979298833, "cdate": 1761979298833, "tmdate": 1762915575102, "mdate": 1762915575102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}