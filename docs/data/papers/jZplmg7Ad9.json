{"id": "jZplmg7Ad9", "number": 17951, "cdate": 1758282360213, "mdate": 1763730031468, "content": {"title": "Alignment-Enhanced Integration of Connectivity and Spectral Sparse in Dynamic Sparse Training of LLM", "abstract": "With the rapid development of large language models (LLMs), identifying efficient strategies for training such large-scale systems has become increasingly critical. Although LLMs have achieved remarkable success across diverse applications, the necessity of maintaining full dense matrices during pre-training has been questioned, giving rise to parameter-efficient sparse pre-training methods which retains parameter-efficiency in both training and inference. These methods can be further divided into connectivity sparse training and spectral sparse training, with dynamic connectivity sparse training and low-rank factorization emerging as representative approaches for the two branches.\nHowever, a unified framework that effectively combines the strengths of both has yet to be established. In this work, we observe that the cancellation effect between the sparse and low-rank branches may limit the expressivity of the model, manifesting as output conflicts when the two components are combined. To address this issue, we propose a novel scheme that integrates dynamic sparse training with low-rank training, introducing a simple yet effective $\\textbf{alignment loss}$ to mitigate the disagreement between the two branches and promote better collaboration. We validate this scheme by combining a representative dynamic sparse training method, CHTs, with low-rank training, resulting in a new parameter-efficient training approach termed $\\textbf{CHTsL}$. The method is evaluated on LLaMA60M and LLaMA130M using the OpenWebText and C4 datasets, where only 10\\%, 20\\%, and 30\\% of the parameters are preserved compared to dense training. Experimental results demonstrate that our proposed scheme effectively alleviates the cancellation effect and improves training stability and performance compared to the naive combination of sparse and low-rank components. Also, the new scheme enables CHTsL to consistently outperform other parameter-efficient sparse training methods under the same parameter budget, achieving performance most close to dense training.", "tldr": "", "keywords": ["dynamic sparse training", "low-rank factorization", "spectral sparse training", "efficient training"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a0c5c5c69f987d709b02530192eabf3a6c240fd.pdf", "supplementary_material": "/attachment/7d69c3c3df55ec38b5a068f77642261055989a3f.zip"}, "replies": [{"content": {"summary": {"value": "The paper explores a combination of LoRA, activation and dynamic sparse connections to improve pre-training efficiency. It attempts to unify CoLA and CHTs approaches through an alignment loss, aiming to maintain performance while reducing parameter usage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of integrating CoLA with dynamic sparse connections is conceptually reasonable and consistent with recent trends in parameter-efficient pre-training. The use of alignment loss to bridge different sparsity paradigms shows an effort toward unified optimization.\n\n2. The experiments show solid performance across different sparsity levels, demonstrating the stability and robustness of the proposed approach.\n\n3. The paper is well-written and clearly structured, allowing readers to easily follow the methodology and experimental design."}, "weaknesses": {"value": "1. The proposed method mainly unifies CoLA and CHTs via an alignment loss, which appears to be an incremental combination and is somewhat limited in novelty.\n2. The evaluation is somewhat limited. It would be beneficial to include downstream task benchmarks such as HellaSwag and COPA, similar to what was done in CHTS, to better demonstrate generalization capability and practical value."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q1NBPVXBts", "forum": "jZplmg7Ad9", "replyto": "jZplmg7Ad9", "signatures": ["ICLR.cc/2026/Conference/Submission17951/Reviewer_iVGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17951/Reviewer_iVGQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660042686, "cdate": 1761660042686, "tmdate": 1762927752887, "mdate": 1762927752887, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposed CHTSL, an approach that unifies connectivity sparse training and spectral sparse training. They introduces an alignment loss mitigate the disagreement between the two branches and promote better collaboration. Experiments on small-scale LLama validate the efficacy of their approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is written and organized well.\n- The idea of alignment loss is reasonable and inspiring."}, "weaknesses": {"value": "- Lack of literature review. The paper discussed with pruning works, accounting for connectivity sparse training, differentiating against spectral sparse (low-rank) training. But in my opinion, structured pruning actually results in low-rank for the full model. Since the structured pruned models would have many columns or rows zero-out. The paper needs to discuss with structured-pruning-aware works, such as Only-Train-Once.\n\nOnly Train Once: A One-Shot Neural Network Training And Pruning Framework\n\n- Lack of sufficient numerical experiments. \n\n  - The numerical results are conducted under small-scale LLMs. It would be better to conduct over larger-scale LLMs to show the generality. \n  \n  -  Besides SiLU, it would be better to show over other activations."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QVNn1dE5rz", "forum": "jZplmg7Ad9", "replyto": "jZplmg7Ad9", "signatures": ["ICLR.cc/2026/Conference/Submission17951/Reviewer_QMmU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17951/Reviewer_QMmU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892568270, "cdate": 1761892568270, "tmdate": 1762927752414, "mdate": 1762927752414, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the cancellation effect between connectivity-sparse and spectral-sparse branches in dynamic sparse training and introduces an alignment loss. The idea is simple and practical, and experiments show clear gains over baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper identifies the \"cancellation effect\" and proposes the OCR metric, which provides a valuable quantitative perspective on hybrid sparse training.\n\n- The proposed method is simple and easy to implement."}, "weaknesses": {"value": "- The alignment loss is conceptually orthogonal to any combination of dynamic sparsity and low-rank training, yet experiments are limited to the CHTs + low-rank setup. Testing additional combinations would be necessary to confirm its generality.\n\n- OCR captures output-level discrepancies but does not fully demonstrate whether alignment mitigates gradient-level conflicts between branches. A more comprehensive analysis at the gradient level is recommended.\n\n- The paper lacks practical efficiency evaluations such as inference memory and throughput."}, "questions": {"value": "- Is there a correlation between OCR and global cosine similarity? Since OCR measures element-wise sign inconsistency, it may be influenced by local fluctuations rather than true directional cancellation. Such analysis could clarify OCRâ€™s distinct role.\n\n- How does the model's performance differ when the alignment loss is applied only to the Q/K layers compared to applying it across Q/K/V/O or FFN layers?\n\n- Do larger models (e.g., LLaMA-7B) exhibit similar cancellation patterns, and does alignment maintain its effectiveness at scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u1s1SNAUcK", "forum": "jZplmg7Ad9", "replyto": "jZplmg7Ad9", "signatures": ["ICLR.cc/2026/Conference/Submission17951/Reviewer_ob7g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17951/Reviewer_ob7g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17951/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914439124, "cdate": 1761914439124, "tmdate": 1762927751983, "mdate": 1762927751983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}