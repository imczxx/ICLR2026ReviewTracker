{"id": "h4FOb6ak0Q", "number": 8074, "cdate": 1758058320715, "mdate": 1759897809964, "content": {"title": "GraphBench: Next-generation graph learning benchmarking", "abstract": "Machine learning on graphs has recently achieved impressive progress across domains like molecular property prediction or chip design. However, benchmarking practices remain fragmented—often relying on narrow, task-specific datasets and inconsistent evaluation protocols---which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols---with consistent dataset splits and performance metrics that account for out-of-distribution generalization---as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using state-of-the-art message-passing neural networks and graph transformer models, providing principled baselines, establishing reference performance.", "tldr": "We present a new benchmarking suite for graph learning, covering diverse domains, meaningful splits, and evaluations.", "keywords": ["graph learning", "graph neural networks", "GNN", "benchmark", "datasets"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f9f5bb46d3f89fe921e7816c6979e8cc58756035.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces GraphBench, a unified benchmarking suite for graph learning that spans multiple domains (social networks, hardware and chip design, reasoning and optimization, and earth systems) and supports node-, edge-, graph-level, and generative tasks under standardized splits and domain-relevant metrics, with explicit tests for out-of-distribution generalization. It releases datasets and a Python package with PyTorch Geometric–compatible loaders, fixed train/validation/test splits, and a common hyperparameter-tuning/evaluation pipeline to promote reproducibility. The authors provide baselines using message-passing neural networks and graph transformers and report cross-domain observations. Overall, the contributions are the curated multi-domain dataset suite, standardized protocols and metrics with OOD evaluation, reference baselines, and accessible software intended to catalyze more robust and comparable graph learning research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s strengths are mostly infrastructural: it assembles a single benchmark that spans multiple domains and task regimes, and bakes in out-of-distribution evaluations rather than treating them as an afterthought. On originality, the contribution is less about a new task than about scope and unification—moving beyond molecule/citation staples to include social networks, chip and circuit design, SAT algorithm selection/performance prediction, and weather, with node-, edge-, graph-level, and generative tasks under one roof. On quality, the suite specifies fixed splits, domain-relevant metrics, hyperparameter tuning scripts, and a common evaluation pipeline, and it runs baselines across multiple seeds—choices that encourage reproducibility. In terms of significance, the inclusion of large, practically motivated datasets (for example, a SAT benchmark with more than 100k instances) and explicit temporal/size splits makes this a potentially useful reference point for the community."}, "weaknesses": {"value": "The main weaknesses are on evaluation depth and model coverage: despite the breadth of tasks, the benchmark tests only a small set of generic baselines (essentially GIN/GINE and one graph transformer, plus MLP/DeepSets), trained with minimal tuning and only three seeds, which limits what we can conclude about method rankings and robustness; broadening to stronger modern variants and running more seeds with proper HPO across domains would materially improve credibility. In SAT, core baselines are constrained by engineering choices: the graph transformer is excluded due to positional-encoding cost and out-of-memory issues, clause graphs are dropped, and evaluation is restricted to small formulas, so the domain’s hardest settings are not probed; meanwhile, classic tabular feature models outperform the provided GNNs by a wide margin, underscoring that the current baselines may not reflect best available practice. For algorithmic reasoning, the paper itself notes seed instability for GIN on several tasks, reinforcing the need for more repetitions and variance-aware reporting. For weather, the baseline substantially underperforms simple persistence and purpose-built systems like GraphCast and is intentionally simplified, which is understandable for a first release but reduces the benchmark’s immediate diagnostic value; adding stronger domain baselines and task-relevant metrics beyond a single MSE would help."}, "questions": {"value": "- Model coverage feels too narrow for the benchmark’s ambitions; can you expand beyond GIN/GINE, a single graph transformer, MLP, and DeepSets to include stronger, task-specific and modern graph baselines (e.g., CO methods the paper itself notes outperform simple baselines, and additional GT variants)?\n\n- Baseline robustness and tuning seem under-probed: results are averaged over only three seeds and automated HPO is demonstrated on a single case; could you increase seeds (e.g., 10+ where feasible), report confidence intervals/significance tests, and roll out a budgeted, uniform HPO protocol so method rankings are more reliable across domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bwf7pj1CVm", "forum": "h4FOb6ak0Q", "replyto": "h4FOb6ak0Q", "signatures": ["ICLR.cc/2026/Conference/Submission8074/Reviewer_TXkq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8074/Reviewer_TXkq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761089866127, "cdate": 1761089866127, "tmdate": 1762920065721, "mdate": 1762920065721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GraphBench, a contribution of around 20 unique datasets from 7 broad and diverse categories for graph learning benchmarking. It complements the existing popular graph learning benchmarks which may be significant for molecular and citation networks, as examples, but often missing for other areas such as chip design, circuit design and weather forecasting, among others (though there are individual areas in the literature that tackle these problems). The paper also highlights the current limitations with graph benchmarks in terms of data diversity reflecting multiple real world scenarios, in/out distribution splits, evaluation consistencies and framework for usage. It finally presents a framework based on Pytorch and Pytorch Geometric which acts as the interface for loaders, optimizers and evaluators."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The scope, size and diversity of datasets in the proposed benchmark is comprehensive, as compared to existing benchmarks such as OGB.\n- Some of the domains, eg. algorithmic reasoning, has benchmarks but require different interfacing. This paper addresses practical gaps like these.\n- Each dataset consists of elaborate details on motivations, statistics, intra-category diversity with multiple unique datasets, alongside baseline MPNN and GT models.\n- The benchmark can be used with a user-friendly python package standard with other graph and other modalities' benchmarks, and can be integrated in existing graph learning workflows conveniently."}, "weaknesses": {"value": "- The manuscript includes reasonable discussion points on limitations of existing benchmarks, however its contribution is limited as a dataset paper, without addressing major prior limitation points. For instance, it does not explore correlations or transfer patterns between domains (e.g., how models that perform well in reasoning tasks transfer in physical or social domains), although size generalization experiments are included.\n- Other evaluations are primarily empirical baseline (MPNN, GT) scores which follow the literature.\n- Many of the limitations with graph benchmarks as mentioned above in summary sections are well known and corresponding remedies are implemented - for eg. OGB has consistent evaluators, realistic splits, and other benchmarks also follow these or part of these characteristics. \n- Writing inconsistencies- some datasets have their experiments in main paper, while some have been pushed to appendix.\n- There are references which may be incorrect and could be a result of LLM-assisted writing or search."}, "questions": {"value": "na"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r9jbX1ilR4", "forum": "h4FOb6ak0Q", "replyto": "h4FOb6ak0Q", "signatures": ["ICLR.cc/2026/Conference/Submission8074/Reviewer_k5Hy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8074/Reviewer_k5Hy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808719875, "cdate": 1761808719875, "tmdate": 1762920065241, "mdate": 1762920065241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GRAPHBENCH, a unified benchmarking suite for graph learning that spans multiple domains (e.g., social networks, chip and circuit design, SAT-solving, combinatorial optimization, and weather forecasting) and supports node-, edge-, graph-level, and generative tasks. The authors argue that current benchmarks are fragmented and overly narrow, and they provide standardized dataset splits, task-relevant metrics, hyperparameter tuning utilities, and baseline comparisons using both MPNNs and graph transformers. The benchmark aims to encourage more realistic evaluation and out-of-distribution generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is easy to follow and well-written. \n2. The paper solves a very important challenge in graph learning, i.e., the limited scope of graph learning. \n3. The authors emphasize splits that reflect realistic deployment scenarios (e.g., temporal splits, size-generalization splits)."}, "weaknesses": {"value": "1. Although hyperparameter tuning is mentioned as standardized, it is not clear how consistent or fair the tuning budgets are across models. More transparency on tuning methodology and resource constraints would strengthen reproducibility.\n2. Some of the datasets, particularly in circuit and combinatorial optimization domains, appear extremely large. Clearer discussion on required computational resources and practical training feasibility would be helpful."}, "questions": {"value": "1. Can the authors provide more details in training and hyperparameter tuning? \n2. Can the authors show the computational resource consumption?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yF906CRIyy", "forum": "h4FOb6ak0Q", "replyto": "h4FOb6ak0Q", "signatures": ["ICLR.cc/2026/Conference/Submission8074/Reviewer_JY6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8074/Reviewer_JY6e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855183524, "cdate": 1761855183524, "tmdate": 1762920064791, "mdate": 1762920064791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces GraphBench, a benchmarking suite that spans diverse domains and prediction tasks—node-level, edge-level, graph-level, and generative—under standardized evaluation protocols. GraphBench also provides classic GNNs and Graph Transformers as principled baselines to establish reference performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The source code and datasets are provided, enabling reproducibility.\n2. The benchmark covers a wide range of domains and prediction tasks."}, "weaknesses": {"value": "The experimental results for the SAT dataset in Table 2 are not informative. The graph-based method (e.g., GIN) is evaluated across different graph representations, while traditional methods are evaluated on hand-crafted features, making the comparison misaligned and difficult to interpret.\n\nBeyond dataset contribution, the **contributions appear limited**. \nI recognize that this paper make a good contribution by providing a benchmark aggregating from several new/existing data sources across diverse domains and tasks.\nHowever, as papers in top-tier venues like ICLR generally expect contributions that extend beyond dataset contribution and standardization. \nDeeper analysis and substantive insights are needed, e.g., principled evaluations, ablations, and diagnostic studies that advance understanding of when and why certain architectures succeed or fail.\n\nMoreover, the main paper should be **self-contained**. Several key performance metrics are relegated to the appendix, which hinders readability and assessment of the core claims. \nFor instance, for the CO dataset in Section 3.3.2, the primary performance table appears only as Table 14 in the appendix.\n\nThe empirical analysis is shallow. \nIn Section 3.3.2, the discussion largely describes surface-level performance differences between methods with intuitive but untested rationales tied to model design. I recommend deeper analysis to investigate the performance difference between different methods."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JeA1zVVLBH", "forum": "h4FOb6ak0Q", "replyto": "h4FOb6ak0Q", "signatures": ["ICLR.cc/2026/Conference/Submission8074/Reviewer_DrSB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8074/Reviewer_DrSB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8074/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998123313, "cdate": 1761998123313, "tmdate": 1762920064315, "mdate": 1762920064315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}