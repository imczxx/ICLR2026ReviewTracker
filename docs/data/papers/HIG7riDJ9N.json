{"id": "HIG7riDJ9N", "number": 13632, "cdate": 1758220144528, "mdate": 1759897423594, "content": {"title": "Exploring Real-Time Super-Resolution: Benchmarking and Fine-Tuning for Streaming Content", "abstract": "Recent advancements in real-time super-resolution have enabled higher-quality video streaming, yet existing methods struggle with the unique challenges of compressed video content. Commonly used datasets do not accurately reflect the characteristics of streaming media, limiting the relevance of current benchmarks. To address this gap, we introduce a new comprehensive dataset - $\\textbf{StreamSR}$ - sourced from YouTube, covering a wide range of video genres and resolutions representative of real-world streaming scenarios. We benchmark 11 state-of-the-art real-time super-resolution models to evaluate their performance for the streaming use-case.\n\nFurthermore, we propose $\\textbf{EfRLFN}$, an efficient real-time model that integrates Efficient Channel Attention and a hyperbolic tangent activation function - a novel design choice in the context of real-time super-resolution. We extensively optimized the architecture to maximize efficiency and designed a composite loss function that improves training convergence. EfRLFN combines the strengths of existing architectures while improving both visual quality and runtime performance.\n\nFinally, we show that fine-tuning other models on our dataset results in significant performance gains that generalize well across various standard benchmarks. We made the dataset, the code, and the benchmark available at $\\textit{[Link is redacted]}$.", "tldr": "We benchmarked 11 advanced real-time SR models on a new UGC dataset for streaming content and proposed EfRLFN model to boost video upscaling quality.", "keywords": ["super-resolution", "dataset", "benchmark", "real-time", "quality assessment", "video compression"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0522f854f6be5859c87ab35b1d5f05df4318f181.pdf", "supplementary_material": "/attachment/a05fa6e922f75a7d8ed0224d3f985d02e5539374.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces StreamSR, a large-scale dataset of 5,200 compressed YouTube videos for realistic benchmarking of real-time super-resolution. It also proposes EfRLFN, an efficient model using tanh activations, channel attention, and a composite loss for better quality-speed trade-offs. Extensive experiments and a large user study show EfRLFN achieves the best performance among real-time SR models and that fine-tuning on StreamSR boosts other models’ results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. StreamSR focuses on real-world compressed videos instead of the clean, synthetic data used in most existing benchmarks.\n\n2. The paper includes both objective metrics and a large-scale subjective user study, which adds credibility to the results.\n\n3. In addition to the compressed dataset, this paper proposes EfRLFN, an efficient real-time super-resolution model designed to further enhance both visual quality and inference speed."}, "weaknesses": {"value": "1. The proposed EfRLFN model is a very incremental change over the existing RLFN . It swaps an attention block (ESA for ECA) and an activation function (ReLU for tanh).\n\n2. The paper frames the problem as super-resolution for streaming content (i.e., video), but the proposed model is a single-image SR model applied frame-by-frame. This approach completely ignores temporal information, which is a critical component of video processing.\n\nDetailed comments:\n 1. My main concern is the disconnect between the problem -- video streaming-- and the proposed solution, single-image super-resolution (SR). Processing each video frame independently can lead to temporal artifacts such as flickering or popping because the model lacks context from previous or future frames. A true video SR model would utilize this temporal information to maintain consistency. The paper justifies its approach by citing that existing VSR models are too slow, which is valid, but it does not address potential quality trade-offs of their method. When applying EfRLFN to video, does it introduce temporal instability compared to models like NVIDIA's VSR—which, although proprietary, likely incorporates temporal awareness?\n\n 2.Furthermore, the paper's own results show that a large part of the performance gain comes from the data, not the model. When SPAN and RLFN are fine-tuned on StreamSR, their performance jumps dramatically. This is great validation for the dataset, but it makes the EfRLFN model itself seem less essential. It seems the paper's primary finding is \"training on real-world compressed video is better,\" which, while true, is not a huge surprise"}, "questions": {"value": "Refer to above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NSffJ7eOMl", "forum": "HIG7riDJ9N", "replyto": "HIG7riDJ9N", "signatures": ["ICLR.cc/2026/Conference/Submission13632/Reviewer_LP9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13632/Reviewer_LP9y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745866494, "cdate": 1761745866494, "tmdate": 1762924209553, "mdate": 1762924209553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical gap in existing super-resolution research: the lack of datasets that realistically model the compression artifacts found in real-world video streaming. To address this, the authors make several contributions. First, they introduce StreamSR, a new, large-scale dataset of 5,200 videos sourced from YouTube, which contains authentic compression artifacts. Second, they conduct a comprehensive benchmark of 11 state-of-the-art (SOTA) real-time SR models on this new dataset, notably including a large-scale subjective user study with over 3,800 participants. Third, they propose EfRLFN, a lightweight SR model based on RLFN, which incorporates modifications like Efficient Channel Attention (ECA) and a tanh activation function to achieve a strong balance of quality and runtime performance. The authors show that their model, as well as other models fine-tuned on StreamSR, achieve significant performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Dataset Significance and Quality: The paper's most valuable contribution is the StreamSR dataset. The authors correctly argue that popular datasets like DIV2K and Vimeo90K do not adequately represent the challenges of real-world streaming media. The effort to collect, filter, and curate a 5,200-video dataset with associated low- and high-resolution pairs from a streaming source is substantial and addresses a clear need in the community .\n\nComprehensive Benchmarking: The second major strength is the systematic and thorough benchmarking of 11 real-time SR methods. This evaluation is rigorous, employing 7 different objective metrics and, most importantly, a large-scale subjective study.\nLarge-Scale Subjective Study: The inclusion of a subjective evaluation with 3,822 participants and over 37,000 valid responses is highly commendable. This provides a much-needed perceptual grounding for the objective metrics, and the community is well-aware of the significant time and financial cost required to obtain such data. The paper's finding that fine-tuning on StreamSR significantly improves the performance of existing models like SPAN and RLFN strongly validates the dataset's utility and \"real-world\" nature."}, "weaknesses": {"value": "Limited Architectural Novelty: The primary weakness is the limited originality of the proposed EfRLFN model. The architecture appears to be a thoughtful and effective combination of existing lightweight techniques rather than a novel design. The key modifications—such as replacing ESA with ECA and using a tanh activation—are well-motivated by prior work (e.g., SPAN ) and common practices from recent NTIRE challenges. While the ablation studies (e.g., Table 3, Figure 5(b)) are detailed and confirm the authors' design choices, they largely reinforce existing knowledge (e.g., the benefit of odd symmetric activation functions) rather than providing fundamentally new insights for the field.\n\nInsufficient Evidence for \"Hardware-Friendly\" Claims: The paper repeatedly emphasizes that EfRLFN's design is \"efficient\"  and \"hardware-friendly\". However, these claims are supported only by FPS measurements from PyTorch on an NVIDIA RTX 2080 GPU. This benchmark is a useful proxy but does not represent the performance in a true deployment scenario. Real-world applications typically involve inference engines (e.g., TensorRT, OpenVINO) and further optimizations like model quantization, operator fusion, and platform-specific kernel tuning. I understand that a full deployment analysis is a complex task, but the \"hardware-friendly\" claims are not sufficiently substantiated by the current experiments. At a minimum, this limitation should be discussed in the paper."}, "questions": {"value": "Reframing of Contributions: Given that the paper's most significant and lasting contribution is arguably the comprehensive StreamSR dataset and the large-scale benchmark, would the authors consider reframing the Abstract and Introduction? Currently, the EfRLFN model is presented as a primary contribution on par with the dataset. However, as noted, the model's architectural novelty is limited, and similar designs are common in NTIRE reports. Perhaps positioning the dataset and benchmark as the central contribution, with EfRLFN serving as a strong new baseline developed for this benchmark, would more accurately reflect the paper's impact.   \n\nClarification of Scope (SISR vs. VSR): The paper's motivation is \"video streaming\" and \"video content\". However, the proposed EfRLFN model is explicitly a single-image SR (SISR) method (\"we focus on image SR for our approach\"). This is a valid and common strategy for real-time applications, but it does not leverage temporal information. Could the authors please clarify this distinction more prominently in the Abstract and Introduction? This would help set reader expectations, as \"video super-resolution\" often implies temporal (VSR) methods.   \nDeployment Performance: Following up on the weakness regarding efficiency claims: I would be very willing to reconsider my score if the authors could provide even preliminary data on deployment. For instance, what is the inference speed, throughput, or power consumption of EfRLFN and a key competitor (like SPAN or RLFN) when exported (e.g., to ONNX) and run on an inference engine like TensorRT or OpenVINO? This would provide much stronger evidence for the model's practical, \"hardware-friendly\" efficiency beyond the PyTorch runtime. If this is not feasible, I strongly suggest adding a discussion of this limitation and positioning platform-specific optimization as future work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "G1iz2vLaeu", "forum": "HIG7riDJ9N", "replyto": "HIG7riDJ9N", "signatures": ["ICLR.cc/2026/Conference/Submission13632/Reviewer_2ofZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13632/Reviewer_2ofZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891484215, "cdate": 1761891484215, "tmdate": 1762924209052, "mdate": 1762924209052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses real-time super-resolution for compressed streaming videos. The authors propose **StreamSR**, a large-scale dataset of real YouTube videos with compression artifacts, and **EfRLFN**, an efficient network designed for high-quality, low-latency upsampling. A comprehensive benchmark on 11 methods, including objective and subjective evaluations, demonstrates the effectiveness of the propsed approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a timely and practical problem: enhancing low-quality compressed videos in real-world streaming scenarios. By introducing both a new dataset and benchmark, it highlights the limitations of current SR methods under realistic conditions.\n2. The StreamSR dataset fills an important gap by providing a large-scale collection of real YouTube videos with natural compression artifacts, offering a more realistic evaluation platform than synthetic benchmarks.\n3. EfRLFN achieves competitive performance among real-time SR models, with strong results on the proposed dataset and favorable efficiency-quality trade-offs, validated through extensive experiments."}, "weaknesses": {"value": "1. The paper focuses on real-time SR for compressed streaming videos, but the related work primarily discusses general real-time SR methods, with little discussion of existing compressed VSR approaches. Important works in this domain—such as *Learning Degradation-Robust Spatiotemporal Frequency-Transformer for Video Super-Resolution*—are not adequately reviewed. A dedicated discussion on compressed VSR literature is needed.\n2. The main claimed contribution is the StreamSR dataset, yet no sample frames or metadata (e.g., compression settings like bitrates, codecs, and resolution scaling methods) are provided in the paper or supplementary material. Including visual examples and detailed encoding information would help reviewers assess the dataset’s quality and realism.\n3. The proposed EfRLFN lacks clear architectural novelty for handling compression artifacts. The design appears to be a refined version of RLFN without explicit mechanisms to address compression distortions (e.g., blocking, blurring). The performance gain may stem largely from training on the new dataset rather than model innovation"}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MGV0ykkULK", "forum": "HIG7riDJ9N", "replyto": "HIG7riDJ9N", "signatures": ["ICLR.cc/2026/Conference/Submission13632/Reviewer_yoXt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13632/Reviewer_yoXt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892632874, "cdate": 1761892632874, "tmdate": 1762924208660, "mdate": 1762924208660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of enhancing video streaming quality via super-resolution (SR) in real-time. The authors note that standard SR models trained on datasets like DIV2K or Vimeo-90K struggle with heavily compressed streaming videos (e.g. YouTube content), which introduce artifacts (blockiness, blur, loss of detail) not represented in those datasets. To bridge this gap, the paper introduces StreamSR, a new dataset of 5,200 YouTube video clips (over 10 million frames) spanning diverse genres and resolutions (360p–1440p) representative of real streaming scenarios. They benchmark 11 state-of-the-art real-time SR models on StreamSR under two upscaling tracks (2× and 4×), using 7 image quality metrics (including PSNR, SSIM, LPIPS, CLIP-IQA, etc.) and a large-scale user study with 3,800+ participants. In addition, the authors propose a new efficient SR model called EfRLFN (Efficient Residual Lightweight Feature Network), which builds upon the prior RLFN architecture with targeted improvements. EfRLFN integrates Efficient Channel Attention (ECA) and hyperbolic tangent activations (tanh), along with a refined training strategy, to achieve better image quality and faster inference. Empirically, EfRLFN achieves state-of-the-art performance among real-time methods on the StreamSR benchmark, delivering favorable quality–complexity trade-offs. Notably, fine-tuning existing SR models on the StreamSR data yields significant performance gains that also generalize to standard benchmarks (the fine-tuned models show improved metrics on Set14, Urban100, DIV2K, etc.). Overall, the paper provides a comprehensive evaluation of real-time SR in streaming contexts: the StreamSR dataset, an improved SR model (EfRLFN), and thorough experiments demonstrating both objective improvements (e.g. higher PSNR/SSIM) and subjective gains (user preference overwhelmingly favoring EfRLFN outputs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelty: The new StreamSR dataset is a valuable contribution, addressing a need for training/evaluating SR on real-world streaming content. It comprises 5.2K YouTube videos (25–30s clips) with aligned low/high-resolution pairs at 360p→1440p (4×) and 720p→1440p (2×) scales. Unlike prior SR datasets which often use pristine images synthetically downscaled (e.g. DIV2K, REDS), StreamSR provides naturally compressed low-resolution frames containing authentic streaming artifacts (from common codecs like VP9, H.264, AV1). This yields a broader quality range (MDTVSFA 0.41–0.61) more reflective of real YouTube content. The dataset spans diverse genres and content types, and the authors carefully filtered and clustered videos to ensure diversity in the test set. Overall, StreamSR is novel and well-constructed, better representing real streaming scenarios than previous SR data (which were shorter or lacked compression artifacts). This greatly enhances the relevance of benchmark results and will likely support future research in streaming video SR.\n\n2. Empirical Performance and Benchmarking Methodology: The experimental evaluation in this paper is comprehensive and rigorous. The authors benchmark 11 SR methods that are capable of real-time upscaling, ranging from classic baselines (Bicubic, ESPCN) to recent NTIRE winners (SMFANet, SPAN, RLFN) and even NVIDIA’s proprietary VSR solution. Each model is evaluated before and after fine-tuning on StreamSR, which provides insight into both out-of-the-box performance and achievable gains with adaptation. They report results on seven objective metrics, covering distortion-oriented measures (PSNR, SSIM), learned perceptual metric (LPIPS), and no-reference quality indices (e.g. CLIP-IQA and MUSIQ for human-aligned scoring), ensuring a well-rounded assessment of quality. Impressively, the paper also includes a user study with ~3,800 participants, collecting pairwise preference judgments. This large subjective test adds strong evidence of real-world efficacy, beyond just numbers. The results show EfRLFN’s outputs are overwhelmingly preferred by users over other methods (e.g. 77.4% of users favor EfRLFN over NVIDIA VSR in head-to-head comparisons). Such a combination of objective and subjective evaluation demonstrates empirical rigor. The benchmarking methodology is solid: the authors detail the evaluation protocol, use statistically significant comparisons (reporting confidence intervals), and even test generalization by applying StreamSR-trained models to standard datasets (Set14, Urban100, etc.). Notably, fine-tuning on StreamSR yields consistent improvements on these benchmarks, confirming the real-world data has broader benefits. Reproducibility is also addressed, the paper promises to release the dataset, code, and even the collected user study data, which should enable others to verify and build upon these results. Overall, the experiments are thorough and convincing, lending credibility to the paper’s claims."}, "weaknesses": {"value": "1. Generalization Concerns: While StreamSR is a strong step toward real-world data, there are some questions about generalization. The dataset and experiments focus solely on YouTube videos and the prevalent codecs used there (VP9, H.264, AV1). This covers a large portion of online content, but it may not generalize to other domains or compression formats. For example, professional streaming or broadcast uses codecs like HEVC or VVC; these might produce different artifacts, and it’s unclear if models fine-tuned on StreamSR would handle them well. Similarly, the resolution range is up to 1440p, the benchmark does not evaluate 1080p→4K (2160p) upscaling, a common high-end scenario. The authors do show that models trained on StreamSR generalize to standard SR test sets, which is encouraging. However, the reverse direction (generalizing to content outside the YouTube distribution) remains uncertain. In future work, it would strengthen the contribution to expand codec and content coverage, e.g. include other video sources or higher-resolution tracks, to ensure the approach is robust beyond the exact domain it was trained on.\n\n2. Missing Ablations and Comparisons: The authors conducted several ablation studies (activation functions, loss functions, etc.), but a few comparisons are either missing or left to supplemental material. For instance, an ablation on the composite loss would be useful to see how each component (Charbonnier, VGG perceptual, Sobel) contributes to final quality, the paper mentions this composite greatly helped training stability, so showing a breakdown of its impact would solidify that claim. Similarly, since EfRLFN is built upon RLFN, it would be enlightening to have a direct comparison of training with vs. without the new StreamSR data for that model: e.g. how much does fine-tuning RLFN on StreamSR improve its performance on compressed videos (the authors do include this result) and on standard benchmarks (they report gains). Such comparisons are partly given (Table 2 and discussion), but a more detailed ablation could isolate the effect of the dataset itself on a model’s performance. Another missing aspect is any comparison (even qualitative) to video-specific SR methods that leverage temporal redundancy. While most such methods are too slow for real-time, it would be interesting to see if, for example, a two-frame fusion approach could outperform single-frame EfRLFN at the cost of some speed, this could inspire future research on reaching a better quality/speed middle ground. Lastly, the main paper emphasizes 2× upscaling results; the 4× track results are deferred to the appendix. It might improve the paper to at least summarize the 4× findings in the core text, since 4× (360p→1440p) is a significant challenge and presumably where EfRLFN’s advantages also shine. In conclusion, additional ablations (loss function components, dataset contribution) and broader comparisons (to non-real-time or multi-frame methods) would further strengthen the work. These omissions do not undermine the existing results, but addressing them could provide a fuller picture and preempt questions from readers. The authors are encouraged to incorporate such analyses either in a revision or future follow-up, to solidify the paper’s conclusions and explore the boundaries of the proposed approach.\n\n3. Methodological Clarity and Experimental Limitations: For the most part the paper is well-written and thorough, but a few methodological points could be clearer. The novelty of the EfRLFN model might be seen as somewhat incremental, it is essentially an improved variant of RLFN with known techniques (ECA attention, tanh activation) adopted from prior works. The paper does justify these choices well and supports them with ablations, but some readers may feel the model innovation is an optimization of existing components rather than a fundamentally new architecture. This is not a severe issue (pragmatic improvements are valuable for real-time tasks), yet highlighting this context is important for clarity. In terms of experiments, one limitation is that the authors restricted comparisons to real-time models, excluding stronger but slower SR networks (like SwinIR, Real-ESRGAN) from quantitative evaluation. Their reasoning is valid, those models fall far short of real-time speeds, but it does slightly limit the perspective on the ultimate achievable quality. Including at least a reference comparison to a top-performing offline model (even if only in an appendix or figure) could help quantify the quality gap between real-time and non-real-time SR. Another minor clarity point is the lack of detail in the main paper about the user study protocol (though presumably in the supplement): understanding how pairs were presented or how scores were aggregated would help gauge reliability. Finally, the work focuses on single-frame SR; there is no exploration of multi-frame/video SR algorithms that exploit temporal information. While this choice is appropriate for real-time efficiency, it’s an implicit limitation, methods that use neighboring frames (even in a limited way) might recover details or reduce flicker that single-frame methods cannot. Clarifying this as a conscious scope choice (and perhaps discussing the potential of efficient multi-frame SR in the future) would strengthen the methodology section. In summary, the paper could be more explicit about these limitations and choices, to enhance transparency and guide future extensions."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GsV1oQ2Tql", "forum": "HIG7riDJ9N", "replyto": "HIG7riDJ9N", "signatures": ["ICLR.cc/2026/Conference/Submission13632/Reviewer_9P2C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13632/Reviewer_9P2C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13632/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900153236, "cdate": 1761900153236, "tmdate": 1762924208301, "mdate": 1762924208301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}