{"id": "91jL62CQF1", "number": 4550, "cdate": 1757705767190, "mdate": 1759898027110, "content": {"title": "IntuitiveGraphLLM: Intuitive Graph-based Text Representation with Large Language Model", "abstract": "Graphical representations of text can sharpen the inductive biases of large language models (LLMs), yet most graph-based approaches rely on co-occurrence, order, or position alone and therefore over-connect unrelated tokens while missing conceptually salient links. We introduce Intuitive Graphs (IGs)—graphs that explicitly encode both (i) structural context (local order/proximity/position) and (ii) conceptual relevance (semantic affinity in embedding space)—and IntuitiveGraphLLM, a framework that builds, encodes, and fuses IGs with pretrained LLMs. Given a passage, we first construct IGs by pruning structure-induced edges with a semantic gate based on cosine similarity between token (or span) embeddings, yielding sparse, human-plausible graphs. We then obtain initial node features from contextual embeddings and apply Graph Attention Networks (GATs) to emphasize informative nodes/edges to produce graph-level features. Finally, we perform hybrid fusion by integrating graph-level embeddings with LLM-based contextual representations, enabling the model to leverage complementary structural and conceptual signals. We evaluate our approach on five benchmark datasets spanning short and long documents and class-imbalance settings. Across benchmarks, IntuitiveGraphLLM consistently improves over strong text-only and graph-only baselines; gains persist under varied IG constructions, node embeddings, GAT depths/heads, and LLM backbones, with ablations confirming that IG is the key driver of performance and reduced edge noise. IntuitiveGraphLLM provides a principled, interpretable way to make text graphs both contextual and conceptually grounded, translating into more faithful reasoning and stronger downstream accuracy.", "tldr": "We introduce Intuitive Graphs (IGs)—graphs that explicitly encode both (i) structural context and (ii) conceptual relevance—and IntuitiveGraphLLM, a framework that builds, encodes, and fuses IGs with pretrained LLMs.", "keywords": ["IntuitiveGraphLLM", "IntuitiveGraph", "Structure-aware", "Graph", "Graph Representation", "Feature Fusion"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/492a4874a8d17f43466fbb96534bff428e48ee9a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a framework called IntuitiveGraphLLM, which combines the power of Large Language Models with an innovative graph-based representation of text. The core idea behind IG-LLM is to construct Intuitive Graphs, which capture both structural context and semantic relevance between tokens, and then fuse these graphs with pretrained LLMs to enhance text representation and downstream performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors provide a clear and interpretable methodology for building the IGs, ensuring that the constructed graphs are both sparse and human-plausible. This interpretability is valuable for understanding how structural and semantic cues contribute to the performance of the model.\n2. The paper reports thorough experiments across five benchmark datasets, covering a diverse range of tasks, including biomedical and commonsense reasoning. The approach consistently outperforms LLM-only models and graph-only models."}, "weaknesses": {"value": "1. The framework introduces complexity by adding a parallel graph-based branch (IG construction, GAT encoding, and fusion). However, the performance benefit on the majority of datasets is negligible, calling the value of this added complexity into question. For example, on the PubMedQA dataset, the accuracy gain for the RoBERTa variant is only +0.12% and for the DeepSeek variant is +0.18%. On the App Review dataset, the gain for the RoBERTa variant is +0.42%. These minimal improvements are likely within the standard error range and do not present a strong case for adopting this complex hybrid model over a standard LLM.\n2. While IntuitiveGraphLLM improves text representations by combining GATs with pretrained LLMs, this approach could introduce higher computational and memory overheads. Specifically, GATs require processing sparse graphs, and the computational complexity might grow significantly as the graph size increases, which can become a bottleneck for long texts or large datasets."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lXmGZ1knlC", "forum": "91jL62CQF1", "replyto": "91jL62CQF1", "signatures": ["ICLR.cc/2026/Conference/Submission4550/Reviewer_3ijq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4550/Reviewer_3ijq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761292128249, "cdate": 1761292128249, "tmdate": 1762917434503, "mdate": 1762917434503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IntuitiveGraphLLM​ is a hybrid framework that enhances Large Language Models (LLMs) by integrating ​graph-based text representations​ that are both ​structurally grounded​ and ​semantically meaningful, while traditional text graphs (e.g., based on co-occurrence or sequential order) often include noisy or irrelevant edges, which can mislead reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The core idea—pruning a structural text graph with a semantic similarity gate—is an elegant and intuitive fusion of two well-established concepts: graph-based text representation and semantic similarity metrics.\n2. The paper frames a critical limitation of LLMs—their difficulty with explicit relational reasoning—and proposes a hybrid solution that is distinct from more common approaches like retrieval-augmented generation (RAG) or chain-of-thought (CoT). It moves beyond simply using graphs as external knowledge bases and instead uses them to reconfigure the internal representation of the input text itself.\n3. The structure from Introduction to Methodology, Experiments, and Conclusion is logical and easy to follow. The problem is well-motivated, and the solution is described in a modular fashion (Construction, Encoding, Fusion)."}, "weaknesses": {"value": "1.  The paper positions Intuitive Graphs (IGs) as a novel concept, but the core idea—pruning graph edges based on semantic similarity—is an established technique in graph-based NLP. The contribution lies in the specific application to LLM fusion, but this is not sufficiently distinguished from prior art.\n2. The related work section (§2, Background) primarily contrasts IGs with simple structural graphs (BoW, sliding windows) but fails to engage with more sophisticated graph construction methods that also aim to reduce noise and incorporate semantics. This creates a strawman comparison.\n3. Lack of Rigor in Semantic Gate Design and Analysis. The choice and analysis of the semantic gate, a central component, are surprisingly heuristic and underexplored. The threshold τis set to a fixed value of 0.3 across all datasets, described as a \"conservative\" choice to \"avoid over-pruning.\" This is a major methodological shortcut. The paper acknowledges using Otsu's method and FDR to estimate dataset-specific thresholds (Appendix D) but does not utilize these estimates in the main experiments or analyze the sensitivity of performance to this critical hyperparameter. able 4 in Appendix D shows that the \"optimal\" threshold varies significantly by dataset and graph construction method (e.g., 0.4023 for BBC News with Window5, up to 1.00 for PubMedQA with Window3). Ignoring this variation suggests the reported gains might not be optimal and that the method is sensitive to an un-tuned parameter.\n4. The ablation study (Section 5.3) is limited and fails to isolate the impact of the most important component: the ​semantic gate​ itself."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Hdf6G7ZXkv", "forum": "91jL62CQF1", "replyto": "91jL62CQF1", "signatures": ["ICLR.cc/2026/Conference/Submission4550/Reviewer_k3Xy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4550/Reviewer_k3Xy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761810399525, "cdate": 1761810399525, "tmdate": 1762917434239, "mdate": 1762917434239, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes IntuitiveGraphLLM that incorporates Intuitive graphs to encode graph by reducing noise from over-connectING unrelated tokens. Comprehensive experiments further verify the effectiveness of the proposed framework."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ This paper is well-organized and easy to follow\n+ This paper conducts comprehensive experiments to verify the effectiveness of IntuitiveGraphLLM"}, "weaknesses": {"value": "- Lack a comprehensive comparison with current State-of-Art graph embedding models.(e.g. ENGINE[1], Unigraph[2], TAPE[3])\n- Lack an analysis on different graph encoder types.\n- Limited task scope. The paper only evaluates IntuitiveGraphLLM on node-level tasks, omitting an evaluation on edge-level tasks, such as link prediction.\n\n\n[1]Efficient Tuning and Inference for Large Language Models on Textual Graphs\n\n[2]UniGraph: Learning a Unified Cross-Domain Foundation Model for Text-Attributed Graphs\n\n[3]Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning"}, "questions": {"value": "- For some datasets, the improvement seems minor (less than 1%); please provide the variance across multiple runs.\n- For the LLM backbones, they are not specially designed for encoding text sequences, I am curious whether the LLM-based embedding models (e.g., NV-EmbEd-V2, Jina-Embeddings) will further improve the performance.\n- Compare IntuitiveGraphLLM with other graph embedding models.\n- As the paper states, their selection stresses domain shift, which makes it important to examine the model’s performance in a transfer setting (e.g., from PubMedQA to IMDB)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RlZaPrpe8D", "forum": "91jL62CQF1", "replyto": "91jL62CQF1", "signatures": ["ICLR.cc/2026/Conference/Submission4550/Reviewer_zVku"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4550/Reviewer_zVku"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922997310, "cdate": 1761922997310, "tmdate": 1762917433975, "mdate": 1762917433975, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new hybrid framework that fuses graph-based relational reasoning with LLM-based contextual understanding to improve semantic and logical comprehension in text-based tasks.The key idea is to prune structure-based text graphs (like windowed or sequential graphs) using a semantic gating mechanism based on cosine similarity between token embeddings, yielding sparser and more semantically meaningful graphs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive experiments.\n- The proposed semantic gating is interpretable."}, "weaknesses": {"value": "- Ambiguous conceptual novelty.\n- The conclusions are not well supported by the experimental evidence."}, "questions": {"value": "1. The semantic gating is conceptually straightforward (cosine similarity filtering), not novel. How does the proposed semantic gate differ fundamentally from prior semantic reweighting or graph pruning mechanisms?\n2. The fusion is a simple concatenation of [GAT output; LLM output]. This is a simple soft prompting strategy. Have the authors tested whether improvements persist if IG features are randomly shuffled before fusion?\n3. The authors mix encoder-only (RoBERTa) and decoder-only (Llama, DeepSeek) models under one framework. A justification for this design choice would be appreciated.\n4. In Table 3, removing the LLM leads to huge drops, but removing IG causes smaller drops — sometimes even an increase in F1. For example, in PubMedQA, w/o IG gives higher F1 (74.81 vs. 73.65). This undermines the claim that IGs are always the key driver of performance. A discussion for these observations would be appreciated.\n5. The term Intuitive Graph is rhetorically strong but methodologically vague."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "acefsmkiUh", "forum": "91jL62CQF1", "replyto": "91jL62CQF1", "signatures": ["ICLR.cc/2026/Conference/Submission4550/Reviewer_9kmx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4550/Reviewer_9kmx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4550/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980583078, "cdate": 1761980583078, "tmdate": 1762917433702, "mdate": 1762917433702, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}