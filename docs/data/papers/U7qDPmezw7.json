{"id": "U7qDPmezw7", "number": 18310, "cdate": 1758286285525, "mdate": 1763711911639, "content": {"title": "EmotionTalk: An Interactive Chinese Multimodal Emotion Dataset With Rich Annotations", "abstract": "In recent years, emotion recognition has played an increasingly crucial role in applications such as human-computer interaction, mental health monitoring, and sentiment analysis. Although a large number of sentiment analysis datasets have emerged for mainstream languages such as English, high-quality and naturally recorded multimodal dialogue datasets remain extremely scarce for Chinese, given its unique linguistic characteristics, rich cultural connotations, and complex multimodal interaction features. In this work, we propose EmotionTalk, an interactive Chinese multimodal emotion dataset with rich annotations. This dataset provides multimodal information from 19 actors participating in dyadic conversational settings, incorporating acoustic, visual, and textual modalities. It includes 23.6 hours of speech (19,250 utterances), annotations for 7 utterance-level emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral), 5-dimensional sentiment labels (negative, weakly negative, neutral, weakly positive, and positive) and 4-dimensional speech captions (speaker, speaking style, emotion and overall). The dataset is well-suited for research on unimodal and multimodal emotion recognition, missing modality challenges, and speech captioning tasks. To our knowledge, it represents the first high-quality and versatile Chinese dialogue multimodal emotion dataset, which is a valuable contribution to research on cross-cultural emotion analysis and recognition. Additionally, we conduct experiments on EmotionTalk to demonstrate the effectiveness and quality of the dataset. The EmotionTalk dataset will be made freely available for all academic purposes.", "tldr": "", "keywords": ["dialogue; speech captioning; Multimodal emotion recognition"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/a9f4283fd743fae90aa83c9a3dbbff1062f5186d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces EmotionTalk, a large-scale, high-quality Chinese multimodal emotion dataset that integrates text, audio, and video modalities. The dataset contains 23.6 hours of recordings from 19 professional actors engaged in 744 dyadic dialogues (19,250 utterances).\n\nThe authors emphasize data authenticity (improvised dialogues, not scripts), annotation rigor (five annotators + confidence weighting + negotiation rounds), and broad applicability (emotion recognition, sentiment analysis, emotion captioning).\n\nThe proposed dataset addresses the severe lack of large-scale Chinese multimodal datasets for emotion analysis.  Ensures authentic emotional expression via semi-improvised dialogue performed by trained actors.\n\nHowever, the work mainly contributes a dataset; experimental analysis (fusion and recognition models) is mostly confirmatory and relies on existing architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. High-quality and novel resource: it addresses the severe lack of large-scale Chinese multimodal datasets for emotion analysis.\n\n2. Rich and well-structured annotation scheme: it contains multi-level labeling (discrete, continuous, and descriptive captions) enables diverse tasks beyond classification.\n\n3. Comprehensive experiments and baselines: it evaluates over 20 models (speech, vision, and text) and multiple fusion methods (TFN, MISA, LMF, etc.)."}, "weaknesses": {"value": "1. Overemphasis on dataset construction, limited methodological novelty: the work mainly contributes a dataset; experimental analysis (fusion and recognition models) is mostly confirmatory and relies on existing architectures.\n\n2. Lack of comparison to other Chinese datasets: while CH-SIMS, M3ED, and MC-EIUch are mentioned, quantitative comparisons (data quality, annotation agreement, or inter-modal correlation) are missing.\n\n3. Emotion captioning evaluation lacks human judgment: The automatic metrics (BLEU, ROUGE, BERTScore) may not adequately reflect caption quality or emotional appropriateness. A small-scale human evaluation would strengthen credibility."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NMbGNySiQh", "forum": "U7qDPmezw7", "replyto": "U7qDPmezw7", "signatures": ["ICLR.cc/2026/Conference/Submission18310/Reviewer_ypS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18310/Reviewer_ypS2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781646125, "cdate": 1761781646125, "tmdate": 1762928028708, "mdate": 1762928028708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "mUfTvspXVY", "forum": "U7qDPmezw7", "replyto": "U7qDPmezw7", "signatures": ["ICLR.cc/2026/Conference/Submission18310/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18310/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763711910687, "cdate": 1763711910687, "tmdate": 1763711910687, "mdate": 1763711910687, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EmotionTalk, the first high-quality Chinese multimodal emotion dataset designed for emotion recognition research. It features 23.6 hours of dyadic conversations (19,250 utterances) from 19 actors, annotated with 7 emotion categories, 5 sentiment levels, and 4 speech caption dimensions across acoustic, visual, and textual modalities. EmotionTalk addresses the scarcity of Chinese multimodal dialogue datasets and enables studies on unimodal and multimodal emotion recognition, missing modality challenges, and speech captioning tasks. Experiments validate its effectiveness, and the dataset will be freely available for academic use."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The results of the experiment are promising."}, "weaknesses": {"value": "1. The authors emphasize the dataset's novelty by highlighting its high-quality recordings conducted by professional actors, improvisational dialogue design, and comprehensive multimodal annotations across text, audio, and video. However, the paper could further clarify how the novelty of EmotionTalk compares to existing datasets beyond general claims. For instance, while the improvisational approach is presented as a novel methodology to enhance emotional authenticity, the paper lacks quantitative or qualitative evidence proving that this approach results in superior realism compared to datasets like IEMOCAP or CH-SIMS. Moreover, the paper could benefit from a more detailed discussion of how the multimodal annotation system (e.g., the four fine-grained emotional speaking style captions) contributes uniquely to affective computing tasks compared to simpler annotation frameworks.\n2. Although the paper's main focus lies in the dataset creation and experimental validation, there is limited theoretical analysis of the underlying framework or proposed methodologies. For instance, the inclusion of fine-grained emotional speaking style captions is described as innovative, but the paper does not provide a theoretical framework or empirical analysis justifying its relevance to downstream tasks. How do these captions improve model interpretability, generalization, or performance compared to using only traditional emotion labels?\n3. The proposed weighted confidence score (Equation 1) and multi-round annotation negotiation process are interesting contributions, but the paper does not analyze how these processes impact annotation consistency, inter-rater agreement, or model accuracy."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vCmk1UTRrH", "forum": "U7qDPmezw7", "replyto": "U7qDPmezw7", "signatures": ["ICLR.cc/2026/Conference/Submission18310/Reviewer_QfPS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18310/Reviewer_QfPS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895686746, "cdate": 1761895686746, "tmdate": 1762928028276, "mdate": 1762928028276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EmotionTalk, a new Chinese multimodal emotion corpus comprising 23.6 hours of dyadic conversational data from 19 professional actors. The corpus provides annotations across three modalities (audio, video and text) and includes seven discrete emotion labels, five-dimensional sentiment intensity scores and novel 'emotional speaking style captions' across four dimensions (speaker, style, emotion and overall). The authors benchmark the corpus against unimodal and multimodal emotion recognition and captioning tasks, employing a variety of encoders and fusion methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) The use of professional actors, theme-driven improvisation and a controlled recording environment produces more naturalistic expressions than scraping TV/movies.\n2) Separate modality-specific labels, confidence scores, and Fleiss' Kappa scores demonstrate the seriousness of the annotation effort across modalities.\n3) The evaluation of encoders and fusion methods provides a useful point of reference for future works."}, "weaknesses": {"value": "1) DeepSeek-R1 generates emotional captions for speech and presents them as authoritative annotations. However, the process of human verification is unclear, as are the methods used to agree on the quality of captions, verify semantic accuracy, and research preferences. This raises a significant risk of LLM hallucinations or stylistic artifacts appearing in the corpus.\n2) Despite the severe class imbalance, the evaluation relies heavily on accuracy.\n3) Although the paper highlights cross-modal label disagreement as a key motivation, it does not analyze whether multimodal models actually outperform unimodal ones in these specific cases.\n4) The corpus comprises only 19 professional actors, who are likely to come from similar socioeconomic and cultural backgrounds. This raises concerns about its applicability to broader populations, particularly in light of the recognized cultural and individual variations in emotional expression.\n5) Although the 'emotional speaking style caption' is presented as innovative, it is not clearly differentiated from existing constructs such as paralinguistic descriptors, prosodic tags or style prompts. While the four dimensions (speaker, style, emotion and overall) are listed, they are not formally defined or validated against linguistic or psychological frameworks.\n6) The paper acknowledges that different modalities yield different labels, but does not explore whether these differences reflect genuine perceptual distinctions or artifacts of the annotation protocol.\n7) Although the paper uses DeepSeek-R1 to generate five variants of each caption, it provides no analysis of their semantic diversity, redundancy or utility. Are all five necessary? Do they improve model training? These questions remain unanswered.\n8) The paper randomly splits the data at the utterance level (8:1:1), but does not ensure that the splits are speaker-disjoint. This increases the risk of data leakage, as the same speaker appears in both the training and test sets. This can inflate performance estimates due to speaker identity cues rather than an understanding of emotion.\n9) It is surprising that the paper does not evaluate zero-shot emotion recognition using LLMs with prompting (in-context learning), given the use of LLMs for caption generation. This omission undermines the assertion that fine-tuning on EmotionTalk is essential or advantageous."}, "questions": {"value": "1) Were the signatures generated by the LLM verified by humans? If so, how was this done? If not, why are they considered a reliable form of markup?\n2) Why aren't other metrics that more accurately account for the strong class imbalance included in the experiments?\n3) Have you analyzed the performance of the model specifically using examples with cross-modal inconsistency?\n4) Why is the division into training, validation and test sets not speaker-independent? Does this not result in information leakage and an overestimation of metrics?\n5) How representative of the population are the 19 actors? Is there any data available on their age, gender or place of birth?\n6) How did you assess the 'naturalness' of improvised dialogue compared to real-life conversations? Are there any objective metrics?\n7) How did you determine that the five LLM signature variants were genuinely diverse and useful? Was an analysis of their semantic variation conducted?\n8) Why haven't the best LLMs been tested using zero-shot methods for emotion recognition without fine-tuning?\n9) Are there any examples of speech overlay?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jDdaveGRW6", "forum": "U7qDPmezw7", "replyto": "U7qDPmezw7", "signatures": ["ICLR.cc/2026/Conference/Submission18310/Reviewer_T25u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18310/Reviewer_T25u"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18310/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929654882, "cdate": 1761929654882, "tmdate": 1762928027878, "mdate": 1762928027878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}