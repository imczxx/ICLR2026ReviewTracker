{"id": "DZeic3NpHy", "number": 2175, "cdate": 1757011474876, "mdate": 1759898164747, "content": {"title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM", "abstract": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world.\nWe introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM.\nWe carefully study the design choices across model architecture and data curation.\nFor model architecture, we present three key innovations:\n(i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space;\n(ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and\n(iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. \nWe introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, \\modelname, improves over Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6$\\times$ reduction compared to Qwen2.5-Omni’s 1.2T.\nWe finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.", "tldr": "", "keywords": ["Omni-modal models", "Multimodal LLMs", "Large Language Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/23a54a9915a52fd6917353452b30da391b5bc9e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces OmniVinci, an open-source omni-modal LLM. The core contributions are twofold: (1) A new model architecture featuring three specific mechanisms (OmniAlignNet, TEG, and CRTE) to improve semantic and temporal alignment between vision and audio. (2) A novel data curation pipeline, the \"Omni-Modal Data Engine,\" designed to mitigate \"modality-specific hallucination\" by using an LLM to correct and fuse single-modality captions. The authors demonstrate that OmniVinci achieves SOTA performance, notably outperforming Qwen2.5-Omni with 6x fewer training tokens."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Novel Alignment System: The paper introduces a novel system for aligning information across text, audio, and video. The proposed architectural modules (OmniAlignNet, TEG, CRTE) are well-motivated and supported by extensive ablation studies (e.g., Table 1).\n- Innovative Data Curation: The authors build an \"Omni-Modal Data Engine\"  to address the common and difficult problem of \"modality-specific hallucination.\" They provide valuable insights (in Figure 4) into the limitations of single-modality captioning .\n- State-of-the-Art Performance: The model delivers SOTA results on numerous industry benchmarks for audio, video, and omni-modal tasks among models of similar scale (including Qwen2.5-Omni). Significant improvements are shown on key tests like DailyOmni (+19.05) and Video-MME (+3.9).\n- High Training Efficiency: The model achieves this strong performance while being highly efficient, using only 0.2T training tokens compared to Qwen2.5-Omni's 1.2T"}, "weaknesses": {"value": "- Limited Gains from RL Post-Training: The performance improvement from the GRPO reinforcement learning (RL) post-training appears relatively modest. As shown in Table 8, the score improvements on Worldsense, Dailyomni, and Omnibench are all less than 1 percentage point. Given the complexity and cost of RL training, does this minor gain suggest a bottleneck in the current method or data?"}, "questions": {"value": "- On the Combination of Loss Functions: The OmniAlignNet module introduces a contrastive loss, $L_{o-align}$ (Eq 1)20, while the LLM backbone uses a standard generative cross-entropy loss (implied in Figure 2). How are these (and potentially other) losses combined during the omni-modal joint training phase? Are they simply summed, or are they weighted (e.g., $L_{total} = \\alpha L_{LLM} + \\beta L_{o-align}$)? If weighted, how were these hyperparameters determined?\n- Regarding Modality Conflict at Inference: The data engine is designed to resolve modality-specific hallucination during training . How does the final OmniVinci model handle new, explicit modality conflicts at inference time? For example, if the model is fed a video showing a dog but the audio narration says, \"this is a cat,\" how does the model prioritize or fuse these contradictory signals?\n- Clarification on 6x Training Efficiency: The 6x reduction in training tokens (0.2T vs. 1.2T)  is a very impressive efficiency claim. To fully contextualize this: (a) Does the 0.2T token count include the computational cost of running the \"Omni-Modal Data Engine\" to generate the 24M samples? (b) What was the total computational cost (e.g., total GPU-hours) and wall-clock time required for the omni-modal joint training phase, compared to the baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "b79ODxwqcU", "forum": "DZeic3NpHy", "replyto": "DZeic3NpHy", "signatures": ["ICLR.cc/2026/Conference/Submission2175/Reviewer_uWWz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2175/Reviewer_uWWz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969351472, "cdate": 1761969351472, "tmdate": 1762916099701, "mdate": 1762916099701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces OmniVinci, an omni-modal Large Language Model (LLM) designed for comprehensive cross-modal understanding by jointly processing vision, audio, and language. The authors present notable architectural contributions, including OmniAlignNet for vision-audio alignment, Temporal Embedding Grouping (TEG) for structured temporal representation of modality tokens, and Constrained Rotary Time Embedding (CRTE) for encoding absolute temporal information. They also curated and synthesized a diverse, large-scale dataset comprising 24 million conversations spanning both single- and multi-modal scenarios. Commendably, the work includes demonstrations of practical applications in real-world settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The work proposes the innovative multi-modal integration architecture, OmniAlignNet, which aligns image and video dimensions within a unified feature space. The introduction of TEG and CRTE further enhances modality feature learning, substantially boosting the model's overall omni-modal understanding performance.\n\n- The work construct a substantial dataset of 24 million samples and implemented multi-modal reasoning augmentation. The workflow illustrat in Figure 4 provides a clear mechanism for handling modality-specific hallucinations and generating high-quality cross-modal supervision.\n\n- The paper includes a relatively comprehensive set of evaluations and ablation studies, effectively demonstrating the model's capabilities. Furthermore, the work showcases initial deployment and application potential in real-world scenarios."}, "weaknesses": {"value": "- The study employs a paradigm where single modalities (image, audio) are trained separately before a unified modal alignment is performed. The paper lacks a performance comparison of modality-specific tasks before and after the cross-modal unified alignment. Reporting the respective performances on pure Image and Audio tasks before and after this alignment stage would significantly help validate the effectiveness of the proposed method.\n\n- The image data significantly outweighs the audio data during pre-training. I wonder what the ratio of image-to-audio modality tokens is during the cross-modal alignment phase. Moreover, if the compression ratio for both the vision and audio encoders is identical during alignment,  how to eliminate potential bias resulting from the inherent token quantity imbalance between these modalities.\n\n- Some reported model results are not reflective of the current state-of-the-art. It would be beneficial to use more recent, cutting-edge model results for comparison (e.g., updating InternVL-2 to InternVL-3 and Qwen2-vl to Qwen3-vl) to ensure the novelty claims are appropriately contextualized against the strongest contemporaries."}, "questions": {"value": "same as weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dt9UBn5vJq", "forum": "DZeic3NpHy", "replyto": "DZeic3NpHy", "signatures": ["ICLR.cc/2026/Conference/Submission2175/Reviewer_xmSv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2175/Reviewer_xmSv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982161316, "cdate": 1761982161316, "tmdate": 1762916099539, "mdate": 1762916099539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a suite of modules to enhance video–audio multimodal understanding, including OmniAlignNet, Temporal Embedding Grouping (TEG), and Constrained Rotary Time Embedding (CRTE).\nOmniAlignNet is proposed to align video and audio latent representations. TEG and CRTE are designed to improve the temporal alignment of video and audio features, thereby facilitating model learning.\nAblation studies validate the effectiveness of each component. The proposed method outperforms Qwen2.5-Omni on several video and audio understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Experiments on dataset engines highlight the importance of fully exploiting cross-modal information in sounding videos, which benefits both clean data construction and model performance. These findings offer meaningful insights for future research.\n- The proposed modules improve model performance from two key perspectives—video–audio semantic alignment and temporal alignment—both of which are well-motivated and demonstrated to be effective for video and audio understanding tasks."}, "weaknesses": {"value": "- Although the authors claim the model is omni-modal, the work primarily focuses on video and audio, leading to degraded performance on the image modality. Furthermore, results for text-to-text tasks are not reported. Combined with the claim that the model uses far more tokens than Qwen2.5-Omni, it raises concerns that the proposed approach may neglect text and image modalities.\n- TEG involves interleaving video and audio tokens when feeding the LLM, which is similar to the approach used in Qwen2.5-Omni. Likewise, contrastive learning is a common practice for aligning video and audio semantic features.\n- Comparisons with Qwen2.5-Omni in terms of audio generation latency, training cost, and total parameter count are not reported."}, "questions": {"value": "Since the 24M multimodal model is part of the contributions, is there a plan to open-source it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QedgswHAuc", "forum": "DZeic3NpHy", "replyto": "DZeic3NpHy", "signatures": ["ICLR.cc/2026/Conference/Submission2175/Reviewer_Tky5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2175/Reviewer_Tky5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761983637094, "cdate": 1761983637094, "tmdate": 1762916098836, "mdate": 1762916098836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OmniVinci with three techniques including i) OmniAlignNet to align vision and audio in a video ii) Temporal embedding grouping for capturing the temporal alignment between vision and audio and iii) Constrained rotary time embedding for adding temporal information into vision-audio embeddings. The experiments show improvement on various multimodal understanding, audio understanding and vision understanding tasks over prior work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important goal of building open-source, omni-modal LLM by incorporating OmniAlignNet, temporal embedding grouping and constrained rotary time embedding. The paper curated a large-scale dataset spanning audio, video and image domains and shows improvements to multimodal understanding, audio understanding and vision understanding tasks over prior work, which would be of interest to the community. Overall, the proposed method is simple and paper is easy to read and well-written."}, "weaknesses": {"value": "* The distinction of OmniAlignNet module, use of position encoding from the current video-audio alignment common in existing work [1,2,3] is unclear. The paper lacks a discussion with these works making its positioning among them unclear.\n* Similarly there exists many studies [4,5] that have incorporated the temporal sequence in multiple ways, which the paper lacks a comparison or distinction with.\n* The paper argues the need of Omnimodal data engine and gives an example of where both audio and video are required. But as shown in many prior multimodal studies [6,7,8], there exists many datasets where one modality suffices and thus explicitly enforcing interactions is suboptimal and often leads to unnecessary correlations. The paper lacks any discussion in this aspect as well. \n* The paper highlights modality-specific training in section 3.1 by using data for each modality but it is unclear how this is incorporated in the omni-modal joint training and more details need to be provided on the separation of the modality-specific and omni-modal training. \n* The claims of the paper are not well supported by the empirical results. For example, i) While OmniVinci only improves the performance on Dailyomni in Table 3, its worse than almost all models on Omnibench with upto 10% worse than Qwen. The performance on Worldsense is also not convincing without confidence intervals. Similar conclusions hold for Image benchmarks in Table 7 and speech recognition benchmarks in Table 5, where OmniVinci obtains worse performance across baselines. \n\nOverall, in the current state I recommend recommend rejection due to the lack of discussion with prior work in multiple aspects and claims not being supported by empirical results.\n\nThe following can improve the paper further:\n* A common trend for multimodal models is the lack of temporal reasoning. It would be useful to see the performance of the proposed method on cases [9,10] which are explicitly designed to evaluate the same. \n* The font size and presentation for most results is extremely small, which makes it challenging to interpret the results meaningfully.\n* The position of Table 5 and Table 6 can be switched.\n\nReferences:   \n[1] Cheng et al. MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis.     \n[2] Kim et al. Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video.    \n[3] Guo et al. Aligned Better, Listen Better for Audio-Visual Large Language Models.  \n[4] Zerveas et al.  A transformer-based framework for multivariate time series representation learning.    \n[5] Eldele et al. TSLANet: Rethinking Transformers for Time Series Representation Learning.  \n[6] Liang et al. Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework.  \n[7] Madaan et al. Jointly Modeling Inter- & Intra-Modality Dependencies for Multi-modal Learning.   \n[8] Wang et al. An Information Criterion for Controlled Disentanglement of Multimodal Data.  \n[9] Shangguan et al. TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models.   \n[10] Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models."}, "questions": {"value": "Please refer to my comments above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lJPB4hWkn4", "forum": "DZeic3NpHy", "replyto": "DZeic3NpHy", "signatures": ["ICLR.cc/2026/Conference/Submission2175/Reviewer_hewe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2175/Reviewer_hewe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2175/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055475243, "cdate": 1762055475243, "tmdate": 1762916096218, "mdate": 1762916096218, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}