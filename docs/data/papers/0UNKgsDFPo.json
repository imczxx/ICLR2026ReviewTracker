{"id": "0UNKgsDFPo", "number": 12763, "cdate": 1758210131366, "mdate": 1763723313114, "content": {"title": "MRAG-Corrupter: Knowledge Poisoning Attacks to Multimodal Retrieval Augmented Generation", "abstract": "Multimodal retrieval-augmented generation (RAG) enhances visual reasoning in vision-language models (VLMs) by accessing external knowledge bases. However, their security vulnerabilities remain largely unexplored. In this work, we introduce MRAG-Corrupter, a novel knowledge poisoning attack on multimodal RAG systems. MRAG-Corrupter injects a few crafted image-text pairs into the knowledge database, manipulating VLMs to generate attacker-desired responses. We formalize the attack as an optimization problem and propose two cross-modal strategies, dirty-label and clean-label, based on the attacker’s knowledge and goals. Our experiments across multiple knowledge databases and VLMs show that MRAG-Corrupter outperforms existing methods, achieving up to a 98% attack success rate with only five malicious pairs injected into the InfoSeek database (481,782 pairs). We also evaluate four defense strategies—paraphrasing, duplicate removal, structure-driven mitigation, and purification—revealing their limited effects against MRAG-Corrupter. Our results highlight the effectiveness of MRAG-Corrupter, underscoring its threat to multimodal RAG systems.", "tldr": "This paper introduces MRAG-Corrupter, the first knowledge poisoning attack on multimodal RAG systems.", "keywords": ["Multimodal Large Language Models", "Retrieval Augmented Generation", "LLM security", "Knowledge Poisoning Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8c776509ba92a8a6cfe4273e1f53f1b9162e6905.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the security vulnerabilities of multimodal retrieval augmented generation. To bridge the gap on attack on multi-modal systems, the paper proposes MRAG-Corrupter, the first knowledge poisoning attack tailored for multimodal RAG. It derives two cross-modal solutions and evaluates it across multiple knowledge databases and victim VLMs. The attack method outperforms baseline methods. Various deffense strategies are also investigated."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The topic studied in this paper is important. In particular, it focuses on multimodal RAG systems, which is under-explored now.\n- The proposed MRAG-Corrupter demonstrated consistent performance improvements over exisiting methods and achieved high ASRs. A set of powerful models are taken as the victim VLMs, which makes the conclusion more generalizable.\n- Two retriever access scenarios, restricted-access and full-accsess, are considered. Attack strategies are designed for the corresponding ones.\n- Overall, the paper structure is clear and figures are nicely presented."}, "weaknesses": {"value": "- In Sec. 2 Problem Formulation and Sec. 3 MRAG-Corrupter, the authors introduced a lot of notations, it is a little bit easy to get lost to follow them. E.g., the use of \\dot over letters in Ln 96."}, "questions": {"value": "- In Ln 180, the heuristic approach directly injects the query image $\\dot{I}_i$ as $\\tilde{I}_i^j$ while keeping $\\tilde{T}_i^j$ unchanged. Why not do it reversely, i.e., inject the text while keeping image unchanged?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FlFXS3gj6R", "forum": "0UNKgsDFPo", "replyto": "0UNKgsDFPo", "signatures": ["ICLR.cc/2026/Conference/Submission12763/Reviewer_ynM3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12763/Reviewer_ynM3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761015806869, "cdate": 1761015806869, "tmdate": 1762923576500, "mdate": 1762923576500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose MRAG-Corrupter, a targeted knowledge poisoning attack on multimodal RAG systems, combining a retriever with a VLM. The attack enables an adversary to control VLM outputs for chosen queries by injecting a small number of carefully crafted image-text pairs into the knowledge base. MRAG-Corrupter combines embedding similarity optimization, surrogate-based generative text refinement, and either direct copying (dirty-label) or stealthy perturbation (clean-label) of images to create “retrieval-dominant” and “generation-dominant” poisoned pairs. These pairs are highly likely to be retrieved and used by the VLM for target queries, thus allowing an attacker to control outputs with minimal, well-crafted database modifications."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clear and easy to follow. It proposes a novel method to attack RAG systems. The authors rigorously formalize the attack as an optimization problem and introduce two cross-modal strategies tailored for varying attacker knowledge and access levels. The authors also provide rigorous experiments, evaluating over leading VLMs and adapted baselines."}, "weaknesses": {"value": "My primary concern pertains to the practical deployment of the proposed MRAG-Corrupter method. While the attack framework assumes access to a well-structured knowledge base comprising image-text pairs, real-world settings commonly employ access controls, provenance tracking, and other integrity mechanisms that may substantially reduce the risk of knowledge poisoning. Additionally, the community may be more interested in attacks targeting online, less curated, or dynamic multimodal knowledge sources rather than static databases. The experimental evaluation is limited to image-text pairs, without consideration of other relevant modalities such as tables, videos, or audio. It remains unclear how transferable the proposed attack strategies and the underlying optimization logic would be to these alternative modalities."}, "questions": {"value": "I encourage the authors to discuss the generalizability of their method and present evidence or analysis regarding its applicability to different types of multimodal knowledge bases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ih0XsBtZcw", "forum": "0UNKgsDFPo", "replyto": "0UNKgsDFPo", "signatures": ["ICLR.cc/2026/Conference/Submission12763/Reviewer_2bGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12763/Reviewer_2bGb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608457837, "cdate": 1761608457837, "tmdate": 1762923576214, "mdate": 1762923576214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MRAG-corrupter, a knowledge poisoning attack on vision-language RAG systems. It aims to manipulate the MRAG system's response to specific target queries by injecting poisoning samples into the knowledge database. The paper formulates the attack as an optimization problem and discusses to solve it in two different settings (i.e., restricted-access and full-access setting) depending on the attacker's knowledge. Extensive experiment results demonstrate that the proposed attack is both effective and robust."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "•\tThis paper have a good writing to express the key idea clearly. The figures and tables are also clear and easy to understand.\n•\tThe paper includes comparison with many baseline methods, demonstrating the effectiveness of the proposed attack.\n•\tThe paper discusses many possible defenses to demonstrate the robustness of the attack."}, "weaknesses": {"value": "•\tThe core idea of this paper is similar to an existing work [1] on corrupting vision-language RAG systems, but [1] is neither cited nor compared in this paper.\n•\tThe paper only discusses MRAG systems that retrieve images and texts. However, MRAG may not limit to images and texts. It can also support other modalities (e.g., audio [2]) and retrieval scenarios [3]. Therefore, the use of MRAG may not be appropriate here.\n•\tThe paper lacks exploration of practical real-world scenarios for the attack.\n[1] PoisonedEye: Knowledge Poisoning Attack on Retrieval-Augmented Generation based Large Vision-Language Models. ICML 2025.\n[2] WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models. ACL 2025.\n[3] UniIR: Training and Benchmarking Universal Multimodal Information Retrievers. ECCV 2024."}, "questions": {"value": "•\tWhat are the main differences between your work and existing work (e.g., [1])? Could you further compare experiment results with [1] to demonstrate the difference?\n•\tIn the clean label attack, why the optimized image still preserves semantic meaning to texts? Instead of human reviewers, is it detectable by an embedding model like CLIP?\n•\tSince the experiments only use CLIP-based models, could the authors conduct more experiments on MLLM-based retrievers like GME [4]?\n•\tFor robustness, is the attack robust to multimodal reranking [5] and defenses such as RoCLIP [6]?\n•\tCould the authors provide any experiments or examples of real-world applications of the attack, such as compromising a RAG-based LLM agent?\n[4] GME: Improving Universal Multimodal Retrieval by Multimodal LLMs. arXiv 2024.\n[5] MLLM Is a Strong Reranker: Advancing Multimodal Retrieval-augmented Generation via Knowledge-enhanced Reranking and Noise-injected Training. arXiv 2024.\n[6] Robust contrastive language-image pretraining against data poisoning and backdoor attacks. NeurIPS 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "S4lMn0T24z", "forum": "0UNKgsDFPo", "replyto": "0UNKgsDFPo", "signatures": ["ICLR.cc/2026/Conference/Submission12763/Reviewer_jEaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12763/Reviewer_jEaj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899479047, "cdate": 1761899479047, "tmdate": 1762923575956, "mdate": 1762923575956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MRAG-Corrupter, a knowledge poisoning attack framework specifically designed for Multimodal RAG systems. Multimodal RAG enhances VLMs by allowing them to retrieve information from external knowledge bases containing images and text. The authors identify a security vulnerability: by injecting a small number of maliciously crafted image-text pairs into the knowledge database, an attacker can manipulate the VLM to generate attacker-desired, incorrect answers to specific user queries. \nThe attack is formalized as an optimization problem with two necessary conditions: a Retrieval Condition (ensuring the malicious pairs are retrieved) and a Generation Condition (ensuring the VLM produces the target answer from the retrieved pairs). To address different threat scenarios, the authors propose two attack strategies: (1) Dirty-Label Attack: For a restricted-access scenario where the attacker has no access to the retriever. It directly reuses the query image and prepends the query text to a maliciously generated description. (2) Clean-Label Attack: For a full-access scenario where the attacker can query the retriever. It generates semantically aligned image-text pairs and adds subtle perturbations to the image to maximize retrieval similarity while appearing legitimate to human moderators."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ Clear Illustration\n+ Technically sound design\n+ Comprehensive evaluation"}, "weaknesses": {"value": "1. I am curious that if a knowledge base already contains an image–text pair that exactly matches a query, how can an attacker craft a new pair that the VLM retrieves and uses to produce the attacker’s desired output. Is this vulnerability primarily due to encoder weaknesses or to retrieval and ranking dynamics?\n\n2. While the paper evaluates four defense strategies, the defenses are tested in isolation, but real-world systems may employ multiple complementary defense. It would be better to discuss this aspect. \n\n3. The clean-label attack requires DALL·E-3 to generate aligned image-text pairs and assumes access to the retriever in full-access scenarios. This creates a significant barrier for real-world deployment where such access might be restricted. Moreover, the clean-label attack's effectiveness is tied to perturbation intensity ε, with higher values (ε = 32/255). Though the author claims that it remains stealthy, but it is better to showcase more concrete examples. It seems there is a practical trade-off in stealth versus effectiveness."}, "questions": {"value": "1. It is suggested to clarify the foundamental reason for such vulnerability. \n\n2. It is better to consider and discuss compound defenses. \n\n3. It is better to analyze the practical trade-off between stealth and effectiveness for clean-label attack."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8cFedagO9r", "forum": "0UNKgsDFPo", "replyto": "0UNKgsDFPo", "signatures": ["ICLR.cc/2026/Conference/Submission12763/Reviewer_sj9S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12763/Reviewer_sj9S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12763/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979040057, "cdate": 1761979040057, "tmdate": 1762923575191, "mdate": 1762923575191, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of Paper Revision"}, "comment": {"value": "# Summary of Paper Revisions\n\nWe thank all reviewers for their constructive feedback and have made the following updates:\n\n1. **Main Results** (Pages 6–7): Cited *PoisonedEye*, added it as a baseline, and included CLIP-based and prompt injection detection.  \n2. **Ablation Studies** (Page 8): Measured LPIPS differences for clean-label attack images and added experiments using the MLLM-based retriever GME.  \n3. **Defense Analysis** (Page 9): Expanded evaluation to include multimodal reranking and RoCLIP as defense strategies.  \n4. **Related Work** (Page 10): Added discussion of *PoisonedEye* as related work.  \n5. **Future Work** (Page 10): Updated future directions to emphasize exploration of other modalities and less curated datasets.  \n6. **Appendix E.3** (Page 19): Added details of the *PoisonedEye* baseline.  \n7. **Appendix E.4** (Page 19): Added detailed CLIP-based and prompt injection detection results for *PoisonedEye* and our attack.  \n8. **Appendix E.5** (Page 20): Added detailed LPIPS results for the clean-label attack.  \n9. **Appendix F.3** (Page 22): Added detailed RoCLIP defense results."}}, "id": "tDXKVxsUy6", "forum": "0UNKgsDFPo", "replyto": "0UNKgsDFPo", "signatures": ["ICLR.cc/2026/Conference/Submission12763/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12763/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission12763/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724983026, "cdate": 1763724983026, "tmdate": 1763724983026, "mdate": 1763724983026, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}