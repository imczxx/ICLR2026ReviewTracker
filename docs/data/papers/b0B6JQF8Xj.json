{"id": "b0B6JQF8Xj", "number": 14065, "cdate": 1758227867873, "mdate": 1759897392666, "content": {"title": "Chart2CSV: Can VLMs Faithfully Convert Complex Charts into Structured Tables?", "abstract": "Charts are widely used for visualizing data in research findings, and many applications require extracting data from charts and converting it into structured tables for large-scale processing and analysis. \nWhile vision-language models (VLMs) have shown promising results on chart digitization and understanding tasks, their effectiveness in fully automating this process remains unclear. Existing benchmarks fall short because (1) they contain overly simplified charts that do not reflect real-world complexity, (2) they fail to comprehensively evaluate critical model capabilities, including perception, reasoning, planning, and long-form output generation, and (3) they lack evaluations on both the completeness and accuracy of the structured outputs.\nTo systematically evaluate the performance of VLMs in extracting and structuring data from charts, we introduce Chart2CSV, a benchmark comprising 812 charts sourced from research papers across 5 scientific domains, paired with expert-validated ground-truth CSVs. In Chart2CSV, VLMs are tasked with extracting data points from these charts and converting them into CSVs. We evaluate 16 VLMs on Chart2CSV and find that even the best-performing model, Claude 3.5 Sonnet, misinterprets nearly half of the data points, underscoring the deficiency of existing VLMs in automating chart data extraction and structuring.", "tldr": "We present Chart2CSV, a benchmark designed to evaluate the ability of VLMs to accurately and comprehensively extract data points from complex charts and convert them into CSVs.", "keywords": ["VLM", "evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/479bfb7a2b1c5edc891aa9f493dcd1ac3abb8645.pdf", "supplementary_material": "/attachment/680dbba5b0997fca3703e1b21030eb7dce732c0d.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces **CHART2CSV**, a new benchmark designed to evaluate the ability of Vision-Language Models (VLMs) to faithfully convert complex charts into structured CSV tables. The authors argue that existing benchmarks are insufficient, as they use overly simplified charts and lack comprehensive metrics for both completeness and accuracy. The benchmark itself, which is the primary contribution, consists of **812 charts** (275 plots, 537 tables) sourced from real-world research papers across five scientific domains. A comprehensive evaluation of 16 VLMs on CHART2CSV revealed a significant deficiency in current models; The analysis further highlights that model performance drops significantly with increased chart complexity, especially on unconventional formats like **rotated tables**, and that advanced prompting techniques like CoT offer only limited improvements, suggesting core model deficiencies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper's core strength is its contribution of the **CHART2CSV benchmark**, a high-quality and significant dataset that is exceptionally useful. Sourced from 812 complex, real-world scientific papers, it moves beyond the oversimplified charts in prior work. The benchmark's main significance lies in its clarity in revealing the profound limitations of current state-of-the-art VLMs, which, as the paper shows, misinterpret nearly half the data points. Furthermore, the work provides a robust analysis of *why* these models fail, identifying specific, actionable weaknesses such as handling increased chart complexity and unconventional formats like rotated tables. In doing so, this paper provides the community with both a crucial diagnostic tool and a clear direction for future research aimed at improving these fundamental model capabilities."}, "weaknesses": {"value": "A significant limitation of this study is the scope of the evaluated models. The paper focuses exclusively on general-purpose VLMs (e.g., Claude 3.5 Sonnet, GPT-4o) and concludes that *all* VLMs are deficient in this area. However, this conclusion feels premature as the evaluation completely omits a critical category of specialized models designed explicitly for chart and document understanding. For instance, the comparison lacks models like **ChartLlama** (Han et al., 2023), a multimodal LLM fine-tuned for chart-specific and pure CV models (Junyu et al., 2021). Furthermore, since the task is heavily reliant on robust text and data extraction from complex layouts, the benchmark should have been tested against state-of-the-art OCR-focused models, such as **DeepSeek-OCR** (Wei et al., 2025) or the unified end-to-end model from Wei et al. (2024). Without including these specialist models, it is unclear whether the observed failures are a fundamental limitation of all current VLM architectures or simply a failure of general-purpose models that more specialized approaches may have already solved. The paper's claims would be much stronger if it included this crucial baseline comparison.\n\nReference:\n\nWei H, Sun Y, Li Y. DeepSeek-OCR: Contexts Optical Compression[J]. arXiv preprint arXiv:2510.18234, 2025. H. Wei, C. Liu, J. Chen, J. \n\nWang, L. Kong, Y. Xu, Z. Ge, L. Zhao, J. Sun, Y. Peng, et al. General ocr theory: Towards ocr-2.0 via a unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. \n\nHan Y, Zhang C, Chen X, et al. Chartllama: A multimodal llm for chart understanding and generation[J]. arXiv preprint arXiv:2311.16483, 2023. \n\nLuo J, Li Z, Wang J, et al. Chartocr: Data extraction from charts images via a deep hybrid framework[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2021: 1917-1925."}, "questions": {"value": "My primary questions concern the scope and evaluation of the benchmark. \n\nFirst, could the authors clarify why specialized chart and document models (e.g., ChartLlama, DeepSeek-OCR) were omitted from the evaluation? This exclusion makes it difficult to ascertain if the observed poor performance is a fundamental limitation of all VLMs or just a failure of the general-purpose models tested. \n\nFinally, how do the current metrics differentiate between structural errors (e.g., wrong columns) and value errors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ArzcVl21YD", "forum": "b0B6JQF8Xj", "replyto": "b0B6JQF8Xj", "signatures": ["ICLR.cc/2026/Conference/Submission14065/Reviewer_7GR9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14065/Reviewer_7GR9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761363036330, "cdate": 1761363036330, "tmdate": 1762924545889, "mdate": 1762924545889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CHART2CSV, a new benchmark designed to evaluate Vision-Language Models' (VLMs) capability to accurately extract all data points from research paper charts and convert them into structured CSV tables. The benchmark addresses the critical limitations of existing evaluations by including complex, real-world charts and requiring advanced VLM capabilities like perception, reasoning, planning, and long-form output generation. Empirical results demonstrate that state-of-the-art VLMs perform poorly on CHART2CSV, with the best model achieving only 51% accuracy, highlighting significant limitations in current models for reliable data digitization from visualizations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation for this study is clear. Although advanced visual language models (VLMs) demonstrate strong performance in general vision tasks, they continue to encounter considerable challenges in accurately extracting complete datasets from charts and organizing them into well-structured tables.\n2.  The paper is easy to read and understand."}, "weaknesses": {"value": "It is essential to introduce a new benchmark that better aligns with real-world applications. However, the Chart2CSV dataset falls short in several key areas.\n\n1. With fewer than 1,000 charts, it lacks the diversity and complexity required for robust evaluation. In contrast, datasets like VG-DCU feature a wide range of chart categories, including Area, Counter, Violin, and Sankey charts. \n\n2. Additionally, there are concerns regarding the accuracy of annotations. The reliance on manual annotation from academic papers further limits the scalability of the benchmark. Platforms such as Plotly allow for the retrieval of charts in various formats (e.g., PNG, SVG) along with raw CSV data. SVG-format charts inherently contain rich structural information, which can be leveraged to generate accurate labels using automated scripts, eliminating the need for manual annotation.\n\n3. Lastly, while the current visual language model (VLM) has been evaluated on a small benchmark, its contribution remains insufficient to meet the standards required for top-tier conferences."}, "questions": {"value": "This work presents two key areas for improvement:\n\n1. The benchmark requires further refinement, especially in terms of its size and diversity.\n\n2. The authors are encouraged to propose methods for enhancing the current visual language model (VLM) on the Chart2CSV task"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mq13rvNgRI", "forum": "b0B6JQF8Xj", "replyto": "b0B6JQF8Xj", "signatures": ["ICLR.cc/2026/Conference/Submission14065/Reviewer_MkjL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14065/Reviewer_MkjL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823353025, "cdate": 1761823353025, "tmdate": 1762924545349, "mdate": 1762924545349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CHART2CSV, a benchmark designed to evaluate vision-language models' (VLMs) ability to extract data from complex charts in research papers and convert them into structured CSV tables. The benchmark comprises 812 charts (537 tables, 275 plots) from 5 scientific domains, with expert-validated ground truth CSVs. The authors evaluate 16 VLMs using various prompting techniques and find that even the best-performing model (Claude 3.5 Sonnet) achieves only 0.51 accuracy/precision, misinterpreting nearly half of the data points. The paper argues that existing VLMs are insufficient for automating chart-to-table conversion tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Rigorous annotation process and practical importance**: The ground truth annotation involves preliminary automated extraction followed by expert review and consensus among five annotators, ensuring high-quality labels. The problem of extracting data from research charts where original data is unavailable has clear practical applications in scientific reproducibility and accessibility.\n\n2. **Comprehensive evaluation with insightful analysis**: The paper evaluates 16 VLMs across multiple prompting techniques and provides systematic analysis of performance characteristics (e.g., scaling effects, chart complexity factors, rotated tables). The findings reveal critical limitations, such as catastrophic failure on rotated tables (0% accuracy for open-source models).\n\n3. **Clear presentation with concrete examples**: The paper is well-written with effective use of examples (Figure 1) to illustrate model failure modes. The task formulation is clear and the experimental setup is transparently described."}, "weaknesses": {"value": "1. **Insufficient differentiation from CharXiv and missing evaluation of reasoning models**: The paper cites CharXiv (NeurIPS 2024) but fails to explain key differences despite both focusing on research paper charts. Table 1 marks CharXiv with \"X\" for \"Verified (Exhaustive)\" without clarification. More critically, CharXiv's online leaderboard shows reasoning models (including OpenAI o3) perform significantly better on chart understanding tasks, yet this paper completely omits reasoning-capable models (o1-series released September 2024, Gemini 2.5 Pro released June 2024) that were available before the deadline. This omission undermines the paper's central claim about VLM limitations and represents a major gap in the evaluation.\n\n2. **Weak justification for real-world charts and unsubstantiated claims**: The paper emphasizes using charts from \"real research findings\" but never articulates what specific complexity cannot be replicated with synthetic data generation (e.g., programmatic variations using matplotlib, pgfplots). The claim of \"real-world complexity\" remains vague. Additionally, the paper makes unsubstantiated claims like \"This can be attributed to the specific fine-tuning of Claude 3.5 Sonnet on complex charts\" (line 335) without evidence. Anthropic has not confirmed chart-specific fine-tuning.\n\n3. **Methodological issues and data imbalance**: The metric terminology is confusing. \"Precision\" for plots actually measures both accuracy and completeness since missing points receive penalties. No inter-annotator agreement metrics are provided despite claims of consensus-based annotation. The dataset is heavily skewed toward economic/political science (794/812 charts), with only 18 charts from psychology, finance, biology, and engineering combined, undermining cross-domain generalization claims."}, "questions": {"value": "1. **CharXiv and reasoning models**: Can you provide detailed comparison with CharXiv's verification process and explain what makes yours more \"exhaustive\"? Why were reasoning-capable models (o1, Gemini 2.5 Pro) excluded despite being available before submission? Given CharXiv's results showing strong performance from reasoning models, how might your conclusions change?\n\n2. **Real-world vs. synthetic justification**: What specific properties of real-world research charts cannot be replicated through synthetic generation with programmatic style variations? Can you provide concrete examples of complexity that synthetic approaches would fundamentally miss?\n\n3. **Methodological details**: What were inter-annotator agreement scores before consensus? Why use \"precision\" terminology when the metric includes recall-like penalties? Given the domain imbalance (97.8% economic/political science), how confident are you in cross-domain generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OEkNT2UFef", "forum": "b0B6JQF8Xj", "replyto": "b0B6JQF8Xj", "signatures": ["ICLR.cc/2026/Conference/Submission14065/Reviewer_SFjA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14065/Reviewer_SFjA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923493409, "cdate": 1761923493409, "tmdate": 1762924545014, "mdate": 1762924545014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Chart2CSV benchmark that evaluates how effectively vision-language models (VLMs) can extract and structure data from complex research charts into CSV format. The authors argue that existing benchmarks are insufficient because they use oversimplified charts that fail to comprehensively assess critical VLM capabilities like perception, reasoning, and long-form output generation. Chart2CSV comprises 812 charts from five scientific domains, each paired with an expert-validated ground-truth CSV, and evaluation reveals that even the top-performing VLM, Claude 3.5 Sonnet, misinterprets nearly half the data, underscoring significant deficiencies in current VLM capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The task of extracting structured data from chart images is a well-motivated and important problem.\n\n2. Created a benchmark from real-world data. With the recent trend of releasing so many synthetic benchmarks, this is a decent contribution. \n\n3. The annotation process is of high quality: uses CV tools for a first pass, followed by expert manual validation.\n\n4. Conducted extensive experiments to evaluate the capabilities and limitations of various LLMs. \n\n5. The dataset is also released. This helps ensure reproducibility."}, "weaknesses": {"value": "1. Lack of a Human Baseline: The paper shows that VLMs are not good at this task (around 50% accuracy). However, a human baseline would be invaluable in such complex benchmarks.\n\n2. Inter Annotator Agreement is not reported. This creates concern on the annotation process.\n\n3. Lack of in-depth discussion on what plot types are added, and what are missing. \n\n4. Missing strong models like Gemini-2.5 (pro and flash), GPT-5, etc."}, "questions": {"value": "Address the mentioned weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "alhq8nbMst", "forum": "b0B6JQF8Xj", "replyto": "b0B6JQF8Xj", "signatures": ["ICLR.cc/2026/Conference/Submission14065/Reviewer_Cs4B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14065/Reviewer_Cs4B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762183614842, "cdate": 1762183614842, "tmdate": 1762924544183, "mdate": 1762924544183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}