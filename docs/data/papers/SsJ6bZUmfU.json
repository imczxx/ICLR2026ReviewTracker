{"id": "SsJ6bZUmfU", "number": 7557, "cdate": 1758027382746, "mdate": 1759897846383, "content": {"title": "From Samples to Scenarios: A New Paradigm for Probabilistic Forecasting", "abstract": "Most state-of-the-art probabilistic time series forecasting models rely on sampling to represent future uncertainty. However, this paradigm suffers from inherent limitations, such as lacking explicit probabilities, inadequate coverage, and high computational costs. In this work, we introduce **Probabilistic Scenarios**, an alternative paradigm designed to address the limitations of sampling. It operates by directly producing a finite set of {Scenario, Probability} pairs, thus avoiding Monte Carlo-like approximation. To validate this paradigm, we propose **TimePrism**, a simple model composed of only three parallel linear layers. Surprisingly, TimePrism achieves 9 out of 10 state-of-the-art results across five benchmark datasets on two metrics. The effectiveness of our paradigm comes from a fundamental reframing of the learning objective. Instead of modeling an entire continuous probability space, the model learns to represent a set of plausible scenarios and corresponding probabilities. Our work demonstrates the potential of the Probabilistic Scenarios paradigm, opening a promising research direction in forecasting beyond sampling.", "tldr": "Introduced a sampling-free paradigm, with which a linear model achieves SOTA against complex forecasting models.", "keywords": ["Probabilistic Time Series Forecasting", "Probabilistic Scenarios", "Time Series Analysis", "Sampling-Free"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab113b9beeb3ae0b63d4655eddef5747f5f80198.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces a new paradigm for time-series forecasting based on the philosophy that predicting directly interpretable scenarios (along with their probabilities) is more effective and relevant than sampling-based methods. This is because (i) sampling-based methods don't directly provide probabilities for each scenario (ii) they are computationally costly at inference, especially when they require many samples to have a good coverage of the target distribution (iii) and they may fail to capture rare (yet critical) events. The proposed approach, TimePrism, is a follow-up of TimeMCL from Cortés et al. (2025). \n\nThe model is non-autoregressive, uses only three linear layers in their architecture (using shared weights across each dimension, and applied in parallel), making it extremely light, and showing strong CRPS and Distortion performance. Theoretical foundations of TimePrism are demonstrated, showing in particular that its non-autoregressive structure makes it easier to establish the conditional quantizer property (Proposition 1) and to capture scenario probabilities (Proposition 2) compared to TimeMCL."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The approach is well motivated for practical applications and conceptually simple. It shows a good trade-off between quality and inference computational cost, and the foundations of the approach based on Multiple Choice Learning are well discussed."}, "weaknesses": {"value": "To make sure I understood well, Tables 1 and 2 use $S = 100$ samples for each model, and $N = 625$ predictions (used for categorical sampling) for TimePrism ? It is very important to add a row in the table where TimePrism uses $N = 16$ (e.g., with $M = 4$ and $K = 4$), because otherwise it is not fair with respect to TimeMCL, which uses only $16$ scenarios (especially for Distortion comparison). I agree that the computational cost of TimePrism is smaller, but the Distortion comparison needs to include a comparison with the same number of hypotheses for completeness.\n\nAlso, in the whole paper, TempFlow and Transformers TempFlow gives the same numbers, which is odd. You need to double-check your code pipeline to verify/correct this potential bug.\n\n*Typos/issues:*\n\nL755: $R_{n}(x)$ should be formally defined.\n\nEq. (13): the left-hand side expectation should be written completely.\n\nEq. (19) / (20): You used two different notations for the cross-entropy. \n\nMore rigour is required in the introduction of the theoretical tools. In particular, the assumptions in Propositions 1 and 2 should be stated explicitly: you did not prove the convergence, but *necessary* conditions of convergence (i.e., if the model reaches a local optima, see e.g., Asm A.3 in Cortés et al., 2025, or Proposition 3.1 in Du et al., 1999).\n\nI am optimistic about the quality of the paper. I think the paper deserves acceptance, provided that the authors can show that these weaknesses and issues will be properly solved in the next revision."}, "questions": {"value": "See the questions in the weaknesses above.\nWhat are the main limitations of the method? Limitations such as the fact that you can’t handle inputs/prediction of variable length during inference, and that weight sharing may fail to model cross-channel relationships during inference, should be acknowledged in the main paper in the next revision."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jkIK1IadLA", "forum": "SsJ6bZUmfU", "replyto": "SsJ6bZUmfU", "signatures": ["ICLR.cc/2026/Conference/Submission7557/Reviewer_NW9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7557/Reviewer_NW9F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574956699, "cdate": 1761574956699, "tmdate": 1762919651807, "mdate": 1762919651807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel paradigm for modeling the probabilistic future evolution of time series by outputting a discrete set of scenario-probability pairs. To demonstrate this paradigm's efficacy, the authors propose a simple model, TimePrism. The work is positioned as a proof-of-concept that the proposed discretization approach is a viable and effective direction for probabilistic forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Simple yet Effective Paradigm: The core modeling paradigm is elegantly simple and, as demonstrated, highly effective. While discretization is a classical machine learning technique, its application to this specific problem is well-motivated. The empirical results strongly suggest this is a worthwhile research direction.\n\n2. Strong Empirical Performance: The proposed TimePrism model, serving as a proof-of-concept, achieves state-of-the-art performance on key metrics (CRPS, Distraction). This makes it a compelling new method that is effectively plug-and-play for probabilistic forecasting tasks."}, "weaknesses": {"value": "The Proof-of-Concept Gap: The primary weakness is the indirect link between the success of the TimePrism model and the proposed paradigm itself. As noted in recent literature [1, 2], simpler models can often outperform complex ones in time series forecasting. Therefore, TimePrism's strong performance could be attributed to its specific architecture rather than the paradigm shift.\nTo more directly validate the paradigm's contribution, the authors should provide a more controlled ablation. This could take one of two forms (preferably the second):\n\n1. A study showing that the TimePrism architecture, when used under a previous paradigm (e.g., predicting a continuous distribution), performs worse.\n\n2. A study adapting a selected existing method to the new discrete paradigm, demonstrating a performance lift attributable to the paradigm itself.\n\nWithout such a study, the claim of the paradigm's superiority remains suggestive rather than conclusive.\n\n[1] Zeng, Ailing, et al. ‘Are Transformers Effective for Time Series Forecasting?’ arXiv [Cs.AI], 2022, arxiv.org/abs/2205.13504. arXiv.\n[2] Lin, Shengsheng, et al. ‘SparseTSF: Modeling Long-Term Time Series Forecasting with 1k Parameters’. arXiv [Cs.LG], 2024, arxiv.org/abs/2405.00946. arXiv."}, "questions": {"value": "The authors should directly address the specific concerns raised in the \"Weaknesses\" section above. Further, there are additional questions that raise my concerns, but I would not directly count them as \"weakness\", which I put in this section:\n \n1. Dead Code Issue: The discrete scenario (\"code\") representation may suffer from a \"dead code\" problem, analogous to the issue in VQ-VAE. Since Figure 3 indicates that only the selected scenario receives gradients, a scenario may never be updated if it is never the best match for any forecast. In particular, a scenario gets update ONLY if a randomly intialized forecast exceed an already-updated scenarios. The authors should investigate and report the code usage rate across datasets. An analysis or experiment confirming that this is not a prevalent issue, or a modification to mitigate it, would strengthen the paper significantly.\n\n2. Interpretation of MSE Results: In the appendix, the MSE scores for short-term horizons (e.g., 24, 30) appear relatively low. Could the authors clarify why this is the case? Providing a baseline from a well-established forecasting model (e.g., DLinear, Koopa) for these horizons as a point of reference would help contextualize these results and furthur clarification of any potential differences in evaluation metrics is helpful.\n\n3. Connections to Discretization Literature: The core idea of discretization has clear parallels in other fields, most notably in VQ-VAE. The paper would benefit from a more thorough discussion of these connections, explicitly outlining the similarities and, more importantly, the distinctions between this work and prior art in discretized representations. This would better position the novelty of the contribution within the broader machine learning landscape."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ymyPGi3dl7", "forum": "SsJ6bZUmfU", "replyto": "SsJ6bZUmfU", "signatures": ["ICLR.cc/2026/Conference/Submission7557/Reviewer_VcU5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7557/Reviewer_VcU5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794770230, "cdate": 1761794770230, "tmdate": 1762919651515, "mdate": 1762919651515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that sampling-based probabilistic forecasting has three shortcomings—no explicit probabilities per sample, coverage gaps for tails, and high inference cost—and proposes a new paradigm that predicts a finite set of {Scenario, Probability} pairs directly in one pass. As a proof-of-concept, TimePrism (three parallel linear layers for trend/seasonality and a probability head) achieves 9/10 SOTA across five benchmarks using CRPS (probability-weighted) and Distortion (best-case coverage) as a unified evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Framing probabilistic forecasting as explicit discrete scenario prediction with calibrated probabilities is a compelling conceptual shift. While related threads exist (MCL/“winner-takes-all” heads; quantile/mixture models), combining explicit scenario probabilities with a simple linear architecture that performs competitively is surprising and potentially impactful for decision-making contexts where scenario weights matter. The evaluation mapping that compares sampling models (uniform weights) and scenario models (predicted weights) under CRPS and Distortion offers a fairer perspective on probability absence and coverage.\n\n- The model is intentionally simple: trend and seasonal linear layers produce a cartesian set of (N=M!\\times!K) scenarios, and a probability layer outputs logits (\\pi), trained with argmin-assignment + cross-entropy (a form of hard assignment akin to WTA), with reconstruction loss for the closest scenario. The training objective is clearly specified.\n\n- The critique of sampling (probabilities/coverage/cost) is accurate, and the metrics (CRPS weighted by (p_n); Distortion as min-RMSE over the set) operationalize those points. Conceptual reframing with practical benefits (explicit probabilities, one-pass inference) with a competitive accuracy with extreme simplicity, brings strong efficiency advantages.\n\n- Claims of broad efficiency gains appear well supported: TimePrism is orders faster than diffusion/flow/copula models evaluated with (S=100) samples while staying competitive on accuracy."}, "weaknesses": {"value": "I see two technical caveats: (i) probability calibration is central but the paper does not show strong calibration diagnostics (e.g., reliability diagrams of event probabilities across horizons); (ii) the discrete scenario set size (N) trades computation vs. fidelity—there is some ablation showing performance improves with (N), but more systematic experiments would help to understand it better."}, "questions": {"value": "- Is it possible to provide calibration diagnostics (e.g., PIT, coverage vs. nominal) for the predicted scenario probabilities?\n\n- Can authors explore soft assignment or entropy-regularized variants to reduce mode collapse and improve probability learning?\n\n- Is it possible to add a discussion on how to grow (N) adaptively at inference based on data complexity, and its computational trade-offs?\n\n- Are you planning to examine multivariate extensions (shared/global scenario bases vs. per-dimension) and correlation structure?\n\n- Is it possible to include decision-centric metrics (e.g., tail-risk costs) to emphasize the benefit of explicit probabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ki0cbelDTE", "forum": "SsJ6bZUmfU", "replyto": "SsJ6bZUmfU", "signatures": ["ICLR.cc/2026/Conference/Submission7557/Reviewer_Ca7z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7557/Reviewer_Ca7z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915181908, "cdate": 1761915181908, "tmdate": 1762919651142, "mdate": 1762919651142, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of probabilistic time series forecasting, challenging approaches based on sampling. Indeed, the authors argue that sampling-based methods (parametric models, generative models, flow-based models) suffer from three issues: absence of probabilities associated with sampled trajectories, failure to cover edge cases, and inference cost. As a remedy, the authors propose TimePrism, an algorithm based on the winner-takes-all training paradigm that consists in three linear layers modelling respectively, scenarios of trend, seasonality, and the associated probabilities. Empirically, the results show that the proposed algorithm performs better than the considered baselines in 5 datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and well motivated.\n- The suggested training paradigm is innovative in the field of probabilistic time series forecasting. And given that it consists of only linear layers, it opens up a new horizon of lightweight methods that can challenge the recent rise of Foundation models that are efficient in the zero shot mode."}, "weaknesses": {"value": "- My main concern with the paper is the evaluation scope which is limited to 5 datasets, some of which have already been criticized in the literature. For instance, [1] shows how many methods become equivalent when looking at a relative scale of the MSE metric for datasets such as Electricity and Exchange. Furthermore, [2] showed that even a constant no-change baseline yields a very strong baseline in the Exchange dataset. Given the simplicity of the proposed algorithm, I believe that the paper will highly benefit from a more comprehensive evaluation in other benchmarks such as Gift-eval [3] and fev-bench [4]. \n- Another concern I have is about the Coverage Inadequacy aspect, which is evaluated by the authors using the error of the best scenario (or best sample for the baselines). I agree that this metric is relevant to evaluate coverage, however, I don't think this means that your method is able to \"represent low-probability, high-impact tail event\" as stated in the introduction. This would have been true if the authors could show what is the percentage of the selected best scenarios that actually are the least probable. Maybe a histogram of the best scenario ranks by probabilities can help further elaborating this aspect.\n- Finally, for the probability absence evaluation metric, the authors assign uniform probability to all samples coming from the sampling-based models. I think that this maneuver is making these models at a disadvantaged position compared to the proposed method. For instance, for parametric models, even without access to true probabilities (as these are 0 for single data points of continuous random variables), one can use the value of the density function on samples as a proxy to their probability, then normalize so that everything sums up to 1. This is just a suggestion and depending on how samples are generated for each baseline there might be smarter ways to make this comparison a bit more fair.\n\n[1] Brigato, Lorenzo, et al. \"Position: There are no champions in long-term time series forecasting.\" _arXiv preprint arXiv:2502.14045_ (2025).\n\n[2] Beck, N., Dovern, J. & Vogl, S. Mind the naive forecast! a rigorous evaluation of forecasting models for time series with low predictability. _Appl Intell_ **55**, 395 (2025)\n\n[3] Aksu, Taha, et al. \"Gift-eval: A benchmark for general time series forecasting model evaluation.\" _arXiv preprint arXiv:2410.10393_ (2024).\n\n[4] Shchur, Oleksandr, et al. \"fev-bench: A Realistic Benchmark for Time Series Forecasting.\" _arXiv preprint arXiv:2509.26468_ (2025)."}, "questions": {"value": "- See Weaknesses.\n- I have a question that might be more related to the winner-takes-all paradigm than to your specific method. How do you make sure that the selected best scenario is different so that different weights from the last layer can get a learning signal? Also in datasets where we only observe one realization of the target (and not very similar inputs either), what makes the different scenarios diverse and prevent them from collapsing into the observed sample?\n- Can you please elaborate more on the differences between your approach and that of TimeMCL? Skimming through the TimeMCL paper, I can see that the loss formulation has a very similar construction, based on a WTA term and a cross entropy term maximizing the probability of the selected scenario, which to me seems similar if not identic to what's proposed in your paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8NC420kqVT", "forum": "SsJ6bZUmfU", "replyto": "SsJ6bZUmfU", "signatures": ["ICLR.cc/2026/Conference/Submission7557/Reviewer_zH2j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7557/Reviewer_zH2j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7557/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956510206, "cdate": 1761956510206, "tmdate": 1762919650248, "mdate": 1762919650248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}