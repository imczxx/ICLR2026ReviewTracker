{"id": "iIvPuXoDs1", "number": 9409, "cdate": 1758121462808, "mdate": 1763771622995, "content": {"title": "Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning", "abstract": "Reinforcement Learning, particularly through policy gradient methods, has played a central role in enabling reasoning capabilities of Large Language Models. However, the optimization stability of policy gradients in this setting remains understudied. As a result, existing implementations often resort to conservative hyperparameter choices to ensure stability, which requires more training samples and increases computational costs. Hence, developing models for reliably tracking the underlying optimization dynamics and leveraging them into training enables more sample-efficient regimes and further unleashes scalable post-training. We address this gap by formalizing the stochastic optimization problem of policy gradients with explicit consideration of second-order geometry. We propose a tractable computational framework that tracks and leverages curvature information during policy updates. We further employ this framework to design interventions in the optimization process through data selection. The resultant algorithm, Curvature-Aware Policy Optimization (CAPO), identifies samples that contribute to unstable updates and masks them out. Theoretically, we establish monotonic improvement guarantees under realistic assumptions. On standard math reasoning benchmarks, we empirically show that CAPO ensures stable updates under aggressive learning regimes where baselines catastrophically fail. With minimal intervention (rejecting fewer than 8% of tokens), CAPO achieves up to 30$\\times$ improvement in sample efficiency over standard GRPO for LLM reasoning.", "tldr": "", "keywords": ["Reinforcement Learning", "Policy Gradients", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/05874844ea4e5d7a65d07a064a5742f605d859fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an algorithm named CAPO which uses the second-order information of the policy objective for LLM reasoning to enhance the training stability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ This problem studied in this paper is well-motivated and important in RL training.\n\n+ The presentation is clear, straight-forward and easy to follow.\n\n+ The method proposed in the paper seems reasonable to me, and the empirical results support the approach."}, "weaknesses": {"value": "+ The main contribution of CAPO is that it incorporates two additional constraints given in Equation 10 which are based on the approximated Hessian and the Fisher matrix, to the original GRPO/PPO surrogate objective. This idea, however, has been proposed in the TRPO paper (https://arxiv.org/pdf/1502.05477, Section 6 and Appendix C). While I recognize that CAPO only estimates those second-order matrices with respect to the last softmax layer, which is a meaningful improvement compared to the original TRPO method that approximates the Hessian of all the model parameters, it would be much more convincing if it can show that CAPO performs well without other constraints such as ratio clipping and KL penalty.\n\nTherefore, I am completely neutral on this work despite my rating as there is no such an option at ICLR. \n\n\nMinor issue: I am unsure if Figure 7 is really about the comparison of CAPO and PPO as the legend says GRPO. The very difference between GRPO and PPO is not the use of ratio clipping, but whether advantages are estimated using a critic model."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "djPnspMDir", "forum": "iIvPuXoDs1", "replyto": "iIvPuXoDs1", "signatures": ["ICLR.cc/2026/Conference/Submission9409/Reviewer_Fbr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9409/Reviewer_Fbr1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761607168031, "cdate": 1761607168031, "tmdate": 1762921015999, "mdate": 1762921015999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Curvature-Aware Policy Optimization CAPO, a reinforcement learning algorithm designed to stabilize policy gradient training for reasoning-focused LLMs. The authors first propose a computationally efficient approximation of second-order geometry using a last-layer curvature model, enabling estimation of directional Hessian and Fisher Information Matrix contributions without computing full curvature. They then use these estimates to detect unstable samples and mask them during gradient computation, enforcing trust-region-like constraints. Theoretically, they prove monotonic policy improvement under bounded curvature conditions. Empirically, CAPO is applied to Qwen2.5-Math-7B, showing up to 30× improvement in sample efficiency under aggressive update regimes where GRPO and other baselines collapse."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a curvature-aware method that is theoretically grounded.\n- It has a clear trust-region interpretation and monotonic improvement guarantee.\n- Empirically, it has substantial gains in sample efficiency with minimal token masking.\n- It is very relevant for real LLM training pipelines as it can be generalized to other methods including Dr.GRPO and REINFORCE.\n\nOverall, I think this paper proposes an interesting and novel algorithm that shows potential in small scale experiments. The method is well theoretically-grounded."}, "weaknesses": {"value": "- Evaluation scale is limited to a single model family (Qwen2.5-Math-7B) and trained on a single dataset. Larger scale experiments could be  helpful to showcase the benefits of CAPO.\n- The algorithm adds complexity to already complex RL frameworks with added components and hyper-parameters."}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PjOMm60SeS", "forum": "iIvPuXoDs1", "replyto": "iIvPuXoDs1", "signatures": ["ICLR.cc/2026/Conference/Submission9409/Reviewer_1Let"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9409/Reviewer_1Let"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931366171, "cdate": 1761931366171, "tmdate": 1762921014639, "mdate": 1762921014639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a last-layer computational model, it tracks directional curvature for both the objective (Hessian) and the policy (Fisher). And uses these diagnostics to reject unstable data subsets/tokens before updating a large language model with policy gradients. The algorithm Curvature-Aware Policy Optimization (CAPO), claims monotonic improvement under local trust-region tests and reports up to $30\\times$ fewer completions to reach target accuracy on math-reasoning benchmarks. While rejecting $<8\\%$ tokens and incurring $<3\\%$ runtime overhead. Experiments are on MATH and a pooled “TEST” suite; GRPO and variants are the primary baselines, with both conservative and “aggressive” update regimes."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Targets a high-impact problem: stabilizing policy-gradient training for LLM reasoning under aggressive hyperparameters. \n2. Provides a tractable, analytic surrogate for curvature using a last-layer model, with clear formulas and pseudocode (Algorithm 1)."}, "weaknesses": {"value": "1. The monotonic-improvement guarantee uses a constant $C=\\frac{2\\gamma}{(1-\\gamma)^2}\\,\\epsilon/\\sqrt{2}$ (Eq. (13)), the paper also states that GRPO’s per-trajectory normalization “effectively assumes $\\gamma=1$.” With $\\gamma \\to 1$, $C$ blows up, making the bound vacuous precisely for the main regime evaluated. No $\\gamma=1$ reformulation or trust-region alternative is provided, and there is no calibration showing that the chosen $m_F$ thresholds enforce a true KL trust region.  \n\n2. The acceptance test relies on thresholds $(\\delta_H,\\delta_F,\\delta_H^{\\text{high}})$ and bounded step norms/curvatures ($M,r,\\epsilon$), but the paper manually tunes thresholds and never verifies that the true (not surrogate) preconditions of Theorem 5.1 hold in practice.  \n\n4. The headline “$30\\times$ fewer completions” is measured in a regime where baselines catastrophically collapse, robust efficiency claims should hold against *stable* baselines at their best hyperparameters (e.g., larger batch/smaller LR, PPO-style clipping with tuned schedules, or natural-gradient/K-FAC/TRPO-style trust regions). Such baselines and sweeps are absent."}, "questions": {"value": "1. Provide a $\\gamma=1$ analogue of Theorem 5.1 (or a trust-region argument) with finite constants, alternatively, empirically show that your $m_F$ threshold enforces a measured average $D_{\\mathrm{KL}}$ bound per update across training. \n\n2. Calibrate the surrogate $m_F(\\Delta\\psi)$ and $m_H(\\Delta\\psi)$ against the true forward KL and objective change measured on the LLM before/after updates; report correlation and calibration plots. \n3. Report sample efficiency vs. accepted tokens and vs. effective gradient norm/variance, not just vs. completions. To isolate the effect of masking.  \n4. Include sensitivity sweeps for $(\\delta_F,\\delta_H,\\delta_H^{\\text{high}})$; explain how choices interact with optimizer modeling (SGD vs. Adam) and aggressive vs. conservative regimes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PjXrcTq90a", "forum": "iIvPuXoDs1", "replyto": "iIvPuXoDs1", "signatures": ["ICLR.cc/2026/Conference/Submission9409/Reviewer_NcAd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9409/Reviewer_NcAd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9409/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762142590179, "cdate": 1762142590179, "tmdate": 1762921014317, "mdate": 1762921014317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank the reviewers for their thoughtful and constructive feedback. We are pleased that you found many aspects of our work compelling, such as:\n\n- **The work targets a relevant problem on RL training for LLM reasoning** (reviewers NcAd, 1Let, Fbr1);\n- **The contributions are theoretically well-grounded and sound, providing a tractable analytical model for approximating curvature** (reviewers NcAd, 1Let);\n- **Empirical results support the approach and demonstrate substantial gains** (reviewers 1Let, Fbr1);\n- **Presentation is clear, straighforward, and easy to follow** (reviewer Fbr1).\n\n\nWe also appreciate your questions and concerns, which we address separately in each review. As a result of the rebuttal process, we updated our initial draft (changes highlighted in blue). We specifically highlight:\n\n- New Appendix K, where we **derive the monotonic performance guarantee assuming a undiscounted, finite-horizon setting**.\n- New Appendix L, where we **relate the directional Fisher curvature estimates with the policy shifts**, presenting **correlation metrics (Table 5)** and **evidence that CAPO induces a bound on the true policy shift (Figure 9)**.\n- New Appendix M, with **further clarification questions** raised by the reviewers.\n\nWe thank you again for your feedback and hope our responses address your concerns. Please let us know if not, and we will be happy to discuss them further."}}, "id": "HxgTH0DuWc", "forum": "iIvPuXoDs1", "replyto": "iIvPuXoDs1", "signatures": ["ICLR.cc/2026/Conference/Submission9409/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9409/Authors"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission9409/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763771754094, "cdate": 1763771754094, "tmdate": 1763771754094, "mdate": 1763771754094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}