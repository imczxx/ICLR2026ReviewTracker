{"id": "vrY91av397", "number": 6120, "cdate": 1757953327560, "mdate": 1759897934501, "content": {"title": "Learning to Generate Object Interactions with Physics-Guided Video Diffusion", "abstract": "Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available", "tldr": "", "keywords": ["Video Diffusion", "Diffusion Models", "Physics", "Velocity"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0c6e6df3bc5779ea4365518b7e6397eda28596f1.pdf", "supplementary_material": "/attachment/347ff9b4016f472a95f5a9c18c2b01857fa0c737.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces KineMask, a novel method for integrating physics-guided control and realistic object interaction into Video Diffusion Models (VDMs). It addresses the key limitation of current video generation models, which often fail to produce physically plausible motion and causal object interactions. A crucial innovation is the two-stage training strategy, consisting of Low-Level Control and High-Level Conditioning. KineMask is trained on simple synthetic scenes (boxes and cylinders), and successfully generalizes to complex interactions in real-world images. It achieves strong improvements over comparable state-of-the-art models in terms of synthesizing realistic motion and object interactions."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. It is interesting that the paper constructs a synthetic dataset of dynamic scenes with simple object (boxes/cylinders) interactions using Blender, and the model trained exclusively on this simple synthetic data can successfully generalize and synthesize complex interactions in real-world scenes.\n2. KineMask successfully integrates two distinct levels of control: precise low-level kinematic control (velocity mask) and semantic high-level textual guidance (predicted outcome description). This is also interesting.\n3. The core strength is enabling the VDM to infer and generate causal physical interactions (like collisions and liquid spilling) without relying on explicit frame-by-frame guidance."}, "weaknesses": {"value": "1. Training data is limited to basic rigid body interactions. Generalization to complex non-rigid materials (e.g., fire, smoke, cloth) or articulated bodies (e.g., machines) is unproven and likely limited.\n2. The use of 3-channel (RGB) velocity encoding in a mask may be a non-intuitive control format for end-users compared to simpler input methods (e.g., a single force vector)."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HKdpMNfq3S", "forum": "vrY91av397", "replyto": "vrY91av397", "signatures": ["ICLR.cc/2026/Conference/Submission6120/Reviewer_VAx1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6120/Reviewer_VAx1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890681502, "cdate": 1761890681502, "tmdate": 1762918479111, "mdate": 1762918479111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper constructs a synthetic datasets with belender, containing the ground-truth label of object mask and motion velocity. Then it trains a controlnet to achieve drag-based image-to-video generation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper is well written."}, "weaknesses": {"value": "This paper is indeed doing drag-based subject-centric image-to-video generation. And there are many existing works in this track, from Drag-Nvwa, DragVideo to MotionI2V. However, the authors do not discuss about why this paper is claimed to be PHYSICS-GUIDED and do not compare with these existing works. This is my major concern.\n\nMeanwhile, this paper is only trained on simplified rendered data, which makes it hard to generalize to real-world data. As the aforementioned existing papers are trained on Internet data. I wonder the reason of this choice."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iDzecl73hp", "forum": "vrY91av397", "replyto": "vrY91av397", "signatures": ["ICLR.cc/2026/Conference/Submission6120/Reviewer_HgKF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6120/Reviewer_HgKF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901964870, "cdate": 1761901964870, "tmdate": 1762918478762, "mdate": 1762918478762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes KineMask, a method for enabling motion control in pretrained video diffusion models. KineMask trains ControlNet conditioned on velocity masks in two stages. In the first stage the per-frame condition signal is provided for all frames, while the second stage introduces truncation mechanism in the velocity masks to encourage learning causal motion patterns in object interaction scenarios. While the training is done on two synthetic datasets with varying motion complexity, the method successfully leverages video model's priors to generalize to real world scenes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, easy to follow, and presents the method clearly. The problem of controllable and physically plausible video generation is relevant and important. KineMask is a simple and intuitive approach that tackles this problem. The paper also studies the impact of using combined low-level (motion) and high-level (prompt) conditioning. The visual results look good."}, "weaknesses": {"value": "1. The novelty of the method is quite limited. In essense, KineMask trains ControlNet over a pretrained video diffusion model. The idea of not providing conditioning for the caused motion to encourage learning object interactions is not new (e.g. see [1, 2, 3]). The method thus appears to be an off-the-shelf composition of existing approaches and models (ControlNet, SAM2, GPT-5, Tarsier). Therefore, this hinders the significance of the contribution to the video generation research.\n\n2. The evaluation is also limited to comparisons to only text-controlled models and a single motion-conditioned competitor (Force Prompting). Adding comparisons to other baselines, including ablating different nature of motion condition would definitely strengthen the paper's claims of improved controllability.\n\n3. What drives the learning of object interactions remains largely unstudied. While the second stage of the training and the dropout of conditioning are clearly playing the crucial role in this, the evaluation of dropout ratio is limited to Table 3b in the supplemetary. In general, I would suggest to move the results in Section A3 to the main paper and shift the focus to studying design choices that influence learning of object dynamics and interactions, such as where and how the velocity masks are provided.\n\n4. The quantitative controllability evaluation is limited to synthetic data. However, the claims of emergent causality can also be tested quantitatively on real data. For instance, one could vary the velocity in a certain range and check if the magnitude of the velocity indeed correlates with the magnitude of the generated motion, e.g. measured by optical flow.\n\n5. As also acknowledged by the authors the current design of the conditioning signal is limited to describing rigid translation.\n\n[1] Blattmann, Andreas, et al. \"ipoke: Poking a still image for controlled stochastic video synthesis.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.\n\n[2] Davtyan, Aram, and Paolo Favaro. \"Learn the force we can: Enabling sparse motion control in multi-object video generation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 10. 2024.\n\n[3] Shi, Xiaoyu, et al. \"Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling.\" ACM SIGGRAPH 2024 Conference Papers. 2024.\n\n[4] Zhou, Haitao, et al. \"Trackgo: A flexible and efficient method for controllable video generation.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 10. 2025.\n\n[5] Lei, Guojun, et al. \"Animateanything: Consistent and controllable animation for video generation.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[6] Chen, Tsai-Shien, et al. \"Motion-conditioned diffusion model for controllable video synthesis.\" arXiv preprint arXiv:2304.14404 (2023)."}, "questions": {"value": "1. It is not clear why the authors opt for providing dense velocity masks rather than sparse velocity vectors. The latter would be more flexible and expressive in describing different kinds of motion. Velocity vectors were explored in prior work (e.g. [1, 2, 4, 5, 6] and the drag-based methods acknowledged by the authors in Section 2. Moreover, there exist approaches to cast sparse velocity vectors to dense masks or optical flow [3, 5, 6].\n\n2. It would be interesting to see if KineMask generalizes to unseen controls at test time. For instance, what would happen if the test velocity masks describe non-rigid or non-translation motion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FFWTM5eZAO", "forum": "vrY91av397", "replyto": "vrY91av397", "signatures": ["ICLR.cc/2026/Conference/Submission6120/Reviewer_ooPX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6120/Reviewer_ooPX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918377929, "cdate": 1761918377929, "tmdate": 1762918478313, "mdate": 1762918478313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces KineMask, a framework for guiding pre-trained video diffusion models (VDMs) to generate physically plausible object interactions from a single image. The core problem it addresses is that VDMs, despite their visual quality, lack physical grounding and controllable dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The two-stage training with mask dropout is an appropriate way to force the model to learn to infer physics rather than just copy it, enabling it to generate entire interaction sequences from a single initial condition.\n2. The combination of a low-level, precise kinematic signal (the velocity mask) with high-level, descriptive text is effective. This allows the model to generate specific, user-defined motions while leveraging the VDM's vast prior knowledge.\n3.  The generated interactions are significantly more physically plausible than those from baseline models."}, "weaknesses": {"value": "1. The control is purely kinematic (based on initial velocity). The model does not explicitly account for true physical dynamics like mass, friction, or material properties. It learns plausible visual consequences of motion rather than simulating the underlying physics. In addition, the title uses the term \"Physics-Guided,\" which might imply the use of a physics engine during inference. In reality, the physics is learned implicitly from simulator-generated data during training.\n2. The model's ability to generate interactions is shown to be dependent on being trained on a dataset containing such interactions. Its ability to produce physically novel types of interactions not seen during training is likely limited.\n3. The demos in Suppl. mostly show the translation of objects and collisions between them, with relatively simple motion types. Furthermore, some samples exhibited issues such as object disappearance and duplication. \n4. The proposed method should compare with more methods, such as the video generation model Google Veo 3, and the recent method PhysGen3D[1].\n\n[1] PhysGen3D: Crafting a Miniature Interactive World from a Single Image. CVPR 2025."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vrDi1iwT5l", "forum": "vrY91av397", "replyto": "vrY91av397", "signatures": ["ICLR.cc/2026/Conference/Submission6120/Reviewer_eW3K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6120/Reviewer_eW3K"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6120/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986434967, "cdate": 1761986434967, "tmdate": 1762918477957, "mdate": 1762918477957, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}