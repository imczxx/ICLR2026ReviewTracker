{"id": "FUp0KeEEBs", "number": 13211, "cdate": 1758215118276, "mdate": 1759897456228, "content": {"title": "Towards Understanding Valuable Preference Data for Large Language Model Alignment", "abstract": "Large language model (LLM) alignment is typically achieved through learning from human preference comparisons, making the quality of preference data critical to its success. Existing studies often pre-process raw training datasets to identify valuable preference pairs using external reward models or off-the-shelf LLMs, achieving improved overall performance but rarely examining whether individual, selected data point is genuinely beneficial. We assess data quality through individual influence on validation data using our newly proposed truncated influence function (TIF), which mitigates the over-scoring present in traditional measures and reveals that preference data quality is inherently a property of the model. In other words, a data pair that benefits one model may harm another. This leaves the need to improve the preference data selection approaches to be adapting to specific models. To this end, we introduce a set of candidate scoring functions (SFs) that are computationally simpler than TIF and positively correlated with it. These functions are also model dependent and can serve as potential indicators of individual data quality for preference data selection. Furthermore, we observe that these SFs inherently exhibit errors when compared to TIF. To this end, we combine them to offset their diverse error sources, resulting in a simple yet effective data selection rule that enables the models to achieve a more precise selection of valuable preference data. We conduct experiments across diverse alignment benchmarks and various LLM families, with results demonstrating that better alignment performance can be achieved using less data, showing the generality and robustness of our findings and new methods.", "tldr": "We assess preference data quality through our newly proposed truncated influence function (TIF), and then we propose a set of candidate scoring functions that are positive correlated with TIF to select valuable preference data.", "keywords": ["Large language model alignment", "preference data", "influence function"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03e4274f44f47d42b8a90a7ac695780eed293ac2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the data quality of preference pairs for LLM alignment.\nThe authors begin with influence function analysis of each preference pair's impact, which measures the consistency of datapoint's gradient with the overall gradient of validation set.\nThe analysis shows the medium-IF preference pairs are more valuable, so the authors design the TIF metric: $\\mathbb I[\\delta_{low} < IF < \\delta_{high}]$ to select medium-IF data for training.\nFurthermore, to reduce the computation cost of IF, the authors introduce two simplified data quality metrics:\n1. LossDiff: the difference of loss between $\\pi_\\theta$ and a validation model $\\pi_{val}$;\n2. IRM: the implict reward margin.\n\nBoth metrics correlate well with IF and can be used to approximate TIF for data selection.\nEmpirical results on different models, DPO variants, and benchmarks demonstrate the effectiveness of the proposed data selection methods in improving alignment performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear Motivation and Well-Justified Methods**: Data quality metrics based on model state serve as an intuitive and reasonable approach to improve preference optimization performance. The paper's derivation from influence function analysis to practical data selection metrics is well-motivated and clearly presented.\n2. **Stable Improvements**: The proposed data selection methods consistently enhance alignment performance across different models, DPO variants, and benchmarks, demonstrating their robustness and effectiveness.\n3. **Comprehensive Ablation**: The paper includes several ablation studies to provide further insights including model-specific selection, noisy robustness, and optimized layers.\n4. **Presentation Quality**: The paper is well-written and easy to follow, with clear explanations of the proposed methods and experimental results."}, "weaknesses": {"value": "My main concern with this work is the lack of comparison with prior model-specific data selection methods.\n\nThe authors state in L56-58:\n> Our above analysis suggests a reasonable yet seldom-discussed viewpoint: preference data selection\nshould be performed for specific models and explicitly related to the training process\n\nHowever, there are already several works discussing model-specific data selection. For example, implicit margin based selection methods [1,2,3,4] also take the model state $\\pi_\\theta$ into consideration when selecting data. \nSo I think it would be better not to claim that this is a \"seldom-discussed viewpoint\".\n\nMoreover, [3,4] utilize IRM to select data in a similar way: They prioritize preference pairs with the smallest absolute IRM value, i.e., medium-IRM data, which is similar to the TIF metric proposed in this paper.\nSo it would be better to cite and compare with these prior works.\n\n---\n\n**References**\n\n[1] Morimura, Tetsuro, et al. \"Filtered Direct Preference Optimization.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.\\\n[2] Deng, Xun, et al. \"Less is more: Improving llm alignment via preference data selection.\" arXiv preprint arXiv:2502.14560 (2025).\\\n[3] Huang, Kexin, et al. \"Larger or Smaller Reward Margins to Select Preferences for LLM Alignment?.\" Forty-second International Conference on Machine Learning. \\\n[4] Yang, Sen, et al. \"Not All Preference Pairs Are Created Equal: A Recipe for Annotation-Efficient Iterative Preference Learning.\" Findings of the Association for Computational Linguistics: EMNLP 2024. 2024."}, "questions": {"value": "Can you provide more comparison with prior model-specific data selection methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KRdhxdEHsA", "forum": "FUp0KeEEBs", "replyto": "FUp0KeEEBs", "signatures": ["ICLR.cc/2026/Conference/Submission13211/Reviewer_sQAd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13211/Reviewer_sQAd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761374169671, "cdate": 1761374169671, "tmdate": 1762923902632, "mdate": 1762923902632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel data selection method for preference data, inspired by two points:\n\n1. Preference data selection based on influence function (IF);\n\n2. DPO formulation implicitly encodes the reward difference, which can be leveraged for better estimation of IF.\n\nThrough preliminary experiments, the authors reveal an interesting phenomenon: data with medium IF values (i.e., within a truncated IF range) yields the best performance. They further propose a new data selection strategy to avoid the heavy computational cost of IF estimation, and demonstrate positive experimental results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea is novel and surprising. Traditionally, it is believed that maximizing the margin (i.e., the loss difference) for single-step optimization is most beneficial for learning. However, this paper shows that truncated medium IF actually performs best.\n\n2. The authors also carefully consider the high computational cost of IF computation in practice and propose an approximation strategy.\n\n3. In the preliminary experiment, both DPO training and validation reward margins are computed on subsets from UltraFeedback, which share similar distributions, while in the main experiments, OOD situations are also tested, and the core design of this paper works on both situations, proving its value."}, "weaknesses": {"value": "1. Although the analysis is thorough, the method itself is relatively simple, and the level of technical innovation is somewhat limited.\n\n2. The proposed LossDiff-IRM takes the intersection of two selection criteria. However, model performance also depends on data scale. If the intersection yields a very small subset, it may pose a risk of data scarcity, which should be addressed.\n\n3. Since pairwise preference data and DPO-style training are becoming less common, the strong coupling between the proposed data selection and model completions might limit its applicability to more recent paradigms such as RL-based methods. It would be helpful to include a discussion on how such approaches might extend to the prompt-level selection process."}, "questions": {"value": "How are the thresholds set? I did not find sufficient details in the paper regarding the choice or tuning of these thresholds."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D5Ae5ei8a6", "forum": "FUp0KeEEBs", "replyto": "FUp0KeEEBs", "signatures": ["ICLR.cc/2026/Conference/Submission13211/Reviewer_CD9n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13211/Reviewer_CD9n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748373687, "cdate": 1761748373687, "tmdate": 1762923902108, "mdate": 1762923902108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes preference-data quality as model-dependent and introduces a Truncated Influence Function (TIF) lens showing that medium-IF pairs—not very small or very large ones—drive the most stable alignment gains. To avoid the high cost of exact IF on LLMs, the authors propose two forward-pass proxies—Loss Difference (LossDiff) and Implicit Reward Margin (IRM)—and a combined LossDiff–IRM selector that closely tracks TIF."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Principled, model-aware data valuation.** The paper grounds selection in a TIF-motivated view of *model-dependent* data value and shows that combining **LossDiff** and **IRM** outperforms either proxy alone; moreover, pairs discarded by the selector are empirically low-value for alignment, confirming the criterion’s discriminative power.  \n\n2. **Strong empirical generality.** Across models, objectives (DPO/SLiC), and benchmarks, the method achieves higher win rates using reduced data (e.g., 64% subset with top performance)."}, "weaknesses": {"value": "1. **Incomplete positioning vs. recent work.** The paper discusses Filtered DPO, but does not engage with several highly relevant baselines:\n\n   * [1] margin-based preference selection for alignment quality (ICML 2025),\n   * [2] RS-DPO (rejection sampling + DPO for cleaner preference data),\n   * [3] active preference learning for LLMs (querying informative pairs instead of passively filtering).\n     These works target the same core problem — selecting / curating high-value preference pairs — and should be compared both conceptually and empirically.\n\n2. **Practicality.** The proposed selectors still require rescoring large volumes of pairs with both the current model and an auxiliary model. The paper does not report the real cost (GPU hours, throughput) or show that this is cheaper/more scalable than RS-DPO-style sampling or active preference acquisition.\n\n3. **Robustness.** The “medium-IF is best” claim is convincing on the reported setups but is not stress-tested across broader domains, model sizes, or stages of alignment; it is unclear how stable this curriculum is outside the presented benchmarks.\n\n[1] Larger or Smaller Reward Margins to Select Preferences for Alignment? ICML 2025.  \n[2] Rs-dpo: A hybrid rejection sampling and direct preference optimization method for alignment of large language models. NAACL 2024.  \n[3] Active preference learning for large language models. ICML 2024."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0qyKcj2SMA", "forum": "FUp0KeEEBs", "replyto": "FUp0KeEEBs", "signatures": ["ICLR.cc/2026/Conference/Submission13211/Reviewer_8ymy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13211/Reviewer_8ymy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988113590, "cdate": 1761988113590, "tmdate": 1762923901852, "mdate": 1762923901852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of identifying valuable preference data for aligning large language models (LLMs) with human preferences. The authors argue that data quality should be viewed as a model-dependent property, not intrinsic to the data itself, and further introduce the Truncated Influence Function (TIF) to evaluate the per-sample effect of preference pairs on validation performance, showing that “medium-IF” data tend to yield the best alignment results.\n\nTo make this computation feasible at scale, they propose two lightweight proxies—Loss Difference (LossDiff) and Implicit Reward Margin (IRM)—and a combined rule LossDiff–IRM for preference data selection. Experiments on various LLM models as well as alignment methods show that the proposed selection could achieve better alignment with about half of the dataset, improving winrate over full-data baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-motivated. The idea that data valuation is model-dependent is insightful and challenges a long-standing assumption in RLHF and preference optimization pipelines.\n\n\n2. The proposed Truncated Influence Function (TIF) seems new in the literature and provides a principled mechanism to quantify individual data influence. \n\n3. Results span several LLM families, datasets, and alignment methods, demonstrating generality and robustness."}, "weaknesses": {"value": "1. The “medium-IF” hypothesis is interesting, but primarily from the empirical observations. It would be better if the authors could elaborate more in a theoretical or formal way. \n\n2.  It seems a commonly used benchmark, arena-hard, is not included. Can authors elaborate more on it?\n\n3. (Mirror Issue) The presentation could also be improved. For example, the captions of Figure 1 and Figure 3 refer to the materials in the appendix. From my personal perspective, it might be better to put the related paragraphs back into the main paper."}, "questions": {"value": "Please see the weakness section.\n\nIn summary, this paper provides a well-motivated perspective on preference data valuation, supported with solid empirical evidence. I currently tend to recommend acceptance. However, I'm willing to re-evaluate this work according to the further dicusssions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ch37JeTvIC", "forum": "FUp0KeEEBs", "replyto": "FUp0KeEEBs", "signatures": ["ICLR.cc/2026/Conference/Submission13211/Reviewer_kSkJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13211/Reviewer_kSkJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13211/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762841582905, "cdate": 1762841582905, "tmdate": 1762923901491, "mdate": 1762923901491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}