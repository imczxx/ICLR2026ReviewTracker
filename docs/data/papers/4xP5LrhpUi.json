{"id": "4xP5LrhpUi", "number": 17267, "cdate": 1758274040562, "mdate": 1759897186909, "content": {"title": "GOOD: Decoding-Time Black-Box LLM Alignment", "abstract": "Large Language Models (LLMs) have demonstrated immense potential across various applications. However, aligning these models with specific real-world tasks and human preferences typically requires resource-intensive fine-tuning processes such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF). In this paper, we propose GOOD (Guided Online Optimal Decoding), a novel alignment method that enhances pre-trained models at decoding time without requiring access to their parameters or vocabularies. We observed that different aligned models exhibit similarities in their decisions of alignment-related tokens. Inspired by this, GOOD utilizes a pair of guiding models to identify critical positions related to alignment and adjusts the model’s output dynamically during the decoding phase. Notably, the interaction between the guiding models and the guided model occurs at the string level, enabling GOOD to be applied to align even black-box models with different vocabularies. Experiments show that in weak-to-strong alignment, GOOD can achieve performance comparable to direct fine-tuning in terms of comprehensive capability and harmless generation, reaching relative scores up to 102% and 99% without sacrificing decoding efficiency. Even when guiding across model families, it can recover 98% and 103% of the target performance on the two tasks, respectively. Moreover, GOOD can be applied to enhance already aligned models (improving pass@1 by 52% in code enhancement), making it compatible with various existing alignment techniques.", "tldr": "We propose a decoding-time alignment method that does not require access to model parameters or vocabulary, achieving performance comparable to fine-tuning-based alignment methods while offering faster speed than vanilla decoding.", "keywords": ["Large language models", "Alignment", "Black-Box", "Speculative Decoding"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1cac4c0fa16e650ea4bde818a3af3839ba757a49.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces $\\text{GOOD}$ , a novel methodology for achieving Large Language Model alignment during the decoding phase, designed specifically for black-box models where internal parameters are inaccessible. The core mechanism relies on the $\\text{Superficial Alignment Hypothesis}$, utilizing an accessible pair of models—one unaligned ($\\text{A}$) and its aligned counterpart ($\\text{A}_{it}$)—to identify critical positions in the target model's output stream where intervention is necessary. $\\text{GOOD}$ substitutes the black-box model's predicted token with the aligned model's output at these specific points. The technique operates at the string level, ensuring compatibility across different vocabularies, and is integrated with speculative decoding to mitigate potential latency, demonstrating strong empirical performance across safety, instruction following, and code generation tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The originality of the $\\text{GOOD}$ method is very high, presenting a unique and timely shift away from resource-intensive fine-tuning towards a flexible, decoding-time intervention policy. The concept of deriving an alignment signal from the behavioral difference between the unaligned and aligned helper models is a highly creative solution to the black-box alignment challenge. The quality is demonstrated by the robust technical integration of the method with speculative decoding, which is essential for making an online intervention method practical for deployment. The experimental results, particularly the significant performance gains observed in code generation, convincingly support the utility of the approach beyond merely stylistic alignment, suggesting successful intervention in core model capabilities. The clarity of the paper is excellent; the underlying assumption and the detailed decoding procedure are explained well, providing a clear pathway for transferring learned behaviors via external guidance. The significance of this work is substantial, as it offers a practical, third-party verifiable mechanism for enforcing alignment policies on proprietary, state-of-the-art $\\text{LLMs}$ where direct parameter access is impossible."}, "weaknesses": {"value": "The primary weakness is the stringent and potentially unrealistic technical requirement placed on the black-box API. The method critically relies on the target $\\text{LLM}$ providing real-time access to $\\text{Logits}$ or $\\text{Top-K}$ predictions at every decoding step. This non-standard access severely limits its practical deployment, as it breaks the conventional text-in, text-out protocol of leading commercial black-box models, thereby undermining the true notion of a black-box solution. Furthermore, the $\\text{Superficial Alignment Hypothesis}$ may prove too weak for tasks requiring deep, multi-step logical reasoning. If alignment necessitates a fundamental change in the model's latent feature space or complex proof structure, merely replacing a few surface tokens may not prevent the target model from reverting to its unaligned, potentially incorrect, internal state, leading to error propagation in long-form generation. Another concern is the robustness of the $\\text{cross-vocabulary}$ string substitution. While aiming to solve the vocabulary mismatch, forcing an external substring into the target model's output stream risks creating tokenization mismatches, where the target model cannot naturally resume generation, potentially leading to gibberish or out-of-vocabulary errors. Finally, the analysis of efficiency needs further depth; the reported marginal speed-up may disappear if the frequency of required alignment intervention (critical positions) is high, suggesting a potential performance bottleneck not fully explored."}, "questions": {"value": "The dependency on $\\text{Logit}$ access is a major barrier to adoption. Could the authors explore a truly $\\text{text-only}$ alternative for identifying critical positions? This might involve a high-fidelity external alignment classifier that evaluates short generated text segments from the black-box model, triggering intervention only upon detecting a misalignment signal in the generated text, thus eliminating the need for internal model probabilities. The paper should include a rigorous study to quantify the $\\text{depth of alignment}$ achievable. Please design a challenging benchmark, such as a multi-step, structured $\\text{chain-of-thought}$ task, where alignment is verified by the adherence to a specific internal logical structure rather than just the final answer or a safety phrase. This would help establish the performance boundary where $\\text{GOOD}$ begins to fail due to its token-level intervention. When using multiple guiding pairs for different alignment objectives, how is the conflict resolved when these pairs propose different tokens at the same decoding step? Please detail the proposed technical solution for $\\text{Multi-Guidance}$ conflict management. Will a learned $\\text{meta-gating}$ mechanism or a simple static priority rule be used, and how does this impact the overall computational load? Please provide a sensitivity analysis detailing the relationship between the $\\text{frequency of alignment intervention}$ (critical position rate) and the performance gain from $\\text{speculative decoding}$. This analysis is crucial to understand the practicality of the method; specifically, at what intervention frequency does the combined $\\text{GOOD}$ and speculative decoding latency exceed that of highly optimized, standard sampling methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NFjOTk1Rry", "forum": "4xP5LrhpUi", "replyto": "4xP5LrhpUi", "signatures": ["ICLR.cc/2026/Conference/Submission17267/Reviewer_uhDe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17267/Reviewer_uhDe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794039610, "cdate": 1761794039610, "tmdate": 1762927216818, "mdate": 1762927216818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GOOD, a decoding-time alignment procedure that uses a pair of guiding models (an unaligned model A and its aligned variant Aᵢₜ) to identify “alignment-critical” positions and splice in Aᵢₜ’s text while generating with a guided target model B. Crucially, B is treated as a black-box (string-level only), with cross-vocabulary transfer handled by re-tokenizing Aᵢₜ’s substrings for B’s tokenizer. The authors claim near–fine-tuning performance on MT-Bench/AlpacaEval and HH safety, plus 3–13% decoding speedups by piggybacking on speculative decoding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Problem framing & practicality. A decoding-time approach that keeps the target model B strictly black-box is timely and practical for closed-source APIs; the paper explicitly sketches an API-level integration. \n\n2. Empirical breadth. Results span MT-Bench, AlpacaEval 2.0, HH-RLHF, and HumanEval, with cross-family guidance and code-enhancement of already-aligned models.\n\n3. Speed. The method reports consistent decoding speedups versus vanilla sampling via speculative integration."}, "weaknesses": {"value": "1. “Optimal” claim not substantiated. The method name promises optimal decoding, but no optimality criterion or guarantee is provided—only heuristics (Max-Match / overlap thresholds) and empirical tuning. This overclaims theoretically.\n2. Evaluation confounds in speed. Speed tests mix different hardware across settings (L40s for one config vs A100s for others), which weakens the fairness of the reported 3–13% gains; an ablation is needed.\n3. Judge dependence & safety measurement. Safety relies on GPT-4o as evaluator; this single-judge setup risks bias and overfitting to the judge prompt. Human or multi-judge triangulation (or calibrated reward models) would strengthen claims. \n4. Limited baselines for true black-box settings. Comparisons emphasize Proxy-Tuning / reward-guided decoding that typically need logits or family-shared vocabularies. Stronger black-box baselines (e.g., prompt-programming / tool-augmented self-critique with no logits) are missing, making it hard to isolate GOOD’s advantage in the claimed setting."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4nvBB4m9q9", "forum": "4xP5LrhpUi", "replyto": "4xP5LrhpUi", "signatures": ["ICLR.cc/2026/Conference/Submission17267/Reviewer_FQxu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17267/Reviewer_FQxu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906267955, "cdate": 1761906267955, "tmdate": 1762927216412, "mdate": 1762927216412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method called GOOD (Guided Output Optimization for Decoding) aimed at improving the alignment of large language models (LLMs) during the decoding process. The authors propose a black-box approach that leverages alignment discrimination mechanisms and guidance transformation processes to enhance model outputs without requiring access to model internals."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel method for aligning LLMs at decoding time, without tuning the LLM's parameters. This approach could potentially broaden the application scope of closed-source LLMs by making them more adaptable to specific tasks without retraining these closed-source LLMs' parameters, which is also not achievable."}, "weaknesses": {"value": "1. The motivation is weak. The authors claim that previous alignment methods, especially tuning-based methods, are resource-intensive and can incur additional test-time computational costs, rendering them less economically viable. However, the proposed method introduces even heavier computational costs by incorporating two extra LLMs as the guiding pair, which does not address these concerns. Although tuning-based methods require additional computational resources during the fine-tuning phase, which only needs to be done once, they are much more computationally efficient during the lifelong deployment phase. The situation for GOOD would worsen if the LLM becomes larger, as the guiding pair models would also need to increase in size to maintain the quality of the replaced token generated from LLM A_it for B, which is not practical.\n\n2. The claim on the inference latency comparison is not fair, as it should also compare with speculative decoding instead of just vanilla decoding. The acceleration is achieved through speculative decoding, which is not the core novelty of GOOD. \n\n3. The problem setting and Figure 1 are unclear, such as the complete process from the user input prompt to the final output response. For example, there should be more information about where the grey block “...” originates from, and the final output response is not explained. How the final aligned response is made from the red “A_it n\" blocks and blue “B n\" blocks is not clear, making it difficult to interpret Figure 1 and the task.\n\n4. The theoretical foundation of the method, based on the Superficial Alignment Hypothesis, is mentioned but not thoroughly explored in the main part of the manuscript. Some key theoretical analyses could be put in the main part of the manuscript to  provide clearer support for the method's novelty and applicability."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BfI58BrbhE", "forum": "4xP5LrhpUi", "replyto": "4xP5LrhpUi", "signatures": ["ICLR.cc/2026/Conference/Submission17267/Reviewer_mpLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17267/Reviewer_mpLJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974163485, "cdate": 1761974163485, "tmdate": 1762927215519, "mdate": 1762927215519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GOOD (Guided Online Optimal Decoding), a decoding-time method to steer a black-box model B using a pair of “guiding” models: A (unaligned) and A_it (aligned). At each step, A vs A_it are compared to flag positions likely to matter for alignment; when flagged, the algorithm converts A_it’s suggested continuation to text and re-tokenizes it into B’s vocabulary, then lets B “verify” and either keep its own tokens or splice in the guided tokens based on simple acceptance rules. The method is designed to require only string-level access to B (no logits/params), and is combined with speculative decoding so the discrimination and guidance happen in parallel with generation. The paper also sketches an API demo to show how this could work with hosted models that only expose streaming text."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Practical black-box setup: GOOD is explicitly aimed at closed-model APIs and keeps the interface to string-level streaming, which is a realistic constraint; the API sketch is useful for practitioners. \n\nSimple, inspectable heuristics: The discrimination rules (max-match, top-K/top-p overlap, plus logits-based variants) are easy to implement and tune, yet already show meaningful gains over the base model. \n\nSpeculative decoding tie-in: Folding alignment guidance into a speculate-and-verify loop is a smart way to get some speed back while doing extra checks during decoding."}, "weaknesses": {"value": "Writing leaves a lot to be desired. It's hard to follow how the method itself works from Algorithm 1. It's even harder to follow the experiments. I suggest you just make it clear early in the experiments section, which models are going to play the role of B, and which ones would play the role of A, and A_it, and why you are making this choice. Also, comment on how the final benchmark scores are computed in each case (e.g. was an llm-as-a-judge used, or are these scores verifiable.)\n\nHow does the proposed GOOD algorithm impact other downstream tasks other than the measured ones? I know that MT-bench covers multiple tasks, but I was wondering if there are any cases, where the performance on other tasks would suffer as a result of GOOD. If this is the  case, is it possible to have some sort of test to check if the performance in a given task would suffer or not.\n\nline 823: grammar error: so its working principle has no fundamentally conflict to the need of protecting model confidentiality ...\n\nThere must be a typo in line 16 of Algorithm 1. I'm assuming you'd like to compare the tokens of B against those of A_it?"}, "questions": {"value": "Black-box vs. tokenizer access. The introduction says the method does not require access to B’s vocabulary. Yet Algorithm 1 is framed in token space. Do you require B’s tokenizer (or any tokenization aligned with B) to compute the mismatch index and apply span replacements, or can the algorithm operate purely at string level (character/byte spans) with the same guarantees? If the tokenizer is needed, please clarify the interface you assume for hosted APIs and whether this contradicts the black-box premise. If not needed, consider rewriting Algorithm 1 with string-level chunks (or explicitly stating a tokenizer-agnostic implementation) to avoid confusion.\n\nAPI feasibility. What parts of Appendix B run on current major APIs without changes (streaming, partial acceptance), and what requires new endpoints? Any latency overhead from round-trips? \n\nCompute accounting. Please report end-to-end cost: How do speedups scale with longer outputs? You run three models at decode time (two guides + target) yet report net speedups; more transparency on wall-clock cost, hardware, and verifier hit-rates would help."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "EIF5ePeMow", "forum": "4xP5LrhpUi", "replyto": "4xP5LrhpUi", "signatures": ["ICLR.cc/2026/Conference/Submission17267/Reviewer_xzpM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17267/Reviewer_xzpM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17267/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021267945, "cdate": 1762021267945, "tmdate": 1762927214799, "mdate": 1762927214799, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}