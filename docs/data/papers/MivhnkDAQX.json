{"id": "MivhnkDAQX", "number": 21545, "cdate": 1758318789082, "mdate": 1759896916452, "content": {"title": "Constrained Preference RLHF", "abstract": "We study offline constrained reinforcement learning from human feedback with multiple preference oracles. Motivated by applications that trade off performance with safety or fairness, we aim to maximize target population utility subject to a minimum protected group welfare constraint. From pairwise comparisons collected under a reference policy, we estimate oracle‑specific rewards via maximum likelihood and analyze how statistical uncertainty propagates through the dual program. We cast the constrained objective as a KL regularized Lagrangian whose primal optimizer is a Gibbs policy, reducing learning to a one‑dimensional convex dual problem. We propose a dual‑only algorithm that ensures high‑probability constraint satisfaction and provide finite‑sample performance guarantees for the resulting Gibbs policy. Our analysis shows how estimation error, data coverage, and constraint slack jointly affect feasibility and optimality.", "tldr": "A dual-only approach to constrained reinforcement learning from human feedback", "keywords": ["reinforcement learning from human feedback", "preference-based reinforcement learning", "reinforcement learning with constraints"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d996903f7707e7b7f854d0c18f8b33f9df65c396.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies offline constrained RL from human feedback with multiple preference oracles, motivated by balancing performance with safety/fairness.\nThe authors propose a dual-only procedure that jointly updates the policy and the Lagrange multiplier, and derive non-asymptotic guarantees.\nTheir analysis shows that dataset coverage is the key driver of both the optimality gap and the constraint violation of the learned policy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s main contribution is the first finite-sample guarantee for offline constrained RLHF with multiple reward oracles. The problem formulation is interesting. Nevertheless, the paper in its current form requires additional revision and polishing before it can be considered for acceptance."}, "weaknesses": {"value": "The main weakness of this paper lies in its lack of completeness. Specifically, it does not include thorough comparisons with prior work, presents too few experimental results (e.g., lacks comparisons with other algorithms and ablation studies over key parameters such as $T$, $B$, and $|\\mathcal{A}|$), and provides insufficiently detailed derivations in the proofs.\n\n- The motivation and real-world use case are unclear. Please clarify a concrete scenario in which your setting is warranted. In RLHF, even with multiple groups, it is common to train a single reward model; training separate group-specific rewards appears naive without stronger justification.\n\n- The work presents a hard-constraint formulation, but Theorem 2 and the experiments appear to allow some violations. It may be clearer to reframe the problem as controlling/penalizing violations instead of enforcing a strict hard constraint.\n\n- In Theorem 2, the upper bound on the constraint violation does not vanish even when $J_\\min = 0$. This makes the theoretical guarantee rather weak, as one would expect zero violation in this case.\n\n- The upper bounds in Theorem 2 depend on the projection radius parameter $R$. When  $R=0$, the bounds appear to become very small. Is this interpretation correct? Intuitively, it seems that we should incur some penalty when $R$ is small.\n\n- The claim that the statistical error is $O(\\sqrt{d/N})$ is overstated. Because $\\beta_N \\approx \\sqrt{\\lambda_{red}B^2} $ for large $N$, the bound need not decay as $N^{-1/2}$.\n\n- The claim that the initial constraint violation increases with larger $w$ is incorrect. In the experiments, when $w = 0.3$, the violation is approximately $0.4$, whereas for $w = 0.9$, it is around $0.2$."}, "questions": {"value": "- What is the motivation for using Gibbs policy updates instead of a primal update? Please clarify why a primal step is not necessary in your setting.\n\n- In Figure 1, the “violation” appears to go below zero. How is this possible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lczDNYlsTa", "forum": "MivhnkDAQX", "replyto": "MivhnkDAQX", "signatures": ["ICLR.cc/2026/Conference/Submission21545/Reviewer_Afcc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21545/Reviewer_Afcc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761217465650, "cdate": 1761217465650, "tmdate": 1762941829249, "mdate": 1762941829249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies offline constrained reinforcement learning from human feedback (RLHF) with multiple preference oracles. The goal is to maximize a target group’s utility while guaranteeing a minimum welfare level for a protected group. The authors formulate a KL-regularized constrained optimization problem with a dual structure and derive a closed-form Gibbs (Boltzmann) form for the optimal policy, which reduces learning to a one-dimensional convex dual problem. They propose a dual-only projected-gradient method and provide theoretical guarantees for high-probability constraint satisfaction and a finite-sample optimality gap. Numerical experiments are presented to support the theory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper introduces a novel formulation named constrained RLHF that combines ideas from constrained RL and RLHF in a coherent way.\n\n2.The writing is clear and the notation/definitions are easy to follow.\n\n3.The theoretical results are strong. The assumptions (linear rewards, Slater’s condition, bounded features) are standard and well-justified, and the paper provides a non-asymptotic finite-sample bound."}, "weaknesses": {"value": "1.The experimental evaluation is limited. Only Gaussian-feature simulations are reported; there are no tests on realistic human-feedback datasets or environments, and no comparisons against existing methods.\n\n2.The algorithmic novelty is modest. The “dual-only” approach is essentially projected gradient descent applied to a known dual structure.\n\n3.The linear-reward and full-support assumptions may limit applicability to large-scale RLHF scenarios where preference models are neural and coverage is incomplete."}, "questions": {"value": "1.How does your dual analysis differ from standard CMDP treatments? Does the KL term specifically enable the reduction to a 1-D dual variable, or would a similar reduction hold without KL regularization?\n\n2.There are related works on “Safe RLHF” and similar formulations. Could you compare problem setups and guarantees (assumptions, types of constraints, and sample-complexity/regret guarantees) more explicitly?\n\n3.Is it feasible to add comparisons with CMDP and RLHF baselines on more complex benchmarks (e.g., preference datasets or simulated environments with partial coverage)? If space is tight, pointers to an appendix with additional experiments would help.\n\n\nWhile the formulation is new, it appears to directly combine RLHF with CMDP machinery, and the analysis seems to lean heavily on existing results. Could you clarify the specific theoretical innovations beyond recombining known components? In addition, the experimental section is quite limited."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "skhFFWaWRe", "forum": "MivhnkDAQX", "replyto": "MivhnkDAQX", "signatures": ["ICLR.cc/2026/Conference/Submission21545/Reviewer_cmqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21545/Reviewer_cmqF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877947587, "cdate": 1761877947587, "tmdate": 1762941829002, "mdate": 1762941829002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the problem of constrained RLHF. The model of RLHF considered is the standard Bradley-Terry model, and the reward function $r\\_1$ in linear in some feature vector $\\theta$. The paper extends standard RLHF to the setting where there is a second reward function $r\\_2$, and the policy is constrained to obtain some minimum reward level with respect to $r\\_2$, while maximizing the standard rewards $r\\_1$. Both these reward functions must be initially learned from preference data, after which the constrained optimization must be solved. The paper proposes an algorithm that leverages closed-form optimization of the primal policy in order to iteratively solve the dual problem."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Fairness in RLHF is an important topic, and the algorithm that this paper develops seems sound for the case of a single additional reward constraint."}, "weaknesses": {"value": "This paper studies a very limited model of constrained RLHF, and simply combines known results regarding reward estimation with standard constrained RL methods. In more detail, this paper studies a single additional reward constraint that is supposed to represent the utility of some protected population. However, a large body of work in fair RLHF already studies the more complex and realistic setting of multiple protected populations. For a few examples of such research see [1,2,3]. The setting of multiple protected populations is much more interesting both because there clearly will be multiple such population in practice, and because it requires more nuanced algorithms that have to make a choice about how to satisfy heterogeneous, possibly competing constraints. This submission does not cite any of this highly-relevant work on fairness in RLHF, and does not really compare favorably to the current body of work in the field, as described above.\n\nTo summarize, the model studied is quite limited compared to already existing theoretical work on fairness in RLHF, and the main results are straightforward applications of known concentration bounds, and constrained RL algorithms.\n\n\n[1] Chakraborty, Souradip, et al. \"MaxMin-RLHF: alignment with diverse human preferences.\" ICML 2024.\n\n[2] Ramesh, Shyam Sundhar, et al. \"Group robust preference optimization in reward-free rlhf.\" NeurIPS 2024.\n\n[3] Ge, Luise, et al. \"Axioms for ai alignment from human feedback.\" NeurIPS 2024."}, "questions": {"value": "1. In what RLHF setting would there only be a single constraint corresponding to one protected population?\n\n2. Is there any sense in which your algorithm is not a special case of existing multi-group RLHF methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sSAjnnZehS", "forum": "MivhnkDAQX", "replyto": "MivhnkDAQX", "signatures": ["ICLR.cc/2026/Conference/Submission21545/Reviewer_u4Ty"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21545/Reviewer_u4Ty"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023138395, "cdate": 1762023138395, "tmdate": 1762941828710, "mdate": 1762941828710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a linear, convex constrained optimization framework for reinforcement learning from human feedback (RLHF) using binary preference data. The authors study offline constrained RLHF where two independent preference oracles—a target population and a protected group—provide pairwise comparison feedback. The goal is to maximize the expected utility of the target oracle while ensuring the protected group’s welfare stays above a specified threshold, formulated as a convex problem with KL regularization and a single expectation constraint.\n\nEach reward function is linear in features and estimated from binary pairwise preferences through a Bradley–Terry logistic model. The resulting problem admits a one-dimensional strongly convex dual formulation in the Lagrange multiplier. The authors propose a dual-only projected gradient descent algorithm that leverages a closed-form Gibbs policy and derive finite-sample guarantees separating statistical estimation error $O(\\sqrt{d/N})$ and optimization error $O(1/\\sqrt{T})$.\n\nTheoretical results show high-probability bounds on suboptimality and constraint violation, ensuring the learned policy is nearly optimal and feasible. Simulations in a synthetic linear feature environment support the analysis, showing that both primal suboptimality and constraint violation decay with data size. The work offers a formal theoretical treatment of constrained RLHF integrating preference-based reward estimation and constraint satisfaction, though it remains limited to a linear and convex setting rather than large-scale empirical validation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **KL-Regularized Dual Formulation of Constrained RLHF**\n    \n    The paper establishes a clear convex dual structure for KL-regularized constrained RLHF.\n    \n    By integrating the objective and constraint into a unified Lagrangian and proving strong duality under Slater’s condition, the authors reduce the primal problem to a one-dimensional convex dual optimization.\n    \n    This formulation enables a provably convergent dual-only projected gradient descent algorithm and provides a more principled convex foundation for constrained RLHF compared to prior heuristic approaches.\n    \n2. **Propagation Analysis of Reward Estimation Uncertainty**\n    \n    Lemma 2–3 rigorously quantify how reward estimation errors from Bradley–Terry MLE propagate into the dual function and its gradient.\n    \n    The authors derive explicit high-probability bounds on $| \\hat{g}(\\lambda)-g(\\lambda) |$ and $| \\hat{g}'(\\lambda)-g'(\\lambda) |$, linking statistical uncertainty to constraint satisfaction and dual stability.\n    \n    This represents a novel theoretical insight that formalizes the coupling between reward estimation accuracy and feasibility in constrained RLHF.\n    \n3. **Finite-Sample Guarantees for Optimality and Feasibility**\n    \n    Theorem 2 provides a unified finite-sample analysis that decomposes the total error into statistical and optimization components:\n    \n    $O(\\sqrt{d/N})$ for reward estimation and $O(1/\\sqrt{T})$ for dual optimization.\n    \n    The result delivers explicit high-probability guarantees for dual suboptimality, constraint violation, and primal optimality gap.\n    \n    This highlights the fundamental trade-off among data coverage, iteration budget, and constraint slack in constrained RLHF.\n    \n\n---\n\nIn summary, the theoretical development is distinctive for (1) framing KL-regularized constrained RLHF as a convex dual problem, (2) explicitly characterizing uncertainty propagation from reward estimation to dual errors, and (3) deriving the first finite-sample optimality and feasibility bounds that quantify key bottlenecks in constrained RLHF."}, "weaknesses": {"value": "- While the paper claims to present “the first formal treatment of constrained RLHF with multiple reward oracles,” this claim seems overstated. In practice, the formulation does not extend to multiple independent oracles but instead focuses on a two-oracle structure (one target and one constraint oracle). This setup is conceptually similar to that of *Safe RLHF: Safe Reinforcement Learning from Human Feedback* (ICLR 2024), which also maximizes a reward signal under a secondary constraint derived from human feedback. The contribution of this work therefore lies primarily in its convex-theoretic formalization and finite-sample guarantees, rather than in introducing a genuinely new multi-oracle framework.\n- The theoretical framework relies on highly restrictive assumptions—linear reward models, full policy coverage, and the existence of a strictly feasible policy under Slater’s condition. While these assumptions make the analysis elegant, they significantly limit the generality and applicability of the results to realistic RLHF settings, such as large-scale language models with nonlinear reward functions or partial coverage data.\n- Although the paper offers a clean convex and dual analysis, its conceptual novelty remains limited. The theoretical results formalize expected convex behaviors (e.g., strong duality, Lipschitz continuity, finite-sample convergence) but do not reveal new mechanisms or interactions unique to constrained RLHF. As such, the work feels more like a rigorous theoretical consolidation of existing ideas than a conceptual leap forward.\n- The paper does not deeply engage with the practical bottlenecks of applying constrained RLHF, such as instability in dual variable updates, reward–constraint coupling, or data imbalance across preference sources. These issues are mentioned but not theoretically explored, leaving a gap between the presented analysis and the empirical challenges of real-world constrained RLHF."}, "questions": {"value": "- The paper refers to “multiple reward oracles” as a defining aspect of its contribution, but the formulation effectively uses only two oracles—one for the target population and one for the protected group. Could the authors elaborate on how the proposed framework would generalize to *more than two* oracles? For example, if several stakeholder groups with distinct preference distributions were involved, would the dual formulation still remain tractable, or would multiple dual variables and constraints be required? A clarification or theoretical sketch of how the method scales to true multi-oracle settings would greatly strengthen the paper’s claim.\n- Given that the current formulation assumes linear reward models and full coverage, it would be helpful to understand how the algorithm behaves when these assumptions are relaxed. Have the authors considered testing a relaxed version of the algorithm—e.g., with approximate coverage or nonlinear (e.g., neural) reward estimators—in more practical RLHF settings such as text-generation or summarization tasks? Even small-scale experiments on realistic datasets could offer valuable insights into whether the theoretical guarantees translate into meaningful safety or fairness effects in practice.\n- Relatedly, could the authors discuss the computational or stability challenges that might arise when extending the dual update or Gibbs policy to more complex preference models or partially observed constraints? It would be interesting to know whether the proposed approach can maintain convergence or constraint satisfaction guarantees under such conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "jsr1GUfLp6", "forum": "MivhnkDAQX", "replyto": "MivhnkDAQX", "signatures": ["ICLR.cc/2026/Conference/Submission21545/Reviewer_AtsF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21545/Reviewer_AtsF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21545/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762073858287, "cdate": 1762073858287, "tmdate": 1762941828289, "mdate": 1762941828289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}