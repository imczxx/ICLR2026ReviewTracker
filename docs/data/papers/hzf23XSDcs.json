{"id": "hzf23XSDcs", "number": 20777, "cdate": 1758309956723, "mdate": 1759896959070, "content": {"title": "CitySeeker: How Do VLMs Explore Embodied Urban Navigation with Implicit Human Needs?", "abstract": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., ''I am thirsty'') in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs’ spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies—Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling ''last-mile'' navigation challenges.", "tldr": "", "keywords": ["Embodied Urban Navigation", "Vision-Language Models", "Urban Intelligence", "Spatial Cognition"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c807ab02a0d05a02fefcb7f0f71f2772a8162740.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces *CitySeeker*, a novel benchmark for evaluating Vision-Language Models (VLMs) in embodied urban navigation driven by implicit human needs. Unlike prior VLN benchmarks that rely on explicit instructions, *CitySeeker* focuses on abstract, functional, and semantic goals (e.g., “I’m thirsty”), spanning 6,440 trajectories across 8 cities. The authors propose a framework for evaluating spatial reasoning and decision-making and identify key bottlenecks in current VLMs. They further introduce three human-inspired strategies—Backtracking, Spatial Cognition Enrichment, and Memory-Based Retrieval (BCR)—to enhance navigation performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- **Benchmark Contribution**: *CitySeeker* fills a critical gap in VLN research by targeting implicit human needs in dynamic, real-world urban environments. It is the first large-scale benchmark to do so across multiple cities and diverse task categories. \n- **Realism and Diversity**:  The benchmark includes diverse urban layouts and visual characteristics, making it highly relevant for embodied AI applications. \n-  Comprehensive Evaluation： \n- **Comprehensive Evaluation**: The paper presents extensive empirical results across 27 VLMs, including proprietary and open-source models. The analysis spans task categories, cities, and trajectory patterns. The authors also  proposed BCR strategies are well-motivated and show measurable improvements in task completion and path efficiency.\n- **Clarity and Presentation**:  The paper is generally well-written, with a clear logical flow and a well-motivated problem statement."}, "weaknesses": {"value": "1.  **Problem Setup**:  \n\n    - The task formulation leans more toward object navigation than traditional VLN. In partially observable environments, defining the agent’s state solely as the current observation $o_t$ is insufficient.\n    - A more appropriate formulation would involve belief state estimation using historical observations $\\{o_0, o_1,...,o_t\\}$, which is a regular practice in POMDP-based problem.\n\n2. **Symbol and Notation Clarity**:\n\n    - The definition of $v$ as a graph node and $v_t$ as the location is inconsistent.\n    - Subscripts such as $o_i$ (different views at a single node) and $o_t$ (timesteps) are used ambiguously.\n    - Lacks a clear explanation of how reasoning $\\phi$ and action $a$ are computed -- what inputs are used ($O_t$ or $S_t$ or something else?).\n\n4. **Evaluation Metrics Misalignment**:\n\n    - Metrics like **nTCE**, which measure trajectory deviation from ground truth, are more suitable for instruction-following tasks (in previous VLN tasks, the agents are asked to follow the excact GT trajectory given a detailed user instruction).\n    - In implicit-need-driven navigation, multiple valid trajectories may exist. Metrics like SPL (Success weighted by Path Length) or goal proximity might be more appropriate.\n\n\n5. **Low Task Completion Rates**:\n\n    -  Even the best-performing models and human baselines achieve low success rates under strict metrics (e.g., 5.7% TCE for humans), raising concerns about benchmark difficulty or metric suitability."}, "questions": {"value": "**Questions**: See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1fI8TZbHKs", "forum": "hzf23XSDcs", "replyto": "hzf23XSDcs", "signatures": ["ICLR.cc/2026/Conference/Submission20777/Reviewer_4hXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20777/Reviewer_4hXJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760934580557, "cdate": 1760934580557, "tmdate": 1762999985856, "mdate": 1762999985856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "As the ability of vision-language models (VLMs) to interpret *implicit human needs* in dynamic urban environments remains underexplored, this paper proposes CitySeeker, a novel benchmark designed to evaluate VLMs’ spatial reasoning and decision-making capabilities in *embodied urban navigation* tasks that involve implicit objectives. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs within 7 goal-driven scenarios. Extensive experiments demonstrate that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion, revealing fundamental weaknesses in long-horizon reasoning, spatial cognition, and experiential recall. To further diagnose these issues, the authors explore a set of human-inspired strategies—Backtracking Mechanisms, Enriched Spatial Cognition, and Memory-Based Retrieval (BCR)—reflecting iterative observation-reasoning cycles and adaptive path optimization in human navigation. The analysis provides valuable insights into developing VLMs with more robust *spatial intelligence* for tackling “last-mile” navigation challenges."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is well-grounded. Identifying that VLMs’ ability to interpret implicit human needs in dynamic urban environments remains underexplored is both timely and significant. It provides a new angle for examining VLMs’ world knowledge and decision-making capabilities.\n\n2. The paper is clearly written and visually appealing. The figures effectively illustrate the framework and experiments, helping readers understand the design and reasoning process.\n\n3. The authors conduct extensive experiments involving 27 different VLMs, and their findings are deep and thought-provoking, revealing critical gaps in current models’ embodied spatial reasoning."}, "weaknesses": {"value": "1. Overall, this is a clear accept-level paper in terms of novelty, clarity, and experimental depth. However, there is a **serious ethics concern**. The paper states:\n   *“CitySeeker dataset was sourced from publicly available APIs (Google Maps and Baidu Maps) and is used in accordance with their terms of service for non-commercial research purposes only.”*\n   After reviewing Google Maps’ Terms of Service[1], it explicitly states:\n   **“Downloading Street View images to use separately from Google services (such as an offline copy) is prohibited. These restrictions apply to all academic, nonprofit, and commercial projects.”**\n   This implies that **the dataset collection may violate Google’s ToS**, introducing a significant **ethical and legal issue**. Consequently, the dataset **cannot be publicly released**, which severely limits the reproducibility and extensibility of this research. This is a **veto-level weakness** for a top-tier venue.\n\n[1] [Brand Resource Center | Products and Services - Geo Guidelines](https://about.google/brand-resource-center/products-and-services/geo-guidelines/)"}, "questions": {"value": "1. Please clarify the ethical compliance issue.\n\n2. The paper claims that these navigation tasks are highly challenging even for humans (e.g., “humans achieve only 5.7% accuracy”). However, tasks such as finding a restaurant or locating a place with Wi-Fi do not seem inherently difficult for human participants. The paper does not adequately explain why humans perform so poorly, or how the tasks are defined, constrained, or evaluated. This undermines the interpretability and credibility of the benchmark’s human baseline."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper states:\n   *“CitySeeker dataset was sourced from publicly available APIs (Google Maps and Baidu Maps) and is used in accordance with their terms of service for non-commercial research purposes only.”*\n   After reviewing Google Maps’ Terms of Service[1], it explicitly states:\n   **“Downloading Street View images to use separately from Google services (such as an offline copy) is prohibited. These restrictions apply to all academic, nonprofit, and commercial projects.”**\n   This implies that **the dataset collection may violate Google’s ToS**, introducing a significant **ethical and legal issue**. Consequently, the dataset **cannot be publicly released**, which severely limits the reproducibility and extensibility of this research. This is a **veto-level weakness** for a top-tier venue.\n\n[1] [Brand Resource Center | Products and Services - Geo Guidelines](https://about.google/brand-resource-center/products-and-services/geo-guidelines/)"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZkLfuYJnl2", "forum": "hzf23XSDcs", "replyto": "hzf23XSDcs", "signatures": ["ICLR.cc/2026/Conference/Submission20777/Reviewer_1jEc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20777/Reviewer_1jEc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931789423, "cdate": 1761931789423, "tmdate": 1762999986509, "mdate": 1762999986509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CitySeeker, a framework that leverages VLMs to understand and predict human urban mobility behaviors based on natural-language queries. The system maps user needs to POI categories using multimodal embeddings and evaluates performance on multiple cities. The goal is to test VLMs’ ability to align semantic and spatial reasoning in real-world city environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Addresses an interesting interdisciplinary question.\n\nThe dataset construction across multiple major cities, combining geospatial and textual information.\n\nThe evaluation is systematic, covering both semantic matching and spatial reasoning tasks.\n\nThe idea of connecting natural language intent to spatial decision-making is novel and potentially impactful."}, "weaknesses": {"value": "The mapping from need to POI type assumes a fixed, deterministic relationship, which may not hold in practice, human intent is subjective and context-dependent.\n\nThe model implicitly assumes people choose the shortest path or most direct POI option, which is unrealistic, behavioral factors like preference, familiarity, and accessibility play major roles.\n\nCross-cultural generalization is a concern: the same “need” may imply different POIs across societies.\n\nThe paper lacks an analysis of cultural and linguistic bias, despite using mixed data from Beijing and New York.\n\nThe need-to-POI mapping seems to reflect designer bias rather than emergent patterns from real user behavior.\n\nNo mention of whether the system can adapt to multi-intent or ambiguous queries.\n\nIt’s unclear whether the evaluation reflects real mobility choices or just semantic alignment accuracy."}, "questions": {"value": "How is user intent variability modeled, are there multiple valid POIs for the same need, or just one ground truth?\n\nHow does the model handle ambiguous or multi-intent queries?\n\nIs the need-POI mapping empirically validated with real mobility data (e.g., GPS traces, check-ins)?\n\nCould the framework integrate behavioral priors (e.g., time-of-day, personal preferences) to better capture real-world decision patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hBolT0N2zq", "forum": "hzf23XSDcs", "replyto": "hzf23XSDcs", "signatures": ["ICLR.cc/2026/Conference/Submission20777/Reviewer_Xv86"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20777/Reviewer_Xv86"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964150721, "cdate": 1761964150721, "tmdate": 1762999986514, "mdate": 1762999986514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CitySeeker, a comprehensive benchmark designed to evaluate the spatial reasoning and decision-making capabilities of vision-language models (VLMs) in the context of embodied urban navigation for addressing implicit user needs. Extensive evaluations across a wide range of VLMs reveal key limitations in long-horizon reasoning, and the authors propose effective strategies to improve model performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Clear and Well-Structured: The paper is well-organized, with thorough explanations of the data collection process, benchmark design, and task formulation.\n\n- Novel and Interesting Setting: The paper proposed the task of embodied urban navigation guided by implicit human needs. This task is currently not widely explored and has significant potential for real-world deployment.\n\n- Extensive Evaluations: A wide range of VLMs are evaluated on the curated benchmarks, accompanied by comprehensive analysis and discussion.\n\n- Actionable Insights: The authors propose concrete strategies to enhance VLM performance, offering insights for real-world deployment scenarios."}, "weaknesses": {"value": "I don't find significant weaknesses in this submission. However, I do have some concerns as follows. Therefore, I give a conservative score of borderline accept. I may consider increasing the rating if the authors adequately address these concerns.\n\n- Open-Source Model Superiority: The paper observes that open-source VLMs (such as Qwen) occasionally outperform the proprietary VLMs. The submission would benefit from a deeper analysis of the underlying reasons behind this phenomenon.\n\n- Presentation Issue: Some figures in the submission (e.g., Figures 5 and 6) require further refinement to improve readability and visual clarity.\n\n- The manuscript would benefit from including illustrations of typical failure cases."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tSPAMmKrEs", "forum": "hzf23XSDcs", "replyto": "hzf23XSDcs", "signatures": ["ICLR.cc/2026/Conference/Submission20777/Reviewer_agkw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20777/Reviewer_agkw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20777/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985301529, "cdate": 1761985301529, "tmdate": 1762999986264, "mdate": 1762999986264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}