{"id": "aWF6wVsGYS", "number": 12450, "cdate": 1758207930622, "mdate": 1759897509347, "content": {"title": "Efficient Embedding-Generation Serving with Heterogeneous Batching", "abstract": "Modern information retrieval increasingly relies on both embedding and generative models to achieve high accuracy. To make such applications more responsive, the underlying serving systems must be optimized for mixed workloads. Yet, current systems suffer from low throughput and poor GPU utilization, primarily because they cannot batch embedding and generation requests together. We address this bottleneck with heterogeneous batching, which schedules embedding and generation requests within the same batch. Realizing this idea requires two changes to the system internals: a \\unified kernel abstraction and fine-grained intra-batch scheduling. The unified abstraction enables concurrent handling of embedding and generation, while the intra-batch scheduler dynamically adapts batch composition to balance end-to-end throughput across both tasks. Our evaluation with four A100 GPUs shows that heterogeneous batching achieves 1.28$\\times$-4.52$\\times$ higher throughput and 35.8-52.0\\% lower latency than default vLLM.", "tldr": "We propose ORTHRUS, a system that co-serves embeddings and generation with heterogeneous batching for higher throughput", "keywords": ["inference", "serving"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/35de1d1b916c87f34b9432363ed6d5859ff98341.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This Paper introduces a serving system that enables heterogeneous batching of embedding and generation requests within the same inference iteration. The key technical contribution is a unified runner abstraction, where both request types share the same schedule–forward–emit execution structure. To support this, the authors develop incremental pooling for embeddings, aligning their computation granularity with token-level decoding. Furthermore, an intra-batch scheduling policy dynamically adjusts batch composition to balance embedding and generation latency. Experimental results on multi-GPU clusters demonstrate that this new system achieves higher normalized throughput and substantially lower tail latency compared to static GPU splitting and homogeneous batching approaches, while maintaining compatibility with existing models and LoRA adapters."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Theoretical Analysis: The work presents a clear theoretical model comparing homogeneous and heterogeneous batching, formally demonstrating how heterogeneous batching improves GPU utilization and reduces total inference steps. \n- Implementation based on vLLM with Triton: The system is implemented by extending vLLM, retaining its asynchronous scheduling and PagedAttention memory management, ensuring practical deployability. Custom Triton kernels enable incremental pooling, allowing embedding computation to be aligned with token-level decoding."}, "weaknesses": {"value": "- Insufficient background information for motivation: The paper does not quantitatively characterize the relative computational and memory costs of embedding versus generation workloads. Providing concrete measurements (e.g., FLOPs, bandwidth usage, KV-cache growth) would strengthen the motivation and clarify the severity of the imbalance the system aims to address.\n- Only one primary baseline: The evaluation primarily compares against GPU-level model splitting, which limits the breadth and persuasiveness of the results. Including additional state-of-the-art serving systems or scheduling strategies would provide a more comprehensive and competitive baseline comparison.\n- No scalability evaluation from single GPU to multi-GPU: The paper does not examine how the proposed method scales with increasing GPU count or model parallel configurations. \n- Lack of discussion on broader inference task diversity: The work focuses solely on embedding and generative workloads, without considering other common inference tasks such as reranking, speculative decoding, or multi-modal encoders. Discussing how the proposed abstraction might generalize to these scenarios would improve completeness and applicability."}, "questions": {"value": "- General questions are given in the weakness part.\n- Will the experimental conclusions change when scaling from a single GPU to 2 / 4 / 8 GPUs?\n- Is there a potential corner case where, under certain embedding–generation workload ratios, the baseline may outperform the proposed approach?‘\n- Is the baseline operator implementation also based on Triton? Was there any comparison regarding operator-level performance? Is it possible that part of the overall performance improvement comes from optimizations at the operator implementation level rather than from the batching or scheduling design itself?\n- In Table 1, as the ratio between the two task types varies, the baseline and the proposed framework exhibit different performance trends. Could this be further explained? For example, in the LLaMA3 row, the baseline performance increases steadily, whereas ORTHRUS increases first and then decreases.\n- Have you experimented with different combinations of token lengths, and how would such variations affect the conclusions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ETp4jMAggl", "forum": "aWF6wVsGYS", "replyto": "aWF6wVsGYS", "signatures": ["ICLR.cc/2026/Conference/Submission12450/Reviewer_Ty67"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12450/Reviewer_Ty67"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786842958, "cdate": 1761786842958, "tmdate": 1762923331638, "mdate": 1762923331638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper mixes embedding requests and generation requests in the same batch, achieving higher throughput and lower latency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Solid implementation.\n* Important problem and good results."}, "weaknesses": {"value": "* Not sure about the validity behind the assumptions."}, "questions": {"value": "* Are you assuming that embedding models and the generation LLMs are the same model or not? If this is the case, the split GPU solution  can already work. If this is not the case, correct me if I am wrong, it means that your system also needs to store both the embedding models and the generation LLMs into GPU. SplitGPU can do the same thing to dynamically change the ratio between embedding LLMs and generation LLMs without reloading the model.\n* The evaluation assumes the embedding LLMs and generation LLMs are LoRA-finetuned from the same base model. This assumption might be too strong: for example, Qwen embedding and Qwen reranking LLMs are two different models. And the size of embedding and reranking models may also be different. Will this change your evaluation takeaway?\n* What are the use cases where the # of generation requests : # of embedding requests significantly vary over time? For example, in recommendation system, the # of generation requests : # of embedding requests is fixed because for each query it typically retrieves the same amount of related documents for recommendation.\n* The LLMs are too large for information retrieval at scale. Typically for retrieval the LLMs are <1B. Will this change your evaluation takeaway or not?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WA8wLQHwaI", "forum": "aWF6wVsGYS", "replyto": "aWF6wVsGYS", "signatures": ["ICLR.cc/2026/Conference/Submission12450/Reviewer_7SYe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12450/Reviewer_7SYe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886883518, "cdate": 1761886883518, "tmdate": 1762923331257, "mdate": 1762923331257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing retrieval augmented generation systems suffer from low GPU utilization due to the inefficiency of mixed workloads consisting of embedding and generation requests, which reduces overall throughput and leads to high latency. To mitigate this issue, the authors propose ORTHRUS, a framework that enables heterogeneous batching. Through a unified kernel abstraction and fine-grained intra-batch scheduling, embedding and generation requests can be processed within the same batch, thereby improving hardware utilization. Experimental results demonstrate that the proposed method achieves both higher throughput and lower latency across various setups."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation to improve mixed workloads in RAG systems is clear, practical, and reasonable.\n\n2. The proposed idea is interesting and has potential.\n\n3. Overall, the paper is well-structured and easy to follow."}, "weaknesses": {"value": "1. The evaluation workload does not align with real-world scenarios:\n\n   a. Dependency between embedding and generation requests is ignored: In RAG, embedding requests typically come before generation requests. However, the evaluation assumes that 1000 generation requests are issued before 1000 embedding requests, which is impractical for typical RAG workloads.\n\n   b. Input length differences are ignored: Retrieved information is usually in the form of paragraphs or articles, making generation inputs much longer than embedding queries. Yet, in the experiments, both request types use an identical input length of 128 tokens, which is not representative of real-world scenarios.\n\n   c. The embedding-to-generation ratio is impractical: Generally, each generation process involves one or no embedding request, so the number of generation requests should be greater than the number of embedding requests. If multiple embeddings are used in one generation, the generation inputs become significantly longer, and the evaluation should be adjusted accordingly.\n\n2. Normalized throughput is not a common or fair metric:\n\n   The throughput of embedding and generation requests heavily depends on input length, model, and hardware. Moreover, GPU utilization is not saturated in most cases (as shown in Figure 5). Thus, applying a constant to compute a weighted sum is unjustified. This metric also lacks interpretability and comparability across different settings. Reporting the overall time and separately showing the latency and throughput for embedding and generation requests would be more appropriate.\n\n3. The baseline comparison might not be fair:\n\n   In the SplitGPU method, the GPU becomes idle after completing all embedding requests. A simple optimization that allows the GPU to process generation requests afterward would significantly improve the baseline performance.\n\n4. Results are self-contradictory:\n\n   Figure 5 and Table 1 lead to opposite conclusions. In Figure 5, the throughput increases more significantly with a higher ratio of generation requests, while Table 1 shows the opposite trend. Additionally, Figure 5 contains a labeling error: Phase 3 should be [9 Emb : 1 Gen] rather than [1 Emb : 9 Gen] as stated in the text, which causes major confusion.\n\n5. Limited evaluation scope:\n\n   Experiments are conducted only on a single system (4 × A100) with one request type (128-token input). This setup does not sufficiently support the generalizability of the results to diverse workloads in real-world scenarios."}, "questions": {"value": "1. Regarding workloads:\n\n   a. In what scenario would 1000 generation requests occur before 1000 embedding requests?\n\n   b. In what case would generation and embedding requests have the same input length (128 tokens)?\n\n   c. Under what conditions would the ratio of embedding to generation requests be as extreme as 1:9 or 9:1?\n\n2. How does the constant for normalized throughput vary with different input lengths, model types, or hardware configurations?\n\n3. Does the proposed method perform better under embedding-heavy workloads or generation-heavy workloads?\n\n4. Does the method rely on LoRA-based embedding models? How would it perform if the embedding and generation models are entirely different?\n\n5. How could SplitGPU and the proposed method be adapted for a single-GPU setup (as in Figure 6)?\n\n6. Could the authors evaluate the framework on a practical RAG system and report improvements over the original implementation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NZKpyHX1k8", "forum": "aWF6wVsGYS", "replyto": "aWF6wVsGYS", "signatures": ["ICLR.cc/2026/Conference/Submission12450/Reviewer_DVbn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12450/Reviewer_DVbn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973182801, "cdate": 1761973182801, "tmdate": 1762923330795, "mdate": 1762923330795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets a practical serving gap: current LLM serving systems handle embeddings and text generation as different workloads, which causes low GPU utilization when the incoming mix drifts. The authors introduce a unified runner that makes embeddings follow the same iteration-level control flow as decoding, enabled by an incremental pooling kernel. On top of this, they add an intra-batch scheduler that matches the number of embeddings vs. generations in a batch to the current queue sizes. On 4×A100 they show 1.28–4.52× higher normalized throughput and lower p99 latency than GPU-split or naïve shared-GPU baselines. The idea is neat and the implementation on vLLM is likely to be useful for practitioners. However, some of the novelty overlaps with or is at least very close to contemporaneous systems that already pack heterogeneous workloads (Sarathi-Serve, TetriInfer/ShuffleInfer, MACE, SageServe), and the experimental section does not compare against these, so the strength of the claim “first to serve embedding+generation in one batch” is weaker than written."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear problem, tight abstraction. The unified iteration that ends with either sampling or incremental pooling is simple and well motivated; it’s the minimum change that makes embeddings schedulable alongside decodes. \n\n- End-to-end implementation on a real engine. Building on vLLM (0.92) rather than a toy server makes this paper much more credible for ICLR systems readers. The design is compatible with paged KV and LoRA pinning. \n\n- Solid throughput numbers on skewed mixes. The paper convincingly shows that any static GPU partition will be bad under changing embed/gen ratios, and ORTHRUS does fix exactly that. \n\n- Reasonable latency story. The IBS derivation is standard queue-proportional batching; the fact that it tracks the empirical p99s is nice. \n\n- Timeliness. vLLM’s own FAQ still says embeddings don’t benefit from its batching pipeline; the paper addresses that missing piece."}, "weaknesses": {"value": "- Positioning vs. very recent work is incomplete. Disaggregated mixed-workload serving (TetriInfer, ShuffleInfer), memory-aware heterogeneous binning (MACE, SageServe), and even 2025 vLLM deployments with disaggregated P/D all address the same underlying challenge — simultaneous, heterogeneous requests — but the paper does not implement or report against them. This makes the \"4.52×\" headline look mostly like \"we compared to a weak static-split baseline.\" \n\n- Synthetic and narrow workloads. All evaluations use fixed-length synthetic traces, one LoRA per GPU, and a single model family; modern RAG pipelines and multi-agent apps have much broader length and adapter distributions. This matters because the key benefit of the unified runner is packing the residual memory — which is exactly what gets harder under wide length variance. \n\n- The theory is nice but optimistic. The analytical model assumes generation cost scales nicely as sg=F·se and memory as mg=F·me, and that embedding requests can always fit in the \"r\" slots. In practice, stepping through decode with a very long output, or serving multiple LoRAs, can break this neat proportionality. The paper does not show a sensitivity analysis to such violations. \n\n- Missing system details. The paper says it \"invokes sampler and pooler kernels in parallel\" but doesn’t give kernel-level timings or overheads; that matters because engines like Punica and S-LoRA have already shown that kernel-fusion/batching for heterogeneous LoRA adapters is doable with ~2 ms overhead, so ORTHRUS should benchmark against that bar. \nproceedings.mlsys.org\n\n- No real-trace or application-level eval. There is no experiment on an actual RAG loop (embed → retrieve → generate → re-embed). That would be the cleanest place to demonstrate end-to-end latency/throughput improvement.\n\n- Novelty claim should be narrowed. Because 2025 systems like MACE actually do memory-aware batching of unrelated tasks in the same iteration, and SageServe does holistic scheduling across SLA tiers, the paper should claim \"first to do embedding-aware heterogeneous batching with an incremental pooling kernel inside vLLM\" — which is still good, just narrower."}, "questions": {"value": "- Positioning against recent mixed-workload serving systems.\nSarathi-Serve, TetriInfer (“Inference without Interference”), and newer memory-aware schedulers like MACE all already batch or co-schedule heterogeneous requests at the iteration level. What, concretely, can your “unified runner + incremental pooling” do that these systems cannot? Please give a side-by-side capability table. \n\n- Missing strong baselines.\nYou mainly compare to static GPU splits / naïve shared-GPU. Why didn’t you evaluate against (i) Sarathi-Serve configured for mixed prefills, or (ii) TetriInfer-style disaggregated executors, or (iii) a memory-aware bin-packing baseline like MACE? Do you expect your 4.5× speedup to still hold under those? \n\n- Robustness of the “fill residual with embeddings” assumption.\nYour analysis assumes leftover memory slots can almost always be filled by embeddings. How does the method behave when decodes are long, KV is fragmented, or multiple LoRAs/models are active so that no embedding fits — the exact cases that TetriInfer/MACE warn about? Please provide a sensitivity or failure-mode experiment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NfeE9O7jpV", "forum": "aWF6wVsGYS", "replyto": "aWF6wVsGYS", "signatures": ["ICLR.cc/2026/Conference/Submission12450/Reviewer_PhM6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12450/Reviewer_PhM6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12450/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762062538922, "cdate": 1762062538922, "tmdate": 1762923330296, "mdate": 1762923330296, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}