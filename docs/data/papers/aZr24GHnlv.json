{"id": "aZr24GHnlv", "number": 1861, "cdate": 1756955443866, "mdate": 1759898181997, "content": {"title": "Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions", "abstract": "Large Language Models (LLMs) have recently demonstrated strong emergent abilities in complex reasoning and zero-shot generalization, showing unprecedented potential for LLM-as-a-judge applications in education, peer review, and data quality evaluation. However, their robustness under prompt injection attacks, where malicious instructions are embedded into the content to manipulate outputs, remains a significant concern. In this work, we explore a frustratingly simple yet effective attack setting to test whether LLMs can be easily misled. Specifically, we evaluate LLMs on basic arithmetic questions (e.g., ``What is 3 + 2?\") presented as either multiple-choice or true-false judgment problems within PDF files, where hidden prompts are injected into the file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt injection attacks, even in these trivial scenarios, highlighting serious robustness risks for LLM-as-a-judge applications.", "tldr": "", "keywords": ["Safety", "LLMs"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7a9bca06927fd038986dd8a70467cb447dc957a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the vulnerability of LLMs to prompt injection when used in an \"LLM-as-a-judge\" capacity for grading. The authors create PDF documents with simple arithmetic problems and embed malicious instructions, using either visible black text or invisible white text, to command the model to output a specific, incorrect answer. The results show that many LLMs are indeed fooled by these prompts, particularly the visible ones, while models with a \"thinking\" mode exhibit greater robustness against the \"hidden\" white-text attacks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper studies the reliability of LLM-as-a-judge applications under prompt injection, which is a timely area as AI integration becomes more common.\n2. The paper is generally well presented and easy to understand."}, "weaknesses": {"value": "1. The core discovery is LLMs are \"fooled\" with injected prompts. However, essentially it's an instruction-following model following instructions (injected prompt) found in the input text. The finding is expected and not surprising. The experiments, for example black-text and white-text prompts in pdf, do not provide much insight. The scientific or technical contribution is quite limited.\n2. Table 1 contains objectively wrong parameter counts for GPT-4o, o3, and DeepSeek.\n3. The authors mentioned the potential impact of prompt injection on peer review (without experiments). It's also a known issue and I believe conferences are aware of this issue and even have rules on it. I'm not sure what the claim is here."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "a3rdEfz17f", "forum": "aZr24GHnlv", "replyto": "aZr24GHnlv", "signatures": ["ICLR.cc/2026/Conference/Submission1861/Reviewer_r4Wf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1861/Reviewer_r4Wf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760475868072, "cdate": 1760475868072, "tmdate": 1762915916798, "mdate": 1762915916798, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the vulnerability of LLM-as-a-judge to adversarial prompt injection. The work presents a preliminary analysis, showing that the evaluation process of LLMs can be easily twisted by certain prompts, resulting in biased judgments."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper discusses the important issue of fairness and robustness of LLMs when used as evaluators. The authors' initial exploration is promising. By successfully applying different prompt injection attacks on powerful LLMs, the paper effectively demonstrates the existence of these vulnerabilities."}, "weaknesses": {"value": "Despite the promising direction, the paper suffers from several major weaknesses in its current form:\n\n1. The main conclusion of the paper appears to be trivial and somewhat obvious. The vulnerability of LLMs to injection attacks has already been well-established in a large body of prior work. This paper's analysis, while confirmatory, does little more than reiterate this known phenomenon. Consequently, the primary research question 1 posed by the authors is not a true research question, as its answer is largely self-evident from existing literature.\n\n2. The paper is underdeveloped in several key aspects, including its motivation, methodology (which is not clearly defined), experimental design, and depth of analysis. The work currently reads more like a preliminary study. A more impactful contribution would involve exploring how to mitigate these vulnerabilities. For example, the authors could investigate methods to enhance the LLM's inherent robustness against such attacks to ensure reliable evaluation outcomes.\n\n3. Overall, the current manuscript resembles a technical report or a simple experimental analysis rather than a rigorous academic paper. The contribution is not substantial enough for a publication at this venue."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "BRBRPUBgAO", "forum": "aZr24GHnlv", "replyto": "aZr24GHnlv", "signatures": ["ICLR.cc/2026/Conference/Submission1861/Reviewer_r6G3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1861/Reviewer_r6G3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760499427888, "cdate": 1760499427888, "tmdate": 1762915916514, "mdate": 1762915916514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a jailbreaking method for LLMs by injecting “hidden prompts” into PDF documents. The authors show that by changing the color of text embedded in a PDF document nefarious actors could inject information to an LLM that is invisible to people. They show that this is a concern for state of the art LLMs. The authors attempt to demonstrate the effectiveness of this attack with a limited number of examples in the form of true or false and multiple choice questions. They also investigate the impact of \"thinking\" on the models susceptibility of the attack."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is overall well written and straightforward to read. The authors also aim to contribute to an important area, LLM security."}, "weaknesses": {"value": "- **This work seems to lack novelty.** The authors overlook a key related work “Invisible Prompts, Visible Threats: Malicious Font Injection in External Resources for Large Language Models” published in EMNLP Findings 2025 which investigates very similar “font injection” attacks. The setting they investigate is more general and their experimental investigation is more extensive.\n- **The experimental investigation is very limited.** It seems like the authors only do an experimental evaluation on 4 total problems. Tables 2, 3, and 4 only cover the results for single problems. The rigor of these experiments could be strengthened by scaling up the investigation to more problems.\n- In my opinion, **the implications and main takeaways of this study are quite limited.**"}, "questions": {"value": "I don't have any substantial questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6PevM1qrUk", "forum": "aZr24GHnlv", "replyto": "aZr24GHnlv", "signatures": ["ICLR.cc/2026/Conference/Submission1861/Reviewer_MD8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1861/Reviewer_MD8p"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761676175868, "cdate": 1761676175868, "tmdate": 1762915916233, "mdate": 1762915916233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates whether hidden textual prompts embedded in PDF files (e.g., white-colored text or invisible LaTeX instructions) can manipulate LLMs’ answers to trivial arithmetic questions.\nThe authors test six models (GPT-4o, GPT-o3, Gemini 2.5 Flash/Pro, DeepSeek-V3/R1) under no prompt, black prompt, and white prompt conditions.\nThey find that even top-tier LLMs can be misled, particularly by visible (black) injections, while “thinking-enabled” models show better robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1) Clear, reproducible setup:** The authors define a simple, interpretable pipeline (Eq. 1) for prompt injection in PDFs using LaTeX color control.\n\n**2) Novel variant of a known problem** Prior works (e.g., Liu et al., 2024c; Guo et al., 2024; Raina et al., 2024) study prompt injection generally, but few explore the PDF-hidden variant. The work highlights this under-studied vector.\n\n**3) Empirical value.** Confirms that even simple visual-level attacks can bypass superficial safety filters in reasoning-oriented LLMs, relevant to \"LLM-as-a-judge\" systems."}, "weaknesses": {"value": "**1) Trivial methodology, no insight beyond anecdote:** This paper's contribution is essentially a reproduction with simpler math tasks. The authors never quantify why certain models succumb, i.e., there is no causal insight, just observed failure.\n\n**2) Excessive space on prompt instantiation, minimal analysis:** Over two pages are devoted to LaTeX examples of \"black\", \"white\", and \"no\" prompt injections. This is implementation detail; the space could instead show token-level model traces or why the white prompt affects GPT-4o but not DeepSeek-V3.\n\n**3) Overclaiming in title and abstract:** \"Breaks LLMs on Frustratingly Simple Questions\" is misleading: the white-text attack consistently fails against several tested models (e.g., DeepSeek-V3). At best, the results show partial vulnerability, not systemic failure.\n\n**4) No discussion or evaluation of defenses:** Appendix C adds a one-line \"defensive prompt\" but provides no systematic mitigation framework.\n\n**5) Missing quantitative metrics:** The analysis is purely categorical (\"correct\" vs \"incorrect\"), lacking statistics such as attack success rate = #misled / #total."}, "questions": {"value": "1) Why does the white prompt succeed only on GPT-4o? Is it due to OCR parsing vs PDF text extraction?\n\n\n2) Do thinking models resist because of reasoning steps or just input preprocessing differences?\n\n3) How many total prompts per model were tested? Please report success rate (%) rather than per-instance tables."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WFNxxauwsD", "forum": "aZr24GHnlv", "replyto": "aZr24GHnlv", "signatures": ["ICLR.cc/2026/Conference/Submission1861/Reviewer_v7xL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1861/Reviewer_v7xL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762075440173, "cdate": 1762075440173, "tmdate": 1762915915956, "mdate": 1762915915956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}