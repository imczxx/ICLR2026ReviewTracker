{"id": "zu2BjeEwRJ", "number": 13922, "cdate": 1758225238906, "mdate": 1759897403266, "content": {"title": "How Predictable is AI Progress?", "abstract": "Benchmarks only serve to measure what models are capable of now, not what they will be capable of in the future. We find that the ordering of acquired capabilities is remarkably consistent across large populations of AI models, which begs the question of whether one can forecast which specific examples and capabilities future models will solve next. We propose formalizing this problem into a new evaluation task called progress prediction: Can we forecast which unsolved problems will be solved next as future models improve? We find that progress is, in fact, predictable. Through an empirical study of hundreds of millions of predictions made by 1,000+ vision models and 1,600+ language models, we find that this predictability is possible due to the consistent order in which models acquire capabilities across architectures, datasets, and modalities.", "tldr": "", "keywords": ["deep learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fdba51fffecf5061fe08cc009445129f6d4edd60.pdf", "supplementary_material": "/attachment/734b09c92dd194ee3a0eb71e83a2f83557cc0145.zip"}, "replies": [{"content": {"summary": {"value": "The paper asks whether AI capability gains are predictable with benchmark examples and introduces a new evaluation task: progress prediction, which aims to forecast which currently unsolved examples will be solved next as models improve.\nTo quatify the consistency of example-solving order across models, the paper introduced a metric: Prediction Order Coherence (POC).\nResults across vision and language models show above-chance progress prediction across modalities and reveals that example difficulty ordering correlates with training dynamics metrics like c-score."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper introduces progress prediction as a new evaluation task for vision and language models.\nPrediction without ground truth labels (using only model logits) is valuable for forecasting on unsolved or ambiguous problems where ground-truth is unknown or scarce."}, "weaknesses": {"value": "1. **Progress prediction evaluated only on small models via retrospective back-testing.**\n(1) The method assumes that future models will follow the same ordering as past models. But distributional shifts in training data, architecture innovations (e.g., mixture-of-experts, retrieval-augmentation), or emergent capabilities could break this assumption.\n(2) The models evaluated in the experiments are in a small scale (at most 14B), thus results are unclear when transferring to larger models.\n2. **POC metric assumes accuracy is a sufficient proxy for capability ordering.**\nSorting models by overall accuracy may mask capability misalignments (e.g., a lower-accuracy model excelling on a specific skill). A more nuanced ordering—e.g., by performance on skill-specific subsets—might reveal finer-grained predictability patterns.\n3. **Improper and insufficient evaluation settings.**\n(1) The “precision vs K/|N|” AUC metric is unconventional and might be sensitive to class imbalance and ranking shape. The binary classification variant in the appendix helps, but the main text relies on the curve AUC without clear statistical properties.\n(2) LLM evaluations mix 0-shot and few-shot settings and restrict answers to single-token multiple choice. These choices can materially affect accuracy and confidence signals. Decoding temperature and prompt templates are also not detailed.\n4. **Limited exploration of why ordering consistency arises.**\nWhile the paper demonstrates that ordering is consistent and correlates with c-score, it doesn’t deeply investigate the causal mechanisms. For example, is this due to shared data priors, optimization biases, or intrinsic problem structure? A more mechanistic analysis (e.g., probing feature representations across models) would strengthen the claims.\n5. **Results lack of statistical rigor and uncertainty quantification.**\nReported AUC gains over chance (e.g., 0.545–0.550 label-free predictions) are modest. No confidence intervals, bootstrapping, or statistical tests are provided to demonstrate significance and robustness.\n6. **Task selection may bias results toward compositional domains.**\nThe paper claims in Section 7 that ordering is weaker on “trivia problems” but stronger on “compositional skills.” However, the chosen benchmarks (MATH, BBH, MUSR) are all reasoning-heavy. Including more diverse tasks (e.g., commonsense QA, creative generation) would test the generality of the claims. \n7. **Lack of ablation on model population diversity.**\nIt’s unclear how many models are needed for a stable population ordering. Does POC saturate with 100 models, or does it keep improving with scale? An ablation study on population size and diversity (e.g., varying architecture families) would inform practical deployment.\n8. **Hard to follow and understand.**\nThe paper lacks of many details and explanations. See questions below."}, "questions": {"value": "1. Whar are the 1,000+vision models and 1,600+ language models used in the experiments? I only see 36 vision models and 26 language models in the appendix.\n2. What does Figure 1 aim to convey?\n3. Errors in Figure 4. Where is the blue dashed line? Where is random order?\n4. How to see different population order difficulties in Figure 7?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vhaIK4mA9X", "forum": "zu2BjeEwRJ", "replyto": "zu2BjeEwRJ", "signatures": ["ICLR.cc/2026/Conference/Submission13922/Reviewer_SiLp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13922/Reviewer_SiLp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760675581615, "cdate": 1760675581615, "tmdate": 1762924427632, "mdate": 1762924427632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the question of whether it is possible to forecast what specific elements (instances) of a benchmark will be solved by upcoming models with higher accuracy. This is connected to whether there is an overlap between the instances that a higher-accuracy model still fails to solve and those that a lower accuracy model fails to solve. They do this by considering various methods, both leveraging and ignoring knowledge of ground truth. Experiments on LLMs, vision models and protein models. They find above chance AUC when using ground truth logits, and slightly above change when not, which indicates some predictive power is possible"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- I think studying how instance-level correctness evolves over models is an interesting and underaddressed question\n- The finding that the difficulty prediction correlates with other difficulty metrics is interested\n- the graphs showing predictions over population are insightful"}, "weaknesses": {"value": "- I think the main issue is the lack of clarity of the prediction method: the authors mention that both methods using and ignoring ground truth are used, but I am uncertain as to how these methods are actually implemented, particularly considering the different classes of models. I suggest the authors discuss this more in detail in the main body. Some of the language around this is quite handweavy. FOr instance\n    - line 248 refers to “easiest K examples”: what does “easiest mean in this context\n    - line 231: “what examples a model will be able to solve next if it improves its overall accuracy”: what does it mean for a model to improve its accuracy? Plausibly, the authors are talking about successive models for the same task\n    - line 316 “examples that it [the model]  thinks will be solved”: how does a model think an example will be solved?\n    - somewhere else, the paper mentions “population prediction order”: I am not clear if this is the same as the prediction methods used elsewhere and, if so, why are they referred to as “populational” now?\n\nOther issues are\n\n- Connected to the point above: the left panel of Fig 1 is also unclear, and I don’t understand the graph on the right hand side.\n- unclear motivation: while it is interesting to know what tasks future model generations are going to solve, assuming that the ground truth is known makes this irrelevant to real-world applications.\n- overselling the results: line 367-372 oversells the predictive power. While 0.642 AUC is quite good, this is only for cases when the ground truth is leveraged which, as I argued above, is not relevant to real-world applications (when the ground truth of a task may be unavailable). If the ground truth is not used, the AUC is only sligh6tly above chance"}, "questions": {"value": "- how is the difficulty prediction obtained?\n- can the difficulty prediction be compared to baseline methods such as populational item-response theory"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HMn0NNxXfD", "forum": "zu2BjeEwRJ", "replyto": "zu2BjeEwRJ", "signatures": ["ICLR.cc/2026/Conference/Submission13922/Reviewer_MZXb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13922/Reviewer_MZXb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672953166, "cdate": 1761672953166, "tmdate": 1762924427080, "mdate": 1762924427080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces “progress prediction”, a task aimed at forecasting which unsolved examples current AI models will solve next as their capabilities improve. It claims that model progress follows a consistent ordering across architectures and datasets, quantified using a new metric called Prediction Order Coherence (POC)."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "I could not identify any clear strengths, as the paper’s main ideas and contributions are difficult to understand and insufficiently explained."}, "weaknesses": {"value": "1. Unfulfilled claims in the introduction: The authors claim that knowing “when this will happen can be more important…” would guide research allocation, but the paper never quantifies or discusses when progress occurs, the claim remains unaddressed.\n\n2. Vague problem framing: The text in the introduction says the problem can be viewed as an ordering problem, but it never explains what is being ordered or how the ordering is established across models, architectures, or datasets.\n\n3. Unclear notation and definitions: In the introduction, definition of progress prediction notations such as  model, data, set of examples are all capitalized and vaguely defined. Sets are usually denoted by $\\mathcal{N}$ rather than simple capital N. What are the models? They are mapping from what to what? What is the sample space of the datasets?\n\n4. Figure organization and readability issues: Figure 1 is confusing and lacks a clear explanation of its meaning. Figures appear out of logical order (e.g., Figure 3 should precede Figure 2). All the figures are misplaced, descriptions are not in order of figure number.\n\n5. Lack of theoretical justification for the POC metric: There is no mathematical or statistical reasoning provided to show that the Prediction Order Coherence (POC) is a valid or consistent estimator of model ordering. \n\n6. Section 4 (“Next Solvable Example Prediction Task”) is unclear. The exposition is confusing and lacks a coherent step-by-step formulation. It is not specified whether the “lower-accuracy model” is temporally earlier or simply less capable — temporal ordering versus capability ordering is never clarified.\n\n7. Figure 4 is ambiguous. The meaning of the overlapping red and orange lines is never explained. The figure appears disconnected from the surrounding text.\n\n8. Overall coherence: The paper reads like a collection of loosely connected results with minimal narrative or logical flow. Core concepts (progress prediction, ordering, frontier advancement) are presented inconsistently, and no unified study emerges.\n\nGiven the unclear definitions, inconsistent figures, and absence of theoretical grounding, the submission appears premature for peer review and would require substantial restructuring before meaningful evaluation."}, "questions": {"value": "See my above points"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0ZWTCNHZ1y", "forum": "zu2BjeEwRJ", "replyto": "zu2BjeEwRJ", "signatures": ["ICLR.cc/2026/Conference/Submission13922/Reviewer_mULo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13922/Reviewer_mULo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705555614, "cdate": 1761705555614, "tmdate": 1762924426659, "mdate": 1762924426659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims to be able to predict capabilities of future AI"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The question is very interesting.\n* There is extensive empirical analysis."}, "weaknesses": {"value": "* The paper's scope is vague (as given by the abstract) and there is overclaiming: They defined a capability as the ability to predict on some samples, e.g., recognize some objects of images, but the term is much broader. The title is more like \"Are there consistently hard and easy samples across models?\"\n* There are no deeper insights and incremental contrribution. The paper is poorly empirical, answering essentially the question \"Are there consistently hard and easy samples across models?\"   This question is not new and thus more depth would be expected. That said, I think this direction has a lot of potential as a deeper understanding of (emergent) model behavior would be great, but that would need more work.\n\n\nDetails:\nFigure 3: on y axis, it would be nice to have sth like time or so to show the evolution across time, which would better align with your title."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zCmHaPovlG", "forum": "zu2BjeEwRJ", "replyto": "zu2BjeEwRJ", "signatures": ["ICLR.cc/2026/Conference/Submission13922/Reviewer_whgK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13922/Reviewer_whgK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827260836, "cdate": 1761827260836, "tmdate": 1762924426167, "mdate": 1762924426167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}