{"id": "9oS7DHIg7f", "number": 8929, "cdate": 1758102892263, "mdate": 1759897753193, "content": {"title": "LiFR-Seg: Anytime High-Frame-Rate Segmentation via Event-Guided Propagation", "abstract": "Dense semantic segmentation in dynamic environments is fundamentally limited by the low-frame-rate (LFR) nature of standard cameras, which creates critical perceptual gaps between frames.\nTo solve this, we introduce *Anytime Interframe Semantic Segmentation*: a new task for predicting segmentation at any arbitrary time using only a single past RGB frame and a stream of asynchronous event data.\nThis task presents a core challenge: how to robustly propagate dense semantic features using a motion field derived from sparse and often noisy event data, all while mitigating feature degradation in highly dynamic scenes.\nWe propose LiFR-Seg, a novel framework that directly addresses these challenges by propagating deep semantic features through time. The core of our method is an *uncertainty-aware warping process*, guided by an event-driven motion field and its learned, explicit confidence. A *temporal memory attention* module further ensures coherence in dynamic scenarios.\nWe validate our method on the DSEC dataset and a new high-frequency synthetic benchmark (SHF-DSEC) we contribute. Remarkably, our LFR system achieves performance (73.82\\% mIoU on DSEC) that is statistically indistinguishable from an HFR upper-bound (within 0.09\\%) that has full access to the target frame.\n% We further demonstrate superior robustness in *highly dynamic* (M3ED-Drone \\& Quadruped) and *low-light* (DSEC-Night) scenarios, where our method can even surpass the HFR baseline.\nWe further demonstrate superior robustness across extreme scenarios: in highly dynamic (M3ED) tests, our method closely matches the HFR baseline's performance, while in the low-light (DSEC-Night) evaluation, it even surpasses it.\nThis work presents a new, efficient paradigm for achieving robust, high-frame-rate perception with low-frame-rate hardware.", "tldr": "", "keywords": ["Event Camera", "Neuromorphic Computing"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49964d3b680a0fa022f22e95c890eb0780a7cfc2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel framework for Anytime Semantic Segmentation to address the challenges of high-temporal-resolution segmentation in dynamic environments by integrating RGB images and event data. Leveraging the complementary strengths of RGB frames (spatially dense but temporally sparse) and event streams (temporally dense but spatially sparse), their method proposes uncertainty-aware optical flow estimation, learnable feature warping, and a memory mechanism to align and enhance features for segmentation at arbitrary time points. The framework is evaluated on the DSEC and a newly introduced synthetic dataset, SHF-DSEC, demonstrating better performance and robustness. However, they didn't elaborate on training time and how fast their model run compared to SOTA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "•\tThe design of the framework is relatively well explained, incorporating uncertainty-aware optical flow estimation, learnable feature warping to achieve precise feature alignment across modalities\n•\tThe proposed synthetic dataset (SHF-DSEC) is a valuable contribution to the field for it provides a high-temporal-resolution segmentation dataset"}, "weaknesses": {"value": "•\tDidn’t elaborate on training time and number of parameters of the proposed model against baselines they compare with\n•\tDid they explain how they generate event streams in SHF-DSEC\n•\tNo report on how fast their model runs compared to other schemes."}, "questions": {"value": "The optical flow estimator, E-Raft, is pretrained on DSEC and SHF-DSEC using their corresponding ground truth optical flow. This setup may lead to unfair evaluation because the model could overfit to the ground truth flow of the corresponding dataset. It is unclear whether the trained model can generalize to unseen data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "27OwNfAYlw", "forum": "9oS7DHIg7f", "replyto": "9oS7DHIg7f", "signatures": ["ICLR.cc/2026/Conference/Submission8929/Reviewer_xGY9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8929/Reviewer_xGY9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761135916190, "cdate": 1761135916190, "tmdate": 1762920677613, "mdate": 1762920677613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents \"LiFR-Seg,\" a novel framework for Anytime Interframe Semantic Segmentation, addressing the perceptual gaps in low-frame-rate (LFR) camera systems by predicting dense semantic maps at any time $ t + \\delta t $ using a single past RGB frame $ I_t $ and an event stream $ E_{t-\\Delta t \\rightarrow t+\\delta t} $. The method introduces an uncertainty-aware motion field estimated via a RAFT-like FlowNet and ScoreNet, uncertainty-guided feature propagation with softmax splatting, and a temporal memory attention module for consistency.\n\nEvaluated on the DSEC dataset, SHF-DSEC (a new 100 Hz synthetic benchmark), M3ED, and DSEC-Night, the experiments aim to demonstrate LiFR-Seg's robustness and effectiveness across diverse scenarios, including standard driving conditions, high-frequency synthetic environments, high-dynamic motion, and low-light settings. The paper contrasts its causal, anytime-capable design with non-causal interpolation and non-anytime fusion baselines (e.g., CMNeXt), highlighting its ability to bridge perceptual gaps inherent in LFR systems, though it lacks efficiency metrics critical for real-time applications like autonomous driving."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel task, \"Anytime Interframe Semantic Segmentation,\" which addresses the critical \"perceptual gap\" in low-frame-rate (LFR) systems by enabling dense semantic segmentation at any arbitrary time\n2. LiFR-Seg proposes a robust framework that leverages the complementary strengths of RGB cameras (dense semantic context) and event cameras (high-temporal-resolution motion cues). The method’s core components—uncertainty-aware motion field estimation (Section 3.2), uncertainty-guided feature propagation (Section 3.3), and temporal memory for long-term consistency (Section 3.4)—are technically sophisticated and well-integrated, as depicted in Figure 2.\n3. Unlike non-causal interpolation methods or non-anytime fusion approaches, LiFR-Seg is explicitly designed to be both causal (relying only on past data) and capable of anytime prediction, as emphasized in Section 3.1 and Figure 3.\n4. The introduction of the SHF-DSEC synthetic dataset with 100 Hz temporal resolution (Section 4) provides a valuable resource for high-frequency evaluation"}, "weaknesses": {"value": "1. The Eq.(3) employs a compact function composition that obscures the specific roles and data flow between $\\phi_{\\text{joint}}$, $F_{\\text{SED}}$, and $\\phi_{\\text{out}}$. A stepwise breakdown would improve readability, and its correspondence to components in Figure 2(b).\n2. The author claims that the pipeline is causal, however, the $E_{t+\\delta t \\rightarrow t+\\Delta t}$ is input the 'Flow estimator' in Fig.2, making it confusing.\n3. The experiments lack detailed ablations for all proposed modules, such as the uncertainty-aware motion field and ScoreNet, with no direct comparison (e.g., with vs. without confidence modulation).\n4. Critical implementation details, such as the use of OhemCrossEntropy loss for training (Appendix C.1), are relegated to appendices rather than the main text, which hinders quick understanding of the optimization strategy and reproducibility.\n5. The paper does not provide a direct comparison of the LiFR-Seg model’s computational efficiency (e.g., inference time, frames per second [FPS], or resource usage) with other settings or baseline models. The real-time processing is important for the application like atonomous driving.\n6. As acknowledged in Appendix E, the benchmarks lack datasets with extreme high-speed dynamics (e.g., severe motion blur or frequent occlusions), which is a crucial application scenarios for usage of event camera."}, "questions": {"value": "1. What is the design rationale behind the multiple modules in LiFR-Seg (e.g., uncertainty-aware motion field, guided propagation, and temporal memory), and how do their outputs complement each other? \n2. Additionally, is the fusion of event data (sparse motion cues) and RGB data (dense semantics) sufficiently effective, or could alternative fusion strategies (e.g., earlier joint embedding) improve performance in sparse-event scenarios?\n3. How does the framework mitigate the impact of noise and sparsity in event data during fusion"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8zCmpoR7z8", "forum": "9oS7DHIg7f", "replyto": "9oS7DHIg7f", "signatures": ["ICLR.cc/2026/Conference/Submission8929/Reviewer_PFAT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8929/Reviewer_PFAT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761569854802, "cdate": 1761569854802, "tmdate": 1762920677286, "mdate": 1762920677286, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new task, Anytime Interframe Semantic Segmentation, which offers a practical, causal, and predictive formulation for real-world autonomous systems. Their approach centers on propagating deep features rather than images or segmentation maps, a strategy that is empirically validated (Table 3). The uncertainty-guided propagation mechanism (Eq. 4) effectively handles the noise and sparsity inherent in event-based motion. Quantitative results closely match the high-frame-rate upper bound on DSEC (0.09% mIoU gap) and surpass it on DSEC-Night. Evaluation across multiple real and synthetic datasets, baselines, and ablations, particularly in warping, flow robustness, and memory, demonstrates the method’s efficacy and supports its design rationale."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The LiFR-Seg framework presents a practical, causal, and predictive formulation for real-world autonomous systems. Its core idea, propagating deep features instead of images or segmentation maps, is empirically validated as shown in Table 3. The uncertainty-guided propagation, described in Equation 4, effectively addresses the inherent noise and sparsity of event-based motion. Quantitative results closely match the high-frame-rate upper bound on DSEC with a 0.09 percent mIoU gap and exceed it on DSEC-Night. Evaluation across multiple real and synthetic datasets, baselines, and ablations, particularly in the areas of warping, flow robustness, and memory, demonstrates the method’s efficacy and supports its design choices. The new SHF-DSEC dataset, which includes 100Hz ground-truth segmentation, although tailored to this setting, may also serve as a useful resource for future research in event-based semantic segmentation."}, "weaknesses": {"value": "The paper argues for being an \"efficient paradigm\" (Abstract, Appendix D), but this argument is based entirely on hardware (cost, power, bandwidth). It provides no analysis of computational cost (e.g., FLOPs, inference latency). The proposed LiFR-Seg framework involves running a feature encoder, a flow network, a ScoreNet, a splatting operation, and a temporal memory module. This is almost certainly more computationally expensive than the HFR baseline (which just runs a SegFormer). This is a critical trade-off for practical deployment that is not discussed. \"Efficiency\" in terms of hardware is traded for what appears to be higher computational load.\n\nThe limitations section (Appendix E) states that a weakness is the \"lack of evaluation on datasets featuring high-speed dynamics.\" This is confusing, as the M3ED dataset is repeatedly described in the main paper as featuring \"high-speed scenarios\" and \"highly dynamic\" motion. The authors should clarify this apparent contradiction.\n\nThe paper correctly ablates against \"Image Warping\" and \"Interpolation\" (TLX+Seg) and shows they are inferior. However, the reason for their failure (especially on SHF-DSEC) is only \"conjectured\" to be \"interpolation artifacts\" (Sec 5.1).  A deeper analysis would be valuable. Is the problem that reconstruction models are optimized for perceptual losses (PSNR/LPIPS) which are not aligned with downstream semantic consistency? This is a key point that could be strengthened."}, "questions": {"value": "Could the authors please provide a detailed comparison of the computational cost (e.g., FLOPs and inference latency in ms) between: (a) the proposed LiFR-Seg, (b) the HFR Upper Bound (SegFormer-B2), and (c) the LFR + Interpolation baseline (TLX + SegFormer)? This is crucial for understanding the true trade-offs of the proposed \"efficient paradigm.\"\n\nIn Appendix C.1, the training strategy mentions that $F_{t+\\delta t}$ is warped a second time to $F_{t+\\Delta t}$ to be compared with the ground truth $Seg_{t+\\Delta t}$. How is this second warp performed? Is a new motion field $M_{t+\\delta t \\rightarrow t+\\Delta t}$ estimated? If so, how, and what are its inputs?\n\nCould the authors please clarify the statement in Appendix E regarding the \"lack of evaluation on datasets featuring high-speed dynamics\"? How does this reconcile with the use of the M3ED dataset, which is presented as high-speed? What specific \"high-speed\" phenomena are not covered?\n\nThe paper convincingly demonstrates that feature-warping is superior to image-warping/interpolation. Why, fundamentally, does the (reconstruction $\\rightarrow$ segmentation) pipeline fail? Is it because the optimization objective of image reconstruction (e.S., PSNR) is misaligned with the goal of semantic-level consistency, even if the reconstruction looks plausible to a human?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9mPY47m9PX", "forum": "9oS7DHIg7f", "replyto": "9oS7DHIg7f", "signatures": ["ICLR.cc/2026/Conference/Submission8929/Reviewer_c5rU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8929/Reviewer_c5rU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725699222, "cdate": 1761725699222, "tmdate": 1762920676903, "mdate": 1762920676903, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LiFR-Seg, a framework for Anytime Interframe Semantic Segmentation, where the objective is to predict dense semantic maps at arbitrary timestamps using only a past RGB frame and a stream of asynchronous event data. The proposed approach estimates an event-driven uncertainty-aware motion field, propagates features through uncertainty-guided Softmax Splatting, and ensures temporal coherence via a temporal memory attention module. The authors also introduce SHF-DSEC, a synthetic benchmark built on CARLA that enables fine-grained evaluation of anytime performance at 100 Hz. Extensive experiments across four datasets (DSEC, SHF-DSEC, M3ED, and DSEC-Night) show that LiFR-Seg achieves performance nearly identical to a high-frame-rate (HFR) upper bound and maintains robustness in highly dynamic and low-light scenarios"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a well-defined and practically important problem at the interface of event-based sensing and dense semantic segmentation. The authors clearly articulate the “Anytime Interframe Segmentation” task, distinguishing it from standard video propagation or multi-modal fusion. The method is technically solid, combining event-driven motion estimation, uncertainty-weighted feature propagation, and temporal memory in a cohesive framework that respects causal and anytime constraints."}, "weaknesses": {"value": "While the overall contribution is convincing, the conceptual novelty is limited. LiFR-Seg builds on well-known components, RAFT-style optical flow, Softmax Splatting, and memory-based refinement, and integrates them effectively rather than introducing a fundamentally new algorithmic idea. The contribution is thus primarily at the systems and task-definition level.\n\nSome implementation details remain underspecified. The design and update mechanism of the temporal memory module are described conceptually but not concretely, leaving ambiguity about how features are written to or retrieved from memory. The uncertainty map, though central to the proposed “uncertainty-aware” warping, is not visualized or quantitatively analyzed, so its empirical behavior remains unclear. In addition, while the new SHF-DSEC dataset is useful, the paper does not analyze its realism or demonstrate transfer from synthetic to real data. Finally, runtime and computational cost comparisons are missing despite claims of efficiency, which limits the evaluation of practical deployment potential.\n\nDespite these issues, the paper is technically sound, and empirically strong. The combination of causal design, uncertainty modeling, and temporal coherence results in a high-quality, well-rounded study that advances event-driven semantic segmentation."}, "questions": {"value": "- Could you clarify how the temporal memory bank is updated over time (e.g., recurrently after each $\\delta t$ or at selected keyframes)?\n\n- How is the uncertainty map trained, does it rely solely on end-to-end supervision, or is any auxiliary loss used?\n\n- What is the computational cost (runtime, memory usage, or FLOPs) compared to fusion-based baselines such as CMNeXt or EISNet?\n\n- Has the SHF-DSEC dataset been validated for realism or bias? How well does a model trained on SHF-DSEC transfer to real DSEC data?\n\n- How sensitive is the system to event noise or missing events, and does the uncertainty weighting mitigate this effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iXBVbpnVzg", "forum": "9oS7DHIg7f", "replyto": "9oS7DHIg7f", "signatures": ["ICLR.cc/2026/Conference/Submission8929/Reviewer_AC8G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8929/Reviewer_AC8G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8929/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987761451, "cdate": 1761987761451, "tmdate": 1762920676558, "mdate": 1762920676558, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}