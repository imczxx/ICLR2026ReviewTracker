{"id": "PoUdJ8bMDQ", "number": 1186, "cdate": 1756861718982, "mdate": 1759898222937, "content": {"title": "TabularGSM: Understanding the Limitations of LLMs in Tabular Math Reasoning", "abstract": "Mathematical reasoning has long been a key benchmark for evaluating large language models (LLMs). Although substantial progress has been made on math word problems, the need for reasoning over tabular data in real-world applications has been overlooked. For instance, applications such as business intelligence demand not only multi-step numerical reasoning with tables but also robustness to incomplete or inconsistent information. However, comprehensive evaluation in this area is severely limited, constrained by the reliance on manually collected tables that are difficult to scale and the lack of coverage for potential traps encountered in real-world scenarios. To address this problem, we propose AutoT2T, a neuro-symbolic framework that controllably transforms math word problems into scalable and verified tabular reasoning tasks, enabling the evaluation of both accuracy and robustness. Building on this pipeline, we develop TabularGSM, a benchmark comprising three progressively complex subsets and a trap subset, with two complementary evaluation settings. Our study reveals three key observations: (1) Tabular structure makes mathematical reasoning more challenging; (2) The difficulties stem from the joint effects of tabular retrieval and reasoning; (3) Reasoning robustness is another significant issue that needs to be addressed in existing LLMs. In-depth analyses are conducted for each observation to guide future research.", "tldr": "We present AUTOT2T and TabularGSM to evaluate LLMs on tabular math reasoning, showing that retrieval–reasoning interplay and robustness are key bottlenecks.", "keywords": ["Math Reasoning", "Table Reasoning", "Neuro-Symbolic", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ac727b6f173adcae291e7d1188d8c091d8d2be0.pdf", "supplementary_material": "/attachment/8e5684945d7b2ee8278022bced16fa83b60e99a3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes AutoT2T to transform math word problems into a robust and large scale tabular-symbolic dataset and provides the benchmark based on this dataset. This paper also shows fine-tuning LLMs on AutoT2T-generated data improves TabularGSM performance by 15+%."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper is the first to systematically evaluate robustness in tabular mathematical reasoning—for instance, by testing whether models abstain from answering when data is incomplete or contradictory.\n- The tabular dataset used is sufficiently large in scale and more complex than those employed in prior studies.\n- The symbolic approach facilitates the creation of diverse instances, effectively avoiding data contamination and enabling a more comprehensive assessment of LLMs’ reasoning capabilities."}, "weaknesses": {"value": "- Limited coverage of multimodal capabilities\nThis paper only evaluates text-modal tables (e.g., serialized or Markdown formats) while overlooking image-modal tables (e.g., business tables commonly used in real-world scenarios). Additionally, multimodal LLMs such as GPT, Gemini, and Seed are not included in the evaluation, leaving a gap in understanding how visual table structures influence reasoning performance.\n\n- Insufficient justification for the robustness definition\nThe paper defines robustness as LLMs’ ability to refuse answering questions. However, prompt design and long contextual inputs can also significantly influence LLMs’ responses. Since this is the first formulation of the definition, a clearer justification for its validity and a method to verify its effectiveness (e.g., controlling for prompt/contextual variables) should be provided.\n\n- Inadequate explanatory depth for key results\n* Accuracy heatmap (Figure 4): For Llama3.1 8B, the heatmap pattern differs from that of the other three models, but no explanation is provided for this discrepancy.\n* Fine-tuning results: While the paper uses small-scale (tiny) models and observes significant performance improvements after fine-tuning, it does not address how these fine-tuned models would perform on other tabular datasets—this limits the generalizability of the findings.\n\n- Lack of substantial innovation in dataset construction\nPrior works [1, 2] have already adopted template-based ideas for dataset expansion. Although this paper adds incremental augmentations to the template framework, it still relies on the same core approach, resulting in a lack of substantial innovation.\n\nReferences\n[1] GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models\n[2] Training and Evaluating Language Models with Template-based Data Generation"}, "questions": {"value": "- Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iPl5bMwJ5E", "forum": "PoUdJ8bMDQ", "replyto": "PoUdJ8bMDQ", "signatures": ["ICLR.cc/2026/Conference/Submission1186/Reviewer_qxLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1186/Reviewer_qxLi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858386808, "cdate": 1761858386808, "tmdate": 1762915699872, "mdate": 1762915699872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AUTOT2T, a neuro-symbolic pipeline that converts math word problems into tabular reasoning tasks using SMT-based verification, and builds TabularGSM, a benchmark with Easy/Medium/Hard subsets plus a Trap subset for robustness (missing/contradictory information). The authors evaluate several models and report three core findings: (1) tabularization significantly degrades math performance, (2) retrieval and reasoning jointly drive difficulty, and (3) robustness to ill-posed tables is poor; even strong models hallucinate instead of abstaining. They also show targeted fine-tuning with on ~6k AUTOT2T-generated data  and achieve improved results on TabularGSM (+15%) ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Authors in this paper replace manual tabular annotation with a scalable, solver-verified generation process, preserving semantic correctness.\n\nAuthors also demonstrate measurable generalization beyond TabularGSM, suggesting AUTOT2T-generated data has reusable inductive value.\n\nThe proposed methods show an ~15% average gain on TabularGSM and around +2-3% on TabMWP/FinQA/TAT-QA using 6k GSM8K AUTOT2T samples.\n\nThis paper also demonstrates on Controlled RowAug/ColAug/OrdShf augmentations to assess retrieval vs. reasoning difficulty, not just overall accuracy.\n\nFinally, This paper also Introduces a “Trap” subsets (Missing/Contradictory) provides a meaningful safety lens for tabular reasoning."}, "weaknesses": {"value": "Core observations (retrieval bottlenecks, layout sensitivity) reaffirm prior TableQA limitations rather than reveal new phenomena. The headline observations (Observation 1 and 2) about tabular shortcomings (retrieval+reasoning bottlenecks; format effects) are largely known and unsurprising.\n\nBeyond “~6k from GSM8K,” there’s little guidance on how to configure AUTOT2T for data building (augmentation ratios, difficulty mix, domain targeting, per-dataset distribution, release details). Limited detail on AUTOT2T configuration augmentation ratios, difficulty sampling, and reproducibility guidelines.\n\nBuilt from GSM8K problems, the benchmark may not reflect messy, multi-table, domain-specific spreadsheets and raises potential contamination concerns for API models. GSM8K-derived problems may not reflect multi-table, domain-specific spreadsheet reasoning; contamination risk for API models remains.\n\nFixing 50% traps inflates the “unsolvable” prior; no sensitivity or calibration analysis over realistic 10–20% rates.  Real-world datasets would rarely have such a high unsolvable rate, so refusal calibration may be misleading. There is no reasoning on how calibrating this percentage affects performance and further fine-tuning. \n\nEvaluation focuses on serialized vs. Markdown; little discussion/experiments with other common encodings (CSV/HTML/JSON), schema randomization (header synonyms, unit perturbations), or stronger tests of row/column invariance beyond OrdShf. Also unclear why Markdown was chosen over other variants. No comparison to semantic parsing approaches (NL→SQL) that are strong on tables."}, "questions": {"value": "Can you please detail the composition of the ~6k AUTOT2T samples: augmentation type proportions (RowAug/ColAug/OrdShf/InfoMut), difficulty mix, and any domain targeting. How should practitioners configure AUTOT2T to cover diversity for a given use case? \n\nFor the reported +2–3% gains on TabMWP/FinQA/TAT-QA under mix-finetuning, can you break down which augmentation types/difficulties contributed most? Any learning curves vs. the amount/type of AUTOT2T data?\n\nHow do NL→SQL or programmatic (CSV+SQL/Pandas) baselines perform on your splits (including the robust setting)?\n\nGiven the GSM8K seed, what steps ensure no leakage for closed-source models, and how representative is TabularGSM of real, messy multi-table spreadsheets? Any experiments with non-GSM seeds?\n\nHow sensitive are your robustness results to different trap priors (e.g., 10–20%)? Can you report calibration/ROC-style analyses for “Unsolvable” detection?\n\nWhy prioritize Markdown over alternatives? Have you tried CSV/HTML/JSON or header/unit perturbations? How invariant are models to stronger row/column and schema changes beyond OrdShf?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UvqfKeiRjt", "forum": "PoUdJ8bMDQ", "replyto": "PoUdJ8bMDQ", "signatures": ["ICLR.cc/2026/Conference/Submission1186/Reviewer_Utwd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1186/Reviewer_Utwd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940242690, "cdate": 1761940242690, "tmdate": 1762915699754, "mdate": 1762915699754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a pipeline called AUTOT2T that converts math word problems (MWP) into tabular reasoning tasks. BAsed on this framework, they build TabularGSM, a  benchmark for tabular mathematical reasoning. The goals is to test reasoning complexity and robustness against incomplete or inconsistent data. Experiments on 18 LLMs show that tabular structures make reasoning harder due to the combined difficulty of retrieval and multi-step reasoning. Models frequently produce hallucinated answers when data are missing or contradictory. Fine-tuning models on their generated data improves accuracy and robustness by up to 15% on TabularGSM and 4% on related datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Data pipeline \n* The use of SMT-Lib and symbolic solvers ensures that the genareted tables are logically correct and consistent.\n* Pipeline automation removes the need for manual annotation, allowing large-scale dataset generation.\n* Multiple verification steps reduce the likelihood of ill-posed or inconsistent problems.\n* The pipeline can be adapted to various domains that require reasoning over tabular data.\n\nBenchmark \n* Diff difficulty levels (Easy–Hard–Trap) support more detailed analysis of reasoning complexity and robustness.\n* they explicitly integrate “trap” problems (missing and contradictory cases) to test models’ response to ill-posed inputs.\n* Experiments provide insights into model behavior under structured data constraints.\n\nExperiments \n* They evaluate the benchmarks across multiple model types (general, math, tabular).\n* Provde insights by clearly identifying retrieval mismatch as the dominant error cause.\n* Give comparison between Serialized and Markdown formats.\n* Novel robustness testing with explicit trap detection, addressing a neglected aspect of reasoning evaluation.\n* Quantitative analysis (ablations, trap comparisons, etc.) provides further insights into model weaknesses."}, "weaknesses": {"value": "Data pipeline \n* Strong reliance on LLM correctness; the semantic parsing and table conversion depend on accurate models, which could introduce errors that propagate through the pipeline.\n* The framework is focused on mathematical problems and may not generalize easily to qualitative or textual reasoning tasks.\n* The piepline work only for problems that can be mapped to tables with clear entities and attributes, limiting scope.\n* Lack of empirical validation for augmentation strategies.\n* Can you give more examples for each step of the pipeline? e.g. 10 examples in the appendix where we see how the data is generated/augmented stepwise for these ten examples.\n\nBenchmark \n* The benchmark is built solely on GSM8K-derived problems, potentially limiting topic diversity and domain generalization. Why didn;t you consider other benchmarks as well? \n* Traps and difficulty tiers are defined synthetically - how well do they reflect real-world inconsistencies?\n* Evaluation is based on static performance metrics, did you analyze the generated reasoning traces of models as well? \n* Typo in caption for table 3.\n\nExperiments \n* Results present accuracy metrics but not a deeper qualitative analysis of reasoning chains.\n* Trap design is synthetic and may not reflect real-world data inconsistencies.\n* Limited exploration of how reasoning format (e.g., throgh analysis of CoTs) affects performance in tabular settings.\n* The don't test if results generalise beyond math reasoning tasks to other structured data domains.\n* Since they dont provide a human baseline, its not clear if the low performance of models stems from task/data difficulty or errors in data generation."}, "questions": {"value": "See section weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tAB8JMYBEn", "forum": "PoUdJ8bMDQ", "replyto": "PoUdJ8bMDQ", "signatures": ["ICLR.cc/2026/Conference/Submission1186/Reviewer_HWiC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1186/Reviewer_HWiC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1186/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762249464263, "cdate": 1762249464263, "tmdate": 1762915699571, "mdate": 1762915699571, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}