{"id": "5scR5FEEIa", "number": 5959, "cdate": 1757948634371, "mdate": 1759897942354, "content": {"title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models", "abstract": "Recent advances in large-scale text-to-image diffusion models have heightened concerns about their potential misuse, especially in generating harmful or misleading content. This underscores the urgent need for effective machine unlearning, i.e., removing specific knowledge or concepts from pretrained models without compromising overall performance. One possible approach is Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models for targeted unlearning. However, LoRA often inadvertently alters unrelated content, leading to diminished image fidelity and realism. To address this limitation, we introduce UnGuide, a novel LoRA-guided model that controls the unlearning process. UnGuide modulates the guidance scale based on the stability of a few first steps of denoising processes. \nFor high-variance denoising trajectories, negative guidance is applied to stabilize sampling along the data manifold, while low-variance trajectories receive positive guidance to maintain fidelity. Empirical results demonstrate that UnGuide achieves controlled concept removal and retains the expressive power of diffusion models, outperforming existing LoRA-based methods in both object erasure and explicit content removal tasks.", "tldr": "UnGuide is an adaptive guidance-based method for machine unlearning in diffusion models that selectively removes targeted concepts while preserving image quality by balancing outputs from a base model and a LoRA adapter.", "keywords": ["Unlearning", "Diffusion Models", "LoRA"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de2b53a374d6c76a827ea717a8f98c1490f34cb4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new unlearning method using LoRA. Prior LoRA-based approaches tended to push the target concept outside the data manifold or degrade the model’s ability to generate non-erasing concepts. To address this, the authors clarify the mapping concept so that the content replacing the erased concept is explicitly defined, and leverage an AutoGuidance-inspired strategy to further enhance unlearning relative to the base model. Additionally, the method automatically determines whether the erasing target appears in the prompt and dynamically adjusts the guidance scale, improving both usability and performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Simple and intuitive writing:** The authors clearly identify limitations of existing methods and effectively address them via an automated dynamic guidance-scaling strategy. Applying guidance between the base model and the LoRA model and adjusting the scale dynamically is intuitive and yields strong results.\n- **Strong performance:** Under the benchmarks selected by the authors, the proposed method demonstrates distinctly superior performance over prior methods."}, "weaknesses": {"value": "- **Limited novelty:** The method largely appears to extend AutoLoRA[1] to an unlearning LoRA model and a base model, with dynamic guidance scaling being the primary novel contribution. This raises concerns about whether the level of novelty is sufficient.\n- **Dependence on mapping content:** As the authors note, avoiding the OoD manifold seems achievable because the mapping content offers a clear alternative for generation after erasing. However, this introduces strong dependence on mapping content, reduces diversity by collapsing the outcome into a single alternative, and imposes overhead on selecting a good mapping concept. While this might be manageable for settings like CIFAR-10 with few candidate classes, it is unclear how mapping content should be selected in a large-scale scenario. The paper provides no convincing rationale or criteria for this selection.\n- **Limited experimental scope:** SD-v1.4 is now considered outdated. It is unclear whether the method generalizes to SD-XL or more recent TTI models such as SD3 or FLUX. Evidence of applicability to these models would demonstrate greater generality. Additionally, CIFAR-10 has only 10 classes, making it a limited benchmark for concept erasing. Although the paper appears to adopt benchmarks used in MACE [2], MACE also includes erasing across ~200 celebrity identities; including such experiments would strengthen claims about scalability to numerous and diverse concepts.\n- **Motivation for AutoGuidance-based formulation:** The authors emphasize that borrowing the AutoGuidance structure improves stability. While it is understandable that it may help strengthen the unlearning effect relative to the base model, the rationale for why it specifically enhances stability is not clearly explained. Stronger motivation or justification is needed.\n- **Disorganized presentation:** While the writing itself is reasonably easy to follow, the figures are poorly placed and disrupt the flow. Reorganizing figure placement to follow the narrative more closely would greatly improve readability.\n\n[1] AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning, Zhang et al., 2024\n\n[2] MACE: Mass Concept Erasure in Diffusion Models, Lu et al., 2024"}, "questions": {"value": "In line 134, “,” should be replaced with “.” as the end of a sentence."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dRNebzys8b", "forum": "5scR5FEEIa", "replyto": "5scR5FEEIa", "signatures": ["ICLR.cc/2026/Conference/Submission5959/Reviewer_Y2JL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5959/Reviewer_Y2JL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746428906, "cdate": 1761746428906, "tmdate": 1762918372810, "mdate": 1762918372810, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author's propose UnGuide, a method for improving LoRA based unlearning methods in diffusion models. In particular, the authors address how base LoRA methods inadvertently alter content unrelated to the desired erased concepts. To do this, they adaptively switch between the base and LoRA models during inference: boosting the LoRA model on prompts related the erased concepts, and boosting the base model on prompts unrelated to the erased concepts. They provide experiments showing their method achieves better targeted removal than other LoRA-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The experimental results are strong, showing clear advantages over prior methods\n- The method is also much simpler than the strongest prior methods, as it does not require external segmentation components."}, "weaknesses": {"value": "- The paper lacks ablations comparing their method to a base LoRA approach i.e. without using un-guidance. The authors state the key component of the work is the dynamic switching between the base and LoRA models, but at least as I can see this is not ablated in the experiments. It would greatly strengthen the insights obtained from the paper if this could be included (or make it more prominent in case I missed it accidentally)\n- I also could not find an explicit formula for w. Is it just a binary switch between 1 and -1 depending on which side of the threshold it is?\n- Table 8 provides average values for the norms of the deltas, but distribution-level results should also be provided i.e. what is the error rate of the test?\n- Lastly, that I can see the paper also lacks ablations for adversarially chosen prompts.\n\nWhile my score recommends reject, I would be happy to raise it if these ablations and my questions below are addressed."}, "questions": {"value": "- Would it be possible to include ablations as mentioned in the weaknesses question? \n- I am also confused as to how UnGuide avoids distorting unrelated concepts. If I understood it right, the switching between the base and LoRA models is controlled by the degree to which they disagree on the outputs. Thus, if the LoRA model wrongly alters an unrelated concepts, causing the two models to disagree, UnGuide would still boost the LoRA model.\n- What is the exact formula for w? Does it depend on the time-step or is it fixed for all generation steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tcToea0xrG", "forum": "5scR5FEEIa", "replyto": "5scR5FEEIa", "signatures": ["ICLR.cc/2026/Conference/Submission5959/Reviewer_uC8E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5959/Reviewer_uC8E"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767908397, "cdate": 1761767908397, "tmdate": 1762918372568, "mdate": 1762918372568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UnGuide, a LoRA-guided diffusion unlearning method that adaptively balances outputs from a pretrained base model and a LoRA adapter. Unlike prior LoRA fine-tuning that unintentionally harms unrelated concepts, UnGuide dynamically adjusts a guidance scale w based on the early denoising variance. High-variance trajectories (indicating the presence of the forgotten concept) receive negative guidance to steer sampling back to the data manifold, while low-variance ones use positive guidance to preserve fidelity.\nExperiments on object erasure and explicit content removal show that UnGuide selectively removes targeted concepts with minimal degradation in visual quality, outperforming existing LoRA-based unlearning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The work is original in its formulation of adaptive guidance for unlearning. While prior methods such as MACE rely on complex prompt or segmentation modifications, UnGuide innovatively combines a standard LoRA fine-tuning setup with a new variance-based guidance mechanism to dynamically balance between forgetting and fidelity."}, "weaknesses": {"value": "The paper presents an interesting direction but suffers from several issues in organization, clarity, and experimental rigor that obscure its true contributions and weaken its overall impact.\n\n* **Poor organization and unclear flow:** The paper’s structure makes it difficult to distinguish between background material and novel contributions. For instance, the description of the *Text-to-Image generation framework*—a preliminary concept—is embedded directly in the *Methodology* section, and is immediately followed by *LoRA for Unlearning* without clear separation. As a result, it is hard for readers to identify where the authors’ original ideas begin. Even within the LoRA section, preliminaries and proposed components are intermixed, and the objective of LoRA training is never clearly introduced before detailing its implementation specifics (e.g., use of predefined target prompts, reliance on the model’s intrinsic capabilities, and updates restricted to Key/Value matrices). The section starting from L240 could be significantly improved by clearly stating the **training objective first**, followed by implementation details.\n\n* **Ambiguity in novelty claim:** The LoRA training procedure itself is conceptually equivalent to *distilling negative prompt guidance* into a student model, an idea already explored in **ESD [1]** and related works. Those methods fine-tuned the full model rather than using LoRA adapters, but this distinction is largely an implementation convenience rather than a novel algorithmic contribution. However, the paper frames this distillation-based objective as an original idea, which overstates its novelty.\n\n* **Unclear and computationally expensive adaptive guidance:** The paper’s main claimed contribution—the *adaptive inference-time guidance*—relies on repeatedly computing the difference between noise predictions of the base and unlearned models to determine whether a prompt contains a forgotten concept. This process requires averaging over 10–30 stochastic samples, which can increase inference cost by an order of magnitude. The paper does not convincingly demonstrate that this heavy computation yields proportional gains in quality or controllability. Furthermore, when (w > 1) (i.e., the prompt is judged not to contain the forgotten concept), it is unclear why extrapolating **away from** the unlearned model’s predictions is necessary—logically, the base model alone should suffice. The motivation for this design choice is missing.\n\n* **Inconsistency in guidance formulation:** The authors state that in practice (w = -1) or (w = 2) (L649), even though Eq. (5) defines (w = 0.5) as the balanced midpoint between models. Therefore, the earlier notation (w < -1) or (w > 1) (L282, L291) is inconsistent with the center defined at 0.5. It would be more natural to express the condition as (w < -1) or (w > 2) to align with the intended midpoint. This inconsistency makes it hard to interpret how (w) influences the final prediction.\n\n* **Lack of analysis for key scaling parameters (γ and w):** In diffusion guidance literature, the guidance scale is known to have a major impact on sample quality—too high causes oversaturation, too low leads to weak effects. Most prior works analyze performance sensitivity to this scale. However, the paper provides no ablation or sensitivity analysis for its two main scaling parameters, (γ) (training repulsion strength) and (w) (inference guidance weight). Without such analysis, it is difficult to assess the stability or robustness of the method.\n\nOverall, the work introduces an interesting adaptive idea but would benefit greatly from a clearer exposition of its contributions, justification for design choices, and more systematic experiments—especially around guidance scale behavior and computational efficiency.\n\n[1] Erasing Concepts from Diffusion Models, Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, David Bau"}, "questions": {"value": "**Questions:**\n\n- Is there a clear justification for why the model needs to interpolate between the unlearned model and the base model at inference time? Why not simply use one of them depending on whether the prompt contains the forgotten concept?\n- Why is extrapolation beyond the base model ((w > 1)) necessary when the prompt does not contain the forgotten concept? Would simply using the base model in such cases yield similar or better results?\n- What is the computational overhead of averaging over 10–30 stochastic samples to estimate LoRA influence? Could a more efficient proxy be used without sacrificing adaptivity?\n- How sensitive is the method to the choice of the key scaling parameters (γ) (training repulsion strength) and (w) (guidance weight)? An ablation or stability analysis could strengthen the claims.\n- Can the authors better distinguish their LoRA training objective from prior negative-guidance distillation methods such as ESD [1]? What conceptual or empirical improvement does UnGuide introduce beyond replacing full fine-tuning with LoRA adapters?\n- Can the authors provide an ablation study comparing results when: (a) only the base model is used, (b) only the unlearned model is used, and (c) their proposed adaptive guidance is applied, to show the quality and unlearning trade-off?\n\n---\n\n* **My general take:** The idea of removing unwanted influence from prompts unrelated to the unlearned concept is interesting, but the current method feels underdeveloped. The central question is whether the cost of repeatedly estimating model differences can be justified — either by improving its efficiency or by demonstrating that the benefit is significant enough to warrant this cost. Showing an ablation that quantifies the efficiency and necessity of guidance between the unlearned and base models (the paper’s core contribution) would be essential. I would welcome clarification if I have misunderstood this aspect, and a convincing explanation or experiment could positively affect my rating."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LcmNZ3Nqqt", "forum": "5scR5FEEIa", "replyto": "5scR5FEEIa", "signatures": ["ICLR.cc/2026/Conference/Submission5959/Reviewer_K6aQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5959/Reviewer_K6aQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904429093, "cdate": 1761904429093, "tmdate": 1762918372339, "mdate": 1762918372339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a LoRA-guided model for controlling the unlearning process, dubbed UnGuide. The approach (i) trains LoRA adapters to forget a target concept $c$ by assigning it a matching concept $c_m$, and (ii) applies an adaptive control rule at the inference stage that mixes predictions from the baseline and LoRA-adapted models depending on the stability of noise suppression in early steps. Empirically, UnGuide is evaluated on object deletion (CIFAR-10) and explicit content removal (NSFW) (I2P) and exhibits lower NSFW detection rates than most baselines while maintaining reasonable FID/CLIP scores on MS-COCO dataaset."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors reframe the removal of the specific concept as LoRA-based adaptation with a linear target in the noise prediction space, rather than as architectural changes or prompt rewrites. This leads to a simple but effective unlearning framework.\n- The new metric is introduced to evaluate the performance of unlearning. It is referred to as the harmonic mean of effectiveness, specificity, and generality, and is meaningful and clear.\n- UnGuide enables controllable concept erausre across different datasets, while CLIP/FID remains competitive compared to conventional algorithms.\n- The paper is well-written and easy to understand"}, "weaknesses": {"value": "- The main concern is the insufficiently justified design of the loss function for unlearning. The objective pushes the noise prediction of the LoRA-guided model for the forbidden concept c toward a linear combination of the predictions of the original model, as given by eq. (4). In the article, the authors do not explain why this geometry in the noise space should be optimal for unlearning, nor why linearity (as opposed to other divergences or constraints) is appropriate. \n- The prompt-conditional guidance is based on a norm gap statistic of early diffusion steps and seeds, compared to a neutral prompt reference, and then switches to negative/positive guidance accordingly. The paper includes some visualizations in the appendix, but there is little quantification of false positives/negatives or stability across time steps.\n- The visualizations are informative, but there is a lack of analysis when the method fails: e.g., in the case of excessive suppression of harmless attributes or semantic drift for non-targets. Furthermore, as shown in Figures 16 to 26, the proposed UnGuide often produced oversaturated images, which is generally considered a limitation of guidance-based image generation."}, "questions": {"value": "- How did you choose $\\gamma$ and how sensitive are the results to it?\n- What is the compute overhead of the multi-seed probing per prompt?\n- Please clarify whether training separate adapters corresponds to training a single adapter with multi-target prompts. A brief experiment could be decisive for practical application."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MoabDY1GDK", "forum": "5scR5FEEIa", "replyto": "5scR5FEEIa", "signatures": ["ICLR.cc/2026/Conference/Submission5959/Reviewer_ZPuh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5959/Reviewer_ZPuh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5959/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919150711, "cdate": 1761919150711, "tmdate": 1762918372074, "mdate": 1762918372074, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}