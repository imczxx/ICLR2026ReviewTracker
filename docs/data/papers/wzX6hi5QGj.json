{"id": "wzX6hi5QGj", "number": 10354, "cdate": 1758167982028, "mdate": 1759897656526, "content": {"title": "Offline Reinforcement Learning via Action-Space Pseudo-Labeling", "abstract": "he critical challenge of offline reinforcement learning (offline RL) is improving\nfrom a fixed dataset while avoiding overestimation on out-of-distribution (OOD)\nactions. Existing methods typically regularize the learned policy to avoid choosing overestimated OOD actions. However, we argue that this often over-constrains policy improvement or requires sensitive hyperparameter tuning. We restate this challenge as the absence of explicit training signals for the\nvalue function in parts of the state‚Äìaction space. A more effective approach is to provide explicit training signals across the entire action space to eliminate overestimation. We introduce a surprisingly simple yet effective method: $\\textbf{Action-Space Pseudo-Labeling (ASPL)}$ to resolve this challenge. It completes the value-function‚Äôs\nmissing signals by assigning pseudo Q-targets that decrease with distance from\nthe behavior support (i.e., the support of the behavior policy). In practice, ASPL achieves an implicit behavior-aware regularization that strengthens as behavior likelihood decreases. On D4RL datasets,\nwe observe stable training and consistent improvements over strong offline baselines with minor tuning burden. Code for reproducing the experiments is provided in the supplementary material.", "tldr": "We propose Action-Space Pseudo-Labeling (ASPL), a simple method that regularizes offline RL with pseudo-labels for unseen actions, yielding stable and consistent gains without fragile tuning.", "keywords": ["offline reinforcement learning", "pseudo-labeling"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6ebaa0dd1a392d50b7d2e901596fa2a81b47f3b.pdf", "supplementary_material": "/attachment/ba2d4f9059d89a0d4b6ee8347b9b0b6769c411c2.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose Action-Space Pseudo-Labeling (ASPL), a simple method that assigns decreasing pseudo Q-targets to actions based on their distance from the behavior action."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Its primary strength lies in proposing a value regularization method, Action-Space Pseudo-Labeling, which directly mitigates overestimation for out-of-distribution actions."}, "weaknesses": {"value": "1. The novelty of the proposed method is limited. The core idea of applying pseudo-labeling to OOD actions is already established in prior works like MCQ, SVR, and OAC-BVR. The paper lacks a compelling justification for why the specific design of ASPL is superior to these existing approaches.\n2. There appears to be a potential error in Equation (6), which warrants clarification from the authors.\n3. By applying an explicit penalty based on the distance, the proposed method introduces a bias estimation that may also propagate to in-distribution state-action pairs through the Bellman backup. This could systematically depress Q-values across the entire state-action space. The authors should provide Q-value curves for all tasks to investigate this potential issue.\n4. The experimental evaluation is insufficient, as it is conducted solely on the Gym domain. For a comprehensive assessment in offline RL, it is essential to include results on more challenging benchmarks, such as the AntMaze domain, which tests an algorithm's ability to perform long-horizon tasks.\n5. There is a significant discrepancy in the reported results for the baseline method SCAL on the halfcheetah-medium-expert task (44.0 in this work vs. 91.7 in the original paper). This major inconsistency casts doubt on the integrity of the experimental setup and the validity of the comparative analysis.\n6. While the authors cite the most relevant prior works (MCQ and SVR), they conspicuously omit direct comparisons with them. This lack of comparison with state-of-the-art methods that share a similar design philosophy is a critical omission and prevents a fair assessment of the method's contribution.\n7.  Given the final performance of ASPL as reported, its overall effectiveness is questionable. A simple comparison with the reported performance of MCQ and SVR would likely confirm this performance gap.\n8. The experimental results report only mean performance without the corresponding standard deviations across multiple seeds. Reporting the variance is essential to properly assess the significance and reliability of the results.\n9. The use of a batch size of 512 deviates from the community-standard value of 256 used in most prior works for fair comparison. The authors must justify this choice and provide an ablation study."}, "questions": {"value": "Please see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ud1q5CtIpN", "forum": "wzX6hi5QGj", "replyto": "wzX6hi5QGj", "signatures": ["ICLR.cc/2026/Conference/Submission10354/Reviewer_gcNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10354/Reviewer_gcNK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637797075, "cdate": 1761637797075, "tmdate": 1762921682539, "mdate": 1762921682539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Action-Space Pseudo-Labeling (ASPL) for offline RL. ASPL augments standard actor‚Äìcritic training by sampling additional actions for each state from the entire action space and assigning pseudo Q-targets that decay with distance to the behavior support. These pseudo-labeled state‚Äìaction pairs are mixed with logged behavior data to train the critic, aiming to mitigate OOD overestimation without introducing explicit policy constraints or auxiliary networks. The implementation is TD3-based; the actor update remains unchanged. The authors report stable training and consistent gains over strong offline baselines on D4RL Gym‚ÄìMuJoCo."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Clear, practical motivation. Frames OOD action value extrapolation as a ‚Äúmissing supervision‚Äù issue and fills it via pseudo-labels in the action space.\n\nLow engineering overhead. Only the critic objective is modified; the actor and architecture remain simple‚Äîno explicit policy constraints or additional networks."}, "weaknesses": {"value": "Incremental novelty / positioning.\nConcept overlaps with conservative value shaping and behavior-regularized approaches. The paper should more rigorously differentiate ASPL from CQL/XQL/IVR-style methods using a clearer formal comparison and targeted ablations.\n\nBaseline scope.\nTo substantiate broad claims, comparisons to additional SOTA offline RL methods beyond the immediate TD3-family/popular baselines would strengthen the case.\n\nBehavior-support distance estimation.\nThe core mechanism hinges on how ‚Äúdistance to behavior support‚Äù is estimated and scaled. More analysis of bias/variance, dimensionality effects, and sensitivity to the scaling constant is needed.\n\nExternal validity / harder settings.\nEvidence is limited to D4RL Gym‚ÄìMuJoCo. Including harder OOD regimes (e.g., AntMaze, Adroit, Kitchen, or random datasets) would better validate the central claim in challenging settings.\n\n#Minor Concerns\n\nFigure clarity. The method diagram should explicitly show that pseudo-targets monotonically decay with behavior-distance and how they combine with the Bellman target.\n\nEvaluation protocol upfront. Consolidate seeds, episode counts, and final-N evaluation aggregation in the main text (not only in the appendix).\n\nCompute/resource reporting. Since ASPL samples extra actions, report training time / memory vs. pseudo-sample budget \nùëÅ"}, "questions": {"value": "How is distance to the behavior support estimated in practice, and how sensitive are results to the scaling constant and normalization?\n\nWhat is the trade-off between pseudo-sample budget ùëÅ, wall-clock time, and returns? \n\nDoes integrating ASPL with IQL/CQL/XQL critics still help when those methods already reduce OOD overestimation?\n\nHow robust is uniform/LHS sampling for high-dimensional actions? Would learned proposal distributions (e.g., uncertainty-aware) improve coverage vs. cost?\n\nAny failure cases when behavior coverage is extremely narrow? How does ASPL behave near the support boundary?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yAYcaxnvnn", "forum": "wzX6hi5QGj", "replyto": "wzX6hi5QGj", "signatures": ["ICLR.cc/2026/Conference/Submission10354/Reviewer_BypT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10354/Reviewer_BypT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896975104, "cdate": 1761896975104, "tmdate": 1762921681960, "mdate": 1762921681960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the overestimation of out-of-distribution (OOD) action values caused by distribution shift in (Offline RL), this paper proposes the Action Space Pseudo-Labeling (ASPL) method. Its core idea draws an analogy to semi-supervised learning: it provides explicit supervision for the entire action space and assigns \"distance-aware pseudo-Q targets\" to state-action pairs randomly sampled from the entire action space."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper features a clear logical flow, rigorous expression, and effective experimental visualizations."}, "weaknesses": {"value": "1. The experiments are insufficient, as they only cover Gym environments and lack more complex ones such as Antmaze, maze, and kitchen.\n2. The method proposed in the paper falls into the category of conservative Q-values. By subtracting $d(a,\\bar{a})$, it shows limited overall innovation and exhibits little difference from CQL.\n3. There is a lack of ablation studies on fixed $\\omega_{\\beta}$ and adaptive weights."}, "questions": {"value": "please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZQ5dilDfWJ", "forum": "wzX6hi5QGj", "replyto": "wzX6hi5QGj", "signatures": ["ICLR.cc/2026/Conference/Submission10354/Reviewer_tqHU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10354/Reviewer_tqHU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898323523, "cdate": 1761898323523, "tmdate": 1762921681435, "mdate": 1762921681435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of offline reinforcement learning (offline RL), particularly in overcoming the issue of overestimating values for out-of-distribution (OOD) actions when training from a fixed dataset. It introduces the Action-Space Pseudo-Labeling (ASPL) method, which assigns pseudo Q-values to randomly sampled actions in the state‚Äìaction space, providing training signals for actions outside the behavior support. This approach mitigates the overestimation problem by using distance-aware pseudo-labels that decay with the distance from the behavior support. The method is shown to outperform existing offline RL methods with minimal tuning on various benchmarks, including D4RL Gym‚ÄìMuJoCo tasks.\n\nThe paper proposes a novel and practical solution to the overestimation problem in offline RL through ASPL, an effective method that augments training with pseudo-labels for OOD actions. The experimental results on D4RL benchmarks demonstrate that ASPL performs\ncompetitively with minimal hyperparameter tuning. However, while the method is shown to work well empirically, the theoretical foundations could benefit from further elaboration, particularly regarding the behavior-aware weighting mechanism. Additionally, a more thorough analysis of the method's generalization across diverse RL tasks would strengthen the claims. The paper provides solid empirical evidence but lacks sufficient theoretical grounding in some areas."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Simple and effective solution to overestimation in offline RL:\n1. The ASPL method tackles the challenge of sparse coverage in the action space, effectively providing training signals for OOD actions (Sec. 4.1; Eq. 6). This addresses a critical issue in offline RL and improves performance without adding complexity to existing actor‚Äìcritic pipelines.\n2. The integration of pseudo-labeling into the critic update is minimalistic yet effective, requiring no additional actor-side constraints or auxiliary networks (Sec. 4.3). This simplicity enhances the method's appeal for real-world applications where computational efficiency is important.\n\nEmpirical validation on D4RL benchmarks:\n1. ASPL consistently outperforms several strong offline RL baselines, including TD3+BC, CQL, and IQL, across various D4RL Gym‚ÄìMuJoCo tasks (Sec. 5.1; Table 1). These results demonstrate the method's robustness and its ability to achieve stable training and competitive performance with minimal hyperparameter tuning.\n2. The paper includes detailed experiments showing the sensitivity of ASPL to key hyperparameters like the number of random actions sampled (N) and the pseudolabeling coefficient (Œ±), highlighting its resilience to hyperparameter changes (Sec.5.2; Fig. 3, Fig. 4).\n\nBehavior-aware weighting mechanism:\n1. The dynamic adjustment of pseudo-label weight based on behavior coverage (Eq. 9) is a unique and promising feature. This ensures that updates are Bellman-dominated in well-supported regions and pseudo-label-dominated in unsupported regions, improving stability and mitigating extrapolation errors (Sec. 4.2).\n2. The simplicity of the behavior-aware weight mechanism‚Äîdecreasing the weight with increasing behavior coverage‚Äîmakes the method both effective and easy to integrate into existing RL pipelines (Sec. 4.2).\n\nMinimal tuning burden:\n1. ASPL reduces the sensitivity to hyperparameters, particularly the pseudo-label weight Œ±, which simplifies the tuning process compared to other methods (Sec. 5.2). This is a significant advantage for practitioners who need reliable methods with minimal configuration effort."}, "weaknesses": {"value": "Limited theoretical analysis of the behavior-aware weight mechanism:\n1. While the paper introduces a behavior-aware weight (Eq. 9), the theoretical explanation and formal analysis of this weight mechanism are not fully developed.\n2. The implications of this dynamic weighting, especially in complex environments, could be better justified with more rigorous theoretical analysis (Sec. 4.2). No direct evidence found in the manuscript.\nThe paper could benefit from a more detailed explanation of why the specific form of distance-aware pseudo-labeling works well across various tasks and datasets. Whilethe empirical results are strong, further formalism in the explanation would add to the method's credibility (Sec. 4.2).\n\nComparative analysis with more baselines:\n1. Although the paper compares ASPL against several state-of-the-art offline RL algorithms, the set of baselines could be expanded to include more diverse approaches, especially methods that use ensemble critics or model-based planning (Sec. 5.1). This would provide a more comprehensive comparison and further demonstrate ASPL's advantages.\n2. The paper mainly focuses on tasks in the Gym‚ÄìMuJoCo environment. Evaluating ASPL on a broader range of tasks, including those with more complex action spaces or other domains like NLP or robotics, would offer insights into the generalizability of\nthe method (Sec. 5.1). No direct evidence found in the manuscript.\n\nInsufficient discussion of generalization to other action spaces:\n1. While the method performs well in continuous action spaces, its applicability to discrete or mixed action spaces is not fully addressed (Sec. 6). A more detailed discussion on how ASPL could be adapted to different action spaces or other types of\ndecision-making environments would be valuable, particularly for applications beyond Gym‚ÄìMuJoCo tasks.\n\nClarifications needed for pseudo-labeling process:\n1. The pseudo-labeling process involves sampling random actions from the action space and using distance-based Q-values as pseudo-targets. However, the handling of these pseudo-labels could be explained more clearly, particularly regarding the distance metric and its effects on training dynamics (Eq. 7). Further clarification of how these pseudo-labels interact with the Bellman backup process would improve understanding and reproducibility (Sec. 4.1)."}, "questions": {"value": "Provide deeper theoretical analysis of the behavior-aware weight:\n1. Elaborate on the formal properties of the behavior-aware weight mechanism and its effects on training stability and performance. A more thorough mathematical analysis could clarify the behavior-aware weight's role and its relationship with other regularization techniques (Sec. 4.2).\n2. Consider formalizing the convergence properties of ASPL to help justify its effectiveness and robustness.\n\nExpand baseline comparisons:\n1. Include additional baselines that use ensemble critics, model-based approaches, or other advanced techniques in offline RL. This will provide a clearer understanding of how ASPL compares to a broader range of methods (Sec. 5.1).\n2. Also, consider evaluating ASPL on tasks with more complex action spaces, such as discrete action tasks or multi-agent settings, to explore its generalization beyond continuous action spaces (Sec. 5.1).\n\nAddress generalization to other action spaces:\n1. Discuss and provide potential extensions of ASPL for discrete or mixed action spaces. A clear discussion on how the method can be adapted for other action representations, such as categorical or structured actions, would help broaden its applicability (Sec. 6).\n\nClarify the pseudo-labeling process:\n1. Provide a more detailed explanation of the pseudo-labeling process, particularly the distance metric used to calculate the pseudo-Q targets. Clarifying how the pseudolabeling interacts with the Bellman backup would help in understanding its impact on training dynamics (Sec. 4.1)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3Pu1o4vvqL", "forum": "wzX6hi5QGj", "replyto": "wzX6hi5QGj", "signatures": ["ICLR.cc/2026/Conference/Submission10354/Reviewer_iaNB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10354/Reviewer_iaNB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10354/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934068567, "cdate": 1761934068567, "tmdate": 1762921680917, "mdate": 1762921680917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}