{"id": "wOubHRUZQU", "number": 16677, "cdate": 1758267551024, "mdate": 1763485565033, "content": {"title": "Adaptive Prototype Learning: Unlocking Intrinsic Features for Texture Recognition", "abstract": "State-of-the-art texture recognition models often rely on cumbersome external memory banks and complex training pipelines. We challenge this paradigm by proposing a simple yet powerful alternative: learning from the rich intrinsic patterns within each image itself. We introduce STP-Former (Simple Texture Prototype Transformer), an architecture that dynamically distills a compact set of intrinsic prototypes for each input sample. A lightweight cross-attention module, the Texture Prototype Extractor (TPE), learns to identify and aggregate an image's most representative texture primitives on-the-fly. These adaptive prototypes, inherently aligned with the input's context, form a powerful basis for robust classification.Our contributions are twofold. First, we propose a decoupled two-stage training strategy where the TPE is pre-trained using a self-supervised objective to capture fundamental texture representations before a classifier is fine-tuned. Second, to endow the learned feature space with a robust geometric structure, we introduce a novel Supervised Topological Loss. Grounded in persistent homology, this objective directly optimizes for intra-class compactness and inter-class separation, pushing the boundaries of discriminability. This synergistic framework yields a remarkable performance leap; on the challenging DTD benchmark, STP-Former improves accuracy from 79% to over 86%. Our work demonstrates that an adaptive, self-contained approach provides a more effective and efficient paradigm for texture recognition.", "tldr": "", "keywords": ["Texture Recognition", "Adaptive Learning", "Intrinsic Features", "Prototype Learning", "Vision Transformer"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4d73e36eb33f95e3d0c150a18633abae6ca1b67.pdf", "supplementary_material": "/attachment/32fc9c760fb5262cc05ff25e29f1fbbc0262d776.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a representation for texture recognition. This builds on a pre-trained image encoder like ResNet50 or DINOv2 and adds additional invariance by pooling the resulting features by means of a small number of learnable queries, via cross-attention. The modified representation is pre-initialised by maximising the coverage of the queries’ responses using an initial set of features extracted from the data, in a label-agnostic and thus self-supervised manner. This pre-initialisation phase is shown to be helpful for the final model performance: compared to end-to-end supervised fine-tuning, it helps with overfitting. The final classifier, which utilises this representation, is further regularised via a heuristic inspired by “topological data analysis”. The latter is similar to a contrastive loss where pulling and pushing relations are established by the class labels as well as a “topological clustering” algorithm. The method performs well when stacked against other approaches for texture classification. Ablations show that each of the proposed components brings some benefit to the final performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The method’s performance on standard texture recognition datasets is solid, at least compared to other texture recognition methods.\n\n* The motivation for the different components in the method is clear enough.\n\n* The method is based on a small transformer layer which is trained ad hoc.  It is kind of surprising that this can boost the performance of modern features like DINOv2, which are already based on transformers and can do much of what this encoding can already, via self-attention. It is likely that much of the benefit comes from regularisations and bottlenecks that allow the model to train effectively from the relatively small training sets available in the texture datasets."}, "weaknesses": {"value": "* Perhaps the most significant limitation of this paper is its potential for impact. The community has largely moved past problems as specific as texture recognition. The latter can be seen as a special case of image classification, which in turn has been largely subsumed by vision-language models. This is not to say that there is no value in further investigating textures, but it is unclear how such ultra-specialised techniques would stack against established modern image classifiers.\n\n* The topological loss (Section 4.3) is not assessed in depth. What would happen if one were to ignore the topological constraint in equations 6 and 7 and simply sum over all pairs (with some balancing between terms)?\n\n* The approach is based on ad-hoc designs, the Texture Prototype Extractor, which, in the end, amounts to pooling some features using cross-attention and a small set of learnable queries (i.e., the prototypes). I can see how this could bring some invariance to the representation, but then again any modern feature encoder, including the ones used here as backbone, would be able to come up with similar strategies automatically. Table 2 suggests that this is not necessarily the case as TPE does improve on top of DINO. Then again, the main benefit comes from the additional losses and two stage learning approach, which are forms of regularisation. Perhaps this is the true reason for the improved performance."}, "questions": {"value": "* Why do you pool the queried features again in eq 5? I would imagine that keeping the stacked feature vectors would be more informative.\n\n* In section 4.2.1, do you need to fine-tune all the last layers of the DINO network, or would it be enough to tune the queries, perhaps introducing as usual tuneable KV matrices?\n\n* See the question above about ablating the topological constraints in the loss.\n\n* While the ablation in table 2 captures most of the key ingredients of the method, it is additive. It would be interesting to also do subtractive tests too. For instance, what if the gather loss is maintained, and the TPE block is replaced by simple averaging?\n\n* Table 2 seems to suggest that most of the advantage of the method comes from using a two-stage training approach (rows D and E) and the \"gather loss\" (row C), which could be seen as forms of regularisation. This can make sense because the texture datasets are relatively small by today's standards. Answering the question immediately above could also help to answer this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "b6U5Ty8huv", "forum": "wOubHRUZQU", "replyto": "wOubHRUZQU", "signatures": ["ICLR.cc/2026/Conference/Submission16677/Reviewer_A3H5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16677/Reviewer_A3H5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761321305768, "cdate": 1761321305768, "tmdate": 1762926734567, "mdate": 1762926734567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STP-Former (Simple Texture Prototype Transformer), a texture recognition framework that learns intrinsic prototypes directly from each image instead of relying on external memory banks or static prototypes. The method addresses the feature misalignment problem by proposing a Texture Prototype Extractor (TPE) trained in a self-supervised stage using a Gather Loss to distill representative texture primitives, followed by a supervised stage with a novel Topological Loss based on persistent homology to enforce intra-class compactness and inter-class separation. Extensive experiments on six benchmark datasets (DTD, FMD, MINC, GTOS, Fabrics, and KTH-TIPS2b) demonstrate consistent state-of-the-art performance, validating the effectiveness of combining intrinsic prototype learning and topological regularization for robust and efficient texture representation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tMoves from static memory-based feature comparison to per-image intrinsic prototype extraction, directly addressing feature misalignment problems in texture recognition.\n\n-\tTopological loss grounded in persistent homology provides a principled way to enforce geometric structure in feature space.\n\n-\tSix datasets, multiple backbones, and detailed ablations (on TPE, Gather Loss, two-stage training, and Topo Loss) provide solid empirical validation.\n\n-\tDetailed pseudocode and ablation tables show strong engineering effort.\n\n-\tBoth base and topologically-regularized variants outperform all prior methods, even with lightweight backbones."}, "weaknesses": {"value": "-\tThe topological loss is intuitively motivated but lacks formal gradient or stability analysis. No discussion on computational complexity of persistent homology.\n\n-\tExperiments are confined to texture datasets; it is unclear whether the proposed intrinsic prototype mechanism generalizes to object or scene recognition.\n\n-\tThe Gather Loss and Topological Loss are evaluated jointly, but deeper decoupled comparisons (e.g., Gather + CE vs. Gather + Triplet vs. Gather + Topo) are missing.\n\n-\tAlthough Appendix A.4 includes FLOPs, persistent homology’s batch-level cost could limit large-scale scalability; this is not discussed."}, "questions": {"value": "-\tHow sensitive is the topological loss to hyperparameters? Would larger λ values lead to unstable training?\n\n-\tCould the Gather Loss be replaced by a clustering-based objective (e.g., Sinkhorn-KMeans or VICReg-style redundancy reduction)?\n\n-\tHow does the method behave under domain shift (e.g., unseen texture domains)?\n\n-\tPersistent homology computation can be expensive — is there any approximation or batching trick used for scalability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kffAWSWdol", "forum": "wOubHRUZQU", "replyto": "wOubHRUZQU", "signatures": ["ICLR.cc/2026/Conference/Submission16677/Reviewer_oD3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16677/Reviewer_oD3B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830904888, "cdate": 1761830904888, "tmdate": 1762926734188, "mdate": 1762926734188, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an STP-Former (Simple Texture Prototype Transformer) architecture for texture recognition, aimed at solving the problem of feature misalignment. Its core is to dynamically extract intrinsic prototypes from each input image through a Texture Prototype Extractor (TPE), and adopt a decoupled two-stage training strategy (the first stage pre-trains TPE with a self-supervised Gather Loss, and the second stage trains the classifier), while introducing a Supervised Topological Loss to optimize the geometric structure of the feature space."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The figures and tables are clear and easy to understand.\n2. Formulas are well used to help readers better understand the concepts."}, "weaknesses": {"value": "1. The idea of using Adaptive Prototype Learning to perform discriminative visual tasks has been proposed in other papers [1-3], which raises the concern about the lack of novelty.\n2. In section 4.3, the description of theoretical foundation of Topological Loss does not contribute to understanding the methods used in this paper, but rather increases confusion.\n3. The citation format of most of the literature in this paper is incorrect, for example, “Liu et al. (2019)” in line 35 should be revised to “(Liu et al., 2019)”.\n[1] Li G, Jampani V, Sevilla-Lara L, et al. Adaptive prototype learning and allocation for few-shot segmentation. CVPR 2021.\n[2] Heidari M, Alchihabi A, En Q, et al. Adaptive Parametric Prototype Learning for Cross-Domain Few-Shot Classification. AISTATS 2024.\n[3] Ma C, Donnelly J, Liu W, et al. Interpretable Image Classification with Adaptive Prototype-based Vision Transformers. NeurIPS 2024."}, "questions": {"value": "1. What are the significant differences between the method proposed in this paper and existing methods based on Adaptive Prototype Learning [1-3]?\n2. L_topo is composed of L_intra and L_inter，what are their respective impacts on model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4y6Hd3SALQ", "forum": "wOubHRUZQU", "replyto": "wOubHRUZQU", "signatures": ["ICLR.cc/2026/Conference/Submission16677/Reviewer_tESo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16677/Reviewer_tESo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762069792631, "cdate": 1762069792631, "tmdate": 1762926733461, "mdate": 1762926733461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces STP-Former, a novel framework for texture recognition that addresses the feature misalignment problem prevalent in methods relying on static, pre-compiled knowledge from a training set."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper is clearly written and easy to follow.\n2.The method achieves new state-of-the-art results across six standard texture recognition benchmarks and makes a thorough ablation study.\n3.The proposed Supervised Topological Loss is a novel and effective application of persistent homology. Instead of using it as a simple regularizer to preserve structure, the authors employ it as a direct supervised objective to actively shape the feature manifold."}, "weaknesses": {"value": "1.\tThe paper divides the framework into STP-Former and STP-Former+, yet the motivation, core idea (extracting intrinsic patterns from single images for representation), and the key objective function “Gather Loss” are explicitly derived from the work of Luo et al. (2025). The primary modifications to the foundational model involve incorporating a cross-attention-based TPE module and implementing two-stage supervised training. Building upon Luo's work, the foundational model's innovative contributions remain limited. Excluding the motivation and architecture proposed by Luo, where does the core novelty of STP-Former's framework lie? Why is it adaptable to texture-related tasks, and how does it address the misalignment issues raised by the authors?\n2.\tAlthough the introduction of novel topological losses has achieved state-of-the-art improvements, it sidesteps the issue of training efficiency for these losses and fails to address the computational cost of supervising them. Given that persistent homology computation is computationally intensive, it remains to be seen whether a balance can be struck between training efficiency and the practicality of the method.\nThis paper demonstrates thorough experimentation and proposes a novel idea for supervised topological loss. If the authors can clearly delineate the key contributions of STP-Former and its boundaries relative to the work of Luo et al., I would be willing to increase my rating."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iqANmU6IsK", "forum": "wOubHRUZQU", "replyto": "wOubHRUZQU", "signatures": ["ICLR.cc/2026/Conference/Submission16677/Reviewer_qUuF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16677/Reviewer_qUuF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16677/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070852554, "cdate": 1762070852554, "tmdate": 1762926732906, "mdate": 1762926732906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}