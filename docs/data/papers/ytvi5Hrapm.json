{"id": "ytvi5Hrapm", "number": 15482, "cdate": 1758251784091, "mdate": 1759897304035, "content": {"title": "Finding Harmony in Chemical Data: Hierarchical and Balanced multimodal Fusion for Reaction Yield Prediction", "abstract": "Multimodal yield prediction aims to integrate heterogeneous molecular descriptors across distinct data modalities to predict the conversion efficiency of chemical reactions. However, existing approaches often face limitations in effectively utilizing multimodal information, primarily due to inadequate consideration of both hierarchical relationships and imbalanced contributions across modalities during the fusion process. To address these challenges, we propose a **H**ier**ar**chical and balanced **m**ulti-m**o**dal fusion framework for reactio**n** **y**ield prediction, termed **Harmony**. Specifically, to enhance multimodal information utilization, we design a hierarchical fusion architecture comprising three modality encoders and two feature fusion modules for different levels of granularity. Furthermore, we introduce a novel contribution assessment mechanism that quantitatively evaluates modality-specific impacts, coupled with a prefer-balancing optimization objective. Extensive experimental evaluations demonstrate that Harmony not only consistently outperforms existing methods but also exhibits robust out-of-sample (OOS) generalization. Specifically, it achieves a **22\\% improvement** in the $R^2$ metric over the strongest baseline on the most challenging Amide Coupling Reaction dataset. Our code can be found at https://anonymous.4open.science/r/F6BB.", "tldr": "", "keywords": ["AI for Chemistry", "Chemical Reaction Yield Prediction", "Hierarchical Multi-Modal Fusion", "Balanced Modality Contribution"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce9c48dd52b07d1ba0d23634823d9f35ac3f56e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Challenges. Early models can only utilize a single modality, which can not capture the full features from structural, topological, and electrical aspects. 1) \"Flat fusion\" approaches lead to feature interference, so that important signals are obscured by noise. 2) In prior work UAM, some modalities were not modeled very well, showing a foundational flaw in multimodal chemical modeling.\n\nNovelties: \n1. The proposed Harmony leverages the hierarchical and balanced fusion framework, integrating molecular-level and fingerprint-level features to prevent modality collapse.\n2. A module can balance modality contribution to ensure real engagement of each chemical feature.\n\nResults: \nTop performance on three datasets and strong generalization on out-of-sample data. 22% improvement on the claimed \"most challenging dataset\"."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is a good attempt at using multimodal methods in reaction yield prediction. The experiments show relatively small but consistent improvement on the benchmark datasets. \n2. The motivation from \"Flat fusion\" is typical, and one of the novelties of this work is trying to figure out modality contribution and pursue balance. \n3. The paper is well organized and written. The logic and the structure is clear and easy to follow."}, "weaknesses": {"value": "1. Figure 2 shows that contrastive learning is established between the fusion feature of SMILES and the graph with fingerprint features; however, the unlabeled equation in line 344 shows it is between the graph feature and the fingerprint feature, which is not consistent. If it is for only graphs and fingerprints, the claim of hierarchy will not be correct.\n2. The novelty may not be strong enough. This work mainly puts existing modules (such as Mamba layer, molecular encoders, MLP) together to make a multimodal pipeline.  The pipeline framework may be simple to use concatenation and MLP blocks. There may not be enough improvement beyond the limited 'flat fusion'. From the code, I see that contrastive learning is only for the graph feature with fingerprint; there is no SMILES feature engaged.\n3. The hierarchical information flow is only serving among three modalities -- SMILES, graph, and fingerprint. The first level is between SMILES and graph, the second is between graph and fingerprint. In each level, the features are fused by \"Concat\" and modeled by MLP (one mamba layer added for fingerprint) instead of choosing the most suitable modeling module and best fusion approach for modalities.\n4. The selection rationale of each module has not been explained very well. For instance, why add a Mamba layer for fingerprint data, why use contrastive learning only for graph and fingerprint features, or the reasoning for choosing each molecular encoder?\n5. The scope of this work may be relatively narrow. This framework can only be used in reaction yield prediction due to its fixed framework design, limiting the contribution and potential to generalize to broader multimodal fields.\n6. The main results only have 3 datasets, which may not be enough to prove the performance of this model."}, "questions": {"value": "1. Related works have used SMILES, 2D, and 3D conformers for the integration and claimed strong performance. Why does this work only consider SMILES and 2D structures? Can 3D conformers further enhance the performance?\n2. For the integration of SMILES and 2D, the work uses two different encoders for each modality. However, the multimodal molecular modeling field has been explored deeply -- why not use a multimodal molecular encoder that can integrate SMILES and gthe raph itself, which may gain more performance? \n3. Following the above question, this work uses a relatively old ChemBERTa-2 (2022) model as its SMILES encoder. Why not try a later one, such as TranFoxMol (2023), MLM-FG (2024), MuMo (2025), with stronger performance?\n4. Did you specify which 2D graph encoder you are using as Enc_g?\n5. The output from Molecular-level Fusion has already integrated the SMILES information, why concatenated with SMILES again? Will this enhance the contribution of the SMILES modality a lot? \n6. In the main model, this work mainly uses MLP as a modeling module for the concatenated features/modalities. Did you try a newer modeling backbone, such as transformers or Mamba-ssm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WxsGhR90PE", "forum": "ytvi5Hrapm", "replyto": "ytvi5Hrapm", "signatures": ["ICLR.cc/2026/Conference/Submission15482/Reviewer_cBXr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15482/Reviewer_cBXr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761248728420, "cdate": 1761248728420, "tmdate": 1762925773078, "mdate": 1762925773078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses reaction yield prediction by integrating multiple chemical modalities (SMILES, molecular graphs, and reaction fingerprints) while mitigating modality dominance during fusion. It proposes Harmony, a hierarchical fusion architecture. Three features from different modality encoders are hierarchically mixed, and the final prediction is yielded with a KAN head with Gaussian output. On top of training that combines MSE/uncertainty loss/contrastive alignment loss, the authors introduced a preference-balancing objective that supervises per-modality heads to quantify and balance contributions. Experiments on different reaction experiment datasets compare against unimodal and multimodal baselines show lower MAE/RMSE and higher $R^2$, with ablations supporting the hierarchy and balancing components."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- To maximize the benefits of multimodality, the paper introduces a preference-balancing loss that trains modality-specific predictors in isolation, thereby encouraging balanced contributions when fused.\n\n- The proposed method consistently achieves superior quantitative performance across datasets and evaluation metrics compared with prior baselines.\n\n- Through very extensive ablation studies and systematic comparisons, the paper provides clear justification for the major design choices made during model construction."}, "weaknesses": {"value": "- Considering that many concepts(e.g., the use of multimodality across SMILES/graphs/fingerprints, contrastive loss alignment, and reparameterization-based range prediction) have already been introduced by prior baseline UAM, certain components for performance improvement (e.g., the adoption of the Mamba and KAN architectures) may fall within the realm of engineering choices rather than constituting substantial methodological advances."}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IsGIturMDf", "forum": "ytvi5Hrapm", "replyto": "ytvi5Hrapm", "signatures": ["ICLR.cc/2026/Conference/Submission15482/Reviewer_xuiP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15482/Reviewer_xuiP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743754678, "cdate": 1761743754678, "tmdate": 1762925772713, "mdate": 1762925772713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Harmony, a hierarchical multimodal fusion model for reaction yield prediction. The method uses SMILES strings, 2D graphs, and molecular fingerprints to encode moleular and reaction-level embeddings, using contrastive learning and a preference-balancing loss to train their method. Experiments show promising performance compared to competing methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The method consistently outperforms baselines\n- Their combination of specific losses make sense, especially $\\mathcal{L}_{info}, \\mathcal{L}_{prefer}$."}, "weaknesses": {"value": "- Some architecture decisions are underexplained, i.e. the use of Mamba for fingerprint embeddings, the use of KANs in the MLPBlock. While Appendix K.2 claims to justify the use of a KAN, it is not convincing given that traditional deep learning components ahve been shown to model \"heterogeneous feature interactions.\" Further, their Mamba justification ignores the fact that fingerprint sequences are fixed one-hot descriptors, not a long, variable-length sequences. Using a sequence-based model for the fingerprints seems overkill, and it's unclear why a transformer/Mamba is considered for this. Exact architecture of the 2D graph encoder is also unspecified.\n- The “causal/counterfactual” contribution estimator feels ad hoc and slightly trivial; its $\\xi$ definition literally comes from massaging an error term into (0,1]). There are no results or theoretical analyses that the resulting scores line up with standard Shapley-style methods.\n- The paper's writing feels salesy and tends to overclaim. For example, they claim their rationale for KAN is a \"scientifically grounded decision\" but only high-level discussion and a single ablation are provided as evidence."}, "questions": {"value": "- What is the performance of the method for the \"w/o $\\mathcal{L}_{prefer}$\" ablation?\n- Why is the InfoNCE contrastive loss only defined for 2D graphs and fingerprints?\n- Are there ablations for the uncertainty loss?\n- Have the authors also tested baselines using their other loss functions as well, e.g. the uncertainty loss and InfoNCE loss? There are a lot of components to this method, and it's unclear whether the biggest performance improvements are because of methods proposed by the paper versus the accumulation of general engineering tricks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NhE3Wu4omR", "forum": "ytvi5Hrapm", "replyto": "ytvi5Hrapm", "signatures": ["ICLR.cc/2026/Conference/Submission15482/Reviewer_8XR5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15482/Reviewer_8XR5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007339360, "cdate": 1762007339360, "tmdate": 1762925772183, "mdate": 1762925772183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a HARMONY framework to predict reaction yield. They specifically leverage hierarchical NN architectures multimodal information (i.e., SMILES strings, graphs and fingerprints) as well as balance different NNs in charge of their unimodal parts.\nThe performance evaluation of HARMONY was performed with three datasets (Buchwald-Hartwig, Suzuki-Miyaura and ACR) in terms of MAE, RMSE and R^2 scores, and the advantages of HARMONY are discussed from various aspects including comparisons against other existing approaches, ablation study, and trainable parameter size.  \n\nYield reaction is an important problem in the AI and chemo-informatics research communities.\nHowever, while HARMONY might be an important framework, I have a difficulty in understanding the research motivation on balancing the model. \nEach model has features that are also commonly/easily identified by the others, while it has its own strengths and weaknesses. \nWithout seeing a combination of other types of the unimodal models under the HARMONY framework, the fact that model imbalance was an issue might happen to be the set of the models they happened to select. \n\nTheir analysis essentially shows only numbers of MAE, MSE and R2 scores,\nand the analysis is shallow in terms of understanding the results from the trends of the molecular structures of reactants and products and the reaction patterns (e.g., structural changes) in the datasets. \nFor example, if complicated rings exist, atom-atom distances of a Transformer-based approach receiving SMILES string as a weakness might be misidentified, leading to poor performance. Analogously, an MPNN might fail to handle global relations of important substructures if their distances are long, etc.\nAlthough the molecules and their reaction patterns are important factor in determining the performance, it is unclear what kind of weakness each unimodal had in each dataset and how frequently HARMONY resolved these weaknesses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Attempt to improve the performance of reaction prediction"}, "weaknesses": {"value": "Unclear motivation to the research question on whether balanced multimodal fusion is a generic principle or not due to a combination of only specific models\n\nLack of analysis of the pros and cons of the unimodal models and HARMONY from a viewpoint of actual molecular structures and patterns of the reactions"}, "questions": {"value": "How would the empirical performance fare if a different set of models is combined to construct HARMONY? Could you provide numbers?\n\nI would like to see a deeper understanding relations between (1) the datasets on the molecular structures of the products and reactants, and reaction patterns and (2) the unimodal/HARMONY models (e.g., weakness of a unimodal model and how and why HARMONY resolved).  Would it be possible to provide evidence showing the trends of all the models for deeper understanding to what happens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cfAYr2aV3h", "forum": "ytvi5Hrapm", "replyto": "ytvi5Hrapm", "signatures": ["ICLR.cc/2026/Conference/Submission15482/Reviewer_hceG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15482/Reviewer_hceG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012935692, "cdate": 1762012935692, "tmdate": 1762925771236, "mdate": 1762925771236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}