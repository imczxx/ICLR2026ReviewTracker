{"id": "sV1wSPy9kj", "number": 11890, "cdate": 1758204504078, "mdate": 1763738569605, "content": {"title": "Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models", "abstract": "We investigate a general approach for improving user prompts in text-to-image (T2I) diffusion models by finding prompts that maximize a reward function specified at test-time. Although diverse reward models are used for evaluating image generation, existing automated prompt engineering methods typically target specific reward configurations. Consequently, these specialized designs exhibit suboptimal performance when applied to new prompt engineering scenarios involving different reward models. To address this limitation, we introduce RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time optimization method applicable across various reward scenarios without modification. RATTPO iteratively searches for optimized prompts by querying large language models (LLMs) *without* requiring reward-specific task descriptions. Instead, it uses the optimization trajectory and a novel reward-aware feedback signal (termed a \"hint\") as context. Empirical results demonstrate the versatility of RATTPO, effectively enhancing user prompts across diverse reward setups that assess various generation aspects, such as aesthetics, general human preference, or spatial relationships between objects.\nRATTPO surpasses other test-time search baselines in search efficiency, running 4.8 times faster than naive reward-agnostic test-time search baseline on average. Furthermore, with sufficient inference budget, it can achieve comparable performance to learning-based baselines that require reward-specific fine-tuning.", "tldr": "We develop a reward-agnostic automated prompt engineer that can be readily applied to diverse reward setups without further modification.", "keywords": ["automated prompt engineering", "text-to-image generation", "diffusion models", "test-time optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f85582db858ac2a8df3adf9e4c66c327260abc8.pdf", "supplementary_material": "/attachment/991d2ed5457a370ed64a4da79d5854b30eec89e0.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a well-motivated approach (RATTPO) for reward-agnostic prompt optimization for text-to-image generation. It iteratively refines an initial prompt by querying LLMs at test time: one optimizer LLM proposes new prompts conditioned on the optimization history and a hint-generator LLM to provide reward-aware feedback. Empirical results demonstrate the versatility and effectiveness of PATTPO across a wide range of rewards, including human preference, text-to-image consistency, and holistic MLLM assessment. RATTPO also shows higher search efficiency compared to other test-time search baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The primary strength is the reward-agnostic nature of RATTPO, which is convincingly demonstrated across different diverse reward functions.\n\n2. The method is training-free and gradient-free and exhibits superior generalization when compared to learning-based baselines.\n\n3. The hint is formatted as natural language feedback, making the optimization process transparent and potentially human-interpretable."}, "weaknesses": {"value": "1. Lack the ablation of using single prompting loop for both prompt generation and hint generation.\n2. The method is computationally demanding, requiring two LLMs (optimizer and hint generator) in an iterative loop and necessitating multiple costly image generation and reward function calls (up to 160 generated prompts) to achieve good performance."}, "questions": {"value": "1. Have you encountered the reward hacking problem in your optimization framework?\n2. Why do you not consider integrating both optimizer and hint-generator in a single loop?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v8qJAfDA2g", "forum": "sV1wSPy9kj", "replyto": "sV1wSPy9kj", "signatures": ["ICLR.cc/2026/Conference/Submission11890/Reviewer_EEck"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11890/Reviewer_EEck"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606327379, "cdate": 1761606327379, "tmdate": 1762922903428, "mdate": 1762922903428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Summary"}, "comment": {"value": "We thank all reviewers for their thoughtful feedback. We have addressed each comment in the individual responses and updated the manuscript accordingly. To avoid confusion with figure and table numbering, all **newly added analyses are placed at the end of the paper (Appendix J-L, pp. 31-34)**. These updates will be later fully integrated into the final version of the manuscript.\n\nBelow is a concise summary of the main revisions:\n\n### **Presentation**\n\n- Added clarification on the search efficiency calculation (Appendix C)  \n- Revised one sentence for improved readability (L054).  \n- Added a visual illustration of the RATTPO workflow (Figure 26, Appendix L)\n\n### **Additional Experiments and Analyses**\n\n- Impact of initial prompt length on optimization performance (Figure 24, Appendix J.1)  \n- Experiments with GPT-family LLMs (Table 16, Appendix J.2).  \n- Comparison with a single-LLM variant (Table 17, Appendix J.3).  \n- Evaluation of T2I transferability of optimized prompts across models (Tables 18-19, Appendix J.4).  \n- Additional comparisons with two prior works (Figure 25 and Table 20, Appendix K).\n\nWe would be glad to clarify any remaining concerns and welcome further suggestions from the reviewers."}}, "id": "wICRSEAhrz", "forum": "sV1wSPy9kj", "replyto": "sV1wSPy9kj", "signatures": ["ICLR.cc/2026/Conference/Submission11890/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11890/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11890/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763738745648, "cdate": 1763738745648, "tmdate": 1763738745648, "mdate": 1763738745648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RATTPO, a reward-agnostic test-time prompt optimization method for text-to-image diffusion models. Unlike previous approaches that are tailored to specific reward functions, RATTPO can flexibly enhance prompts across various evaluation scenarios by leveraging LLMs and a novel reward-aware feedback signal. Experimental results show that RATTPO significantly improves search efficiency and prompt quality for diverse reward models. With adequate inference budget, RATTPO achieves performance comparable to specialized learning-based baselines without requiring task-specific tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The motivation and significance of the proposed scenario are clearly articulated and highly relevant.\n- The experimental results convincingly demonstrate the superiority of the proposed method over existing approaches."}, "weaknesses": {"value": "- The paper is poorly written, with an overly brief description of the methodology. It lacks essential details about the input prompts used for the first LLM to generate candidate prompts for image generation, the input prompts for the second LLM, and the specific format of the \"hint\" texts, all of which are critical to understanding the core approach.\n- The paper lacks a clear diagram illustrating the overall workflow of the proposed method; Algorithm 1 alone is insufficient for conveying the process.\n- Lines 054-057 contain two sentences that redundantly express the same idea."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "K7mYpwd0PV", "forum": "sV1wSPy9kj", "replyto": "sV1wSPy9kj", "signatures": ["ICLR.cc/2026/Conference/Submission11890/Reviewer_oSLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11890/Reviewer_oSLi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658094004, "cdate": 1761658094004, "tmdate": 1762922902983, "mdate": 1762922902983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose a new method that uses LLMs to perform automated prompt engineering for text-to-image generation. In particular, they suggest using an LLM as “prompt optimizer” and another LLM as “hint generator” to iteratively improve the generated prompts based on images generated from a text-to-image generative model and an external reward model. At every iteration, the LLM prompt optimizer will generate a prompt given the history of improvements, then the text-to-image model will generate images based on the new prompt and the reward model will output the reward w.r.t. the generated images. The LLM hint generator then produces edit suggestions for the prompt and then the LLM prompt optimizer will improve the prompts produced based on these suggestions. They also conduct experiments to show the effectiveness of their method in comparison to multiple baselines on various datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The experimental results look very promising, especially in Figure 1 where they show great test time scaling.\n2. The algorithm is fairly simple and easy to implement.\n3. The paper is well written and easy to understand."}, "weaknesses": {"value": "1. My main concern about the paper is regarding its novelty. The idea of both LLM as automated prompt generator and as a judge/hint giver has been thoroughly explored both in the context of LLM self-improvement/RLAIF [2,3,4] [(Madaan et al., 2023; Wang et al.,\n2023a; Shinn et al., 2023) from the paper] and text-to-image generation [1] [(Yang et al.,\n2024; Fernando et al., 2023; Du et al., 2024; He et al., 2024; Mañas et al., 2024) from the paper]. In fact, the algorithm proposed in this paper is strikingly similar to [1] and He et al. 2024. It is unclear to me the marginal changes made in this paper are significant enough.\n2. The experiment results, while showing a lot of promise, do seem a bit selective and incomplete. For example, \n\n    (i) In Figure 1, OPT2I only shows up in one out of eight subplots, which is also the only place where this baseline is compared. Given the extreme similarity of the methodology, it would make sense for the authors to include OPT2I in all comparisons that they conduct. Similarly, somehow not all baselines are compared in all experiments.\n\n    (ii) When comparing the inference time, the authors denote the wallclock time for their method as “Time, RATTPO at win”, which seems to indicate that they are only accounting for the cases where their method outperformed the baseline. It is very unclear why they would make this selection, i.e. why not just calculate the wallclock time for all RATTPO runs?\n3. Besides the concerns above, the authors should also consider adding the following experiments to strengthen the paper:\n\n\t(i) The authors should include the comparison against [1], as it is a highly related and similar work (specifically section 6 in [1])\n\n\t(ii) The authors only use the Gemma model family in their experiment and they should consider other MLLMs like the GPT family, etc.\n\n\nReference:\n\n[1] Liu et al. Language Models as Black-Box Optimizers for Vision-Language Models. 2024.\n\n[2] Chao et al. Jailbreaking black box large language models in twenty queries. 2023.\n\n[3] Wang et al. Self-Instruct: Aligning Language Models with Self-Generated Instructions. 2022.\n\n[4] Huang et al. Large Language Models Can Self-Improve. 2022.\n\n[5] Lee et al. RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. 2024."}, "questions": {"value": "How transferable are the prompts that are optimized for one text-to-image model to another one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YP5vtFnb6I", "forum": "sV1wSPy9kj", "replyto": "sV1wSPy9kj", "signatures": ["ICLR.cc/2026/Conference/Submission11890/Reviewer_gfqR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11890/Reviewer_gfqR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943240682, "cdate": 1761943240682, "tmdate": 1762922902630, "mdate": 1762922902630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a novel framework for optimizing prompts in text-to-image (T2I) diffusion models without requiring reward-specific training or modifications. The method uses a dual-LLM approach:\n\nAn optimizer LLM iteratively refines prompts based on historical optimization trajectories.\nA hint-generator LLM provides reward-aware feedback (\"hints\") derived from optimization history, replacing manual task descriptions.\nRATTPO is training-free, gradient-free, and adaptable to diverse reward models (e.g., human preference, text-image alignment, multimodal LLM assessments). Experiments show it outperforms baselines in search efficiency (4.8× faster) and matches reward-specific methods with sufficient inference budget."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Extensive experiments across 8 reward setups, showing versatility and efficiency."}, "weaknesses": {"value": "- Computational cost: Despite efficiency gains, RATTPO requires multiple image generations per iteration (line 7, Algorithm 1). Potential optimizations (e.g., caching) are unexplored.\n- Prompt length constraints: The impact of initial prompt length on optimization is not analyzed.\n- Novelty limited, because iteratively prompt optimization is trivial."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DT0otlOu4E", "forum": "sV1wSPy9kj", "replyto": "sV1wSPy9kj", "signatures": ["ICLR.cc/2026/Conference/Submission11890/Reviewer_SnvR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11890/Reviewer_SnvR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989679779, "cdate": 1761989679779, "tmdate": 1762922902195, "mdate": 1762922902195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}