{"id": "BpXkl2Y5UG", "number": 14414, "cdate": 1758234769284, "mdate": 1763695159824, "content": {"title": "The Anatomy of Alignment: Decomposing Preference Optimization by Steering Sparse Features", "abstract": "Prevailing alignment methods induce opaque parameter changes, obscuring what models truly learn. To address this, we introduce Feature Steering with Reinforcement Learning (FSRL), a framework that trains a lightweight adapter to steer model behavior by modulating interpretable sparse features. First, we theoretically demonstrate that this mechanism is expressive enough to approximate the behavioral shifts of post-training processes. We then apply FSRL to preference optimization and perform a causal analysis of the learned policy. Our analysis reveals a crucial insight: the model learns to reward stylistic presentation as a proxy for quality, disproportionately relying on features related to style and formatting over those tied to alignment concepts like honesty. By effectively optimizing the preference objective, FSRL serves as a transparent proxy for observing the alignment process. Overall, FSRL offers an interpretable control interface and a practical way to diagnose how preference optimization pressures manifest at the feature level.", "tldr": "We created a transparent AI alignment tool and discovered that models learn to satisfy human preferences by making their outputs more stylish and well-formatted, rather than by using more 'honesty' or 'safety' concepts", "keywords": ["Reinforcement Learning", "Mechanistic Interpretability", "Large Language Models", "Steering"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cf590180cda3a6e2d5446cce20cdab253faf9b3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Feature Steering with Reinforcement Learning (FSRL) — a framework that aligns large language models by steering one-layer interpretable sparse features rather than updating dense parameters directly. This allows for more transparent and auditable control over model behavior during preference optimization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed FSRL algorithm empirically optimizes preference alignment tasks using an interpretable basis, providing a possible interpretable framework for downstream task fine-tuning.\n---\n2. The authors provide causal analysis using zero-out steering on different feature categories and find evidence that preference alignment places most pressure on stylistic presentation rather than alignment-related concepts such as safety, honesty, and helpfulness. \n---\n3. Given the assumptions of local linearity and the empirical sparsity of active features, the authors build the theoretical relation between FSRL and the expressive power of LORA.  Specifically, the authors prove that the FSRL steering can be interpreted as an affine map given the local linearity assumption and relate FSRL’s soft-thresholding in sparse features to LoRA’s low-rank decomposition given the empirical sparsity assumption."}, "weaknesses": {"value": "1. Requires more rigorous analysis of the trade-off between math reasoning degradation and preference alignment. It's unclear whether the preservation of math reasoning capability results from limited learnable capacity or from the authors' proposed methodology.\n---\n2. Table 3's intervention over feature activations is insightful, but more analysis is needed. What is the intervention loss difference between the unmodified model and the FSRL-trained model? Would the FSRL-trained model rely more on alignment features rather than disproportionately prioritizing style features? If not, what are the implications of observing this disproportionate prioritization of style-related over alignment-related features during preference alignment? The paper lacks empirical evidence supporting the claim that FSRL \"provides a controlled framework for auditing alignment pressures\".\n---\n3. The performance of FSRL is not sufficiently convincing (see Table 1). More empirical experiments are needed to demonstrate the benefits of FSRL—beyond interpretability—compared to interpretable model-diffing, e.g., other interpretable steering baselines like SAE-lens direct editing or neuron activation control."}, "questions": {"value": "My questions are presented in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "XmpIjF7PRj", "forum": "BpXkl2Y5UG", "replyto": "BpXkl2Y5UG", "signatures": ["ICLR.cc/2026/Conference/Submission14414/Reviewer_WRMW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14414/Reviewer_WRMW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760895126941, "cdate": 1760895126941, "tmdate": 1762924823135, "mdate": 1762924823135, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of our Changes"}, "comment": {"value": "We thank the reviewers for their insightful and constructive feedback. In response to the reviews, we have performed a major revision of the paper, including scaling our experiments to the Gemma 2 9B model, adding new baselines (CAA, SAS), and conducting extensive causal analysis.\n\nBelow is a summary of the key updates and new experimental results:\n1. **Correction of SAE Selection and Retraining:** We identified an oversight in our initial submission where the SAEs used did not align with the feature explanations available on Neuronpedia. We have corrected this by selecting the appropriate canonical SAEs and retraining all adapters reported in the paper. Additionally, we introduced a non-negativity constraint (see Appendix A) on the steered features to ensure adherence to the SAE decoder's training assumptions. This architectural change significantly reduced the average L0 norm of the steering vector. While this updated training changed the magnitude of certain results, most notably the 2B model now exhibits a more dramatic drop in GSM8K performance, our core qualitative claims and findings remain constant. We have added a discussion in Section 6 explaining that this reasoning degradation likely stems from feature entanglement within the specific SAEs used.\n2. **Reframing FSRL as a Diagnostic Instrument:** Reviewers bEBw and m73n noted that FSRL does not outperform full fine-tuning on standard benchmarks. While we did not originally intend to frame FSRL as a competitor to state-of-the-art fine-tuning, we acknowledge that the text may have implied this. We have revised the manuscript to explicitly position FSRL as a transparent diagnostic instrument. Its primary value lies in decomposing the opaque alignment process into interpretable feature modulations, allowing us to audit how the model satisfies the preference objective even when that optimization comes at the cost of generation coherence.\n3. **Scaling to Gemma-2-9B-it:** To address concerns regarding the generality of our findings (Reviewers T5Np, m73n), we replicated our experiments on Gemma-2-9B-it. We find that:\n- The \"Style over Alignment\" phenomenon holds across model scales.\n- Mathematical reasoning (GSM8K) collapses in the 9B model, while improving in the style-ablated variants. This suggests that feature entanglement varies across model scales and depends on the specific underlying SAEs used.\n- This confirms that FSRL provides consistent mechanistic insights across model sizes.\n4. **Causal Validation via Style Ablation:** To causally link the style over alignment bias to model degradation (Reviewer m73n), we trained new adapters where style features were masked (ablated) during the training process.\n- Result: These \"Style-Ablated\" models achieve significantly higher scores on TruthfulQA and recover generation coherence compared to the standard FSRL models.\n- Implication: This confirms that the standard optimization process minimizes loss by prioritizing style features at the expense of semantic quality and truthfulness.\n5. **Quantifying Coherence: AlpacaEval and Qualitative Analysis:** We introduced length-controlled AlpacaEval 2.0 win rates and a qualitative analysis of generated text (Appendix M, Reviewer m73n).\n- Finding: Standard FSRL models suffer from a collapse in coherence (win rates < 1%), driven by an over-optimization of style features where the model forces artifacts, such as excessive bolding, regardless of the prompt.\n- Mechanism: This highlights a failure mode of SimPO (which lacks a hard KL penalty) that FSRL makes visible. The model maximizes reward by driving specific stylistic features to extreme values, breaking coherence.\n6. **New Baselines: Static vs. Dynamic Steering:** We compared FSRL against static steering methods: CAA and SAS (Reviewers T5Np, bEBw, WRMW).\n- Result: Static methods fail to minimize the SimPO loss (reaching ~5.4 compared to FSRL’s ~2.6).\n- Conclusion: Alignment is a complex, context-dependent task. A single static vector is insufficient. An input-dependent adapter (FSRL) is required to effectively optimize the preference objective.\n7. **Robustness and Methodology**\n- Sensitivity Analysis: We performed a sensitivity analysis (Appendix L, Reviewer m73n, T5Np) incorporating the confusion matrix of our automated classifier. Even in the worst-case scenario, the finding that the policy disproportionately targets style features remains robust.\n- Computationally Cheap Layer Selection: We validated our intervention layer choice by training linear probes (logistic regression) across model layers. We found that Layer 12 achieves the highest classification accuracy (54.75%) for separating chosen vs. rejected responses. This confirms that the optimal intervention layer can be identified using computationally cheaper methods rather than expensive full training sweeps (Reviewers bEBw)."}}, "id": "gTWFiPgun3", "forum": "BpXkl2Y5UG", "replyto": "BpXkl2Y5UG", "signatures": ["ICLR.cc/2026/Conference/Submission14414/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14414/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14414/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763686812436, "cdate": 1763686812436, "tmdate": 1763686812436, "mdate": 1763686812436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Feature Steering with Reinforcement Learning (FSRL), a framework for interpretable alignment of large language models. With pretrained sparse autoencoder (SAE) and frozen LLM, FSRL trains a lightweight adapter to modulate interpretable SAE features of a frozen LLM, rather than performing full-model fine-tuning. Additionaly, the authors theoretically establish that FSRL's activation-space corrections are equivalent to a restricted class of LoRA updates, inheriting LoRA's expressive power. Empirically, they apply FSRL to preference optimization on the UltraFeedback dataset using the Gemma-2-2B-it model and pretrained SAEs from GemmaScope, and demonstrate a moderate gain in MMLU, TruthfulQA, GSM8K compared to baseline while enabling interpretable interface. Through causal analysis based on their method, they find that style features contribute more than alignment features, suggesting preference optimization treats stylistic presentation as a proxy for quality rather than prioritizing deeper alignment concepts."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "< Strength >\n\n- The work addresses an important and timely problem. The opacity of standard alignment methods makes it difficult to diagnose issues like reward hacking. The use of SAE is theoretically justified and empirically shown for its interpretability.\n- The theoretical justification connecting FSRL to LoRA updates is well-constructed, providing principled grounding for the approach. The limitation of the justification, the needs for adaptation across all layers, and empirical evidence is also discussed sufficiently.\n- The paper is well-written with precise specification of the methodology. Figure 1 effectively illustrates the architecture.\n- The causal ablation study provides unique compelling evidence for the disproportionate importance of style features, going beyond correlational analysis and has practical impact"}, "weaknesses": {"value": "< Weakness >\n\n- The claimed advantage on interpretability still on a coarse high-level. Although the main advantage of the method is interpretability, the paper doesn’t have generation samples for steered generation except one in Figure 1. The fact that the policy was highly distributed (mentioned in the Appendix) implies the interpretability based on the proposed method remains on coarse level. As Figure 1 describes the methods as a sample-wise fine-grained steering, the paper should explicitly mention about the coverage in the main paper.\n- Similarly, the analysis focuses on relatively simple, coarse-grained concepts of alignment vs. style, and MCC 0.448 for alignment features also raise concern for the substantial noise.\n- Further, given the marginal performance gains in Table 1, this limited interpretability is difficult to justify.\n- SimPO loss is used as the primary metric throughout (Tables 1 and 3), but this is merely the training objective, not a direct measure of alignment quality. Larger gap on SimPO loss between Base and FSRL (37.19%) in Table 1 despite a marginal gain on downstream performance (14.44% on MMLU and 0.71% on TruthfulQA) also raise the question on the performance. The addition of standard metrics like win-rates evaluations (even with LLM-as-judge) would bridge this gap.\n- The degradation of full fine-tuning on mathematical reasoning (GSM8K) is acknowledged as a limitations from SimPO, however the paper doesn’t explain why FSRL shows significantly less degradation than full fine-tuning despite both optimizing the same SimPO objective.\n- Is this due to insufficient training? The authors mention checking SimPO loss convergence would help\n- DPO, a more popular and GSM8K-friendly baseline, was not evaluated despite acknowledged SimPO limitations\n\n< Minor issues >\n\n- $\\boldsymbol{\\theta}\\in R_{+}^{d_{\\text{sae}}}$ in adapter is confusing with $\\theta$ in $\\pi_\\theta$, as those two models are different modules.\n- Typo in line 105, “adding adding”"}, "questions": {"value": "<  Questions >\n\n- Can you provide more qualitative generation examples showing how style and alignment feature steering manifests in actual text? While Figure 1 offers one illustration, more diverse qualitative samples would help readers better understand the behavioral changes induced by FSRL and strengthen the interpretability claims.\n- In lines 349-351, “It enables researchers to causally link undesirable behaviors to the optimization of specific feature categories, thereby clarifying their root causes.”, could you provide empirical evidence demonstrating that steering specific feature categories can successfully mitigate or induce undesirable behaviors? The current results in Table 1 show that FSRL does not fully match the alignment efficacy of full fine-tuning, which makes this claim somewhat difficult to catch from the presented experiments.\n- Would it be possible to supplement the evaluation with more standard alignment metrics such as win-rates or LLM-as-judge assessments? These metrics would provide a more direct measure of alignment quality.\n- Given that DPO is more widely adopted in the community and is known to work well with mathematical reasoning tasks, could you provide results using DPO as the optimization objective? This would help clarify whether the findings such as style-over-alignment phenomenon or FSRL better performing on math are specific to SimPO or represents a more general characteristic of preference optimization methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yBiTmu0R6l", "forum": "BpXkl2Y5UG", "replyto": "BpXkl2Y5UG", "signatures": ["ICLR.cc/2026/Conference/Submission14414/Reviewer_m73n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14414/Reviewer_m73n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767627369, "cdate": 1761767627369, "tmdate": 1762924822682, "mdate": 1762924822682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Feature Steering with Reinforcement Learning (FSRL), where an adapter is learned from preference data to steer model behavior. In particular, the adapter operates over the activations of a particular layer in the model. The adapter learns to perturb these activations by modulating additive features learned by SAEs that correspond to higher level semantic concepts (e.g., alignment or style). This results in a method that can steer model behavior by only updating the activations for a single layer in a model in a supposedly more interpretable manner.\n\nThe authors show that FSRL improves the preference loss of a 2B model on several standard benchmarks, comparing to SimPO. The authors then also show that the proportion of features corresponding the “alignment’ and “style” concepts decreases when training with FSRL, and that preventing adaptations to certain features (either “alignment” or “style”) impacts the model’s resulting loss differently. These results indicate that certain high-level features—in this case “style”—are more important for reducing preference loss than others. \n\nFinally, the authors also prove that FSRL updates fall within the class of LORA updates, providing theoretical justification for their method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The author’s theoretical analysis makes an interesting connection between LORA updates and steering model behavior via updating only the activations of the model. Additionally, the paper is well written and easy to follow. The method, FSRL, is clearly explained and well situated in prior work. FSRL is also evaluated on standard benchmarks making it easy to understand comparisons to prior work."}, "weaknesses": {"value": "I have two major concerns: (1) I am left unsure of the benefit of FSRL over full-fine tuning (2) I am not convinced of the novelty of this work. I additionally have a third, lower priority concern (3) the mechanistic insights could use more explanations or analysis. \n\nRegarding (1): Table 1 shows that SimPO outperforms FSRL across all benchmarks except for GSM8K, but here FSRL is outperformed by the base model. In terms of performance, it is not clear to me that there is any benefit to using FSRL; I don’t think it is sufficient to argue that FSRL is more performant than SimPO on one benchmark (although worse than the base model on that benchmark) but less performant on all others. Additionally, the authors of SimPO indicate that their method may do worse on GSM8K because of the training data; if you use different training data (e.g., perhaps more tailored to reasoning tasks) do you still see relatively better performance than SimPO? Its also not clear to me if FSRL possesses a benefit in terms of interpretability: the authors note that the FSRL adapter outputs a steering vector that is significantly more dense than the SAE’s features, and the empirical evaluations with regards to interpretability are limited. Finally, it is not clear if FSRL possesses any benefits in terms of computational expense: while it only requires limited adaptations to the models activations for a single layer, finding that layer required extensive search. Therefore, I am left unsure of where FSRL outperforms prior work. \n\nRegarding (2): This recent work from Bayat et al. (https://arxiv.org/pdf/2503.00177) also focuses on learning to steer a model by leveraging SAEs with targeted updates. Bayat et al. also learns how to steer model behavior from preferences, and evaluates on the same benchmarks as this submission. I think this work would be a fair comparison empirically—-or at least the authors should justify conceptually how their work differs.\n\nRegarding (3): The authors note that, given Table 2, “the adapter learns to decrease the proportional activation of both alignment and style features relative to the baseline.” Why would the adapter learn to decrease the number of alignment features and style features? What type of behavior does this correspond to? My initial thought is that this observation might indicate that the adapter results in different features that are not classified as alignment or style features, possibly because these features are now different from the ones used to train the classifier. These empirical results do not tell me sufficient information about how the model behavior is changing. This analysis, without further explanation or connection to model behavior, feels incomplete."}, "questions": {"value": "Where is the benefit of FSRL over prior work? \nWhat is the novelty of FSRL with respect to Bayat et al.? Why do you observe decreasing proportions of alignment and style features after training with FSRL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Js8FvrZLYC", "forum": "BpXkl2Y5UG", "replyto": "BpXkl2Y5UG", "signatures": ["ICLR.cc/2026/Conference/Submission14414/Reviewer_bEBw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14414/Reviewer_bEBw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761773050501, "cdate": 1761773050501, "tmdate": 1762924822264, "mdate": 1762924822264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method (FSRL) for preference alignment with a lightweight adaptor that also enables interpretable steering. The method uses the SimPO objective and an adaptor to transform activations into features to pass into a sparse autoencoder for updating the residual stream. The authors show that FSRL's updates are equivalent to an input-dependent LoRA update, conduct some ablations on the form of the adaptor itself, demonstrate that the method yields performance between that of full finetuning and the original base model, and show that the trained adaptor can be used to provide insights into the alignment process (through observation and intervention on the latent features)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written: Figure 1 makes the high-level idea clear and experimental setups are well-explained.\n2. The paper performs ablations to motivate the setup.\n3. The paper provides an interesting analysis on the contribution of different high-level categories (e.g., alignment, style) to the post-training objective."}, "weaknesses": {"value": "1. The paper could benefit from more direct comparisons with related work, including other forms of steering as well as other adaptors (such as non-interpretable ones). The latter are not discussed in the related work, and additional experimental comparisons would strengthen the paper in the context of the existing landscape of training with adaptors and interpretable steering. For instance, how does the additional interpretability of this setup compare to vanilla adaptors for training?\n2. The paper only runs experiments on a single model. Running experiments on one additional model would strengthen the paper (and potentially offer important practical insights on the application of this method). For instance, can this method with the same SAE be used on a model from a different model family?\n3. To be able to more confidently draw conclusions from section 5, it would be helpful to propagate the error from the categorization process into the results of the analysis. Do the claims still hold when this is taken into account?"}, "questions": {"value": "Please see the above for questions!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sqscOBzYKo", "forum": "BpXkl2Y5UG", "replyto": "BpXkl2Y5UG", "signatures": ["ICLR.cc/2026/Conference/Submission14414/Reviewer_T5Np"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14414/Reviewer_T5Np"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969996906, "cdate": 1761969996906, "tmdate": 1762924821644, "mdate": 1762924821644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}