{"id": "cS9sDsfErj", "number": 19396, "cdate": 1758295902058, "mdate": 1759897041553, "content": {"title": "From Data to Rewards: a Bi-level Optimization Perspective on Maximum Likelihood Estimation", "abstract": "Generative models form the backbone of modern machine learning, underpinning state-of-the-art systems in text, vision, and multimodal applications. While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. However, these approaches depend on explicit reward signals, which are often unavailable in practice, leaving open the fundamental problem of how to align generative models when only high-quality datasets are accessible. In this work, we address this challenge via a Bilevel Optimization framework, where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. We then conduct a theoretical analysis of this optimization problem in a tractable setting and extract insights that, as we demonstrate, generalize to applications such as tabular classification and model-based reinforcement learning.", "tldr": "We propose a way to learn reward functions directly from data through a bi-level optimization formulation of the maximum likelihood estimation objective.", "keywords": ["Maximum likelihood estimation", "Policy gradient", "Generative models", "Bi-level optimization"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6da941a33369947421f734f72f3277f1619b5e4a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a bilevel optimization (Bi-O) framework that reinterprets Maximum Likelihood Estimation (MLE) for generative models as a reward-learning problem. The key idea is to treat the reward function as an outer-level optimization variable and the policy gradient (PG) objective as an inner-level problem.\nThe authors show that under Gaussian assumptions, the Bi-O problem admits a closed-form solution. Then, they generalize to applications such as tabular classification and model-based reinforcement learning."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is generally well-written. The mathematical derivations are well organized.\n- Conceptual novelty: Provides a unifying view linking MLE and RL-based training through a bilevel optimization formulation."}, "weaknesses": {"value": "- Experiments are restricted to small-scale synthetic and tabular datasets. It’s unclear how the method scales to large generative models (e.g., language models). Although the paper motivates connections to LLM training (e.g., RLHF), the experiments do not test any large-scale or sequential generative task.\n- The Gaussian assumption (Assumption 4.1) limits generality; it’s not clear how the conclusions extend beyond this setting.\n- The reward model (Assumption 4.2) is in a limited form. In many real-world applications, the reward function does not take such quadratic form, for example, in recent LLM RL training such as in (Shao et al., 2024; DeepSeek-AI et al., 2025).\n- The expriments in 6.1 (TABULAR CLASSIFICATION) and 6.2 (MODEL-BASED REINFORCEMENT LEARNING) are not detailed. The presentation should be improved so that the readers can more easily grasp the important part of developments and results related to real-world applications. Instead, the space for introducing the Gaussian example can be shortened."}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kn7WVKUfgW", "forum": "cS9sDsfErj", "replyto": "cS9sDsfErj", "signatures": ["ICLR.cc/2026/Conference/Submission19396/Reviewer_ZjsV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19396/Reviewer_ZjsV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761402782244, "cdate": 1761402782244, "tmdate": 1762931316136, "mdate": 1762931316136, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a principled way to turn supervised data into an RL-style reward so that a policy-gradient (PG) inner loop can be used while still targeting a maximum-likelihood (MLE) outer objective. Concretely, MLE is recast as a bilevel optimization where the outer problem optimizes a reward function $r$, and the inner problem optimizes model parameters $\\theta$ via an entropy-regularized PG objective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The Gaussian/quadratic analysis yields a closed-form U and a clear reverse-KL equivalence, offering intuition for why PG with the right reward can mimic (or complement) MLE.\n- The problem setup and contribution bullets are clear; figures for synthetic experiments."}, "weaknesses": {"value": "- The main theorem relies on Gaussian conditionals and a quadratic (Mahalanobis) reward; it is unclear how sensitive the conclusions are when these assumptions fail.\n- The paper said it aims at providing a general framework (i.e. beyond control tasks) than Zeng et. al. I am winding how such bi-level optimization perform in LLM setting. For example, a simple comparison with LLM's reasoning task using grpo/ppo with verifiable reward and some open-end domains with non-verifiable reward. So that we can see the practical usage of this bi-level optimization algorithm in different cases."}, "questions": {"value": "same as weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cbDxzk5DZO", "forum": "cS9sDsfErj", "replyto": "cS9sDsfErj", "signatures": ["ICLR.cc/2026/Conference/Submission19396/Reviewer_pYjD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19396/Reviewer_pYjD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761543090724, "cdate": 1761543090724, "tmdate": 1762931315723, "mdate": 1762931315723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose to reframe maximum likelihood estimation as a bi-level optimization problem. They perform theoretical analysis of doing so in the Gaussian setting, before performing experiments on toy RL problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "N/A"}, "weaknesses": {"value": "(-) I thought about this for a while and I am fairly convinced this paper is re-inventing Ziebart's classical MaxEnt IRL paper. Looking at the core optimization problem (Bi-O), we see it is set up as finding a reward function whose soft optimal policy has a high likelihood of generating the observed data. This is precisely what MaxEnt IRL is. \n\nTo compute the likelihood gradient of the reward function (which requires knowing the soft optimal policy), Ziebart et al. propose computing the policy via soft value iteration. I've read this paper a few times but I still can't tell what the authors are doing instead. I could there being see a difference between the classical formulation and this work here. However, it should be made explicit that this is the novel contribution, rather than the formulation itself.\n\n(-) I would add in a reference to Ziebart's MaxEnt IRL paper above the definition of PG. Similarly, I'd add in a reference to Swamy's \"All Roads Lead to Likelihood\" paper re: MaxEnt RL minimizing reverse KL (Corr. 4.5).\n\n(-) The experiments are limited and performed on toy domains. There is almost no connection between the theory and the practice"}, "questions": {"value": "(1) Can you spell out in greater detail what you're doing for your MBRL experiments?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FLGo15Nq6E", "forum": "cS9sDsfErj", "replyto": "cS9sDsfErj", "signatures": ["ICLR.cc/2026/Conference/Submission19396/Reviewer_VZ8W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19396/Reviewer_VZ8W"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19396/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762295067245, "cdate": 1762295067245, "tmdate": 1762931314996, "mdate": 1762931314996, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}