{"id": "AaAbeUp7O4", "number": 1903, "cdate": 1756963094560, "mdate": 1763630250678, "content": {"title": "Improving Diffusion Language Model Reasoning through Joint Search in Generation Order and Token Space", "abstract": "The order-agnostic generation of Diffusion Language Models (DLMs) presents a promising alternative to autoregressive models for complex reasoning. We model reasoning as traversals of a problem-specific graph of logical dependencies, and view DLM decoding as sampling trajectories from a joint space over generation orders and token values. We show that standard decoding heuristics such as low-confidence remasking collapse this reasoning space. To address this, we introduce Order-Token Search, an algorithm that jointly searches over token content and generation order. Its core is a likelihood estimation function that scores block-level denoising actions, enabling stable path pruning. This allows for efficient exploration of diverse reasoning trajectories. Extensive experiments on mathematical reasoning and planning benchmarks show that our method consistently outperforms baselines, matching or surpassing the gains of fully post-trained d1-LLaDA with diffu-GRPO on Countdown, GSM8K, and MATH500 (e.g. achieving a 13.7% absolute gain on Countdown). Our work establishes structured search as a key missing component for advancing reasoning in DLMs.", "tldr": "Order-Token Search enhances Diffusion LM reasoning by jointly searching generation order and token space, overcoming greedy methods' limitations.", "keywords": ["Diffusion Language Model", "Masked Diffusion Model", "Test-Time Search Algorithm"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fba86428dc5dcc88441dae2c26ac213e7b852f3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper describes a new method for remasking in the denoising process of Diffusion Language Models that searches jointly over generation orders and token choices to maintain high accuracy while promoting diverse reasoning. The paper empirically validates their proposed method in comparison with standard remasking strategies over various mathematical reasoning and problem solving benchmark datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is very well written\n- The problem setting is clearly stated and motivated\n- Experimental results are clearly presented"}, "weaknesses": {"value": "- For a paper with only experimental results, the extent of the experiments seem lacking to prove significance. It seems that only the Countdown task exhibits strong performance gains and little explanation is given for why that seems to be.\n- The computational complexity of the Order-Token Search is not clearly stated, is there a strong incentive to use this new algorithm despite marginal gains in performance?\n- What is being meant by “reasoning” in this paper? What is it about the new masking strategy that unlocks “reasoning” capability in the MDMs, which seems to be a central claim of the study.\n- The case study is informative to the intuition behind the proposed algorithm, but one example is not convincing to the soundness of an algorithm. Are there other wider trends or more concrete theoretical explanations that hint at better performance on the benchmark datasets?"}, "questions": {"value": "See Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "niL4u3IJzq", "forum": "AaAbeUp7O4", "replyto": "AaAbeUp7O4", "signatures": ["ICLR.cc/2026/Conference/Submission1903/Reviewer_HT8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1903/Reviewer_HT8y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547850132, "cdate": 1761547850132, "tmdate": 1762915937741, "mdate": 1762915937741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "> **Expanded baselines and additional backbone model**\n\n\nIn response to requests for more baseline and backbone comparisons, we have substantially expanded our decoding baselines and backbones; please see the updated Table 1 in the revised manuscript, as well as the sampled result reported in the two tables below.\n\n\nOn the baseline side, we now include four stronger decoding strategies—**low-confidence + majority-voting**, **AR**, **AR + majority-voting**, and **AR + beam-search**. These additions primarily address Reviewer TD2K’s requests for stronger confidence-style and autoregressive baselines under comparable compute and Reviewer 34SD’s request for a beam-search counterpart to the AR baseline.\n\n\nOn the backbone side, following Reviewer RQJW’s concern about relying on a single model, we now evaluate OTS on both LLaDA-instruct and the RL-tuned LLaDA-1.5. Together with added baselines, the result addresses Reviewer HT8y’s concern that the extent of the experiments seemed lacking to prove significance. \n\n\nAs shown in the “All” (avg of 4 benchmarks), “MATH500-Avg”, and “Countdown-Avg” columns of the two tables below, OTS remains the best method on both backbones: for LLaDA, OTS achieves **35.2** in the All column, with **32.8** on MATH500-Avg and **28.4** on Countdown-Avg, outperforming the strongest non-OTS baselines (e.g., All = 33.3 with AR + beam-search). For LLaDA-1.5, OTS reaches **36.7** in the All column, with **33.8** on MATH500-Avg and **28.0** on Countdown-Avg, again strictly better than the strongest non-OTS baselines (All = 35.0, MATH500-Avg = 31.3, Countdown-Avg = 22.5 with Low-conf + MV). Thus, even with these stronger, compute-matched baselines and an additional backbone, OTS is still the overall top-performing decoding strategy.\n\n\n**Table R1 (LLaDA backbone)**\n\n\n| Method             | All |  |      |      |      |   MATH500   | |      |      |      |   Countdown   |\n|--------------------|-----|---------|------|------|------|------|-----------|------|------|------|------|\n|                    |     | L=64    | L=128 | L=256 | L=512 | Avg | L=64      | L=128 | L=256 | L=512 | Avg |\n| Low-confidence     | 31.1| 21.2    | 26.0  | 32.4  | 36.2  | 29.0| 25.8      | 20.7  | 19.5  | 16.0  | 20.5 |\n| Low-conf + MV      | 32.5| 20.2    | 27.4  | 35.0  | 36.2  | 29.7| 22.7      | 23.8  | 18.4  | 18.0  | 20.7 |\n| Random + MV        | 28.8| 17.2    | 26.2  | 31.8  | 31.8  | 26.8| 6.3       | 15.2  | 14.1  | 15.2  | 12.7 |\n| Order-Token Search | **35.2** | 22.4 | 30.4  | 36.0  | 42.4  | **32.8** | 27.7 | 34.4  | 26.2  | 25.4  | **28.4** |\n| AR                 | 28.8| 18.8    | 23.4  | 27.4  | 34.4  | 26.0| 10.6      | 12.9  | 13.3  | 14.1  | 12.7 |\n| AR + MV            | 31.0| 17.4    | 23.0  | 32.2  | 39.9  | 28.1| 10.2      | 13.3  | 11.3  | 13.7  | 12.1 |\n| AR + beam-search   | 33.3| 22.2    | 26.6  | 35.4  | 39.8  | 31.0| 18.4      | 23.1  | 21.5  | 21.9  | 21.2 |\n\n\n\n\n**Table R2 (LLaDA-1.5 backbone)**\n\n\n| Method             | All     | |        |        |        |   MATH500   | |        |        |        |  Countdown    |\n|--------------------|---------|---------|--------|--------|--------|------|-----------|--------|--------|--------|------|\n|                    |         | L=64    | L=128  | L=256  | L=512  | Avg  | L=64      | L=128  | L=256  | L=512  | Avg  |\n| Low-confidence     | 32.3    | 20.2    | 26.4   | 32.5   | 36.2   | 28.8 | 19.8      | 19.7   | 17.9   | 21.8   | 19.8 |\n| Low-conf + MV      | 35.0    | 21.2    | 30.0   | 34.8   | 39.3   | 31.3 | 20.7      | 23.8   | 20.3   | 25.4   | 22.5 |\n| Random + MV        | 30.2    | 22.4    | 26.2   | 31.0   | 30.4   | 27.5 | 5.5       | 16.4   | 9.0    | 14.5   | 11.4 |\n| Order-Token Search | **36.7**| 24.4    | 30.8   | 37.4   | 42.4   | **33.8** | 27.7  | 31.3   | 23.8   | 29.3   | **28.0** |\n| AR                 | 30.3    | 17.2    | 23.5   | 31.2   | 35.0   | 26.7 | 12.3      | 15.2   | 14.5   | 15.7   | 14.4 |\n| AR + MV            | 32.2    | 18.4    | 26.4   | 33.6   | 37.9   | 29.1 | 12.5      | 17.2   | 14.8   | 16.0   | 15.1 |\n| AR + beam-search   | 33.5    | 19.0    | 26.4   | 35.2   | 38.8   | 29.9 | 14.8      | 21.1   | 16.0   | 20.3   | 18.1 |"}}, "id": "0QFrPro79r", "forum": "AaAbeUp7O4", "replyto": "AaAbeUp7O4", "signatures": ["ICLR.cc/2026/Conference/Submission1903/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1903/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1903/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763629376047, "cdate": 1763629376047, "tmdate": 1763629376047, "mdate": 1763629376047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a search strategy for generation with discrete diffusion models to enable both high quality and exploration compared to things like low-confidence remasking which sacrifice exploration for quality. At a high-level it is a kind of beam search over both token orderings and token selections. Specifically they adopt the block-wise autoregressive decoding pattern commonly used in past work. They generate multiple possible blocks and only keep the candidates with the highest scores, similar to beam search. For their proposed scoring function, they unmask all tokens and compute the likelihood of the (re-masked) block conditioned on all other tokens. They report results across different decoding methods for standard math benchmarks and some synthetic puzzle benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "One of the most interesting aspects of this class of generative models is their improved decoding flexibility compared to autoregressive models. Exploring decoding strategies over both positions and tokens that are only possible for this class of models is an interesting research direction.\n\nThe motivation from the limitation of existing approaches (i.e. random is diverse but low-quality, and high-confidence is high-quality but not diverse) is clear and intuitive.\n\nA naive beam search over every decoding choice would be extremely expensive. Performing it block-wise is a clever way to balance the benefits of search while reducing the computational overhead.\n\nTheir method outperforms low-confidence sampling (see question 2) and random sampling with majority baseline. Their ablation is Table 2 demonstrates the benefits of their joint search."}, "weaknesses": {"value": "Although their approach is motivated by balancing exploration and quality, this benefit is not validated for their method. They present Pass@k curves to argue for the limitation of low-confidence remasking strategies, but never present such a curve for their approach to validate that it solves the problem with low-confidence remasking. For autoregressive models, beam search often has diversity issues, so a similar thing could be happening here.\n\nAlthough the asymptotics of their search algorithm is discussed, the computational cost (at least in terms of NFE) of various decoding settings should be reported alongside all results. There are many ways to improve performance by expending more compute. It is important to quantify this rigorously to ensure that the method does not impose unreasonable tradeoffs. In general for test-time scaling approaches like these, results should be reported across a range of inference-matched settings to get a true picture of the tradeoffs.\n\nSome additional decoding baselines should be included. Based on their own figure 2, autoregressive decoding appears to be a very strong baseline and should be included. Majority voting with both random and autoregressive baselines in compute matched settings should be included. \n\nIn the appendix, the authors mention that they add gumbel noise to the logits to improve exploration. This is not discussed in the main paper. This feels like an important implementation detail that should not be relegated to the appendix. Is this critical? How does Order-Token Search perform without it? Can you similarly apply this perturbation to low-confidence re-masking to improve diversity. A majority voting baseline with low-confidence remasking and gumbel noise should also be reported, given the use of gumbel noise in the proposed method.\n\nThe analysis in A.2. is not convincing. Figure 2 shows that restricting the model to the autoregressive ordering is extremely effective, dramatically outperforming the other decoding strategies in most settings. Analyzing whether correct solutions under random sampling arose from landing on the autoregressive ordering is a very indirect way to study it. The odds of actually achieving the autoregressive ordering (or even getting very close to it) when doing random decoding is extremely small as demonstrated by the overwhelming amount of large chaotic values. The finding in Figure 2 pretty strongly suggests that the autoregressive ordering is particularly effective, at least for many tasks and settings."}, "questions": {"value": "1. Equation two doesn’t seem to batch the text and figure. My understanding is that you are measuring the likelihood of the block given the surrounding context. Is that correct? As written, it is the probability of the clean sequence conditioned on the block.\n2. What is the performance of the proposed method without Gumbel noise?\n3. What are the total NFEs for all reported evaluation settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "23ATdDmV1V", "forum": "AaAbeUp7O4", "replyto": "AaAbeUp7O4", "signatures": ["ICLR.cc/2026/Conference/Submission1903/Reviewer_TD2K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1903/Reviewer_TD2K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949526671, "cdate": 1761949526671, "tmdate": 1762915937613, "mdate": 1762915937613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "> **Core contribution: structured search over orders and tokens in DLMs**\n\n\nTo help reviewers better assess the novelty and significance of our work, we summarize here what is fundamentally new and why it matters for diffusion language models. At a high level, our central contribution is to treat DLM decoding not just as token prediction, but as *trajectory selection* through a space of reasoning orders. We formalize each problem as inducing a latent graph of logical dependencies, and view a DLM decoding trace as one concrete traversal of this graph: at every denoising step the model decides both **which position to update** and **which token to place there**. Standard MDM training only supervises token predictions under random remasking, so the distribution over trajectories is determined entirely by the inference-time remasking rule. Our analysis shows that low-confidence remasking collapses this trajectory space to a single greedy path that boosts pass@1 but hurts pass@k by restricting exploration, while random remasking explores many more orders and improves pass@k at the cost of weaker single-sample accuracy.\n\n\nBuilding on this perspective, our algorithmic contribution is **Order–Token Search (OTS)**, a decoding procedure that performs *structured search* over both generation orders and token choices rather than committing to a single heuristic traversal. OTS uses the parallel denoising capability of MDMs to expand multiple candidate sequences in each search interval, scores them with a stable sequence-level likelihood estimator, and prunes to the most promising trajectories. Empirically, this joint order–token search reconciles the pass@1 vs. pass@k trade-off: across GSM8K, MATH500, and Countdown, OTS consistently improves pass@1 over low-confidence remasking while matching or surpassing the gains of fully post-trained d1-LLaDA with diffu-GRPO—using only test-time search instead of additional training. Together, the conceptual framing of reasoning as trajectory selection and the OTS decoding algorithm constitute the main contribution of our work, and we have incorporated this perspective explicitly into the Introduction of the revised manuscript."}}, "id": "Yke9t4FMm9", "forum": "AaAbeUp7O4", "replyto": "AaAbeUp7O4", "signatures": ["ICLR.cc/2026/Conference/Submission1903/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1903/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1903/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763629419557, "cdate": 1763629419557, "tmdate": 1763629419557, "mdate": 1763629419557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Order-Token Search, a novel decoding algorithm for Diffusion Language Models that jointly explores generation orders and token choices to improve reasoning performance. The work addresses a trade-off in DLM decoding strategies provides improvements on reasoning-intensive tasks. The work is generally interesting and highlights the importance of masking order in generation, and introduces a new test-time scaling method for DLM."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel problem identification: The paper highlights current limitation in diffusion language model decoding, which lacks the generation diversity offered by AR. The systematic analysis of the pass@1 vs pass@k trade-off in existing decoding strategies (Section 3) is compelling, showing that low-confidence remasking improves single-trial accuracy but limits exploration diversity.\n\n2. Improved Empirical Results: The method achieves consistent improvements across multiple benchmarks, with particularly notable gains on Countdown that rival post-training methods. The comparison showing Order-Token Search outperforming computationally expensive baselines like Order Search and Token Search demonstrates the value of the dedicated likelihood estimation."}, "weaknesses": {"value": "1. The decoding method computation complexity is not shown empirically. How does the method perform compared to baselines under the same FLOPs, e.g. draw a test-time scaling curve? \n\n2. Only one backbone model is studied. The paper would hold a stronger claim with more than one pretrained backbones studied.\n \n3. Scoring Function Stability: How sensitive is the block-level likelihood estimation to the choice of block size? \n\nMinor Issues\n\n- The notation in Equation 2 could be clearer - the function b(xs, xt, x0) is introduced but not precisely defined."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RHKZ5kciLG", "forum": "AaAbeUp7O4", "replyto": "AaAbeUp7O4", "signatures": ["ICLR.cc/2026/Conference/Submission1903/Reviewer_RQJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1903/Reviewer_RQJW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962137327, "cdate": 1761962137327, "tmdate": 1762915937486, "mdate": 1762915937486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Order-Token Search, a structured search algorithm that jointly explores both generation order and token space. The method employs a likelihood-based scoring function to evaluate block-level denoising actions and prune unstable paths efficiently. Experiments on mathematical reasoning and planning tasks demonstrate the gains, suggesting that structured search can meaningfully enhance reasoning in DLMs without additional training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces a beam-search-like decoding algorithm (Order-Token Search) tailored for diffusion LMs, demonstrating improvement on challenging reasoning tasks.\n2. Provides a clear analysis of the trade-off between accuracy and exploration in standard decoding methods (e.g., low-confidence remasking, AR-order, random sampling)."}, "weaknesses": {"value": "1. The paper fails to discuss sampling diversity controls (e.g., temperature, top-k/top-p sampling used in Dream) that could also improve pass@k performance when using low-confidence remasking methods, as studied in prior work (e.g., DiffuCoder [1], showing higher temperature will lead to more diverse token / order exploration and having high pass@k results).\n\n2. In Figure 2, the “AR-order decoding” baseline lacks a beam search counterpart, leading to an unfair comparison—especially since the “token search” baseline is guided by other selected positions, as in line 412, \"Token search is guided by a sequence of greedily-decided positions (selected via low-confidence remasking)\". This undermines the validity of the baseline.\n\n3. There is an inconsistency in Countdown accuracy between Table 3 (16–21%) and the main results (25.4–34.4%), which should be clarified.\n\n4. The paper does not mention the extra inference overhead introduced by the joint search algorithm, which is important for evaluating practicality.\n\n[1] https://arxiv.org/abs/2506.20639"}, "questions": {"value": "1. In Figure 1, the legend contains a typo: “within a forwar”\n2. How is Order-Token Search integrated with full diffusion generation (rather than block diffusion)? Is it applied only to per-block, and does it scale with the full diffusion length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gnYs9ACquG", "forum": "AaAbeUp7O4", "replyto": "AaAbeUp7O4", "signatures": ["ICLR.cc/2026/Conference/Submission1903/Reviewer_34SD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1903/Reviewer_34SD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008965301, "cdate": 1762008965301, "tmdate": 1762915937332, "mdate": 1762915937332, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}