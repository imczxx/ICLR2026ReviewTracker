{"id": "FCFM3Yxtsm", "number": 13627, "cdate": 1758220053064, "mdate": 1759897423864, "content": {"title": "One for all: Zero-Shot Cross-Hardware Performance Modeling with LLMs for Tensor Program Tuning", "abstract": "Tensor program tuning is critical for inference acceleration of deep neural networks (DNNs), especially Large Language Models (LLMs). Yet its effectiveness hinges on cost models for accurate performance estimation. \nExisting cost models rely on manually designed hardware-specific features and extensive profiling data. Thus they suffer from high development costs, poor efficiency, and limited generalization, and become to a significant bottleneck in the face of rapidly evolving models and hardware.\nIn this paper, we propose LLMTuner, a novel framework enabling LLMs to analyze tensor program execution behaviors and accurately estimate tensor program performance across diverse hardware. LLMTuner introduces a coarse-to-fine process: a lightweight LLM-based classifier first filters out suboptimal programs, then a finetuned LLM infers multi-dimensional execution behavior scores to predict latency across different hardware. \nExperiments demonstrate that LLMTuner significantly improves estimation accuracy by up to 64.8\\%, compared with general-purpose LLMs and other cost models on benchmark datasets across 6 CPU and 5 GPU platforms.\nIt can even accurately estimate performance on unseen hardware, achieving 49.2\\% accuracy improvement over other cost models.\nFor practical DNN and LLM tuning tasks, compared with other cost models,  LLMTuner could discover superior program performance (1.47$\\times$) with up to 3.27$\\times$ tuning efficiency.\nMoreover, LLMTuner with finetuned lightweight LLMs reduces the estimation time by over 30$\\times$ compared to DeepSeek R1.", "tldr": "", "keywords": ["Tensor Program Tuning", "LLM", "Hardware"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d17cf0ed707a2b2d95f07f5bc11af696e5250207.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes using large language models (LLMs) to predict and select high-performance kernels from a set of candidates, a process known as kernel tuning. The proposed system, LLMTuner, employs an LLM-based classifier for coarse-grained selection of potentially optimal kernels, followed by a fine-grained selection stage. In the second stage, a fine-tuned LLM predicts execution behavior and selects the best kernel based on its inference. Experiments show that LLMTuner improves estimation accuracy by up to 64.8% compared with general-purpose LLMs and other cost models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper proposes a framework for predicting execution behavior and performing kernel tuning.\n2. A coarse-to-fine framework improves both efficiency and accuracy.\n3. Experimental results demonstrate improved accuracy compared with general-purpose LLMs and other cost models."}, "weaknesses": {"value": "1. Weak motivation\n2. Weak evaluation"}, "questions": {"value": "Thank you to the authors for submitting to ICLR 2026.\n\nI appreciate the idea of leveraging LLMs to analyze kernel programs and demonstrate their ability to recognize efficient kernels. However, as a paper introducing a cost model for kernel tuning, this work has relatively weak motivation and evaluation.\n\n**1. Weak motivation**\n\nDuring the era of CNN optimization, systems such as TVM and its schedulers (AutoTVM/Ansor) were widely used for end-to-end compilation. At that time, the search space for even a single convolution kernel was enormous, making exhaustive benchmarking infeasible. Thus, AutoTVM proposed using cost models (e.g., XGBoost, MLP) to guide the search for high-performance schedules. These models were crucial for accelerating the tuning process.\n\nHowever, with the evolution of deep learning and the increasing importance of LLMs, most kernels are now produced by compilers such as Triton, template libraries such as CUTLASS or Composable Kernels, or vendor libraries such as cuBLAS. In frameworks like Triton, the search space is much smaller (typically 10 to 100 candidates), making it feasible to compile and benchmark all candidates directly on the target hardware. Therefore, the motivation for using a cost model like LLMTuner in this context needs to be strengthened.\n\n**2. Weak evaluation**\n\nThe paper builds on the TenSet dataset, which is relatively outdated, as noted in Appendix B. The authors extend it to TenSetPerf by adding more hardware and workloads. However, the programs are collected using Ansor, which, to my knowledge, does not support Tensor Cores, the key computational units in modern GPUs, and provides limited support for dynamic shapes, which are essential for LLM workloads (e.g., prefill operations).\n\nIt would strengthen the paper if the authors could evaluate their approach on real workloads (e.g., Llama-3, or ResNet50) and compare the best kernels found by LLMTuner with state-of-the-art implementations from Triton, CUTLASS, cuDNN or cuBLAS. \n\nThat said, I believe the authors’ exploration has potential value. A promising future direction might be to use LLMs not just for tuning but also for generating efficient kernel code."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xAiFCkEOMU", "forum": "FCFM3Yxtsm", "replyto": "FCFM3Yxtsm", "signatures": ["ICLR.cc/2026/Conference/Submission13627/Reviewer_DUzA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13627/Reviewer_DUzA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761342093837, "cdate": 1761342093837, "tmdate": 1762924205704, "mdate": 1762924205704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLMTuner, a novel framework that leverages large language models to analyze tensor program execution behaviors and estimate performance across diverse hardware. This paper introduces a coarse-to-fine design in which a lightweight LLM-based classifier filters suboptimal programs and a finetuned LLM infers execution behavior scores to predict latency. This paper demonstrates strong empirical results compared to existing cost models while maintaining over faster inference speed than DeepSeek R1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a novel cross-hardware cost modeling paradigm that leverages large language models to analyze unified program execution behaviors, effectively overcoming the limitations of traditional hardware-specific cost models.\n\n2. This paper designs a well-structured coarse-to-fine framework that fine-tunes LLMs for accurate and efficient tensor program performance estimation across diverse hardware platforms.\n\n3. This paper demonstrates strong empirical results, showing substantial accuracy gains and tuning efficiency improvements across 11 hardware types, while maintaining low inference cost with lightweight finetuned models."}, "weaknesses": {"value": "1. This paper claims that \"our approach removes the dependence on hand-crafted features\". However, in the methodology, the proposed framework uses LLM to evaluate three core dimensions \"computation, memory, parallelism\". The two statements appear to be contradictory.\n\n2. In hardware-aware structured reasoning, this paper considers three core dimensions: parallelism, memory, and computation. Why do you ignore communication? Could you please clarify it?\n\n3. There is no ablation study for multi-dimension scores (e.g., parallelism, memory, computation). It would be hard to verify whether these \"execution behavior scores\" truly capture transferable performance factors."}, "questions": {"value": "Why ignore communication but only consider three dimensions of parallelism, memory, and computation in hardware-aware structured reasoning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FaJy5oJVbb", "forum": "FCFM3Yxtsm", "replyto": "FCFM3Yxtsm", "signatures": ["ICLR.cc/2026/Conference/Submission13627/Reviewer_wniB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13627/Reviewer_wniB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833800706, "cdate": 1761833800706, "tmdate": 1762924204539, "mdate": 1762924204539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the bottleneck of traditional tensor program cost models—high development costs, poor efficiency, and limited cross-hardware generalization (due to reliance on manual hardware-specific features and extensive profiling)—by proposing LLMTuner, a two-stage coarse-to-fine framework that leverages large language models (LLMs) for cross-hardware tensor program performance estimation.\n- Coarse Filtering Stage: A lightweight LLM-based classifier (using Qwen2.5-0.5B-Instruct) rapidly filters out suboptimal tensor programs, reducing computational overhead for subsequent fine selection. The classifier is trained on the TenSet dataset, with programs labeled \"good\" (top-low latency candidates) or \"bad\" (remaining candidates).\n- Fine Selection Stage: A fine-tuned LLM (Qwen2.5-7B-Instruct via LoRA) performs hardware-aware structured reasoning to infer multi-dimensional execution behavior scores (covering parallelism, memory, and computation metrics). These scores are fed into a 3-layer MLP regression model to predict actual program latency, guiding selection of high-performance candidates.\n- Proposes a cross-hardware cost modeling paradigm that uses LLMs to analyze unified program execution behaviors (e.g., SM utilization, cache hit rates) instead of hardware-specific static features, overcoming limitations of traditional models.\n- Validates LLMTuner’s effectiveness across 6 CPU and 5 GPU platforms (including legacy hardware like NVIDIA K80 and modern platforms like A100/H100): it improves performance estimation accuracy by up to 64.8% vs. general-purpose LLMs (e.g., Gemini, GPT-4o) and traditional cost models (e.g., Ansor, TenSetMLP), and achieves 49.2% accuracy improvement on unseen hardware.\n- Demonstrates practical value for DNN/LLM tuning: LLMTuner discovers 1.47× better program performance with up to 3.27× higher tuning efficiency vs. baselines, and reduces estimation time by over 30× vs. DeepSeek R1 when using fine-tuned lightweight LLMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-  Addresses a practical issue: Traditional tensor program cost models have poor cross-hardware generalization; the work’s motivation is relevant for LLM deployment across diverse hardware.\n- Logical framework design: The coarse-to-fine (lightweight LLM classifier + fine-tuned LLM for execution behavior reasoning) structure balances efficiency and accuracy.\n- Broad experimental scope: Covers 6 CPUs/5 GPUs and diverse workloads (ResNet, BERT, Qwen2-7B), with basic ablation studies supporting design choices."}, "weaknesses": {"value": "- Lack of novelty: Core ideas overlap with prior work (e.g., LLMPerf for LLM-based performance estimation, Moses for cross-hardware models) without clear differentiation.\n- Insufficient technical details: No justification for LLM size choices, underspecified fine-tuning hyperparameters, and no clarity on how execution behavior scores map to latency.\n- Experimental gaps: No tests on edge/accelerator hardware (e.g., Jetson, TPU), no comparison with modern tuners (e.g., MetaSchedule), and no sensitivity analysis for large search spaces.\n- Clarity issues: Table 2 has formatting errors, Figure 4 lacks error bars, and \"zero-shot\" is not clearly defined."}, "questions": {"value": "- How does your \"hardware-aware reasoning\" differ from LLMPerf’s prompt design?\n- Can you provide full fine-tuning hyperparameters for the LLM classifier and reasoner?\n- What metrics are in the parallelism/memory/computation scores, and how do they drive latency predictions?\n- Will you test edge/accelerator hardware and compare with MetaSchedule?\n- How do you define \"zero-shot cross-hardware\" (e.g., is hardware info provided manually/automatically)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EYgjQAq0eu", "forum": "FCFM3Yxtsm", "replyto": "FCFM3Yxtsm", "signatures": ["ICLR.cc/2026/Conference/Submission13627/Reviewer_Guhc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13627/Reviewer_Guhc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908054619, "cdate": 1761908054619, "tmdate": 1762924204244, "mdate": 1762924204244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses about using LLMs to autotune tensor programs. It uses a two stage approach, where it first uses a lightweight LLM classifier to filter out poor candidates and then uses a fine-tuned LLM reasoner to analyze execution behavior and predict latency. It uses a well-established dataset in the domain (Tenset) for its experiments and claims this approach is superior to the existing solutions for training cost models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The direction of work discussed here is timely and interesting. \n2. The paper is written well and easier to understand the content.\n3. It claims to achieve superior results outperforming SOTA LLMs and prior cost models. (Experiments demonstrate that LLMTuner significantly improves estimation accuracy by up to 64.8%, compared with general-purpose LLMs and other cost models on benchmark datasets across 6 CPU and 5 GPU platforms)\n4. The solutions presented can be extended to unseen hardware (A100 and Intel 8575)\n5. It was integrated into a well known framework in the domain (TVM)"}, "weaknesses": {"value": "1. Overall, it feels like an ad hoc solution where multiple LLMs are brought together to come up with some solution rather than a carefully well thought out problem solution pair. \n2. Since data collection on GPUs is fast, whether we need this kind of solution itself is a question. \n3. The hardware factors are still limited to the specific architectures. Like, there are no SMs in CPUs. \n4. Would have been better if numbers about finetuning overhead and resource consumptions were added comparatively with existing approaches.\n5. Interpretability of the approach and formalization is questionable (added more context in the questions section)"}, "questions": {"value": "1. Do we need LLMs for this kind of solution in the first place? It is true that it can take a few days to collect data for a classical ml-based cost model. However, data collection in both CPUs and especially GPUs is pretty fast.\n2. Regarding the hardware factor extraction step, how are you making sure that the LLM is not hallucinating when getting data from manuals?\n3. Can you formalize the reasoning process explained in the paper mathematically? For example, how representative and explainable are the intermediate scores to the runtime performance of tensor programs? How do you validate that LLMTuner is learning new reasoning patterns?\n4. Why do we need a two-step process here? Wouldn't initially pre-training the first classifier make the results biased? Did you evaluate whether the reasoning model overfits to known hardware specifications mentioned during finetuning?\n5. It seems the regression model at the end is trained separately. Doesn't it break the end-to-end differentiability?\n6. Will this solution work beyond CPUs and GPUs? (e.g, TPUs) Can this go across heterogeneous hardware (CPU to GPU)?\n7. Did you explore other NN architectures, such as GNNs, to see their performances? I suspect they can perform well too, considering we are dealing with tensor programs here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RT4v02Ddtl", "forum": "FCFM3Yxtsm", "replyto": "FCFM3Yxtsm", "signatures": ["ICLR.cc/2026/Conference/Submission13627/Reviewer_zf6E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13627/Reviewer_zf6E"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976658838, "cdate": 1761976658838, "tmdate": 1762924203909, "mdate": 1762924203909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LLMTuner, a framework using LLMs to predict tensor program performance by analyzing unified execution behaviors (parallelism, memory, computation) instead of hand-crafted features. It employs a two-stage approach: coarse filtering via LLM classifier and fine ranking via a distilled LLM reasoner that generates scores for regression-based latency prediction. Experiments show 64.8% accuracy improvement on seen hardware and 49.2% on unseen hardware."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The shift from hand-crafted, hardware-specific features to LLM-based unified execution behavior analysis tackles a significant pain point in tensor program tuning—poor cross-platform generalization of traditional cost models. This is a meaningful direction that addresses real engineering challenges.\n\n2. The two-stage coarse-to-fine design achieves a good balance between accuracy and efficiency. Knowledge distillation from DeepSeek-R1 to Qwen2.5-7B reduces inference time by 30 times while maintaining comparable performance, demonstrating practical applicability for real-world deployment."}, "weaknesses": {"value": "1. The core technical contribution is essentially fine-tuning LLMs with structured prompts and supervised distillation from DeepSeek-R1, followed by a regression model on top of the LLM outputs. While the application domain is novel, the methodology itself (prompt engineering + distillation + regression) is relatively incremental and lacks fundamental innovation in either the LLM techniques or the performance modeling approach.\n\n2. The generalization claims are a bit overstated in two ways. First, the \"unseen hardware\" in Table 3 (A100, H100, RTX 4070) all share the CUDA architecture with the training platforms (K80, T4). True zero-shot generalization should be demonstrated on fundamentally different ISAs like AMD GPUs, Apple Silicon, or Google TPU. Second, the LLMTuner-Reasoner outputs scores for what appears to be a fixed set of metrics like SM utilization and warp efficiency. It's unclear whether these metrics are hardware-adaptive or remain static across all platforms. For example, Blackwell architecture supports native FP4 Tensor Cores while earlier architectures don't—does the LLM understand such hardware-specific capabilities? How does a fixed metric schema handle the fundamental differences between NVIDIA's streaming multiprocessors, AMD's compute units, and Apple's neural engine? The paper doesn't clarify whether the evaluation schema varies by hardware or if it assumes a one-size-fits-all metric set, which undermines the claim of hardware-agnostic reasoning.\n\n3. Section C.3 reveals a concerning data leakage issue. The paper states that \"real latency measurements are provided as additional input to DeepSeek-R1 to guide its reasoning\" during training data generation. This creates a problematic chain: DeepSeek-R1 receives ground-truth latency hints, generates reasoning and scores based on these hints, then Qwen2.5-7B learns from this distillation data, and finally the regression model is trained on these scores paired with the same ground-truth latencies. Even though Listing 1 shows the hint is not explicitly included in the output, the reasoning and scores may still implicitly encode latency information rather than representing genuine program-hardware analysis. The paper provides no ablation to quantify what percentage of the 100K training samples used hints, nor does it compare performance when trained purely on hint-free reasoning data. This makes it impossible to assess whether the model's strong performance comes from learned reasoning patterns or from memorizing latency-conditioned outputs."}, "questions": {"value": "1. The paper claims to use a structured multi-dimensional evaluation schema but Appendix E only shows a partial example with 3 sub-metrics. Can you provide the complete metric schema with all sub-metrics used in practice? More importantly, how does this schema adapt to different hardware architectures? Even within the same vendor, GPU architectures have significant differences—for instance, Blackwell supports native FP4 Tensor Cores, Hopper supports FP8, while Ampere supports neither. Does the LLM understand such generation-specific capabilities, or does it treat all NVIDIA GPUs uniformly? Beyond NVIDIA, how do you handle metrics for platforms with fundamentally different designs—NVIDIA SMs versus AMD CUs versus Apple Neural Engine? Are the metrics hardware-adaptive or do you use a fixed schema across all platforms?\n\n2. Regarding the latency hints mentioned in Section C.3, what percentage of the 100K distillation samples actually used ground-truth latency as hints? This is critical for understanding potential data leakage. Can you provide an ablation comparing LLMTuner-Reasoner's Top-1 and Top-5 accuracy when trained on distillation data generated with hints versus without hints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zMw5PvuU8I", "forum": "FCFM3Yxtsm", "replyto": "FCFM3Yxtsm", "signatures": ["ICLR.cc/2026/Conference/Submission13627/Reviewer_EMgf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13627/Reviewer_EMgf"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13627/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762093899957, "cdate": 1762093899957, "tmdate": 1762924203546, "mdate": 1762924203546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}