{"id": "tds7TrLx9N", "number": 11138, "cdate": 1758190512168, "mdate": 1759897605566, "content": {"title": "Teachers That Listen: Adaptive Student-Aware Distillation for Reasoning", "abstract": "Knowledge distillation is a standard approach to compress the capabilities of large language models  into smaller students. However, standard  distillation methods often produce suboptimal results due to a mismatch between teacher-generated rationales and the student's specific learning requirements. In this paper, we introduce the Adaptive student-aware Distillation for Reasoning (AdaptDistill), designed to bridge this gap by iteratively identifying the student's errors and allowing the teacher to refine its explanations according to the student's needs. Each iteration directly targets the student's learning deficiencies, motivating the teacher to provide tailored rationales that specifically address these weaknesses for better learning. Empirical evaluations on various challenging mathematical and commonsense reasoning tasks demonstrate that our adaptive distillation approach, AdaptDistill, significantly outperforms standard distillation methods, achieving significant performance gains. Our work fundamentally reframes knowledge distillation as an iterative teacher–student interaction, effectively leveraging dynamic refinement by the teacher for better knowledge distillation.", "tldr": "An adaptive and iterative knowledge distillation approach", "keywords": ["Iterative Distillation", "Knowledge transfer", "Smaller models"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/37ce80f786810ad05e75075c55df7c82dd450984.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The core idea of this work is to reframe knowledge distillation as an iterative teacher-student interaction process, aiming to bridge the gap between teacher-generated generic reasoning chains (Rationales) and the specific learning needs of the student model. In each iteration, the teacher first generates initial rationales. The student model learns from them, attempts the task, and reveals its learning difficulties and errors. Subsequently, the teacher refines and regenerates explanations specifically, based on the student model's error feedback and historical performance. The student model is ultimately fine-tuned on a curated dataset mixed with its own correct reasoning traces and the teacher's refined traces.Empirical results show that the AdaptDistill approach achieves significant performance gains compared to standard one-shot distillation methods on mathematical and common-sense reasoning tasks, including GSM8K and MATH."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The paper proposes a novel closed-loop adaptive distillation framework. By explicitly reintroducing the student model's error feedback into the teacher's generation step, it achieves customized guidance targeting individual student weaknesses, effectively addressing the mismatch between teacher output and student needs in standard distillation.\n2) The method achieves a performance increase of up to 20% in accuracy on several challenging reasoning tasks compared to basic one-shot distillation baselines.\n3) Experiments demonstrate that the method not only improves accuracy on tasks within the training distribution but also maintains and improves performance on Out-of-Domain (OOD) tasks (such as StrategyQA and TheoremQA), indicating an enhancement in the transferability of reasoning skills."}, "weaknesses": {"value": "1) The core technical component of this method relies on the powerful teacher model (Llama-3.2-70B) performing the steps of \"identifying learning gaps\" and \"generating customized, refined explanations\" via Prompt Engineering. Although the framework concept is novel, the mechanism for \"identifying learning gaps\" and \"refining explanations\" lacks a quantifiable, learnable, modular design. Instead, it relies on the black-box reasoning capability of a Large Language Model (LLM) to act as an \"in-context optimizer.\" This diminishes the purely technical innovation of the method itself and poses difficulties for future research in reproduction and improvement.\n2) The paper's primary comparative baseline is \"Standard One-Shot Knowledge Distillation\" (i.e., performing CoT distillation in a single pass, following Shridhar et al. (2023)). This is overly simplistic given the current advancements in the knowledge distillation field. As you pointed out, the paper fails to compare against several recent and comparable methods. It lacks empirical comparison against recent State-of-the-Art (SOTA) works like DistilLLM and MiniLLM, which makes it difficult to conclusively prove AdaptDistill's leading position at the current technological frontier. While the paper mentions several iterative or adaptive distillation methods in the related work section (e.g., Wang et al. (2023), Adarsh et al. (2025), Agarwal et al. (2024)), it does not conduct direct performance comparisons between AdaptDistill and these iterative methods that are most similar in mechanism, making it difficult to fully demonstrate the superiority of its unique teacher feedback mechanism.\n3) The validation set V used in the paper consists of only 20 samples. The authors justify this by the need to ensure the teacher model's context window can accommodate the historical records H for all iterations. However, using the historical performance of a set of only 20 samples to guide the teacher in targeted content generation over the entire training set $\\mathcal{D}$ for multiple rounds may lead to biased guidance or cause the teacher's refinement process to overfit to these 20 samples, thereby affecting the final student model's generalization and robustness."}, "questions": {"value": "1) Given that your method is iterative and adaptive reasoning distillation, please supplement the experimental results with performance comparisons against representative similar works, such as Wang et al. (2023) or Adarsh et al. (2025), which you cited in your related work. Please also explain why comparisons with DistilLLM Ko et al. (2024) and MiniLLM Gu et al. (2023), which are established or recent works in knowledge distillation, were omitted. If there are technical differences that make direct comparison infeasible, please clarify this in the paper.\n2) The \"identifying learning gaps\" and \"generating refined explanations\" steps are crucial to AdaptDistill, primarily implemented via prompting the teacher model. Please publicly release the complete Prompt templates used to generate the gap information and the refined rationale in the Appendix. This is necessary to ensure the reproducibility of the experiments and allow readers to better understand the teacher model's decision mechanism.\n3) Please provide a more in-depth discussion on the choice of using only 20 samples for the validation set. Furthermore, conduct an ablation study to compare the performance when using a larger validation set (e.g., 100 or 200 samples) under the constraint that the historical record H is limited to a small sliding window (e.g., only considering the 20 most recent samples instead of accumulating all history), to verify the robustness and representativeness of the current small validation set."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ln2cuoNFt2", "forum": "tds7TrLx9N", "replyto": "tds7TrLx9N", "signatures": ["ICLR.cc/2026/Conference/Submission11138/Reviewer_A1UH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11138/Reviewer_A1UH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957925892, "cdate": 1761957925892, "tmdate": 1762922309588, "mdate": 1762922309588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The Adaptive student-aware Distillation for Reasoning (AdaptDistill) is designed to bridge this gap by iteratively identifying the student’s errors and allowing the teacher to refine its explanations according to the student’s needs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes an innovative solution to the distributional mismatch between the teacher’s rationales and the student’s learning bottlenecks, effectively enhancing instructional alignment.\n\n2. The framework is rigorously defined and the experiments are carefully designed, ensuring the reliability and reproducibility of the results.\n\n3. The proposed method shows strong potential to significantly improve the distillation process, especially for tasks involving complex reasoning."}, "weaknesses": {"value": "- Lack of Ablation Studies: While the method shows strong results, the paper could benefit from more ablation studies comparing AdaptDistill with other state-of-the-art iterative distillation methods. This would provide a clearer picture of how AdaptDistill fares relative to similar approaches.\n- Limited Task Variety: The experiments primarily focus on mathematical and commonsense reasoning tasks. While these are important, the paper could be strengthened by demonstrating the method’s effectiveness across a broader range of tasks or domains.\n- Scalability Concerns: The iterative nature of the method requires multiple rounds of distillation, which can become computationally expensive. The paper could discuss strategies for making this process more efficient or scalable, particularly when applying it to larger datasets or models.\n- Model Transferability: Although the paper tests AdaptDistill on different student models, further exploration into the transferability of the learned knowledge across different architectures would be valuable. It would also be useful to understand how AdaptDistill performs with varying model sizes."}, "questions": {"value": "1. Could the authors provide more details on the scalability of AdaptDistill? How would it perform with much larger datasets or models?\n2. How does AdaptDistill compare to other advanced distillation methods, such as reinforcement learning-based or self-guided distillation approaches?\n3. What are the potential limitations of the iterative refinement process in terms of model convergence and overfitting after many iterations?\n4. How would AdaptDistill perform on tasks beyond the domains tested, particularly on tasks that require high levels of generalization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "w1MnPuw14F", "forum": "tds7TrLx9N", "replyto": "tds7TrLx9N", "signatures": ["ICLR.cc/2026/Conference/Submission11138/Reviewer_w69m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11138/Reviewer_w69m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972943908, "cdate": 1761972943908, "tmdate": 1762922309195, "mdate": 1762922309195, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a distillation framework where the student is placed in the loop of data construction instead of passively imitating the teacher's CoT. For a pool of training questions, the student first attempts to solve them. If the student’s answer is judged correct (using an automatically gradable validation/evaluation setup), that example is directly added to the training set as a “student-solvable” instance. If the student’s answer is incorrect, the method prompts the teacher with information about the student’s observed mistakes (from a validation set) and asks the teacher to regenerate a reasoning trajectory that is tailored to the student’s current weaknesses. The student is then trained on this mixed set (student-solvable + teacher-regenerated-for-student), and the process is iterated."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Instead of assuming “teacher CoT = optimal supervision,” the paper explicitly conditions data generation on the current student’s performance. This is a reasonable correction to the common mismatch between long, teacher-style CoT and what a smaller student can actually learn.\n\n2. The Iteration ablation shows progressive gains across iterations, which is good evidence that the loop is actually doing work, not just adding noise.\n\n3. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. Strong, partly unstated assumption on the teacher. \nThe method implicitly assumes the teacher is strong and instruction-following enough to 1) interpret student errors (potentially noisy, coming from a validation-based diagnosis) and 2) rewrite a solution in a more student-friendly style. That’s a stronger assumption than vanilla CoT distillation, where the teacher only needs to solve the task. \n\n2. The paper mainly compares (base) vs standard distillation vs its own iterations 1, 2 and 3. It does not compare against closely related baselines (e.g., [1], [2], [3]) that also do error-/fault-/student-aware distillation or iterative, student-on-policy data collection. Without these, it’s hard to tell whether the gain comes from the specific “validation-conditioned prompting” the paper proposes, or just from doing iterative, student-aware distillation.\n\n3. All main tasks are auto-gradable math/logic. This is the friendliest setting for the method, because correctness is easy to detect. It is unclear how the same loop would work for non-gradable or open-ended tasks (dialogue, safety, long-form QA) where validation cannot simply say correct/incorrect.\n\n4. Since the student’s own success controls which data gets kept, there is a risk of data distribution collapsing around what the current student already finds learnable, unless the teacher’s regenerated data is sufficiently diverse and genuinely addresses the failure. The paper does not deeply analyze this risk.\n\n5. The loop requires: student forward on (many) training items $\\rightarrow$ judging $\\rightarrow$ teacher regeneration for the failed ones $\\rightarrow$ retraining. For small math benchmarks, this is fine; for larger, multi-domain corpora, the cost of per-failure teacher prompting could become substantial.\n\n\n6. Because the paper reports iterations vs baseline but not “without error-conditioned prompting” or “with a weaker teacher,” it’s unclear which part of the pipeline (iteration, student-in-the-loop selection, or error-informed teacher prompting) contributes most.\n\n[1] Li Z, Ji Y, Meng R, et al. Learning from committee: Reasoning distillation from a mixture of teachers with peer-review[J]. arXiv preprint arXiv:2410.03663, 2024.\n\n[2] Wu Z, Li X, Liu Z, et al. Enhancing Long-Chain Reasoning Distillation through Error-Aware Self-Reflection[J]. arXiv preprint arXiv:2505.22131, 2025.\n\n[3] Zhao X, Xu T, Wang X, et al. Boosting LLM Reasoning via Spontaneous Self-Correction[J]. arXiv preprint arXiv:2506.06923, 2025."}, "questions": {"value": "1. When you say the teacher is “prompted by validation-set errors,” is the teacher given (a) each instance’s wrong student attempt and asked to fix it, or (b) an aggregated description of common student mistakes (e.g. “the student often skips steps / omits units / stops early”) that is then applied to new items? \n\n2. How sensitive is the method to the teacher’s capability? If you replace the teacher with a weaker model of the same family, does the student still benefit from the error-informed regeneration? This matters because your method delegates the hard part (understanding and rewriting mistakes) to the teacher.\n\n3. There are recent works that also “let the teacher see the student’s mistake and generate a better rationale” or that do iterative, student-on-policy distillation. Why are these not included as baselines? Can you add at least one representative error-aware or verifier-/review-based distillation method under the same compute / teacher-token budget?\n\n4. Did you evaluate your loop on tasks where correctness can’t be checked automatically and you have to use an LLM-as-judge? If yes, that seems to introduce a second strong model into the pipeline. Can you clarify whether this changes the method’s assumptions or makes it less practical?\n\n5. How do you prevent the iterative process from overfitting to the current student’s local failure modes (e.g., always generating longer, more verbose CoT for everything)? Do you track diversity/length/structure drift of the regenerated rationales over iterations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qYU0yGy1ww", "forum": "tds7TrLx9N", "replyto": "tds7TrLx9N", "signatures": ["ICLR.cc/2026/Conference/Submission11138/Reviewer_cY2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11138/Reviewer_cY2L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980346242, "cdate": 1761980346242, "tmdate": 1762922308734, "mdate": 1762922308734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AdaptDistill, an adaptive and iterative distillation framework for reasoning tasks. Unlike standard one-shot distillation, which ignores student-specific mistakes, AdaptDistill continuously refines the teacher’s rationales based on the student’s observed errors. The student is then updated using a curated mix of its own correct traces and the teacher’s improved explanations. Experiments on mathematical and commonsense reasoning benchmarks demonstrate consistent accuracy gains (up to +20%) across multiple student models. The method also improves out-of-domain generalization and outperforms longer standard training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well-motivated.\n\nKnowledge distillation for reasoning is an important problem, especially for improving inference efficiency.\n\nThe proposed adaptive interaction between teacher and student is intuitively appealing and empirically effective."}, "weaknesses": {"value": "1. The novelty of the approach is somewhat limited. The idea of iterative, student-aware feedback has been explored in several prior distillation frameworks where teachers provide targeted corrections based on student failures.\n\n2. The paper lacks comparison with stronger or more recent state-of-the-art distillation baselines, which makes it difficult to fully assess the relative improvement."}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7xvDUKvamL", "forum": "tds7TrLx9N", "replyto": "tds7TrLx9N", "signatures": ["ICLR.cc/2026/Conference/Submission11138/Reviewer_7wcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11138/Reviewer_7wcD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762387925705, "cdate": 1762387925705, "tmdate": 1762922308289, "mdate": 1762922308289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}