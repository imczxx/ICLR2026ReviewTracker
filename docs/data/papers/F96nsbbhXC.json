{"id": "F96nsbbhXC", "number": 4133, "cdate": 1757608848354, "mdate": 1759898051495, "content": {"title": "AutoCode: LLMs as Problem Setters for Competitive Programming", "abstract": "Writing competitive programming problems is exacting. Authors must: set constraints, input distributions, and edge cases that rule out shortcuts; target specific algorithms (e.g., max-flow, dynamic programming, data structures); and calibrate complexity beyond the reach of most competitors. We argue that this makes for an ideal test of general large language model capabilities and study whether they can do this reliably. We introduce AutoCode, which uses multiple rounds of validation to yield competition-grade problem statements and test cases. On held-out problems, AutoCode test suites approach 99% consistency with official judgments, a significant improvement over current state-of-the-art methods like HardTests, which achieve less than 81%. Furthermore, starting with a random seed problem, AutoCode can create novel variants with reference and brute-force solutions. By cross-verifying these generated solutions against test cases, we can further filter out malformed problems. Our system ensures high correctness, as verified by human experts. AutoCode successfully produces novel problems judged by Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.", "tldr": "We introduce AutoCode, which uses multiple rounds of validation to yield competition-grade problem statements and test cases, far surpassing current state-of-the-art performance.", "keywords": ["Large Language Models", "Competitive Programming", "Test Case Generation", "Problem Generation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d12c6908b6b39e820e9ba31ca7f2ab9860fdf2c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper focuses on test generation and problem creation for competitive programming problems. First, they show that their test generation framework can achieve very low false positive and false negative rates compared to prior SOTA work. The test generation framework consists of using LLMs to construct validator, generator, and checker with novel prompting techniques specifically designed for more robust checking. Building on the robust test generation, they show that they can prompt LLMs to create very high quality problems (judged by top competitive programmers) by seeding from existing problems and then going through testing to filter problems such that the LLM-generated brute-force solution (acting as ground truth) and the LLM-generated solution agree."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Significantly improve the test generation false positive and false negative rate for the competition programming\n- Demonstrate that model can generate novel problems not only useful for training but also can generate very high quality deemed as good as top competition problems (ICPC/IOI-Level)\n- Analysis and ablation showing many insights of the results, the finding from the human experts annotation is especially valuable"}, "weaknesses": {"value": "The critical weakness of this paper is the lack of reproducibility. The paper uses specific LLM prompting techniques to achieve impressive results but fails to provide any of the prompts used in the work. Many details are also missing, for example, how the generator works and the problem creation procedure are not clearly explained."}, "questions": {"value": "- Although stated in Finding 1: \"LLMs can generate solvable problems that they themselves are unable to solve,\" this kind of problem would be filtered by the solvability check right? Could you clarify if the system using the filtering process described in the beginning of Section 5 would be able to produce such problems at the end?\n- What is the \"prompt optimization\" refer to? Line 291 shows the ablation \"without prompt optimization\" and in line 321 \"Prompt optimization turns out to be especially important;\" there seems to be no explanation of the prompt optimization in the paper.\n- In 3.2 for the generator, it is stated that they use strategies: Small Data Exhaustion, Randomized and Extreme Data, and TLE-Inducing Data, but didn't explain how these strategies are implemented. Are they being implemented with LLM by using LLM to generate a program to perform this kind of strategy?\n- Can you explain GENERATECHECKERSCENARIOS in Section 3.3? How does it generate the (input, contestant_out, ref_out, verdict) data?\n- Can you clarify which parts of the test generation required access to ground-truth programs? is it just the for thchecker?\n- In line 285 \"all results in this table are generated by GPT-5-High\", do you mean the model used in the test generation system or the 33 submissions?\n- Since competition programs accept or not sometimes depending on the timeout set for the program, how is timeout set for the generated problems?\n- For the problem creation prompt that prompts the model to create new problems from seed problems, do you need to specifically guide the model what kind of variations they can make and does it bias the model to alter the seed problem in specific ways?\n- Are the LLM prompting used in this work zero-shot or few-shot like you need to provide more examples to guide the model? Can you provide the prompts used in this work to help with reproducibility?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dCvnpeIisN", "forum": "F96nsbbhXC", "replyto": "F96nsbbhXC", "signatures": ["ICLR.cc/2026/Conference/Submission4133/Reviewer_gTC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4133/Reviewer_gTC1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985440350, "cdate": 1761985440350, "tmdate": 1762917191407, "mdate": 1762917191407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AutoCode, an LLM agent system for writing and developing competitive programming problems. AutoCode generally consists of two parts: a test case generator, and a problem writer.  The test generation component implements multiple components including validator, generator, checker, and interactor. Experimental results indicate that the test cases generated by AutoCode outperform current state-of-the-art baselines in terms of both FPR and FNR metrics. The problem writing component has achieved excellent results in human evaluation, and the findings show that LLMs are capable of generating high-quality and high-difficulty problems."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. In terms of test case generation, AutoCode has indeed gone deeper and more refined than previous work (e.g., HardTests, CodeContests+). For instance, it enhances the validator by adding validator tests to further ensure the validator’s correctness, and it is capable of writing interactors for interactive problems.\n2. This paper conducts pioneering research on problem writing using LLMs and validates the problem-writing capabilities of LLMs through large-scale human evaluations.\n3. The study on the value alignment between LLMs and human experts is highly insightful. Given that LLM-as-a-judge is currently a highly mainstream method, the results (Fig. 4) demonstrate the limitations of LLMs in such complex OOD tasks.\n4. This paper is well-written. I am enjoying reading this paper."}, "weaknesses": {"value": "1. **Does this paper propose a better test case construction method?**, The generator-validator-checker-interactor workflow is a widely adopted problem-construction process in competitive programming, and has also been used more or less in previous studies (e.g., HardTests, CodeContests+). The similarity between AutoCode’s overall workflow and that of previous works may raise questions about the source of improvements in its evaluation. First, this study is conducted based on SOTA models such as o4 and GPT-5, while previous works are before the release of such advanced models. Second, prompts are a critical factor affecting performance—providing a open agent framework that includes prompts would be a significant contribution to the community. However, this paper does not disclose the prompts it used, thus diminishing its reproducibility and overall contribution.\n2. **Questions on the soundness of the LLM problem writing study.** I find the topic of LLM problem writing very interesting, but I do not believe this paper has conducted an in-depth and rigorous study. First, it is an experiment entirely based on human evaluations, yet I think some of the criteria could have been tested through objective experiment. For instance, the paper mentions a criterion called \"Model-Training Usable\"—this criterion could be fully verified through actual training results, rather than relying on human evaluations. Additionally, the human evaluation lacks details, such as the average time each expert spent evaluating each piece of data and the profiles of the experts. Some conclusions also lack sufficient evidential support; for example, for Findings 2, it is recommended to incorporate more case studies for further discussion. Furthermore, the sample size of this experiment should be placed in a more prominent position, such as the caption of Figure 3. I read through this section several times but still could not find this number. Finally, regarding the most critical part of this experiment: how did the authors guide LLMs to generate problems? The entire problem-writing process lacks a detailed description,  nor does the prompt provided, so I believe it lacks reproducibility."}, "questions": {"value": "1. How do you develop the sandbox system? Submissions in CodeContests may come from many different online judge systems, using a variety of operating systems (Linux, Windows), compilers (GCC and MSVC spanning multiple eras), and interpreters (Python 2, Python 3, and JVMs of various versions). Therefore, if you want to evaluate the consistency with official verdicts, the first thing to do is achieve environment alignment—and I think this is extremely difficult. Many submissions fail to produce the same verdicts even when using the exact same original test cases. How do you solve this problem?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dt2rC5oJ2f", "forum": "F96nsbbhXC", "replyto": "F96nsbbhXC", "signatures": ["ICLR.cc/2026/Conference/Submission4133/Reviewer_hPcp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4133/Reviewer_hPcp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099397601, "cdate": 1762099397601, "tmdate": 1762917190005, "mdate": 1762917190005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Auto-code is about being able to generate new competitive programming problems. This requires being able to generate new problem descriptions from scratch or by modifying existing coding problem descriptions. And it requires providing reference solutions to the new problems to show they are solvable and well formed.  And to do that requires being able to generate high quality test cases and be able to verify if a code solution can pass all the test cases, and verify the code solution is solving the new programming problem description.\n\nThis paper describes a pipeline of many agents that work together to accomplish that task, and benchmarks some of the components in the pipeline against other approaches previously tried."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper describes a system of agents that produce new competitive programming problems and solutions that appear to be of high quality when judged by human experts.  The paper does nicely describe the rigorous validation and verification required to vet out proposed problems and their proposed solutions as correct or incorrect, by ensuring a reliable and broad set of test cases is produced. The paper does a number of comparisons that shows the quality of the system, like how well it is able to generate test cases compared to other test case generation systems."}, "weaknesses": {"value": "The description of the system in the paper is well written and extensive, but I do worry that without a repo of the source code accessible to researchers the ability to leverage this work and build upon it will be very difficult.\n\nThe ability to produce new competitive programming problems that humans judge as good is useful on it's own, but if those new problems and correct solutions are able to be leveraged to further finetune the LLM and improve it's ability to solve competitive programming problems, that would be strong indicator that the problems being generated are useful in an interesting way."}, "questions": {"value": "Are you planning to release the source code for the system described in the paper?\n\nAre you able to leverage the generated programming problems and solutions to finetune an LLM and show it leads to the LLM being a better problem solver?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W4TV0hFTBw", "forum": "F96nsbbhXC", "replyto": "F96nsbbhXC", "signatures": ["ICLR.cc/2026/Conference/Submission4133/Reviewer_2TeP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4133/Reviewer_2TeP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146601982, "cdate": 1762146601982, "tmdate": 1762917189717, "mdate": 1762917189717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}