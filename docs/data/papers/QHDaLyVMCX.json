{"id": "QHDaLyVMCX", "number": 281, "cdate": 1756733369798, "mdate": 1759898269374, "content": {"title": "GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models", "abstract": "While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations—where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.", "tldr": "", "keywords": ["LVLM", "GUI Agent", "Critic Model", "Data Flywheel", "Test-Time Scaling"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/70a7de78084192527e38421a333414c1d3d526ca.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tries to address the challenge of \"the irreversibility of agent operations\" in current GUI agents by proposing GAIA, a framework for iteratively training a GUI critic model. Through a two-round training process, the resulting critic model, ICM and ICM-r2, demonstrates improved performance across several benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel Training Framework:** The paper introduces an innovative Data Flywheel System for training GUI action critic models. The two-round training process enables the critic model's capabilities to be iteratively refined and improved.\n2. **Improved Agent Performance:** By leveraging the proposed critic model (ICM), the evaluated agents show enhanced performance across multiple benchmarks, demonstrating the practical utility of the approach.\n3. **High-Quality Data Curation:** The method for constructing positive and negative samples is well-designed. By using existing GUI agents and validating their actions on human-annotated datasets, the authors ensure that the negative samples reflect plausible, realistic errors that an agent might make. This process guarantees the quality and relevance of the training data."}, "weaknesses": {"value": "1. **Ambiguous Definition of TTS:** The concept of TTS remains vaguely defined. The paper emphasizes TTS in the title, abstract and introduction, yet the GUI Agent community has at least two distinct interpretations: one involving proposing multiple candidate operations at each step (e.g., GTA1[1]), and another involving magnifying candidate regions via cropping (e.g., DiMo-GUI[2]). The authors should provide a more precise definition of their interpretation of TTS to avoid confusion.\n2. **Efficiency Concerns:** The most significant drawback of the proposed method is its inefficiency. The inference paradigm, which requires the agent to perform N rollouts for ICM to judge, is impractical for real-world applications. Moreover, many of these rollouts are likely to be redundant, yielding identical outcomes from the critic model. An approach like GTA1[1], which generates multiple distinct candidates in a single forward pass, would seem to be a much more sensible and efficient pairing for this type of critic model.\n3. **Justification for Excluding Reasoning:** The paper's justification for not incorporating a reasoning step in the critic model (lines 74-78) is unconvincing. Firstly, the analogy to biological research is weak; the behaviors of LLMs/VLMs are shaped by their training data and architecture and do not necessarily align with human cognition. Secondly, there is strong evidence from online benchmarks results that including a reasoning step significantly improves GUI agent performance[3, 4]. The claim that omitting reasoning is superior cannot be accepted without solid experimental proof, which is currently lacking (see Weakness 6). Intuitively, a reasoning process would help the agent better analyze scenarios with multiple valid actions or misleading cues, leading to a more robust judgment.\n4. **Missing Key Baselines:** The experimental comparison is missing some crucial baselines. For example, GUI-Critic-R1[5] is mentioned in the introduction as related work but is not included as a baseline in Table 1, where it would be more compelling than some of the other baselines listed. Furthermore, since ICM is fine-tuned from Qwen-2.5-VL, the original Qwen-2.5-VL should also be evaluated as a critic to clearly demonstrate that the proposed training is necessary and effective.\n5. **Unconvincing Ablation Study on RCM:** The results for the Reasoning Critic Model (RCM) in the ablation study are not convincing because the comparison is unfair. The RCM was trained with a different setting and on a dataset that is only about one-tenth the size of that used for ICM-r2. To make a fair and meaningful comparison, I would request an experiment where the RCM is trained using the same methodology and data scale as ICM and ICM-r2, followed by a direct comparison of their Critic Accuracy.\n6. **Lack of Online Benchmark Evaluation:** The evaluation is confined to offline benchmarks (AndroidControl, GUI-Odyssey, Screnspot-v2), which have a significant gap compared to dynamic, real-world environments. The advancement of the GUI agent field depends on progress in real-world applicability, not just performance on static benchmarks. It is critical to get the true performance gain provided by ICM in a live setting and whether this benefit outweighs its substantial efficiency costs. This point is fundamental to assessing the overall impact of this work.\n\n[1] Yang Y, Li D, Dai Y, et al. Gta1: Gui test-time scaling agent[J]. arXiv preprint arXiv:2507.05791, 2025.\n\n[2] Wu H, Chen H, Cai Y, et al. DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning[J]. arXiv preprint arXiv:2507.00008, 2025.\n\n[3] Wang X, Wang B, Lu D, et al. Opencua: Open foundations for computer-use agents[J]. arXiv preprint arXiv:2508.09123, 2025.\n\n[4] Liu Z, Xie J J, Ding Z, et al. ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data[J]. arXiv preprint arXiv:2509.15221, 2025.\n\n[5] Wanyan Y, Zhang X, Xu H, et al. Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation[J]. arXiv preprint arXiv:2506.04614, 2025."}, "questions": {"value": "1. I noted an interesting and seemingly contradictory trend in the results. On the grounding tasks in AndroidControl and GUI-Odyssey, ICM and ICM-r2 provide a significant performance boost for UI-Tars but only a marginal improvement (or even a slight degradation) for Qwen-2.5-VL. However, this trend is completely reversed on Screenspot-v2. Could the authors provide some insight into why this discrepancy exists across different grounding tasks? It does not appear to be a simple domain-shift issue, as the mobile subset of Screenspot-v2 exhibits the same pattern.\n2. In Figure 8, there is a large gap between the ICM/ICM-r2 performance and the Pass@N upper bound (It doesn't matter). While using Pass@N as a performance ceiling is reasonable, why was a simpler baseline, such as a majority voting mechanism over the N rollouts, not included? I am very interested to see where a voting-based approach would fall on this chart, would it be comparable to the ICM-series methods, or significantly worse?\n3. I am curious about the performance of the ICM-series on OOD benchmarks, such as Mind2Web. Such an evaluation would be valuable for demonstrating the generalization capabilities of the critic model to new domains.\n4. A minor suggestion for writing clarity: In Section 3.2.1, it would be helpful to explicitly state that AndroidControl and GUI-Odyssey are high-quality, human-annotated datasets. This would clarify for readers less familiar with the field why this data can be reliably used as ground truth without extensive cleaning or filtering."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C6ZjHriLH9", "forum": "QHDaLyVMCX", "replyto": "QHDaLyVMCX", "signatures": ["ICLR.cc/2026/Conference/Submission281/Reviewer_ZSGR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission281/Reviewer_ZSGR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760529948581, "cdate": 1760529948581, "tmdate": 1762915484910, "mdate": 1762915484910, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "1. The paper proposes GAIA (GUI Action Critic’s Data Flywheel System), a framework to address the irreversibility of errors in GUI agents by introducing an iterative critic mechanism. The system leverages a data flywheel that curates positive and negative action examples to train an Intuitive Critic Model (ICM).\n\n2. The ICM evaluates action correctness, selecting higher confidence operations and guiding agent actions. Through repeated refinement, a self-improving cycle is established, where successive critics (e.g., ICM-r2) achieve enhanced discrimination ability.\n\n3. The approach is integrated with Test-Time Scaling (TTS) to filter stochastic actions and ensure only reliable operations are executed. The framework further employs a Best-of-N strategy during inference, aiming to improve robustness of GUI agents.\n\n4. Experiments are conducted with both closed-source and open-source agents, including GPT-4o and UI-TARS, across multiple benchmarks. Results suggest that GAIA provides performance improvements in action correctness, task planning, and grounding capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper addresses a clear challenge in GUI agents, namely the irreversibility of erroneous actions, and attempts to mitigate catastrophic deviations through a critic-guided mechanism.\n\n2. The proposed GAIA framework is clearly structured, with the notion of a data flywheel and iterative critic model (ICM, ICM-r2) presented in a systematic way.\n\n3. The integration with Test-Time Scaling (TTS) and the Best-of-N strategy is straightforward and easy to follow, showing how the critic can filter unreliable actions.\n\n4. The paper includes both closed-source and open-source agents (e.g., GPT-4o, UI-TARS) in the evaluation, which helps illustrate the generality of the proposed framework.\n\n5. The presentation quality is reasonable, with figures such as the data flywheel pipeline aiding in understanding the proposed iterative self-improving cycle."}, "weaknesses": {"value": "1. The overall contribution of GAIA appears somewhat incremental, since the Intuitive Critic Model (ICM) and the data flywheel mainly combine existing elements such as Test-Time Scaling and iterative filtering.\n\n2. The framework is more system- and process-oriented, presenting an engineering pipeline rather than offering a fundamentally novel algorithmic or theoretical contribution. This limits the originality of the work in an academic sense.\n\n3. The description of the “data flywheel” remains relatively high-level, and its distinction from conventional self-training or data augmentation pipelines is not entirely clear.\n\n4. The experimental validation, while covering both closed-source and open-source agents, could benefit from deeper analysis to more convincingly support the claimed improvements in robustness and grounding."}, "questions": {"value": "1. The notion of a “data flywheel” is central to the paper. Could the authors clarify in more concrete terms how this differs from standard self-training or iterative data augmentation pipelines?\n\n2. The framework is described as a self-improving cycle with ICM and ICM-r2. How sensitive is the performance to the number of critic iterations, and is there a point of diminishing returns?\n\n3. The experiments show improvements on both closed-source and open-source agents, but the analysis of robustness and catastrophic deviations is limited. Could the authors provide more detailed evidence on how GAIA handles failure cases in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "P8ui9jil1W", "forum": "QHDaLyVMCX", "replyto": "QHDaLyVMCX", "signatures": ["ICLR.cc/2026/Conference/Submission281/Reviewer_JEcH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission281/Reviewer_JEcH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761666320484, "cdate": 1761666320484, "tmdate": 1762915484773, "mdate": 1762915484773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GAIA, a data flywheel system for training GUI action critics that enhance GUI agents at test time. GAIA collects positive and negative action samples from real agent rollouts by comparing actions to ground truth on AndroidControl and GUI-Odyssey. An Intuitive Critic Model (ICM) is trained as a binary classifier over “correct” or “wrong” labels given the screen, global instruction, action history, and a candidate action, and is used to select the best action among multiple stochastic rollouts during inference. GAIA iteratively refines itself as ICM-guided rollouts produce harder samples to train a stronger critic (ICM-r2). Experiments show consistent improvements across closed- and open-source agents, on both planning and grounding tasks, with ICM-r2 further improving over ICM. Ablation studies on rollout numbers and comparisons with heuristic and reasoning-based critics demonstrate that the intuitive critic achieves higher classification accuracy and larger test-time performance gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: Proposes a practical “data flywheel” for critic training using real agent actions rather than heuristic negatives, better matching the true error distribution (Section 3.2.1; Figure 2). The “intuitive” binary critic is a focused design choice that reduces token overhead versus reasoning critics and fits the best-of-N TTS paradigm (Sections 1, 3.2.2).\n- Quality: Solid empirical study across multiple agents and benchmarks, including closed-source models via API (Section 4.1). Results show substantial SR/GR improvements, especially for weaker agents (e.g., UI-TARS 1.5*: +17.3 SR on GUI-Odyssey; Table 1), and gains generalize to ScreenSpotV2, which is not in the flywheel (Table 3). The iterative round (ICM-r2) consistently adds improvements (Figure 1b; Tables 1, 3).\n- Clarity: The overall pipeline and roles of ICM/ICM-r2 are clear (Figures 1–3). Implementation details and prompts are provided (Section 4.1; Appendix B–C), including LoRA fine-tuning setup and rollout hyperparameters.\n- Significance: Addresses a key bottleneck for GUI agents—irreversible errors—by providing a model-agnostic test-time augmenter that scales with N and improves safety/robustness (Section 1; Figure 4). The approach is likely to be broadly useful to the community."}, "weaknesses": {"value": "- Novelty relative to contemporaries: While the data flywheel with real negative samples is valuable, the high-level idea—critic-guided best-of-N test-time scaling—has appeared in GUI/agent works (e.g., GUI-Genie, GUI-Actor, GTA1, GUI-Critic-R1). The paper’s comparison focuses mainly on UI-Genie-RM (Table 4) and an in-house reasoning critic; broader, controlled comparisons to recent critics/TTT methods are limited.\n- Labeling assumptions and potential noise: Positives/negatives are defined by exact GT matching (Section 3.2.1), which may mislabel alternative valid actions (e.g., equivalent taps or multiple solution paths), potentially biasing the critic. Tolerance details for coordinates/text (synonyms, minor offsets) are not specified; exact-match SR suggests strictness (Section 4.1 “Evaluation Metrics”).\n- Fairness of intuitive vs reasoning critic comparison: The RCM is trained on a 30k subset while ICM/ICM-r2 leverage larger flywheel data (Section 4.4; Table 5). This weakens the claim that intuition “outperforms” reasoning, as performance may be data-limited rather than inherently inferior. Token/time efficiency advantages are hypothesized but not measured.\n- Runtime/compute overhead: The method adds N forward passes for the agent plus N critic evaluations (default N=8; Section 4.1). There is no latency or cost analysis, especially for closed-source APIs, nor a performance vs. compute trade-off study beyond accuracy curves (Figure 4).\n- Generalization scope: Training data are drawn from the same benchmarks used for evaluation (AndroidControl/GUI-Odyssey; Section 4.1). Although ScreenSpotV2 shows positive transfer (Table 3), more rigorous cross-dataset or cross-app generalization (e.g., train on one dataset, test on another unseen suite) is not reported.\n- Clarity/formatting issues: Several typos/garbled equations and tables (e.g., Section 3.1/3.2.2 formulas; Table formatting) impede readability despite the core idea being understandable."}, "questions": {"value": "- Labeling and multi-validity: How are “correct” labels determined when multiple actions are acceptable (e.g., multiple clickable regions leading to the same outcome, minor coordinate tolerances, synonymous text)? What spatial tolerance or string normalization is used to avoid mislabeling plausible actions as wrong (Section 3.2.1; Evaluation Metrics)?\n- Sequence alignment: When the base agent diverges early, later steps may compare against misaligned GT. How do you ensure negative labels stem from the proposed step rather than misalignment across the trajectory? Any filtering for such cases?\n- Thresholding and calibration: Section 1 mentions releasing only “high-confidence” actions above thresholds, but the main method defaults to choosing the highest-scoring “correct” candidate (Section 3.2.2). Do you use a probability threshold? How calibrated are the “correct” logits across different agents? Any temperature scaling or calibration study?\n- Compute and latency: What are the runtime/latency and token cost impacts for N=8 across models (especially API-based GPT-4o/Doubao)? Can you report wall-clock overheads and accuracy vs latency curves to guide practitioners (Figure 4 currently shows accuracy only)?\n- Fair comparisons: Can you (a) match N with UI-Genie-RM and report deltas, and (b) train the reasoning critic with the same data volume as ICM-r2 to isolate the effect of “intuitive vs reasoning” (Table 4–5)? Also compare against recent GUI critics (e.g., GUI-Actor, GTA1, GUI-Critic-R1) under the same rollout and datasets.\n- Data flywheel scaling: The ICM→ICM-r2 gains are sometimes modest. Can you provide a learning curve vs. added D+ size and a third iteration to quantify scaling? Which action types benefit most (click vs. navigation vs. text input)?\n- SoM visualization ablation: Does marking clicks with SoM red circles bias the critic to surface cues not available during execution? Please provide an ablation without SoM to test reliance on the mark (Appendix B.3).\n- Cross-dataset generalization: Can you train the critic on one dataset (e.g., AndroidControl) and evaluate on GUI-Odyssey/ScreenSpotV2 only to quantify out-of-domain transfer?\n- Low-N and N=1 regimes: What is the gain in practical settings with small N (e.g., 2–4) and N=1 “gating” (reject/accept)? This would inform cost-sensitive deployments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "30egw3afI5", "forum": "QHDaLyVMCX", "replyto": "QHDaLyVMCX", "signatures": ["ICLR.cc/2026/Conference/Submission281/Reviewer_dudK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission281/Reviewer_dudK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747231214, "cdate": 1761747231214, "tmdate": 1762915484650, "mdate": 1762915484650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the GUI Action Critic's Data Flywheel System, a framework designed to address the critical problem of irreversible errors in GUI agents. The core idea is to train a separate Intuitive Critic Model (ICM) that evaluates an agent's proposed action before it is executed, allowing for Test-Time Scaling (TTS) of the base agent's performance. The GAIA system includes 1) Phase 1 initializes the model on generated actions on standard benchmark. 2) Phase 2 deploys the GUI agent and the ICM, then collects more hard cases, which will be added to the flywheel for improving ICM. Experiment results show that this method can improve the performance of various GUI models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is good. The irreversibility of agent operations is a primary obstacle to deploying GUI agents in real-world, high-stakes environments. A single error can be catastrophic, and a pre-execution validation mechanism is a practical and necessary solution.\n2. The data flywheel with the help of pre-trained GUI models is crucial. By using the actual errors generated by a base agent, the method can curate a dataset of positive and negative samples that are closely aligned with the actual error distribution.\n3. The experiments are comprehensive, demonstrating performance gain across various models and benchmarks."}, "weaknesses": {"value": "1. One main claim of the paper is the data flywheel effect. However, according to the experiment, round two training of the critic model does not lead to steady performance gain. In a lot of dimensions, ICM shows even better performance than ICM-r2. The flywheel appears to stall after a single turn.\n2. The experimental setup simplify the operation for the base agents by discarding excessive historical image input and only feeding the text description of the historical steps. This simplification makes it difficult to compare the results to the original models and raises the question of whether the critic is succeeding simply because the task itself was made easier.\n3. The idea of using Best-of-N test-time-scaling with a critic model is too straightforward and widely used in many domains. Then the main contribution becomes the flywheel part. However, as noted in W1, the flywheel appears to stall."}, "questions": {"value": "Is the ICM critic model trained for each GUI agent individually?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "enphk9zW1R", "forum": "QHDaLyVMCX", "replyto": "QHDaLyVMCX", "signatures": ["ICLR.cc/2026/Conference/Submission281/Reviewer_q4WG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission281/Reviewer_q4WG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931246324, "cdate": 1761931246324, "tmdate": 1762915484331, "mdate": 1762915484331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}