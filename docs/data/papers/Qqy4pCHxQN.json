{"id": "Qqy4pCHxQN", "number": 10733, "cdate": 1758180700636, "mdate": 1759897632749, "content": {"title": "TrafficBT: Advancing Pre-trained Language Models for Network Traffic  Classification with Multimodal Traffic Representations", "abstract": "Advances in pre-training and large language models have led to the widespread adoption of pre-trained models for network traffic classification, enhancing service quality, security, and stability. However, most existing pre-trained methods focus solely on payload semantics, neglect temporal dependencies between packets, and rely on single-dimensional static feature learning. This limitation reduces their robustness and generalization capabilities in dynamic and heterogeneous network environments. To address these challenges, we propose TrafficBT, a universal traffic classification framework combining pre-training with multimodal fine-tuning. It extracts both semantic and spatio-temporal features and uses data augmentation to handle data scarcity and class imbalance. During pre-training, TrafficBT leverages large-scale public and real-world traffic datasets to learn domain-specific semantic representations from payloads. In the fine-tuning stage, it adopts a multimodal learning framework that employs a gating network to fuse BERT with a three-layer Transformer architecture, enabling the model to effectively capture both payload semantics and temporal transmission patterns. Experiments show that TrafficBT achieves F1 scores above 0.99 on most real-world and benchmark datasets and outperforms eight state-of-the-art baselines across eight downstream tasks. Notably, it improves performance by 21% in encrypted proxy website classification, demonstrating strong robustness and generalization.", "tldr": "This paper introduces TrafficBT, a novel framework that achieves state-of-the-art network traffic classification by fusing payload semantics from a pre-trained BERT model with spatio-temporal features captured by a dedicated Transformer architecture.", "keywords": ["Network traffic clssification", "pre-trained language models", "multimodal representation learning", "semantic", "data augmentation"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ba190331c93c097926df8a0b4d5e0b9317ffb76f.pdf", "supplementary_material": "/attachment/21978e6ccd1da23fcc5f646b8872fc8ef529377a.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes TrafficBT, a multimodal pre-training and fine-tuning framework for encrypted traffic classification. It integrates a BERT encoder with a lightweight Transformer module (TriFormer) to model payload semantics and spatio-temporal statistical features, which are fused through a gating network. In addition, the framework employs data augmentation strategies—including masking, shuffling, and noise perturbation—to address class imbalance and enhance model robustness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tClear motivation: The paper identifies a meaningful gap in existing pre-trained traffic models that mainly capture payload semantics, and systematically extends this paradigm to multimodal (semantic + spatio-temporal) learning.\n\n2.\tWell-written and well-organized: The paper is clearly structured and effectively presented, with smooth transitions and clear figures, which greatly improve readability.\n\n3.\tExtensive experiments on 15 datasets: The authors conduct a comprehensive evaluation on 15 public datasets covering eight typical downstream tasks, including VPN detection, malware classification, encrypted proxy identification, and cross-platform traffic analysis."}, "weaknesses": {"value": "1.\tIncremental innovation: The novelty is limited. Existing approaches such as ET-BERT, TrafficFormer, and YaTC already employ pre-training for traffic classification. The contribution mainly extends these methods by adding a spatio-temporal encoding module (TriFormer) and a fusion gate, which appears as an incremental improvement rather than a conceptual breakthrough.\n\n2.\tInsufficient experimental validation: Although the paper compares TriFormer with a TCN-based temporal encoder (Figure 5), it still lacks ablation experiments isolating the effect of the spatio-temporal module itself. Specifically, results without any spatio-temporal encoding (e.g., using only the BERT backbone) are not presented, making it difficult to quantify the overall contribution of TriFormer to performance gains.\n\n3.\tUnclear dataset setup and potential data leakage: Although the paper mentions the use of a “leave-one-out” strategy, the datasets employed for pre-training (e.g., NUDT-Mobile, ISCXVPN2016, Tor2016) appear to share similar distributions and application types with those used for fine-tuning. The authors should provide more detailed descriptions of the datasets to demonstrate that there is no significant risk of feature or distribution leakage, thereby ensuring that the reported performance can be fairly evaluated."}, "questions": {"value": "1.\tClarification on Bigram Splitting Strategy: In Lines 177–179, the payload feature is extracted by splitting a sequence into overlapping bigrams; could the authors elaborate on the motivation and advantages of this design choice?\n\n2.\tLimited modeling of spatio-temporal dependencies: The current spatio-temporal representations seem to rely mainly on simple flow- and packet-level statistics, rather than capturing fine-grained inter-packet or intra-packet relationships. It remains unclear whether the proposed Packet Feature Meta-Sequence Learning can effectively model such complex dependencies. Additional clarification and discussion on this aspect would strengthen the paper.\n\n3.\tMissing strong baselines: The paper should compare against more recent and advanced baselines such as TrafficLLM[1] to provide a fairer performance benchmark.\n\n4.\tScalability and efficiency: How do the computational costs of TrafficBT compare with models like NetMamba or YaTC? Since the proposed framework combines BERT, Transformer, and a gating network, it is unclear whether it remains computationally efficient.\n\n5.\tLack of hyperparameter sensitivity analysis: Most hyperparameter choices appear to be empirical, and no sensitivity analysis is provided. For example, the 10% masking, shuffling, and noise perturbation rate used to simulate real-world network noise lacks experimental justification. The authors are encouraged to conduct hyperparameter experiments to evaluate model robustness and provide stronger support for these design choices.\n\n\n[1] Tianyu Cui, et al. TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation. arXiv"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KPUZrhLcNF", "forum": "Qqy4pCHxQN", "replyto": "Qqy4pCHxQN", "signatures": ["ICLR.cc/2026/Conference/Submission10733/Reviewer_hLDJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10733/Reviewer_hLDJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747706075, "cdate": 1761747706075, "tmdate": 1762921961936, "mdate": 1762921961936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TrafficBT, a framework for overcoming the limitations of existing pre-trained models in network traffic classification. In general, a network traffic classification task maily depends on single-modality payload semantics. The introduced approach utilizes a multimodal concept with consideration of packet payload information with additional modalities (temporal dependencies between packets for making traffic representation). The model employs a pre-trained backbone for semantic feature extraction with a gating mechanism for multimodal fusion."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1) The authors attempts to address the current limitation in NTC tasks by introducing temporal and potentially static features is a significant step towards creating a truly robust traffic fingerprint.\n\nS2) Utilizing the gating mechanism allows to address the task with a commitment to its efficiency.\n\nS3) By basing the semantic stream on established pre-trained language models, the framework benefits from deep linguistic understanding of the payload, providing a powerful, high-quality feature backbone before fusion occurs."}, "weaknesses": {"value": "W1) The authors do not consider any detailed quantitative approaches for the contribution of each modality. It means that it is insufficient for showing how much of the performance is provided from the temporal features.\n\nW2) Selecting the gating mechanism may limit the complexity of the interactions, which can be trained between modalities.\n\nW3) The robustness and generalization issues should be considered and validated acrosss a wider range of heterogeneous and noisy datasets."}, "questions": {"value": "Q1) Can we meet the dimensionality of the modality embeddings before the fusion layer?\n\nQ2) Can we review any metrics for computational costs of the employed approaches and modules?\n\nQ3) Do the authors considered early fusion (concatenating raw inputs or early embeddings) or late fusion (fusing only the classification logits)?\n\nQ4) Is there any specific information for the learned gate weights?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KHdDlhuUJu", "forum": "Qqy4pCHxQN", "replyto": "Qqy4pCHxQN", "signatures": ["ICLR.cc/2026/Conference/Submission10733/Reviewer_bQYM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10733/Reviewer_bQYM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755068683, "cdate": 1761755068683, "tmdate": 1762921961559, "mdate": 1762921961559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed TrafficBT, which combines BERT with Transformer to model spatio-temporal characteristics of network traffic and achieve multimodal fusion. It also introduced the Gating Network to adaptively combine payload semantics and transmission modes to improve the generalization ability of the model in complex scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ Systematic evaluation was carried out on several public datasets, covering diversity types of downstream tasks. The experimental scale is large.\n+ Design modal-specific data enhancement strategy for category imbalance and sample scarcity in traffic data to effectively improve the robustness of the model.\n+ Provides codes to enhance the reproducibility and impact of research."}, "weaknesses": {"value": "+ The pre-trained datasets and fine-tuned data overlap, and the results on the NUDT-Mobile, ISCXVPN2016 and ISCXTor2016 data sets may be overestimated, which raises concerns about generalization.\n+ The methods used are a combination of existing methods, such as BERT, data enhancement and Transformer, which have been included in previous work respectively. The technical novelty of TrafficBT is limited.\n+ Traffic BT has a complex structure and includes multiple modules such as BERT, TriFormer, and gated Network. Although it performed well in experiments, its deployment feasibility in resource-limited scenarios (such as edge devices, real-time detection) has not been fully evaluated.\n+ In this paper, payload semantics and statistical characteristics are regarded as \"multimodal\", but both essentially come from the same traffic data. It is recommended to define the concept of \"multimodal\" more strictly, or consider introducing truly heterogeneous modes (such as traffic + log, etc.).\n+ There are some errors in presentation, such as the missing captions of Figure 3 and Figure 4."}, "questions": {"value": "+ Why choose 80% as the MASK rate during pre-training?\n+ In Fusion, how is parameter a selected, and how to identify the importance of features extracted by BERT and TriFormer for different datasets?\n+ Why BERT can capture semantic information.\n+ Could you elaborate on how the training set and test set are segmented during data processing, whether based on flow, packet or random? In addition, the overview of the test dataset is missing, and how you avoid test data leakage?\n+ Will you release the pre-trained model and the processed datasets to promote the related research?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPskfIJSpl", "forum": "Qqy4pCHxQN", "replyto": "Qqy4pCHxQN", "signatures": ["ICLR.cc/2026/Conference/Submission10733/Reviewer_uWj7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10733/Reviewer_uWj7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10733/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035203912, "cdate": 1762035203912, "tmdate": 1762921959962, "mdate": 1762921959962, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}