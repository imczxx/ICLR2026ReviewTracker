{"id": "UcaSiq18Tb", "number": 9908, "cdate": 1758148430490, "mdate": 1763583585478, "content": {"title": "Features Emerge as Discrete States: The First Application of SAEs to 3D Representations", "abstract": "Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the first application of SAEs to the 3D domain, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: such models approximate a discrete state space, driven by phase-like transitions from feature activations. Through this state transition framework, we address three otherwise unintuitive behaviors — the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model actively manipulates the interference caused by superposition to prioritize the saliency of different features. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also describes a generally applicable, state-based feature framework.", "tldr": "We present the first application of SAEs to the 3D domain, framing model internals as a state-based feature space governed by phase transitions.", "keywords": ["sparse autoencoders", "mechanistic interpretability", "computer vision"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fc52fadbb846566c8c77689f68631858d6402574.pdf", "supplementary_material": "/attachment/54d27bfbeb112e8910b554c693d3dabc1438c928.zip"}, "replies": [{"content": {"summary": {"value": "This is a poorly written paper that seems to claim to be the first study on the application of SAEs on 3D datasets and presents corresponding findings and interpretations of the learned features."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Unfortunately, given the current organization and writing of the main content in this paper, it is extremely difficult to identify any valuable insights for readers to learn."}, "weaknesses": {"value": "(W1) The contributions of this paper are vague and poorly discussed. For example, in line 41, the author states that \"the scope of data domains has been limited — recent feature interpretability studies have focused on discrete and structured data, like image and text, rather than continuous or unordered data, ...\" However, I don't see why the solution to this would be to study SAE applications on 3D data, as the authors mentioned in lines 48–49 and highlighted in the abstract. Why don't the authors try continuous and unordered data in 1D and 2D spaces first? Besides, a contradictory and confusing point is that the 3D point cloud data used in this paper's experiments is also discrete.\n\n(W2) The writing of the methods section is very confusing and difficult to follow. It seems that the author proposes a summary formula for SAE works in Eqn (2). However, first, it is not clear how the authors derive and denote the well-known representation learning formula of Eqn (1) into Eqn (2). Second, it is unclear how the authors convert the related SAE works into their formula. For example, in lines 113–114, the authors state that \"Recent LLM studies (Cunningham et al., 2024) use a sparse autoencoder (SAE) to approximate this decomposition with the assumption that $\\bf{α}$ is sparse.\" However, it seems that the SAE formula in Cunningham et al. (2024) has nothing to do with the left part of Eqn (3) of this paper, and their α is a hyperparameter in the final loss function to control the sparsity of the reconstruction. And it has a completely different meaning from the proposed set of scalars $\\bf{α}$ in this paper. Similar confusing descriptions and links to related works are everywhere in the methods section. Lastly, it is unclear how the authors \"apply\" an SAE to Dora-VAE in their major 3D data application. What is FPS in Eqn (7)? Where are the proposed E and $\\bf{α}$ notations in Section 3? How exactly do the authors modify a VAE framework using an SAE structure?"}, "questions": {"value": "In generanl, the authors are encouraged to re-setup the goal and scope of this research and rewrite the whole paper for the future submissions.\n\nPlease also respond and provide clear explanations for the confusions I described above, which may change my opinions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sxpeQFmVbn", "forum": "UcaSiq18Tb", "replyto": "UcaSiq18Tb", "signatures": ["ICLR.cc/2026/Conference/Submission9908/Reviewer_JL4Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9908/Reviewer_JL4Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761255214257, "cdate": 1761255214257, "tmdate": 1762921364719, "mdate": 1762921364719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work provides the first application of Sparse Autoencoders (SAEs) to 3D data, decomposing the internal representations of a 3D Variational Autoencoder (VAE). The paper's central finding is that the model learns features as discrete states rather than continuous values, which are activated via phase transitions. This claim is supported by a large-scale analysis of 848k feature ablations, which reveal that: (1) High-impact features have a unimodal (single-peak) distribution of transition points. (2) Low-impact features have a bimodal (two-peak) distribution of transition points. The authors hypothesize this bimodality is evidence that the model actively manipulates superposition interference at inference time, pushing interference onto low-impact features to preserve the saliency of high-impact ones."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- Interpretability for 3D data is underexplored and interesting!\n- Lots of experiment runs (e.g., 848k feature interventions) which makes the results robust.\n- The theoretical explanation of how models see features as a decomposition of presence and identity is also interesting. \n- The bimodal experiments (Fig 5) and visualization (Fig 3) are insightful. \n- Validating the threshold t with max slope experiments is really nice!"}, "weaknesses": {"value": "- The learning dynamics and 3D contributions seem completely disjoint (although both interesting). Moreover, the paper makes broad claims about \"a generally applicable, state-based feature framework.\" However, all the evidence is derived from a single model architecture (Dora-VAE) on a single data modality (3D point clouds). It's impossible to know if these findings (especially the bimodal transitions) are a fundamental property of feature learning, or a specific quirk of the Dora-VAE architecture e.g., and its cross-attention mechanisms.\n- I am having difficulty understanding Figure 6 and more generally, the explanation for the bimodality. I generally get the intuition that the model would put more interference with the low-impact concepts, but have difficulty following the explanation in Lines 459-475. \n\nSmall stuff\n\n- Figure captions should be more detailed and explain (i) why this plot is being shown and (ii) what takeaways the reader should make. Not just a short description of what is shown."}, "questions": {"value": "- Do these findings hold across different models and data types? Or is this only for Dora-VAE and 3D pointcloud data? Showing these findings hold on another model/domain would increase the contribution strength for the learning dynamic theory section a lot. Or, this could be added as a weakness/limitation to the paper since it is only applied to 3D data (even though applying SAEs to 3D data is a contribution itself, it does not mean all general findings on 3D data will hold to other domains).\n- What is Figure 6 showing? It is not clear if it is a real experiment or just a visual aid. It is just mentioned in the first line of Line 454 and 459, but the text and caption are insufficient to parse the figure. \n- Moreover, could the authors kindly explain in more clear terms how the bimodal transition arises? I think this section could even be dropped, or written as a 'hypothesis' rather than making stronger claims about superposition/interference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WQ3VkmAdKd", "forum": "UcaSiq18Tb", "replyto": "UcaSiq18Tb", "signatures": ["ICLR.cc/2026/Conference/Submission9908/Reviewer_nEVZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9908/Reviewer_nEVZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684042768, "cdate": 1761684042768, "tmdate": 1762921364398, "mdate": 1762921364398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the internal features of a 3D reconstruction model by being the first to apply Sparse Autoencoders (SAEs) to the 3D domain. The authors use an SAE to analyze the latent vectors of a Dora-VAE (a 3D reconstruction VAE) trained on the Objaverse dataset.  The central finding is that the Dora-VAE's internal features are **discrete, not continuous**.\n\nThis paper proposes a novel theory framework oo explain \"unintuitive\" behaviours of VAEs. They analyze the gradient and decompose it into two components: the feature's **\"presence** ($\\alpha_j$) and its **\"identity\"** ($e_j$). The framework suggests that the model prefers to learn discrete features because the learning signal for a feature's *identity* is scaled by its *presence*. Specifically, the **unimodal** (single-peaked) distribution of transition points for **high-impact features** is explained by the feature's \"presence\" term. In contrast, the **bimodal** (two-peaked) distribution observed for **low-impact features** is explained by the model's active manipulation of superposition. The authors further apply SAEs into VAEs for 3D data and explain the unusual properties."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. New Theoretical Framework: The decomposition of the learning gradient into \"presence\" ($\\alpha_j$) and \"identity\" ($e_j$) provides a new and powerful lens for why features emerge in a certain way, moving beyond just observing what features are learned.\n2. Well-motivated. The application of SAEs to the 3D domain is motivated by the theory analysis to extend interpretability.\n3. Strong Explanatory Power: The proposed framework successfully connects several counter-intuitive empirical observations (discrete features, sigmoidal loss curves, and bimodal transition points) into a single, cohesive theory."}, "weaknesses": {"value": "1. The writing can be improved for clarity, such as the definition of ARC and loss difference. It is not clear how to calculate them. What the colored points representing are also not depicted. \n2. The findings are based on the Dora-VAE architecture, which processes 3D models by sampling points and using point features and positional encodings. The introduction of SAE is an incremental contribution.\n3. The paper uses the terms \"discrete state space\" and \"phase transition\" heavily. While this is a helpful analogy for the observed sigmoidal loss curves, it may overstate the case."}, "questions": {"value": "How important it is for the application of SAE to 3D data? What is performance of the proposed method in Dora-bench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "w1qf44pfIF", "forum": "UcaSiq18Tb", "replyto": "UcaSiq18Tb", "signatures": ["ICLR.cc/2026/Conference/Submission9908/Reviewer_y1WU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9908/Reviewer_y1WU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9908/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138794379, "cdate": 1762138794379, "tmdate": 1762921364152, "mdate": 1762921364152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Rebuttal Revision"}, "comment": {"value": "Thank you, reviewers, for your comments. We have responded to each of your suggestions, and have posted a revised version of paper significantly improved by your comments. \n\n**We have colored specific changes for each reviewer:**\n\n**Reviewer 1: Red**\n\n**Reviewer 2: Teal**\n\n**Reviewer 3: Blue**\n\nWe are looking forward to further comments."}}, "id": "qUACbAaNFN", "forum": "UcaSiq18Tb", "replyto": "UcaSiq18Tb", "signatures": ["ICLR.cc/2026/Conference/Submission9908/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9908/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission9908/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763583958264, "cdate": 1763583958264, "tmdate": 1763583958264, "mdate": 1763583958264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}