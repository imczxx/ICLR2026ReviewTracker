{"id": "bUtLHJn90a", "number": 16988, "cdate": 1758270963699, "mdate": 1759897206045, "content": {"title": "Rethinking the shape convention of an MLP", "abstract": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs.\nImplementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search.\nResults show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks—a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.", "tldr": "", "keywords": ["Skip connections", "MLP architecture", "Residual networks", "Generative modeling", "Deep learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76a4db02d72f638aec03e7552a01b7356a9e57b4.pdf", "supplementary_material": "/attachment/b85efc82aa4cab9d97445b90ad71e114b18da721.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes “Hourglass MLP” blocks, a bow-tie like architecture pattern that inverts typical MLP architecture shape.  The work studies this shape variant, presents some pareto plots. The bow-tie + shape & skip placement principle ideas in residual networks are touched on via related work (UNet,MoE,LoRA) & mirror established bottleneck tactics. Experiments with 2-epoch MNIST & Imgnet32; small delats show efficiency in params (PSNR) [aruged from pareto front plots] & the results do show this approach to be better than conventional MLP (with skip connects) at input dim. The work is more a baseline starting study & needs more focused effort & engineering to arrive at when and hwere to organise resid connections at higher dim in MLP stacks -- this, if backed by more evidence, can be of value."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Architectural description with a compact formulation (Eq3) and schematic (Fig1)\nPareto framing is well structured. Explicitly searches (dz, dh, L) and reports fronts on multiple tasks/datasets (Tbls 1,2) matching good practice to study shape trade-offs.\nKnobs to control are pragmatic showing that frozen random input projection is competitive for at least one config (Fig5); maybe helpful for certain design choices upstream/downstream."}, "weaknesses": {"value": "Novelty is limited / feels incremental as this is similar to resnet bottleneck tactic transposed to MLP. While motivation is there for hiher-dim learning, the shape+skip placement principle is used earlier (covered in related work). The new part can only be argued at best as an incremental variation in images with processing adjustments (vectorized & projection). Interesting as a variation, but not directly new knowledge as well.\n\nEfficiency claims focused on param count, but are not backed up around compute. This is a mis-match to the promise made in abstract & early sections of the narrative around compute efficiency. The specific measures to showcase this value (compute/latency/activation mem etc.) are not presented. The hourglass/bow-tie arch would mean the block multilies on matrix sizes; so there will be a mismatch as things scale up. More so in this work as the tactic is adding an input up-project layer [even if claim can be made about it being frozen, it is adding to compute].\n\nEvaluation scope & measures do not support ambition. The experiment tasks are small-data, low-res, PSNR only. Improvements at high-budget end are small (Tab2). Aslso, given no cross-ref with human baselines (not that critical).  The measrues as reported does not show if the representation helps classification as such. Only 2-epocs of training ~ why? Is it possible that some configs are under-trained? Would PSNR diffs be different with more training?\n\nReproducibility inconsistencies. Appendix A shows dz [8,2200]; Tables in paper report values well above that. Also, 5-sigma is an odd choice of reporting.\n\nBaselines/ablations: The various aspects as indicated & reported do not fully align. e.g. in Sec4.2 it is stated that MLPs can benefit from rnd projections, but no specific details are not shown. Frozen projection claim is supported by a single config on a single task [Fig5]. This needs more testing spaning datasets, dz & noise regimen. \n\t\nThe theoretical motivations are not offering explanations, rather present some plausibles. JL lemma, Cover’s theorem, random features, compressive sensing justify that “high‑dimensional random projections preserve structure” but no analysis targets residual learning specifically (why residual updates at higher dimension should systematically dominate under parameter/compute constraints?). Something that offers progress on this would help support the ambition of this work."}, "questions": {"value": "If Hourglass is truly more compute‑efficient, this must show up beyond parameter count. Why was htis not more actively reported?\n\nYour block looks architecturally similar to classic bottlenecks where residuals live at the wider ends. What, concretely, is new beyond placement in an MLP with an upfront up-projection, relative to U-Net skips, MoE “temporary widenings,” or LoRA-style paths?\n\nOn MNIST, you transform to a (class) prototype then classify. Why not report standard classification accuracy as well, to show utility of the representation? How sensitive are results to the choice of the single prototype image per class?\n\nGiven Transformers typically expand FFN by 2x to 4×; how would your wider residual space interoperate with attention without exploding compute?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rzeC4YwmjV", "forum": "bUtLHJn90a", "replyto": "bUtLHJn90a", "signatures": ["ICLR.cc/2026/Conference/Submission16988/Reviewer_cH7F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16988/Reviewer_cH7F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894297809, "cdate": 1761894297809, "tmdate": 1762927003845, "mdate": 1762927003845, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes:\n1. To use a wide-narrow-wide approach in MLP.\n1. Instead of training an input projection matrix, the paper proposes to use a fixed random matrix as a projection matrix in the first layer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A simple approach that defies convention.\n2. The 'hourglass' (wide-narrow-wide) MLP achieves significant results compared to 'conventional' (narrow-wide-narrow) MLP."}, "weaknesses": {"value": "1. The random fixed input matrix is not convincing enough to warrant a switch from the conventional trainable weights. As the reduction in parameter count  is only marginal and the fixed matrix model even had a decrease in PSNR performance. It would be great if the author is able to provide values on the number of parameters reduced against the model parameters size on different model sizes.\n2. The paper claims that hourglass is Pareto-Optimal with no theoretical backing. Only based on empirical results from a small subset of conventional models.\n3. The current experiments are limited it would be more convincing to compared on more computer vision task like Image classification and across more variety of models like Bottleneck-Resnet[Zagoruyko, Sergey, and Nikos Komodakis] and MLPMixer[Tolstikhin, Ilya O., et al].\n\n\nZagoruyko, Sergey, and Nikos Komodakis. \"Wide residual networks.\" arXiv preprint arXiv:1605.07146 (2016).\n\nTolstikhin, Ilya O., et al. \"Mlp-mixer: An all-mlp architecture for vision.\" Advances in neural information processing systems 34 (2021): 24261-24272."}, "questions": {"value": "1. The paper mentions linear separability at high dimensions as a theoretical foundation but does this contradicts the paper which project from a higher dimension to a lower dimensional? And if it does not why?\n2. How does the randomness of the input project matrix affect the variance of the results obtained? It would be good if you could reflect this in the paper with more training across different seeds.\n3. In Table 1. and Table 2. is the hourglass model trained with a fixed W_in or a trainable one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fYJi4z6A6x", "forum": "bUtLHJn90a", "replyto": "bUtLHJn90a", "signatures": ["ICLR.cc/2026/Conference/Submission16988/Reviewer_rEMH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16988/Reviewer_rEMH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913029660, "cdate": 1761913029660, "tmdate": 1762927003362, "mdate": 1762927003362, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to flip the narrow-wide-narrow convention for MLP and skip connections in deep neural networks to instead use an hourglass shape with wide features at skip connections and narrower processing inside MLP layers. This paper addressed the added computational cost of this by keeping the expanding projection fixed with random initialised weights during training. The results on generative tasks on MNIST and ImageNet-32 data show that the hourglass networks have stronger Pareto frontiers than the conventional structures."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of reversing the traditional MLP shape to explore the effect of skip connection dimensionality is interesting and goes against the existing convention. This could potentially lead to significant architectural changes for e.g. LLMs.\n- The paper takes a strong focus on the performance/efficiency Pareto frontier, recognising the importance of these two aspects of model quality. \n- The evaluation tasks are chosen to be well-suited to the incremental refinement of residual networks, the main focus of the paper."}, "weaknesses": {"value": "- The experiments of the paper are too narrow and simple, focusing on small-scale tasks on MNIST and ImageNet-32. This is in contrast to the high-dimensional, large-scale settings for modern LLMs and diffusion models, where architectural design trade-offs make a large difference on both performance and efficiency. I therefore think that experiments on Transformer networks need to be run, or alternatively something like MLP-Mixers, CNNs. Would the hourglass structure help in these scenarios or are the expanded input/output dimensions too costly? Similarly, the datasets considered need to be larger in scale, beyond MNIST and 32x32 images. While extremely large-scale training isn’t required, testing the design on at least one Transformer model on a text-based task would give an indication to whether the approach scales.  Currently that is not at all clear, and the claims about extensions to Transformers and ViTs are speculative and not supported by any experiments. To achieve real impact, this paper will need more large-scale or real-world validation.\n\n- The fixed random projection at the start of the network reduces the number of trainable parameters but doesn’t reduce the cost of the forward pass, so I think the “efficiency” claims are a bit overstated. It also adds to the inference time cost and memory bandwidth, which is an important aspect for modern deep neural networks.\n\n- There is no clear explanation of why the wide skip connections outperform the conventional narrow ones, apart from an appeal to high-dimensional intuition and random projection theory. Analysing this more would improve the paper.\n\n- There is quite a bit of repetition in the intro and contributions, and space could be found by reducing this redundancy."}, "questions": {"value": "- Why are there more models trained using the Hourglass networks compared to the Conventional ones, in Figures 2a, 3 and 4?\n- Can experiments be run on a transformer-based language model architecture? And similarly a diffusion model, which is argued in the paper are well suited to this incremental refinement. If such experiments showed similar improvements, this would significantly strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YFbZmUO4aq", "forum": "bUtLHJn90a", "replyto": "bUtLHJn90a", "signatures": ["ICLR.cc/2026/Conference/Submission16988/Reviewer_ryCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16988/Reviewer_ryCo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960672219, "cdate": 1761960672219, "tmdate": 1762927002419, "mdate": 1762927002419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to revert the conventional residual connection pattern \"narrow - wide - narrow\"  in MLP to \"wide - narrow -wide\". The core idea of the design is that doing representation refinement at high dimensional space is more effective than in low dimensional space, as the former has more expressivity. The paper further proposes to use fixed random projection layer to up project the initial low dimensional input, avoiding additional training budget due to design. Experimental results show that that proposed architecture achieve better parameter / performance trade off than traditional MLP in several generative learning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written and easy to follow\n2. The paper is well motivated, as existing theoretical and empirical evidence indeed suggest that a wide - narrow - wide projection structure may bring better representation learning performance\n3. The paper proposes to use fixed random projection to reduce training cost\n4. The experimental results on MLP shows that the proposed architecture achieve better performance / parameter count trade off than conventional MLP in generative learning tasks, positively support the proposed idea."}, "weaknesses": {"value": "1. Besides the performance on generative tasks, the paper doesn't provide insights about how residual learning in high dimensional space help representation refinement\n2. The paper only conducts experiments on MLP architecture. Although the paper sketches potential extension to other modern architecture like transformer, it's unclear whether it will bring similar improvement, thus the contribution is limited"}, "questions": {"value": "1. A visualization comparison showing how the wide high dimensional space help representation learning could better support the proposed  design, refer to figure 1 of [1], specifically, it shows how high dimensional feature map provides rich and detailed gradient feedback during training. \n\n[1] Schonfeld, Edgar, Bernt Schiele, and Anna Khoreva. \"A u-net based discriminator for generative adversarial networks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Eu75CyENIm", "forum": "bUtLHJn90a", "replyto": "bUtLHJn90a", "signatures": ["ICLR.cc/2026/Conference/Submission16988/Reviewer_X86f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16988/Reviewer_X86f"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16988/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762231975741, "cdate": 1762231975741, "tmdate": 1762927001835, "mdate": 1762927001835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}