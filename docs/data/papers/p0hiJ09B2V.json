{"id": "p0hiJ09B2V", "number": 20936, "cdate": 1758311829582, "mdate": 1759896950808, "content": {"title": "Zhyper: Factorized Hypernetworks for Conditioned LLM Fine-Tuning", "abstract": "Large Language Model (LLM) conditioning refers to instructing an LLM to generate content in accordance with the norms and values of a specific culture, beliefs of a particular political orientation, or any desired text-specified semantic conditioning. Unfortunately, prompt engineering does not ensure that LLMs behave in accordance with a desired conditioning due to the inductive bias of the pre-training and alignment datasets. Prior works have focused on fine-tuning LLMs by directly conditioning the LoRA weights; however, such methods introduce a large number of parameters. As a remedy, we propose Zhyper, a parameter-efficient factori$\\textbf{z}$ed $\\textbf{hyper}$network framework that generates context-aware LoRA adapters from textual descriptions. Experiments on multiple benchmarks show that Zhyper achieves competitive performance with up to $\\textbf{26x}$ fewer parameters than the state-of-the-art baselines. Furthermore, we extend Zhyper to cultural alignment, demonstrating improved generalization to out-of-domain settings and a better capturing of fine-grained contextual values.", "tldr": "", "keywords": ["LLMs", "Hypernetworks", "Alignment", "Cultural Alignment"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4e000813b22c07dca883c4ab05f9117ddc3eebae.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents Zhyper, a parameter-efficient, factorized hypernetwork framework designed for context-aware fine-tuning of Large Language Models (LLMs). Zhyper employs a lightweight hypernetwork that generates LoRA adapters from textual inputs—such as task instructions or cultural descriptions—enabling effective adaptation to both specific tasks and cultural contexts. Compared to existing methods like T2L and HyperLoRA, Zhyper achieves this with significantly fewer parameters. Comprehensive evaluations across multiple benchmarks show that Zhyper matches or exceeds the performance of prior approaches while reducing parameter overhead by up to 26 times. It also exhibits strong generalization to unseen domains and supports fine-grained cultural alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Unlike earlier hypernetwork-driven LoRA approaches such as T2L and HyperLoRA, Zhyper delivers on-par or superior results while drastically cutting down the number of parameters required per context, marking a significant leap in parameter efficiency.\n\n2. By introducing a novel factorized design that separates diagonal and square modulation pathways, Zhyper advances beyond conventional hypernetworked LoRA strategies, enabling more efficient use of low-rank intermediate representations for conditional adaptation—this leads to lower memory consumption and computational load with minimal impact on model accuracy.\n\n3. Evaluations across diverse standard benchmarks demonstrate that Zhyper consistently performs competitively with or even surpasses state-of-the-art baseline methods, highlighting its robustness and broad applicability."}, "weaknesses": {"value": "1. While Zhyper’s core innovation, which replaces dense hypernetwork outputs with a factorized diagonal/square modulation mechanism, does lead to notable gains in parameter efficiency over predecessors like T2L and HyperLoRA, the underlying conceptual framework remains closely aligned with existing hypernetwork-based LoRA methods. The approach appears more as a refined optimization within the current paradigm rather than a transformative rethinking of how contextual signals are integrated into LLM fine-tuning.\n\n2. The paper overlooks several key developments in efficient LLM adaptation that are directly relevant to its methodology [1,2,3,4]. Incorporating comparisons and contextualizing Zhyper against these approaches would strengthen the paper’s contribution claims and better delineate its novelty.\n\n[1] HyperTuning: Toward Adapting Large Language Models without Back-propagation\n\n[2] LoSiA: Efficient High-Rank Fine-Tuning via Subnet Localization and Optimization\n\n[3] S²FT: Efficient, Scalable and Generalizable LLM Fine-tuning by Structured Sparsity\n\n[4] LoQT: Low-Rank Adapters for Quantized Pretraining\n\n3. The experimental evaluation is entirely based on Mistral-7B-Instruct, with no testing on larger-scale models, multilingual benchmarks, or alternative architectures outside the standard decoder-only transformer family."}, "questions": {"value": "1.  Can the authors provide empirical evidence of Zhyper’s parameter efficiency and accuracy in LLMs larger than Mistral-7B, and non-transformer models?\n\n2.  Beyond asymptotics, can the authors discuss or compute tighter data-dependent or task-specific generalization bounds for Zhyper-diag vs. T2L? Does reduced Rademacher complexity translate into observable gains as $n$ grows?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PkdoC7wGKo", "forum": "p0hiJ09B2V", "replyto": "p0hiJ09B2V", "signatures": ["ICLR.cc/2026/Conference/Submission20936/Reviewer_jNbZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20936/Reviewer_jNbZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760589123110, "cdate": 1760589123110, "tmdate": 1762939037392, "mdate": 1762939037392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed a new method for parameter efficient finetuning. \nThe key idea is that the Lora weight for making a model meet a new requirement can be generated from the textual description using a hypernetwork.\nThe method generate the parameters for the Lora adapter by using an embedding that can tell the contextual information, the layer number and the attention module for the lora.\nExperiments show that the proposed method can generate useful weights for LLMs to transfer to unseen tasks or align with an unseen culture during the hypernetworks training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The task is an important one, and the paper proposed method seems to be effective.\n2. The proposed method can save a magnititude of number of parameters compared to similar methods."}, "weaknesses": {"value": "1. The experiments should try to cover a wide range of base LLM to show the proposed method is not just useful for one base LLM.\n2. Would reasoning LLM benefit from this method?\n3. The method only modifies a very small part of the base LLM, thus I think the models abilities are still bounded by the base LLM. It would be great if the paper can discuss on that and maybe show some negative results that the proposed method and other method cannot help base LLM learn tasks beyond the base LLM."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iDWF9onHIM", "forum": "p0hiJ09B2V", "replyto": "p0hiJ09B2V", "signatures": ["ICLR.cc/2026/Conference/Submission20936/Reviewer_cgzs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20936/Reviewer_cgzs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909891534, "cdate": 1761909891534, "tmdate": 1762939036508, "mdate": 1762939036508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Conditioning Large Language Models (LLMs) to align with specific textual descriptions, such as tasks or cultural values, often introduces a large number of parameters when using existing hypernetwork-based LoRA methods. This paper proposes Zhyper, a parameter-efficient factorized hypernetwork framework that generates a compact, context-aware modulation signal z from these textual descriptions. Instead of generating the full adapter weights, Zhyper injects this small z signal between shared, trainable LoRA matrices A and B to compute the final weight update. Experiments show Zhyper achieves competitive performance on task-conditioning benchmarks with up to 26x fewer parameters than baselines, while also demonstrating improved generalization in the novel use case of cultural alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper proposes ZHYPER, which enables efficient conditional generation while significantly reducing the number of parameters compared to previous methods.\n* Experimental results show that ZHYPER achieves fine-tuning performance comparable to larger models while using only one-tenth of the parameters.\n* The authors provide a theoretical analysis demonstrating the superior generalization ability of ZHYPER.\n* The paper is well-written."}, "weaknesses": {"value": "* Overall, this work presents an improvement over T2L. Instead of generating the entire LoRA, it only needs to generate a low-rank embedding. However, the contribution is still largely incremental.\n* The experiments are conducted on too few models. The authors only evaluate on Mistral-v0.2, lacking experiments on a wider range of models such as Qwen3, Llama3, and Gemma3. I believe the authors should test across different model families and scales.\n* There is no ablation study on the embedding model. Since all experiments use the same embedding model, the authors should perform ablation experiments to examine its influence. Additionally, what would happen if the model’s own embedding outputs were used as inputs?\n* The paper lacks comparison with Task Vector, which serves a similar purpose by injecting domain-specific information."}, "questions": {"value": "* Is it necessary to apply LoRA to every layer? Would applying it only to certain layers lead to better performance?\n\n* Could the authors provide experiments with more embedding models and large language models (LLMs)?\n\n* Could the authors compare their method with the Task Vector approach and clarify the differences between the two?\n\n* Why does the method generate fewer parameters? Since both approaches use LoRA, the total number of additional model parameters is not reduced — only the number of generated parameters is smaller."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "igl20POQnr", "forum": "p0hiJ09B2V", "replyto": "p0hiJ09B2V", "signatures": ["ICLR.cc/2026/Conference/Submission20936/Reviewer_W8mz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20936/Reviewer_W8mz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915291750, "cdate": 1761915291750, "tmdate": 1762939035954, "mdate": 1762939035954, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is built on top of Text-to-LoRA (T2L). T2L trains a hypernetwork that generates a task-specific LoRA matrix given a textual task description. This paper proposes two architecture modifications that reduce the number of the parameters of the hypernetwork used to generate task-specific LoRA. The proposed model performs competitively with SOTA methods with reduced number of parameters."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Thorough empirical comparison with existing baselines on various tasks\n- The architecture is simple and effective at reducing the number of parameters"}, "weaknesses": {"value": "- This paper misses important prior works, i.e., LoRA-XS [2], VeRA [3], which propose exactly the two parameterizations used in this work (square and diag). The existence of these two prior works directly dilutes the contributions of the paper, making it hard to justify acceptance as the paper's core idea is simply chaning the output space of T2L from LoRA to LoRA-XS or VeRA without much insights gained.\n- This paper focuses specifically on reducing the number of parameters of the hypernetwork. However, the cost (time and memory) of running a forward pass over the hypernetwork is a fraction of generating a response done by the target LLM. Therefore, I respecfully doubt that the propose achitecture would impact any meaningful metrics related to LLM inference (e.g., VRAM, latency, etc.) \n- Section 2.1 and 2.2 are largely based on T2L [1] but is not explicitly mentioned in the text, which makes it hard to identify the novelty of this paper. I believe that by explicitly mentioning how T2L formalizes the problem and what its architecture looks like in Section 2.2 would be beneficial for reader's understanding and scientific credit assignment.\n- Various notation mistakes, causing significant confusion (see Typos and Notation Mistakes)\n- Table 5 is highly subjective without any supporting empirical evidence\n\n#### Typos and Notation Mistakes\n- $r$ is reused with different meanings in line 107 and 108\n- At line 108, the hypernetwork maps a layer-specific description vector to the entire parameters of the target network\n- $H_\\phi$ is defined three times with different meanings (line 108, 130, and 142)\n- $z$ is defined with two different meanings (line 108 and 132)\n- $\\theta$ is defined with two different meanings (line 109 and 146)\n- $j$ is not defined in Eq. 4 making it illegible\n- line 175: 'HyprLoRA' should be 'HyperLoRA'\n- 'Zhyper-full' is never defined but used at line 186\n- line 460-461; opening parentheses should have a preceding white space\n\nOverall, I believe that there are several major concerns in the current version of the submission and, therefore, I recommend reject. \n\n[1] Charakorn, Rujikorn, et al. \"Text-to-LoRA: Instant Transformer Adaption.\" ICML 2025\n\n[2] Bałazy, Klaudia, et al. \"Lora-xs: Low-rank adaptation with extremely small number of parameters.\" arXiv preprint 2024\n\n[3] Kopiczko, Dawid J., Tijmen Blankevoort, and Yuki M. Asano. \"Vera: Vector-based random matrix adaptation.\" ICML 2024"}, "questions": {"value": "- What does \"conditioning the LoRA weights\" (line 17) mean?\n- What does \"large contextual spaces\" (line 50) mean?\n- How come the hypernetwork can \"induce descriptive information\" (line 94)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VMfPi4kByc", "forum": "p0hiJ09B2V", "replyto": "p0hiJ09B2V", "signatures": ["ICLR.cc/2026/Conference/Submission20936/Reviewer_NBTB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20936/Reviewer_NBTB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20936/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993381547, "cdate": 1761993381547, "tmdate": 1762939035566, "mdate": 1762939035566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}