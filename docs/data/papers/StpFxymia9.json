{"id": "StpFxymia9", "number": 1343, "cdate": 1756873592499, "mdate": 1759898213960, "content": {"title": "Manipulation Concept: Towards Deriving Generalizable and Physics-informed Manipulation Knowledge of Articulated Objects", "abstract": "Gripper-based articulated object manipulation requires robots to reason about both object structure and the physical constraints of grippers, yet prior work has paid little consideration to grippers’ unique characteristics and their interaction with object structures. To alleviate this gap, we introduce **Manipulation Concept**, a novel analytic representation that encodes gripper manipulation skills as parameterized program templates. Each concept formalizes the interaction between a specific actionable part featuring structure and semantic (*e.g.*, cuboid door, ring handle) and a gripper action (*e.g.*, push, lift), linking geometries and semantics with executable robot actions. Building on this representation, we develop an end-to-end framework that (i) leverages a vision-language model to select the most suitable concept for the actionable part, (ii) estimates geometric and affordance parameters to instantiate the selected concept and ground it in the physical world, and (iii) generates precise gripper-specific actions to complete the task. Extensive experiments in simulation and real world demonstrate that our method outperforms prior approaches in both accuracy and generalization, achieving stronger generalization across object categories, and reliable execution in manipulation tasks.", "tldr": "", "keywords": ["Object Manipulation", "Robot Cognition"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bc3711e7025468d79f709c444328938d83412462.pdf", "supplementary_material": "/attachment/a466b66331f6bc77f0b3c9d5c651a1684a0ce044.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Manipulation Concept, an analytic representation that encodes gripper-part interaction skills as parameterized program templates, combining geometric and affordance parameters. A Manipulation Concept Library (MCL) is constructed and integrated with perception modules (VLM, Grounded-SAM) and parameter estimation networks to ground these templates into executable actions. The method is evaluated on the PartNet-Mobility dataset and several real-world objects, showing improved success rates over baselines (Where2Act, ManipLLM, A3VLM) in articulated object manipulation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Technical quality: The system integrates strong perception and grounding modules into an end-to-end pipeline. Ablations and comparisons support the claimed performance improvements.\n- Evaluation: Results on both simulation and real-world settings demonstrate feasibility and robustness."}, "weaknesses": {"value": "- Clarity and writing: The paper introduces many self-defined terms with limited formalization, making it difficult to follow the core idea. The exposition should be more concise and structured.\n- System complexity: The pipeline relies on multiple perception and estimation modules, which increases fragility and makes it harder to isolate the contribution of the proposed concept itself.\n- Task coverage: Most tasks are relatively simple (e.g., opening covers, lifting handles). The method does not address dynamic or non-prehensile interactions and is limited to quasi-static manipulation.\n- Evaluation scope: Real-world results are encouraging but cover a narrow set of scenarios, raising questions about generalization to more complex articulated objects and force-sensitive tasks."}, "questions": {"value": "- Please clarify the formal definition of a manipulation concept and how it differs from prior structured representations (e.g., articulation keypoints, action primitives).\n- How would the method extend to dynamic or non-prehensile tasks (e.g., pushing, sliding, compliant doors)?\n- Can you provide concrete code/template examples to better illustrate the representation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9Q1ZvDNAJ8", "forum": "StpFxymia9", "replyto": "StpFxymia9", "signatures": ["ICLR.cc/2026/Conference/Submission1343/Reviewer_rSyi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1343/Reviewer_rSyi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760882831827, "cdate": 1760882831827, "tmdate": 1762915743639, "mdate": 1762915743639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Manipulation Concept, a representation for articulated-object manipulation. Instead of only predicting “where to act” or only estimating articulation, the method encodes how a parallel gripper should interact with a specific actionable part as a parameterized code template (e.g., Ring_Handle_Lift, Cuboid_Cover_Open). Each concept is defined over (i) geometric parameters (a small set of primitives: cuboid, ring, cylinder, sphere) and (ii) affordance parameters (in (−1, 1), controlling concrete grasp pose / force direction). On top of this representation, the authors build a pipeline: a VLM picks the object / part / action → Grounded-SAM extracts the part point cloud → a point-transformer-style module regresses geometric and affordance parameters → the instantiated concept analytically produces gripper actions. Experiments on PartNet-Mobility (sim) and several real-world articulated objects show clearly higher success rates than Where2Act, ManipLLM, and a re-implemented A3VLM, especially on unseen categories. A useful system-level error breakdown shows that concept selection and part grounding are current bottlenecks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The authors are going in the right direction by making affordance explicitly gripper-conditioned. A lot of prior “pixel-to-action” or “part-to-action” works conflate “this part is movable” with “this is how my specific end-effector should move it.” This paper makes that mapping explicit: (actionable part, gripper action) → an executable template. For robotics this is the level that’s actually useful.\n* Although many submodules are off-the-shelf (Grounded-SAM, FoundationPose, Point Transformer, GMM), this is acceptable in robotics: integration is 50% of the contribution. The paper shows a working stack from language → vision → part → parameters → executable gripper command, in sim and on a real arm. That’s valuable.\n* On both train and especially test categories, the method beats Where2Act / ManipLLM / A3VLM under the same parallel-gripper setup (Table 1).\n* The progressive “give GT to the next stage” analysis is very informative: it cleanly shows that perception-side errors (concept selection, part grounding) dominate, and that once the concept is right and geometry is right, the analytic template really works. I like this style of system-error decomposition."}, "weaknesses": {"value": "* The whole pipeline assumes that target parts can be simplified to cuboid / ring / cylinder / sphere. That’s elegant, but in real homes/industry we see non-convex, multi-material, undercut, and visually weird handles. Those are precisely the OOD cases where you need affordance generalization, and here the abstraction is at odds with the goal.\n* Figure 3 (right) has three “?”: It’s not clear what those three question marks represent.\n* You call Table 2 “ablation,” but it’s really cumulative system error decomposition (“what if this stage were perfect?”).\n* In autonomous driving there are works that also take the stance “keep a small program / template then fill it,” e.g. “Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents,” CVPR 2024 and “Chameleon: Fast–Slow Neuro-Symbolic Lane Topology Extraction,” ICRA 2025. Citing them would make the story stronger."}, "questions": {"value": "The authors say they generate a canonical mesh at a standard pose using the estimated geometric parameters and then send it to FoundationPose. Is this mesh literally just the primitive (e.g., a cylinder with radius R, height H)? If so, how robust is FP when the real part has extra geometry (ribs, a spout, fillets)? How about trying a learned single-view CAD recovery method (e.g., One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation) to produce a more realistic surrogate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W6rciIcYji", "forum": "StpFxymia9", "replyto": "StpFxymia9", "signatures": ["ICLR.cc/2026/Conference/Submission1343/Reviewer_rMeD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1343/Reviewer_rMeD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827606571, "cdate": 1761827606571, "tmdate": 1762915743532, "mdate": 1762915743532, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel pipeline for general articulated object manipulation with robot grippers, particularly parallel grippers. To address the complexity of articulated objects and gripper interactions, the authors introduce the concept of \"manipulation,\" a library of templates for different object part connections and shapes. To enable generalizable perception, the authors utilize a Vision-Language Model (VLM) to identify actionable parts and the corresponding manipulation concepts. Geometry and affordance parameters are then estimated using a learned neural network. These parameters facilitate precise motion planning, thereby improving both accuracy and success rates. Experiments are conducted in both simulation and real-world settings. The reported success rates demonstrate the superiority of the proposed method over previous approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors abstract interactions with articulated objects into a series of manipulation concepts, each described by geometric and affordance parameters. This provides a generalizable and accurate manipulation paradigm.\n- The affordance parameters offer greater control over gripper motion, enabling more dexterous and multi-modal manipulation.\n- The proposed method is supported by extensive experimentation. It is evaluated on the Where2Act simulation benchmark and a real-world robot arm, showing significant improvements over three baseline methods."}, "weaknesses": {"value": "- The most relevant prior work to this paper is SAGE[1], which also incorporates a VLM and Grounded-SAM to determine actionable part categories and uses learned networks to measure object sizes and poses. While SAGE is mentioned in the related works section, the authors do not clarify the novelty of their approach or provide a comparison of performance in the experiments.\n- The manipulation concepts still rely on hand-crafted templates, which can be labor-intensive when scaling to additional object categories.\n[1] Haoran Geng, Songlin Wei, Congyue Deng, Bokui Shen, He Wang, and Leonidas Guibas. Sage: Bridging semantic and actionable parts for generalizable manipulation of articulated objects. In ICLR 2024 Workshop on Large Language Model (LLM) Agents."}, "questions": {"value": "- In the geometry parameter estimation, the combination of Point Transformer and MLP seems excessive for the problem. Why not consider traditional parameterization techniques, such as RANSAC?\n- Although the paper claims to account for the unique characteristics of different grippers, it focuses only on parallel and suction grippers. The actions of both can be modeled with simple 6-DoF poses. Would the proposed method still apply if extended to fingered or flexible grippers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FALRbt9AB4", "forum": "StpFxymia9", "replyto": "StpFxymia9", "signatures": ["ICLR.cc/2026/Conference/Submission1343/Reviewer_tECJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1343/Reviewer_tECJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889273597, "cdate": 1761889273597, "tmdate": 1762915743429, "mdate": 1762915743429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript introduces a novel analytic representation called Manipulation Concept, which encodes gripper-based manipulation skills as parameterized program templates. Each concept formalizes the interaction between an actionable part (e.g., cuboid door, ring handle) and a gripper action (e.g., push, lift), linking geometric and semantic information with executable robot actions.\n\nThe authors build a Manipulation Concept Library (MCL) and propose an end-to-end framework that integrates:\n\n1/ A vision-language model (VLM) for concept selection based on language instructions and visual inputs;\n2/ A Grounded-SAM module for actionable part grounding;\n3/ A parameter estimation network predicting geometric and affordance parameters;\n4/ A programmatic instantiation step that outputs precise gripper poses and force directions for execution.\n\nExperiments on the PartNet-Mobility dataset and real-world articulated objects demonstrate that the approach outperforms prior state-of-the-art methods (e.g., Where2Act, ManipLLM, A3VLM) in success rate and generalization ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1/ The “Manipulation Concept” framework provides a clean, modular, and interpretable way to bridge geometric reasoning, affordances, and physical execution — something often lacking in purely data-driven manipulation approaches.\n2/ The method generalizes across unseen object categories, with substantial improvements in both simulated and real-world settings, indicating that the analytic abstraction is meaningful and transferable.\n3/ The paper is well written and logically organized, with clear diagrams (e.g., Fig. 1–3) illustrating the concept structure and pipeline."}, "weaknesses": {"value": "1/ The approach focuses mainly on parallel grippers. Although the authors briefly mention possible extensions to suction grippers, broader applicability to multi-fingered hands or tool use remains unexplored.\n2/ While programmatic templates improve interpretability, they require manual definition and careful parameterization. It’s unclear how scalable the approach is when facing highly deformable or irregular geometries not well approximated by simple primitives.\n3/ The multi-stage process (VLM → segmentation → pose estimation → parameter inference) could introduce latency. Real-time performance metrics are not discussed."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "s84PSwxeqX", "forum": "StpFxymia9", "replyto": "StpFxymia9", "signatures": ["ICLR.cc/2026/Conference/Submission1343/Reviewer_v61i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1343/Reviewer_v61i"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903200586, "cdate": 1761903200586, "tmdate": 1762915743313, "mdate": 1762915743313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}