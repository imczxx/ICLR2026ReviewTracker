{"id": "b75gIu5adT", "number": 25085, "cdate": 1758363937321, "mdate": 1759896734909, "content": {"title": "Warfare: Breaking the Watermark Protection of AI-Generated Content", "abstract": "AI-Generated Content (AIGC) is rapidly expanding, with services using advanced generative models to create realistic images and fluent text. Regulating such content is crucial to prevent policy violations, such as unauthorized commercialization or unsafe content distribution. Watermarking is a promising solution for content attribution and verification, but we demonstrate its vulnerability to two key attacks: (1) Watermark removal, where adversaries erase embedded marks to evade regulation, and (2) Watermark forging, where they generate illicit content with forged watermarks, leading to misattribution. We propose Warfare, a unified attack framework leveraging a pre-trained diffusion model for content processing and a generative adversarial network for watermark manipulation. Evaluations across datasets and embedding setups show that Warfare achieves high success rates while preserving content quality. We further introduce Warfare-Plus, which enhances efficiency without compromising effectiveness.", "tldr": "", "keywords": ["Content watermark", "watermark removal", "watermark forging"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9d2f0ace841c9fe71274103dac0d46680adc4a36.pdf", "supplementary_material": "/attachment/b0a00593f9b238b97402849a699c49cccc156add.zip"}, "replies": [{"content": {"summary": {"value": "A unified watermark attack method is proposed, which consists of a pre-trained diffusion model and a generative adversarial network. Watermark removal and forgery are achieved jointly via using both watermarked data and its denoised version, through which the GAN is implemented. Thorough evaluations have been provided with clearly articulated loss functions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The proposed idea is clearly conveyed and it is quite enjoyable to read the paper.\n\nThe proposed method does not require the unwatermarked counterpart of the watermarked images or the watermarking schemes, due to the use of a pretrained diffusion model to create a denoised copy.\n\nThe proposed design of using GAN to discern watermarked data and denoised data is quite novel, which leads to the good advantage that both post-hoc and prior watermarking methods can be attacked."}, "weaknesses": {"value": "It is unknown how the denoised images replicate their original counterparts. On the one hand, this depends on the capability of the pre-trained model. On the other hand, this also depends on the underlying watermarking model -- different watermarking models may render different denoised results, leading to certain xhat being not similar to the original x."}, "questions": {"value": "Now that the proposed method does not require any information about the watermarking schemes, do the watermarked images have to be AIGC? Would the proposed method be applicable to other watermarked images? The answer to the latter question seems to be yes, due to the definition of x, x' and xhat. Please clarify.\n\nHow effective is the adopted pre-trained diffusion model H? It would be helpful if the authors could clarify what kinds of watermarks can/cannot be removed by H.\n\nIt seems to me that watermark removal and forgery have an intrinsic tradeoff, which is analogous to the tradeoff between type I and II errors (or that between false positive and negative). Can the authors provide some insights on this aspect, for example, whether and how the proposed method may also experience such a tradeoff?\n\nI understand that visible watermarks are not considered because the denoised version would not be similar to the original version. However, I'm not sure why only the steganographic approach is considered within the invisible category. It might be helpful if the authors could explain what aspects of steganographic approaches are taken advantage of when designing the attack."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gF8sRychhU", "forum": "b75gIu5adT", "replyto": "b75gIu5adT", "signatures": ["ICLR.cc/2026/Conference/Submission25085/Reviewer_1uLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25085/Reviewer_1uLi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761617988652, "cdate": 1761617988652, "tmdate": 1762943319342, "mdate": 1762943319342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Warfare, an image watermark removal and forging attack. Warfare utilizes a public diffusion model (DM) to generate unwatermarked images, and subsequently trains GANs to learn mappings between watermarked and unwatermarked images in both directions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is clearly described, and the paper is well written and easy to follow.\n\n2. The black-box attack setting is realistic and relevant to practical watermarking scenarios."}, "weaknesses": {"value": "1. The goal of watermark removal appears largely accomplished in the first step, using the pre-trained diffusion model to generate unwatermarked images, a method already explored in prior work. The additional GAN mapping step seems redundant. The authors are encouraged to more clearly articulate the novelty and benefits of their approach relative to established methods such as DiffPure. In this light, statements like “the first work on…” feel somewhat overstated. \n\n2. Training a GAN typically requires thousands of diverse samples; otherwise, the model risks overfitting or mode collapse. It seems unrealistic that a GAN can successfully learn watermark patterns from as few as ten training examples. The authors should provide full details on the training of GANs (complete architecture, initialization, etc.) and convincingly explain the reason of success under such data scarcity. I also invite other reviewers and the AC to share their perspectives on this point. \n\n3. Following the above concerns, if the proposed method indeed requires a large number of training samples, its practicality and advantage would be diminished. In contrast, DiffPure can perform watermark removal on a single image without additional knowledge."}, "questions": {"value": "Please see Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KRgzk4QSJO", "forum": "b75gIu5adT", "replyto": "b75gIu5adT", "signatures": ["ICLR.cc/2026/Conference/Submission25085/Reviewer_6gnC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25085/Reviewer_6gnC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788792263, "cdate": 1761788792263, "tmdate": 1762943318480, "mdate": 1762943318480, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Warfare and Warfare-Plus, as a unified framework for attacking watermarks in AI-Generated Content. The proposed method operates under a black-box threat model. The core idea involves three steps: (1) collecting watermarked images, (2) using a pre-trained diffusion model or unconditional sampling to generate non-watermarked mediator images, and (3) training a GAN to translate between the watermarked and mediator image distributions. This allows the framework to perform both watermark removal and watermark forgery attack."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The conceptualization of a single framework that can perform both watermark removal and forgery by reversing the mapping in a GAN is an elegant idea.\n2. The paper proposes Warfare-Plus as a more time-efficient alternative to Warfare, correctly identifying data pre-processing as a potential bottleneck. This focus on practical efficiency is a positive aspect."}, "weaknesses": {"value": "1. The paper's literature review is severely lacking. For a submission to ICLR 2026, it is surprising that there are no references to work published in 2025, especially in a fast-moving field like generative AI security. The authors appear to be unaware of the recent progress in both watermark removal and forgery, which leads them to make unsupported claims about their work's novelty.\n2. The central claim that this is the first work to forge watermark is demonstrably false. Several prior works have explored black-box watermark forgery.\n3. The experimental evaluation is not convincing because it omits comparisons with many recent and highly relevant black-box attack methods, such as CtrlRegen. The authors compare against simple image transformations and a few selected baselines but fail to benchmark against the true state-of-the-art in watermark removal and forgery. This omission makes it impossible to assess whether Warfare offers any meaningful improvement in performance, efficiency, or applicability over existing techniques. A thorough experimental comparison against recent literature is essential for a paper in this field.\n4. The methodology rests on a critical but unsubstantiated claim. The authors state, \"The mediator dataset $\\hat{\\mathcal{X}}$ can be seen as being drawn from the same 'non-watermarked' distribution as $\\mathcal{X}$\". However, the paper provides no theoretical analysis to support this."}, "questions": {"value": "1. The reference list for this ICLR 2026 submission appears to stop in early 2024. Can you confirm if a thorough literature search was conducted for relevant work published in 2024 and 2025? If so, why were more recent state-of-the-art attack methods not included as baselines?\n2. On what theoretical basis do you claim that the mediator dataset $\\hat{\\mathcal{X}}$ (generated by adding large noise and denoising) is drawn from the same distribution as the original clean dataset $\\mathcal{X}$? Given that your own text states the resulting images are visually different, how does this assumption hold, and how does a potential distributional shift affect the validity of your framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jx4HL6vnD9", "forum": "b75gIu5adT", "replyto": "b75gIu5adT", "signatures": ["ICLR.cc/2026/Conference/Submission25085/Reviewer_PXX6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25085/Reviewer_PXX6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25085/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934424500, "cdate": 1761934424500, "tmdate": 1762943318199, "mdate": 1762943318199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}