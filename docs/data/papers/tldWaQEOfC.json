{"id": "tldWaQEOfC", "number": 3166, "cdate": 1757346905875, "mdate": 1759898104714, "content": {"title": "Delayed Momentum Aggregation: Communication-efficient Byzantine-robust Federated Learning with Partial Participation", "abstract": "Federated Learning (FL) allows distributed model training across multiple clients while preserving data privacy, but it remains vulnerable to Byzantine clients that exhibit malicious behavior.\nWhile existing Byzantine-robust FL methods provide strong convergence guarantees (e.g., to a stationary point in expectation) under Byzantine attacks, they typically assume full client participation, which is unrealistic due to communication constraints and client availability. \nUnder partial participation, existing methods fail immediately after the sampled clients contain a Byzantine majority, creating a fundamental challenge for sparse communication. \nFirst, we introduce delayed momentum aggregation, a novel principle where the server aggregates the most recently received gradients from non-participating clients alongside fresh momentum from active clients. Our optimizer D-Byz-SGDM (Delayed Byzantine-robust SGD with Momentum) implements this delayed momentum aggregation principle for Byzantine-robust FL with partial participation. \nThen, we establish convergence guarantees that recover previous full participation results and match the fundamental lower bounds we prove for the partial participation setting. \nRemarkably, experiments on deep learning tasks showed our method not only maintained stable convergence under various Byzantine attacks, but also outperformed standard FL methods with partial participation in non-Byzantine settings.", "tldr": "", "keywords": ["Byzantine-robust Learning with Partial Participation", "Communication-efficient Federated Learning", "Byzantine-robust Federated Learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abd99fab9002dcbe5f65e32009603570217a107d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes **D-Byz-SGDM**, a very simple trick for partial-participation FL with Byzantine clients: at each round the server aggregates the fresh momentums from sampled clients and reuses the last stored momentum for the non-sampled ones, so aggregation always includes all clients.  The algorithm uses a (δ, c)-robust aggregator and standard FL assumptions. Theory gives an upper bound that converges to an \\(O(\\delta\\,\\zeta^2/p)\\) neighborhood, and there is also a matching lower bound under Bernoulli sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Idea is neat & minimal.** Delayed momentum aggregation is tiny change; server keeps one vector per client (same memory as full participation).\n- **Clear motivation.** Nicely explains why robust aggregation with partial participation can fail (some rounds have Byzantine majority), which motivates the method.\n- **Theoretical coverage.** Upper bound has the right dependence on \\((\\delta,\\zeta^2,p)\\); when \\(p=1\\) it matches known results; the **lower bound** suggests \\(\\Omega(\\delta\\zeta^2/p)\\) is unavoidable, so rates are tight in the main parameters."}, "weaknesses": {"value": "- **Assumption 5 (bounded gradients) is very strong, and also this is mentioned in the paper.** The authors say it is kept “for simplicity.” What does simplicity mean exactly here? Is it related to the proof technique? I beleive claiming that it is possible to remove the assumption and not doing it only weakens the paper.  If it is possible to obtain the main results without this assumption, it would **heavily strengthen** the contribution. Even if constants get worse, removing (or relaxing) it would be big.\n\n- ** Limited application. ** \nThe method requires the server to keep a momentum vector per client. this can be very limiting in the cross-device FL when we might have a very large number of clients, and partial participation is mostly motivated and useful in these cases.\n\n- **Lower-bound proof (Theorem 7) construction detail looks fragile.**  \n  The proof builds two worlds and, in **World 1**, uses $p\\delta n$ “biased” honest clients to create heterogeneity while keeping the global objective nice. The paper then needs to verify the heterogeneity assumption (Assumption 4), essentially\n  $$\n  \\frac{1}{n}\\sum_{i=1}^n \\big\\|\\nabla f_i(x)-\\nabla f_1(x)\\big\\|^2 \\;\\le\\; \\zeta^2\n  \\quad \\text{for all } x.\n  $$\n  With the specific linear shift chosen in World 1, the calculation (as written) gives\n  $$\n  \\frac{1}{n}\\sum_{i=1}^n \\big\\|\\nabla f_i(x)-\\nabla f_1(x)\\big\\|^2\n  \\;=\\; \\frac{1-\\delta p}{p^2}\\,\\zeta^2 .\n  $$\n  For Assumption 4 to hold uniformly, one needs $\\frac{1-\\delta p}{p^2}\\le 1$, i.e.\n  $$\n  p^2+\\delta p-1\\;\\ge\\;0\n  \\quad\\Longleftrightarrow\\quad\n  p\\;\\ge\\;\\frac{-\\delta+\\sqrt{\\delta^2+4}}{2}.\n  $$\n  Example: with $\\delta=0.2$ this threshold is $\\approx 0.905$. So for common partial-participation regimes $p\\in(0,0.2]$ the displayed World-1 construction **does not** satisfy the heterogeneity bound; the assumption is violated exactly where partial participation is strongest. **My current score does not reflec this point as I think I am probalby missing something here, but if it is not addressed, I will reconsider the score as i belive this is a major concern.**"}, "questions": {"value": "."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SM1D2hHOa6", "forum": "tldWaQEOfC", "replyto": "tldWaQEOfC", "signatures": ["ICLR.cc/2026/Conference/Submission3166/Reviewer_dJvy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3166/Reviewer_dJvy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760699225342, "cdate": 1760699225342, "tmdate": 1762916581479, "mdate": 1762916581479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the Byzantine client problem in a practical federated learning system with partial client participation. Most of the prior work studies the ideal case where we have full client participation. Confronting this challenge, this paper proposes a delayed momentum aggregation tool to tolerate the Byzantine partial client participation. Both theoretical and numerical results are provided."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The Byzantine problem is extremely important for the real-life deployment of a federated learning system.\n* The proposed algorithm is, by design, simple and intuitive."}, "weaknesses": {"value": "* The paper's claim contradicts itself. In Section 1 (lines 72 - 74), the introduction talks about the problem of the variance reduction method, such that the variance reduction method performs poorly for deep neural networks. However, momentum and memorization are different forms of variance reduction techniques. In fact, the proposed algorithm is a variant of the renowned SAG algorithm [1]. Moreover, the author further criticizes some prior works for not having variance reduction in lines 238 - 239. This causes significant confusion to the reviewer. The author should consider another round of proofreading to make sure their claims are consistent.\n* Following up on the previous point, the authors also need to discuss the variance-reduced FL algorithms in Section 2. In particular, the algorithm proposed in this paper has a close connection with MIFA [2], which the authors overlook in the submission.\n* The authors may also wish to be more careful when making a statement. For example, the authors claimed that \"in some rounds, the sampled set may contain a Byzantine majority, despite the global condition $\\delta < 1/2$. In such cases, no robust aggregator can distinguish adversarial from honest updates. The likelihood of such Byzantine-majority rounds grows with time.\"\nin line 214-215. However, this statement is incorrect when we have side information available. A trivial example is that we know exactly the identity of a Byzantine adversary.\n* Typo: the initialization of the memorized momentum in line 219 should be a vector $\\mathbf{0}$.\n* Redundant Assumptions. In Theorem 6, the results are based on Assumptions 4 and 5. However, Assumption 5 naturally leads to Assumption 4. That is\n$$\\frac{1}{G} \\sum_{i \\in {\\mathcal{G}}} \\\\|\\nabla f_i(x) - \\nabla f(x)\\\\|^2 \\le \\frac{2}{G} \\sum_{i \\in {\\mathcal{G}}} \\\\|\\nabla f_i(x)\\\\|^2 + 2\\\\|\\nabla f(x)\\\\|^2 \\le  \\frac{4}{G} \\sum_{i \\in {\\mathcal{G}}} \\\\|\\nabla f_i(x)\\\\|^2 \\le 4 B^2,$$\nwhere the last inequality holds due to Assumption 5.\n* One of the novelties of the proposed method is that it doesn't require the Byzantine fraction of the sampled clients to be bounded per round. However, the authors fail to discuss why it works. The experiments also fail to study the impact of different Byzantine client fractions on algorithm performance, whether partial or full participation. The Byzantine fraction is always set as a constant $0.2$.\n\n\n**Reference**\n\n[1] Blatt, D., Hero, A. O., & Gauchman, H. (2007). A convergent incremental gradient method with a constant step size. SIAM Journal on Optimization, 18(1), 29-51.\n\n[2] Gu, X., Huang, K., Zhang, J., & Huang, L. (2021). Fast federated learning in the presence of arbitrary device unavailability. Advances in Neural Information Processing Systems, 34, 12052-12064."}, "questions": {"value": "* Can the authors clarify their opinions on variance reduction techniques?\n* Can the authors enhance the related work with significantly more evidence?\n* Can the authors discuss their connection with MIFA and why adopting naive gradient won't come with a robust Byzantine guarantee? Preferably, the claims will be accompanied by reasons beyond intuition.\n* Can the authors thoroughly check their statements in the paper to make sure they are scientifically correct and rigorous?\n* Can the authors thoroughly check their proofs to make sure all the assumptions are necessary and derivations are correct?\n* Can the authors provide more numerical evidence regarding the impacts of Byzantine fraction on the algorithm performance under both partial and full participation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BNjqGTRJYG", "forum": "tldWaQEOfC", "replyto": "tldWaQEOfC", "signatures": ["ICLR.cc/2026/Conference/Submission3166/Reviewer_TprE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3166/Reviewer_TprE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761232228197, "cdate": 1761232228197, "tmdate": 1762916581302, "mdate": 1762916581302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes D-Byz-SGDM algorithm for Byzantine-robust training, which can provably converge also in the partial participation regime, where only a subset of clients is selected. The main challenge is that in such a case, Byzantine clients can form a majority and destroy the training. The authors demonstrate that the proposed algorithm can converge even with partial participation by using momentum buffers from the previous round for the clients that were not selected in the current round. The authors provide convergence proof under the bounded gradient assumption and test the performance in training standard models against several baselines. Empirical results demonstrate the competitive performance of the proposed algorithm in practice."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Detailed experimental setup, where all training details are properly described along with sweep details. \n\n- The proposed method is tested in various settings: IID and Non-IID, under different attacks, models, and datasets. In all cases, the proposed algorithm achieves competitive performance. Surprisingly, it even outperforms FedAvg in the absence of Byzantine clients.\n\n- The method is simple and easy to incorporate into existing code bases."}, "weaknesses": {"value": "- Convergence under the bounded gradient assumption. This is the key limitation of this work in theory. The authors made a small note that it can be avoided. I encourage the authors to provide a discussion and a proof sketch, so that it can indeed be avoided. Otherwise, such claims do not bring any value.\n\n- In the experiments, the authors compare against a modified version of Byz-VR-MARINA-PP (where I guess variance reduction is replaced by momentum averaging), instead of the one originally proposed. Therefore, the comparison against the original Byz-VR-MARINA-PP is missing, which is the only existing baseline in the literature. I encourage the authors to include a discussion on the modification of Byz-VR-MARINA-PP and explain why this modification was chosen over the original algorithm.\n\n- Increased memory footprint as the server has to store all momentum buffers. This is not the case for Byz-VR-MARINA-PP, where only one vector of parameters should be stored. The authors are encouraged to provide a more detailed discussion on the algorithm design, its advantages, and disadvantages.\n\n- The proof of lower bounds consist the restriction $p \\ge \\frac{-\\delta + \\sqrt{\\delta^4+4}}{2}$, which must be specified in the statement. In my view, this is quite restrictive in opposite to what the authors claim. In particular, for $\\delta=0.1$, it requires $p \\gtrsim 0.95$, which means \"almost\" a full participation regime. Moreover, the choice made in experiments, $\\delta=0.2$ and $p=0.5$, does not satisfy it. Finally, for any choice of $\\delta$, we have $p \\gtrsim 0.78$. I encourage the authors to recheck their proofs and provide a more relaxed condition on $p$ and $\\delta$, which makes sense in real scenarios."}, "questions": {"value": "- This sentence: \"However, variance reduction methods perform poorly for deep learning model\" is partially wrong and refers to old work. There are many recent successful variants of variance-reduced deep learning optimizers [1,2,3] that are competitive in practice.\n\n[1] Yin, Yida, et al. \"A coefficient makes svrg effective.\" arXiv preprint arXiv:2311.05589 (2023).\n\n[2] Yuan, Huizhuo, et al. \"Mars: Unleashing the power of variance reduction for training large models.\" arXiv preprint arXiv:2411.10438 (2024).\n\n[3] Chang, Da, Yongxiang Liu, and Ganzhao Yuan. \"On the Convergence of Muon and Beyond.\" arXiv preprint arXiv:2509.15816 (2025).\n\n- How difficult is it to generalize the method to other sampling strategies (e.g., when we sample $S$ clients from all $n$ clients)?\n\n- Some captions are confusing and not clear on the first read. The authors are encouraged to make all figures self-contained by explaining what changes from one line of figures to another (e.g., IID or Non-IID should also be specified)\n\n- Why are FedAvg and FedAvgM not included in the Resnet-18 results?\n\n- Can the authors provide more details behind (1)? In my view, this equality is not obvious and should be clearly derived. From the first glance, it looks like the RHS of (1) should scale with $\\alpha$, not $\\alpha^2$ (at least, it is the case in the full participation regime). Moreover, it is not obvious to me why the probability parameter $p$ is not involved in (1). I have similar questions regarding (2-6). I encourage the authors to provide more sophisticated derivations, where each step is clearly derived. Moreover, (1) contradicts the derivations in line 1524. Which one is actually correct?\n\n- What does it mean \"technically we can prove this but omit it for simplicity maybe icnluded to time 0\" in lines 1438-1439? What to prove? How can the word \"maybe\" be used in such a claim?\n\n- In line 1516, there is a typo; I believe it should be $\\nabla f_i(x^{t-1})$, not $\\nabla f(x^{t-1})$.\n\n- Derivations in lines 1569-1588 are also unclear; the authors hide most of the steps and show only the final bound, which makes it difficult to check the correctness. \n\n- In line 1397, the momentum parameter $\\alpha$ satisfies $\\alpha^2 \\ge \\frac{90L^2\\eta^2}{p^2}$, while in the statement of theorem 13 $\\alpha^2 = \\min\\{1, 9L\\eta/p\\}^2 \\le 81L^2\\eta^2/p^2$. Is this only a typo?\n\n- In the statement of Theorem 13, why do the authors need such an initialization that decays with $n$, not $G$? Is it a typo?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5qxX2GSIYD", "forum": "tldWaQEOfC", "replyto": "tldWaQEOfC", "signatures": ["ICLR.cc/2026/Conference/Submission3166/Reviewer_VRBG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3166/Reviewer_VRBG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761573392288, "cdate": 1761573392288, "tmdate": 1762916581078, "mdate": 1762916581078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes distributed stochastic gradient methods with **asynchronous momentum updates** for **Byzantine-robust federated learning under partial client participation**, a setting where standard robust aggregation fails. The methods achieve **sub-linear convergence with residual error**, matching known lower bounds, and are validated empirically on MNIST and CIFAR-10, showing superior accuracy across various robust aggregators and data heterogeneity levels. Key limitations include reliance on restrictive assumptions, lack of analysis for communication delays, and missing discussion of related work on momentum-based Byzantine-robust methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes distributed stochastic gradient methods with asynchronous momentum updates under **partial client participation** for solving Byzantine optimization problems. It is the first Byzantine-robust method capable of handling partial client participation, a crucial feature for federated learning (FL) settings.  The main technical difficulty is that the robust aggregation strategy under partial client participation fails to be Byzantine-robust. The proposed solution is to leverage asynchronous/delayed momentum aggregation protocols. \n- The proposed methods achieve robustness against Byzantine clients while attaining **sub-linear convergence with residual error**, matching known lower bounds. This indicates the tightness of the derived convergence guarantees.  \n- **Extensive experiments**, including training a convolutional network on MNIST and a ResNet-18 on CIFAR-10, demonstrate that the proposed algorithms consistently achieve superior convergence performance in terms of final accuracy. These results hold across various settings, including different robust aggregators and levels of data heterogeneity, in both Byzantine and non-Byzantine scenarios."}, "weaknesses": {"value": "- The convergence results are limited because they rely on three restrictive assumptions: Assumptions 3, 4, and 5. First, Assumptions 3 and 5 appear redundant. These assumptions were not required in Gorbunov et al. (2023), so removing one could simplify the analysis without affecting convergence. Second, Assumption 4 is stronger than the $(G, B)$-gradient dissimilarity condition used in [1], which may unnecessarily restrict the applicability of the results.\n\n\n\n- The analysis does not impose any assumptions on communication delays in the asynchronous protocol. Consequently, Theorem 6 does not capture the effect of asynchrony on the stepsize or convergence of the proposed algorithms. This is unusual, as asynchronous or delayed gradient-based methods, e.g., [2,3], typically require the stepsize to account for the delay, which directly impacts the convergence bounds.\n\nRef: \n\n[1] Allouah, Y., Guerraoui, R., Gupta, N., Jellouli, A., Rizk, G., & Stephan, J. (2024). Adaptive gradient clipping for robust federated learning. _arXiv preprint arXiv:2405.14432_.\n\n[2] Feyzmahdavian, H. R., & Johansson, M. (2023). Asynchronous iterations in optimization: New sequence results and sharper algorithmic guarantees. _Journal of Machine Learning Research, 24_(158), 1–75.  \n\n[3] Gurbuzbalaban, M., Ozdaglar, A., & Parrilo, P. A. (2017). On the convergence rate of incremental aggregated gradient algorithms. _SIAM Journal on Optimization, 27_(2), 1035–1048."}, "questions": {"value": "- One prior work closely related to this paper is [1], as it also proposes algorithms using momentum updates in Byzantine settings. It would strengthen the manuscript to include a discussion of [1] and clarify how the proposed methods compare or differ from it.\n\n[1] Özfatura, K., Özfatura, E., Küpçü, A., & Gunduz, D. (2023). Byzantines can also learn from history: Fall of centered clipping in federated learning. _IEEE Transactions on Information Forensics and Security, 19_, 2010–2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EhZLd3pTaV", "forum": "tldWaQEOfC", "replyto": "tldWaQEOfC", "signatures": ["ICLR.cc/2026/Conference/Submission3166/Reviewer_thWo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3166/Reviewer_thWo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3166/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949299713, "cdate": 1761949299713, "tmdate": 1762916580920, "mdate": 1762916580920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}