{"id": "qLaUGPq6h6", "number": 23555, "cdate": 1758345373015, "mdate": 1759896808078, "content": {"title": "Attribution Guided Distillation for Matryoshka Sparse Autoencoders", "abstract": "Sparse autoencoders (SAEs) aim to uncover monosemantic, interpretable features from deep networks but they face key limitations such as feature splitting and absorption. Matryoshka SAEs (MSAEs) partially mitigate these issues with a nested dictionary architecture, yet they still suffer from redundancy, hyperparameter sensitivity, and higher reconstruction loss. To address these limitations, we propose a Distilled Matryoshka SAE (DMSAE) method that uses an attribution-guided selection strategy to iteratively identify and preserve high attribution neurons. In our method, these neurons are promoted into a frozen \"core\" that serves as the innermost prefix shared across all outer prefix reconstructions. When trained at layer 12 of Gemma-2-2B, DMSAE outperforms the MSAE baseline on most SAEBench benchmarks. DMSAE is model agnostic and may extend to other models where distilling a minimal core of features is desirable for interpretability and control.", "tldr": "We address limits of sparse autoencoders for mechanistic interpretability by iteratively distilling high-attribution neurons into a frozen core that is shared across training cycles.", "keywords": ["Mechanistic Interpretability", "Sparse Autoencoders", "Matryoshka Sparse Autoencoders"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/08c229ee8bc402589599b7e8f6ef903b9aabc1e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Distilled Matryoshka SAEs (DMSAEs), an extension of Matryoshka SAEs (MSAEs). The method first trains an MSAE, then uses gradient-based attribution to identify important latents in the smallest prefix. These latents are promoted into a frozen \"core,\" which is exempt from the sparsity penalty (BatchTopK) applied to other features. The authors evaluate this approach on layer 12 of Gemma-2-2B, comparing it against MSAE baselines on the SAEBench suite, and claim improvements"}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "See below"}, "weaknesses": {"value": "See below"}, "questions": {"value": "**Overall Rating:**\nI am not comfortable recommending acceptance for this paper. The core idea of an attribution-guided distillation process is interesting, but the work in its current form is undermined by key missing details, and what I believe to be a critical methodological flaw in its experimental comparison (not including the latents in the core in the L0 calculation). The evidence provided is insufficient to conclude that the proposed method offers a genuine improvement over existing techniques. I'm open to increasing my score substantially if these issues are addressed.\n\n### Major Comments\n\nThe following are things that, if adequately addressed, would increase my score.\n\n1. **The L0 comparison is critically flawed and likely invalidates the main results.** My primary concern is that the evaluation does not account for the additional active features in the DMSAE's core. The core is exempt from the BatchTopK penalty (Lines 191-193, 206-215), yet the L0 plots (Fig. 2, Fig. 4) report identical, discretized sparsity levels for all methods. This strongly suggests that L0 is only being measured on the non-core latents. If so, the DMSAE's effective L0 is actually K + c (non-core sparsity + core sparsity), while the baseline's is just K. This is a major confounder, as it is well-established that SAE performance improves with higher L0. The fact that core latents are frozen improves this somewhat, but it's still an unfair comparison. The claimed gains may simply be an artifact of DMSAE using more features. For a fair comparison, you must re-run your experiments matching the total effective L0 across all methods.\n2. **Missing Crucial Experimental Details.**\nThe paper omits several key details required for reproducibility and a full assessment of the methodology.\n\n\t- **Distillation Loop:** How many train -> promote -> retrain cycles were performed for the final models in Figure 4?\n\t- **Re-initialization:** When features are promoted to the core, are the weights of the remaining non-core latents re-initialized or do they continue training from their previous state? \n\t- **Architecture:** What is the total dictionary width (m)?\n\t- **Baselines:** Where do the baseline models in Figure 4 come from? Were they trained from scratch for this paper, or are they pre-trained models from SAEBench?\n3. **Lack of a Compute-Matched Comparison.**\nThe paper acknowledges in the limitations that each DMSAE cycle \"costs the same as training the original baseline Matryoshka SAE\" (lines 354-356). If the final DMSAE model required multiple cycles, its total training compute could be much more than the baseline. Training FLOPs improve performance, so this is another major confounder. A fair comparison requires training the MSAE baseline for an equivalent number of FLOPs to see if its performance improves and closes the gap. Without this, it's impossible to distinguish algorithmic improvements from the brute-force benefits of more training.\n4. **Interpretability Claims Are Unsubstantiated.**\nThe paper's primary motivation is to distill a \"monosemantic and interpretable\" core (line 360), but it offers no direct evidence that the promoted features are, in fact, interpretable. High-attribution features are not necessarily monosemantic; they could be high-frequency, polysemantic features that are crucial for reconstruction but semantically messy. While they are frozen after promotion to the core, methods like TopK can often find seemingly uninterpretable, high frequency latents. Suggested experiments:\n\t- An analysis of the **activation frequency** of the core latents.\n\t- **Frequency-weighted auto-interpretability scores** to ensure high scores aren't driven by a few clean but rare features.\n\t- A small-scale **human interpretability study** on the core features to sanity-check their quality.\n\n### Minor Comments\n\nThe following are suggestions that I hope will improve the paper, but are unlikely to change my score.\n\n- The attribution-guided promotion is the core technical contribution, but it lacks a simple ablation. It would be valuable to compare against a baseline that promotes features based on a simpler heuristic, such as **activation frequency**, to demonstrate that the gradient-based attribution is providing value. \n\nI found this work somewhat difficult to follow, so apologies if I have missed or misunderstood any key details. Please correct me if I have!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WlalYrhRs6", "forum": "qLaUGPq6h6", "replyto": "qLaUGPq6h6", "signatures": ["ICLR.cc/2026/Conference/Submission23555/Reviewer_Ndtg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23555/Reviewer_Ndtg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761256973355, "cdate": 1761256973355, "tmdate": 1762942710190, "mdate": 1762942710190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Distilled Matryoshka Sparse Autoencoders (DMSAEs), an attribution-guided extension of Matryoshka SAEs for extracting monosemantic features from LLM activations. The method iteratively identifies neurons with high attribution to the model’s next-token loss and freezes them into a permanent “core” shared across all Matryoshka prefixes. The goal is to distill a compact and interpretable latent basis that avoids redundancy and feature fragmentation. The authors evaluate on Gemma-2-2B layer 12 using SAEBench and report modest but consistent improvements in interpretability metrics over standard MSAEs. The paper is conceptually interesting and technically competent, though empirically limited and computationally heavy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The attribution-guided distillation idea is elegant and grounded in a clear causal motivation, namely selecting features that matter for model behavior, not just for reconstruction. The design integrates attribution analysis with hierarchical feature learning in a way that directly addresses known SAE pathologies such as feature absorption and redundancy. The experimental methodology is transparent and uses established SAEBench benchmarks, showing reproducible gains in interpretability at minimal cost in reconstruction fidelity."}, "weaknesses": {"value": "The empirical scope is narrow, limited to a single layer of one model. The attribution signals used for core promotion are noisy and their semantic validity is unclear, high gradient attribution does not necessarily imply human-interpretable meaning or actual causal relevance, but it is probably an okay approximation. The method is computationally expensive, requiring multiple full training cycles, which undermines its scalability to larger models or multilayer analysis. The evaluation only shows limited improvement over baselines and is limited to a single layer, which make this hard to judge as SAE Bench results can differ significantly by layer. Overall the paper’s promise exceeds the evidence currently presented."}, "questions": {"value": "- How stable are the promoted “core” neurons across different random seeds, datasets, or layers? \n- Can the authors provide qualitative examples of distilled features that illustrate improved monosemanticity beyond benchmark scores?\n- What is the computational cost per distillation cycle in practice, and are there strategies to improve that for larger-scale applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PuvC6mWxr9", "forum": "qLaUGPq6h6", "replyto": "qLaUGPq6h6", "signatures": ["ICLR.cc/2026/Conference/Submission23555/Reviewer_KLH6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23555/Reviewer_KLH6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839468191, "cdate": 1761839468191, "tmdate": 1762942709109, "mdate": 1762942709109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Distilled Matryoshka SAEs. Starting from a standard Matryoshka sparse autoencoder, the method periodically computes an attribution score, then \"promotes\" the highest attribution neurons into a frozen core that is shared by all prefixes. Encoder rows for promoted neurons are frozen, decoder weights remain trainable, and BatchTopK sparsity is applied only to the remaining non core latents. Attribution uses a GradInput projected onto the unit decoder direction and aggregated across token positions with a high quantile; promotion selects the smallest prefix of neurons that reaches a target fraction of total attribution. Proof of concept experiments at layer twelve of Gemma two two B report small but consistent gains over a Matryoshka baseline on four of six SAEBench metrics at low $\\ell_0$, with weaker results at higher $\\ell_0$."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper reads well, here are in my opinion the positive points (P) of the article:\n\nP1. The idea of a small, dense, reusable core shared by all prefixes is clear and easy to reason about, and Algorithm 1 makes it implementable within existing Matryoshka code paths. The schematic on page four is helpful although a bit complex.\n\nP2. The attribution definition is explicit. Equation 7 specifies Grad times Activation projected onto the decoder direction, with a quantile over token positions, I believe i could reproduce the ranking used for promotion.\n\nP3. The empirical sweep is at least systematic at low sparsity, and the heatmap on page six relating core size to attribution coverage and sparsity is interesting!"}, "weaknesses": {"value": "In my opinion, this paper is interesting but has some weaknesses thta I will describe here, I will group them in major (M) and minor (m) problems.\n\nM1. Method complexity versus modest gains. The pipeline adds attribution computation, ranking, promotion, and retraining. Each cycle costs roughly one full Matryoshka training according to Section 5, yet Figures 2 and 4 show improvements that are small and not universal, with drops at higher $\\ell_0$ and slight deficits on Loss Recovered and RAVEL even at the best operating point. Without stronger gains or a clearer story for when the method helps, the cost benefit picture is not compelling. \n\nM2. Missing baselines. MP-SAE [1] and collaborators, as well as orthogonal matching pursuit style encoders, create a conditional selection effect that is highly relevant to absorption and composition. They should appear in the benchmark and in related work. The absence makes it hard to tell whether your benefit is coming from conditional selection like behavior or from the distillation loop.\n\n[1] From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit, Costa & al.\n\nM3. Neuron basis attribution versus directional derivatives at the dictionary level. The paper ranks individual latent neurons using Grad times Activation projected on the unit decoder. This is close to a directional derivative along decoder directions, but the exposition leaves open alternatives that might better match the objective. For example, rank by the directional derivative of the language model loss with respect to the reconstruction vector after projecting the gradient onto the full dictionary span, or decompose the gradient into the current active set and measure marginal attributions conditional on co active atoms? Can you expand on that and if possible do a short comparison?\n\nM4. Why use a high quantile aggregator. Equation 7 aggregates per token scores with a quantile but gives limited motivation beyond robustness. Can you justify the choice of q? compare to an average, a trimmed mean? And if possilble show stability of the ranking across batches. Sensitivity of the promoted set to q should be reported, since Figure 2 suggests limited effect from promotion fraction while the aggregator might still change which neurons enter the core.\n\nM5. Related work is thin for ICLR expectations. The background is largely limited to recent SAE variants and Matryoshka works and contain 17 citations. This undercuts positioning and novelty arguments.\n\nM6. Ambiguity in attribution definition across the paper. Figure 1 caption refers to Integrated Gradients, while the method and Algorithm 1 actually implement a GradientInput quantile. This must be made consistent and the reason for the final choice explained.\n\nNow for the minor problems:\n\nm1. Can you clarify notation and normalisation in Equation 7? Also state whether decoder columns are normalised continuously during training and whether the ReLU term uses pre or post normalisation activations?\n\nm2. Fonts are really small in the six panels of Figure 2 and in the heat map of Figure 3. Also if possible can you add uncertainty bands or error bars to all panels and ensure captions state the exact operating point and the absolute deltas.\n\nm3. Can you explain the choice to restrict candidates to the first non core block? A short ablation that allows selection across deeper prefixes would test whether important features are being excluded by construction.\n\nm4. Writing and structure. Several sections read densely and the related work omits key areas as noted. A careful pass to streamline Section 3 and to broaden Section 2 would help."}, "questions": {"value": "See the Major points M1-6 and the minor points 1-4."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ThQUTEbMoS", "forum": "qLaUGPq6h6", "replyto": "qLaUGPq6h6", "signatures": ["ICLR.cc/2026/Conference/Submission23555/Reviewer_af9R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23555/Reviewer_af9R"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23555/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031263827, "cdate": 1762031263827, "tmdate": 1762942708776, "mdate": 1762942708776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}