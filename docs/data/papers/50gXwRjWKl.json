{"id": "50gXwRjWKl", "number": 19065, "cdate": 1758293225652, "mdate": 1759897062960, "content": {"title": "TEL: A Thermodynamics-Inspired Layer for Adaptive, and Efficient Neural Learning", "abstract": "We introduce the Thermodynamic Equilibrium Layer (TEL), a neural building block that replaces fixed activations with a short, $K$-step energy-guided refinement. TEL performs $K$ discrete gradient steps on a Gibbs-inspired free energy with a learnable step size and an entropy-driven, adaptable temperature estimated from intermediate activations. This yields nonlinearities that are dynamic yet stable, expose useful per-layer diagnostics (temperature and energy trajectories), and run with a fixed, predictable compute budget. Across a broad suite of classification, regression, and reconstruction tasks, TEL matches or exceeds strong baselines, including MLPs, modern implicit/energy-based layers under compute matched dimensionality, FLOPs, and parameters. Swapping TEL in place of MLP feed forwards in standard CNN, LSTM, and Transformer blocks incurs minimal overhead while consistently improving accuracy. Together, these results position TEL as a scalable, drop-in alternative for constructing adaptable nonlinearities in deep networks.", "tldr": "", "keywords": ["Iterative learning", "Phsics based architecture", "Gibbs free energy", "adaptive nonlinearity", "Non linear layer"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b90c0349d1bbfd3ec8faf53116a4f946329e944f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a Thermodynamic Equilibrium Layer (TEL), which replaces fixed activations with a K-step refinement inspired by Gibbs free-energy minimization. While the idea is somewhat novel in framing, it functions simply as a residual block with adaptive gain rather than a true thermodynamic mechanism."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "(1) The idea of interpreting nonlinear activations through thermodynamic principles is interesting and could inspire further exploration of physics-inspired learning layers.\n\n(2) TEL is implemented with few extra parameters and simple iterative updates.\n\n(3) They provide theoretical bounds to ensure non-expansiveness and gradient stability, which is sound."}, "weaknesses": {"value": "(1) My main concern is the ambiguity on their concepts. The thermodynamic framing is not well-grounded. The terms Enthalpy and entropy are arbitrary mathematical terms: the free energy objective simply mixes a quadratic penalty and a learned nonlinearity. Meanwhile, there is no clear explanation of how thermodynamic reasoning improves learning, generalization, or localization. The theoretical results only ensure convergence and boundedness; they do not justify any learning benefit or representational improvement.\n\n(2) Another major concern is their weak empirical validations. CIFAR-10 accuracy is below 50%, and CIFAR-100 stays below 25%, far lower than standard baselines (ResNet or even small CNNs can over 90% in CIFAR-10 and 70% in CIFAR-100). This suggests the authors use extremely shallow or narrow models, which raises concerns if the proposed TEL is practical. Moreover, there are no ImageNet, Tiny-ImageNet, or large-scale experiments, despite claims of scalability. The improvements over weak baselines (Linear, MLP, KAN, DEQ, EBM) are minor (1 to 3%) and do not justify the additional complexity. Thus, the empirical evaluation remains proof-of-concept level, insufficient for a major-venue claim of a general adaptive nonlinearity.\n\n(3) The paper claims that the proposed TEL is a principled alternative to MLP nonlinearities, but in essence, it is a residual update with adaptive gain, a concept explored in Dynamic ReLU or gated activations. The supposed thermodynamic interpretation does not add theoretical or practical value beyond rebranding a weighted residual block.\n\n(4) Although the authors claim TEL exposes diagnostics like temperature and energy trajectories, these are presented only as scalar traces without insight into how they correlate with uncertainty, convergence, or task complexicity.\n\n(5) Another minor concern is the readability of their figures. Many figures cram too many datasets, baselines, and metrics into a single panel, making them visually overwhelming. Axes labels and legends are tiny and cluttered, which require readers to zoom-in."}, "questions": {"value": "(1) Why are the reported accuracies on CIFAR so low compared to standard baselines? Could you include results on a larger benchmark (e.g., Tiny-ImageNet or ImageNet-1k) to support the claim that TEL is a scalable and drop-in nonlinearity?\n\n(2) Why were only small MLP, KAN, DEQ, and EBM baselines used? Have you tested TEL inside modern architectures such as ResNet or ViT for fair comparison under realistic training regimes?\n\n(3) How do you ensure that the reported improvements are not due to different initialization, optimizer, or hyperparameter settings?\n\n(4) Could you report training and inference time or memory usage relative to standard activations?\n\n(5) What is the variance across runs for all experiments? Are the reported 1 to 3% performance gains statistically significant given the low baseline accuracies?\n\n(6) Beyond being a scaling factor, does the adaptive temperature correlate with sample difficulty, confidence, or generalization? Any quantitative evidence connecting the temperature T or energy trajectories to learning dynamics?\n\n(7) The ablations explore step count K and temperature adaptation, but what about the entropy estimator architecture and its sensitivity? How stable is TEL when stacked in deeper networks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X2ZQufYz2q", "forum": "50gXwRjWKl", "replyto": "50gXwRjWKl", "signatures": ["ICLR.cc/2026/Conference/Submission19065/Reviewer_QKX9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19065/Reviewer_QKX9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761423519620, "cdate": 1761423519620, "tmdate": 1762931093331, "mdate": 1762931093331, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Thermodynamic Equilibrium Layer (TEL), a neural network building block designed as an alternative to the standard fixed activation MLP layer. Instead of using a fixed one-step nonlinear transformation, TEL employs a K-step refinement process:  inspired by the minimization of Gibbs free energy of a physical system, TEL is optimized by balancing an “enthalpy\" term (anchoring the output to its initial linear projection, $Wx$) and an \"entropy\" term (a nonlinear activation $\\phi(y)$). An important feature of TEL is that it has an adaptive temperature parameter, $T$, which scales the entropy term and is updated at each step based on statistics of the intermediate activations. \n\nThe authors argue that TEL has a predictable compute budget compared with other alternatives like DEQ and energy-based models. Empirical results across classification, regression, and reconstruction tasks show that TEL matches or exceeds baselines (MLP, KAN, DEQ, EBM) at the same computational cost level and can serve as an effective drop-in replacement for MLP blocks in CNN, LSTM and Transformers on small to mid scale datasets."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- This paper presents a novel, thermodynamics-inspired idea for building an adaptive layer with controllable computational cost (via the step number K). The idea of treating deviation from linear projection as enthalpy (need to reduce) and non-linear activation as entropy (need to increase) also sounds intriguing.\n- The authors have tested TEL extensively in a wide range of tasks including classification, regression, and reconstruction. A representative suite of different models have been evaluated as the baseline to compare with. The compute-matched comparison suggests that TEL can consistently outperform or at least match the performance of other models.\n- The authors have also demonstrated that the proposed TEL can be a ‘drop-in’ function to replace the MLP blocks in existing model architectures and improve performance. The result that replacing only one layer (the first) can outperform replacing all MLP layers is also interesting."}, "weaknesses": {"value": "- While I find the proposed TEL is powerful, the design motivation of different components in it is a bit unclear to me. The proposed architecture presented in Figure 1 seems to be really complicated, but the functionality of each part lacks a clear explanation. This makes the methodology section difficult to follow and understand. \n- The instability observed in stacking multiple TEL together suggests that TEL cannot be viewed as a general replacement for MLP but more like an alternative option for ‘encoding’ (as the reported performance is for replacing the first MLP layer). This seems to be the most critical limitation of TEL.\n- Given that the TEL is a K-step sequential refinement process, it is reasonable to expect that it is not parallelizable across the K steps.\n\nMinor points about the clarity:\n- The legend of main text figures are too small to read. In addition, there is no legend for the bottom figure in Figure 3.\n- The ablation study on temperature is not explicitly referenced in the text (Figure 4)."}, "questions": {"value": "1) My main question is about the key differences between TEL and the regular K-step unrolled RNN. It seems that y in TEL can be taken as the hidden states and the parameters of $\\phi_{\\theta}$ are also shared at all K steps. Does this suggest that the performance gain is primarily due to the adaptive temperature term in TEL?\n2) Do the authors have any insights about why TEL stacking does not work? Is this due to the complexity of TEL or vanishing/exploding gradients or something else?\n3) For results presented in Figure 5, can the authors explain why the first MLP is chosen for replacement with TEL? Would it be more intuitively straightforward to change the last MLP? Does this result, combined with the stacking instability, suggest that TEL is best suited for early-stage feature extraction rather than as a general-purpose block? \n4) How should we interpret the temperature term in TEL? When putting into the thermodynamics context, temperature should depend more on the enthalpy term as it is directly correlated with energy. However, here in the TEL, temperature is updated based on entropy estimates. Why was $T$ designed to be a function of $S$ rather than $H$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3jSp12OeMo", "forum": "50gXwRjWKl", "replyto": "50gXwRjWKl", "signatures": ["ICLR.cc/2026/Conference/Submission19065/Reviewer_GxfC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19065/Reviewer_GxfC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937776767, "cdate": 1761937776767, "tmdate": 1762931092893, "mdate": 1762931092893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed the Thermodynamic Equilibrium Layer (TEL) to replace traditional neural layer with fixed activation function. TEL performs K-step iterative refinement to update the hidden state using both enthalpy and entropy. TEL can provide dynamic, input-dependent nonlinearities with fixed compute budget controlled by K. The proposed TEL is compared with multiple baselines, including MLP, KAN, DEQ, and EBM on classification, regression, and reconstruction benchmarks. Experimental results show that TEL achieves comparable or superior performance under matched parameter and FLOP budget, with better stability and interpretability."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* This paper provides strong theoretical proofs for stability, convergence, and expressivity.\n\n* TEL can be used in all common neural networks, such as CNN, LSTM, and Transformer. It can improve the accuracy of these networks in small to mid scale tasks.\n\n* This paper validated the performance of TEL on over 10 datasets across different tasks: classification, regression, and reconstruction.\n\n* At the same FLOPs, TEL can show consistent gain over baselines including MLP, KAN, and EBM.\n\n* Ablation studies are also implemented thoroughly to show the individual effect of K-step refinement and adaptive temperature. A good performance can achieve when K is between 3 and 5, which doesn't cause too much computational cost.\n\n* The temperature and energy can provide some interpretable signals."}, "weaknesses": {"value": "* TEL was only evaluated in small to mid scale tasks. It is unclear if TEL can perform better on larger dataset and more complex tasks.\n\n* Stacking many TEL layers can negatively affect the training performance. However, neural networks are usually deep. This may limit TEL to be applied on deeper neural networks.\n\n* TEL is more complex than simple ReLU and may complicate the training pipeline."}, "questions": {"value": "* How does TEL’s runtime and memory scale with K in real GPU wall-clock time compared to standard MLPs and DEQs?\n\n* How sensitive is the performance to the entropy estimator model, such as Gaussian vs tiny MLP?\n\n* Could you show the performance of the TEL with deep transformers on a larger-scale pretraining task? Such as ImageNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "GxUO40b7vd", "forum": "50gXwRjWKl", "replyto": "50gXwRjWKl", "signatures": ["ICLR.cc/2026/Conference/Submission19065/Reviewer_3SX3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19065/Reviewer_3SX3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971726601, "cdate": 1761971726601, "tmdate": 1762931092507, "mdate": 1762931092507, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Thermodynamic Equilibrium Layer (TEL): a novel neural network component inspired by thermodynamic principles. Instead of using fixed activation functions (e.g., ReLU, GELU), TEL performs a K-step gradient-based refinement on a Gibbs free energy functional, with a learnable, input-dependent temperature that evolves according to entropy estimates from intermediate activations. This formulation yields adaptive yet stable nonlinearities, provides per-layer diagnostics (temperature, energy trajectories), and maintains a fixed, predictable computational cost"}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. TEL connects neural computation to thermodynamic equilibrium, modeling activations as the outcome of energy minimization\n2. Unlike implicit or equilibrium models (DEQ/EBM), TEL uses a fixed number of refinement steps K, offering deterministic compute and stable training\n3. Solid theoretical analysis\n4. TEL integrates easily into existing architectures as a drop-in replacement, without redesigning the model"}, "weaknesses": {"value": "1. The presentation of the paper should be strongly improved. There are a lot of redundancies (eq 7 is equal to eq 13, eq 9 to eq 14, and eq 8 to eq 15), lack of connection with the appendix, missing definitions of terms and acronyms\n2. Experiments are confined to small-to-midscale benchmarks; no large-scale evaluations (e.g., ImageNet, large Transformers) are provided\n3. There are no details to fully understand if comparisons are fair"}, "questions": {"value": "1. Could you improve the presentation of the paper to address the issues mentioned in the weaknesses section? The current structure makes it difficult to follow the flow of ideas. Clarifying the transitions between the theoretical foundations, algorithmic formulation, and experiments would greatly enhance readability\n2. Could you provide all the details of your experimental setup? In particular, please specify how you chose the learning rates $\\eta_i$ for your model and for the baselines, as well as which loss functions and optimizers you used. If my understanding is correct, TEL effectively introduces a regularization term into the loss function (the entropy component $\\phi$, weighted by the temperature $T$) and employs a dynamic learning rate $\\eta$. Knowing these aspects is essential to properly assess the improvements your method achieves over the competitors"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YC6VHrOxgh", "forum": "50gXwRjWKl", "replyto": "50gXwRjWKl", "signatures": ["ICLR.cc/2026/Conference/Submission19065/Reviewer_BZ3t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19065/Reviewer_BZ3t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19065/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994029959, "cdate": 1761994029959, "tmdate": 1762931091672, "mdate": 1762931091672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}