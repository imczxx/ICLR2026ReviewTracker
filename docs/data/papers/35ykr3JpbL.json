{"id": "35ykr3JpbL", "number": 10033, "cdate": 1758157328664, "mdate": 1759897679605, "content": {"title": "StyleStream: Real-Time Zero-Shot Voice Style Conversion", "abstract": "Voice style conversion aims to transform an input utterance to match a target speaker’s timbre, accent, and emotion. A central challenge is disentangling linguistic content from style attributes. While prior work has investigated this disentanglement, the conversion quality remains suboptimal. Moreover, no existing work addresses real-time voice style conversion. To address these limitations, we propose StyleStream, the first streamable zero-shot voice style conversion system that achieves state-of-the-art conversion performance. StyleStream mainly consists of two components: a destylizer, which removes style attributes (timbre, accent and emotion) while retaining linguistic content, and a stylizer, which is a diffusion transformer (DiT) that reintroduces style conditioned on the target speech. Content–style disentanglement is enforced in the destylizer through two mechanisms: (i) automatic speech recognition (ASR) loss that provides text-level supervision, and (ii) a finite scalar quantization (FSQ) module with a compact codebook of size 45, which serves as a strong information bottleneck. The continuous representations preceding the FSQ layer are treated as the content features. By combining chunked-causal attention masking with a non-autoregressive architecture, StyleStream enables real-time voice style conversion with an end-to-end latency of 1 second.", "tldr": "We propose StyleStream, the first real-time zero-shot voice style (timbre, accent, emotion) conversion system that achieves state-of-the-art conversion performance.", "keywords": ["voice conversion", "accent conversion", "emotion conversion", "real-time", "zero-shot"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2677da6d477112f4a138f150f5d599481d01a43e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces StyleStream, the first system designed for real-time, zero-shot voice style conversion, which aims to transform a source utterance to match a target speaker's timbre, accent, and emotion. The work addresses two primary limitations of prior methods: suboptimal conversion quality due to incomplete disentanglement of linguistic content from style, and the lack of any existing real-time solution for this task.\nThe main contributions of the paper are:\n\n1.A Novel Two-Stage Architecture: StyleStream consists of a destylizer and a stylizer. The destylizer's key function is to achieve clean content-style disentanglement. It does this by combining two mechanisms: (1) text-level supervision via an Automatic Speech Recognition (ASR) loss, which forces the model to preserve only linguistically relevant information, and (2) a strong information bottleneck implemented with a compact Finite Scalar Quantization (FSQ) module (codebook size of 45). Crucially, it uses the continuous features before the quantization step as the final content representation to maintain high fidelity. The stylizer is a Diffusion Transformer (DiT) that reintroduces the target style onto the clean content features using a spectrogram inpainting objective.   \n\n2.First Real-Time Implementation: By using a non-autoregressive architecture and applying chunked-causal attention masking, StyleStream is the first framework to enable streaming voice style conversion. It achieves an end-to-end latency of approximately 1 second on a single GPU.   \n\n3.State-of-the-Art Performance: The proposed method achieves state-of-the-art results in zero-shot voice style conversion. Experimental evaluations show that StyleStream significantly outperforms previous models in preserving content intelligibility (lowest Word Error Rate) while demonstrating substantially higher similarity to the target speaker's accent and emotion."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a highly original and technically impressive contribution to speech generation research. It introduces StyleStream, the first framework capable of real-time zero-shot voice style conversion that jointly transfers timbre, accent, and emotion. This problem setting is genuinely new: previous real-time systems only handled timbre, while prior style-conversion frameworks were all offline. The proposed architecture achieves this by combining two complementary modules — a destylizer that performs ASR-supervised content–style disentanglement with finite scalar quantization (FSQ), and a stylizer based on a diffusion transformer (DiT) that reintroduces the target style in a streamable manner. The integration of these components demonstrates clear technical originality and conceptual coherence. The use of ASR loss together with a compact FSQ bottleneck represents a clever hybridization of ideas from CosyVoice and SoftVC, and provides an elegant, explainable mechanism for disentangling linguistic content from paralinguistic style.\n\nFrom the standpoint of technical quality, the paper is thorough and well executed. The experimental design is robust: the authors compare against strong recent baselines including Vevo, Vevo 1.5, CosyVoice 2.0, FACodec, and SeedVC, using both objective metrics (WER, speaker/accent/emotion similarity) and subjective MOS/SMOS ratings. The ablation studies are comprehensive and illuminating. For instance, the authors carefully vary the FSQ bottleneck size, the type of content features, and the use of the style embedding, showing quantitatively how each factor influences intelligibility and style fidelity. The methodology for streaming inference is also well justified — chunked-causal masking, causal convolutions, and teacher–student distillation together enable an end-to-end latency around one second without severely compromising perceptual quality. These results, combined with clear and well-documented training configurations, strongly support the claimed contributions.\n\nIn terms of clarity, the paper is mostly well organized and easy to follow. Figures 1–3 effectively illustrate the pipeline, and the motivation for each design choice is well connected to empirical evidence. The authors clearly articulate why continuous pre-quantization features offer a better balance between content preservation and disentanglement, and the discussion in Section 5.3 provides convincing support for this claim. Although the model is complex, the presentation remains coherent and the technical details are traceable."}, "weaknesses": {"value": "Despite its merits, the paper has several areas that could be improved.\n\n1. The most notable limitation is that all experiments are conducted on English-only datasets. Since accent and emotion transfer are highly language-sensitive, the current evidence does not demonstrate whether StyleStream generalizes across languages or linguistic families.\n\n2. The latency–quality trade-off is not analyzed in sufficient depth. The paper reports an average end-to-end latency of roughly 1 s on an NVIDIA A6000 but does not explore how latency scales with chunk size or hardware constraints.\n\n3. The effects of individual streaming modifications (chunked-causal attention, causal convolutions, and the distillation procedure) are not disentangled, making it unclear which component contributes most to the performance gap between the streaming and offline variants."}, "questions": {"value": "1. In Section 3.2, several key symbols in the Diffusion Transformer formulation are introduced without any accompanying explanation, which makes the mathematical description difficult to follow. For instance, the equation “” appears without defining what and denote; the variable “the noisy mel-spectrogram ” is used before introducing how it is generated or how the time parameter relates to the diffusion process\n\n2. The effects of individual streaming modifications (chunked-causal attention, causal convolutions, and the distillation procedure) are not disentangled, making it unclear which component contributes most to the performance gap between the streaming and offline variants. A targeted ablation isolating each factor would make the streaming design easier to evaluate and reproduce.\n\n3. Please quantify how much of the perceptual improvement stems from the vocoder versus the destylizer + stylizer. Run an ablation in which (a) the stylizer outputs are passed to an off-the-shelf vocoder (not the warmed Vocos-based causal model), and (b) an oracle mel input (ground-truth mel) is provided to the vocoder while comparing different stylizers.\n\n4. The paper would benefit from a clearer analysis of the latency–quality relationship. At present, only a single latency figure is reported, and it is difficult to judge how latency and perceptual quality trade off under different configurations or hardware conditions. Even a brief discussion or comparison illustrating how model performance changes with varying processing latency would substantially strengthen the practical relevance of the results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jN4wbUfhi1", "forum": "35ykr3JpbL", "replyto": "35ykr3JpbL", "signatures": ["ICLR.cc/2026/Conference/Submission10033/Reviewer_HLfV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10033/Reviewer_HLfV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975289272, "cdate": 1761975289272, "tmdate": 1762921440668, "mdate": 1762921440668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents StyleStream, a real-time zero-shot voice style conversion system with 1-second end-to-end latency. It consists of a destylizer that removes style attributes while preserving linguistic content and a stylizer based on a diffusion transformer that reconstructs the target style. Content–style disentanglement is achieved through an ASR loss and a finite scalar quantization bottleneck. Trained on 50k hours of English speech, StyleStream achieves state-of-the-art conversion quality and improves accent and emotion similarity over prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: Clear and meaningful problem setup. The paper clearly defines the task of real-time zero-shot voice style conversion, distinguishing it from conventional real-time voice conversion (which mainly handles timbre) and offline style conversion systems such as Vevo and CosyVoice 2.0.\n\nS2:Well-designed experiments that convincingly validate the disentanglement and component choices. The experiments are carefully designed to test whether the destylizer truly separates style from linguistic content. In addition to reporting speaker/accent/emotion classification accuracy as indicators of style leakage, the authors also include ASR WER to measure how much linguistic information is preserved. This is important, since low classification accuracy alone does not guarantee semantic fidelity. The results in Tables 2 and 3 show that StyleStream achieves both strong style removal and low WER, suggesting that its content representation remains intelligible.\nThe ablation studies are also well targeted—they directly test the impact of using pre-quantization continuous features, the size of the FSQ codebook, and key components in the stylizer. Each experiment supports the authors’ design choices.\n\nS3: Balanced performance between quality and real-time efficiency. The system integrates both offline and streaming modes and achieves around 1 s end-to-end latency while maintaining high conversion quality. In Table 1, the offline model outperforms strong baselines (Vevo, CosyVoice 2.0, FACodec) in WER, A-SIM, E-SIM, and MOS for both accent and emotion similarity. The streaming model shows only a modest degradation (A-SIM ≈ 0.635, E-SIM ≈ 0.803), demonstrating that the proposed framework maintains style controllability and intelligibility under real-time constraints."}, "weaknesses": {"value": "W1: The CFM loss in Eq. (3) is consistent with conditional flow matching with an OT path. However, (i) the notation for the OT path $\\psi_t(\\cdot)$ should be made consistent ($x$ vs. $x_0$), and (ii) since the loss is computed only on the masked region, the mask $m$ should ideally appear inside the norm to make the objective fully explicit.\n\nW2: The “real-time” claim is not fully substantiated. The paper mainly achieves streaming inference by applying chunked-causal masking and non-autoregressive decoding, which are well-established techniques in existing real-time voice conversion systems such as DualVC, StreamVoice, and RT-VC. No additional architectural or algorithmic change appears to make the model inherently more real-time beyond this causalization. Moreover, the paper does not provide latency or throughput comparisons with prior real-time VC works. As a result, the “first real-time” contribution seems incremental—StyleStream is essentially a well-engineered streaming adaptation rather than a new approach to real-time generation.\n\nW3: The “real-time” evaluation in Sec. 5.2 is conducted with a 600 ms chunk size on a single NVIDIA A6000 GPU.\nThis configuration corresponds to an update rate of roughly 1.6 Hz, which would already cause perceptible delay in interactive scenarios.\nIn contrast, recent real-time voice conversion systems such as DualVC 2 (Wang et al., 2023) and StreamVoice (Huang et al., 2024) report total latencies of 186 ms and 124 ms, respectively—both substantially lower than StyleStream’s 1 s latency.\nThe evaluation is performed on high-end data-center hardware (NVIDIA A6000, 48 GB VRAM), and the paper does not report results on consumer-grade GPUs or CPU inference, which would be more relevant for deployment.\nThe claimed “real-time” capability therefore reflects an idealized server-side environment rather than a generally efficient or portable implementation."}, "questions": {"value": "Q1: Although Table 2–3 include comparisons with Vevo (VQ-VAE) and CosyVoice 2 (FSQ tokens), the results combine multiple factors—ASR supervision, codebook size, and continuous pre-quantization features—making it unclear whether the improvement is due specifically to FSQ. The paper would benefit from either a controlled comparison (e.g., replacing FSQ with VQ-VAE under the same setup) or a clearer explanation of why FSQ theoretically helps remove style information.\n\nQ2: In Sec. 5.2, the streaming setup assumes a fixed 5-second target utterance and a pre-extracted style embedding, meaning only the source stream is processed online.In many real-time interaction scenarios—such as conversational voice imitation or streaming target input—the target style may not be fully available beforehand.Could the authors clarify whether StyleStream could operate when the reference style segment is shorter, or when the target style is streamed in real time?How would such conditions affect the model’s stability and style-similarity metrics (A-SIM, E-SIM)?\n\nQ3: In Table 1, the paper compares StyleStream with offline voice conversion and style conversion systems such as CosyVoice 2.0, FACodec, Vevo, Vevo 1.5, and SeedVC v24, but not with the real-time VC systems discussed in Sec. 2.3 (e.g., StreamVoice, DualVC 2/3, RT-VC). Even if these models do not perform full “style” conversion, reporting their latency and speaker-similarity metrics would make the claim of being the first real-time voice style conversion system more convincing. Could the authors provide such a comparison or at least discuss how their latency and streaming efficiency compare with prior real-time VC frameworks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QYxF1WT1Bf", "forum": "35ykr3JpbL", "replyto": "35ykr3JpbL", "signatures": ["ICLR.cc/2026/Conference/Submission10033/Reviewer_MYRv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10033/Reviewer_MYRv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984204963, "cdate": 1761984204963, "tmdate": 1762921440030, "mdate": 1762921440030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces StyleStream, a real-time zero-shot voice style conversion system that disentangles linguistic content from style using ASR supervision and a compact FSQ bottleneck, then re-synthesizes timbre, accent, and emotion through a diffusion transformer."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This work proposes a framework for real-time zero-shot voice style conversion, integrating ASR-based content supervision, a compact FSQ bottleneck, and a diffusion transformer stylizer."}, "weaknesses": {"value": "- My primary concern is the lack of novelty and research contribution in the proposed methods. The paper’s main components, the destylizer, FSQ bottleneck, ASR supervision, and DiT stylizer, are all derived from prior works without introducing a truly new conceptual contribution. \n\n- The paper fails to reference several key prior studies that have already explored DiT-based in-context learning for voice conversion and style transfer, which weakens the positioning of the proposed work within existing literature.\n\n- The system represents a well-engineered synthesis of existing techniques but lacks distinct methodological insight or theoretical advancement in disentanglement of speech, style controllability, or generative modeling. The paper focuses primarily on engineering integration and empirical optimization rather than advancing the theoretical understanding of speech disentanglement or style control."}, "questions": {"value": "- What are the model sizes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uaFNCrmZAq", "forum": "35ykr3JpbL", "replyto": "35ykr3JpbL", "signatures": ["ICLR.cc/2026/Conference/Submission10033/Reviewer_UdFS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10033/Reviewer_UdFS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989429351, "cdate": 1761989429351, "tmdate": 1762921439483, "mdate": 1762921439483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a streaming voice conversion system using information bottleneck and ASR supervised learning based encoder, and a diffusion transformer (DiT)-based decoder."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper utilizes a well-known method for speech disentanglement and a conditional flow matching (CFM)-based speech generation using diffusion transformers."}, "weaknesses": {"value": "Overall, my primary concern lies in the lack of novelty. I could not find any clear novel contribution in this paper and it fails to include a sufficient discussion of related works. All the techniques employed are already well-known.\n\n[ASR-based linguistic information retrieval]\n\nThe main contribution claimed by the paper is the destylizer, which is trained using ASR-based text supervision and FSQ-based information bottleneck. However, the tokenizer of CosyVoice was already trained with ASR prediction. While CosyVoice used larger codebook size, the paper only used smaller codebook size for information bottleneck. While the approach is effective for speech disentanglement, it does not introduce any novelty for research paper. \n\nFurthermore, the proposed destylizer is trained only on English-only dataset, resulting in higher performance on English test set. It would be beneficial to include multi-lingual evaluation to assess the generalization performance. \n\n[DiT Decoder]\nThere already exist numerous DiT and CFM-based VC models. However, the paper do not cite or discuss any of them. TBH, I do not want to waste my time to review this paper. Please examine related works carefully. You can find similar works published at ICASSP and Interspeech. \n\n[Streaming Design]\nThe use of causal Layer and ring buffer has already been proposed in previous work [1]. \n\n[1] Yang, Yang, et al. \"Streamvc: Real-time low-latency voice conversion.\" ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024.\n\n[Overclaimed statement]\n\nI strongly disagree with the statement in line 69. The paper only employs a style encoder that do not control the timbre, accent, and emotion separately. If such claims are to be made, please add the experiment of the disentangled control for these attributes (timbre, accent, and emotion). Please avoid overstating the contributions of the paper.\n\nAll models trained on various datasets can jointly mimic these attributes from the reference speech. However, achieving disentangled control over them remains challenging."}, "questions": {"value": "Q1. Please add the sampling steps and sampling methods for CFM decoder. \n\nQ2. Do you really consider your model to be a real-time system with a latency of 1 second?\n\nQ3. Which frame rate did you use for the FSQ tokens? In line 179, it states, \"Unlike CosyVoice2, where a large codebook size (6561) is used, we use a much smaller codebook to enforce a narrower information bottleneck.\" This implies that the paper used 25 Hz for FSQ. However, if the model actually used 50 Hz for FSQ, this statement appears to be an overclaim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZKRGe0lcBh", "forum": "35ykr3JpbL", "replyto": "35ykr3JpbL", "signatures": ["ICLR.cc/2026/Conference/Submission10033/Reviewer_EHyw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10033/Reviewer_EHyw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998904076, "cdate": 1761998904076, "tmdate": 1762921439169, "mdate": 1762921439169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces StyleStream, a framework for real-time zero-shot voice style conversion capable of transforming an input utterance’s timbre, accent, and emotion to match a target speaker’s style, while preserving the original linguistic content. The proposed system combines a destylizer trained with ASR supervision and a compact finite scalar quantization (FSQ) bottleneck to achieve cleaner content–style disentanglement, and a diffusion transformer stylizer for style reconstruction. The system achieves ~1s end-to-end latency, representing a significant advance toward practical real-time style conversion. Experimental results show that StyleStream outperforms previous methods (CosyVoice 2.0, Vevo, SeedVC) across multiple style similarity and MOS metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Real-time voice style conversion is an important and challenging problem that requires transferring not only timbre but also higher-level stylistic cues (accent and emotion). Tackling this in a streaming setting is timely and practically valuable.\n2. Experimental results are strong, and the demo samples sound convincing, with clear style transfer and good intelligibility.\n3. The method design is well thought out: combining ASR loss with a small quantization codebook effectively improves the destylizer’s disentanglement ability. The use of continuous pre-quantization features is a useful empirical trick that improves performance and is well justified by ablation studies."}, "weaknesses": {"value": "1. The main contribution lies in the task and empirical achievement—a functioning real-time voice style conversion system—rather than in methodological novelty. The ASR-supervised tokenizer and DiT-based spectrogram generator follow ideas already seen in recent works (e.g., CosyVoice2, E2-TTS, F5-TTS). Hence, the paper’s conceptual originality for the machine learning community may be somewhat limited, though it is impactful for speech research.\n\n2. A stronger baseline would help: a simple cascaded ASR + zero-shot TTS pipeline could demonstrate more clearly where StyleStream’s advantages lie (e.g., latency, style fidelity).\n\n3. The experimental scope could be expanded. Multilingual and higher-sampling-rate (e.g., 24 kHz) experiments would make the evaluation more solid and test the model’s generalization."}, "questions": {"value": "1. In Table 1, for CosyVoice 2, Vevo, and Vevo 1.5, did the authors use only the flow-matching (FM) component for zero-shot conversion, or the full AR + FM pipeline? My concern is that using only the FM module may limit those baselines to timbre conversion, not full style transfer.\n2. Regarding the streaming design, how exactly is DiT modified to support causal or chunked inference? Is the approach similar to that of CosyVoice 2?\n3. The training data split is somewhat unclear: the destylizer and stylizer use different datasets. Could the destylizer benefit from being trained on the same large-scale dataset as the stylizer (e.g., Emilia)? What are the constraints preventing this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z7T6j7wdDZ", "forum": "35ykr3JpbL", "replyto": "35ykr3JpbL", "signatures": ["ICLR.cc/2026/Conference/Submission10033/Reviewer_FPmA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10033/Reviewer_FPmA"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10033/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762061155524, "cdate": 1762061155524, "tmdate": 1762921438712, "mdate": 1762921438712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}