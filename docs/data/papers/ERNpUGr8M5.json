{"id": "ERNpUGr8M5", "number": 20359, "cdate": 1758305060814, "mdate": 1763612311694, "content": {"title": "Self-Destructive Language Models", "abstract": "Harmful fine-tuning attacks represent a major threat to the security of large language models (LLMs), allowing adversaries to compromise safety guardrails with minimal harmful data. While existing defenses attempt to reinforce LLM alignment, they fail to address models' inherent `trainability' on harmful data, leaving them vulnerable to stronger attacks with increased learning rates or larger harmful datasets. To overcome this limitation, we introduce SEAM, a novel alignment-enhancing defense that transforms LLMs into self-destructive models with intrinsic resilience to misalignment attempts. Specifically, these models retain their capabilities for legitimate tasks while exhibiting substantial performance degradation when fine-tuned on harmful data. The protection is achieved through a novel loss function that couples the optimization trajectories of benign and harmful data, enhanced with adversarial gradient ascent to amplify the self-destructive effect. To enable practical training, we develop an efficient Hessian-free gradient estimate with theoretical error bounds. Extensive evaluation across LLMs and datasets demonstrates that SEAM creates a no-win situation for adversaries: the self-destructive models achieve state-of-the-art robustness against low-intensity attacks and undergo catastrophic performance collapse under high-intensity attacks, rendering them effectively unusable. The code\nis available: https://anonymous.4open.science/r/seam-5C7E (warning: this paper contains potentially harmful content generated by LLMs.)", "tldr": "This work proposes SEAM, a novel alignment enhancing defense that transforms the LLM into a self-destructive model resistant to harmful finetuning attacks.", "keywords": ["Self-destructive Model", "Safety Alignment", "Harmful Fine-tuning Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0edc5c9f3d8abf3ab5cf951fc310ee9035d6d948.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a novel safety finetuning strategy that induces a strong negative coupling between gradients from adversarial examples and gradients from benign samples. The result is a model that largely preserves the harmlessness and helpfulness of the base model and is such that, when adversarial finetuning aimed at stripping safety guardrails is applied, either the model remains safe, or the model is so degraded as to become useless. This is achieved by adding a term to the loss that promotes dissimilarity between gradients from contrastive sets. A hessian-free estimate is proposed for the gradient of this loss to make it practical."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* The proposed method is clever, non-obvious, and proved effective\n* The Hessian-free gradient estimate for $\\mathcal{L}_{SD}$ is a valuable contribution in itself and is likely to find further applications\n* The paper is clearly written and sound.\n* The experiments are sufficiently complete and support the claims.\n* The problem is highly relevant and pressing, especially for the developers of open-weight models."}, "weaknesses": {"value": "* [W1] There is evidence (e.g in the cited [Qi et al. 2023]) that also well-intended fine-tuning leads to catastrophic forgetting of safeguards. The paper does not consider this aspect. While the contributions in the paper are already sufficient for making it worth including at ICLR, it may create a blind spot and a false sense of security unless either there are experiments on the effects of non-adversarial fine-tuning, or this limitations is emphasized upfront."}, "questions": {"value": "* [Q1] What would be the effect of applying to a SEAM model the weight orthogonalization procedure of [Arditi et al. 2024]? Since that method is not based on gradient descent, could it still be effective?\n* [Q2] It would be interesting to see a plot of $\\mathcal{L}_{SD}$: is it driven all the way down to -1 (in the case of cosine similarity)? How rapidly?\n\nSuggestions and comments:\n\nL031-L33: garbled citation text, Latex error.\n\nL157-161: At this point this is redundant.\n\nL195-197: What is exactly the logarithmic transformation, and how is it involved in preventing catastrophic forgetting?\n\nTheorem 1: how tight is the bound? How does one estimate the Lipschitz constant?\n\nL275-276: $\\beta$ is small. What are the variances of the different loss components? They would be useful to understand the real relative importance of the loss terms.\n\nL362-364: There is no model in Figure 3(b) for which the harmfulness may be said to be 'high'.\n\nL378-393: Layout too compressed\n\n[Arditi et al. 2024] Arditi, A., Obeso, O., Syed, A., Paleka, D., Panickssery, N., Gurnee, W. and Nanda, N., 2024. Refusal in language models is mediated by a single direction. Advances in Neural Information Processing Systems, 37, pp.136037-136083."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VaDYaicK5X", "forum": "ERNpUGr8M5", "replyto": "ERNpUGr8M5", "signatures": ["ICLR.cc/2026/Conference/Submission20359/Reviewer_1ay1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20359/Reviewer_1ay1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585576948, "cdate": 1761585576948, "tmdate": 1762933814833, "mdate": 1762933814833, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the vulnerability of Large Language Models (LLMs) to harmful fine-tuning attacks. The authors state that existing defenses do not sufficiently address the models' inherent 'trainability' on harmful data.\n\nThe primary contribution is SEAM, a novel alignment defense that transforms an LLM into a \"self-destructive model\". This model is designed to retain its performance on legitimate (benign) tasks while exhibiting substantial performance degradation when an adversary attempts to fine-tune it on harmful data.\n\nThis self-destructive mechanism is achieved through a new loss function that couples the optimization trajectories of benign and harmful data. Specifically, it encourages their respective gradients to point in opposing directions. This effect is amplified using adversarial gradient ascent.\n\nTo make this optimization computationally practical, the authors also develop an efficient Hessian-free gradient estimate and provide its theoretical error bounds.\n\nEvaluations demonstrate that SEAM models show robustness against low-intensity attacks and undergo significant performance collapse, rendering them unusable, under high-intensity attacks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper's primary strength is its originality. The proposal to counter harmful fine-tuning by making the model \"self-destructive\" is a technically novel concept. This approach moves beyond simply reinforcing existing alignment and instead introduces a mechanism that actively degrades the model's core capabilities when a harmful objective is pursued. This formulation presents a new direction for model defense.\n\nQuality: The paper exhibits high quality in its technical execution. The theoretical background for the proposed self-destructive loss function is well-presented. The authors provide a practical Hessian-free gradient estimation to address the computational challenges of their method, complete with theoretical error bounds, which demonstrates technical depth. The empirical evaluation is extensive, testing the defense against various attack intensities and methods (SFT, PEFT) and including relevant ablation studies to validate the components of the loss function.\n\nClarity: The paper is clearly written. The problem of harmful fine-tuning and the limitations of existing defenses are well-defined. The proposed solution, SEAM, is explained in a structured manner, and the core concept of self-destruction is illustrated effectively (e.g., in Figure 1).\n\nSignificance: The work is significant as it addresses a critical and persistent vulnerability in large language models. By proposing a defense that creates a \"no-win situation\" for adversaries—where attacks either fail to compromise safety or destroy the model's general utility—the paper offers a promising alternative to current alignment techniques that remain vulnerable to more intensive attacks."}, "weaknesses": {"value": "A primary weakness concerns the practical applicability of the method, particularly regarding hyperparameter tuning. The paper relies on grid search to find optimal values for key hyperparameters such as alpha, beta, and epsilon. This approach was feasible for the experimental setup, which used relatively small-scale models (in the 3B to 8B parameter range) and datasets. However, this tuning method does not scale efficiently to large, production-grade foundation models, where the computational cost of repeated grid search would be prohibitive. The paper would be significantly strengthened by providing a more detailed sensitivity analysis for these parameters or, ideally, discussing a more principled or efficient method for setting them, which would be crucial for real-world adoption.\n\nFurthermore, the main evaluation framework for attack intensity is a limitation. The study largely equates \"low-intensity\" and \"high-intensity\" attacks with the learning rate used during the harmful fine-tuning process. This assumption about adversarial strategy is somewhat naive. A sophisticated adversary has more variables to control. For instance, an attacker could use a low or moderate learning rate over a significantly longer number of training steps to gradually compromise the model. This might allow them to find a path that bypasses the sharp self-destruction mechanism that the paper demonstrates is triggered by high learning rates. The current evaluation does not fully explore this trade-off between learning rate and the duration of the attack."}, "questions": {"value": "1.  The paper mentions that grid search was used to find the optimal hyperparameters (e.g., alpha, beta, epsilon). For clarity and to aid in reproducibility, it would be helpful if the authors could provide the specific ranges and values tested for each hyperparameter during this grid search.\n\n2.  The default harmful fine-tuning attack is set at 1K harmful samples and 250 training steps. Could the authors provide more justification for this setting as an adequate representation of a typical attack? We are particularly interested in the authors' perspective on what might happen if the attack was more intensive in terms of data volume or duration, rather than just learning rate. For example, what effect would be expected if an adversary used 10K samples and trained for 2,500 steps? Would the self-destructive mechanism still be triggered as effectively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pjTmc5njvG", "forum": "ERNpUGr8M5", "replyto": "ERNpUGr8M5", "signatures": ["ICLR.cc/2026/Conference/Submission20359/Reviewer_b8Na"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20359/Reviewer_b8Na"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887216415, "cdate": 1761887216415, "tmdate": 1762933814122, "mdate": 1762933814122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SEAM, an alignment-enhancing defense against harmful fine-tuning. Unlike prior defenses, SEAM introduces a loss function that couples the optimization trajectories of benign and harmful data. Its main contribution is an efficient, Hessian-free gradient-based optimization with a theoretical guarantee."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces SEAM, which creates an optimization trap by coupling benign and harmful optimization trajectories\n2. The paper validates SEAM's effectiveness through extensive evaluation across a diverse range of LLMs and various attack configurations, demonstrating SEAM's robust resistance to low-intensity attacks and self-destruction under high-intensity attacks\n3. The mechanism is validated through PCA visualization of gradients, confirming that SEAM successfully forces benign and adversarial gradients into opposite directions. The theoretical proof is also provided."}, "weaknesses": {"value": "1.  The success of the gradient coupling mechanism critically depends on the adversarial dataset and the benign dataset, and to ensure consistently opposing gradients. Therefore, the robustness highly depends on the quality and representativeness of these datasets, rather than their quantity (Fig.3). As the authors also knowledge, there is a need to \"explore identifying or generating optimal benign datasets that maximize the self-destructive effect\".\n2. The core success of SEAM is turning a strong harmful fine-tuning attack (aiming for a jailbroken model) into model collapse (aiming for destruction). However, if the attacker's primary objective is to disrupt model availability or utility (not jailbreak), the self-destruction mechanism, which causes performance drop, actually helps in achieving this attacking goal. This suggests the defense effectively shifts the attack outcome rather than neutralizing all adversarial objectives. \n3. The defense method against attacks incorporating benign data mixing, benign task regularizers, and random gradient perturbations is tested. However, other strategies, such as fine-tuning only non-critical layers, using a covert target/loss to avoid the self-destruction trigger condition, are not extensively evaluated.\n4. Some writing and presentation issues. For example, there is a missing full stop at the end of the first paragraph, and some references appear incorrectly formatted in the first sentence."}, "questions": {"value": "1. How does SEAM perform under stronger or more adaptive attack settings (such as training with extremely low learning rates or fine-tuning only specific LoRA layers)? Can the authors provide empirical evidence that SEAM remains effective in these challenging cases?\n2. More detailed analysis of the attacking samples is needed, not only in terms of their quantity, but also their characteristics compared to normal samples. For example, examining their distributional differences could provide deeper insight into how SEAM responds to harmful data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X7lGLVqjzb", "forum": "ERNpUGr8M5", "replyto": "ERNpUGr8M5", "signatures": ["ICLR.cc/2026/Conference/Submission20359/Reviewer_tToc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20359/Reviewer_tToc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888182097, "cdate": 1761888182097, "tmdate": 1762933813353, "mdate": 1762933813353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a defence method against harmful fine-tuning attacks. The proposed method SEAM puts the model in a peculiar zone in the parameter space where any attempt of harmful fine-tuning will result in partial or total degradation of the model’s benign capabilities, therefore making the attack useless. The method achieves that by optimizing a self-distruction loss which tries to minimize the alignment between the potential gradients of harmful and benign fine-tuning. The experimental results show that the method has superior defence capabilities compared to previous works when evaluated on a wide range of harmful fine-tuning settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper approaches an interesting and practical issue: harmful fine-tuning of LLMs. The proposed solution SEAM solves this problem in a clever and original way: by self-destructing the model’s capabilities when harmful fine-tuning is attempted.\n\nThe paper is well structured and easy to read. The main idea is clearly explained and the presentation intuitively builds the solution. \n\nThe Hessian-free gradient estimate makes the optimization problem tractable and the method’s costs comparable to other related methods tackling the same problem.\n\nThe paper presents thorough experiments on multiple settings and extensive comparisons with related works. The additional experiments, ablations, visualizations and insights are also interesting and valuable."}, "weaknesses": {"value": "**Limited fine-tuning evidence**\n\nThe paper claims that SEAM does not interfere with a model's ability to be fine-tuned on benign datasets, but there’s not a lot of evidence in this regard. As I understand, Table 1 reports model performance after fine-tuning on 4 tasks in the FS column, but these tasks are not very diverse and are likely aligned with the model’s base training, so they might not be very challenging to begin with. I would like to see some experiments with fine-tuning on more diverse / OOD benign datasets. (See also Question 2.)\n\nI also feel that the structure of Table 1 is not obvious on the first read, maybe the caption should be a bit more explicit. I think you should add at least something like: “... zero-shot (ZS) and fine-tuning (FS) capabilities …”, but a more detailed caption or even a separate table if you decide to add some more datasets would be beneficial.\n\n**Limited Utility Results**\n\nTable 1 presents accuracy results for SEAM-trained models on MCQ tasks. While useful for evaluating any performance degradation, MCQ tasks are rather limited. For a more thorough evaluation, the paper should include results on some open-ended text generation tasks, like MT-Bench [1]. \n\nI would also be interested to see the model’s results on XSTest [2] - a dataset specifically designed to assess the models’ tendency to be overly cautious when answering benign questions formulated to closely resemble harmful ones. While SEAM does not directly do refusal training, I would like to see if it interferes with the model's ability to properly answer XSTest questions. \n\n**Difficulties to implement in practice**\n\nWhile the idea is interesting and effective, and the costs might be negligible compared to pre-training, it is very unlikely that any open-source models will be released with SEAM safe-guards, because companies would not risk their models collapsing when users try to fine-tune them. While I don’t see it as a weakness of the paper, I would like to hear the authors’ opinion on this issue.\n\n**Minor**\n\nThe first sentence seems to have a wrong \\citep command which does not render the citations properly.\n\n**References**\n\n[1] https://arxiv.org/abs/2306.05685\n\n[2] https://arxiv.org/abs/2308.01263"}, "questions": {"value": "1. Would it be possible to undo the SEAM training by training to maximize the self-destruction loss (essentially training with negative beta)?\n2. Could SEAM trigger self-destruction when finetuned on some out-of-distribution domain-specific datasets that might come around as potentially not safe? I am thinking of medical datasets where medical advice could be classified by some models as harmful, or science datasets which could contain descriptions of dangerous chemicals / details about how atomic bombs work etc.\n3. Is it possible that SEAM-trained models might be vulnerable to poison attacks? I think that including a few harmful (or even specifically crafted) examples in a benign dataset could cause the model to collapse. Would it be possible to actually identify such samples from their gradient information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BpQyuJSbQf", "forum": "ERNpUGr8M5", "replyto": "ERNpUGr8M5", "signatures": ["ICLR.cc/2026/Conference/Submission20359/Reviewer_Y52o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20359/Reviewer_Y52o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20359/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994390442, "cdate": 1761994390442, "tmdate": 1762933812409, "mdate": 1762933812409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}