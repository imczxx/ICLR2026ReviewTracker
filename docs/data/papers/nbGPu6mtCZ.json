{"id": "nbGPu6mtCZ", "number": 14442, "cdate": 1758235476873, "mdate": 1759897369811, "content": {"title": "The Extra Token Matters: Disentangled Representation Learning with Vision Transformers", "abstract": "Inspired by Darcet et al. (2024)  where extra tokens (or registers) are introduced to offset the artifacts in feature maps due to high-norm tokens, this paper presses further and asks a more challenging question: Can we find a suitable regularization term such that the extra tokens can evolve into disentangled representations, capable of attending to finer details of objects (e.g., parts)? We propose XTRA, an intuitive yet powerful framework that augments Vision Transformers with dedicated ``factor tokens'' and enforces disentanglement via a novel Minimum Volume Constraint (MVC). A multi-stage aggregation process further confines these factor tokens into semantically pure components, especially when the amount of hyperparameters is large. On ImageNet-1K, XTRA boosts KNN accuracy by 5.8\\% and linear-probe accuracy by 2.3\\% over leading self-supervised learning (SSL) baselines, outperforming even models trained on larger datasets.", "tldr": "", "keywords": ["representation learning", "self supervised learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d19f25cae673853e7fdc5f1352eb45f1e1806659.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework that regularizes extra tokens in Vision Transformer for learning disentangled representation. Specifically, the authors introduce a Minimum Volume Constraint (MVC) to encourage each patch token to be expressed as a linear combination of a few factor tokens, and design a multi-stage aggregation module to explicitly promote diversity among factor tokens by making each encode distinct information. Evaluations on ImageNet-1K using linear probing, kNN accuracy, and SEPIN@k demonstrate that the proposed method outperforms recent self-supervised learning frameworks in both representation quality and disentanglement."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The idea of incorporating extra tokens for disentangled representation learning seems interesting. \n- Empirically, the method shows consistent gains on ImageNet-1K (linear probing and kNN accuracy) over recent self-supervised learning baselines."}, "weaknesses": {"value": "**Presentation**\n- The main claim and the empirical evidence are not closely aligned. The paper frames its primary objective as disentangled representation learning (as in title, abstract, and introduction), yet the main experimental results emphasizes generic representation quality on ImageNet-1K (linear probe and kNN accuracy). These metrics do not directly validate disentanglement of the proposed factor tokens, making it unclear whether the proposed framework actually induces disentanglement of the representations.\nFor example, the main result appears to be Section 4.1 (as the abstract highlights this result), but the evaluations there primarily measure XTRA’s representation quality on ImageNet-1K (e.g., linear probing or kNN accuracy) rather than the disentanglement of the learned representation. Moreover, the ablation studies also focus on representation quality, providing no information about which component influences disentanglement (which seems to be the main interest of this paper, if I understand correctly).\n\n**Clarity** \n- The clarity of the paper could be improved. Many details of the main method are missing from the main text. Although several are provided in the Appendix, these contents should be included in the main paper to make it self-contained. Please refer to the questions below.\n\n**Novelty**\n- The proposed multi-stage aggregation appears very similar to GroupViT [1]. If the authors are aware of this work and intend to claim novelty for this component, the differences from GroupViT should be clearly presented in the paper.\n\n**Reference**\n\n[1] Xu et al., GroupViT: Semantic Segmentation Emerges from Text Supervision, in CVPR 2022."}, "questions": {"value": "**Questions**\n- Since the definition of “disentanglement” in Equation (1) differs from common definitions in the disentangled representation learning the literature [2,3,4,5],  it would improve the clarity if the authors provide more justifications and explanation regarding this definition. For example, what kinds of factors should $\\mathbf{f}_i$ encode under this definition? Does it have to encode ground-truth generative factors (i.e., GT factors should be identifiable)? Does the Equation (1) allow each factor $\\mathbf{f}_i$ to be not independent of other factors, which would deviate from standard uses of “disentanglement”?\n - L359-360 states that SEPIN@k measures how each token $\\{\\mathbf{p}_i\\}$ is disentangled from other tokens. However, based on my understanding of Equation (1), aren’t the factor tokens (sort of base features) the ones intended to be disentangled, rather than $\\{\\mathbf{p}_i\\}$? There seems to be no reason for $\\{\\mathbf{p}_i\\}$ being disentangled from each other. They just share the same basis rather than being disentangled. \n\n**Suggestions regarding the clarity** \n- Section 3.2 is difficult to follow at first read. In my understanding, training with Equation (3) alone is not sufficient for representation learning, and the authors conjecture that this is because multiple tokens encode the same information rather than distinct information. Therefore, the authors propose aggregation stages to explicitly constrain each token to encode different information. It would improve the clarity if the transition between first and second paragraphs of Section 3.2 is more clearly written and provide the empirical evidence supporting the hypothesis of token redundancy and how the aggregation module mitigates it. \n- In L270, reference for “self-knowledge distillation framework.” is missing.\n- In L275, although the equation for \\mathcal{L}_{distill} is presented in the appendix, including the equation in the main paper would make paper self-contained. \n- What are the differences of Figure 1, 8, 9 from the ViT-register [1]? Aren’t those semantic attention of extra tokens already presented in ViT-register? The distinction from ViT-register is unclear.\n\n**Minor Fixes**\n- Typos in L65 : remove the duplicated “also”, and “teh” -> “teh” \n- Equation (13), (14) : $i$ should be changed to $j$ in the denominator. \n\n**Reference**\n\n[1] Xu et al., GroupViT: Semantic Segmentation Emerges from Text Supervision, in CVPR 2022. \n\n[2] Bengio et al.,  Representation learning: A review and new perspective, in TPAMI 2013. \n\n[3] Higgins et al., Towards a definition of disentangled representations., arXiv preprint arXiv:1812.02230 (2018)\n\n[4] Roth et al., Disentanglement of Correlated Factors via Hausdorff Factorized Support, in ICLR 2023. \n\n[5] Mita et al., An Identifiable Double VAE For Disentangled Representations, in ICML 21."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oFaMgpy6NS", "forum": "nbGPu6mtCZ", "replyto": "nbGPu6mtCZ", "signatures": ["ICLR.cc/2026/Conference/Submission14442/Reviewer_B7wW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14442/Reviewer_B7wW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568210855, "cdate": 1761568210855, "tmdate": 1762924847394, "mdate": 1762924847394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes XTRA, a framework for learning disentangled representations in Vision Transformers by introducing regularized factor tokens. The key innovation is adapting the minimum volume constraint (MVC) from spectral unmixing, combined with a multi-stage aggregation mechanism, to enable factor tokens to attend to fine-grained object parts. Experiments on ImageNet-1K demonstrate improvements in representation quality and disentanglement metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper has several notable strengths:\n\n1. The motivation is clear and the cross-domain inspiration is interesting. Borrowing the volume constraint idea from spectral unmixing and applying it to visual representation learning is a creative connection, even if the analogy is not perfectly rigorous. It provides a reasonable technical direction worth exploring.\n\n2. The method design is relatively complete. The combination of MVC, multi-stage aggregation, and knowledge distillation addresses practical training challenges. Figure 6b effectively demonstrates that multi-stage aggregation is necessary for convergence when training deep networks, which is a useful practical contribution.\n\n3. The experiments are fairly comprehensive. The authors achieve strong results on ImageNet-1K alone, outperforming models pretrained on much larger datasets, which is impressive. The ablation study (Table 5) is thorough and clearly shows that the volume penalty is the critical component. The quantitative evaluation of disentanglement (SEPIN@k) combined with visualizations (Figures 1, 8-9) provides multi-faceted evidence. The generalization to downstream tasks is also well validated.\n\nFinally, interpretability is a highlight. The attention maps of factor tokens show reasonably clear decomposition of object parts (e.g., heads, bodies, legs), which helps understand what the model has learned."}, "weaknesses": {"value": "### Major Issues\n\n**1.** The core assumption is that minimizing ∣∣FTF−I∣∣F2​ leads to semantic disentanglement, but this connection lacks sufficient theoretical justification. Orthogonality ensures independence of representations, but does not equal semantic disentanglement. Why would orthogonal factor tokens automatically correspond to meaningful object parts (heads, legs, tails) rather than arbitrary orthogonal decompositions? The paper provides no theoretical analysis or toy examples to clarify how MVC guides semantically meaningful decomposition. The relationship to other disentanglement methods (e.g., mutual information-based approaches) is also unexplored.\n\n**2.**  While the paper shows nice visualizations of disentangled representations, the actual benefits of disentanglement remain unclear. Beyond improving interpretability, Tables 1-3 mainly reflect overall improvement in representation quality, without specifically demonstrating the value of \"disentanglement\" as a property. For instance, are there downstream tasks that explicitly require part-level information? Do disentangled representations show significant advantages on such tasks? Currently, the experiments mostly prove that the entire XTRA framework works, but the unique contribution of disentanglement is not sufficiently highlighted. It would be helpful to see experiments specifically designed to validate the value of the disentanglement property rather than just overall representation quality.\n\n**3.** XTRA introduces many hyperparameters: three loss weights (λdistill​,λfactor​,λvolume​), number of aggregation stages, number of tokens per stage, etc. Table 6 shows the authors chose [1,0.45,0.05] for the weights, but doesn't explain how these were determined or provide sensitivity analysis. Do these hyperparameters need retuning for different datasets or tasks? If so, the generalizability and practicality of the method are questionable. Additionally, Figure 6b shows that 0 aggregation stages completely fails (13.9% KNN), but the paper's explanation is insufficient—is this because MVC itself fails in high-dimensional space, or is it an optimization issue?\n\n**4.** The main selling point is that training on ImageNet-1K alone outperforms models pretrained on LVD-142M. However, XTRA actually uses a pretrained DINOv2 as the teacher, which means it still leverages knowledge from large-scale datasets, just transferred through distillation. Table 2 attempts to show \"without pre-trained teacher\" results, but performance drops significantly (KNN from 84.2% to 81.9%), indicating the pretrained teacher contributes substantially. Compared to truly from-scratch methods (e.g., DINO v1), how much of XTRA's advantage comes from the method itself versus the strong teacher? This question is not adequately addressed.\n\n### Minor Issues\n\n**5.** The 32→16→8 aggregation path and aggregating every 4 blocks appear to be based on heuristics rather than principled design. The paper doesn't systematically explore different aggregation strategies. Additionally, the comparison between hard assignment (Gumbel-Softmax + one-hot) versus soft assignment is missing, making it unclear whether the one-hot constraint is necessary.\n\n**6.** Figures 8-9 primarily show success cases, with insufficient analysis of failure cases. The airplane and ambulance in Figure 9 seem unable to decompose into meaningful parts, but the paper simply mentions they are \"difficult to disentangle\" without deeper analysis of why. Moreover, the semantic mapping of factor tokens requires manual inspection, lacking automated evaluation methods, which limits the possibility of large-scale verification.\n\n**7.** While the geometric interpretation in Figure 2a (internal vs external forces) is intuitive, it's only a 2D illustration—the actual high-dimensional case may be much more complex. The logical flow in the method section (3.1-3.3) is somewhat disjointed, particularly the introduction of multi-stage aggregation in 3.2 feels abrupt—why is this mechanism suddenly needed? Was it discovered experimentally that MVC alone doesn't work? Additionally, notation usage is inconsistent (e.g., {fi​} sometimes with superscripts, sometimes without)."}, "questions": {"value": "Can you provide theoretical analysis or proof for why MVC leads to semantic disentanglement? Are there toy examples (e.g., simple synthetic data) that demonstrate its working mechanism?\n\nHow were the three loss weights selected? Do they need retuning for different datasets? Can you provide hyperparameter sensitivity analysis?\n\nIf comparing DINOv2 (trained from scratch) + XTRA versus DINOv2 (pretrained) + XTRA, what is the specific performance gap? How can you quantify the contribution of the pretrained teacher?\n\nRegarding aggregation design: Why does 0 aggregation stages fail completely? Can you use a data-driven approach to automatically determine the number of aggregation stages and tokens per stage? \n\nBeyond interpretability, what practical tasks benefit uniquely from disentangled representations? Can you design experiments specifically validating the value of the \"disentanglement\" property itself?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IVJLjl0SvX", "forum": "nbGPu6mtCZ", "replyto": "nbGPu6mtCZ", "signatures": ["ICLR.cc/2026/Conference/Submission14442/Reviewer_kDao"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14442/Reviewer_kDao"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726181821, "cdate": 1761726181821, "tmdate": 1762924847009, "mdate": 1762924847009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds upon prior work, which showed that register tokens in ViTs can often implicitly yield disentangled representations, by introducing a regularizer which aims to explicitly yield disentangled representations. This regularizer, referred to as minimum volume constraint,  essentially estimates a linear latent variable model along with additional penalties. Adding the regularizer to pre-trained DINO models, as well as models trained from scratch, yields improvements in disentanglement relative to ViTs with registers along with improvements in linear and KNN readout accuracy on ImageNet-1k."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses important problem. Namely, understanding how to go from patch based features to more abstract disentangled features in large scale vision models.\n \n* The paper is well written and is relatively straightforward to understand.\n\n* The three part loss in Section 3 is interesting and is well motivated with descriptive figures.\n\n* The empirical study is thorough, with interesting quantitative and qualitative results. Namely, the disentangled heat maps are interesting and the gains over baselines methods in disentanglement and linear probing seem promising."}, "weaknesses": {"value": "### **Main weakness**\n\nMy main issue with this work pertains to the novelty of the proposed MVC method in Section 3.1. The authors method involves a few loss terms. The first term L_factor is essentially a reconstruction loss on the patch features with a linear encoder/decoder. This idea is very similar to the DINOSAUR loss proposed by [1] in which patch features are reconstructed in order to achieve object disentanglement but using more flexible, neural network based encoders and decoders.\n\nThe second term in the loss, L_volume, drives the encoder/decoder map to be orthogonal and acts as an information constraint. In [2] Appendix I, a KL term was added to the DINOSAUR loss to achieve superior object disentanglement, yielding a VAE loss. This KL term serves a very similar function to L_volume in restricting the information of the latents. Indeed, it has been shown theoretically that the VAE loss enforces orthogonality of the learned decoder [3]. \n\nA difference worth noting is that the authors have some experiments training models from scratch in Section 4, while [1,2] use pre-trained encoders (though see [4] for a similar disentanglement method to DINOSAUR which trains from scratch). With this being said, however, my current view is that the MVC based method proposed by the authors is a less flexible variant of methods used in prior works [1, 2]. Thus, I would appreciate clarification in the novelty of this work relative to these works in order to better understand the contribution. For the time being, however, I would not recommend acceptance given my present concerns.\n\n### **Minor Issues**\n\n* There is a small typo in line 75, “teh”, should be “the”.\n\n* In line 131, it is implied that “disentangling shape and texture” is the focus of object-centric learning. My understanding is that object-centric learning instead focuses on disentangling each object in a scene.\n\n* In line 132, it is stated that “ to the best of our knowledge, no research has addressed the explicit disentanglement in self-supervised learning”. This is not an accurate statement as I understand it as many theoretical works, e.g., [5, 6], have specifically studied conditions under which disentanglement is possible in self-supervised learning.\n\n* In line 202, it is stated that “empirical studies showed that the MVC regularization is effective when only one block of the student network is trained…” Is there a reference or something to back up this statement?\n\n\n**References**\n\n\n[1] Seitzer et. al, 2022 Bridging the Gap to Real-World Object-Centric Learning\n\n[2] Brady et. al, 2024 https://openreview.net/forum?id=cCl10IU836\n\n[3] Reizinger et. al, 2022, Embrace the Gap: VAEs Perform Independent Mechanism Analysis\n\n[4] Dukic et. al, 2025, Object-Centric Pretraining via Target Encoder Bootstrapping\n\n[5] Zimmermann et. al 2021, Contrastive Learning Inverts the Data Generating Process\n\n[6] Reizinger et. al, 2024, Cross-entropy is all you need to invert the data generating process"}, "questions": {"value": "* Can the authors comment on the novelty of their method relative to the aforementioned works in [1, 2].\n\n* What factors of variation are the authors interested in disentangling? Objects, parts of objects, or both?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PB2MHUNZTL", "forum": "nbGPu6mtCZ", "replyto": "nbGPu6mtCZ", "signatures": ["ICLR.cc/2026/Conference/Submission14442/Reviewer_bY9Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14442/Reviewer_bY9Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962370505, "cdate": 1761962370505, "tmdate": 1762924846217, "mdate": 1762924846217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to achieve disentangled representation learning in VITs using the proposed method, XTRA, which augments ViTs with learnable factor tokens and uses Minimum Volume Constraint and a multi-stage aggregation mechanism to enforce disentanglement. On ImageNet-1K, XTRA outperforms leading self-supervised baselines, improving KNN accuracy by 5.8% and linear-probe accuracy by 2.3%."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The geometric interpretation provided in page 4 is excellent and made a big difference to my understanding of the method. Generally, various aspects of the method have strong principle justifications, such as the $J(F)$ encouraging orthogonality of $F$. \n2. While a lot of disentangled representation learning work is done on small models in almost toys settings, this paper conducts experiments on a ViT pretrained on ImageNet1K without labels suggesting scalability. The paper compares against DINO models. \n3. The change in disentanglement across aggregation stages is a neat evaluation and the monotonic improvement is a strong result. Furthermore, the simultaneous improvement in disentanglement (using SEPIN) and performance on downstream tasks is a practically useful result."}, "weaknesses": {"value": "1. Clarity is a serious weakness in my opinion. Especially the second paragraph of the introduction was both dense and incomprehensible because of the lack of detail and rigor.\n2. Kind of related to (1), I think the motivation for borrowing the techniques from remote sensing and spectral unmixing is not well-explained. Disentangled representation learning attracts a fairly wide audience and it might not be a good idea to assume familiarity with these specific topics. \n3. Table 1, 2, 3 and 4 require variance reporting across seeds."}, "questions": {"value": "1. I am curious as to whether there are cases where the linear mixing model assumption is harmful for disentangled representation learning. It seems like a fairly strong assumption, and it would be good to discuss cases (even if they are extreme or unlikely) where this might be an issue. \n2. How computationally expensive is the SVD computation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5ATBKCjlTA", "forum": "nbGPu6mtCZ", "replyto": "nbGPu6mtCZ", "signatures": ["ICLR.cc/2026/Conference/Submission14442/Reviewer_vH77"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14442/Reviewer_vH77"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14442/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985411389, "cdate": 1761985411389, "tmdate": 1762924845877, "mdate": 1762924845877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}