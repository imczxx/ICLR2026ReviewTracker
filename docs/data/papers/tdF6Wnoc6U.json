{"id": "tdF6Wnoc6U", "number": 8377, "cdate": 1758080574387, "mdate": 1762933837131, "content": {"title": "Enhancing Adversarial Robustness of Vision Language Models via Adversarial Mixture Prompt Tuning", "abstract": "Large pre-trained Vision Language Models (VLMs) demonstrate excellent generalization capabilities but remain highly susceptible to adversarial examples, posing potential security risks. To improve the robustness of VLMs against adversarial examples, adversarial prompt tuning methods are proposed to align the text feature with the adversarial image feature without changing model parameters. However, when facing various adversarial attacks, a single learnable text prompt has insufficient generalization to align well with all adversarial image features, which ultimately results in overfitting. To address the above challenge, in this paper, we empirically find that increasing the number of learned prompts yields greater robustness improvements than simply extending the length of a single prompt. Building on this observation, we propose an adversarial tuning method named Adversarial Mixture Prompt Tuning (AMPT) to enhance the generalization against various adversarial attacks for VLMs. AMPT aims to learn mixture text prompts to obtain more robust text features. To further enhance the adaptability, we propose a conditional weight router based on the input adversarial image to predict the mixture weights of multiple learned prompts, which helps obtain sample-specific mixture text features aligning with different adversarial image features. Extensive experiments across 11 datasets under different settings show that our method can achieve better adversarial robustness than state-of-the-art approaches.", "tldr": "", "keywords": ["Vision Language  Models", "Adversarial Robustness", "Adversarial Mixture Prompt Tuning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/c478870478d2a2b7fb71af04b9b4e5d15672a0dc.pdf", "supplementary_material": "/attachment/86d01629252daae78ad7a6e8edce9b9782bdf8e0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Adversarial Mixture Prompt Tuning (AMPT), a parameter-efficient method to enhance the adversarial robustness of Vision-Language Models (VLMs), such as CLIP. Unlike prior adversarial prompt tuning (APT) approaches that optimize a single learnable text prompt, AMPT introduces multiple learnable prompts and a conditional weight router that dynamically combines them based on the adversarial image features. Experiments on 11 datasets under PGD and AutoAttack show improved robustness and generalization over baselines including APT, AdvPT, and FAP."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using multiple learnable prompts and dynamically weighting them via an adaptive router is both intuitive and technically sound.\n2. The effectiveness of the conditional weight router is theoretically verified, providing a solid justification that adaptive weighting can reduce the expected adversarial error.\n3. Experimental results demonstrate consistent and substantial improvements of AMPT over state-of-the-art baselines such as APT and AdvPT in both clean and robust accuracy across almost all datasets, demonstrating the method's effectiveness.\n4. The paper is well-organized and easy to follow."}, "weaknesses": {"value": "1. The experiments are not sufficient to fully demonstrate the effectiveness of AMPT:\n-  In Section 3.2, the authors show that increasing the number of prompts can further improve robustness compared with increasing the prompt length. However, to more convincingly support this claim, Table 1 should include comparison results with APT under the same total number of prompt parameters, ensuring a fair comparison between the two settings.\n- While APT, AdvPT, and FAP are included as baselines, the paper lacks comparisons with Test-Time Adversarial Prompt Tuning (TAPT) (Wang et al., 2025) under the same experimental setup. Including TAPT would provide a more comprehensive evaluation against recent state-of-the-art methods.\n- The proposed method relies on a pre-trained VLM backbone. However, the paper does not explore how AMPT’s performance varies or degrades when different backbone models are used, which limits understanding of its robustness transferability and general applicability across architectures.\n-  Although the generalization across different datasets are evaluated, the paper does not include a distribution shift test (as conducted in Section 5.2 of APT), which is important for assessing the model’s robustness to domain or input distribution variations.\n- Finally, the paper lacks quantitative illustrations or concrete examples of the learned prompts. It remains unclear what specific prompts are obtained through AMPT and how they differ from those generated by baseline methods such as APT. Including representative prompt examples or visualizations would enhance interpretability and transparency.\n2. Significant computational overhead: As shown in Table 10, the training cost of AMPT remains considerably higher than that of APT, raising concerns about its practicality for large-scale or real-time applications."}, "questions": {"value": "1. Theorem 1 (Sec. 4.3) claims that the conditional prompt weight router reduces the expected error risk compared to uniform weighting. However, the proof in Appendix A.1 assumes that the risk of each prompt is static and independent of the weight optimization process. In AMPT’s joint training setting, the prompt embeddings and the conditional weights are optimized simultaneously, meaning that each prompt’s risk may depend on the weight updates. Could the authors clarify whether the theoretical guarantee of Theorem 1 still applies under joint training, or is it only valid in the fixed-prompt setting? \n2. How sensitive is AMPT to the number of mixture prompts (K) across different datasets? Would adaptive K selection improve further?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "bLuvJhpmCA", "forum": "tdF6Wnoc6U", "replyto": "tdF6Wnoc6U", "signatures": ["ICLR.cc/2026/Conference/Submission8377/Reviewer_AQpm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8377/Reviewer_AQpm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472572184, "cdate": 1761472572184, "tmdate": 1762920284465, "mdate": 1762920284465, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "TGV0leB2nb", "forum": "tdF6Wnoc6U", "replyto": "tdF6Wnoc6U", "signatures": ["ICLR.cc/2026/Conference/Submission8377/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8377/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762933835105, "cdate": 1762933835105, "tmdate": 1762933835105, "mdate": 1762933835105, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the adversarial robustness problem of Vision-Language Models via adversarial prompt tuning. The authors empirically find that, rather than using a single learnable text prompt, training multiple short prompts and mixing them achieves stronger generalization. Based on this observation, the paper proposes Adversarial Mixture Prompt Tuning (AMPT), which learns mixture text prompts with a routing module that adaptively weights the text prompts. Experiments using CLIP ViT-B/32 on 11 image classification datasets show that the proposed method outperforms existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Combining short adversarial prompts with a routing module is novel yet simple idea.\n- Empirical validation across multiple datasets confirms consistent improvement compared to baselines."}, "weaknesses": {"value": "- The experiments are limited to the CLIP ViT-B/32 model. This is a considerable weakness that limits the generalizability of the claim.\n- The routing module introduces additional parameters, so part of the performance improvement may simply stem from the increased model capacity.\n- It appears that a long single text prompt is not suitable for a relatively weak model like CLIP ViT-B/32, which is known to struggle with long text sequences. [Zhang+2024] have shown that CLIP’s performance saturates when the input length exceeds around 20 tokens, indicating that using a 64-token prompt is simply beyond the effective capacity of such a small model. The authors should conduct additional experiments with CLIP ViT-L/14, Long-CLIP, or other recent VLMs that better handle extended text inputs.\n- Considering that the number of parameters and training cost are larger than in APT, the observed improvement of only 0–1% appears relatively minor.\n\n\n[Zhang+2024] Zhang, Beichen, et al. \"Long-clip: Unlocking the long-text capability of clip.\" European conference on computer vision. Cham: Springer Nature Switzerland, 2024."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a0CQVVXRy0", "forum": "tdF6Wnoc6U", "replyto": "tdF6Wnoc6U", "signatures": ["ICLR.cc/2026/Conference/Submission8377/Reviewer_Bv1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8377/Reviewer_Bv1L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660855170, "cdate": 1761660855170, "tmdate": 1762920283937, "mdate": 1762920283937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Adversarial Mixture Prompt Tuning (AMPT) to enhance adversarial robustness of Vision-Language Models (VLMs). The authors first observe that using multiple short prompts outperforms a single long prompt (with equal total parameters) for adversarial robustness. Building on this, AMPT learns K adversarial text prompts and uses a conditional router network to predict sample-specific mixture weights based on adversarial image features. The method is evaluated on 11 datasets against PGD and AutoAttack, showing improvements over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": ". Finding that multiple short prompts outperform a single long prompt with the same parameter count is an interesting core observation.\n\n. The comprehensive experimental scope that covers 11 datasets demonstrates that the method works across different scenarios.\n\n. While some gains are modest, they are consistently positive, suggesting the approach has merit.\n\n. The method is practical for deployment scenarios where computational resources are limited.\n\n. Unlike many adversarial training methods, the method doesn't sacrifice clean accuracy."}, "weaknesses": {"value": ". You compare K=4 short prompts vs K=1 long prompt and say “more prompts > longer prompt.” But this is really an ensemble (K=4) vs a single model (K=1). Ensembles usually win because of diversity, even with the same total tokens.\nPlease add a plain ensemble baseline, I mean train K independent APT models (same total tokens), and average the predictions. Then we can see if your mixture is better than a standard ensemble, or not.\n\n. Your ablations show the router adds only ~0.33% over a uniform mixture (and sometimes 0%). With no error bars or tests, this can be noise. Most gain seems to come from more prompts, not from the router. I suggest you add mean and std over multiple seeds and a significance test. My question is, why is the router needed if the gain is this small?\n\n. I think Theorem 1 is too trivial for your method since the theorem says that optimized weights are better than uniform. Also, the proof assumes independent weights, while your weights are softmax‑coupled. It does not show that your two‑layer router can learn good weights and generalize. I suggest removing the theorem and presenting only the intuition for it. \n\n. Inconsistent experimental choices, such as different K across datasets. τ = 0.7 is employed everywhere, but the choice is not discussed. A different protocol is used for ImageNet but not with a full justification.\n\n. Recent works have shown that the test-time defences mostly fail under defence-aware attacks. Your router is a key step and should be targeted. For example, router‑aware adaptive attacks will try to push the router to bad mixtures.\n\n. You claim that prompts play \"unique roles,\" but you don't prove this. You should measure the similarities among learned prompts, demonstrate leave‑one‑prompt‑out impact, and provide a visualization of router weights for both clean and adversarial images.\n\n. Some terms are used without clear definitions, such as \"unique roles\" and \"on‑the‑fly\". \n\n. Only CLIP ViT‑B/32 (with TeCoA backbone) is tested; generalization to other VLMs/backbones should be tested.\n\n. You train on Caltech101 but test on other datasets; prompts are per‑class in (Eq. 8). How do you apply them when classes differ? please explain the cross‑dataset procedure."}, "questions": {"value": "Please check weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vDGRDI0sHu", "forum": "tdF6Wnoc6U", "replyto": "tdF6Wnoc6U", "signatures": ["ICLR.cc/2026/Conference/Submission8377/Reviewer_HDVm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8377/Reviewer_HDVm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865344682, "cdate": 1761865344682, "tmdate": 1762920283446, "mdate": 1762920283446, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an adversarial prompt tuning method termed AMPT, which utilizes multiple prompts with a conditional router mechanism during training. Its key findings are: (1) Using multiple prompts is more effective than using a single long prompt. (2) The conditional router outperforms a simple averaging approach. While this paper provides a reasonable approach for improving existing APT methods, its contributions are ultimately incremental, and the empirical gains do not appear to justify the significant increase in computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  It can be inspiring for improving the robustness of VLMs, especially regarding the use of multiple prompts.\n2. The proposed method is easy to follow and intuitive."}, "weaknesses": {"value": "1. The novelty is limited. The paper states that utilizing multiple prompts with a router mechanism, as opposed to a single long prompt, can lead to better robustness in adversarial prompt tuning. However, the proposed method feels like a straightforward application of a conditional weighted sum, which is a widely used technique.\n\n2. The experimental setting needs to be clarified, and more experiments could be added to strengthen the authors' claims. Further details are provided in the \"Questions\" section.\n\n3. The computational cost of this method is high relative to the gain in robustness. The improvements over the state-of-the-art APT baseline are modest: in the all-data setting (Table 1), AMPT achieves an average improvement of only 2.16% on PGD and 1.58% on AutoAttack (AA). In the 16-shot setting (Table 4), this gap shrinks even further to 0.68% on PGD and 0.87% on AA."}, "questions": {"value": "1. Does this trend hold for other prompt lengths? The paper argues that using multiple prompts is better than a single long prompt by analyzing cases where the product (number of prompts × prompt length) is 32 and 64. Could you also provide results for other settings, such as when the product is 16 or 128?\n\n2. What is the effect of the maximum perturbation budget on your findings? For example, with a maximum perturbation of 8/255, would the optimal number of prompts change? How would this convergence point (e.g., an optimal length of 8) differ from the one discussed in the paper?\n\n3. Could you please clarify the prompt lengths used in Table 5?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VgrXi0Jg0j", "forum": "tdF6Wnoc6U", "replyto": "tdF6Wnoc6U", "signatures": ["ICLR.cc/2026/Conference/Submission8377/Reviewer_gNKs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8377/Reviewer_gNKs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975576596, "cdate": 1761975576596, "tmdate": 1762920283010, "mdate": 1762920283010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}