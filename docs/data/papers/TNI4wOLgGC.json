{"id": "TNI4wOLgGC", "number": 1816, "cdate": 1756942498571, "mdate": 1763003396735, "content": {"title": "PRISM: Diversifying Dataset Distillation by Decoupling Architectural Priors", "abstract": "Dataset distillation (DD) promises compact yet faithful synthetic data, but existing approaches often inherit the inductive bias of a single teacher model. As dataset size increases, this bias drives generation toward overly smooth, homogeneous samples, reducing intra-class diversity and limiting generalization. We present PRISM (PRIors from diverse Source Models), a framework that disentangles architectural priors during synthesis. PRISM decouples the logit-matching and regularization objectives, supervising them with different teacher architectures: a primary model for logits and a stochastic subset for batch-normalization (BN) alignment. On ImageNet-1K, PRISM consistently outperforms single-teacher methods (e.g., SRe2L) and recent multi-teacher variants (e.g., G-VBSM) at low- and mid-IPC regimes. The generated data also show significantly richer intra-class diversity, as reflected by a notable drop in cosine similarity between features. We further analyze teacher selection strategies (pre- vs. intra-distillation) and introduce a scalable cross-class batch formation scheme for fast parallel synthesis. Code will be released after the review period.", "tldr": "PRISM makes distilled datasets more diverse and useful by using different neural networks for the classification task and for regularization, breaking the bottleneck of relying on a single model's limited \"view\" of the world.", "keywords": ["Dataset Distillation", "PRISM", "Architectural Priors", "Intra-class Diversity"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/3e52a17d1536c2d310a387283c68c65f1935d032.pdf", "supplementary_material": "/attachment/142ef5ec7a8073c7354b2a35792ee2557934e9b4.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces PRISM, a dataset distillation framework designed to overcome the inductive bias from using a single teacher model. By decoupling logit matching and BN regularization across diverse teacher architectures, PRISM enhances intra-class diversity and improves generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The presentation is clear and easy to follow."}, "weaknesses": {"value": "The overall writing and experimental presentation are quite rough, and the manuscript is not ready for publication in its current form.\n\nThe proposed method shows very limited novelty, appearing to be only a marginal modification of G-VBSM without substantial conceptual advancement.\n\nSeveral important results are missing from the main comparison tables. Even if the original paper did not report these results, the referenced baseline methods are open-sourced, and the authors are expected to run them to ensure a fair and complete evaluation.\n\nMoreover, the paper lacks comparisons with recent methods in dataset distillation. The authors should include or discuss relevant up-to-date works such as:\n\n\t•\tA Label is Worth a Thousand Images in Dataset Distillation\n\t•\tDiversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment\n\t•\tDataset Distillation via the Wasserstein Metric\n\nThe ablation study is insufficient. In particular:\n\n\t•\tCross-architecture performance, which is crucial for evaluating ensemble-based approaches, has not been discussed.\n\t•\tThe effect of different teacher model combinations should be analyzed, as this is closely related to the contribution claim of “decoupling architectural priors.”\n\nAdditionally, it is unclear what the “??” in lines 407–408 refers to, which raises concerns about the completeness and clarity of the manuscript."}, "questions": {"value": "pls see the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gSdbDMjGTu", "forum": "TNI4wOLgGC", "replyto": "TNI4wOLgGC", "signatures": ["ICLR.cc/2026/Conference/Submission1816/Reviewer_iNou"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1816/Reviewer_iNou"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761300331582, "cdate": 1761300331582, "tmdate": 1762915898587, "mdate": 1762915898587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "9v4Wb7dq7w", "forum": "TNI4wOLgGC", "replyto": "TNI4wOLgGC", "signatures": ["ICLR.cc/2026/Conference/Submission1816/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1816/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763003395741, "cdate": 1763003395741, "tmdate": 1763003395741, "mdate": 1763003395741, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces PRISM, a dataset distillation framework that improves sample diversity by decoupling logit supervision and BN alignment across multiple teacher architectures. Experiments on ImageNet-1K show performance gains over existing methods such as SRe2L and DELT, especially at higher IPCs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method and experiments are presented in a structured and easy-to-follow manner.\n\n2. From the experimental results, the proposed method is very effective."}, "weaknesses": {"value": "1. The proposed idea is **highly similar to the CV-DD [1] paper** released on arXiv in early 2025, which also employs an ensemble-style multi-model framework combining multiple model predictions and BN distribution alignments. PRISM mainly removes the multi-prediction matching component from CV-DD [1] and retains only a single prediction alignment, making the modification relatively **minor and incremental**. Furthermore, the paper does not cite CV-DD [1], raising concerns about the authors’ awareness and acknowledgment of closely related prior work.\n\n2. All experiments are conducted only on ImageNet-1K. Smaller-scale benchmarks such as Tiny-ImageNet or CIFAR-100 are missing, making it difficult to assess the general applicability of the method across dataset sizes.\n\n3. Lack of Cross-Architecture Evaluation: The experiments are performed exclusively on ResNet architectures. There is no analysis on whether PRISM generalizes to other backbones such as EfficientNet, ViT, or MobileNet, which limits the claim of broader effectiveness.\n\n4. Missing Theoretical Support: The paper provides no theoretical justification or analytical discussion explaining why decoupling priors or using multiple teachers should yield performance gains. This weakens the conceptual contribution and leaves the empirical improvements largely unexplained.\n\n5. Code not provided, limiting reproducibility."}, "questions": {"value": "1. Could the authors further clarify the differences between PRISM and CV-DD [1]? The two methods appear to be conceptually very similar, and the modification introduced in PRISM seems rather minor and incremental.\n\n2. Since the authors remove the multi-prediction component from the original CV-DD [1] framework, I would like to understand the rationale behind this design choice. Is there any justification or theoretical reasoning for removing it? Have the authors conducted any experiments or analyses demonstrating that excluding multi-prediction alignment leads to the observed performance gain?\n\n3. From Figure 1, the visualization of PRISM is very messy, different classes are mixed together, and I think Sre2l is better.\n\n[1] Dataset Distillation via Committee Voting"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "98Hlt0nZGo", "forum": "TNI4wOLgGC", "replyto": "TNI4wOLgGC", "signatures": ["ICLR.cc/2026/Conference/Submission1816/Reviewer_PrvM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1816/Reviewer_PrvM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761475344717, "cdate": 1761475344717, "tmdate": 1762915898483, "mdate": 1762915898483, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the lack of diversity in distilled data caused by the inductive bias of a single teacher architecture. The method decouples priors by a logit teacher for classification supervision, and multiple BN teachers for BN alignment. Experiments on ImageNet-1K show improvements over SRe2L, G-VBSM, EDC, and DELT, with notable gains at IPC=50 and 100."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear motivation**: The paper identifies an issue in prior DD works and presents a simple idea (multi-teacher decoupling) to mitigate it.\n- **Technical soundness and orthogonality**: The decoupling formulation is well-grounded and orthogonal to existing improvements (e.g., DELT). The derivation is clear.\n- **Clarity and completeness**: The manuscript is well-structured, and the supplementary material provides additional training configurations, privacy discussions, and ablation studies."}, "weaknesses": {"value": "- **Lack of evidence for diversity claim**: The authors claim that PRISM produces distilled data with higher intra-class diversity and lower cosine similarity, but the evidence is limited to one similarity curve (Fig. 4) and qualitative samples. Besides, there is no comparison to the real dataset’s diversity level, leaving it unclear whether PRISM truly approximates or exceeds the diversity of raw data. The claim of “diverse distilled samples benefit to DD” remains unsubstantiated.\n- **Marginal performance gains**: Although the paper reports SOTA numbers on ImageNet-1K, the improvements over the strong DELT baseline are mostly within 0.5–1.0%, which make it hard to attribute the improvement confidently to the proposed architectural decoupling. BTW, the results of resnet101@IPC=10 is significantly lower than the DELT and there is no explanation.\n- **Limited evaluation scope**: Experiments are conducted solely on ImageNet-1K. The paper does not include results on smaller datasets or challenging subsets (e.g., Tiny Imagenet, ImageNetIDC, nette, or woof), nor any cross-domain evaluations (CIFAR). This lack of evaluation datasets limits the demonstrated generalization ability of PRISM.\n- **CNN-only evaluation**: All experiments use only CNN-based architectures (ResNet, MobileNet, ShuffleNet, EfficientNet).\nModern architectures such as Vision Transformers (ViT) or ConvNeXt are missing, which weakens the central claim that PRISM generalizes across diverse inductive biases. Besides, the paper motivates PRISM as a way to “decouple architectural priors,” yet it does not evaluate whether synthetic data distilled by PRISM transfers well to various networks beyond resnet family (**No cross-architecture generalization analysis**).\n- **Missing comparisons with modern generative DD methods**: The evaluation excludes diffusion-based dataset distillation baselines such as Minimax, D4M, IGD, and MGD3, etc., which represent current frontiers of DD and they also addressed the same issue of diversity distillation as PRISM."}, "questions": {"value": "- **Confounding from powerful teachers**: The paper does not disentangle the influence of teacher networks from the PRISM framework itself. It remains unclear whether the observed gains stem from the architectural decoupling or simply from using the more powerful teachers itself. Can you provide the ablation results with different single BN teachers?\n- **Diversity metrics**: Can you report diversity metrics relative to the original dataset to clarify whether PRISM preserves or exceeds raw data variation? If the behavior is different, how to explain the performance gains?\n- **Confused fig1**: I don't believe the classification accuracy of the fig1(right) can be better than the left ones since the decision boundary is much more mixed. If I missed something, please let me know.\n- **Experiments**: Would the benefits of PRISM persist on lower-resolution or domain-shifted dataset?\n- **Incremental contribution**: The use of multiple teacher models closely resembles the model pool concept in prior dataset distillation methods. The paper should make it clearer what **fundamentally** (rather than just framework) differentiates PRISM from these existing frameworks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "h0AVCrfi6i", "forum": "TNI4wOLgGC", "replyto": "TNI4wOLgGC", "signatures": ["ICLR.cc/2026/Conference/Submission1816/Reviewer_Ghc3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1816/Reviewer_Ghc3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761635656259, "cdate": 1761635656259, "tmdate": 1762915898304, "mdate": 1762915898304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper presents PRISM, a dataset distillation algorithm which uses an ensemble of teacher models to generate distilled images\n- The main idea is to take the SRe2L loss, and rotate through a pool of teacher models for the batchnorm loss, while using the same model for label alignment loss\n- Empirical results show good improvement over baselines"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Pretty intuitive motivation, and presented clearly\n- Strong empirical results\n- Thorough ablation studies"}, "weaknesses": {"value": "- Relatively incremental contribution. The use of multiple architecture backbones has been studied in much prior work [1, 2], along with the choice of using teacher emsembles for prediction generation. In particular [2], uses a very similar idea of rotating teacher models during optimization, albeit using an ensemble of the same resnet-18 model retrained multiple times (rather than different backbones) in this paper\n\n- The marginal contribution of this work over previous work seems minor. In particular, from Table 1, gains are rather small over the baselines presented. Furthermore, PRISM, as discussed in section 4.4, uses pretty much every optimization trick presented in all the prior work, so it is unclear what the exact contribution is. For example, the margin between EDC and PRISM in table 1 is rather small, but PRISM additionally uses the \"varying the distillation iterations per image\" technique from DELT. So it is unclear, for example, if EDC with that trick would outperform PRISM (given than the gains from table 3b appear to be ~1.3%, approximately the same as the margin between PRISM and EDC).\n\n- Privacy analysis in Appendix is completely incorrect. Differential privacy can only be guaranteed with two main components: **bounded sensitivity**, and **calibrated noise addition**. PATE adds privacy by adding calibrated noise to the queries of the teacher. Furthermore, this privacy loss is composed over every query to these teacher models. PRISM claims \"The ”noise” required for privacy is not explicitly added but arises organically from the inherent disagreement between the gradients of two distinct model architectures\", however without clipping gradients from these teachers, or calibrated the actual noise scale, there is no privacy bound whatsoever, so it is useless to discuss differential privacy. Furthermore, teachers in PRISM are trained on the full dataset, while in PATE, they are trained on subsets which is necessary for privacy. I strongly suggested removing this discussion or adding a strong disclaimer that there are **no real privacy benefits** afforded by using multiple teachers, and everything in that section is purely speculative.\n\n\n\n[1] Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching\n\n[2] Large Scale Dataset Distillation with Domain Shift"}, "questions": {"value": "- What are the results in terms of cross-architecture generalization? Given that the method uses multiple backbones, we would expect better architecture generalization."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uItKlc1Gau", "forum": "TNI4wOLgGC", "replyto": "TNI4wOLgGC", "signatures": ["ICLR.cc/2026/Conference/Submission1816/Reviewer_LBFu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1816/Reviewer_LBFu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1816/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875024983, "cdate": 1761875024983, "tmdate": 1762915898116, "mdate": 1762915898116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}