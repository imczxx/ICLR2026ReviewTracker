{"id": "ZETkK0zfrt", "number": 18660, "cdate": 1758289810430, "mdate": 1759897089320, "content": {"title": "Latent Reasoning via Sentence Embedding Prediction", "abstract": "Autoregressive language models (LMs) generate one token at a time, yet human reasoning operates over higher-level abstractions - sentences, propositions, and concepts. This contrast raises a central question- Can LMs likewise learn to reason over structured semantic units rather than raw token sequences? In this work, we investigate whether pretrained LMs can be lifted into such abstract reasoning spaces by building on their learned representations. We present a framework that adapts a pretrained token-level LM to operate in sentence space by autoregressively predicting continuous embeddings of next sentences. We explore two embedding paradigms inspired by classical representation learning: 1) semantic embeddings, learned via autoencoding to preserve surface meaning; and 2) contextual embeddings, trained via next-sentence prediction to encode anticipatory structure. We evaluate both under two inference regimes: Discretized, which decodes each predicted embedding into text before re-encoding; and Continuous, which reasons entirely in embedding space for improved efficiency. Across four domains - mathematics, logic, commonsense, and planning - contextual embeddings under continuous inference show competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs on average by half. We also present early signs of scalability and modular adaptation. Finally, to visualize latent trajectories, we introduce SentenceLens, a diagnostic tool that decodes intermediate model states into interpretable sentences. Together, our results indicate that pretrained LMs can effectively transition to abstract, structured reasoning within latent embedding spaces.", "tldr": "We adapt a pretrained language model to autoregress over sentence embeddings instead of tokens.", "keywords": ["Sentence-level autoregression", "Latent reasoning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/75a7a0b43f4e62db42840a3fd182c224f9245b72.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel framework that elevates pretrained language models from token-level generation to sentence-level reasoning. Instead of predicting tokens, the model autoregressively predicts continuous embeddings of the next sentence, supporting two embedding paradigms—semantic and contextual. The authors further design two inference modes: discretized and continuous. Experiments show that contextual embeddings under continuous inference achieve competitive performance with CoT reasoning while halving inference FLOPs."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1-The paper takes a new step beyond token-level CoT reasoning by framing reasoning as prediction over sentence embeddings.\n\n2-The experiments span mathematical, logical, commonsense, and planning tasks, showing consistent findings and careful analysis of efficiency and robustness."}, "weaknesses": {"value": "1-Although the idea is intriguing, the improvements over CoT are modest and not statistically analyzed; performance is often close to, but not consistently better than, token-level baselines.\n\n2-All experiments are limited to sub-1B GPT-2 models; the claimed scalability to larger models is only hypothesized.\n\n3-SentenceLens is interesting but lacks quantitative validation (e.g., measuring interpretability gains or correlation with reasoning correctness).\n\n4-The paper contains many technical details and citations but could be clearer.\n\n5-The reported performance gains may largely stem from fine-tuning effects rather than genuine enhancements in reasoning capability. The proposed method’s contribution to reasoning improvement is therefore not convincingly demonstrated.\n\n6-Several comparative algorithms and components are mentioned, yet the paper lacks rigorous ablation or empirical evidence showing their individual effectiveness or contribution to the final results.\n\n7-As stated on page 5, it remains unclear whether the observed reasoning ability truly originates from the latent model itself, or from the pretrained transformer’s inherited capability.\n\n8-While SentenceLens offers interpretability, it requires an additional decoding module and increases computational complexity. Thus, the interpretability improvement comes at the cost of additional model overhead rather than being an inherent property of the latent reasoning framework."}, "questions": {"value": "1-Can the proposed latent reasoning generalize to open-ended generation tasks (e.g., summarization or dialogue) rather than step-bounded reasoning?\n\n2-How does the approach compare to recent reasoning compression methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "riHdJNUN0L", "forum": "ZETkK0zfrt", "replyto": "ZETkK0zfrt", "signatures": ["ICLR.cc/2026/Conference/Submission18660/Reviewer_Ncgs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18660/Reviewer_Ncgs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658041605, "cdate": 1761658041605, "tmdate": 1762928361404, "mdate": 1762928361404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study addresses the issues of inefficient computation and suboptimal reasoning granularity encountered by LLMs when generating step-by-step reasoning chains on a token-by-token basis. To this end, the authors introduce a novel framework designed to lift the abstract reasoning capabilities of pre-trained language models from the token level to the sentence level. The central concept of this framework is to have the model autoregressively predict the continuous embedding vectors of the next sentence, instead of generating raw tokens. Specifically, the work develops two types of embedding vectors—semantic embeddings and contextual embeddings—and investigates both discretized and continuous inference modes. Experimental findings demonstrate that, under the setting of “contextual embeddings with continuous inference”, the proposed method achieves performance comparable to CoT, while simultaneously reducing the inference computational cost by half."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written, and the figures are well-designed and easy to follow.\n2. The attempt to perform auto-regressive decoding at the sentence level is innovative."}, "weaknesses": {"value": "1. I understand the authors' experimental scope was limited to GPT-2-sized models due to computational constraints. To strengthen the experimental evidence, I suggest the authors broaden their evaluation to include more recent models, such as Llama-3.2-1B-Instruct, Qwen3-0.6B, and Qwen3-1.7B.\n2. I am puzzled by the mere 2x speedup achieved for sentence-level auto-regressive decoding. Given the degree of parallelism involved, a more significant gain (e.g., 10x) would be expected intuitively. I would appreciate a detailed explanation or analysis from the authors to clarify why the speedup was only twofold.\n3. Could the authors clarify what the bolded figures in Table 2 specifically signify? My interpretation is that they are intended to highlight the best-performing method within each dataset. However, based on the data presented, it appears the best method has been incorrectly marked across all four datasets. The authors should verify and correct these annotations to ensure the table's accuracy."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "52Jy4PBNHh", "forum": "ZETkK0zfrt", "replyto": "ZETkK0zfrt", "signatures": ["ICLR.cc/2026/Conference/Submission18660/Reviewer_yGxr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18660/Reviewer_yGxr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987128086, "cdate": 1761987128086, "tmdate": 1762928361048, "mdate": 1762928361048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper proposes DynaGNN,      a a framework that adapts GNN architectures to handle temporal changes in graph structure through a meta-learning approach with edge-aware attention mechanisms. The key contribution is enabling GNNs to dynamically adjust their parameters based on evolving graph topology without ful  retraining."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I appreciate the comprehensive evaluation across multiple dynamic graph benchmarks and the practical applicability to real-world scenarios like social networks and traffic prediction. The edge-aware attention mechanism is a nice touch that effectively captures local topology changes, and the meta-learning framework provides good theoretical grounding for adaptation."}, "weaknesses": {"value": "The computational overhead isn't thoroughly analyzed, which concerns me for large-scale deployment. I also think the paper oversells the novelty a bit since similar meta-learning approaches exist in the literature, and the comparison with some recent temporal GNN methods like EvolveGCN seems incomplete."}, "questions": {"value": "How does the method scale to graphs with millions of nodes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zywN393uK2", "forum": "ZETkK0zfrt", "replyto": "ZETkK0zfrt", "signatures": ["ICLR.cc/2026/Conference/Submission18660/Reviewer_v7p4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18660/Reviewer_v7p4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035251781, "cdate": 1762035251781, "tmdate": 1762928360453, "mdate": 1762928360453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a framework that adapts pretrained token-level language models to operate in sentence-level embedding spaces. The authors explore two embedding paradigms: semantic embeddings (trained via autoencoding) and contextual embeddings (trained via next-sentence prediction). They evaluate these under discretized (decode-text-encode) and continuous (pure latent) inference modes across mathematics, logic, commonsense, and other tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents a well-motivated approach to lifting pretrained LMs to higher-level abstractions without expensive retraining."}, "weaknesses": {"value": "The experiments are limited to GPT-2 scale models, leaving questions about scalability to modern LLMs unanswered. The paper acknowledges but doesn't fully address the fragility of continuous embeddings to perturbations, which could limit practical deployment."}, "questions": {"value": "How does performance scale with larger pretrained models (e.g., 7B+ parameters)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zywN393uK2", "forum": "ZETkK0zfrt", "replyto": "ZETkK0zfrt", "signatures": ["ICLR.cc/2026/Conference/Submission18660/Reviewer_v7p4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18660/Reviewer_v7p4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762035251781, "cdate": 1762035251781, "tmdate": 1763165183311, "mdate": 1763165183311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new paradigm for language model reasoning, called Sentence-level Latent Reasoning. Unlike traditional token-level autoregressive generation, this approach enables the model to perform prediction and reasoning in the sentence embedding space, thereby achieving a more abstract and efficient reasoning process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel idea of performing autoregressive reasoning in the sentence embedding space, elevating the reasoning hierarchy of language models from the token level to the sentence level.\n2. The proposed SentenceLens enables decoding intermediate latent states into natural language sentences, making the model’s “thought process” interpretable and analyzable.\n3. By reasoning directly in the continuous embedding space, computational efficiency is significantly improved (1.5–2.5× speedup)."}, "weaknesses": {"value": "1. Although the paper proposes the concept of “sentence-level reasoning,” it does not sufficiently justify why sentence-level embeddings can necessarily capture the logical structures required for reasoning that token-level embeddings cannot.\n2. The comparison is quite limited: it only benchmarks against one latent reasoning model (Coconut), and shows no significant advantage except on the Blocksworld task (where the performance gap is unusually large, raising concerns about possible evaluation bias). Additionally, computational efficiency is not compared with Coconut."}, "questions": {"value": "1. Does your method offer any computational efficiency advantage compared to Coconut? If so, by how much?\n2. Why do you introduce an encoder during training? Why are its parameters shared with the decoder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6inyPOoE9G", "forum": "ZETkK0zfrt", "replyto": "ZETkK0zfrt", "signatures": ["ICLR.cc/2026/Conference/Submission18660/Reviewer_7gE9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18660/Reviewer_7gE9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150145403, "cdate": 1762150145403, "tmdate": 1762928359410, "mdate": 1762928359410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for performing latent reasoning in the sentence embedding space. The authors repurpose a pretrained language model (GPT-2) and construct two types of sentence-level embeddings (semantic and contextual) to map natural language into a continuous latent space, enabling autoregressive generation within that space. The paper further introduces a visualization tool, *SentenceLens*, to analyze the model’s “latent reasoning trajectories” in embedding space."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The experiments span diverse reasoning domains (mathematical, logical, commonsense, and planning), demonstrating the general applicability of the framework.\n\n- The paper provides an interpretability tool (*SentenceLens*) for analyzing latent reasoning trajectories.\n\n- Despite the complex design, the authors’ exposition is overall clear and logically structured."}, "weaknesses": {"value": "- The InfoNCE loss ratio and λ parameter for CTX-C are not specified, making the experiments hard to reproduce.\n\n- The scope of fine-tuning (which components are updated) is unclear, and gradient flow is not described.\n\n- No comparisons are made against strong latent-reasoning baselines such as CoCoMix, CoDi, or Token Assorted.\n\n- Table 2 reports only a single inference mode; it should separately present accuracy and FLOPs across different reasoning modes.\n\n- The empirical improvements over coconut and cot are relatively minor and do not convincingly demonstrate the claimed efficiency–performance trade-off."}, "questions": {"value": "- Is $h_N$ the final-layer hidden state or a multi-layer aggregation?\n\n- What value of λ is used for CTX-C? Have you performed a λ-sweep analysis?\n\n- How does *SentenceLens* decode embeddings originating from different encoders?\n\n- What is the exact architecture of the M-projection head? Is it used during both training and inference?\n How does the Continuous mode avoid semantic drift without the M head?\n\n- Which components are frozen during fine-tuning? Do the encoder and decoder receive gradient updates?\n\n- Why are CoCoMix, CoDi, and Token Assorted excluded from the baseline comparisons?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "igoDg0TaLr", "forum": "ZETkK0zfrt", "replyto": "ZETkK0zfrt", "signatures": ["ICLR.cc/2026/Conference/Submission18660/Reviewer_rMkZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18660/Reviewer_rMkZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18660/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185270911, "cdate": 1762185270911, "tmdate": 1762928358835, "mdate": 1762928358835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}