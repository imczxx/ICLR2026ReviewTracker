{"id": "sSuZjk7zFb", "number": 22275, "cdate": 1758328819733, "mdate": 1759896875507, "content": {"title": "The Era of Real-World Human Interaction: RL from User Conversations", "abstract": "We posit that to achieve continual model improvement and multifaceted alignment, future models must learn from natural human interaction. Current conversational models are aligned using pre-annotated, expert-generated human feedback. In this work, we introduce Reinforcement Learning from Human Interaction (RLHI), a post-training paradigm that learns directly from in-the-wild user conversations. \nWe develop two complementary methods: \n(1) RLHI with User-Guided Rewrites, which revises unsatisfactory model outputs based on users' natural-language follow-up responses,\n(2) RLHI with User-Based Rewards, which learns via a reward model conditioned on knowledge of the user's long-term interaction history (termed persona). \nTogether, these methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on conversations derived from WildChat, both RLHI variants outperform strong baselines in personalization and instruction-following, and similar feedback enhances performance on reasoning benchmarks. These results suggest organic human interaction offers scalable, effective supervision for personalized alignment.", "tldr": "We introduce Reinforcement Learning from Human Interaction (RLHI), a post-training paradigm that learns directly from in-the-wild user conversations.", "keywords": ["Reinforcement Learning", "Large Language Models", "Preference Optimization", "Human Interaction", "Personalization", "Instruction Following"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/db03aa8061cea8b3156354c62942736210300c00.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents RLHI (Reinforcement Learning from Human Interaction), a framework for post-training personalization of language models through continuous learning from real-world user interactions. The authors introduce two methods for this process: User-Guided Rewrites, which revises unsatisfactory model outputs based on users' follow-up responses, and User-Based Rewards, which ranks responses based on a reward model conditioned on long-term user personas. Both methods aim to align model outputs with users' ongoing, evolving preferences. Evaluations show that these methods outperform baseline models on personalization and instruction-following tasks on datasets like AlpacaEval 2.0 and Arena-Hard, with additional improvements in reasoning tasks. The results highlight the effectiveness of learning directly from organic human interaction in achieving personalized and contextually aligned models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "RLHI is an advancement in aligning language models with personalized user preferences by directly learning from ongoing user conversations. And the research question is important. By focusing on real-world human interaction, the paper pushes the field towards a more scalable, personalized approach to model alignment."}, "weaknesses": {"value": "1. Complexity in Application: While the methods work well in controlled settings, they involve managing user persona conditioning, which could become cumbersome for models with large vocabularies or diverse users. Is it feasible to apply these methods at scale without significant overhead, especially when it comes to managing persona conditioning across millions of users?\n2. Method Simplicity: The methods proposed, while effective, are relatively straightforward. The first method uses persona rewriting and the second applies persona-conditioned reward modeling. These are not particularly novel compared to existing techniques, such as RLHF. The simplicity of the methods raises questions about why such straightforward approaches result in such impressive outcomes (77.9 on AlpacaEval2.0 and 64.3 on ArenaHard)."}, "questions": {"value": "1. In Table 3, it seems that RL with User-Agnostic Rewards setting achieves a relatively good performance without a human persona. How to explain it?\n2. In Table 3, RL with Rewrites from Scratch and User-Agnostic Rewards seems to have a higher LC win compared to Win on AlpacaEval2. Does it indicate that these two methods may favor shorter responses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WxU2xRojrW", "forum": "sSuZjk7zFb", "replyto": "sSuZjk7zFb", "signatures": ["ICLR.cc/2026/Conference/Submission22275/Reviewer_w7qf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22275/Reviewer_w7qf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760701385393, "cdate": 1760701385393, "tmdate": 1762942147732, "mdate": 1762942147732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RLHI (Reinforcement Learning from Human Interaction), a framework that learns directly from user interaction in real conversations. RLHI includes User-Guided Rewrites and User-Based Rewards, leveraging both long-term preference and turn-level feedback to achieve continual personalized alignment. Experimental results demonstrate that RLHI not only improves performance in user-based evaluations but also achieves notable gains in instruction-following and reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Grounding in Real-World Human Preference**: RLHI learn human preference directly from natural user interactions, which provide more diverse signals than than existing curated datasets.\n\n2. **Comprehensive Experiments and Strong Performance**: RLHI methods are evaluated across user-based evaluation, instruction-following, and reasoning tasks, achieving significantly performance improvements. Further ablation studies validate the effectiveness and contribution of each proposed component.\n\n3. **Insightful Analysis** : This paper provides meaningful anaysis and insight about the properties of current human interaction data, demonstrating that feedback often occur in user conversations and highlighting these statics are likely to change with the evolve of LLMs."}, "weaknesses": {"value": "1. The authors use Llama-3.1-8B to reconstruct responses while retaining the original user feedback (Line 269). However, since different models may behave differently, the preserved feedback may no longer be accurate or applicable, potentially misguiding the model.\n\n2. The term “user-based rewards” can be misleading, as it suggests a reward model that scores based on user persona. In practice, the framework just samples multiple responses conditioned on the user persona and selects among them using a reward model.\n\n3. The experiments on reasoning tasks assumes that human feedback is correct (synthesize conversations based on PRM dataset). However, human users may not be experts and could introduce errors, leading the model to learn incorrect preferences. In my view, RLHI is not well suited for objective tasks like math reasoning.\n\n4. The proposed method heavily relies on external reward models for data filtering and for providing high-quality supervision. Its improvements on non-personalized tasks are limited (comparing RLHI with user-based and user-agnostic rewards on user-free evaluation), suggesting that much of the gain may stem from the reward model rather than from the RLHI framework.\n\n5. I have some concerns that the paper presents the method as reinforcement learning, while the experiments are conducted solely using DPO."}, "questions": {"value": "See above section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o107yN40hJ", "forum": "sSuZjk7zFb", "replyto": "sSuZjk7zFb", "signatures": ["ICLR.cc/2026/Conference/Submission22275/Reviewer_6TvU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22275/Reviewer_6TvU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994672935, "cdate": 1761994672935, "tmdate": 1762942147535, "mdate": 1762942147535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "# Summary\nA post-training framework is introduced to learn directly from in-the-wild user–LLM conversations instead of pre-annotated expert feedback. Two complementary variants are presented: **RLHI with User-Guided Rewrites** (convert user follow-ups that revise unsatisfactory answers into preference pairs) and **RLHI with User-Based Rewards** (rank candidates using a reward model conditioned on a summarized user “persona” from long-term histories). On **user-based evaluations** built from WildChat conversations, both RLHI variants improve personalization and instruction-following over strong baselines; **RLHI-User-Based Rewards** attains **77.9% length-controlled win rate on AlpacaEval 2.0** and strong Arena-Hard results under **GPT-4-Turbo** judging (Table 3). **RLHI-User-Guided Rewrites** also raises accuracy on four reasoning benchmarks from **26.5 to 31.8** despite training only on math conversations (Table 4). Ablations indicate benefits from user guidance, interaction diversity, RL over SFT, and quality filtering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "# Strengths\n* **Realistic supervision source:** Moves beyond curated annotator pipelines to leverage in-situ signals (SAT/DSAT, rewrites, follow-ups).\n\n* **Compelling empirical improvements (user-based & standard).**\n\n    • On **WILDCHAT USEREVAL**, RLHI variants improve personalization and overall user preference (Table 2).\n\n    • On **AlpacaEval 2.0** and **Arena-Hard**, RLHI-User-Based Rewards achieves 77.9% LC win and 83.4 Arena-Hard win (Table 3).\n\n    • On reasoning, **RLHI-User-Guided Rewrites** lifts average accuracy **26.5 to 31.8** across Minerva, OlympiadBench, GPQA, and   MMLU-Pro (Table 4)."}, "weaknesses": {"value": "# Weaknesses\n\n1. **Evaluator and judge bias**\n\n   * *Example (benchmarks):* MT-Bench uses **GPT-4o** as judge; AlpacaEval 2 and Arena-Hard use **GPT-4-Turbo**, all closed-source LLM judges. \n   * *Example (user eval):* RLHI reports user-based results judged by **OpenAI o3** on WILDCHAT USEREVAL, again relying on a closed-source judge. \n\n2. **Persona inference via LLM**\n\n   * *Example:* RLHI **prompts an LLM to summarize each user’s persona** from long-term histories to steer both training (persona-conditioned DPO) and inference—useful but introduces risk of hallucination/leakage without audits."}, "questions": {"value": "## What Could Be Improved.\n1. **Add strong baselines**\n* **Teacher-distill baseline**: SFT on *ChatGPT/strong model* responses for the same turns. This tests whether mined preferences beat a simple, distillation.\n* **Ground-truth tasks**: Include **GSM8K**, **MATH**, **MBPP/HumanEval** with exact-match verify gains on **verifiable** reasoning/coding.\n\n2. **Robust judging**\n* Evaluate with **multiple independent judges** (closed and open), randomize templates, and report **win-rate consistency** across judges. Include **anti-style** prompts (brevity/conciseness constraints)."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "## Responsible research practice.\nThe data originates from real user interactions (WildChat) rather than synthetic or crowdsourced text."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sa01rLHJOd", "forum": "sSuZjk7zFb", "replyto": "sSuZjk7zFb", "signatures": ["ICLR.cc/2026/Conference/Submission22275/Reviewer_YARd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22275/Reviewer_YARd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094539737, "cdate": 1762094539737, "tmdate": 1762942146949, "mdate": 1762942146949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RLHI, a post-training paradigm that learns directly from real-world user interactions to align large language models with evolving, long-horizon preferences. RLHI converts organic conversations into preference signals via two complementary routes: (1) User-Guided Rewrites, which transforms user follow-ups into improved revisions to form preference pairs; and (2) User-Based Rewards, which ranks multiple candidates using a reward model conditioned on a natural-language user persona. Over user-based evaluation, standard instruction-following benchmarks, and reasoning suites, RLHI delivers consistent gains over baselines. Ablations highlight the importance of leveraging explicit feedback, RL over SFT on interaction data, and the effect of quality filtering."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors clearly pinpoint an important gap that current alignment underutilizes rich multi-turn user feedback. They leverage long-horizon personas and turn-level corrections to drive personalized learning.\n\n2. The paper uses two complementary, lightweight routes (user-guided rewrites and user-based rewards) under a DPO-style objective, making the approach easy to integrate and broadly usable even with sparse explicit ratings.\n\n3. It demonstrates effectiveness on both user-based evaluations and standard public leaderboards, with consistent gains and informative ablations across diverse tasks and domains."}, "weaknesses": {"value": "1. Limited novelty. The user-guided rewrites track prior WildChat-style pipelines [1,2]; the user-based rewards overlap with user-conditioned reward models [3,4] and conditioning LM/RM/judges on user summaries [5,6,7]. If those lines cannot solve the paper’s target problem, the manuscript should specify why; otherwise, stronger baseline comparisons are needed.\n\n2. Human study is under-specified. Results are summarized in one sentence with no details.\n\n3. Reasoning part uses PRM data to simulate conversations without analyzing its limitations. Also, if personas are not used at inference, the setup collapses to vanilla DPO, weakening the core claim.\n\n[1] WildFeedback: Aligning LLMs With In-situ User Interactions And Feedback. arXiv:2408.15549.\n\n[2] User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal. arXiv:2507.23158.\n\n[3] Personalized Language Modeling from Personalized Human Feedback. arXiv:2402.05133.\n\n[4] Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning. arXiv:2408.10075.\n\n[5] RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs. arXiv:2409.04421.\n\n[6] Learning to summarize user information for personalized reinforcement learning from human feedback. arXiv:2507.13579.\n\n[7] Can LLM be a Personalized Judge? arXiv:2406.11657."}, "questions": {"value": "1. What's the core novelty comparing with the papers listed in the weaknesses?\n\n2. Please provide more details about the human study and the experiments on reasoning benchmarks. (See Weaknesses)\n\n3. How does the system adapt to truly new users? Do the identified preference dimensions and feedback types generalize under cold-start conditions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GF6wjJLZSD", "forum": "sSuZjk7zFb", "replyto": "sSuZjk7zFb", "signatures": ["ICLR.cc/2026/Conference/Submission22275/Reviewer_Fryz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22275/Reviewer_Fryz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22275/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762164528888, "cdate": 1762164528888, "tmdate": 1762942146672, "mdate": 1762942146672, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}