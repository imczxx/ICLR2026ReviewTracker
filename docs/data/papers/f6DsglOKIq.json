{"id": "f6DsglOKIq", "number": 5018, "cdate": 1757834827302, "mdate": 1759898000037, "content": {"title": "Disentangle and Align: Structured Contrastive Learning with Semantic–Domain Separation", "abstract": "Learning compact representations that preserve semantics while discarding nuisance variation is central to self-supervised learning (SSL).\nHowever, when training data come from heterogeneous domains, instance-level contrastive learning often treats cross-domain yet semantically similar samples as false negatives and entangles domain cues with semantic features, yielding domain-clustered representations that generalize poorly to novel domains. To address this issue, we propose Structured Contrastive Learning (SCL).\nThis unified framework jointly learns (i) a semantic representation $\\mathbf{z}_s$ via semantic contrast, (ii) a domain representation $\\mathbf{z}_d$ via domain contrast, and (iii) their disentanglement by minimizing the dependence (mutual information) between $\\mathbf{z}_s$ and $\\mathbf{z}_d$. This structure preserves domain-invariant semantics in $\\mathbf{z}_s$ while isolating domain factors in $\\mathbf{z}_d$, enabling robust self-supervised training on data from a mixture of domains and out-of-domain(OOD) generalization on novel domains. Theoretically, we proved that the training objective of SCL is to extract semantic and domain information separately, and minimizing the mutual information between $\\mathbf{z}_s$ and $\\mathbf{z}_d$ can enhance the model’s generalization ability under domain shift. Empirically, we validate the performance of SCL on multi-domain training tasks and its generalization to novel domains through experiments on multiple datasets from multiple modalities.", "tldr": "", "keywords": ["self-supervised learning", "domain shift"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e86e70fbda720ef28e44cff744ef93e4aba12c5.pdf", "supplementary_material": "/attachment/d627327dd8bfe3e9c7e8a7952ae7fac7e69a672b.zip"}, "replies": [{"content": {"summary": {"value": "This paper tackles a fundamental challenge in self-supervised learning (SSL): how to learn semantically meaningful representations that generalize across domains, especially under the presence of domain shift. The authors observe that existing instance-level contrastive learning methods often entangle semantic and domain factors, leading to poor generalization on out-of-distribution (OOD) data. Specifically, they point out that semantically similar samples from different domains may be mistakenly treated as negatives, resulting in domain-clustered and semantically misaligned representations.\nTo address this, the authors propose a novel framework called Structured Contrastive Learning (SCL), which introduces explicit separation and alignment of semantic and domain representations in the contrastive learning pipeline.\n1.The SCL model separates semantic and domain features using dual contrastive objectives and a disentanglement loss via HSIC.\n 2.The authors prove that minimizing mutual information between zs and zd improves generalization under domain shifts. \n3.The paper benchmarks SCL against strong baselines across four datasets(MNIST-C,RotatedMNIST, PACS,ADNI),consistently showing improvements in ID and OOD accuracy. \n4.Rigorous ablation and significance tests(t-test,Wilcoxon)support the empirical findings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Tackles a real,unsolved problem in SSL:the entanglement of domain and semantic features under domain shift. \nMoves beyond adversarial or label-free methods by proposing a principled mutual information-based separation framework. \nOffers a general-purpose approach that is applicable to multiple modalities (images,medical tabular data,etc.)"}, "weaknesses": {"value": "Although the approach is novel, the disentanglement mechanism is an elegant recombination of known tools (InfoNCE, HSIC), not a fundamentally new algorithmic paradigm.\nApplication to larger-scale or real-world tasks (e.g., ImageNet, language–vision tasks) is not explored; this limits immediate impact beyond academic datasets."}, "questions": {"value": "1. Idealized assumptions in theory, The sufficiency and conditional independence assumptions in Theorems 1-3 are strong. Add discussion or diagnostics to assess whether these assumptions approximately hold in practice.\n2. All datasets are relatively small-scale and synthetic (except for ADNI). It is recommended to increase or discuss the expansion to real-world datasets.\n3. How sensitive is the HSIC disentanglement penalty to the kernel choice (e.g., Gaussian vs. linear)?\n4. Does minimizing I(z_s; z_d) always improve generalization? Are there scenarios (e.g., medical imaging) where domain information correlates with class labels and disentanglement may hurt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lQYll1nIKL", "forum": "f6DsglOKIq", "replyto": "f6DsglOKIq", "signatures": ["ICLR.cc/2026/Conference/Submission5018/Reviewer_2fPs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5018/Reviewer_2fPs"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761400334669, "cdate": 1761400334669, "tmdate": 1762917824082, "mdate": 1762917824082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a structured latent space approach for unsupervised domain adaptation (UDA), aiming to disentangle domain-specific and domain-invariant representations while aligning the invariant ones between source and target domains. The method introduces a disentanglement module using orthogonality constraints and a mutual information loss to enforce independence between domain-specific and domain-invariant subspaces. Furthermore, a cross-domain alignment loss is used to align shared representations in the latent space. Experiments on common benchmarks such as Office-31, Office-Home, and VisDA demonstrate that the proposed approach achieves competitive performance compared to state-of-the-art UDA methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear motivation: Effectively identifies the problem of domain-specific and invariant feature entanglement in existing UDA methods.\n\n- Structured latent space: Introduces a well-defined framework that explicitly separates domain-specific and domain-invariant features.\n\n- Competitive performance: Demonstrates consistent improvements over existing UDA methods on popular benchmarks such as Office-31 and Office-Home."}, "weaknesses": {"value": "- Incremental contribution: The proposed approach is similar to previous methods like DSN and MCD, offering limited novelty.\n\n- Weak theoretical foundation: Lacks formal theoretical justification for how the disentanglement and alignment constraints ensure effective generalization.\n\n- Scalability concerns: Unclear how well the approach generalizes to high-dimensional data or larger models, like transformers."}, "questions": {"value": "- How do the authors ensure that the domain-invariant features remain class-discriminative in the latent space?\n\n- What happens if the approach is applied to multi-source domain adaptation with highly diverse domain distributions?\n\n- Could the method benefit from using contrastive learning techniques instead of adversarial training for alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JTBSWQJ9cU", "forum": "f6DsglOKIq", "replyto": "f6DsglOKIq", "signatures": ["ICLR.cc/2026/Conference/Submission5018/Reviewer_uegn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5018/Reviewer_uegn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946338546, "cdate": 1761946338546, "tmdate": 1762917823842, "mdate": 1762917823842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Structured Contrastive Learning (SCL), a self-supervised learning framework designed to address the problem of learning domain-invariant semantic representations from multi-domain data. The core contribution is a unified framework that simultaneously learns three objectives: (i) semantic representations zs via semantic contrast (using augmented views of the same sample), (ii) domain representation zd via domain contrast (using domain labels), and (iii) disentanglement between zs and zd by minimizing mutual information I(zs;zd). The authors provide theoretical analysis demonstrating that minimizing mutual information improves generalization under domain shift and empirically demonstrate consistent improvements over SSL and domain generalization baselines on multi-domain benchmarks (MNIST-C, Rotated MNIST, PACS, and ADNI)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear Framework and Methodology:** The paper presents a clear, end-to-end framework with well-motivated loss components: InfoNCE for semantic and domain contrasts, and HSIC for disentanglement. The practical training procedure is straightforward to implement.\n2. **Multi-applications Evaluation:** The paper includes diverse datasets (synthetic with controlled shifts, natural domain generalization benchmark, and medical data), multiple baselines spanning both SSL and domain generalization, and evaluation through the standard leave-one-domain-out protocol.\n3. **Visualization and Interpretability:** Figure 4's t-SNE visualizations provide good intuition that zd successfully clusters by domain while zs captures semantic information, supporting the disentanglement claim."}, "weaknesses": {"value": "1. **Severely Limited Scalability and Practicality of Experimental Setup:** The paper uses small-scale datasets (MNIST-C, RotatedMNIST, PACS, ADNI) and lightweight CNN/MLP backbones for all experiments. While this choice aids interpretability, it raises questions about real-world applicability, especially given that modern DG/SSL research typically evaluates on larger, heterogeneous datasets (e.g., WILDS, DomainNet) (Kalibhat et. al. 2023) using ubiquitos ResNet/ViT-based backbones.\n2. **Absence of full finetuning or transfer analysis:** The paper exclusively uses linear probe evaluation on representations learned by SCL. However, linear probes are mainly diagnostic; most real applications involve end-to-end finetuning. It remains unclear whether SCL’s benefits persist when the backbone is finetuned on downstream tasks, or whether disentanglement constraints help or hurt transfer performance.\n3. **Pretraining data and SSL framing:** This setup differs from canonical SSL, which allows backbones to be pretrained on large unlabeled corpora and then transferred to labeled downstream tasks. However, pretraining here is performed on the source datasets which already have labels to leverage. With such a setup, there is no reason not to simply evaluate SOTA DG methods on the source data.\n4. **No Limitations Analysis of zs-zd Disentanglement:** The assumption that semantic and domain information should be strictly independent for robust generalization is actively debated (key focus of causal-based DG methods e.g., “Causality Inspired Representation Learning” Lv et. al., 2022). For example, in many visual settings, domain-specific cues can also be semantically informative (e.g., texture or color for animal categories). The paper does not analyze such trade-offs, nor cases where zs-zd independence could suppress useful signal."}, "questions": {"value": "1. Have the authors evaluated SCL on larger backbones (e.g., ResNet-50, ViT-B) or higher-capacity datasets (e.g., DomainNet, WILDS)? Would the disentanglement objective remain stable and beneficial at those scales?\n2. How does SCL perform when the pretrained encoder is fully finetuned on a supervised task rather than linearly probed?\n3. How does SCL’s effectiveness scale with fewer or more source domains? The current experiments (4 domains) are fixed; analyzing this could reveal the method’s data efficiency.\n4. What is the intended pretraining scenario in practice? Could SCL be combined with a related larger unlabeled corpus (domain-labeled or not)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FXXEK40zGx", "forum": "f6DsglOKIq", "replyto": "f6DsglOKIq", "signatures": ["ICLR.cc/2026/Conference/Submission5018/Reviewer_qLCo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5018/Reviewer_qLCo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977668640, "cdate": 1761977668640, "tmdate": 1762917823527, "mdate": 1762917823527, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Structured Contrastive Learning (SCL), a framework for learning disentangled semantic and domain representations from multi-domain data. The key idea is to jointly learn 1.semantic representation $z_s$ via semantic contrast, 2. domain representation $z_d$ via domain contrast, and 3. their disentanglement by minimizing mutual information $I(z_s; z_d)$. The authors provide theoretical analysis showing that minimizing $I(z_s; z_d)$ improves OOD generalization, and empirically validate SCL on MNIST-C, Rotated MNIST, PACS, and ADNI datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Problem Formulation: The paper addresses a genuine problem in multi-domain SSL - the conflation of semantic and domain factors leading to poor OOD generalization. The motivation is well-articulated.\n2. The theoretical framework is mathematically sound.\n3. Comprehensive Evaluation: The paper evaluates across multiple modalities (images, tabular data) and consistently shows improvements over baselines."}, "weaknesses": {"value": "1. Limited Novelty of Core Components: The main contribution is combining InfoNCE and HSIC. \n2. The proof technique is standard - combining MMD bounds with Kantorovich-Rubinstein duality offers no new insight"}, "questions": {"value": "1. Can you provide more discussions on the training dynamics that actually lead to disentanglement?\n2. Please add DomainBed benchmark."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ougnpW6MMO", "forum": "f6DsglOKIq", "replyto": "f6DsglOKIq", "signatures": ["ICLR.cc/2026/Conference/Submission5018/Reviewer_WRvS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5018/Reviewer_WRvS"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5018/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003596452, "cdate": 1762003596452, "tmdate": 1762917823233, "mdate": 1762917823233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}