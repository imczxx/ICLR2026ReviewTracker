{"id": "Twcq83ljWz", "number": 22111, "cdate": 1758326180682, "mdate": 1759896885981, "content": {"title": "Uncertainty Drives Social Bias Changes in Quantized Large Language Models", "abstract": "Post-training quantization reduces the computational cost of large language models but fundamentally alters their social biases in ways that aggregate metrics fail to capture. We present the first large-scale study of 50 quantized models evaluated on **QuantizedBiasBench**, a unified benchmark of 13 closed- and open-ended bias datasets. Despite minimal changes in aggregate bias scores, we identify a phenomenon we term *quantization-induced behavior flipping*, where up to 38% of responses switch between biased and unbiased post-quantization. These flips are strongly driven by model uncertainty, where responses with high uncertainty are 3-11x more likely to change than confident ones. Quantization strength amplifies this effect, with 4-bit quantized models exhibiting 4-6x  more behavioral changes than 8-bit quantized models. Critically, these changes create asymmetric impacts across demographic groups, where bias can worsen by up to 18.6% for some groups while improving by 14.1% for others, yielding misleadingly neutral aggregate outcomes. Larger models show no consistent robustness advantage, and group-specific shifts vary unpredictably across model families. Our findings demonstrate that compression fundamentally alters bias patterns, necessitating crucial post-quantization evaluation to ensure reliability in practice.", "tldr": "Post-training quantization causes up to 21% of LLM responses to flip between biased and unbiased, driven by model uncertainty rather than size, creating hidden asymmetric impacts across social groups that standard evaluation metrics completely miss.", "keywords": ["model quantization", "fairness", "benchmarking"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1869b80de3a17db05b254d386759492c91b36d30.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how post-training quantization affects social bias in large language models through a large-scale empirical study. The authors curate QuantizedBiasBench by aggregating existing bias datasets and evaluate instruction-tuned models with 5 different quantization methods, resulting in various quantized variants. The core finding is that quantization induces what the authors term \"behavior flipping,\" where up to 38% of responses switch between biased and unbiased states post-quantization. These changes are strongly correlated with model uncertainty, with high-uncertainty predictions more likely to flip than confident ones. The paper also reveals asymmetric impacts across demographic groups, where aggregate metrics mask substantial group-specific changes. The authors argue that 8-bit quantization is more stable than 4-bit methods and that model size provides no consistent robustness advantage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper makes several valuable empirical contributions to understanding quantization's effects on model behavior. The scale of the evaluation is impressive, which provides a level of comprehensiveness missing from prior work. The identification of uncertainty as the primary driver of bias changes is an important finding that could inform future quantization strategies. The use of geometric mean probability for closed-ended response selection is a methodologically sound approach that addresses known issues with next-token probability selection and parsing failures.\n\n+ The discovery of asymmetric social group impacts is significant from a fairness perspective. The paper demonstrates that aggregate metrics can mask substantial harm to specific demographic groups, which has serious implications for deployment. The finding that 8-bit quantization consistently shows 4-6× fewer behavioral changes than 4-bit methods provides practical guidance. \n\n+ The methodology is rigorous, employing permutation-based significance tests with multiple comparison adjustments and effect size calculations. The authors' decision to ensure paired responses before and after quantization enables proper causal inference about quantization effects."}, "weaknesses": {"value": "- The most significant weakness lies in what the authors claim as a primary contribution: QuantizedBiasBench. While the paper frames this as a novel unified benchmark, it is essentially an aggregation of 13 existing datasets without any new data collection, annotation, or curation beyond simple concatenation. Since quantized models use the same inference interface as their unquantized counterparts—only the underlying parameters differ—it remains unclear why existing benchmarks cannot be directly applied. The paper does not articulate what specific challenge necessitates creating this aggregate benchmark rather than simply evaluating on each constituent dataset independently. This raises questions about whether QuantizedBiasBench constitutes a genuine contribution or merely a repackaging of existing resources.\n\n- The use of LLaMA Guard 3 8B for classifying bias in open-ended responses introduces a potential confound, as this classifier may itself be affected by the same biases being measured. The paper does not validate LLaMA Guard's reliability across different demographic groups or assess whether its judgments are consistent with human annotations. For datasets like FMT10K where the paper only evaluates the \"Interference Misinformation\" subset, the rationale for this selection is not adequately justified.\n\n- The analysis conflates statistical significance with practical significance. While the paper reports that 11.4% of aggregate metric changes are statistically significant after multiple comparison correction, it does not establish what magnitude of bias change would constitute a practically meaningful harm. The focus on Cohen's d effect sizes helps but lacks grounding in real-world impact assessments. The paper also does not address how practitioners should interpret the finding that effects are zero-centered—does this mean quantization is \"safe on average\" despite high variance?"}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HQaNgenHxu", "forum": "Twcq83ljWz", "replyto": "Twcq83ljWz", "signatures": ["ICLR.cc/2026/Conference/Submission22111/Reviewer_ghdj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22111/Reviewer_ghdj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760672499801, "cdate": 1760672499801, "tmdate": 1762942068424, "mdate": 1762942068424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines how model quantization affects social bias in large language models (LLMs). Rather than simply measuring performance degradation, the authors analyze how quantization can change bias direction and magnitude across demographic attributes. Using several bias benchmarks and multiple quantization levels, the study finds that quantization changes model bias patterns, sometimes amplifying, attenuating, or even reversing the direction of bias. The authors attribute these shifts to increased prediction uncertainty and representation distortion after quantization, arguing that uncertainty mediates how bias manifests. They further analyze correlations between uncertainty metrics and bias changes, and propose uncertainty-aware decoding as a potential stabilizer.\n\nAs the authors pointed out, the idea of connecting bias and quantization has been explored before in several papers. The main contribution to me is the strong execution of the empirical analysis and organized summary of how quantization reshapes biased behaviors."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive empirical study and strong execution: \nThe paper conducts a systematic evaluation of how quantization affects bias behavior across multiple LLMs, quantization levels, and bias benchmarks. The experiments are well-organized and presented clearly, allowing readers to trace how quantization alters model outputs in nuanced ways. This level of empirical thoroughness is rare in the fairness–quantization intersection. The paper is well-written, transparent about its setup, and likely reproducible too.\n\nMeaningful interpretation of the bias-quantization relationship and strong implications: \nThe paper’s empirical findings go beyond surface-level correlations: they reveal consistent, interpretable patterns in how quantization reshapes bias behavior. The observation that bias direction and magnitude shift systematically with changes in predictive uncertainty provides a concrete, data-driven lens for understanding compression side effects. These patterns can inform future research by serving as diagnostic signals and by motivating new approaches to quantization or calibration that explicitly preserve fairness properties. In this sense, the results themselves are a valuable contribution, mapping a space that future theoretical and mitigation studies can build upon.\n\nRelevance and timeliness: \nWith growing interest in efficient LLM deployment, understanding fairness and bias behavior under quantization is practically important. This study provides a useful empirical foundation for the community, even if it lacks algorithmic innovation."}, "weaknesses": {"value": "Mostly an empirical study and limited novelty: \nThe paper offers a careful empirical analysis but no substantial methodological or theoretical innovation. The link between quantization, calibration drift, and fairness degradation has been noted in prior work, and the “uncertainty drives bias change” framing mainly restates these existing insights. \n\nNo actionable mitigation or theoretical advancement: \nThe study refines measurement rather than introducing new modeling ideas or actionable mitigation strategies. This limits its scientific and practical contribution.\n\nScope limited to established benchmarks and settings: \nThe experiments rely on standard bias datasets and do not explore real-world, multilingual, or compositional contexts. As a result, the findings may not generalize to practical downstream applications where bias surfaces differently.\n\nCorrelation vs causation on the uncertainty causing bias change point: (which is actually one of the biggest contributions in this paper)\nThe paper’s central claim is that quantization -> higher uncertainty -> bias change. However, the experiments show that 1. quantization changes outputs, 2. quantization increases uncertainty, and 3. quantization changes bias. There doesn't seem to be a direct test showing causation. Maybe some type of controlled intervention, for example increasing uncertainty through noise or errors, and observing bias will prove this point more strongly."}, "questions": {"value": "Causation point from above: \nHow can we distinguish this from a correlation where both uncertainty and bias shift due to shared representational distortions?\nHave you attempted interventions that manipulate uncertainty directly (e.g., entropy regularization, confidence penalization) to observe causal effects on bias?\n\nRobustness of findings: \nTo what extent can variance in bias metrics be statistically explained by changes in predictive uncertainty? Are the direction reversals stable across random seeds, datasets, and decoding strategies, or are they stochastic artifacts of quantization noise?\nDo the observed bias shifts generalize across different quantization methods?\nCan you provide qualitative examples where bias direction flipped, and explain what linguistic or representational changes may underlie the shift?\n\nHigher level stuff: \nThe results suggest that quantization changes bias patterns, not consistently amplifies them. How should we interpret such reversals in terms of fairness?\nBeyond post-hoc decoding, do your findings suggest principles for designing fairness-preserving quantization methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ATXtBcKU7Q", "forum": "Twcq83ljWz", "replyto": "Twcq83ljWz", "signatures": ["ICLR.cc/2026/Conference/Submission22111/Reviewer_nrYG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22111/Reviewer_nrYG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760835231014, "cdate": 1760835231014, "tmdate": 1762942068103, "mdate": 1762942068103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the effect of post-training quantization on the social bias of large language models. The authors selected a set of evaluation benchmarks and metrics and evaluated a large set of models on them. They show that model uncertainty is the primary driver of the \"behavior flipping\" phenomenon, where responses change post-quantization. The study demonstrates that aggregation-based metrics are not capable of showing the full extent of this behavioral change. The models do not simply become more or less fair; rather, quantization causes their behavior to change in more complex and asymmetric ways."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strength of this paper is its extensive study. The authors clearly invested significant time to investigate this phenomenon. They experimented with a large number of datasets, models, and quantization methods, and they evaluated the results using statistical tests. This thoroughness is valuable."}, "weaknesses": {"value": "- The core idea, that model compression affects social bias due to uncertainty of the models, is not entirely new. Prior work, like Gonçalves & Strubell (2023), Zakizadeh et al. (2023) and Delobelle and Berendt (2022), has shown similar effects for distillation and other compression methods. This study feels like an extension of this known phenomenon to a wider set of quantization techniques. However, I appreciate that the authors clearly position their work among these related studies.\n\n- The abstract's main message is difficult to understand quickly. The term \"quantization-induced behavior flipping\" is vague. I suggest the authors state their main finding more directly. For example, something like: \"We find that aggregate bias metrics are misleading because quantization causes models to change their behavior completely, in ways aggregate scores do not capture.\"\n\n- The main finding that increased model uncertainty leads to flipped decisions seems somewhat expected. When a model is compressed, it is logical that its least confident predictions would be the first to change. I am more curious about related factors, such as the effect of generation temperature. For example, are quantized models less stable, meaning more likely to change their answers in different sessions? Figure 2(b) seems to suggest the overall uncertainty distribution is similar, but this could be explored further.\n\n- As a minor issue, I am not sure if it is intended, but in the references, the Qwen model technical report (cited as Qwen et al. in Line 253) lists the model name as the first author, which is an unusual format for a citation.\n\n\nReferences: \n- [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://aclanthology.org/2023.emnlp-main.161/)\n- [DiFair: A Benchmark for Disentangled Assessment of Gender Knowledge and Bias](https://aclanthology.org/2023.findings-emnlp.127/) \n- [FairDistillation: Mitigating Stereotyping in Language Models](https://arxiv.org/abs/2207.04546)"}, "questions": {"value": "- The paper states that high-uncertainty responses are more likely to flip. Based on this, I expected the \"response changed\" (blue) distribution in Figure 2(a) to be skewed to the right (high uncertainty) and the \"response unchanged\" (light orange) distribution to be skewed to the left (low uncertainty). However, the plots do not all show this clear separation. Could the authors clarify this? Also, please specify if the y-axis represents the raw count of samples or a normalized density (percentage)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F1zSNtqNW3", "forum": "Twcq83ljWz", "replyto": "Twcq83ljWz", "signatures": ["ICLR.cc/2026/Conference/Submission22111/Reviewer_7VRD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22111/Reviewer_7VRD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934801096, "cdate": 1761934801096, "tmdate": 1762942067806, "mdate": 1762942067806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate the role of post-training quantization methods in large language models (LLMs), and how quantization can affect the social bias of models, i.e. biases with respect to protected social groups. The authors propose a unified benchmark of 13 existing bias datasets, and evaluate 50 quantized models on this dataset across a broad range of post-quantization methods and representation lengths. The authors use this analysis to make several observations they claim to be novel: \n\n1. The changes of social bias are asymmetric across different social groups\n2. Up to 21% of responses flip between biased and unbiased states post-quantization, and this is invisible in aggregate metrics. \n3. The amount of response flip between biased and unbiased states is correlated with model uncertainty and quantization strength\n4. The amount of response flip is uncorrelated with model size."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is overall well-written and presented, with clear claims of contributions and explanation of methodology/experimental setting.\n* The background is very well written and appears to cover much of the relevant literature on quantization and bias in LLMs (if not uncertainty/bias as discussed in weaknesses)\n* The paper is very well motivated with post-quantization of LLM models being pervasive in application and research, although the study of bias in this context is far from a unique motivation at this point.\n* The author's point that existing papers present conflicting findings is interesting and rings true, and motivates well a broader study of more quantization methods across more large models.\n* Three \"capabilities\" or bias metrics were defined that appear reasonable, if not exhaustive.\n* The main finding that the authors claim as novel, i.e. the correlation between uncertainty and bias, is interesting and intuitive."}, "weaknesses": {"value": "* Several of the claimed contributions in terms of conclusions appear far from novel, in particular:\n  - per-group evaluation differs from aggregate evaluation significantly --- this is what motivates all work fairness/bias analysis.\n  - impact on different social groups is asymmetrical --- again this seems to be a consistent finding in other work\n\n  I think if the authors instead claim to have replicated these findings at a larger/broader scale than existing work, that would be fair, but that's not how they frame it currently.\n* The main finding that the authors claim as novel, i.e. the correlation between uncertainty and bias, might be intuitive but of course is very far from causal. And yet unfortunately much of the working in the paper is clearly claiming a causal relationship, e.g. on pg 5: \"Our analysis reveals that uncertainty is the primary driver of quantization-induced bias changes\". As clearly worded here, this is a massive and dangerous over-claim that is not supported by evidence, and is not the only time the wording suggests it.\n* The proposed dataset is of course a meta-dataset, in that the authors have put together a dataset of existing datasets, and would not stand alone as a dataset contribution. I'm aware the authors do not seek this, and a meta-dataset is a valid contribution, but I believe it's important to be explicit as to the strength of the contribution on this front.\n* There is existing literature linking confidence calibration/uncertainty and bias, for example \"Uncertainty-based Fairness Measures\" by Kuzucu et al. in 2023 suggests that a better metric of fairness is if a model has similar uncertainty across social groups. While I believe the author's observation is novel in the context of quantization, the authors have not made it clear enough in the paper why this is a significant finding given that we know uncertainty and bias are linked, and that quantization can change the uncertainty of a model.\n\n\nComments to improve the paper:\n* Check all phrasing of claims of confidence/bias and insure they are rephrased such that there is no causal claim.\n* Explain the less novel-sounding findings like aggregate metrics being different than per-group, in a context where they are potentially novel (e.g. scale of number of models/different quantization methodology, etc).\n* I'm somewhat surprised that there is no effort to demonstrate that confidence calibration has an effect on bias as the authors themselves suggest for future work, even if it's in a smaller toy-like setting. This would go a long way further in making the observation and contribution significant."}, "questions": {"value": "* What is preventing demonstrating the effect of confidence calibration in a toy-like setting? I'm simply trying to understand if there is a better way to explain why it wasn't in the scope of this work, as it's not clear to the reader as things stand why it wasn't done.\n* Is there any evidence to support the existing wording of a causal link between uncertainty and bias?\n* Why is this a significant finding in the context of quantization given the existing work pointing to the correlation between uncertainty and fairness/bias in the general case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "yeDagE3PU3", "forum": "Twcq83ljWz", "replyto": "Twcq83ljWz", "signatures": ["ICLR.cc/2026/Conference/Submission22111/Reviewer_oEM7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22111/Reviewer_oEM7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762026479614, "cdate": 1762026479614, "tmdate": 1762942067467, "mdate": 1762942067467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}