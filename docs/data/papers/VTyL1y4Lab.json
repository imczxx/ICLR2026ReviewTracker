{"id": "VTyL1y4Lab", "number": 7924, "cdate": 1758043519781, "mdate": 1759897822128, "content": {"title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning Recipe for Strong Prompt Injection", "abstract": "Prompt injection poses a serious threat to the reliability and safety of LLM agents. Recent defenses against prompt injection, such as Instruction Hierarchy and SecAlign, have shown notable robustness against static attacks. However, to more thoroughly evaluate the robustness of these defenses, it is arguably necessary to employ strong attacks such as automated red-teaming. \nTo this end, we introduce RL-Hammer, a simple recipe for training attacker models that automatically learn to perform strong prompt injections and jailbreaks via reinforcement learning. RL-Hammer requires no warm-up data and can be trained entirely from scratch. To achieve high ASRs against industrial-level models with defenses, we propose a set of practical techniques that enable highly effective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR against GPT-4o with the Instruction Hierarchy defense.\nWe further discuss the challenge of achieving high diversity in attacks, highlighting how attacker models tend to ``reward-hack'' diversity objectives. Finally, we show that RL-Hammer can evade multiple prompt injection detectors. We hope our work advances automatic red-teaming and motivates the development of stronger, more principled defenses.", "tldr": "we train LLM to come up with better prompt injection attacks", "keywords": ["Prompt Injection", "Adversarial Attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/97ab8f85762286f1e80b4516fce8cdcb9b07a4d0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses generating strong prompt injections to show the weakness of existing detectors and defenses. The method builds on existing GRPO attacking method with a bag of tricks to make it work for robust models. The results show high ASR, low detectability, and high diversity (as assessed by common metrics)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The work studies prompt injection from various perspectives such as ASR, diversity, and stealthiness.\n1. The work includes a detailed qualitative study of the generated attacks, which shows interesting phenomena, such as reward hacking around the diversity measures.\n2. Study on realistic agentic systems and scenarios such as those in AgentDojo."}, "weaknesses": {"value": "1. I have concerns regarding the algorithmic novelty of this work. While it is definitely pretty encouraging to see good results, the method is basically just a bag of existing tricks applied over existing works to make them work for stronger models. It is also unclear how the work developing on Guo et al. (2025) overcomes their requirement for a warm-up dataset and achieves better results. Many of the results are less theoretically justified, e.g., why despite removing KL-divergence term, the model doesn't include any incoherent/partially incoherent or even language switch within prompt injection?\n2. The writing misses important clarifications and definitions. E.g.:\n    1. Clarify state and action space for the RL problem. What all can the RL agent do? How does that action space change for AgentDojo?\n    2. Why is prompt injection the focus, when the solution is general and can generate jailbreaks too?\n    1. What are the \"Enhanced\" and \"Llama-3.1-8B\" attack methods? They are not explained in the main text.\n    4. The order of presenting the experiments can be adjusted to make the important result of comparison with Guo et al and detectability be shown earlier.\n2. I believe the following experiments are important to include for a comprehensive study - \n    1. Influence of $\\alpha$ on trained policy: It would be important to understand how $\\alpha$ affects the ASR for both the robust and easy models. An ablation study on that would be nice.\n    2. Compare \"restricted format\" with penalizing length of generated prompt injections."}, "questions": {"value": "1. How is the stealthiness reward formulated?\n2. L322 - 333: how is attacker instruction as user instruction, an upper bound of the attack?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FMLtxEX8mI", "forum": "VTyL1y4Lab", "replyto": "VTyL1y4Lab", "signatures": ["ICLR.cc/2026/Conference/Submission7924/Reviewer_iL2z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7924/Reviewer_iL2z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970512846, "cdate": 1761970512846, "tmdate": 1762919947350, "mdate": 1762919947350, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new training strategy, RL-Hammer, to train attacker models in the black-box prompt injection attack setting. The training process adopts a revised GRPO algorithm, avoiding the need for a warm-up dataset or an additional value model. The attack prompts generated by RL-Hammer show strong effectiveness and transferability across a range of commercial-level LLMs like Llama3.2, Secalign, GPT-4o, and Gemini. The paper also evaluates several prompt injection detectors including perplexity-based filters, PromptGuard, ProtectAI-Guard, and LLM-based judges and shows that these detectors are not robust enough for defense, especially under an adaptive attack that uses the detector's output as a reward."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. RL-Hammer demonstrates strong effectiveness against many state-of-the-art commercial (closed-source) models.\n\n2. The proposed training techniques (removing KL, joint training, soft/format rewards) are intuitive and supported by ablations. They substantially improve GRPO’s ability to learn strong prompt injections.\n\n3. The paper shows RL-Hammer can produce diverse attack strategies under different reward signals, which makes the attack more flexible and powerful."}, "weaknesses": {"value": "1. The paper doesn’t clearly define its threat model for the black-box setting. Without explaining the attacker’s goals, knowledge, and capabilities, it’s hard to tell whether the proposed attack truly makes sense or matters in practice.\n\n2. The transferability shown in Table 1 is not good when the training-time target model has a significantly different structure from the test-time model.\n\n3. The paper only tests a few defense methods, so the claim that “current defenses are not enough” feels less convincing.\n\n4. The baseline attacks used for comparison are quite basic, which makes it hard to judge how much better RL-Hammer actually is compared with stronger black-box attack methods.\n\nTypo: “Attack Type” in Table 1 should probably be “Attack Method”."}, "questions": {"value": "Threat model\n\nIt appears the paper primarily targets the backend LLM itself rather than the full agent pipeline. These are two different threat scenarios and should be distinguished:\n\nTargeting the LLM directly. In this scenario, the attacker behaves like a malicious user who submits crafted prompts to the model and causes it to produce improper outputs. This setting corresponds to a black-box jailbreak attack.\n\nTargeting LLM-based agents. Here the attacker is not the ordinary user who queries the agent. A few questions therefore arise:\n\n- How does the attacker actually inject adversarial prompts into the agent’s tool responses?\n\n- Does the attacker need knowledge of or control over the tools the agent uses, and is that realistic in practice?\n\n- Training RL-Hammer appears to require many queries. Can you provide a concrete example or realistic scenario in which an attacker could repeatedly query an LLM agent enough times to train the attacker model?\n\nDefenses\n\nBesides the detectors and Meta-SecAlign evaluated in the paper, there exist other defenses reported to be effective against advanced prompt injection, such as Attention Tracker [1] and DataSentinel [2]. Could you report RL-Hammer’s performance against these defenses?\n\nBaselines\n\nThere are other stronger black-box attack approaches that seem relevant. For example, Tree-of-Attack [3], AutoDAN-Turbo [4]. Please clarify how the baselines were selected and, if possible, evaluate RL-Hammer against stronger black-box methods.\n\nReferences\n\n[1] Hung, Kuo-Han, et al. \"Attention tracker: Detecting prompt injection attacks in llms.\" arXiv preprint arXiv:2411.00348 (2024).\n\n[2] Liu, Yupei, et al. \"DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks.\" 2025 IEEE Symposium on Security and Privacy (SP). IEEE, 2025.\n\n[3] Mehrotra, Anay, et al. \"Tree of attacks: Jailbreaking black-box llms automatically.\" NeurIPS 37 (2024): 61065–61105.\n\n[4] Liu, Xiaogeng, et al. \"Autodan-turbo: A lifelong agent for strategy self-exploration to jailbreak llms.\" arXiv preprint arXiv:2410.05295 (2024)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o98KmQrB9n", "forum": "VTyL1y4Lab", "replyto": "VTyL1y4Lab", "signatures": ["ICLR.cc/2026/Conference/Submission7924/Reviewer_4D3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7924/Reviewer_4D3v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985539861, "cdate": 1761985539861, "tmdate": 1762919946731, "mdate": 1762919946731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces RL-Hammer, a simple reinforcement learning recipe for training attacker models to perform strong prompt injection and jailbreak attacks entirely from scratch using GRPO. The method demonstrates that recent defenses like Instruction Hierarchy and SecAlign, previously thought robust, can be reliably bypassed—achieving 98% ASR against GPT-4o with Instruction Hierarchy and high success rates across multiple commercial models. Key technical contributions include removing KL regularization, joint training on easy and robust targets with soft rewards, and enforcing restricted output formats. The paper also analyzes diversity (showing reward-hacking behavior) and detectability (attacks evade most detectors), highlighting fragility of current defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Strong results, for example showing 98% ASR on GPT-4o and strong cross-model transferability.\n- Comprehensive experimental evaluation across three different datasets (InjecAgent, AgentDojo, AdvBench) with consistent strong performance.\n- Interesting ablation studies which justify each technique and discussion of diversity/detectability."}, "weaknesses": {"value": "- **Query Efficiency**: The method requires a large number of queries to target models limiting real-world practicality, the paper does not provide enough details as to the cost both in training time and queries of the method.\n- **Diversity Analysis**: While the paper identifies reward-hacking on diversity metrics, it doesn't propose effective solutions for achieving genuine strategic diversity. This leaves a key weakness as this method is posed as an effective way to bypass defenses but undiverse attacks should be relatively easy to defend against either with some finetuning or by explicitily removing or teaching the model to ignore such attacks."}, "questions": {"value": "### Important\n\n1. Can you provide concrete numbers on training costs—number of queries, wall-clock time, and API costs for each target model? This is critical for understanding practical feasibility.\n2. Can you hypothesize why removing the KL term does not cause the model to collapse into gibberish? How does the policy naturally avoid unreadable outputs?\n3. Given that diversity metrics are easily reward-hacked, can you propose any alternative approaches for measuring or encouraging genuine diversity in attacks? Line 455 seems quite important and undiverse methods are quite easy to defend against, this also seems a bit counter to the statement in 427 that RL-Hammer is capable of generating diverse prompts overall. I'm curious if this method is defeneded against or is 'hacked' in the reward as not working, does RL-Hammer converge to a different approach?\n4. Throughout the paper, I did not really see an actual definition or example of how enforcing restricted format is implemented/used. Can you provide more details/an example?\n5. The joint training uses $\\alpha$ to weight rewards, how sensitive is the training to the selection of $\\alpha$. Can the authors suggest a general strategy for more than two models?\n6. Can you explain what the stealthiness reward is, this result is stated without numbers or methodology.\n\n\n### Minor\n\n[167] impedes\n\n[Table 1] The way this table is layed out is quite confusing. The left column is labeled attack method which is slightly confusing as there are models and methods presented, but also for RL-Hammer line 260 indicates that 98% attack success rate is obtained by joint training on Llama-3.1-8B-Instruct and GPT-4o; however, the table suggests that it is only GPT-4o. \n\n[Table 2] Can you provide more examples or diversity of lack of diversity?\n\n[Table 3] What is the Tool Knowledge attack, maybe I missed it but I didn't see it mentioned.\n\n[Table 3-4] What models are used for RL-Hammer for these tables"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LkCR2jVV6E", "forum": "VTyL1y4Lab", "replyto": "VTyL1y4Lab", "signatures": ["ICLR.cc/2026/Conference/Submission7924/Reviewer_MA3Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7924/Reviewer_MA3Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993679991, "cdate": 1761993679991, "tmdate": 1762919946315, "mdate": 1762919946315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use GRPO (with a few tricks) to fine-tune an LLM to be strong at writing prompt injection attacks. Their attacks are able to work on both open-source and production models. In addition, they show that existing LLM-based detectors are often incapable of detecting their attacks, but at the same time the attacks collapse into one specific mode per training run."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I think the paper has a technically sound contribution in showing that popular LLMs are easy to prompt inject using a trained attacker models. The results are shown across quite a few different datasets and models, and the paper raises a few questions worthy of future exploration (e.g., how to properly train for diversity)."}, "weaknesses": {"value": "I think the technical contribution of this work could be far stronger. At the end of the day, this work is running off-the-shelf GRPO from Huggingface with a few hyperparameter changes for the task of jailbreaking. It is easy to imagine many extensions that could make the work stronger, e.g, does performing adversarial training on the attacks generated from this method lead to stronger robustness? Are adversarially-trained models still easy to break with learned attackers? Do the attacks generalize to black-box production models without using direct query access? etc. A lot of these directions would also further underscore various research directions such as the need for attack diversity."}, "questions": {"value": "* I would be curious to see how the attacks qualitatively evolve during the course of training. \n* I am curious to see the types of reward hacks that the model does for attack diversity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "043IFRhzLl", "forum": "VTyL1y4Lab", "replyto": "VTyL1y4Lab", "signatures": ["ICLR.cc/2026/Conference/Submission7924/Reviewer_GfC1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7924/Reviewer_GfC1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7924/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136295792, "cdate": 1762136295792, "tmdate": 1762919944565, "mdate": 1762919944565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}