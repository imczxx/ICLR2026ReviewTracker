{"id": "219Pn3D4Md", "number": 20635, "cdate": 1758308416842, "mdate": 1759896966836, "content": {"title": "IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation", "abstract": "As the mathematical capabilities of large language models (LLMs) improve, it becomes increasingly important to evaluate their performance on research-level tasks at the frontier of mathematical knowledge.\nHowever, existing benchmarks are limited, as they focus solely on final-answer questions or high-school competition problems.\nTo address this gap, we introduce IMProofBench, a private benchmark consisting of 39 peer-reviewed problems developed by expert mathematicians.\nEach problem requires a detailed proof and is paired with subproblems that have final answers, supporting both an evaluation of mathematical reasoning  capabilities by human experts and a large-scale quantitative analysis through automated grading. Furthermore, unlike prior benchmarks, the evaluation setup simulates a realistic research environment: models operate in an agentic framework with tools  like web search for literature review and mathematical software such as SageMath.\nOur results show that current LLMs can succeed at the more accessible research-level  questions, but still encounter significant difficulties on more challenging problems. Quantitatively, Grok-4 achieves the highest accuracy of 52% on final-answer subproblems, while GPT-5 obtains the best performance for proof generation, achieving a fully correct solution for 22% of problems.\nIMProofBench will continue to evolve as a dynamic benchmark in collaboration with the mathematical community, ensuring its relevance for evaluating the next generation of LLMs.", "tldr": "We introduce IMProofBench, a peer-reviewed, tool-augmented, multi-turn benchmark of 39 research-level math problems that hybridizes human and automatic grading to assess LLM proof-writing.", "keywords": ["benchmark", "mathematical reasoning", "Large Language Models", "agentic evaluations"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa62b3c8115699eb1cfc4021a813578d0de77880.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces IMProofBench, a benchmark for evaluating LLMs on research-level mathematical proof generation rather than only final answers. The current release contains 39 peer-reviewed problems contributed and reviewed by professional mathematicians, each paired with auto-gradable follow-up subquestions. Models are evaluated in an agentic framework with access to tools (Python, Bash/GAP/Maxima, SageMath, and web search) and generous token budgets; main proofs are graded by the problem author on a 0–3 scale, while follow-ups are auto-graded. On this benchmark, GPT-5 achieves complete, fully correct proofs on 22% of tasks; Grok-4 attains the best final-answer accuracy (52%). There are also some qualitative and quantitative analyses showing that the final answer evaluations are informative but insufficient, LLMs are still struggling with open problems, etc."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- **Clear problem framing & High-fidelity evaluation setup:** Directly targets proof writing at the research level, filling a documented gap left by final-answer benchmarks and high-school/undergrad datasets. Agentic environment with real tools (SageMath, Python, web search), submit tool to separate reasoning from final answers, and substantial token budgets appropriate for deep math. This successfully simulates the real mathematician's working scenario.\n- **Quality control & community process:** Problems are authored and peer-reviewed by domain experts; the site/workflow formalizes submission, review, and grading instructions.\n- **Forward-looking & contamination-aware:** Authors are considering dynamic retirement of problems to maintain fair evaluations. Human mathematicians in the loop could make the LLM evaluation promising and truly tailored to the needs of human users."}, "weaknesses": {"value": "- **Tool-access parity and fairness:** Different models use different web-search providers (internal vs external) and may leverage shell utilities to fetch PDFs, raising parity questions about retrieval power across vendors\n- **Potential grader bias & reliability issues:** Main proofs are graded by the problem’s author; without inter-rater studies, there’s risk of variability or unconscious bias."}, "questions": {"value": "- **Contamination controls:** For the dynamic problem management system, how will you detect that a problem has become easier due to new publications, and how often will items be rotated or retired?\n- **Open-problem protocol:** Since no open problems were solved here, will future releases track partial advances on open questions differently (e.g., author judgment of novelty), and how will you guard against over-claiming?\n- **Scaling human effort:** As the benchmark grows, how will you mitigate grader load while preserving rigor (e.g., rubric refinements, rubric-guided LLM pre-screening, partial formalization)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "X3UzgwvR7v", "forum": "219Pn3D4Md", "replyto": "219Pn3D4Md", "signatures": ["ICLR.cc/2026/Conference/Submission20635/Reviewer_H7Lc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20635/Reviewer_H7Lc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760561586442, "cdate": 1760561586442, "tmdate": 1762934034520, "mdate": 1762934034520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "IMProofBench contains 39 problems authored and reviewed by expert mathematicians; each main question asks for a full proof and comes with FA subquestions for scalable scoring. Evaluation occurs in an agentic framework with Python, Bash, SageMath, and web search; models submit a final answer distinct from intermediate reasoning; token budgets are 300k for the main question and 100k per follow-up. Reported headline numbers: GPT-5 achieves 22% complete solutions on proof tasks; GROK-4 achieves 52% FA accuracy on the subset with follow-ups; the correlation between human proof scores and FA scores is 0.45.       \n\nThe paper positions itself against prior math benchmarks focused on high-school/undergrad or FA-only tasks, and argues for proof-based, research-level evaluation with human graders. Core contributions are explicitly listed as the benchmark itself, a cross-model proof study, and a qualitative analysis of errors and strengths."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear gap and strong motivation. The need to evaluate research-level proof writing rather than only FA questions is well motivated in the abstract and introduction. The benchmark directly targets that gap and brings expert graders into the loop. \n\n- Realistic evaluation environment. The agentic setup—with Python, SageMath, Bash, and web search—matches how a human would work. The required “submit” step separates scratch work from the final proof, which helps graders. The token budgets are large enough to explore tools without being trivially restrictive. \n\n- Thoughtful human grading design. Authors grade main proofs on a 0–3 progress scale and also tag error types (logic, concept, hallucination, calculation) and achievement indicators (understanding, insight, usefulness). This captures more signal than a single pass/fail.   \n\n- Quantitative and qualitative results. The paper reports both proof-score distributions and FA accuracy, plus the 0.45 correlation between FA and human proof scores, making a concrete case that FA alone is an imperfect proxy. The qualitative section usefully documents typical failure modes (confident but incorrect claims, “well-known result” shortcuts) and behavior (rare abstention).   \n\n- Problem sourcing and review. Problems go through an authoring pipeline with reviewer feedback and acceptance only after concerns are resolved; guidelines stress PhD-level insight and proof-centric tasks, with FA subquestions for automated scoring."}, "weaknesses": {"value": "- Private and evolving dataset. The benchmark is private, with plans to open-source the platform code and release open sample problems; for ICLR, private data can limit reproducibility and independent verification. Because the set is evolving, comparability across time may drift unless a frozen v1.0 test split is committed. \n\n- Scale and composition. N=39 is small for a general benchmark, and the paper itself highlights this. The topic distribution currently leans toward areas favored by organizers. Both factors raise questions about coverage and selection bias relative to “research-level mathematics” as a whole. \n\n- Human-in-the-loop reliability. The protocol records rich grader signals, but the paper does not report inter-rater reliability or consistency checks across graders for the same solution. Without IRR, it is difficult to quantify grading variance and potential systematic bias.\n\n- Agent and tool confounds. Allowing web search and shell access reflects reality, but it also introduces test-time retrieval confounds (e.g., locating key lemmas on the web). The appendix even notes models downloading papers via wget/curl. This is realistic but complicates claims about reasoning versus retrieval. A controlled ablation (no web; no Bash; SageMath only; different token budgets) would clarify how much performance comes from tool-mediated lookup versus algebraic reasoning. \n\n- Evaluation fairness across model APIs. The paper mentions that some models (e.g., GROK-4) give short final answers and have hidden reasoning tokens, complicating assessment. Tiering and API differences can affect both behavior and resource use. More detail on API normalization and prompting parity would strengthen fairness claims. \n\n- Statistical characterization. The headline metric “% complete solutions” on 39 items is informative but noisy. Confidence intervals, per-area breakdowns, and problem-level difficulty calibration would help. The 0.45 correlation between FA and proof scores is useful; expanding this analysis (e.g., partial correlations by topic, difficulty, or tool usage) would improve interpretability. \n\n- Reproducibility timing. The paper commits to releasing platform code and open sample problems before Nov 30, 2025, which would ease review concerns if delivered, but program committees must judge the submission as it stands. A clear plan for a frozen public subset and exact grader guidelines would materially improve reproducibility."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "g7Hly5ot6q", "forum": "219Pn3D4Md", "replyto": "219Pn3D4Md", "signatures": ["ICLR.cc/2026/Conference/Submission20635/Reviewer_Bjxz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20635/Reviewer_Bjxz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821866482, "cdate": 1761821866482, "tmdate": 1762934033881, "mdate": 1762934033881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents **IMProofBench**, a private, evolving benchmark aimed at **research‑level, proof‑centric** evaluation of LLMs in an agentic environment with tools (Python, SageMath, Bash, web search) and human expert grading on a 0–3 rubric (peer‑reviewed problem pipeline; automated grading for follow‑up subquestions). Key results: among 39 problems (with 79 subquestions), GPT‑5 achieves 22% complete proofs, while GROK‑4 leads on final‑answer subquestions with 52% perfect accuracy (Figs. 5 & 4; §§3–4; App. E; pp. 4–9, 21–23). Overall, the work targets an under‑served evaluation axis—proof quality rather than answer‑only scoring (motivation and setup; §1–§3; Fig. 3), but it currently has limited scale, private data, and heterogeneous tool/search access that may confound cross‑model comparisons (State & limits; §§3.3–3.4; App. A/E; pp. 5–9, 14–24)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Proof‑centric, research‑level focus**\n\nArticulates a gap in answer‑only benchmarks and centers proof writing with human expert grading (Intro; §1; §3.3), clarifying capabilities beyond final answers (novel evaluation dimension). Problems span pure/applied topics; 7 are flagged “open,” signaling frontier intent (State; §3.4; p. 6)—useful for probing limits.   - Worked sample (stable graphs) illustrates target rigor and expected solutions (App. C; pp. 19–20), aiding clarity and reproducibility expectations. \n\n2. **Documented pipeline and blind expert grading**\n\nTwo‑stage review with admin + domain expert; acceptance only after “no further comments” (Fig. 2; §3.2; pp. 4–5) → quality control.\n\n3. **Balanced quantitative and qualitative evaluation**\n\nReports proof vs final‑answer outcomes and a 0.45 correlation between author‑weighted subquestion score and 0–3 progress (§4.1; Figs. 4–5; p. 7) → answer accuracy ≠ proof quality."}, "weaknesses": {"value": "1. **Private dataset limits replication and audit**\n- Dataset is private; only code + sample problems planned for release (Reproducibility; p. 10) → third‑party re‑grading is constrained.\n- No public leaderboard or downloadable test set is provided (No direct evidence found in the manuscript) → weakens community tracking.\n- Human‑graded proofs cannot be independently re‑assessed at scale without access to items/outputs (§3.3; p. 5–6; Reproducibility; p. 10).\n2. **Small scale and topical skew** \n- 39 problems from 23 contributors is modest for statistical comparisons (§3.4; p. 6) → low power. \n- Tag distribution dominated by algebraic geometry (App. A Fig. 12; pp. 14–15) → representativeness concerns. \n- Only 7 “open” problems (§3.4; p. 6), so “research‑level” spans a broad difficulty band → interpretability issues. \n3. **Relation to similar benchmarks is not fully disentangled**\n- FrontierMath also targets advanced/research‑level problems but emphasizes final‑answer evaluation; fairness concerns around access are documented (Related Work §2; arXiv; EpochAI clarification). The paper cites FrontierMath (§2) but lacks side‑by‑side comparisons or controlled discussion beyond final‑vs‑proof framing (No direct evidence of direct cross‑eval in manuscript)."}, "questions": {"value": "Can you situate IMProofBench against FrontierMath/RealMath/HLE/UQ/USAMO/PutnamBench/MiniF2F with a small cross‑evaluation or calibrated narrative to clarify novelty and complementarity? (Related Work §2)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CMf4lSFWQ6", "forum": "219Pn3D4Md", "replyto": "219Pn3D4Md", "signatures": ["ICLR.cc/2026/Conference/Submission20635/Reviewer_YB3b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20635/Reviewer_YB3b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987569293, "cdate": 1761987569293, "tmdate": 1762934032970, "mdate": 1762934032970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Reply to all reviewers addressing shared concerns"}, "comment": {"value": "We thank the reviewers for their feedback. We are happy to hear that they found our problem creation pipeline rigorous (KECi, YB3b, Bjxz, H7Lc), our qualitative and quantitative analysis interesting (Bjxz, YB3b, xDmc), and our realistic evaluation environment thorough (Bjxz,xDmc). We have extracted three main concerns that are shared by most reviewers that we will answer in this main reply.\n\n[Note on revision of our paper: the reviews raised excellent points, which we are addressing with several new experiments and evaluations (ablation tests for model scores given fewer or no tools, additional grading and reliability analysis, and various additional statistical tests). Due to additional prior time commitments of the main authors, we were not able to finish a revision incorporating all the new content this week, but will upload a full revision of the paper by November 28th at the latest. We apologize for the inconvenience.]\n\n**Q1. Why did you opt for private benchmarking?**\n\nWe chose private over public benchmarking for three main reasons:\n* **Contamination Prevention**: Each of our questions takes considerable effort to create, with a rigorous review policy in place to ensure their high quality. Therefore, they are not easily replaced by newer questions, making any form of contamination detrimental for performance measurements.\n* **Web Search is Enabled**: We enable web search in our agentic setup, which we would need to disable if our benchmark questions were public. Indeed, once our questions appear online, there is no sufficiently rigorous way to prevent models from finding solutions.\n* **Work by Contributors**: Contributors often submit questions from research projects they are currently working on. Since this work is not published, it is unreasonable to ask our contributors to allow us to publish these questions, as their own work could suffer from this.\nFinally, we note that this decision is in line with other research-level benchmarks, such as FrontierMath [1] and MathScienceBench [2].\n\n**Q2. How does the small scale affect your analysis?**\n\nWhile the small scale somewhat limits our analysis, we find that this is the case for many benchmarks in the mathematical area. For instance, AIME 2025 and HMMT 2025 (two benchmarks reported on by almost all major model providers) only contain 30 questions each. USAMO 2025 and IMO 2025 (the former reported on in the release of Gemini-2.5-Pro Deep Think [3] and Grok-4 [4], and the latter used in evaluation by OpenAI and Gemini in July [5]) only contain six questions. Similarly, FrontierMath only contains 50 questions in its advanced Tier 4 subset and the more recent IMOBench [6] only contains 60 questions in its ProofBench (which is autograded). Therefore, the community seems to prefer a small set of very high-quality questions, rather than a larger set of more noisy samples. We take the same approach, implementing rigorous reviewing of submitted questions to ensure high quality, rather than going for the easier option that scales better.\n\nOf course, the limited size of our dataset does limit our overall analysis. For instance, we cannot provide breakdowns by topic in research-level areas, since we do not have enough questions to do so. However, our analysis also provides a lot of value: we can actively compare models, we are fully independent from model providers, and while small, 39 questions still gives us a lot of signal to compare models with. For instance, the fact that model answers could help in approximately 50% of its replies, is a valuable and interesting conclusion.\n\nWe also note that, at the time of writing, we have added a total of 10 further questions to the benchmark, increasing the number of questions from 39 to 49. We are in the process of making an active push to review, evaluate and grade several further questions during the next 7 days.\n\n**Q3. How does the current topical skew towards algebraic geometry affect your analysis?**\n\nThe reviewers correctly point that our current dataset is skewed towards algebraic geometry, mainly due to initial contributors being centered in that area. As our benchmark increases in size, we hope to rectify this skew by attracting more contributors from other areas as well.\n \nHowever, we do not believe that this skew currently affects our analysis in any significant way. First, our dataset does contain questions from a range of other mathematical areas. Second, while quantitative results could somewhat differ between mathematical areas, our qualitative observations are very likely to generalise beyond our current dataset.\n\n[1] https://epoch.ai/frontiermath\n\n[2] https://math.science-bench.ai/\n\n[3] https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/#flash-improvements\n\n[4] https://x.ai/news/grok-4\n\n[5] https://deepmind.google/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/\n\n[6] https://imobench.github.io/"}}, "id": "1pUY4q5Eak", "forum": "219Pn3D4Md", "replyto": "219Pn3D4Md", "signatures": ["ICLR.cc/2026/Conference/Submission20635/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20635/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20635/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763753401903, "cdate": 1763753401903, "tmdate": 1763753627662, "mdate": 1763753627662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IMProofBench, a private benchmark consisting of 39 peer-reviewed problems developed by expert mathematicians, designed to assess LLMs on research-level mathematical proof writing rather than just final-answer prediction. Each problem requires a detailed proof and is paired with subproblems that have final answers. The authors evaluate LLMs on IMProofBench in an agentic environment, giving models access to Python, SageMath, and web tools for reasoning and computation. Human mathematicians grade main proof tasks, while subproblems are auto-scored. Experiments show that GPT-5 achieves the best overall performance, solving 22% of full proofs, while GROK-4 leads in final-answer accuracy. Analysis reveals that LLMs are prone to reasoning errors, ranging from simple logical mistakes to deep misconceptions, and they frequently hallucinate existing results to obtain a flawed answer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The scope of the IMProofBench is novel, aiming to measure LLM's capability in research-level proof generation rather than just problem-solving accuracy.\n2. The problems peer-reviewed by mathematicians in IMProofBench would be a valuable resource for the LLM research community\n3. The evaluation is done in an agentic setting rather than pure LLM reasoning, reflecting a more realistic research workflow.\n4. The authors provide valuable insights into current frontier LLMs on research-level proof generation, reveals that LLMs are prone to reasoning errors, ranging from simple logical mistakes to deep misconceptions, and they frequently hallucinate existing results to obtain a flawed answer."}, "weaknesses": {"value": "1. The scale of IMProofBench is extremely small, containing only 39 problems.\n2. The reliance on experts for proof evaluation limits its scalability and usefulness for the research community in model evaluation and development.\n3. While the authors evaluate frontier LLMs in an agentic setting, it would be nice to compare their performance in a single-model reasoning mode without tools, to ablate the effect of tool use in the agentic setup.\n4. Each problem in IMProofBench is paired with subproblems that have final answers that can be auto-graded. What is the correlation between subproblem correctness and the correctness of the entire proof? Could this serve as a proxy for overall proof correctness when lacking human expert graders?"}, "questions": {"value": "1. While the authors mention community outreach to recruit professional mathematicians in the paper, what are their qualifications?\n2. Is there plan to host a leaderboard for IMProofBench? If so, do you need professional mathematicians to grade every submission?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wUDC0VzlJG", "forum": "219Pn3D4Md", "replyto": "219Pn3D4Md", "signatures": ["ICLR.cc/2026/Conference/Submission20635/Reviewer_xDmc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20635/Reviewer_xDmc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762040878041, "cdate": 1762040878041, "tmdate": 1762934032543, "mdate": 1762934032543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new benchmark that aims to evaluate LLMs performing research-level mathematics, with access to tools. The authors present a platform that they used to collect questions, as well as a question collection protocol. The benchmark currently contains 39 questions, and the authors state that more questions will be added. Several state-of-the-art LLMs are evaluated, and the authors describe various particularities of how the models reason, as well as promising results w.r.t. to solving rates (22% in case of GPT-5)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The question-creating pipeline is the strongest part of the paper. It is significantly more rigorous than what typical papers in the math benchmarking space achieve. The associate web codebase seems to be a genuine improvement that makes problem collection easier."}, "weaknesses": {"value": "- **tiny benchmark size**: My main issue that prevents me from recommending this paper for acceptance is the size of the dataset: 39 questions, even if very carefully authored, manually reviewed (and accounting for 79 follow-up questions), fall significantly short of the number of questions needed to rigorously establish a benchmark that, \"evaluates their [LLMs'] performance on research-level\ntasks at the frontier of mathematical knowledge\". To achieve this goal, many more data points need to be sampled, as each mathematical subdomain comes with its own issues (algebraic geometry exhibits different typical ways of reasoning than differential topology), which probably require a few dozen data points each to rigorously test.\n\n- **the growth targets will not solve the problems** For comparison, papers typically used thousands of datapoints when autograding is possible (e.g. https://arxiv.org/html/2501.13766v1), and many hundreds when solely manual evaluation is employed (e.g., https://arxiv.org/abs/2301.13867).  Growing the dataset by the numbers the authors mention will not bring them to this level.\n\n- **bias**: The word cloud (fig 12) shows a clear bias towards certain areas of mathematics; which is not surprising given the small sie of the benchmark.\n\n- **open questions are not a plus**: \"Of the 39 benchmark problems, authors characterize 7 as open research questions.\" I am unconvinced of the value of adding open research questions to a benchmark. The value of an open research question for a benchmark is only worthwhile if an LLM solves that question and thus highlights an awesome display of reasoning. Otherwise, the output of the LLM is hard to quantify (e.g., \"meaningful but ultimately insufficient progress towards a solution\")\n\n- \"Currently, IMProofBench consists of 39 problems developed in collaboration with over 23 mathematicians, with 30 more questions in the latest stages of the problem creation pipeline.\" I am not sure that the fact that \"more is coming\" (30 questions) is a good argument, as it is unclear when the questions will be devised. \n\n- **benchmark is not repeatable:** The fact that the authors evaluated commercial LLMs makes the subsequent use of these questions unsuitable for these LLMs, as they have likely been ingested in the training data. Further, a lot of space is used for a verbal assessment of how the models perform. While this provides a snapshot, this information is already folklore, given the many posts on X/Twitter; at the same time, this information will soon be outdated with the advancements of LLMs. The value of a benchmark lies mostly in the fact of being able to rank models across time (e.g., the success of ImageNet) - and this benchmark fails at that task. The authors state that their benchmark will grow over time, but simply scaling it will not resolve these problems.\n\n- **benchmark not public** In line with current trends, the authors do not make their benchmark public. This is claimed to preserve \"intellectual property\" of the creators (line 508). I am doubtful of this claim, as one could waive IP claims. This reduces reproducibility. I recommend the authors read on the FrontierMath debacle (https://www.reddit.com/r/math/comments/1iadcqw/the_frontiermath_scandal) that highlights the pitfalls of private benchmarks."}, "questions": {"value": "- Were some existing benchmarks, such as https://arxiv.org/html/2501.13766v1, deliberately omitted from the Related Work section? \n\n- \"high-school or undergraduate-level mathematics (Balunovic et al., 2025; Frieder et al., 2023)\" Some problems from the latter seem to be rather at the graduate level (at least in some universities' master curricula), as graduate-level textbooks feature in those benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4EuCLjOuZE", "forum": "219Pn3D4Md", "replyto": "219Pn3D4Md", "signatures": ["ICLR.cc/2026/Conference/Submission20635/Reviewer_KECi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20635/Reviewer_KECi"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762368894019, "cdate": 1762368894019, "tmdate": 1762934032143, "mdate": 1762934032143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}