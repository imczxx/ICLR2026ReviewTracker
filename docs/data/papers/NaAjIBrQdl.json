{"id": "NaAjIBrQdl", "number": 8301, "cdate": 1758077943384, "mdate": 1759897793364, "content": {"title": "STARTrack:Learning Spatio-Temporal Representation Evolution for Target-Aware Tracking", "abstract": "Efficient modeling of spatio-temporal representations in videos is crucial for achieving accurate object tracking. Existing popular one-stream tracking frameworks typically introduce memory mechanisms or specialized modules for temporal modeling. However, due to the gradual degradation of the initial template and unstable updates of target representations, their performance often deteriorates over time. To address this issue, we propose a simple yet effective video-level tracking framework, STARTrack, which realizes the temporal evolution of target and context representations through an iterative token propagation mechanism. Our framework takes as input the features of the search region along with two types of tokens that carry historical representations, and employs a visual encoder for joint modeling. This design enables target-aware perception and adaptively fuses current and historical representations. The proposed method explicitly avoids the sustained reliance on the initial template during long-term tracking, without introducing additional complex context inputs or motion modeling modules, thereby achieving faster inference. Furthermore, we develop a training strategy tailored to our framework. It enhances the semantic coherence of target representations over time via a representation consistency constraint, and for the first time explicitly incorporates occluded frames into the training process. This guides the tracker to learn context representations that are highly correlated with the spatio-temporal state of the target, thereby reducing the reliance on target appearance itself. Extensive experiments on standard benchmarks demonstrate that STARTrack achieves state-of-the-art performance, while maintaining a favorable balance between accuracy and efficiency. The code will be released.", "tldr": "", "keywords": ["Visual tracking"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/acf8dd794dc366cac1abfbe54a6891f956ad3ee5.pdf", "supplementary_material": "/attachment/83b7566ca12cef1d0868741ae8187189867dede6.zip"}, "replies": [{"content": {"summary": {"value": "STARTRACK is a novel video-level object tracking framework that redefines visual tracking as a spatio-temporal representation evolution problem. Unlike traditional trackers that rely on static template matching or sparse temporal updates, STARTRACK introduces an iterative token propagation mechanism to dynamically model the evolution of both target and context representations across video frames.\n\nKey components include:\n\nTwo types of tokens: target tokens and context tokens, which store and propagate spatio-temporal representations.\n\nA dual-stream causal attention mechanism with negative attention guidance to ensure temporal consistency and avoid semantic entanglement.\n\nA Frame-wise Information Gain Principle (FIGP) to ensure high-quality token updates.\n\nDense frame sampling and explicit inclusion of occluded frames during training to enhance robustness.\n\nThe method achieves state-of-the-art performance on multiple benchmarks (LaSOT, LaSOText, GOT-10K, TrackingNet, UAV123, TNL2K) while maintaining high inference speed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. State-of-the-Art Performance Across Diverse and Challenging Benchmarks\nSTARTRACK does not merely achieve top results on one or two benchmarks; it demonstrates generalized superiority across a wide spectrum of challenges, which is a strong indicator of its robustness.\n\nLaSOT & LaSOText: Achieving 75.2% AUC on LaSOT and 53.2% on LaSOText is significant because these are large-scale, long-term benchmarks. LaSOText, in particular, with its focus on similar distractors and frequent occlusions, directly validates the core claim that STARTRACK excels at spatio-temporal modeling and discrimination beyond simple appearance matching.\n\nGOT-10K: The high score of 78.5% AO under the strict one-shot protocol (training only on GOT-10K's training split) is a powerful testament to the model's generalization capability. This shows that the learned representation evolution strategy is not overfitted to specific object categories.\n\nUAV123 & TNL2K: Superior performance on UAV123 (aerial perspective, small objects, fast motion) and TNL2K (diverse media including cartoons) proves the framework's adaptability to different domains and data sources, moving beyond conventional RGB video.\n\n2. A Novel and Paradigm-Shifting Pipeline: From Matching to Evolution\nThe most profound contribution is the conceptual shift from a static matching paradigm to a dynamic evolution paradigm.\n\nBeyond Template Degradation: Traditional trackers, even those with dynamic template updates, fundamentally perform matching. STARTRACK abandons this entirely. Its iterative token propagation mechanism allows the tracker to build a continuously updating \"memory\" of the target and its relationship with the environment. This explicitly mitigates the Achilles' heel of long-term tracking: the gradual irrelevance of the initial template.\n\nToken as a Dynamic State Vector: The target and context tokens act as a compact, learned state vector that carries all necessary historical information. This is more elegant and potent than hand-crafted update strategies for multiple templates or complex cross-frame attention mechanisms, leading to a simpler yet more effective architecture.\n\n3. A Holistic and Innovative Training Strategy\n\nDense Sampling with Occlusion: By using densely sampled sequences and, for the first time, explicitly including occluded frames in training, the model is forced to learn a crucial skill: reasoning without appearance. This trains the context tokens to capture the underlying spatial structure and motion patterns of the scene, enabling the tracker to hypothesize the target's location even when it is invisible.\n\nFrame-wise Information Gain Principle (FIGP): This is a clever solution to a key problem in propagation-based models: ensuring that each update is beneficial. FIGP provides a self-supervised, internal consistency signal that actively prevents representation collapse and encourages the tokens to become progressively more informative, ensuring stable long-term performance."}, "weaknesses": {"value": "1. Insufficient Depth in Related Work on Temporal Modeling\nThe paper's review of existing temporal methods is somewhat narrow, missing a discussion of several influential works that would provide a richer context for its contributions. PrDiMP (Probabilistic Regression and DIMP)，STMTrack (Spatio-Temporal Memory Trackers)，TCTrack (Temporal Context Trackers)，MeMOTR (Memory-Augmented MOT with Transformers)\n\n2. Limited Analysis of Failure Modes and Robustness Boundaries\nThe paper convincingly demonstrates success but offers less insight into its limitations.\n\nExtreme Deformation or Fast Motion: How does the token propagation mechanism cope when the target undergoes radical non-rigid deformation or moves so fast that its appearance changes drastically between frames? The dense sampling may help, but the upper limits are not explored.\n\nFull Scene Changes: What happens when the camera cuts to a completely different scene (a common challenge in long-term TV show tracking)? The reliance on spatio-temporal context would likely break down, and it's unclear how the model would recover.\n\nInitialization Sensitivity: While an ablation on target token initialization is provided, a deeper analysis of how sensitive the entire tracking process is to errors or noise in the initial bounding box is missing.\n\n3. Practical Deployment Considerations\n\nComputational Cost of Dense Sampling: While the inference FPS is high, the training cost of using densely sampled sequences is significantly higher than sparse sampling. The paper does not discuss the computational overhead of this training strategy.\n\nHyperparameter Sensitivity: The performance appears sensitive to hyperparameters like the token matrix size (Fig. 5) and sampling length (Fig. 6). This suggests that optimal deployment on a new dataset might require non-trivial tuning, potentially limiting its applicability."}, "questions": {"value": "1. Insufficient Depth in Related Work on Temporal Modeling\n2. Limited Analysis of Failure Modes and Robustness Boundaries\n3. Practical Deployment Considerations"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "rBZvmianoA", "forum": "NaAjIBrQdl", "replyto": "NaAjIBrQdl", "signatures": ["ICLR.cc/2026/Conference/Submission8301/Reviewer_cD5U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8301/Reviewer_cD5U"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760460092102, "cdate": 1760460092102, "tmdate": 1762920229461, "mdate": 1762920229461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a simple yet effective video-level tracking framework termed STARTrack, which realizes the temporal evolution of target and context representations using an iterative token propagation mechanism. It takes two types of tokens that carry historical representations to update the static target templates over time for target state inference. The tracker has been evaluated on various public benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The writing is clear and method is easy to follow. The introduction of two distinct types of tokens to decouple and model the target's appearance and environmental relationship seems reasonable.\n2. The paper provides extensive quantitative results on the public tracking benchmarks, demonstrating the method's robustness, and efficiency."}, "weaknesses": {"value": "1. The strategy of using dynamic template tokens to propagate temporal information is not very novel. Numerous prior works like HIPTrack, SPMTrack also employ token storage and temporal information propagation techniques to improve the performance. The proposed dual-stream token mechanism is an incremental combination of existing ideas rather than a groundbreaking new paradigm. \n2. The performance gain compared to SPMTrack in Table 1 is marginal.\n3. The in-depth analyses of the two types of tokens have not been clearly provided."}, "questions": {"value": "1. Could the authors provide more detail analyses or visualizations of the two types of tokens?\n2. What’s the reason that the AUC drops significantly (from 73.8% to 44.7%) when the Frame-wise Information Gain Principle (FIGP) is removed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Tt9PH53I4J", "forum": "NaAjIBrQdl", "replyto": "NaAjIBrQdl", "signatures": ["ICLR.cc/2026/Conference/Submission8301/Reviewer_LieF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8301/Reviewer_LieF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924961821, "cdate": 1761924961821, "tmdate": 1762920229138, "mdate": 1762920229138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "STARTrack is a video-level visual object tracking framework that departs from static template-matching paradigms by explicitly modeling the spatio-temporal evolution of both target and context representations. The method uses an iterative token-propagation design: two kinds of learnable representation tokens (target tokens and context tokens) are propagated across frames and fused with current-frame visual features through a dual-stream causal attention mechanism. The model also incorporates negative attention guidance to encourage context tokens to focus on spatio-temporally relevant cues. To support stable temporal updates, STARTrack introduces a tailored training strategy centered on the Frame-wise Information Gain Principle (FIGP), dense sampling, and occlusion-aware training; losses include a temporal refinement term that enforces that current-frame predictions should improve (or not be worse than) those from previous tokens. Experiments on multiple benchmarks (LaSOT, GOT-10K, TrackingNet, TNL2K, UAV123, LaSOText) report state-of-the-art accuracy while maintaining competitive inference speed. Ablations validate the benefits of dual token types, dense sampling, FIGP, and occlusion inclusion during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Iterative token-propagation for spatio-temporal representation evolution: STARTrack proposes propagating learned target and context representation tokens across frames and fusing them with current-frame features. This token-centric propagation explicitly models representation evolution over time and reduces dependence on a fixed initial template, addressing long-term drift.\n\n- Dual-stream causal attention with negative attention guidance: The architecture separates modeling of target and context into parallel causal attention streams to avoid mutual interference, and introduces negative attention guidance to help context tokens converge faster and focus on discriminative spatio-temporal context relevant to the tracked object.\n\n- Training innovations for temporal consistency and occlusion robustness: They formulate the Frame-wise Information Gain Principle and add a temporal refinement term to losses that enforces improvement (or non-degradation) of current predictions relative to previous tokens. They also densely sample frames and explicitly include occluded frames in training, helping the model learn context-driven localization when appearance cues are weak."}, "weaknesses": {"value": "- Complexity and interpretability of token dynamics: While token propagation is powerful, the paper relies on many design choices (two token types, negative guidance sign flips, dual-stream masking/ordering, initialization strategies). These choices introduce complexity; ablation shows sensitivity (e.g., FIGP removal causes large drops), but the conceptual interpretability and theoretical understanding of token dynamics and stability over very long sequences remain limited.\n- Dependence on many engineered training choices and hyperparameters: The method’s strong performance hinges on several specific training strategies (dense sampling, occlusion inclusion, FIGP temporal loss terms, representation initialization variants). This could make reproducibility or transfer to other datasets/domains sensitive to hyperparameters and dataset composition; the paper indicates some instability when modifying multi-frame variants.\n- Limited analysis of failure cases and computational trade-offs in diverse settings: Although STARTrack reports good FPS and benchmark numbers, the paper contains limited discussion of failure modes (e.g., heavy crowding, severe distractors, extreme viewpoint/scale changes) and how the token mechanism behaves in such scenarios. Also, while the framework avoids explicit motion modules to keep inference fast, more explicit comparison of compute/memory cost vs. competing long-term trackers (especially under very long sequences or constrained hardware) would strengthen practical claims."}, "questions": {"value": "SEE WEAKNESS"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NO"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rMefWNzGUR", "forum": "NaAjIBrQdl", "replyto": "NaAjIBrQdl", "signatures": ["ICLR.cc/2026/Conference/Submission8301/Reviewer_i6rU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8301/Reviewer_i6rU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972915021, "cdate": 1761972915021, "tmdate": 1762920228736, "mdate": 1762920228736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method to tackle object tracking problem. The proposes method introduces a few new designs in the network architecture to enhance the performance of object tracking. It adopts ViT as visual encoder to embed the input images, together with historic tokens into embeddings. Secondly, it introduces extra query tokens that helps to decouple semantic features. It also adopts dual stream attention mechanism to learn discriminative features from targets and contexts. Extensive experiments show that the proposed method outperforms baseline methods on the benchmark quantitatively."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The quantitative results show that the proposed method performs the best compared to baseline methods."}, "weaknesses": {"value": "1. What are $H_1, H_2, ... H_n$ in Eq.(2) ?   $H_i$ and $W_i$  are used to represent both image size and network parameters in a confusing way. \n2. What do symbols $q, k, v$ represent in Eq. (2)?  How do they differ from $Q, K, V$?\n3. Where are the dual stream attention shown in the Figure 2?  It's difficult to understand how the dual stream work in the entire network architecture. \n4. Section 3.3.2 Why do the query tokens avoid semantic entanglement? How do they achieve this goal?\n5. The experiments do not have any qualitative results."}, "questions": {"value": "The main issue of this paper submission is the presentation and the development of the paper content.  The organization and presentation of this manuscript is poor such that it's hard to follow the logic flow. The mathematical symbols are not properly defined and explained progressively. The figures and diagrams are not properly referred from the text.  The authors are suggested to paraphrase and polish the manuscript text carefully."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DpCBvYIu7V", "forum": "NaAjIBrQdl", "replyto": "NaAjIBrQdl", "signatures": ["ICLR.cc/2026/Conference/Submission8301/Reviewer_iYMN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8301/Reviewer_iYMN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8301/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194448550, "cdate": 1762194448550, "tmdate": 1762920228344, "mdate": 1762920228344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}