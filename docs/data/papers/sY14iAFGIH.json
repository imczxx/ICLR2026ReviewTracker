{"id": "sY14iAFGIH", "number": 22007, "cdate": 1758324746276, "mdate": 1759896891298, "content": {"title": "How Transformers Learn In-Context Recall Tasks? Optimality, Training Dynamics and Generalization", "abstract": "We study the approximation capabilities, convergence speeds and on-convergence behaviors of transformers trained on in-context recall tasks -- which requires to recognize the \\emph{positional} association between a pair of tokens from in-context examples.\nExisting theoretical results only focus on the in-context reasoning behavior of transformers after being trained for the \\emph{one} gradient descent step. It remains unclear what is the on-convergence behavior of transformers being trained by gradient descent and how fast the convergence rate is. In addition, the generalization of transformers in one-step in-context reasoning has not been formally investigated. This work addresses these gaps. We first show that a class of transformers with either linear, ReLU or softmax attentions, is provably Bayes-optimal for an in-context recall task. When being trained with gradient descent, we show via a finite-sample analysis that the expected loss converges at linear rate to the Bayes risks. Moreover, we show that the trained transformers exhibit out-of-distribution (OOD) generalization, i.e., generalizing to samples outside of the population distribution. Our theoretical findings are further supported by extensive empirical validations, showing that \\emph{without} proper parameterization, models with larger expressive power surprisingly \\emph{fail} to generalize OOD after being trained by gradient descent.", "tldr": "properly parameterized one-layer transformers are provably optimal and OOD generalizable for in-context reasoning tasks", "keywords": ["one-layer transformers", "out-of-distribution generalization", "approximation power"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebf5cabba0f8f98e4f87d0a0d643ac9b14d569f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors study a simple model of associative recall. They show how a simplified Transformer model may solve a recall task, but demonstrate empirically that a model trained from scratch does not generalize on this task unless parameterized in a specific way."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors study a timely and fascinating topic. Particularly as LLMs make their way into more and more aspects of our lives, understanding their basic capabilities is important."}, "weaknesses": {"value": "It remains unclear to me how novel the authors' contribution is. I'm not intimately familiar with this literature, but it seems to me many of the basic results the authors present have already been discovered and analyzed extensively before? \n\nSpecifically, the central claim seems to be that transformers solve an in-context recall task optimally. This seems to have been well studied and validated since Bietti et al (https://arxiv.org/abs/2306.00802). It seems the authors seem to claim novelty by asserting that their analysis studies multiple occurrences of multiple query tokens, but I'm unsure if Bietti et al's analysis is invalidated in this setting? See also Chan et al (https://arxiv.org/abs/2410.23042), which explicitly study the role of having multiple query tokens on associative recall. Also Reddy (https://arxiv.org/abs/2312.03002, https://arxiv.org/abs/2412.00104) performs a detailed theoretical analysis of this phenomenon. Many of these studies also consider softmax attention with noise. Also, the convergence result asserting a linear rate seems to be shown already in Huang et al (https://arxiv.org/abs/2409.17335), no? Is their result invalid in your setting?\n\nSeparately, the empirics demonstrating that a Transformer trained from scratch *fails* to generalize well on your task seems to be a severe weakness. LLMs are able to solve associative recall tasks presumably without your parametrizations. Wouldn't this suggest that the phenomena you characterize with your parametrizations are not reflective of actual models?\n\nA small aside, there seem to be frequent accidentally-omitted-words and grammatical typos that hinder your manuscript's clarity. You draft may benefit from a careful read-through to catch these typos."}, "questions": {"value": "See Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "E3yDvcy0WE", "forum": "sY14iAFGIH", "replyto": "sY14iAFGIH", "signatures": ["ICLR.cc/2026/Conference/Submission22007/Reviewer_e9iq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22007/Reviewer_e9iq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409825699, "cdate": 1761409825699, "tmdate": 1762942017180, "mdate": 1762942017180, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical and empirical analysis of how one-layer transformers learn in-context recall tasks—synthetic settings requiring recognition of positional token associations. The authors show (i) that such transformers are Bayes-optimal for both noiseless and noisy recall tasks, (ii) that gradient descent achieves linear convergence to the Bayes risk, and (iii) that the learned representations can generalize out-of-distribution (OOD) to unseen tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper extends prior studies that only analyzed the first training step or infinite-sample limits by providing finite-sample analyses, explicit reparameterizations, and empirical validations demonstrating when proper parameterization is crucial for OOD generalization\n\n2. This paper theoretically characterizes the different behaviors of the feed-forward layer and the attention layer in in-context recall tasks, which is insightful."}, "weaknesses": {"value": "1. Limited architecture depth: Results are confined to one-layer, single-head transformers, far from the multi-layer, residual, or multi-head dynamics that dominate real LLMs.\n\n2. The experiments in this paper focus on synthetic tasks; it would be better if the authors could consider real-world language tasks."}, "questions": {"value": "The authors claim that “without proper parameterization, models with larger expressive power surprisingly fail to generalize OOD after being trained by gradient descent.” However, real-world LLMs usually do not employ such specific parameterizations, then how do LLMs acquire their generalization ability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pkYOLoKain", "forum": "sY14iAFGIH", "replyto": "sY14iAFGIH", "signatures": ["ICLR.cc/2026/Conference/Submission22007/Reviewer_xnEr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22007/Reviewer_xnEr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966906508, "cdate": 1761966906508, "tmdate": 1762942016723, "mdate": 1762942016723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a comprehensive analysis of the approximation, optimization, and generalization problem of in-context reasoning tasks with Transformers. The studied Transformers include linear, relu, and softmax attention. The generalization also involves the out-of-domain case on unseen data. Some experiments are provided to support the reparameterization considered in this work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The setting of in-context reasoning is an important and interesting problem to study. \n\n2. The analysis is comprehensive, which contains many aspects of the theory."}, "weaknesses": {"value": "1. The analysis is simplified to consider only $\\lambda$ as the trainable parameter. This is too restrictive. \n\n2. The writing can be improved. Why not put real-world examples right after Definition 2.1? \n\n3. The experiments only show the necessity of reparameterization. However, I think it only applies to synthetic experiments with two-layer models. It is clear whether reparameterization is important in real-world experiments."}, "questions": {"value": "1. I am fine with experiments with synthetic data and settings. However, I feel experiments that can verify Theorems 5.4, 5.5, and Eqn. (4) are more interesting. I noticed the results of Figure 3. Why not put them in the main body?\n\n2. Can your analysis and the results be extended to multi-head and/or multi-layer Transformers?\n\n3. Why do your gradient updates need to be normalized?\n\n4. How do you motivate the reparameterization with only $\\lambda$ as the learnable parameter? Maybe you can cite some papers that support the formulation of $W$, e.g., some works [1, 2, 3, 4] show that attention scores are concentrated on tokens with the same feature as the query. \n\n[1] Huang et al., ICML 2024. In-context convergence of transformers.\n\n[2] Li et al., ICML 2024. How Do Nonlinear Transformers Learn and Generalize in In-Context Learning?\n\n[3] Li et al., ICLR 2025. Training nonlinear transformers for chain-of-thought inference: A theoretical generalization analysis.\n\n[4] Huang et al., ICLR 2025. A Theoretical Analysis of Self-Supervised Learning for Vision Transformers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sqtXxdahVY", "forum": "sY14iAFGIH", "replyto": "sY14iAFGIH", "signatures": ["ICLR.cc/2026/Conference/Submission22007/Reviewer_4JXf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22007/Reviewer_4JXf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014657179, "cdate": 1762014657179, "tmdate": 1762942016256, "mdate": 1762942016256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies a stylized in-context recall task and trains a one-layer decoder transformer with linear/ReLU/softmax attention under a reparameterization. It proves Bayes-optimality of the construction (noiseless and noisy), linear-rate convergence under normalized GD, and OOD generalization to unseen output words; it also gives a finite-sample guarantee and a result showing attention vs. FFN role separation at convergence."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear theoretical guarantees across three attention types. Linear/ReLU (Lemma 3.1, Thm. 3.2) and softmax (Lemma 4.1, Thm. 4.2) get explicit parameterizations with linear convergence proofs.\n\n- OOD to unseen outputs is formalized and proved in both noiseless and noisy settings (Thm. 3.3, Thm. 5.5).\n\n- Mechanistic interpretability hook: theorem showing attention predicts outputs while FFN handles noise after enough steps (Thm. 5.6)."}, "weaknesses": {"value": "1. The abstract states: Existing theoretical results only focus on the in-context reasoning\nbehavior of transformers after being trained for the one gradient descent step.\nThis is not correct. Several papers analyze full training dynamics over many GD steps and prove convergence (often linear/finite‐time), not merely “one step” (you cited the first one, and didn't cite the last two), e.g.:\n\n[1] Huang, Cheng & Liang (2023). In-context convergence of transformers. and In-context learning with representations: Contextual generalization of trained transformers. \n\n[2] Yang, Huang, Liang & Chi (2024). In-context learning with representations: Contextual generalization of trained transformers.\n\n[3] Shen, Zhou, Yang, Shen (2025). On the Training Convergence of Transformers for In-Context Classification of Gaussian Mixtures.\n...\n\n---\n\n2. It seems to me that there are some quantitative mismatch that makes some theorem statements numerically wrong (too optimistic) in their dependence on $|Q|$ and $t$:/\n\n- Theorem 3.2 (NGD dynamics): Main text states $\\lambda_{q,t} = \\eta t/|Q|$ and uses $L(\\lambda_t)=O(Ne^{-\\eta t})$. But the appendix’s NGD derivation (equal gradient components $\\Roghtarrow ||\\nabla L||_2 = \\sqrt{|Q|}|\\partial L/\\partial \\lambda_q|) gives $\\lambda_{1,t}=\\eta t/\\sqrt{Q}$. Elsewhere you even plug $\\lambda_{q,t}=\\eta t$. So the printed $|Q|$ and $\\eta t$ versions overstate how fast $\\lambda$ grows and thus how fast the loss decays.\n\n- In Theorem 3.3, The lower bound that uses $exp(\\eta t)$ should be using $\\exp(\\eta t/|Q|)$. As printed, it predicts higher accuracy sooner.\n\n- Theorem 4.2, a softmax loss bound is written as $O(Ne^{-t})$, but it should retain $\\eta: O(Ne^{-\\eta t})$.\n\n3. The task seems a bit artificial to me: The vocabulary is partitioned into trigger tokens $Q$ and output tokens $O$, plus a single “generic noise” token $\\tau$. Sentences are forced to contain at least one $(q,y)$ bigram; any $(q,x)$ bigram must have $x\\in\\{y,\\tau\\}; and the final position is fixed to the trigger $z_H=1$. This raises SNR but is artificial as a language model training distribution."}, "questions": {"value": "1. Can you compare with the missing literature? What are the novelty/contribution compared to them? e.g., the attention machanism seems very similar to Huang et al ([1], you both consider analyzing one matrix W, and for the softmax version, it's the same as Huang et al and converges to $s I$ with $s\\rightarrow \\infty$)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4D6U6vxFx4", "forum": "sY14iAFGIH", "replyto": "sY14iAFGIH", "signatures": ["ICLR.cc/2026/Conference/Submission22007/Reviewer_aJo2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22007/Reviewer_aJo2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22007/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762292793377, "cdate": 1762292793377, "tmdate": 1762942015617, "mdate": 1762942015617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}