{"id": "v7k84w4Rq5", "number": 12279, "cdate": 1758206820042, "mdate": 1759897520570, "content": {"title": "Progressive Acyclicity Induction for Effective DAG Learning under Heteroscedastic Noise Models", "abstract": "This study focuses on the heteroscedastic noise model (HNM), wherein an effect is a function of its cause and a Gaussian noise term whose variance depends on the cause. Integrating HNMs into a continuous optimization framework allows us to learn a causal directed acyclic graph (DAG) under an acyclicity constraint by maximizing a likelihood objective parameterized by both mean and variance. However, DAG learning under HNM inherit the challenges of gradient-based likelihood optimization: the gradient is scaled by the predictive variance, which introduces a new optimization issue in DAG learning under an acyclicity constraint. In early training, because the gradients of reconstruction loss is scaled by the predicted variance, it becomes heavily attenuated; as a result, the DAG parameters are updated primarily by the acyclicity constraint, hindering effective structure learning. To address this, we propose a graduated optimization strategy with weighted loss scheduling. We introduce a scheduling coefficient into the loss, starting with a high weight for stable mean and variance learning, then gradually lowering the coefficient to transition to the standard likelihood objective and enforce acyclicity. This approach ensures that the learned DAG more faithfully reflects the data. Experimental results on synthetic and real-world data show that our method outperforms existing approaches in terms of structure learning accuracy.", "tldr": "This work highlights a unique optimization challenge that arises specifically when learning DAGs under HNMs and proposes a novel optimization strategy to overcome this challenge.", "keywords": ["DAG learning", "graduated optimization", "heteroscedastic noise models", "loss weight schedule"], "primary_area": "causal reasoning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/88f945d456c43fb720028058f78714ffc92c9933.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of learning causal directed acyclic graphs (DAGs) under heteroscedastic noise models (HNMs), where noise variance depends on parent variables. The authors identify that when using negative log-likelihood (NLL) loss with gradient-based optimization, the acyclicity constraint can dominate early training due to variance-scaled gradients, hindering effective structure learning. To address this, they propose a graduated optimization strategy with weighted loss scheduling that uses a surrogate loss (combining MSE, variance regularization, and stop-gradient NLL) in early training, then gradually transitions to standard NLL."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies and empirically demonstrates (Figure 1, contribution ratio analysis) a specific optimization challenge in DAG learning under HNMs that hasn't been explicitly addressed before.\n2. The paper includes extensive experiments with ablation studies (Section 4.2), hyperparameter analysis (Appendix D), trajectory visualizations (Figures 10-11), and comparisons across multiple baselines.\n3. The proposed method achieves substantial improvements over baselines, particularly for larger and denser graphs, demonstrating practical value for the causal discovery community."}, "weaknesses": {"value": "1. Lemma 3.1 only provides a sufficient condition without proving it's satisfied during training.\n2. Claim 1 relies on multiple strong assumptions (uniform convergence, PL condition, \"slow transition,\" \"reconstruction-dominant phase\") that are neither formally defined nor empirically verified.\n3. The \"proof\" in Appendix E.6 is informal with undefined constants (C₁, C₂, C', κ).\n4. Paper suggests ranges (λ_reg(0)∈[100,130], t*∈[1000,1200]) but no automatic selection procedure.\n5. No guidance on adapting these parameters to different problem characteristics (graph density, sample size, etc.).\n6. The main contribution is the scheduling heuristic, but similar ideas exist in the graduated non-convexity literature.\n7. Section I.7 compares with methods (DARING, ReScore) designed for different problem settings (heterogeneous data vs. heteroscedastic noise). The comparison appears somewhat unfair, as these methods solve related but distinct problems, and there are missing comparisons with some recent gradient-based DAG learning methods.\n8. The contribution score metric (Eq. 5) needs a more intuitive explanation.\n9. For Claim 1, can you make the constants (C₁, C₂) explicit and empirically verify that the stated assumptions (uniform convergence bound, PL condition, etc.) actually hold in your experimental settings?\n10. How does the computational cost (wall-clock time, number of gradient evaluations) compare to baselines, especially ICDH, which also uses alternating optimization?\n11. Why does ICDH fail catastrophically at d=1000 (SHD=5158) while your method still performs reasonably (SHD=1130)? This difference seems surprisingly large and warrants investigation.\n12. The contribution ratio (Eq. 5) is a novel diagnostic. Have you considered using it as a stopping criterion or to adjust scheduling parameters during training adaptively?\n13. You mention connections to homotopy optimization and GNC, but the analysis is informal. Can you formalize this connection and leverage existing homotopy theory for convergence guarantees?"}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2pAGXq9KQC", "forum": "v7k84w4Rq5", "replyto": "v7k84w4Rq5", "signatures": ["ICLR.cc/2026/Conference/Submission12279/Reviewer_fhFt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12279/Reviewer_fhFt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12279/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761675880713, "cdate": 1761675880713, "tmdate": 1762923215221, "mdate": 1762923215221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses an optimization challenge in learning causal structure on Heteroscedastic Noise Models (HNMs). The paper identifies that during training with a negative log-likelihood (NLL) loss, the gradient from the reconstruction term is attenuated by the predicted variance. This allows the acyclicity constraint to dominate early optimization, hindering effective structure learning. To mitigate this, this paper proposes a novel graduated optimization technique that uses a weighted loss schedule. This method starts with a relaxed loss to focus on learning the mean and variance functions before gradually transitioning to the standard NLL and enforcing the acyclicity constraint. The proposed method is empirically shown to outperform existing baselines on synthetic and real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths\n\n- The analysis of how the variance term in NLL influences the reconstruction gradient and the acyclicity constraint is insightful.\n\n- The proposed graduated optimization technique is a straightforward yet well-motivated solution to the identified problem. \n\n- The paper provides comprehensive empirical evidence of the proposed method."}, "weaknesses": {"value": "Weaknesses\n\n- The contribution of this work is somewhat incremental. The work builds directly upon the continuous optimization framework and the recent HNM-based DAG learning approach of Yin et al. (2024). The core idea of a loss schedule or curriculum learning, while novel in this specific context, is a known technique in optimization. The advance is therefore more of an important and effective improvement to an existing line of research rather than a novel breakthrough.\n\n- The proposed solution is largely heuristic. The choice of the scheduling function and the specific rate at which the loss transitions are presented as design choices validated empirically, but they lack a rigorous theoretical justification. Moreover, it is unclear whether the change of the objective function to a surrogate will affect the identifiability of the HNM model."}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gdJmzOmlMa", "forum": "v7k84w4Rq5", "replyto": "v7k84w4Rq5", "signatures": ["ICLR.cc/2026/Conference/Submission12279/Reviewer_1251"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12279/Reviewer_1251"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12279/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789295958, "cdate": 1761789295958, "tmdate": 1762923214582, "mdate": 1762923214582, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identify a challenge of learning Directed Acyclic Graphs (DAGs) under a Heteroscedastic Noise Model (HNM), where noise variance depends on causal variables. The authors identify that during early training, the Negative Log-Likelihood (NLL) loss gradient is scaled by the predicted variance, weakening the reconstruction signal and allowing the acyclicity constraint to dominate prematurely, leading to suboptimal DAG structures. To address this, this paper propose a graduated optimization strategy with a weighted loss schedule. Initially, a surrogate loss (combining MSE, variance regularization, and a stop-gradient NLL) is used to stabilize mean and variance learning. And then it  transition to the standard NLL objective and progressively enforcing acyclicity. Experiments on synthetic and real-world data demonstrate improved structure learning accuracy over existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is very well written and easy to follow.\n1. The paper clearly identifies a previously unexplored optimization challenge specific to DAG learning under HNMs\n1. The paper introduces a new optimization strategies for nonlinear heteroscedastic noise model  via continuous optimization.\n1. The experimental results well support the claims."}, "weaknesses": {"value": "1. The proposed scheduling mechanism introduces additional hyperparameters (e.g., initial weight $\\lambda_{reg}(0)$, transition points $t$ and $t^*$), which require tuning and may affect reproducibility.\n2. The surrogate loss and scheduling strategy likely increase the computational cost and training complexity. It would be helpful to include an analysis of computational complexity and report runtime comparisons in the experiments.\n3. Including more baseline methods, such as [1, 2, 3], would strengthen the empirical evaluation.\n4. Beyond SHD and F1, additional metrics (e.g., FDR and TPR) could provide a more comprehensive evaluation of performance.\n\n\n\n[1] Chen, Weilin, et al. \"On the role of entropy-based loss for learning causal structure with continuous optimization.\" *IEEE Transactions on Neural Networks and Learning Systems* (2023).\n\n[2] Gao M, Ding Y, Aragam B. A polynomial-time algorithm for learning nonparametric causal graphs[J]. Advances in Neural Information Processing Systems, 2020, 33: 11599-11611.\n\n[3] Lachapelle S, Brouillard P, Deleu T, et al. Gradient-Based Neural DAG Learning[C]//International Conference on Learning Representations."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A5dhNEn1Ce", "forum": "v7k84w4Rq5", "replyto": "v7k84w4Rq5", "signatures": ["ICLR.cc/2026/Conference/Submission12279/Reviewer_tzGf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12279/Reviewer_tzGf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12279/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897503182, "cdate": 1761897503182, "tmdate": 1762923213728, "mdate": 1762923213728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenging optimization problem of performing continuous DAG learning under a heteroscedastic noise model. To mitigate the issue of the reconstruction loss constraint being overly attenuated, the paper proposes a graduated optimization strategy that gradually decreases the corresponding coefficients to better enforce both the likelihood objective and the acyclicity constraint. The method demonstrates strong performance on both synthetic and real-world datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-\tThis paper tackles a practical and existing challenge in continuous DAG learning under heteroscedastic noise.\n-\tThe paper provides a thorough discussion of relevant related works and compares the proposed method against appropriate baselines on a reasonable set of benchmark datasets."}, "weaknesses": {"value": "- **Overstated novelty and unclear theoretical distinctions:** Many of the proposed formulations and objectives closely resemble existing methods, raising concerns about the true conceptual contribution. Please see the question 6 and 8 in **Questions** section.\n- **Overclaims and inaccurate statements:** The paper contains several overclaims or incorrect assertions, such as implying that using MSE assumes equal noise variance, or misrepresenting how Yin et al. (2024) establishes identifiability and performs DAG recovery. These inaccuracies undermine the technical rigor and weaken the overall argumentation. Please see my specific questions below.\n- **Insufficient empirical and theoretical validation:** The paper lacks convergence guarantees or complexity analysis for the proposed algorithm, and some experimental results appear counterintuitive (e.g., heteroscedastic methods underperforming simpler baselines). These issues weaken the empirical credibility of the claimed advantages in both performance and efficiency."}, "questions": {"value": "1.\tIn lines 39–41, I do not think it is rigorous to claim that using MSE as the reconstruction loss implies an equal noise variance assumption. In particular, Zheng et al. (2020) considers various types of noise, not limited to Gaussian noise.\n\n2.\tIn lines 50–52, Yin et al. (2024) establishes identifiability by first providing sufficient conditions for the bivariate case and then extending them to the multivariate case using standard procedures from Peters et al. (2014). Algorithmically, it performs gradient-based continuous optimization to directly recover the DAG. In either case, it does not “first learn a causal ordering of the variables and then reconstruct the DAG via conditional independence tests.”\n\n3.\tIt seems somewhat overstated to claim that one of this paper’s contributions is identifying a new challenge specific to structure learning with HNMs. This optimization issue has been discussed previously, such as in Yin et al. (2024), though perhaps not as clearly formulated or presented as in this work. Moreover, the problem of variance explosion hindering accurate mean estimation under the NLL objective has been well-documented in prior studies. The paper’s novelty lies in proposing a new approach to address this issue within the context of DAG learning, rather than introducing it as a new challenge.\n\n4.\tWhat do the weights in the weighted matrix $W$ represent? In linear SEMs, these weights correspond to causal mechanisms, but since this paper focuses on the nonlinear setting, their interpretation is less clear.\n\n5.\t**[Very Important]** What is the difference between the **Theorem 2.1** and **Theorem 1** in Yin et al. (2024)? \n\n\n6.\tWhat are the specific choices of the functions $\\mu_j$ and $\\sigma_j^2$ in Eq. (2)? Also, when you mention that $W_j$ represents the directed connectivity from $X_{\\text{pa}_j}$ to $X_j$, does this imply that $W$ is a binary vector of 0s and 1s? If so, why is $W$ referred to as a weight matrix?\n\n7.\tI wonder if the authors could include a comparison between the proposed algorithm and that of Yin et al. (2024). My concern about novelty arises because, in essence, Eqs. (8) and (9) appear quite similar to the two-phase training objectives in Yin et al. (2024). In particular, the supposed novelty—the stop-gradient NLL—does not seem to contribute to the actual training process, but rather serves as an evaluation metric for model selection.\n\n8.\tSince the contributions are merely algorithmic, is there any convergence guarantee for the proposed algorithm? Does the algorithm guarantee to learn the stationary solution as all the existing DAG learning algorithm, or it can go beyond this limitation? What is the complexity analysis of proposed algorithm against its major competitor, ICDH? \n\n9.\tThe results in Figure 2 appear counterintuitive. Methods specifically designed for heteroscedastic data, such as HOST and ICDH, perform worse than NOTEARS-MLP, which completely ignores potential heteroscedasticity. These results are inconsistent with the original findings reported for HOST and ICDH. It is possible that both methods, being relatively complex, are highly sensitive to initialization and hyperparameter settings, which may have hindered their performance in this evaluation. Therefore, it may not be entirely fair to claim superiority in DAG learning accuracy based on these results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4cgWlWjklO", "forum": "v7k84w4Rq5", "replyto": "v7k84w4Rq5", "signatures": ["ICLR.cc/2026/Conference/Submission12279/Reviewer_9YYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12279/Reviewer_9YYo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12279/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762828573539, "cdate": 1762828573539, "tmdate": 1762923213226, "mdate": 1762923213226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}