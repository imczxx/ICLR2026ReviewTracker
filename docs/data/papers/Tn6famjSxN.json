{"id": "Tn6famjSxN", "number": 14260, "cdate": 1758231419025, "mdate": 1759897380355, "content": {"title": "Fresh in memory: Training-order recency is linearly encoded in language model activations", "abstract": "We show that language models’ activations linearly encode when information was learned during training. Our setup involves creating a model with a known training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but otherwise similar datasets about named entities. We find that the average activations of test samples corresponding to the six training datasets encode the training order: when projected into a 2D subspace, these centroids are arranged exactly in the order of training and lie on a straight line. Further, we show that linear probes can accurately (∼90%) distinguish “early” vs. “late” entities, generalizing to entities unseen during the probes’ own training. The model can also be fine-tuned to explicitly report an unseen entity’s training stage (∼80% accuracy). Notably, the training-order encoding does not seem attributable to simple differences in activation magnitudes, losses, or model confidence. Our paper demonstrates that models are capable of differentiating information by its acquisition time, and carries significant implications for how they might manage conflicting data and respond to knowledge modifications.", "tldr": "Training order is linearly encoded in LLM activations — specifically, *how recently* similar information was seen during training.", "keywords": ["Language models", "interpretability", "training dynamics", "representation learning", "memorization", "confidence", "knowledge awareness"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/826e32a112b421ed9d8291fdcbe7aded20378f96.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how the activations of large language models (LLMs) can be linearly encoded as new information is learned during training. They achieve this by sequentially fine-tuning Llama-3.2-1B on six distinct, non-overlapping datasets containing information about named entities. They found out very interesting insights such as the centroids of the average activations of test samples in 2D subspace lie on straightline, using linear probes can accurately distinguish between the early and late entities and also found that this training-order is not attributable from the differences in activation magnitudes, losses or model confidence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well motivated: it systematically explores how fine-tuning sequentially can lead to the fact that the model activations are linearly encode\n2. The finding of the paper is interesting: the paper introduce new kind of interpretability result and the setup of the experiment is very rigorous"}, "weaknesses": {"value": "1. I think that the evaluation is still a bit limited in the sense that the model being used is small scale <=8B. So using larger scale might be needed and also does this still remain true for model like reasoning model.\n2. I think it is fair to point out the attribute that does not cause the training-order such as activation magnitudes, losses or model confidence. However, it would be better if the author could also provide some evidence to why this interpretability result occur."}, "questions": {"value": "It would also be nice to read more discussion on the implications of these results. While the results are indeed interesting, would be nice to hear from the authors as to where these ideas can be used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fUv3vz0P6f", "forum": "Tn6famjSxN", "replyto": "Tn6famjSxN", "signatures": ["ICLR.cc/2026/Conference/Submission14260/Reviewer_KRf2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14260/Reviewer_KRf2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751476939, "cdate": 1761751476939, "tmdate": 1762924712412, "mdate": 1762924712412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors discover that, when training on multiple different datasets over time, the training order can be recovered using linear probes or a projection into a lower dimensional subspace. They rule out a variety of plausible explanations related to the training data through additional experiments and robustness checks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a surprising, clever finding that has implications for model security.\n\n- The authors rule out many plausible differences in datasets that could account for their findings.\n- I liked the experiment on the model being able to access this information. One would generally expect models to be able to access things they store linearly, but nice to see that this is true.\n- I also like the re-exposure experiments; those are pretty convincing that this is a real effect"}, "weaknesses": {"value": "- The biggest weakness I see is the lack of discussion or experimentation with the optimizer (see \"Questions\" below)\n\nSuggestions:\n- I found the PCA explanation (paragraph starting at line 142) to be fairly confusing. I think I get it, but had to read it multiple times. Not sure if I have a specific suggestion, but this could benefit from some rephrasing for clarity."}, "questions": {"value": "- I am not quite sure how to evaluate this paper because the finding is very weird and surprising and that may be sufficient to publish. Still, my first thought was \"Oh, this is probably some artifact from Adam or whatever the optimizer is (since the gradients have momentum, the learned features for batches seen close together probably have some optimizer artifacts).\" Looks like you trained with Adafactor; does this effect go away if you train with vanilla SGD? (More complicated: Could you save the optimizer state and \"force\" new data to be categorized with one of the datasets by training it with the same optimizer state?)\n- Can you elaborate on this? I don't think I understand: Interestingly, re-exposure fine-tuning sometimes reverses the order of the intermediate centroids, e.g. when re-exposing stage 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MQzDtU5LrP", "forum": "Tn6famjSxN", "replyto": "Tn6famjSxN", "signatures": ["ICLR.cc/2026/Conference/Submission14260/Reviewer_3wAz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14260/Reviewer_3wAz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795310819, "cdate": 1761795310819, "tmdate": 1762924711978, "mdate": 1762924711978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work demonstrates that large language models (LLMs) encode information about when they learned specific facts during training. Using sequential fine-tuning of Llama-3.2-1B (and Qwen) on six disjoint datasets, the authors show that test-sample activations form centroids aligned along a linear “training-order axis” that corresponds to the chronological order of fine-tuning stages. Linear probes and auxiliary fine-tuning tasks confirm that models can reliably identify whether information was learned early or late in training, suggesting that temporal metadata about learning is explicitly represented and accessible in model activations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper presents a useful and clearly defined research question, whether language models encode information about when they learned specific knowledge—addressing a fundamental aspect of LLM representation.\n\n- Features an extensive and rigorous experimental setup, involving sequential fine-tuning over multiple stages, diverse datasets (synthetic and natural), and multiple model families (Llama and Qwen), ensuring robustness and reproducibility.\n\n- Conducts careful and systematic controls for confounding factors, such as activation magnitudes, loss statistics, model confidence, and logit distributions, convincingly ruling out simple statistical artifacts as explanations.\n\n- The paper is exceptionally thorough and well-documented, with transparent methodology, detailed appendices, and strong visual evidence that make the results easy to interpret and verify."}, "weaknesses": {"value": "- The authors only evaluate models up to 8B parameters (with explicit mention of this), which limits the generality of the findings. Since larger models often exhibit qualitatively different internal representations, a replication of the core effect on a single ≥70B model would greatly strengthen the paper’s empirical claim and its relevance for state-of-the-art LLMs.\n\n- A potential data leakage or alias-recovery issue may undermine part of the conclusions. Although entity names were replaced with aliases, the content of the questions (e.g., birth dates, professions, regions) might allow the model to infer the original entities seen during pretraining. If so, the observed training-order signal could partially reflect pretraining familiarity rather than fine-tuning chronology. A simple control experiment, e.g., removing or masking date and other entities feature that are part of the questions, could rule out this confound and would make the findings substantially more convincing.\n\n- Minor: While the paper is methodologically sound, it is dense and somewhat overextended, making it difficult for readers to extract the central contributions from the many auxiliary experiments. A slightly more focused presentation might improve clarity and impact.\n\n-----\n\n\nOverall, this is a well-executed and carefully written paper with a compelling experimental setup and thoughtful analysis of an important phenomenon in model representations. I am in favor of acceptance at ICLR. With minor clarifications, particularly addressing possible data leakage and larger-model replication, I would be happy to revise my score to a strong accept during the rebuttal phase."}, "questions": {"value": "- Why did you use layer 13/16? Did you validate this design choice through an ablation?\n\n- In section 2 the authors say that most experiments use a single QA template never seen during fine-tuning. What experiments used more? Is it only the experiments in section 3.4 on template generalization?\n\n- At the end of Section 3.2, in the experiment about mixed-data. Did you try a 5-epochs additional stage with a downsampled data mix that is comparable to the original fine-tuning regime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TdM4ewtDWU", "forum": "Tn6famjSxN", "replyto": "Tn6famjSxN", "signatures": ["ICLR.cc/2026/Conference/Submission14260/Reviewer_Maoj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14260/Reviewer_Maoj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14260/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762021742977, "cdate": 1762021742977, "tmdate": 1762924711500, "mdate": 1762924711500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}