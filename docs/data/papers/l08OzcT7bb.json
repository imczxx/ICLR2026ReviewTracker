{"id": "l08OzcT7bb", "number": 6752, "cdate": 1757994568813, "mdate": 1763003816739, "content": {"title": "Trash to Treasure: Paving a New Way for Improving Video Understanding via Counterfactual Video Generation", "abstract": "Multimodal Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to ''visual ungrounded hallucination'', especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of generating and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to transform real-world videos into counterfactual scenarios. By embedding structured context throughout the editing and QA generation pipeline, it not only automates the generation of high-quality QA pairs but also inherently yields the paired data (original vs. edited) essential for contrastive training. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high‑quality QA pairs together with original–edited video pairs for contrastive training.\nBased on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. Besides, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase incorporates $\\ell_1$ normalization of advantages for each real-counterfactual pair, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0\\% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.", "tldr": "", "keywords": ["multimodal", "video understanding", "dataset and benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/844ee0619a4e9d88c07d0f98b63e53e385eb7d48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces DualityForge, a generative augmentation framework designed to reduce hallucinations in multimodal large language models (MLLMs). The core idea is to synthesize counterfactual videos using an off-the-shelf text-to-video diffusion model, which transforms real videos into controlled perturbations across three categories, visual (e.g., pixel-level distortions), semantic (e.g., appearance of unexpected objects), and commonsense (e.g., physically implausible object motions). Using this approach, the authors construct DualityVidQA, a paired dataset containing 144K realistic–counterfactual video pairs with automatically generated question–answer annotations. They further propose DNA-Train, a two-stage supervised fine-tuning (SFT) and reinforcement learning (RL) post-training procedure to teach MLLMs to detect anomalies (hallucinations). Experiments with Qwen2.5-VL-7B and LLaVA-Next-Video show substantial reductions in hallucinations across two hallucination-focused benchmarks, including one constructed by the authors, while maintaining or slightly improving performance on general-purpose video reasoning datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles a crucial and timely problem of hallucination reduction in MLLMs by proposing a generative counterfactual augmentation strategy that is both intuitive and generalizable.\n- The idea of constructing structured counterfactual perturbations at visual, semantic, and commonsense levels is original and nicely bridges data-centric and model-centric mitigation approaches.\n- The proposed DNA-Train post-training strategy yields clear gains on multiple hallucination-oriented benchmarks, while avoiding catastrophic forgetting on general reasoning tasks.\n- The overall framework is technically sound and well-motivated."}, "weaknesses": {"value": "- The paper’s central hypothesis that hallucinations primarily arise from language priors within MLLMs is plausible but not convincingly validated in the context of this work. Although the results demonstrate reduced hallucinations, the paper lacks direct evidence showing that DualityForge specifically mitigates this root cause rather than merely benefiting from additional post-training. Including illustrative qualitative examples or control experiments would strengthen this argument.\n- The category-wise analysis of counterfactuals (visual vs. semantic vs. commonsense) is missing. Since these categories likely engage different levels of reasoning (from perceptual recognition to causal inference), an ablation or per-category breakdown would provide valuable insight into what kinds of hallucinations the model actually learns to avoid.\n- The use of the term “counterfactual” could be more rigorously justified, as some generated perturbations (e.g., brightness changes) may be better described as synthetic noise rather than true counterfactuals. Clarifying this conceptual distinction would improve precision.\n- The paper overlooks recent hallucination evaluation benchmarks such as MHBench (Kong et al., 2025) and VidHalluc (Li et al., 2025), which are highly relevant for contextualizing the contribution and ensuring fair comparison.\n\nMinor Issues:\n- Verify the publication status of cited preprints (e.g., Zhang et al., 2025a appeared in ICLR 2025).\n\nMissing References:\n- Kong et al. MHBench: Demystifying Motion Hallucination in VideoLLMs. AAAI 2025.\n- Li et al. VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding. CVPR 2025."}, "questions": {"value": "1. Regarding your claim on mitigating language priors: how can we ensure that the observed gains stem from the counterfactual augmentation strategy, rather than simply from additional fine-tuning? Some baseline results comparing DNA-Train with standard SFT-only training would clarify this.\n2. To strengthen empirical support, please evaluate on additional hallucination benchmarks (e.g., MHBench, VidHalluc) to better situate your improvements within the recent literature.\n3. How were the general-purpose evaluation datasets selected? Did you follow a specific criterion (e.g., emphasizing temporal understanding or open-ended QA coverage)? Clarifying this would help assess the comprehensiveness of the evaluation setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2sckiciTun", "forum": "l08OzcT7bb", "replyto": "l08OzcT7bb", "signatures": ["ICLR.cc/2026/Conference/Submission6752/Reviewer_348k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6752/Reviewer_348k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761043571117, "cdate": 1761043571117, "tmdate": 1762919036078, "mdate": 1762919036078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "78Hkwk1ArW", "forum": "l08OzcT7bb", "replyto": "l08OzcT7bb", "signatures": ["ICLR.cc/2026/Conference/Submission6752/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6752/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763003815912, "cdate": 1763003815912, "tmdate": 1763003815912, "mdate": 1763003815912, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a large scale video (144k) QA dataset by visually augmenting the videos to tackle data imbalance problem. Subsequently, authors suggested a 2-staged (SFT+DAPO) DNA-Train with Duality Advantages Normalization that addresses falling into trivial solutions. Experiments show substantial gains in both hallucination and general video understanding benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The main idea of the DNA-Training strategy is mathematically sound and well-motivated for stable policy optimization and generalizable with other RL-training with binary rewards.\n- Provides a solid problem statement that MLLMs relies on textual priors rather than visual inputs. \n- This paper provides clear and logical ablation experiment settings and analysis that corroborates the claim of the authors, especially on dataset pairing, model scale, and DNA efficacy."}, "weaknesses": {"value": "(Major)\n- I am doubtful that such subtle video edits (blurring the frames in a short clip) can truly be counterfactual video contents to the original video.\n- I understood the idea of Eq. 1 why such data samples are effective, adding short writings to explain the motivation would help the overall readability. Also does it indicate we are actually filtering data above/below the threshold? Or is it just a concept of the goal?\n- There is a concern regarding circular annotation. AI generated synthetic data has a risk to encode the same underlying linguistic priors.\n- Benchmark size is relatively small compared to existing literature. Also, I am curious how the performance would change if we scale or decrease the number of training sets. There might be further findings on this setting and comparing it to SFT only or RL only training configuration.\n- How is the human verification process designed for Test split? Are DualityVidQA and DualityVidQA-RL also human verified?\n\n(Minor)\n- Inconsistent notation: DualityForge at line 77, DualityForce at Section 3.2 title and so on.\n- The tested models are limited to the Qwen2.5-VL family. I am not necessarily asking the authors to perform this experiment, but would like to know if the proposed method generalizes well with other models, and other RL-training with binary rewards.\n\n\nI will increase the score once current concerns are addressed."}, "questions": {"value": "- Figure A1 is more helpful for understanding the data construction framework. Can authors consider moving it to the main script?\n- Can’t we simply prompt generative models without visual input constraints for data generation pipeline? This may introduce bigger discrepancies of paired visual inputs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WFSpuQgTfv", "forum": "l08OzcT7bb", "replyto": "l08OzcT7bb", "signatures": ["ICLR.cc/2026/Conference/Submission6752/Reviewer_BoRi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6752/Reviewer_BoRi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761717994363, "cdate": 1761717994363, "tmdate": 1762919034706, "mdate": 1762919034706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In video understanding tasks, models often over-rely on linguistic priors, leading to hallucinations. This issue arises from the imbalance between textual and visual data. To address this, the paper proposes a counterfactual data synthesis framework that generates a large number of counterfactual scenarios. Furthermore, to fully leverage the counterfactual data, the paper introduces a DNA-training framework, which enables more stable and effective policy optimization."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed counterfactual data synthesis framework requires no human intervention, making it highly scalable.\n2. The DNA training framework effectively leverages counterfactual data by performing contrastive learning on paired video samples, which helps the model reduce its reliance on linguistic priors and thereby mitigates hallucinations."}, "weaknesses": {"value": "1. The counterfactual video generation process heavily relies on diffusion models, yet the temporal consistency control of these models remains unstable. The paper does not present any mechanism for controlling the quality of the generated videos (e.g., quantitative metrics such as FVD or CLIP Score).\n2. The paper repeatedly claims improvements in “stability” and “balanced gradient updates,” but provides no mathematical derivations or convergence analysis—only empirical explanations—thus lacking rigorous theoretical support.\n3. The core of “DualityForge” lies in using diffusion models to generate counterfactual videos and corresponding QA data. However, compared with existing frameworks combining video editing and synthetic QA generation (e.g., ShareGPT4Video, LLaVA-Video), its main innovations—“structured context embedding” and “contrastive QA”—are relatively limited. Moreover, “DNA-Train” merely adds l1 normalization to the advantage computation in the DAPO framework, which is essentially a minor optimization of training techniques.\n4. The paper lacks comparisons with recent counterfactual generation or de-hallucination methods, such as Vision-R1[1], VLM-R1[3], and OpenVLThinker[2], which adopt RL-based approaches.\n5. The ablation study does not examine the effect of l1 normalization on gradient variance or provide quantitative metrics of training stability.\n6. Dataset validation is missing, making it difficult to ensure the visual plausibility of the edited videos. Furthermore, the automatically generated QA pairs may contain labeling errors, which might require human verification to assess the quality of the data generation pipeline.\n7. Figure 2 is visually dense and lacks clear annotations, making it difficult to identify the key steps of DualityForge.\n\n[1] Huang W, Jia B, Zhai Z, et al. Vision-r1: Incentivizing reasoning capability in multimodal large language models[J]. arXiv preprint arXiv:2503.06749, 2025.\n\n[2] Deng Y, Bansal H, Yin F, et al. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement[J]. arXiv preprint arXiv:2503.17352, 2025.\n\n[3] Shen H, Liu P, Li J, et al. Vlm-r1: A stable and generalizable r1-style large vision-language model[J]. arXiv preprint arXiv:2504.07615, 2025."}, "questions": {"value": "1. What are the actual costs of generating 140000 samples, including time, computing power, and editing success rate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pdGr9G1kA2", "forum": "l08OzcT7bb", "replyto": "l08OzcT7bb", "signatures": ["ICLR.cc/2026/Conference/Submission6752/Reviewer_LwFK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6752/Reviewer_LwFK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761806745620, "cdate": 1761806745620, "tmdate": 1762919034320, "mdate": 1762919034320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles visual hallucination in Multimodal Large Language Models (MLLMs), where models rely on language priors instead of visual evidence when reasoning about counterfactual videos. The authors propose **DualityForge**, a controllable diffusion-based framework that edits real videos into counterfactual versions with structured contextual cues, enabling automatic generation of paired video–QA data. They further introduce **DualityVidQA**, a 144K-pair dataset for contrastive video QA, and **DNA-Train**, a two-stage (SFT + RL) training paradigm that applies ℓ₁-based *duality-normalized advantages* to balance learning between real and counterfactual samples. Experiments show significant reductions in hallucination without degrading general video understanding, achieving 76.8% pairwise accuracy on DualityVidQA-Test and competitive performance across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality.**  \nThe paper introduces a novel perspective on mitigating *visual hallucinations* in MLLMs through counterfactual video synthesis — a rarely explored yet highly relevant direction. The integration of diffusion-based controllable video editing with structured contextual grounding is both creative and technically sound, expanding beyond conventional data balancing or textual augmentation methods.\n\n**Quality.**  \nThe work demonstrates a strong methodological design. The DualityForge pipeline is clearly defined, fully automated, and experimentally validated. The proposed DNA-Train training paradigm, particularly its ℓ₁-based *duality-normalized advantage* mechanism, is mathematically well-motivated and empirically effective, yielding consistent improvements across hallucination and general benchmarks.\n\n**Clarity.**  \nThe paper is well-written and logically structured. Figures and equations (especially Eq. (1)–(7)) effectively illustrate the conceptual and mathematical flow. The narrative maintains good coherence between the problem motivation, method design, and experimental validation.\n\n**Significance.**  \nThe contributions are practically important and theoretically meaningful. The construction of DualityVidQA provides a valuable benchmark for evaluating grounded reasoning in MLLMs, while DNA-Train offers a scalable training paradigm applicable to other multimodal tasks. Together, these innovations establish a solid step toward more visually grounded multimodal understanding."}, "weaknesses": {"value": "**1. Data construction transparency and scale.**  \nThe paper does not clearly quantify how many *unique* videos and edited counterparts underpin the 144K QA pairs, nor the average QA-per-video ratio and the real:synthetic balance. The editing configurations (e.g., duration, resolution, number of frames, edit categories per level) and the automatic QA generation settings are also under-specified. Without these essentials, DualityVidQA’s scalability and reproducibility are hard to assess, and downstream users cannot compare fairly to prior VLM/MLLM data regimes such as Flamingo-style video ingestion [1] or LLaVA-style instruction tuning [2].  \nA small table reporting per-split counts (unique videos, pairs, QA-per-video), edit parameters, and failure rates of edits/QA would materially improve clarity.\n\n**2. Missing computational cost and throughput analysis.**  \nThe framework couples diffusion-based video editing, automatic QA synthesis, SFT, and RL. Yet the paper omits:  \n(i) per-video editing time and success rate;  \n(ii) tokens/sec during QA synthesis;  \n(iii) SFT epoch time and RL sampling throughput (responses/prompt/sec); and  \n(iv) overall GPU-hours by stage.  \nSuch numbers are standard for judging adoption cost and carbon/engineering budgets. Please report wall-clock numbers on the stated H200 setups and provide an efficiency comparison against common video-MLLM pipelines.\n\n**3. Reward design remains ambiguous.**  \nThe RL stage is described as “verifiable,” but it is unclear whether the reward checks *only* textual answer correctness or also enforces *visual grounding* (e.g., via caption or evidence alignment verifiers).  \nIf rewards are purely answer-based, RL might reinforce language priors rather than visual grounding. A decomposition of the reward (correctness vs. format vs. grounding), plus ablations where each component is toggled, would strengthen the claim of reducing visual hallucination.  \nPrior RL-with-verifiable-rewards work (GRPO [4], DAPO [5]) discusses verifier design choices that could be cited and compared.\n\n\n**4. Justification for SFT → RL ordering and algorithm choice.**  \nThe paper fixes the training order to SFT then RL and adopts DAPO but does not argue *why* this order (vs. RL→SFT) or *why* DAPO over GRPO for this task.  \nSince training order affects convergence, exploration, and mode-collapse, a brief controlled study (SFT→RL vs. RL→SFT) and a head-to-head comparison (GRPO vs. DAPO) on the proposed tests would make the methodology more convincing.  \nThe GRPO/RLVR literature emphasizes stability in verifiable tasks; it would be valuable to explain why DAPO is preferable for long-CoT video QA here.\n\n**5. Limited analysis of *why* hallucinations drop.**  \nBeyond aggregate accuracy, there is no mechanistic evidence that visual grounding truly improves.  \nRecommended analyses include:  \n(a) cross-modal attention maps over training;  \n(b) modality-wise gradient norms;  \n(c) token-level attribution to frames vs. text; and  \n(d) attention or entropy diagnostics that have been used to study attention head roles and perception in LMs/VLMs [6].  \nThese would connect DNA-Train’s ℓ₁ “duality-normalized advantage” to observable shifts in the model’s reliance on video signals.\n\n\n**6. Benchmark dependence vs. structural limitation (language attention dominance).**  \nEven large counterfactual sets may not resolve the deeper, architecture-level issue where linguistic priors overshadow visual cues at inference—long documented from caption hallucination to instruction-tuned VLMs [3].  \nThe paper should discuss to what extent DNA-Train *rebiases attention* rather than merely adapting to the benchmark distribution.  \nConsider adding stress tests (unseen counterfactual types, adversarial or low-quality edits) and reporting failure modes where plausibility still overrides visual evidence.\n\n\n**7. Ablations and interpretability are too light for the claimed contributions.**  \nKey components (paired data, contrastive QA, SFT, RL, ℓ₁ normalization) are not sufficiently disentangled.  \nProvide ablations isolating each piece (and their interactions), plus qualitative case studies tracing how the *same* example evolves across training (e.g., before/after attention shift, evidence tokens used).  \nThis would separate data-scale gains from genuine grounding effects.\n\n**8. Generalization and robustness beyond video QA are unclear.**  \nClaims are centered on counterfactual video QA.  \nIt remains unknown whether the approach extends to related tasks (temporal localization, retrieval), other modalities (image, audio-video, 3D), or noisy/OOD distribution settings.  \nA short transfer study or discussion of negative results would help bound the method’s scope.\n\n**9. Human evaluation is missing.**  \nBecause “hallucination” is partly perceptual, small-scale human studies (e.g., rating correctness, evidence grounding, and explanation helpfulness) would triangulate automatic metrics.  \nThis is common in prior work on hallucination and relevance [3] and would make claims about *visual* grounding more credible.\n\n---\n\n### References\n\n1. Alayrac, J.-B., et al. (2022). *Flamingo: A Visual Language Model for Few-Shot Learning.* NeurIPS.  \n2. Liu, H., Li, C., Wu, Q., Lee, Y. J. (2023). *Visual Instruction Tuning (LLaVA).* NeurIPS (Oral) / arXiv:2304.08485.  \n3. Rohrbach, A., et al. (2018). *Object Hallucination in Image Captioning.* EMNLP.  \n4. Mroueh, Y., et al. (2025). *Reinforcement Learning with Verifiable Rewards: GRPO’s Effective Loss Dynamics.* arXiv:2503.06639.  \n5. Yu, Q., et al. (2025). *DAPO: An Open-Source LLM Reinforcement Learning System at Scale.* arXiv:2503.14476.  \n6. *Unveiling Visual Perception in Language Models.* (2024). arXiv:2412.18108. (Attention head/perception analysis)"}, "questions": {"value": "> I encourage the authors to thoroughly address the weaknesses and questions raised in this review. If the authors can provide detailed explanations and in-depth clarifications during the rebuttal, and if the revised version demonstrates substantial progress in both clarity and improvement, I will be willing to reassess the manuscript and **adjust my overall rating** accordingly, based on the quality and depth of the revision.\n\n---\n\n**1. Data Construction Transparency and Scale**\n\n* Could the authors specify the exact number of *unique* real and counterfactual videos used to generate the 144K QA pairs?\n* What is the average number of QA pairs per video, and what is the ratio of real to synthetic samples?\n* Please provide detailed editing parameters (e.g., frame rate, resolution, edit category taxonomy, diffusion prompt types) and QA generation latency statistics.\n* Was any human verification applied during QA generation or post-edit filtering? If so, what proportion of samples were manually corrected or rejected?\n\n**2. Computational Cost and Efficiency**\n\n* How long does it take, on average, to synthesize a single counterfactual video and its corresponding QA pair?\n* Could the authors provide a breakdown of GPU hours and throughput (responses/sec, tokens/sec) for each phase—diffusion editing, QA generation, SFT, and RL?\n* How does the proposed pipeline compare in resource efficiency to other large-scale video instruction tuning frameworks such as LLaVA-Video or ShareGPT4Video?\n\n**3. Reward Design and Verifiability**\n\n* Does the reward function evaluate *only* textual correctness or also verify *visual grounding* (e.g., matching temporal regions, visual evidence, or captions)?\n* How are conflicting signals handled when textual reasoning seems correct but contradicts visual content?\n* Could the authors include ablations showing the effect of removing or modifying each reward component (correctness, format, grounding)?\n* Are any external verifiers or discriminators used during RL for grounding validation?\n\n**4. SFT → RL Order and Algorithm Choice**\n\n* Why was the order fixed to SFT followed by RL, rather than experimenting with RL pretraining followed by SFT (which might stabilize reward learning)?\n* What specific factors motivated the selection of DAPO instead of GRPO or PPO-based alternatives, especially regarding stability or entropy control?\n* Did the authors attempt or observe any divergence when reversing the training sequence (RL→SFT)? If not, could this be tested?\n* Could the authors share loss and reward trajectories to demonstrate that DAPO indeed avoids entropy collapse in this multimodal setting?\n\n**5. Mechanistic Understanding of Hallucination Reduction**\n\n* Beyond quantitative accuracy, can the authors visualize cross-modal attention maps before and after RL to demonstrate stronger video grounding?\n* Have they measured per-token attribution or gradient norms across modalities to show that video signals receive higher relative weights?\n* Is there empirical evidence linking the ℓ₁ “duality-normalized advantage” term to improved multimodal feature alignment?\n* Would the authors consider incorporating diagnostic analyses (e.g., entropy, attention shift, visual feature coverage) to verify this mechanism?\n\n**6. Benchmark Dependence and Architectural Limitations**\n\n* Could the performance improvements stem from benchmark-specific adaptation rather than genuine robustness?\n* How does the model behave under unseen or adversarial counterfactual edits (e.g., physically impossible events or low-quality videos)?\n* Have the authors explored whether the language module still dominates decision-making attention during inference, as reported in prior perception studies?\n* Could they consider measuring modality-wise attention weights during inference to confirm reduced “language attention dominance”?\n\n**7. Ablation and Interpretability**\n\n* Could the authors conduct systematic ablations isolating the effects of paired data, SFT, RL, and advantage normalization separately?\n* Please include qualitative examples where the model’s reasoning or answer changes after each training phase.\n* Would it be possible to visualize token-level or frame-level contributions to illustrate the internal grounding process?\n\n**8. Generalization and Robustness**\n\n* Have the authors tested DNA-Train on related tasks such as temporal localization, captioning, or retrieval?\n* Does the method generalize to still images or multimodal datasets with different temporal dynamics?\n* What happens when trained on a different base model (e.g., InternVideo, LLaVA-Next-Video)?\n* Could the authors comment on out-of-distribution performance (e.g., unseen domains, distorted videos)?\n\n**9. Human Evaluation**\n\n* Have the authors considered conducting a small-scale human evaluation (e.g., via pairwise preference or grounding quality ratings)?\n* How consistent are automatic hallucination metrics with human judgments of correctness and evidence grounding?\n* Could the authors share examples where the model is “technically correct” but visually misleading?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GODMIhLJ5Q", "forum": "l08OzcT7bb", "replyto": "l08OzcT7bb", "signatures": ["ICLR.cc/2026/Conference/Submission6752/Reviewer_fzc9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6752/Reviewer_fzc9"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6752/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762362705865, "cdate": 1762362705865, "tmdate": 1762919033940, "mdate": 1762919033940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}