{"id": "E0WSAugJ0j", "number": 19066, "cdate": 1758293233472, "mdate": 1763729424686, "content": {"title": "LiveClin: A Live Clinical Benchmark without Leakage", "abstract": "The reliability of medical LLM evaluation is critically undermined by data contamination and knowledge obsolescence, leading to inflated scores on static benchmarks. To address these challenges, we introduce LiveClin, a live benchmark designed for the faithful replication of clinical practice. Built from contemporary, peer-reviewed case reports and updated biannually, LiveClin ensures clinical currency and resists data contamination. Using a verified AI-human workflow involving 239 physicians, we transform authentic patient cases into complex, multimodal evaluation scenarios that span the entire clinical pathway. The benchmark currently comprises 1,407 case reports and 6,605 questions. Our evaluation of 26 models on LiveClin reveals the profound difficulty of these real-world scenarios, with the top-performing model achieving a Case Accuracy of just 35.7%. We find that the era of \"free lunch\" improvements from simple model scaling is over, as newer models do not consistently outperform their predecessors. Furthermore, our analysis uncovers distinct reasoning weaknesses across model classes. LiveClin thus provides a continuously evolving, clinically-grounded framework to steer the development of medical LLMs towards greater reliability and real-world utility.", "tldr": "LiveClin is a live benchmark that evaluates medical LLMs on the entire clinical pathway", "keywords": ["MultiModal Medical Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52a17e4c1bf80e1e634c9b46d40840f34c97be8b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "LiveClin tackles the problem of LLM evaluation in healthcare settings. Recognizing the need for benchmarks that are dynamic, to prevent inflation of scores due to contamination, and multi-turn, to evaluate multi-stage clinical reasoning, the authors propose a benchmark they call LiveClin, a novel biannually updated clinical benchmark of over 1,000 multi-question cases. The authors build the benchmark from the last six months of PubMed case reports and evaluate in-depth across LLMs.\n\nLiveClin is generated in a human-AI agentic workflow and constructed to cover comprehensive disease clusters. Evaluation is comprehensive and informative. The authors discover that while individual question accuracy can be high for most models, overall case accuracy is low and decreases with model updates. The detail of the dataset allows for an understanding of model performance by clinical area."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe benchmark is constructed to be very comprehensive across medical topics. The taxonomy is clear, well explained and well evaluated. \n-\tThe authors include a clear description and depiction of the benchmark generation process and evaluate with creative ablations."}, "weaknesses": {"value": "-\tThere could be further discussion of the fact that the accuracy for specific questions is high for most models, but that total case accuracy is low. This is an interesting finding of the benchmark consistent with other claims that LLMs lack longitudinal reasoning.\n-\tIt would also be helpful to argue or explain that cases from PubMed will be sufficient for evaluation. What kinds of cases are typically published in this forum? Does this bias the benchmark?\n-\tThere are very few medical specific LLMs tested. This is surprising given the authors claim that domain specific models may be the solution to low overall performance by general models that decreases with newer versions.\n-\tThe authors should compare to other recent medical reasoning benchmarks using case reports including: MedCaseReasoning Wu et al 2025, McDuff et al 2025 (NEJM CPC), and CaseReportBench Zhang et al 2025.\n\nAdditional notes (not a part of recommendation)\n-\tLine 116: Figure is missing an A in NARRATIVE.\n-\tLine 185: COnstruction\n-\tLine 266: experts is written twice.\n-\tLine 273: cut-off in figure could be improved.\n-\tTable 1: I think it should be “Cost ($).”\n-\tIt is odd that the related work is in the appendix.\n-\tWhat is the horizontal line in Figure 7B?"}, "questions": {"value": "How will this benchmark be prevented from being contaminated if the data is on open access PubMed? There could be further flushing out of the live generation methods. Will old cases be dropped once they have become available for long enough for models to be trained on? Where will this benchmark be managed and who will have access? How will over 200 physicians be employed regularly to maintain this benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XfNDl4G1DZ", "forum": "E0WSAugJ0j", "replyto": "E0WSAugJ0j", "signatures": ["ICLR.cc/2026/Conference/Submission19066/Reviewer_KBAZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19066/Reviewer_KBAZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412962339, "cdate": 1761412962339, "tmdate": 1762931094787, "mdate": 1762931094787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper’s primary work is developing LiveClin, a dynamic and contamination-resistant evaluation benchmark for medical large language models (LLMs), addressing the limitations of static benchmarks in clinical relevance and anti-leakage. Built on contemporary peer-reviewed case reports from PubMed Central (1,407 cases, 6,605 questions), LiveClin integrates multimodal data (e.g., CT scans, pathological slices, tables) and simulates the full clinical pathway—spanning initial assessment, diagnostic testing, treatment planning, and long-term management—to assess models’ sequential reasoning capabilities.\n The benchmark is validated through comprehensive testing of 26 mainstream models (proprietary, open-source general, open-source medical), revealing distinct reasoning weaknesses across model classes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The overall logic is fairly clear, and the writing is relatively well-structured.\n2. Built from the latest peer-reviewed clinical cases and updated biannually, it effectively mitigates data leakage and knowledge obsolescence issues that plague static benchmarks, ensuring long-term clinical relevance of evaluations.\n3. It simulates the patient care process (from initial consultation to long-term management) and integrates diverse multimodal data (images, tables, etc.),  reflecting real-world clinical reasoning scenarios.\n4. The AI-human collaborative (Generator-Critic-Judge) pipeline, combined with rigorous review by 239 physicians, balances clinical accuracy, construction efficiency, and question challenge—solving the trade-off between scalability and quality in medical benchmark development."}, "weaknesses": {"value": "Major Comments\n\n1.The benchmark is constructed using case reports from the first half of 2025 in the PubMed Central (PMC) Open Access subset. Could there still be potential data leakage risks for some newly released models such as GPT-5? \n\n2.Does multiple physicians verify the same piece of data? If yes, what is the inter-annotator agreement (e.g., Cohen’s Kappa coefficient) among different physicians? \n\n3.The paper proposes \"updating the benchmark biannually\" but lacks details on the specific cost and efficiency of the update process. Will all test data be completely replaced during updates? If so, is it necessary to retest all models entirely after replacement? \n\n4.No human baseline based on physicians’ performance is provided. \n\n5.It appears that a single prompt was used for testing with a temperature setting of 0, and no multiple rounds of testing were conducted. This may lead to accidental errors due to randomness. \n\nMinor Comments\n\n1.Details about the review process involving 239 physicians are insufficient. What is the distribution of their professional fields? Is there a risk that some disease types lack review by specialized physicians? \n\n2.Although 41.9% of the questions require multimodal interpretation, relevant details are lacking. \n\n3.Some statements are overly absolute or exaggerated. For example, the claim that \"the era of 'free lunch' is over\" is based on only two model examples, and \"faithful replication of clinical practice\" overstates the benchmark’s alignment with real clinical scenarios."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f0YTSsF9CT", "forum": "E0WSAugJ0j", "replyto": "E0WSAugJ0j", "signatures": ["ICLR.cc/2026/Conference/Submission19066/Reviewer_y36J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19066/Reviewer_y36J"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664882005, "cdate": 1761664882005, "tmdate": 1762931093898, "mdate": 1762931093898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their detailed feedback and constructive suggestions across multiple aspects of our work. We greatly appreciate that reviewers recognized the uniqueness, rigor, and clinical relevance of **LiveClin**, a clinician‑validated, multimodal benchmark designed to be contamination‑resistant, regularly updated, and capable of simulating the entire patient care pathway.\n\nReviewers highlighted several strengths of our work:\n\n- Use of a large, multimodal, clinician‑annotated dataset that integrates imaging, labs, and other signals, reflecting real‑world workflows and enabling multi‑resolution taxonomy analysis (ICD‑10 chapters, disease clusters, codes) (sxpY, 1rMm, y36J, KBAZ)\n- An AI–human collaborative Generator–Critic–Judge pipeline balanced for scalability and clinical rigor, ensuring domain‑appropriate review by 239 physicians from diverse specialties (sxpY, y36J)\n- Demonstrated empirical contamination control through a *live* design with biannual full‑set replacement and private monthly leaderboard monitoring (sxpY, 1rMm, y36J, KBAZ)\n- Rich and challenging evaluation covering longitudinal reasoning, revealing that newer models do not always outperform predecessors and identifying distinct failure patterns (sxpY, KBAZ)\n- Clear writing and structured presentation, with comprehensive ablation analysis and thoughtful discussion on model gaps and failure modes (y36J, KBAZ)\n\n**Key additions and clarifications based on reviewer feedback:**\n\n1. **Ablation Attribution Analysis**: Added tables showing most trivial→non‑trivial transitions stem from authentic clinical complexity rather than generation artifacts (W1, sxpY). Details are shown in lines 469–474, 1487–1532.\n2. **Few‑Shot Experiments**: Reported one‑ and three‑shot results; most models changed ≤±0.8 pp, confirming zero‑shot stability (W2, sxpY, y36J). Details are shown in lines 356–378, 1443–1452.\n3. **Physician Baselines**: Accuracy for Attending and Chief Physicians on 100 cases shows physicians outperform almost all models (Q2, 1rMm, y36J). Details are shown in lines 324–338, 348–350.\n4. **Biannual Update Plan**: Detailed update workflow, resource estimates (USD 53.5K, 11.3 days per cycle), and full retesting commitment; confirmed sustainability within current team capacity (W3, sxpY, y36J, KBAZ). Details are shown in lines 501–506.\n5. **Contamination Controls**:  Quantified model release vs. data cut‑off gaps (6–8 months), described private leaderboard guardrail; demonstrated stable scores and rankings across monthly sets (W4/Q2, sxpY, y36J). Details are shown in lines 501–513.\n6. **Case Report Rarity Analysis**: Annotated cases by rarity and compared performance across rare/unrare subsets; found ≤5 pp differences for most models, with stronger models less affected, mitigating bias concerns (Q1, sxpY, 1rMm, KBAZ). Details are shown in lines 514–523 and 1547–1610.\n7. **Benchmark Correlation Tests**: Correlation analysis with seven established clinical benchmarks; highest alignment with complex, multimodal MedXpertQA (ρ=0.62); added physician comparison to show real‑world relevance (Q3, sxpY). Details are shown in lines 324–338 and 348–350.\n8. **MCQ Design Rationale and Mitigation**:  Justified MCQs for objectivity and scalability with scenario design and 10 distractors (W4, 1rMm).\n9. **Inter‑Annotator Agreement Data**:  Reported 8.7% disagreement rate between physicians, 90% resolved in one loop; all resolved in ≤2 loops, indicating high agreement (W2, y36J).  Details are shown in lines 260–261 and 1129–1138.\n10. **Additional Benchmark Comparisons**: Compared to MedCaseReasoning, NEJM CPC, CaseReportBench; LiveClin uniquely combines multimodality, multi‑stage pathways, and live contamination resistance (W4, KBAZ). Details are shown in lines 780–785.\n\n**Overall**, these changes strengthen the paper’s empirical grounding, transparency, and operational details, while directly addressing each reviewer’s concerns. LiveClin occupies a distinctive position in the research ecosystem:\n\n- A *live*, contamination‑resistant, clinically grounded benchmark\n- Scalable yet rigorously physician‑validated through an AI–human collaborative pipeline\n- Covering the full clinical pathway with aligned multimodal reasoning tasks\n- Providing both granular taxonomy analysis and correlation with real‑world benchmarks and physician performance\n\nWe thank the reviewers again for their constructive insights, which have helped us significantly improve the clarity, completeness, and evidential support of this work. We look forward to further advancing contamination‑resistant, clinically relevant evaluation for medical AI."}}, "id": "CzYs093O5u", "forum": "E0WSAugJ0j", "replyto": "E0WSAugJ0j", "signatures": ["ICLR.cc/2026/Conference/Submission19066/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19066/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19066/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728382931, "cdate": 1763728382931, "tmdate": 1763729619901, "mdate": 1763729619901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their detailed feedback and constructive suggestions across multiple aspects of our work. We greatly appreciate that reviewers recognized the uniqueness, rigor, and clinical relevance of **LiveClin**, a clinician‑validated, multimodal benchmark designed to be contamination‑resistant, regularly updated, and capable of simulating the entire patient care pathway.\n\nReviewers highlighted several strengths of our work:\n\n- Use of a large, multimodal, clinician‑annotated dataset that integrates imaging, labs, and other signals, reflecting real‑world workflows and enabling multi‑resolution taxonomy analysis (ICD‑10 chapters, disease clusters, codes) (sxpY, 1rMm, y36J, KBAZ)\n- An AI–human collaborative Generator–Critic–Judge pipeline balanced for scalability and clinical rigor, ensuring domain‑appropriate review by 239 physicians from diverse specialties (sxpY, y36J)\n- Demonstrated empirical contamination control through a *live* design with biannual full‑set replacement and private monthly leaderboard monitoring (sxpY, 1rMm, y36J, KBAZ)\n- Rich and challenging evaluation covering longitudinal reasoning, revealing that newer models do not always outperform predecessors and identifying distinct failure patterns (sxpY, KBAZ)\n- Clear writing and structured presentation, with comprehensive ablation analysis and thoughtful discussion on model gaps and failure modes (y36J, KBAZ)\n\n**Key additions and clarifications based on reviewer feedback:**\n\n1. **Ablation Attribution Analysis**: Added tables showing most trivial→non‑trivial transitions stem from authentic clinical complexity rather than generation artifacts (W1, sxpY). Details are shown in lines 469–474, 1487–1532.\n2. **Few‑Shot Experiments**: Reported one‑ and three‑shot results; most models changed ≤±0.8 pp, confirming zero‑shot stability (sxpY, y36J). Details are shown in lines 356–378, 1443–1452.\n3. **Physician Baselines**: Accuracy for Attending and Chief Physicians on 100 cases shows physicians outperform almost all models (1rMm, y36J). Details are shown in lines 324–338, 348–350.\n4. **Biannual Update Plan**: Detailed update workflow, resource estimates (USD 53.5K, 11.3 days per cycle), and full retesting commitment; confirmed sustainability within current team capacity (sxpY, y36J, KBAZ). Details are shown in lines 501–506.\n5. **Contamination Controls**:  Quantified model release vs. data cut‑off gaps (6–8 months), described private leaderboard guardrail; demonstrated stable scores and rankings across monthly sets (sxpY, y36J). Details are shown in lines 501–513.\n6. **Case Report Rarity Analysis**: Annotated cases by rarity and compared performance across rare/unrare subsets; found ≤5 pp differences for most models, with stronger models less affected, mitigating bias concerns (sxpY, 1rMm, KBAZ). Details are shown in lines 514–523 and 1547–1610.\n7. **Benchmark Correlation Tests**: Correlation analysis with seven established clinical benchmarks; highest alignment with complex, multimodal MedXpertQA (ρ=0.62); added physician comparison to show real‑world relevance (sxpY). Details are shown in lines 324–338 and 348–350.\n8. **MCQ Design Rationale and Mitigation**:  Justified MCQs for objectivity and scalability with scenario design and 10 distractors (1rMm).\n9. **Inter‑Annotator Agreement Data**:  Reported 8.7% disagreement rate between physicians, 90% resolved in one loop; all resolved in ≤2 loops, indicating high agreement (y36J).  Details are shown in lines 260–261 and 1129–1138.\n10. **Additional Benchmark Comparisons**: Compared to MedCaseReasoning, NEJM CPC, CaseReportBench; LiveClin uniquely combines multimodality, multi‑stage pathways, and live contamination resistance (KBAZ). Details are shown in lines 780–785.\n\n**Overall**, these changes strengthen the paper’s empirical grounding, transparency, and operational details, while directly addressing each reviewer’s concerns. LiveClin occupies a distinctive position in the research ecosystem:\n\n- A *live*, contamination‑resistant, clinically grounded benchmark\n- Scalable yet rigorously physician‑validated through an AI–human collaborative pipeline\n- Covering the full clinical pathway with aligned multimodal reasoning tasks\n- Providing both granular taxonomy analysis and correlation with real‑world benchmarks and physician performance\n\nWe thank the reviewers again for their constructive insights, which have helped us significantly improve the clarity, completeness, and evidential support of this work. We look forward to further advancing contamination‑resistant, clinically relevant evaluation for medical AI."}}, "id": "CzYs093O5u", "forum": "E0WSAugJ0j", "replyto": "E0WSAugJ0j", "signatures": ["ICLR.cc/2026/Conference/Submission19066/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19066/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19066/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728382931, "cdate": 1763728382931, "tmdate": 1763735599769, "mdate": 1763735599769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present a new dataset and a generation method to assess the medical knowledge and capabilities of LLMs. The method proposes to update the benchmark twice a year to ensure updated and uncontaminated evaluations. They generate multiple-choice questions based on open-access cases and evaluate 26 AI models on the generated benchmark. They show that models struggle to answer the questions and obtain low scores compared to commonly used evaluations such as MedQA."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Contamination and the rapid evolution of medical knowledge are significant concerns for the evaluation in the medical domain. This approach presents a solution to both of these issues. The dataset is sufficiently large, and the multistep approach is a welcome addition compared to previous evaluations that test zero-shot knowledge on a complete vignette. The dataset is also multimodal, integrating imaging, labs, and other signals. It was also validated by a large number of clinicians, which strengthens the method's validity.\n\nThe findings regarding model performance are interesting and demonstrate the need for more thorough validation for safe and effective clinical use."}, "weaknesses": {"value": "While the method is solid, I am concerned by the reliance on case reports, as, by definition, case reports are published to communicate unusual or rare cases to the medical community. This reliance on unusual/rare cases induces a bias in the knowledge and reasoning capabilities of the models. I am also concerned about the lack of a physician baseline to compare the accuracy of models with what is expected of an attending physician.\n\nThe reported metric for case accuracy scores appears too strict and not representative of the models' actual capabilities, as a single error causes the models to obtain a score of 0 on that case. A rubric-based assessment would strengthen the evaluation and enhance the interpretability of mistakes and areas for improvement in these models.\n\nThe reliance on MCQs also weakens the benchmark, considering the identified limitations of this testing methodology."}, "questions": {"value": "# Major concerns\n\n1) The authors should at least discuss the limitations of using case reports that are likely not representative of clinical workflows. A subset containing more common cases would help clarify whether the errors occur due to out-of-distribution cases or if they result from intrinsic shortcomings of LLMs.\n\n2) A baseline of physicians on a subset (100 cases), including residents and attendings, would help with the interpretation of the results. At the moment, 35% seems relatively low, but if attendings score 20% it would demonstrate that LLMs may already be ready for clinical decision support. As a physician myself, I would not be surprised if I obtained a low score due to the nature of the cases included.\n\n3) A more balanced scoring methodology beyond simple accuracy would help to identify the issues. For instance, scoring based on the severity of the error, for example, suggesting the second-best exam should not carry the same weight as sending a patient with a STEMI home. HealthBench, for example, weights rubrics differently [1].\n\n4) The reliance on MCQ should be acknowledged as a limitation, as it has been identified that LLMs exploit patterns, even more so when the question generator is also an LLM [2].\n\n# Minor\n\n1) The authors should discuss the biases of case reports that are likely not representative of medicine worldwide, as publications are very US-centric, with minimal case reports from low-resource settings. \n\n[1] HealthBench: Evaluating Large Language Models Towards Improved Human Health (Arora et al. Preprint 2025)\n\n[2] Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine (Griot et al., ACL 2025)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YDAhwYhgr4", "forum": "E0WSAugJ0j", "replyto": "E0WSAugJ0j", "signatures": ["ICLR.cc/2026/Conference/Submission19066/Reviewer_1rMm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19066/Reviewer_1rMm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111608698, "cdate": 1762111608698, "tmdate": 1762931093193, "mdate": 1762931093193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces LiveClin, a dynamic medical benchmark addressing data contamination and knowledge obsolescence through biannually updated case reports from PubMed Central. The benchmark comprises 1,407 cases with 6,605 questions spanning the entire clinical pathway, revealing that even top models achieve only 35.7% case accuracy, with distinct failure modes across model classes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Pilot study convincingly shows 10-point performance drop on post-cutoff data.\n\n* The three-tier taxonomy (ICD-10 chapters, disease clusters, individual codes) enables multi-resolution analysis while ensuring broad disease representation.\n\n* The 239-physician verification pipeline with both annotation and inspection phases demonstrates exceptional attention to clinical validity.\n\n* The multimodal integration is high quality, where images are naturally embedded in clinical workflow.\n\n* Reveals newer models don't always outperform predecessors; identifies distinct failure patterns."}, "weaknesses": {"value": "* The ablation study shows AI generates more \"challenging\" questions (lower trivial ratio), but doesn't validate whether this difficulty stems from genuine clinical complexity or artifacts of the generation process.\n\n* he zero-shot, conversational evaluation may disadvantage models not optimized for this specific format. It's possible that the performance differences reflect not clinical reasoning ability but the adaptation to the evaluation format. Adding few-shot experiments would help.\n\n* Maintaining physician review for biannual updates seems resource-intensive. While the paper reports $42K for initial construction, the long-term sustainability of this approach remains unclear. More details on this would be helpful to strengthen this claim.\n\n* Despite being a core motivation, the paper doesn't empirically demonstrate that its approach prevents contamination better than decontamination methods or how quickly new cases might enter training corpora."}, "questions": {"value": "1. How do you ensure the case reports selected are representative of disease distributions in real clinical practice? Does collecting data from PMC result in higher probability of rare cases?\n\n2. To support the claim of the paper better, can you demonstrate empirically that LiveClin remains contamination-free over time? For instance, tracking whether newly released cases appear in web crawls or model training data?\n\n3. How does model performance correlate between LiveClin and real clinical decision-making tasks or other clinical benchmarks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mPUq4BAMzd", "forum": "E0WSAugJ0j", "replyto": "E0WSAugJ0j", "signatures": ["ICLR.cc/2026/Conference/Submission19066/Reviewer_sxpY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19066/Reviewer_sxpY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762461929915, "cdate": 1762461929915, "tmdate": 1762931092774, "mdate": 1762931092774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}