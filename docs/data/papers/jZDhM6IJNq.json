{"id": "jZDhM6IJNq", "number": 8135, "cdate": 1758067829416, "mdate": 1759897804903, "content": {"title": "Attention Slipping: A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs", "abstract": "As large language models (LLMs) become more integral to society and technology, ensuring their safety becomes essential. Jailbreak attacks exploit vulnerabilities to bypass safety guardrails, posing a significant threat. However, the mechanisms enabling these attacks are not well understood.  In this paper, we reveal a universal phenomenon that occurs during jailbreak attacks:  **Attention Slipping**. During this phenomenon, the model gradually reduces the attention it allocates to unsafe requests in a user query during the attack process, ultimately causing a jailbreak. We show **Attention Slipping** is consistent across various jailbreak methods, including gradient-based token replacement, prompt-level template refinement, and in-context learning. Additionally, we evaluate two defenses based on query perturbation, Token Highlighter and SmoothLLM, and find they indirectly mitigate **Attention Slipping**, with their effectiveness positively correlated with the degree of mitigation achieved. Inspired by this finding, we propose Attention Sharpening, a new defense that directly counters **Attention Slipping** by sharpening the attention score distribution using temperature scaling. Experiments on four leading LLMs (Gemma2-9B-It, Llama3.1-8B-It, Qwen2.5-7B-It, Mistral-7B-It v0.2) show that our method effectively resists various jailbreak attacks while maintaining performance on benign tasks on AlpacaEval. Importantly, Attention Sharpening introduces no additional computational or memory overhead, making it an efficient and practical solution for real-world deployment.", "tldr": "A Mechanistic Understanding of Jailbreak Attacks and Defenses in LLMs Via Attention Perspective.", "keywords": ["Large Language Models", "Attention", "Jailbreaks"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abbd01ce3cf75e99a8b9e7f60375a2d047a52e06.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper explores the internal attention dynamics that underlie jailbreak attacks on LLMs and then proposes a defense mechanism based on the findings. The authors first conduct a mechanistic analysis of attention changes during the optimization process of jailbreak prompts, revealing a phenomenon they term attention slipping. They found out that there is a gradual shift in attention distribution across tokens that leads models to be vulnerable. Building on this observation, the authors propose a defense technique called attention sharpening, which introduces a temperature-based adjustment to restructure the attention layout and hence defend the model against jailbreak attacks. Experiments show that attention sharpening modestly reduces attack success rates while preserving relatively high benign task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written, and the exploratory study provided some insight into the jailbreak attacks.\n2. The idea of using a temperature coefficient to sharpen the attention structure is interesting and novel.\n3. This paper conducted an adequate number of experiments to demonstrate its effectiveness in both safety preservation and benign performance."}, "weaknesses": {"value": "1.\tMarginal efficacy on core objective (ASR)\n\nThe reported reduction in attack success rate (ASR) under attention sharpening is small. Given the modest gains, the defense’s practical value is unclear, especially when lightweight alternatives (e.g., system-prompt safety reminders or brief refusal primes) may preserve benign utility while matching or exceeding the observed ASR drop at near-zero cost.\n\n2.\tLimited novelty of the central insight\n\nThe mechanistic observation that jailbreaks manipulate attention away from safety-critical representations is not new. Section 5 acknowledges several prior works with similar premises and downstream defenses. The paper’s claimed contribution, “distinguishing the dynamic process from the static outcome”, is not applicable in the attention sharpening method which is deployed only at the end of the jailbreak process (i.e., at generation time), functionally still making it a “static” intervention. This defense does not exploit temporal information (e.g., step-wise tracking or counter-drift controls) that would substantiate a dynamic approach.\n\n3.\tInsufficient analysis of model-specific failures\n\nAppendix results indicate the ASR increases instead of dropping on certain models (e.g., Gemma-2 9B) against adaptive attacks. This suggests model-specific brittleness and raises concerns about transferability among model architectures and worst-case resilience. This paper does not explain why attention sharpening underperforms on models like GEMMA-2 9B. Without diagnostic analyses (e.g., architecture-level attention statistics, head-type sensitivity, tokenizer effects), it is difficult to generalize lessons or prescribe fixes."}, "questions": {"value": "1. What would be the effect of simply adding a Self-Reminder (https://doi.org/10.1038/s42256-023-00765-8) ?\n2. What could be the reason for attention sharpening's differences in performance among different architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QeugOLp5qe", "forum": "jZDhM6IJNq", "replyto": "jZDhM6IJNq", "signatures": ["ICLR.cc/2026/Conference/Submission8135/Reviewer_RYfr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8135/Reviewer_RYfr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761538775976, "cdate": 1761538775976, "tmdate": 1762920107210, "mdate": 1762920107210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a mechanistic explanation for how jailbreak attacks bypass LLM safety guardrails. Spesifically, they introduce the concept of Attention Slipping, which is a universal phenomenon where models gradually reduce attention to unsafe portions of input during adversarial prompt optimization. The authors empirically observe this across multiple jailbreak methods (GCG, AutoDAN, MSJ) and models (Gemma2, Llama3, Qwen2.5, Mistral). They further evaluate existing defenses (Token Highlighter, SmoothLLM) and show that their efficacy correlates with mitigating Attention Slipping. They also propose Attention Sharpening, a lightweight defense that modifies attention softmax with temperature scaling to counteract this slipping effect, achieving a favorable trade-off between safety and utility without extra computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper provides a mechanistic interpretation of jailbreak behavior through “attention slipping,” connecting model interpretability with adversarial robustness. \n\n2. Experiments span four major LLMs and multiple jailbreak families, making the observed phenomenon and proposed defense appear robust and generalizable."}, "weaknesses": {"value": "1. My major concern is although the paper emphasizes its focus on the dynamics of attention slipping rather than static attention redistribution, the mechanism and mitigation strategy (temperature scaling / KV modification) are conceptually similar to RobustKV (ICLR 2025) and related attention-based defenses. The actual distinction feels incremental rather than fundamental.\n\n2. The claim that “attention slipping causes jailbreak success” is largely correlational. There is no controlled intervention or causal ablation verifying that manipulating attention alone changes safety behavior.\n\n3. The proposed “attention sharpening” may oversimplify the multifaceted nature of attention in LLMs. Although the author provide results on ASR and win rate metrics. It is not enough as this is the core method in this work but the authors do not comprehensively evaluate it."}, "questions": {"value": "1. What’s the proposed method's defense result against flipattack? \n\n[1] FlipAttack: Jailbreak LLMs via Flipping"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tNfwQMwCZS", "forum": "jZDhM6IJNq", "replyto": "jZDhM6IJNq", "signatures": ["ICLR.cc/2026/Conference/Submission8135/Reviewer_ng4Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8135/Reviewer_ng4Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767149874, "cdate": 1761767149874, "tmdate": 1762920106800, "mdate": 1762920106800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper identifies a purportedly universal dynamic in LLM jailbreaks, Attention Slipping, wherein a model’s attention to an 'unsafe prototype' diminishes as an attack prompt is optimized, facilitating policy evasion. They also propose Attention Sharpening, a temperature-based rescaling of attention logits that aims to counteract the phenomenon with no extra compute/memory."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Cross-attack, cross-model evidence of a single mechanism (relationship between AR and ASR) via both optimization trajectories and reverse masking.\n- Mechanistic angle that links attacks and defenses at the attention level\n- Simple, efficient defense that avoids multi-pass overhead and large memory costs claimed for alternatives."}, "weaknesses": {"value": "- In Figure 5, the increase in attention rate with stronger defense strength does not appear very pronounced—especially compared to the degree of ASR reduction—so the claim in L361–362 seems somewhat overstated.\n- The authors devote substantial space to analyzing and illustrating the Attention Slipping phenomenon; however, as they themselves discuss in Section 5, this phenomenon appears to have been proposed in previous work under different formulations (see L469, “two ways of looking at the same thing”). Moreover, I note that the AttnGCG paper mentioned at L456 already observed that during GCG/AutoDAN attacks, attention shifts from the unsafe prototype to other adversarial contexts—an observation largely consistent with the analyses presented in this paper. In light of this, could the authors further clarify the distinctions between their work and the related studies discussed in Section 5, particularly AttnGCG? This would help me better understand the novelty of this paper, as at present I have concerns that the contribution may be incremental rather than fundamentally new. Alternatively, the authors could consider providing a deeper exploration and demonstration of the Attention Sharpening component, which appears more distinct from prior research but currently occupies only a small portion of the paper.\n- There is a trade-off between ASR and utility, but the method sometimes leaves substantial residual ASR (e.g., for Qwen and Mistral), suggesting that the security margin under adversarial adaptive attacks may be limited—Table 1 illustrates this point.\n- On benign long-context tasks (such as code generation or retrieval-style QA), does Attention Sharpening impair the model’s ability to recall distant tokens? Were any long-sequence stress tests performed?\n- How sensitive are the ASR results to the choice of judge models?"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "haeUo2Ib22", "forum": "jZDhM6IJNq", "replyto": "jZDhM6IJNq", "signatures": ["ICLR.cc/2026/Conference/Submission8135/Reviewer_FmPW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8135/Reviewer_FmPW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761779403818, "cdate": 1761779403818, "tmdate": 1762920106315, "mdate": 1762920106315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the mechanisms underlying jailbreak attacks on large language models and proposes a novel defense strategy. The authors identify a universal phenomenon called Attention Slipping, where models gradually reduce attention allocated to unsafe prototypes during jailbreak attacks, enabling these attacks to bypass safety mechanisms."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important research question by investigating the role of attention mechanisms in jailbreak attacks on LLMs. \n2. The paper is exceptionally well-written with clear narrative structure and strong logical flow. \n3. Valuable findings and insights regarding the Attention Slipping phenomenon are presented, enhancing understanding of jailbreak mechanisms."}, "weaknesses": {"value": "1. **Limited Conceptual Contribution:** The observation that attention decreases during jailbreak attacks has been previously reported in prior work, which diminishes the technical novelty of this paper. As the authors acknowledge in Section 5, concurrent studies like RobustKV (Jiang et al., 2025) and AttnGCG (Wang et al., 2025b) have already identified that jailbreaks manipulate attention patterns. The primary contribution of this paper is demonstrating that attention slips *gradually* during the optimization process and that this phenomenon is *universal* across different attack families. The authors are encouraged to further clarify their contributions, especially from a conceptual standpoint, to distinguish their work from existing literature.\n2. **Limited Scale of Evaluation:** It is commendable that the authors analyze several classic jailbreak attacks (GCG, AutoDAN, MSJ) based on the Attention Slipping phenomenon. Is is suggested, however, to extend the analysis to more recent and advanced jailbreak methods, such as those based on chain-of-thought prompting or multi-turn interactions, e.g., Crescendo. This would help validate the universality of Attention Slipping across a broader spectrum of jailbreak techniques. Besides, all experiments are conducted on relatively small models (7-9B parameters), and the effectiveness on larger, more capable models (e.g., 30B+ parameter models) remains unknown. Given that larger models often exhibit different behavioral patterns and may have more robust safety mechanisms, it is unclear whether Attention Slipping remains as pronounced or whether Attention Sharpening remains effective at scale. This limits confidence in the generalizability of findings to state-of-the-art LLMs deployed in production environments.\n3. **Insufficient Evaluation of Performance Impact:** The defense method may inadvertently degrade model performance on benign tasks, particularly for longer inputs where diffused attention could prevent the model from focusing on key information. The evaluation using AlpacaEval (805 prompts, primarily short questions) is insufficient to fully characterize potential performance degradation. More comprehensive evaluations on diverse and longer-context tasks are needed to assess the trade-off between safety and utility.\n4. **Marginal Performance Gains Over Baselines:** The proposed Attention Sharpening shows limited improvement compared to existing defenses like Token Highlighter. In Table 1, the ASR reduction is often modest, and in some cases (e.g., Mistral-7B on WildGuardMix: 29.2% vs. 26.5%), Attention Sharpening actually performs worse than Token Highlighter."}, "questions": {"value": "1. Can this explanation be extended to more recent jailbreak techniques, such as chain-of-thought or multi-turn attacks?\n2. Does the Attention Slipping phenomenon and the effectiveness of Attention Sharpening persist in larger models (30B+)?\n3. How does Attention Sharpening affect model performance when processing longer and more complex inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rdqBzhBle7", "forum": "jZDhM6IJNq", "replyto": "jZDhM6IJNq", "signatures": ["ICLR.cc/2026/Conference/Submission8135/Reviewer_RaK3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8135/Reviewer_RaK3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8135/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924243491, "cdate": 1761924243491, "tmdate": 1762920105801, "mdate": 1762920105801, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}