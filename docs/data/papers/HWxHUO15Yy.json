{"id": "HWxHUO15Yy", "number": 13335, "cdate": 1758216702718, "mdate": 1759897444056, "content": {"title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "abstract": "While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on various problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.", "tldr": "An agentic benchmark and framework for LLM reasoning capability in combinatorial optimization problems", "keywords": ["Benchmark", "Large Language Models", "Combinatorial Optimization", "Code Generation", "Agent", "Automatic Heuristic Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efca781cf1404bbef21f31092881b2593f4ce0e6.pdf", "supplementary_material": "/attachment/011fb3957bcedb639cd3f31ae59dc91bde383505.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces HeuriGym, an agentic framework designed to systematically evaluate large language models (LLMs) on heuristic generation tasks within combinatorial optimization. Its major contributions can be summarized as follows:\n\nThe authors propose a new agentic framework that formalizes the process of LLM-driven heuristic generation through iterative prompting, execution, and feedback. Unlike single-turn benchmarks, HeuriGym allows LLMs to iteratively refine their algorithms by integrating structured feedback such as execution logs, verification outcomes, and cost evaluations. This closed-loop setup mirrors human-like problem-solving and provides a rigorous testbed for assessing LLM reasoning capabilities in constrained optimization settings.\n\nThey also introduce a novel set of evaluation metrics beyond traditional pass@k, most notably the Solveₛ@i metric, which measures whether an LLM can solve constrained problems within a given number of iterations across execution, solution generation, and verification stages. To assess performance quality, they define the Quality-Yield Index (QYI), combining success rate and relative solution quality to reflect both feasibility and optimality.\n\nIn addition, the paper presents a comprehensive benchmark suite of nine combinatorial optimization problems across domains such as electronic design automation, compilers, computational biology, and logistics. These problems were carefully selected to avoid overfitting to well-known tasks (like TSP or SAT) and emphasize reasoning over memorization. Each benchmark includes demonstration and evaluation sets to support few-shot and iterative learning analysis.\n\nFinally, the authors conduct a large-scale empirical evaluation comparing several state-of-the-art LLMs—including GPT-4-mini, Gemini-2.5-Pro, Claude-3.7-Sonnet, and DeepSeek models—under the HeuriGym framework. The results show that while models like GPT-4-mini and DeepSeek-R1 achieve strong reasoning consistency, overall performance still lags behind expert-designed heuristics, highlighting the current limitations and future potential of LLMs for agentic reasoning in optimization tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-presented and systematic framework:**  \n  The paper is clearly organized and methodologically rigorous, providing a formalized agentic framework (HeuriGym) that integrates problem definition, heuristic generation, verification, and iterative feedback in a coherent workflow.\n\n- **Goes beyond standard benchmark tasks:**  \n  Unlike prior works focusing on well-studied problems such as TSP or SAT, HeuriGym introduces diverse, domain-rich combinatorial optimization tasks (e.g., operator scheduling, protein sequence design, airline crew pairing), enhancing the benchmark’s generality and realism.\n\n- **Novel evaluation metrics capturing agentic reasoning:**  \n  The introduction of `Solveₛ@i` and the Quality-Yield Index (QYI) provides a more holistic evaluation than standard `pass@k`, accounting for feasibility, constraint satisfaction, and iterative refinement quality."}, "weaknesses": {"value": "- **Lack of comparison with ILP solvers:**  \n  The paper claims that the proposed framework can solve larger instances than traditional ILP solvers such as Gurobi. However, no quantitative comparison is provided to substantiate this claim. For smaller-scale problems, it would be important to report the optimality gap or at least benchmark against Gurobi solutions to demonstrate relative performance.\n\n- **Missing evaluation against state-of-the-art heuristics:**  \n  While the framework aims to generate heuristics automatically, it remains unclear whether the resulting solutions improve upon or even match the performance of existing state-of-the-art heuristic algorithms in the literature. Without such comparison, it is difficult to gauge whether HeuriGym advances the frontier of heuristic design or merely replicates baseline performance.\n\n- **Marginal novelty in the agentic framework:**  \n  The proposed agentic framework, although well structured, shows limited conceptual innovation beyond existing approaches such as AlphaEvolve. The iterative feedback and refinement loop are largely similar in spirit, and the paper would benefit from introducing more distinctive mechanisms or theoretical insights that clearly differentiate it from prior work.\n\n- **Limited engagement with more advanced related work:**  \n  Recent studies have explored more sophisticated integrations of LLMs with optimization processes. For instance, *van Stein et al. (2024)* proposed an in-the-loop hyperparameter optimization approach for LLM-based heuristic design (*ACM Transactions on Evolutionary Learning*), which achieves a tighter coupling between LLMs and evolutionary optimization. Compared with such work, HeuriGym appears relatively simplistic, relying mainly on AlphaEvolve-style iteration without deeper algorithmic co-adaptation or automated search over LLM configurations."}, "questions": {"value": "1. In the demonstration set, only five small instances were used for each task. This sample size seems too limited to capture the variability and difficulty spectrum of real-world combinatorial problems. Could the authors clarify the rationale behind choosing such a small number of demonstration instances, and whether larger or more diverse demonstration sets were considered?\n\n2. The paper mentions applying a citation-based filtering criterion (less than 1,000 citations) when selecting benchmark problems. This threshold appears somewhat arbitrary and potentially large, as problems with hundreds of citations could already be well-studied. What is the justification for using this particular cutoff, and how does it effectively ensure that the benchmark tasks are not overexposed or memorized by LLMs?\n\n3. What does large solution space mean? It's not rigorously defined."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "teXYlWNSsc", "forum": "HWxHUO15Yy", "replyto": "HWxHUO15Yy", "signatures": ["ICLR.cc/2026/Conference/Submission13335/Reviewer_QPGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13335/Reviewer_QPGo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760904228874, "cdate": 1760904228874, "tmdate": 1762923992772, "mdate": 1762923992772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "It introduces a new benchmark named HeuriGym for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems. It also presents a new criterion with three stages for the benchmark. Experiments were conducted on around 10 LLMs and different important yet not so well-known combinatorial optimization tasks. It contributes to both algorithm development and LLM communities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "LLM-driven heuristic design benchmarking is vital for both algorithm development and LLM communities, serving as an open-ended, challenging, and evaluative benchmark. \n\nThree stages with new criteria are designed for this benchmark."}, "weaknesses": {"value": "Further clarification is needed regarding the methodology for designing the benchmark, specifically concerning the tasks, prompts, and evaluation procedures. \n\nAs a benchmark paper, more comprehensive results, including a greater number of iterations and diverse prompt strategies, are expected."}, "questions": {"value": "1. The problem specifications are generated by prompting DeepSeek-v3 and iteratively refined until the description is unambiguous. However, the optimal prompt for different LLMs (even different versions within the same LLM series) can vary significantly, especially for complex problem-solving tasks. While acknowledging the difficulty of designing a universal prompt, the iterative approach on a single LLM to design the prompts should be reconsidered or further clarified.\n\n2. Please provide reasons for the timeout settings in Table 7. Why are different timeouts assigned? It appears that 10 seconds may be insufficient for many metaheuristics when implemented in Python.\n\n3. Testing evolutionary frameworks, such as EoH and ReEvo, with only 10 iterations might be misleading. Common practice typically involves hundreds of evaluations to effectively assess evolutionary algorithms. Ten iterations with single-point-based refinement (i.e., without a population or with a population size of 1) closely resemble a random search or local refinement. To ensure effectiveness, a population size of 10 or 20 is often used. The settings for assessing evolutionary frameworks should be reconsidered, incorporating more iterations and appropriate population sizes.\n\n4. Furthermore, I do not believe that designing a piece of an algorithm or the entire algorithm constitutes a key methodological improvement or difference in comparison with existing works, as it primarily involves letting the LLM generate longer code or complex metaheuristics. When the template and evaluation block are replaced with new settings, existing methods like EoH and ReEvo can readily be used for entire algorithm design, as evidenced in many recent works. While the benchmark's importance is not disputed, it is better to fit within the existing body of LLM-driven algorithm design research.\n\n5. The selected tasks seem to be either too simple or too hard. For instance, results for the operator scheduling problem show 100% success for most models, even without any iteration. Conversely, why do all LLMs fail on the global routing problem (i.e., 0.0% even after 10 iterations)? Is this due to hard constraints? Would results improve with more iterations?\n\n6. The authors argue that they focus on novel yet foundational problems, rather than commonly used combinatorial optimization tasks such as TSP and Bin Packing. However, establishing clear boundaries for task selection is challenging. Although the authors provide a methodology for selecting tasks based on Google Scholar searches, this approach can hardly guarantee the absence of pattern matching during heuristic design.\n\n7. Could you provide the details on the training and testing instances? Are they of the same distributions and sizes? How is the out-of-distribution performance of design algorithms, which is one of the main limitations of current automated heuristic design methods.\n\n8. A discussion and comparison with related platforms and benchmarks is suggested, such as LLM4AD and Co-Bench.\nLLM4AD: A platform for algorithm design with large language model. arXiv 2024.\nCo-bench: Benchmarking language model agents in algorithm search for combinatorial optimization. arXiv 2025.\n\nFrom my point of view, I think the capability of LLMs to continuously improve and learn from experience is more important than zero-shot generation from prompts for algorithm/heuristic design tasks. It is similar to how human experts design high-performance algorithms through trial and error. Moreover, the results of different LLMs are not robust given different initial prompts, while the performance improvement is always observed during evolution/iterative search. However, assessing this learning capability (or, more broadly, open-ended problem-solving) in a principled manner presents a significant challenge, given its dynamic and costly evolutionary nature."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aXagzyPxBa", "forum": "HWxHUO15Yy", "replyto": "HWxHUO15Yy", "signatures": ["ICLR.cc/2026/Conference/Submission13335/Reviewer_ZY54"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13335/Reviewer_ZY54"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621074065, "cdate": 1761621074065, "tmdate": 1762923992385, "mdate": 1762923992385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HeuriGym, an agentic benchmark for testing whether large language models can invent and refine heuristic algorithms for combinatorial optimization. Instead of closed-ended Q&A, models write Python solvers, run them, see feedback, and iteratively improve under verification, mirroring real engineering workflows. Tasks span nine realistic problems across EDA, compilers, computational biology, and logistics. Performance is scored by the Quality-Yield Index (QYI)—the harmonic mean of solution quality (relative to expert heuristics) and pass rate. Across nine state-of-the-art models, top systems such as GPT-o4-mini-high and Gemini-2.5-Pro reach only ~0.6 QYI versus an expert baseline of 1.0, revealing persistent gaps in tool use, planning, and constraint handling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- It is a well-motivated benchmark that challenges the models in their critical agentic capabilities such as tool-augmented reasoning, multi-step planning, and instruction following.\n- The benchmark involves well-defined continuous objectives, large solution spaces, and agentic settings. They are suitable for benchmarking current fast-evolving LLMs.\n- The paper conducts an extensive evaluation and verifies that the benchmark is challenging for SOTA models.\n- The paper reveals current limitations that are insightful for future methodological development."}, "weaknesses": {"value": "- In Table 4, you use the metric SOLVE@10. Does this mean only 10 generations are allowed for LLM+EA frameworks? This may be an unreasonable budget for EA frameworks, which typically require more iterations to achieve performance gains. Also, is it possible to incorporate feedback from your benchmark into these EA frameworks for a fairer comparison?\n\n- Is it possible to include black-box problems, as emphasized in ReEvo, to test LLMs’ generalizable reasoning capabilities without relying on internal knowledge?\n\n- How does this benchmark compare with other similar benchmarks, such as ALE-Bench?"}, "questions": {"value": "Please see the questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "djhRjIIMGN", "forum": "HWxHUO15Yy", "replyto": "HWxHUO15Yy", "signatures": ["ICLR.cc/2026/Conference/Submission13335/Reviewer_iG2L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13335/Reviewer_iG2L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761747927614, "cdate": 1761747927614, "tmdate": 1762923992057, "mdate": 1762923992057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HeuriGen, a benchmark for evaluating LLMs on combinatorial optimization tasks that require generating and refining executable heuristics through an agentic feedback loop. Covering nine diverse problems across science and engineering, the framework measures both solution feasibility and quality using the proposed Quality–Yield Index (QYI). Results on nine leading LLMs show large performance gaps, revealing current models’ limited capabilities in adaptive reasoning and real-world problem solving."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. I personally appreciate that this benchmark supports not only Python but also C++. Most other heuristic-generation benchmarks are limited to Python, so this cross-language support is valuable for real-world deployment scenarios and makes the benchmark more practical for diverse research settings. The released codebase is also clearly structured and of good quality, which will be appreciated by the community.\n\n2. The problem selection process is interesting and reasonable. I was interested by the use of Google Scholar search and citation statistics as part of the selection criteria.\n\n3. Choosing CO as the core evaluation domain for LLMs is strongly motivated. LLM for CO heuristic generation is a spot topic recently. This choice provides a solid foundation for evaluating the true problem-solving ability of LLMs in realistic, structured scenarios."}, "weaknesses": {"value": "1. One concern is about the token usage. In the feedback loop: “After each iteration, we log the LLM-generated solution, execution trace, verification result, and evaluation score.” I feel that it appears token-inefficient and may not scale well to larger problems or longer iterations. Appendix E.9 shows multimillion-token runs, confirming substantial computational overhead. A more sustainable design might summarize or structure feedback (e.g., key errors, constraint metrics) rather than concatenating full logs, preserving the agentic realism while improving efficiency and model focus.\n    \n2. Another concern is about the fairness of the QYI design. I do like the current F-score-like design, neat and clear. But my *intuitive* feeling is that the current design puts the “quality” and “success rate” at equal importance, while in some contexts, achieving feasible solutions is more critical (at least in the beginning iterations), while in others, solution quality should dominate. Moreover, the binary definition of yield ignores near-feasible outputs and can disproportionately penalize partial success.\n    \n3. The authors mention that “we intentionally exclude ubiquitous problems”. However, I have a different opinion that this complete omission of such tasks may reduce interpretability and comparability. Classical problems serve as essential anchors, providing intuitive performance references, allowing transparent quality validation, and enabling clearer analysis of the heuristics that LLMs discover. Including even a small subset of randomized or reparameterized classical problems would not compromise contamination control and would significantly enhance the benchmark’s scientific and pedagogical value.\n    \n4. While the introduction of a weighted QYI is conceptually appealing, averaging QYI values across heterogeneous problem domains may not be statistically or conceptually justified. QYI is a relative metric that compares each problem’s ratio to its expert baseline, so cross-task comparability is limited by differences in inherent difficulty and cost scaling. For instance, achieving a 0.8 QYI on an arduous routing task may represent stronger heuristic reasoning than a 0.9 QYI on an easier scheduling task, yet the weighted average treats them equivalently."}, "questions": {"value": "1. I have one question not directly related to the proposed contributions: in section 3.1.1, the authors mention that the optimization object and the constraints will be presented as mathematical notations. I saw some papers about LLM for heuristic generation, but they only do this by text description. Based on your knowledge and your experimental experience, do you feel that using math notations instead of pure text descriptions will bring some advantages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1c0BbM4PXK", "forum": "HWxHUO15Yy", "replyto": "HWxHUO15Yy", "signatures": ["ICLR.cc/2026/Conference/Submission13335/Reviewer_NKuf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13335/Reviewer_NKuf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981090982, "cdate": 1761981090982, "tmdate": 1762923991533, "mdate": 1762923991533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}