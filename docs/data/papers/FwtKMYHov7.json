{"id": "FwtKMYHov7", "number": 2961, "cdate": 1757307408864, "mdate": 1763522309427, "content": {"title": "STANCE: Motion Coherent Video Generation Via Sparse-To-dense Anchored Encoding", "abstract": "Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components.\nFirst, we introduce Instance Cues—a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D drag/arrow inputs while remaining easy to user. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with time-addressable rotary embeddings. Paired with joint RGB + auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.", "tldr": "", "keywords": ["Video Generation", "Generative Model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dcb00196385b99164d59c430fb8613633f2432c0.pdf", "supplementary_material": "/attachment/415a700d351d8bd244411f2900432bea49e53253.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes STANCE, a controllable image-to-video framework that focuses on rigid object interactions and aims to improve motion coherence and interaction plausibility. The method converts sparse 2.5D user cues into dense instance-aligned motion fields, enhances them with a Dense RoPE tagging scheme, and jointly predicts RGB and structural maps to stabilize training. Experiments show improved temporal and physical consistency compared to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear problem focus**  \n   The paper identifies practical challenges in motion-coherent video generation, particularly the loss of control signal density and the difficulty of achieving temporal and physical consistency.\n\n2. **Practical design**  \n   The sparse-to-dense instance cue formulation is intuitive, human-editable, and easy to integrate into existing video diffusion pipelines.\n\n3. **Reasonable overall design**  \n   While the components are not entirely novel, the overall framework is coherent and the design choices are appropriate for improving controllability and motion coherence in rigid-object video generation."}, "weaknesses": {"value": "1. **Limitation of Using 2.5D Conditioning**  \n   The method relies only on 2.5D cues for motion conditioning, while recent approaches such as *Diffusion as Shader* [1] have demonstrated the feasibility of incorporating full 3D conditioning for controlled video generation. Since 3D information can potentially enhance both spatial and physical coherence, which are central to this work, the decision to limit conditioning to 2.5D appears restrictive. Moreover, for example, point-based 3D representations are inherently denser than the 2D control masks discussed in the paper and could naturally alleviate the sparsity issue that the proposed method aims to address. A clearer justification for this design choice would strengthen the paper.\n\n2. **Limitation of Dense RoPE Design**  \n   Dense RoPE is an effective heuristic for retaining sparse motion cues, but it does not fundamentally resolve the sparsity of the input control signals. The task itself focuses on representing object motion, yet the current RoPE design does not explicitly account for dynamic spatial changes or motion-aware positional encoding. Recent work on dynamic or trajectory-aligned positional embeddings [2] has explored such aspects, and considering this trend, the use of a static first-frame RoPE feels somewhat outdated and not particularly efficient, making it difficult to see clear advantages of this choice.\n\n3. **Limited Novelty in Joint Auxiliary Generation**  \n   The idea of jointly generating auxiliary structural cues alongside RGB has been explored in prior work (e.g., [3], [4]) and is not particularly novel. While this strategy can help stabilize training and improve motion coherence, similar multi-head or multi-stream supervision schemes have been widely adopted in recent video generation and 3D-aware diffusion models. Therefore, it is difficult to attribute clear novelty to this component. If the authors intend to highlight this as a contribution, it would be helpful to cover relevant prior work in the related works section and clarify how their formulation differs from existing approaches.\n\n[1] *Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control*  \n[2] *RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers*  \n[3] *World-consistent Video Diffusion with Explicit 3D Modeling*  \n[4] *JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers*"}, "questions": {"value": "(3.2.1 Training)  \nHow do you distinguish individual instances here? Are the instance masks generated by a segmentation model, or are they included in the dataset itself? If they are obtained from a model such as SAM during inference, mentioning this explicitly in the training section would make it clearer."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FAJACqB9j6", "forum": "FwtKMYHov7", "replyto": "FwtKMYHov7", "signatures": ["ICLR.cc/2026/Conference/Submission2961/Reviewer_97AZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2961/Reviewer_97AZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787258671, "cdate": 1761787258671, "tmdate": 1762916464207, "mdate": 1762916464207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents **STANCE**, a controllable image-to-video generation framework designed for physically coherent motion and per-instance editing, built upon the CogVideoX (5B-parameter) backbone. The approach targets the gap between intuitive per-object controls and globally consistent motion in video diffusion models.\n\n**Key Contributions:**\n1. Instance Cues: A mechanism that transforms sparse, user-provided annotations, such as object-level arrows, masks, optional mass, and a scalar ∆z into a dense, pixel-aligned 2.5D motion field used to condition the diffusion model. This formulation grounds the motion control in geometric consistency and user editability.\n2. Dense RoPE: A dense control-token strategy employing rotary positional embeddings to preserve spatially localized motion anchors within a DiT-based video diffusion backbone, ensuring better control fidelity and reduced spatial drift.\n\nAdditionally, the framework supports joint generation of RGB frames and a secondary \"structural witness\" (e.g., depth or segmentation), intended to stabilize temporal dynamics and improve motion realism.\n\nQuantitative evaluations using Physics-IQ and FVD demonstrate consistent improvements over strong SVD-based baselines such as SG-I2V, Drag-Anything, MoFA-Adapter, and MotionPro (all ~1.5B parameters). Qualitative results show noticeably improved motion coherence, temporal stability, and control accuracy in both synthetic and simple real-world scenes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear motivation and problem focus\n    - Addresses the specific gap of achieving physically coherent, instance-controllable motion in image-to-video generation rather than general visual quality.\n- Effective integration of control signals\n    - Combines arrows, masks, ∆z, and optional mass into a unified 2.5D motion field, creating an interpretable and editable conditioning scheme.\n- Strong experimental design\n    - Ablations clearly show the contribution of each proposed component, and the method consistently outperforms all SVD-based baselines (~1.5B) in Physics-IQ and FVD despite being built on a larger 5B backbone."}, "weaknesses": {"value": "- Model scale and fairness of comparison\n    - Using a 5B-parameter CogVideoX backbone on a relatively simple synthetic dataset seems excessive. The observed gains may partly arise from sheer model capacity rather than the proposed techniques. Since all baselines (SG-I2V, Drag-Anything, MoFA-Adapter, MotionPro) are SVD-based models around 1.5B parameters, a more balanced comparison would involve using an SVD backbone or a smaller-scale CogVideo variant.\n    - Additionally, instead of full fine-tuning, LoRA-based or other efficient adaptation methods could be explored, especially given the strength of the base model, to verify whether the improvements generalize without retraining the entire network.\n- Computational cost of joint generation\n    - The joint RGB + structure (depth/segmentation) generation likely increases training and inference cost, yet the paper doesn’t quantify the overhead or analyze the trade-off between the added cost and the relatively small metric gain.\n- Limited evaluation scope\n    - Experiments are confined to synthetic or very simple real scenes with short sequences and limited dynamics. The method’s effectiveness on more complex, realistic datasets remains untested.\n- Physics-IQ explanation could be clearer\n    - While the metric itself is cited, providing a short subsection or appendix note summarizing its components and interpretation would make the evaluation more self-contained and easier to follow for readers unfamiliar with the metric."}, "questions": {"value": "- Computational cost\n    - How much additional training time, GPU memory, or inference latency does joint RGB + structure generation introduce compared to single RGB output?\n    - Is there any measurable efficiency or stability gain that justifies this added cost?\n- Evaluation scope\n    - The paper does not explore how STANCE behaves on longer sequences or more complex real-world scenes. Although the model is trained primarily on simple synthetic data, it would be valuable to understand its ability to generalize or adapt to more complex motion and interactions.\n    - The robustness of the method to imperfect user inputs, such as noisy masks, misaligned arrows, or ambiguous object boundaries, is not analyzed. An evaluation of sensitivity to such input noise would strengthen the empirical section.\n- Missing related work\n    - The paper does not cite Wan-VACE, which is relevant as a recent video generation model emphasizing motion coherence and controllable conditioning.It should be discussed and cited in the related works section to provide a complete comparison landscape.\n- Writing style\n    - Although the paper explicitly mentions the use of language-model assistance under **\"G Writing Assistance (LLM Use Disclosure)\"**, some passages, especially those with repeated em-dash usage and overly polished phrasing, sound distinctly machine-generated. A careful human language revision could improve readability and overall presentation quality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SwN91knFeU", "forum": "FwtKMYHov7", "replyto": "FwtKMYHov7", "signatures": ["ICLR.cc/2026/Conference/Submission2961/Reviewer_itDA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2961/Reviewer_itDA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814038529, "cdate": 1761814038529, "tmdate": 1762916463981, "mdate": 1762916463981, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes STANCE, an image-to-video generation method that improves motion coherence and physical plausibility. The key ideas are: 1 Sparse-to-Dense motion cues: converting instance-level user inputs (arrows, masks, mass) into dense 2.5D control fields. 2. Dense RoPE: selecting active motion tokens and tagging them with rotary embeddings to prevent control collapse. 3. Joint RGB + auxiliary map generation (segmentation or depth) to stabilize spatio-temporal consistency. The method is built on CogVideoX and trained on ~200k simulated rigid-body scenes. Experiments show improvements on a Physics-IQ metric and qualitative control results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Clear practical focus on controllable object motion\nThe design targets rigid-body interactions and physical plausibility, which is aligned with real-world applications (AR/VR, robotics, creative tools).\n\n2.Simple modular ideas with observable gains\nSparse→dense cues and Dense-RoPE are easy to implement and demonstrate measurable improvements in control fidelity and Physics-IQ.\n\n3.Reasonable dataset and ablation studies\nThe authors curated a specialized Kubric-style dataset and ablated control injection and joint auxiliary supervision."}, "weaknesses": {"value": "1.Dataset scope is narrow and biased toward synthetic rigid-body scenes\nThe method is primarily validated on artificial Kubric-like collisions, which limits generalization claims. Real-world evaluations are limited to simple tabletop toys and lack diverse environments or camera motions.\n\n2.Limited quantitative evidence and baselines\nQuantitative evaluation relies mainly on Physics-IQ and FVD, without human studies or perceptual realism metrics. Baselines include SG-I2V, DragAnything, MoFA-Video, etc., but more recent state-of-the-art controllable video frameworks are missing (e.g., VACE frameworks).\n\n3.Technical novelty is incremental\nThe contributions largely combine known ingredients—instance masks, flow-derived cues, and RoPE tagging—into a pipeline. While practical, the approach feels like an engineering refinement of existing image/video control pipelines. The method does not propose fundamentally new control paradigms or physical modeling insights."}, "questions": {"value": "Does your method work for non-rigid motion as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jBSqPVb3yz", "forum": "FwtKMYHov7", "replyto": "FwtKMYHov7", "signatures": ["ICLR.cc/2026/Conference/Submission2961/Reviewer_kddo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2961/Reviewer_kddo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976677336, "cdate": 1761976677336, "tmdate": 1762916463517, "mdate": 1762916463517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Summary:**  \nThis paper presents STANCE, a controllable image-to-video generation framework that aims to improve physical and temporal coherence in diffusion-based video generation. The authors identify two key issues in prior methods—loss of signal density after encoding sparse controls and entanglement of appearance and motion supervision—and propose two simple but effective solutions: Instance Cues, which expand sparse user-editable arrows and masks into dense, camera-relative 2.5D motion fields, and Dense RoPE, which assigns spatially addressable rotary embeddings to selected motion tokens, keeping them spatially anchored during generation. The model jointly predicts RGB and an auxiliary structural map (depth or segmentation), which acts as a consistency witness. Experiments on a large synthetic dataset and real-world examples show that STANCE yields more coherent physical interactions and consistent motion, outperforming multiple baselines such as VLIPP, MoFA-Video, and MotionPro."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Strengths:**  \n- Clearly identifies key weaknesses in existing controllable video generation methods: sparse tokenization and entangled training objectives.  \n- Proposes **Instance Cues** that make user input dense, interpretable, and camera-relative, improving control precision.  \n- The **Dense RoPE** mechanism elegantly addresses loss of spatial anchoring after tokenization, preserving effective motion tokens even for small or thin objects.  \n- Joint RGB + structural stream training is simple yet empirically effective in stabilizing geometry and improving contact plausibility.  \n- Comprehensive experiments, including synthetic and real-world captured videos, demonstrate consistent improvements in physical coherence (reported with Physics-IQ metric).  \n- The model allows intuitive editing (direction, speed, mass, ∆z) with visually consistent outcomes and realistic cause–effect relations."}, "weaknesses": {"value": "Weaknesses:  \n1. While practical, the technical novelty is moderate—Instance Cues and Dense RoPE extend existing token-density and positional embedding ideas rather than introducing entirely new principles.  \n2. Real-world validations are limited in scale; demonstrations mostly show toy cases with one or two rigid objects, leaving uncertainty for complex multi-agent or deformable dynamics.  \n3. The method depends heavily on precomputed instance masks and monocular depth estimation, which may constrain general applicability outside clean lab setups.  \n4. It remains unclear how robust the model is when user inputs deviate from the training distributions (e.g., unrealistic mass or velocity values).  \n5. The auxiliary head’s role is primarily empirical; there is little theoretical or analytical discussion explaining why it particularly improves temporal stability."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "tZECOKXTxy", "forum": "FwtKMYHov7", "replyto": "FwtKMYHov7", "signatures": ["ICLR.cc/2026/Conference/Submission2961/Reviewer_jb9S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2961/Reviewer_jb9S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2961/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095185031, "cdate": 1762095185031, "tmdate": 1762916463204, "mdate": 1762916463204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "To all reviewers:\nWe thank you all for your insightful suggestions.\n\nWe realize that our current presentation may have given the impression that STANCE is just another trajectory-control interface on top of an I2V model. This is not our intention. Our design explicitly targets a **learned, physics-aligned rigid-body “simulator”**: Instance Cues turn low-level flow/depth into a human- and model-friendly description of initial conditions (who moves, where, how fast, with what mass), Dense RoPE keeps these conditions stably anchored in token space so the DiT can learn to roll out physically plausible collisions over time, and the jointly integrated auxiliary heads act as a geometry-aware regularizer that stabilizes this rollout. We will clarify this motivation more clearly in the revised version."}}, "id": "VvARDhcd4H", "forum": "FwtKMYHov7", "replyto": "FwtKMYHov7", "signatures": ["ICLR.cc/2026/Conference/Submission2961/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2961/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2961/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763519454178, "cdate": 1763519454178, "tmdate": 1763519454178, "mdate": 1763519454178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}