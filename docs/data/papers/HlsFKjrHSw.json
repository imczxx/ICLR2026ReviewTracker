{"id": "HlsFKjrHSw", "number": 14835, "cdate": 1758244515901, "mdate": 1763716599418, "content": {"title": "Feed-forward Human Performance Capture via Progressive Canonical Space Updates", "abstract": "We present a feed-forward human performance capture method that renders novel views of a performer from a monocular RGB stream. A key challenge in this setting is the lack of sufficient observations, especially for unseen regions. Assuming the subject moves continuously over time, we take advantage of the fact that more body parts become observable by maintaining a canonical space that is progressively updated with each incoming frame. This canonical space accumulates appearance information over time and serves as a context bank when direct observations are missing in the current live frame. To effectively utilize this context while respecting the deformation of the live state, we formulate the rendering process as probabilistic regression. This resolves conflicts between past and current observations, producing sharper reconstructions than deterministic regression approaches. Furthermore, it enables plausible synthesis even in regions with no prior observations. Experiments on both in-domain (4D-Dress) and out-of-distribution (MVHumanNet) datasets demonstrate the effectiveness of our approach.", "tldr": "", "keywords": ["human performance capture; monocular human performance capture; feed-forward reconstruction"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4b1c2ef224d56d7af26b4ce88c0ffe9531ec5c04.pdf", "supplementary_material": "/attachment/b082be7ba154650841328c3881dffb85af48a5a5.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a feed-forward human performance capture method that reconstructs and synthesizes novel views of human subjects from a monocular RGB video stream. The key idea is to maintain a progressively updated canonical space, where visual features from consecutive frames are accumulated and fused using visibility-based weighting. This canonical space serves as a temporal memory that compensates for missing observations in the current live frame. To render novel views consistent with both past observations and current deformation, the authors employ a probabilistic regression formulation based on diffusion models. The probabilistic rendering alleviates blurriness caused by misalignment between frames and enables plausible synthesis even for previously unobserved regions.\nThe method is evaluated on several datasets and compared against deterministic and probabilistic baselines such as NHP, SHERF, Champ, and GauHuman, showing improved perceptual quality and temporal consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper elegantly combines canonical-space-based temporal accumulation with a diffusion-based rendering framework. This hybrid approach effectively mitigates the limitations of deterministic regression. Unlike other optimization-based methods , this approach runs efficiently and generalizes to unseen subjects without per-frame or per-scene training.\n\n\nThe probabilistic rendering formulation allows plausible completion of unseen regions and reduces the dependency on perfectly aligned geometry"}, "weaknesses": {"value": "The method relies on accurate SMPL-X alignment to build temporal correspondences, but the paper does not quantify how fitting errors or template inaccuracies affect the reconstruction quality. Including such results will make this paper stronger.\n\nAlthough the probabilistic rendering can hallucinate plausible details, the canonical space itself remains template-driven, which may limit fidelity for highly non-rigid clothing dynamics, which can be seen from the demo video."}, "questions": {"value": "What is the inference time of the proposed method?\n\nHow long an sequence can it support?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eHs9VQKptO", "forum": "HlsFKjrHSw", "replyto": "HlsFKjrHSw", "signatures": ["ICLR.cc/2026/Conference/Submission14835/Reviewer_BCER"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14835/Reviewer_BCER"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771820778, "cdate": 1761771820778, "tmdate": 1762925186098, "mdate": 1762925186098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to synthesize novel view of human images from monocular videos. At the core of the method is a “progressive feature fusion” module, which fuses features in a canonical space utilizing a parametric SMPL model . The authors conducted experiments on 4D-Dress and MVHumanNet to verify the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow.\n\nSynthesizing novel view images from monocular videos is an interesting task with practical applications.\n\nThe method is technically sound, leveraging a parametric model to fuse features in a canonical space."}, "weaknesses": {"value": "**Insufficient experiments and generalization issues.** \n\nThe paper claimed that the method can reconstruct humans from a monocular RGB stream, but the model was trained and evaluated on multiview video datasets. The experimental results on in-the-wild videos are required to illustrate the generalization capability. \n\n**Insufficient evaluations and comparisons.**  \n\nThe baseline methods (e.g., NHP, SHERF) are old. The comparisons and discussions with SOTA generalizable human generations are not included, such as AniGS, LHM, Human4DiT, and Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior [Guo et al. CVPR 25]. \n\n**Setup.** \n\nWhat are the advantages of this method which requires video as input over one shot method which only requires one image such as LHM, Human4DT?"}, "questions": {"value": "How to handle loose clothing such as dress as the method requires SMPL/SMPLX model and clothing warping is not accurate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "g6961EmZ1A", "forum": "HlsFKjrHSw", "replyto": "HlsFKjrHSw", "signatures": ["ICLR.cc/2026/Conference/Submission14835/Reviewer_a8WJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14835/Reviewer_a8WJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986877586, "cdate": 1761986877586, "tmdate": 1762925185580, "mdate": 1762925185580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a feed-forward method for human Performance Capture from a monocular input video stream. The core is a conditional diffusion model, serving as a renderer, conditioned by a canonical feature context fused from previously seen frames and image features at the current time step. The SMPL-X model is adopted to bridge the canonical context with live frames. The experiments show some good results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper presents an effective combination of existing ideas. As is the usual practice, progressively fusing information from previous frames to a canonical SMPLX model, which can then be naturally warped to live frames and provide some missing features that may be occluded at the current live frame. \nRather than directly render the feature context into an image with pixel-level loss, the authors use it to condition a diffusion model to 'generate' the image with a given camera view. Overall, it's a good combination. \n\n2. The proposed method is technically sound, with clear justifications for each component. I believe it can be reproduced with the given supp details. \n\n3. The experimental validations are reasonable and robust, and evaluate the method across various dimensions, especially Cross-dataset and In-the-wild generalization ability. \n\n4. The method consistently outperforms baselines, and visual results are compelling.\n\n5. This paper, which represents the integration of generative AI with 3D reconstruction / Performance Capture, is an interesting and meaningful direction."}, "weaknesses": {"value": "1. The paper does not explicitly discuss performance over very long sequences (e.g., minutes of video). Does the proposed method suffer from the long sequence forgetting issue? There should be an experiment. \n\n2. Such diffusion-based generative renderers fail to capture human performance accurately. The most evident issue is color bias, as demonstrated in the paper, which overall results in much lower PSNR, along with the synthesis of spurious or unrealistic details.\n\n3. The dependence on the SMPL-X model, especially the overly simplistic fusion strategy based on a moving feature average of SMPL-X vertices, introduces sparsity that directly limits model performance. Furthermore, such an approach fails to account for dynamic garments and, in particular, topological variations.\n\n4. The failure cases should be included in the discussion."}, "questions": {"value": "Mainly listed in weakness. Besides, could you provide quantitative results for the inference latency of your full method? Is it possible to achieve real-time performance capture, e.g., with fewer diffusion steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5EIlnefMj", "forum": "HlsFKjrHSw", "replyto": "HlsFKjrHSw", "signatures": ["ICLR.cc/2026/Conference/Submission14835/Reviewer_qWe6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14835/Reviewer_qWe6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762194959954, "cdate": 1762194959954, "tmdate": 1762925185050, "mdate": 1762925185050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified framework for aligning human motion sequences with multiple modalities (text, video, audio) within a shared embedding space, alongside a novel generative pipeline for motion synthesis conditioned on arbitrary inputs. The key contributions are:\na) MuTMoT: a multi-scale temporal motion Transformer that hierarchically encodes and decodes 3D motion sequences;\nb) REALM: a retrieval-augmented latent diffusion model that utilizes learnable frame tokens and cross-modal conditioning to generate high-quality motion.\nThe model is evaluated across several tasks, including text-to-motion generation, motion retrieval, and zero-shot action recognition."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The paper proposes a modular and extensible architecture that combines multi-modal alignment, contrastive learning, and latent diffusion. The use of learnable frame-level tokens and time-aware modulation is a particularly notable design choice.\n2.The model achieves strong quantitative performance on text-to-motion benchmarks (e.g., HumanML3D), outperforming existing baselines in standard metrics.\n3.The supplementary ablation studies are reasonably thorough, covering most core components. \n4.The architecture appears broadly generalizable to other multi-modal backbones."}, "weaknesses": {"value": "1.Despite claiming robust multi-modal alignment and generative capabilities, the method relies entirely on frozen, pretrained LanguageBind encoders for all non-motion modalities (text, audio, image, video). As a result, the framework lacks novel contributions toward modality-specific understanding. Moreover, only text-conditioned generation is quantitatively evaluated, while other modalities (audio, video, image) are not assessed in the main paper.\n2.The supplementary generation videos exhibit noticeable artifacts, such as foot sliding and physically implausible transitions (e.g., in stands_up_from_a_laying.mp4, the subject appears to float unnaturally), which undermines the claimed motion quality.\n3.Although the model achieves strong retrieval performance, a significant portion of the improvement appears to stem from GPT-4o-based text paraphrasing augmentation. As shown in Sec. B.3, Table 3, removing this augmentation causes R@1 to drop from 69.56 to 62.74. This raises concerns that the architectural contributions alone may not fully account for the observed gains. Clarifying the role of this augmentation and evaluating performance without it would strengthen the claims.\n4.While training and inference are briefly described in the supplement, the paper lacks a clear, step-by-step explanation or diagram of the overall motion generation pipeline, which affects both clarity and reproducibility.\n\nLimitations\n1. The training process is resource-intensive, requiring 8× RTX A5000 GPUs and ~5 days for REALM to converge, which may limit reproducibility and accessibility.\n2. During contrastive training, all non-corresponding modality-motion pairs appear to be treated as negative samples, without consideration for potential semantic similarity. This could penalize semantically related but unmatched pairs (false negatives), potentially degrading the embedding granularity and generalization ability."}, "questions": {"value": "1.Can you provide quantitative or user study results for motion generation from non-text modalities (e.g., video-to-motion, audio-to-motion)?\n2.Could you clarify the impact of frame-wise conditioning versus simpler global token conditioning through ablation?\n3.How are positive and negative samples selected?\n4.How sensitive is the model to the quality or relevance of the retrieved reference motions? And how are the candidate motion embeddings collected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iumH7iGyzg", "forum": "HlsFKjrHSw", "replyto": "HlsFKjrHSw", "signatures": ["ICLR.cc/2026/Conference/Submission14835/Reviewer_jzfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14835/Reviewer_jzfT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762467189123, "cdate": 1762467189123, "tmdate": 1762925184094, "mdate": 1762925184094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a feed-forward human performance capture pipeline that progressively constructs a canonical space representation and formulates the rendering process as a probabilistic regression using a diffusion-based model.\n\nThe method employs SMPL-X to establish consistent 4D correspondences across frames. Multi-scale features are extracted from each frame via ResNet-18, projected onto and accumulated over the vertices in canonical-space with visibility weighting. The resulting sparse vertex features are then back-projected into the target view through barycentric interpolation, forming conditional inputs for a diffusion-based denoising network, which jointly processes them with live-frame encodings to render the final output.\n\nComparisons with both deterministic (per-frame and temporal) and probabilistic regression baselines show that this formulation benefits from canonical-space fusion and probabilistic rendering, yielding superior preservation of fine-grained details.\nExperiments conducted on 4D-Dress and MVHumanNet demonstrate consistent improvements in both quantitative metrics (PSNR, LPIPS-VGG, FVD) and qualitative results, validating the method’s effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Progressive canonical-space updating is simple yet effective. Visibility-weighted accumulation of vertex features efficiently integrates contextual information across frames.\n\n2. Extensive experiments on in-domain, out-of-distribution, and in-the-wild data compare deterministic vs. probabilistic and per-frame vs. temporal variants. The proposed approach achieves superior PSNR, LPIPS-VGG, and FVD scores, supporting the paper’s claim of consistency with prior observations.\n\n3. Reproducbility. The paper and supplementary material clearly report design choices (VAE from sd-vae-ft-mse and U-Net from SD v1.5), context encoder fusion strategy, as well as memory and runtime statistics—facilitating reproducibility and fair engineering assessment."}, "weaknesses": {"value": "1. The methodological novelty mainly lies in the combination of temporal canonical accumulation and probabilistic regression; both diffusion-based rendering and SMPL-X alignment are established techniques.\n\n2. The approach heavily relies on pre-computed or dataset-provided pose fittings, and remains sensitive to occlusion contamination. As acknowledged, large non-rigid deformations or clothing occlusions can imprint incorrect appearances onto the canonical surface, leading to semantic inconsistency.\n\n3. Lack of ablation studies on architectural choices, particularly regarding the selection of the ResNet-18 backbone and the VAE encoder."}, "questions": {"value": "1. Is Visibility map $V_t$ obtained via SMPL-X rasterization or provided by the dataset? How is vertex visibility handled when the subject is partially occluded by clothing or other objects, given that garments exhibit non-rigid motion?\n2. Does the mappings $Warp(S_{can}, X_t)$ depend directly on SMPL-X correspondences? Are these mappings entirely derived from the provided pose estimates? How is it handled during inference in-the-wild data when pose quality may degrade?\n3. Why was a fully feed-forward setting chosen without temporal regularization?\n4. In light of the supplementary note that occlusions may inject incorrect appearances into the canonical representation, have the authors explored confidence weighting of visibility or occlusion detection strategies (e.g., flow consistency or volumetric visibility tests) to mitigate contamination?\n5. Lines 278–290 indicate that the temporal stride $K$ is randomly sampled from the set ${1,5,10}$, a design intended to “encourage the canonical features $S_can$ to capture a rich temporal context.” Could the authors provide an analysis or experimental justification for this choice of $K$? Specifically, how does varying $K$during training affect convergence and temporal consistency, and what impact does using different $K$ values at inference time have on performance?\n\n(Sorry for the mistake earlier — I submitted the wrong one by accident. This is the corrected version.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The model involves processing of real human video data and can potentially be misused for unauthorized human reconstruction or identity replication. Proper consent, anonymization, and responsible data handling should be ensured."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iumH7iGyzg", "forum": "HlsFKjrHSw", "replyto": "HlsFKjrHSw", "signatures": ["ICLR.cc/2026/Conference/Submission14835/Reviewer_jzfT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14835/Reviewer_jzfT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14835/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762467189123, "cdate": 1762467189123, "tmdate": 1763273041569, "mdate": 1763273041569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}