{"id": "NlgDZ1KWzD", "number": 11648, "cdate": 1758202807966, "mdate": 1759897562975, "content": {"title": "On the Convergence of FedProx with Extrapolation and Inexact Prox", "abstract": "Enhancing the FedProx federated learning algorithm (Li et al., 2020) with server-side extrapolation, Li et al. (2024a) recently introduced the FedExProx method. Their theoretical analysis, however, relies on the assumption that each client computes a certain proximal operator exactly, which is impractical since this is virtually never possible to do in real settings. In this paper, we investigate the behavior of FedExProx without this exactness assumption in the smooth and globally strongly convex setting. We establish a general convergence result, showing that inexactness leads to convergence to a neighborhood of the solution. Additionally, we demonstrate that, with careful control, the adverse effects of this inexactness can be mitigated. By linking inexactness to biased compression (Beznosikov et al., 2023), we refine our analysis, highlighting robustness of extrapolation to inexact proximal updates. We also examine the local iteration complexity required by each client to achieved the required level of inexactness using various local optimizers. Our theoretical insights are validated through comprehensive numerical experiments.", "tldr": "We analyzed FedProx with extrapolation and provide convergence guarantees when proximal updates are inexact theoretically and empirically.", "keywords": ["Optimization", "Distributed Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6dee9f9d4211ca483b09a50f1319203ab9a6f275.pdf", "supplementary_material": "/attachment/fb88e28a2efcf03b0c55c0286f9aace2976d1abf.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents an inexact analysis on the convergence of the FedExProx (FedProx with extrapolation, Li et al., 2024a) algoithm under practical inexact updates. For smooth and strongly convex problems, it shows that inexact updates lead to convergence to a solution neighborhood, and relative approximation eliminates bias for exact convergence. The authors further refined their analysis by linking proximal inexactness to biased compression theory, validating the robustness of extrapolation even with imperfect client computations. The theoretical findings are complemented with a set of empirical evidence on federated quadratic optimization and CNN training tasks for validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a practical gap of FedExProx by relaxing the unrealistic exact proximal operator assumption, establishing convergence under inexact updates, which bridges theory and practical FL applications.\n\n2. The paper is generally well organized and clearly presented, with sufficient technical details and proactive supplementary analyses offered to enhance readability and applicability.\n\n3. The idea of linking proximal inexactness to biased compression is somewhat interesting, and it turns out to be useful for showing the robustness of extrapolation to inexact proximal operator evaluations."}, "weaknesses": {"value": "The major concern goes to the novely of analysis and significance of results, given that this work essentially represents a theoretical contribution to FL. \n\n1. The convergence analysis is incrementally novel, mostly extending existing proof techniques without any particularly new ideas/tools developed. While the core results are intuitive and interesting, they are not expected to generate significant impact on FL research, both in thoery and practice. \n\n2. The analysis relies on overly strong assumptions, such as smoothness, global strong convexity, and shared client solutions, which rarely hold in real-world settings (e.g., non-convex, non-smooth deep federated learning). These assumptions, especially when combined, greatly limit the generalizability of the results.\n\n3. The provided experimental study is illustrative but rather weak, in the sense that it fails to align with real scenarios and theoretical scope. Concerning the experiment on quadratic programming (in the main text), only synthetic data is used without comparisons to closely relevant baseline methods like FedAvg (which also inexactly minimizes local objectives with multiple iterations of SGD). Concerning the DL experiments on ResNet training (in the appendix), there exists a clear disconnect between theory and experiment as the key assumptions like smoothness/strong convexity are not fulfilled by the considered DL models. Such a gap weakens the convincingness of this group of experimental results, though they were observed in a more realistic setting."}, "questions": {"value": "Is there any way to analyze the (inexact) FedExProx method in non-convex (or non-smooth) settings that are commonly considered in FL studies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ihhV5kD377", "forum": "NlgDZ1KWzD", "replyto": "NlgDZ1KWzD", "signatures": ["ICLR.cc/2026/Conference/Submission11648/Reviewer_w6Do"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11648/Reviewer_w6Do"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472257190, "cdate": 1761472257190, "tmdate": 1762922712145, "mdate": 1762922712145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies FedExProx, an existing proximal-point method for federated optimization, with inexact local solvers. The analysis of the method is provided under the assumptions of: 1) individual convexity and smoothness, 2) global strong convexity, and 3) Interpolation. Finally, the number of local gradient oracle calls required to achieve the established communication complexity is provided for GD and AGD."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of the paper is clear. The first paper that analyzes FedExProx assumes exact subproblem solvers, which cannot be implemented. \n2. The theory is backed by reasonable proofs."}, "weaknesses": {"value": "The contribution is overall a bit limited. \n\n> Scope \n\nThe main contribution of the paper is to extend the analysis of FedExProx to allow inexact local solutions in the proximal steps. This has already been partially done in [1] (Appendix E) using the definition of Absolute approximation, under PL conditions.\n\nFedExProx was originally proposed and analyzed in [1,2]. These papers also study client sampling, adaptive stepsize, and demonstrate the benefits of extrapolation in terms of communication speed-up under interpolation.\n\nIn contrast, the current submission focuses only on the strong-convexity-with-interpolation setting (full participation and constant stepsize), which is a bit more restrictive. (Note that the current theory breaks when $\\mu \\to 0$.) Moreover, the proof strategy for the outer iterations (communication) follows [1] almost directly, relying on the (S)GD reformulation and Moreau envelope. The main difference is the addition of an error term due to the inexact proximal-point step. \n\n> Novelty and results. \n\nFedExProx can be reformulated as a standard GD iteration (eqns (10)–(11)), with the only change being that the gradient is now biased. The bias term appears in eqn (11), and the analysis follows that of [1, 2] by additionally bounding this bias under the stated assumptions. These error terms then accumulate and appear in the final convergence rate. In this sense, the novelty is limited to handling an additional and simple bias term.\n\nFurthermore, the analysis of GD (or SGD) with biased gradients — and its connection to compression — is well-established in the literature [3]. More connections and better theory can be done under approximately smooth assumptions [4], which is not considered here.\n \n>  Comparisons with other methods\n\nGiven that this work targets federated optimization, the comparisons with other FL methods are a bit limited, both in theory and in experiments. Several state-of-the-art federated proximal-point methods can achieve stronger convergence guarantees (including acceleration), better local computation complexity, under weaker assumptions. The discussions about other federated proximal-point methods, except FedProx, seem to be missing.\n\n\n\n\n\n[1] The Power of Extrapolation in Federated Learning, Neurips 2024.\n\n[2] Tighter Performance Theory of FedExProx, arxiv 2024. \n\n[3] On the Convergence of SGD with Biased Gradients, ICML workshop 2020.\n\n[4] First-order methods of smooth convex optimization with inexact oracle, Mathematical Programming 2013"}, "questions": {"value": "1. It would be great to validate the theory by comparing AGD against GD as different local solvers in numerical experiments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "33DIgVrJi2", "forum": "NlgDZ1KWzD", "replyto": "NlgDZ1KWzD", "signatures": ["ICLR.cc/2026/Conference/Submission11648/Reviewer_Kjjx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11648/Reviewer_Kjjx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786983555, "cdate": 1761786983555, "tmdate": 1762922711624, "mdate": 1762922711624, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an improved convergence analysis of the FedExProx algorithm by removing the assumption that the proximal operator is exactly computed. FedExProx is a recently proposed extension of the FedProx algorithm that combines the proximal local updates of FedProx with the global stepsize schedule proposed in FedExp."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper fills a gap in the analysis of the previous work that introduced FedExProx by removing the assumption of exactness of the local proximal operator."}, "weaknesses": {"value": "- While the paper's analysis is sound and well-presented, its contribution is narrowly focused on a single variant of FedAvg—the FedExProx algorithm. It is unclear how the insights from the analysis apply more broadly and how they guide the design of federated optimization algorithms in general.\n- In the literature review, please cite previous analyses of FedProx, such as FedNova (https://arxiv.org/abs/2007.07481), which includes as a special case the convergence analysis of FedProx (with $\\tau$ local updates at each client, and thus inexact solution of the proximal operator). I also have questions (see below) about how the analysis of this work relates to that analysis of FedProx.\n- The FedExProx algorithm considers an extrapolation parameter $\\alpha_k$ that varies with the round number $k$. However, Theorem 3.2 and Theorem 4.2 assume that $\\alpha_k = \\alpha$, fixed across rounds. Wouldn't this reduce the algorithm being analyzed to FedProx?\n- The simulation result plots are not easily readable. Please increase the font size."}, "questions": {"value": "- In practice, clients are likely to simply perform $\\tau$ local SGD updates where gradients are computed for the local objective plus the proximal penalty term. I don't quite understand how one would find the inexactness $\\epsilon_1$ of the local solution. In the case of local FedProx-style SGD updates, the convergence analysis in https://arxiv.org/abs/2007.07481 can be applied. How does the analysis in this paper (and FedExProx) relate to the prior work? Would you need to consider an $\\epsilon_1$ that decays with the number of communication rounds completed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FP0nmM1tPx", "forum": "NlgDZ1KWzD", "replyto": "NlgDZ1KWzD", "signatures": ["ICLR.cc/2026/Conference/Submission11648/Reviewer_n37t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11648/Reviewer_n37t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11648/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042209381, "cdate": 1762042209381, "tmdate": 1762922710660, "mdate": 1762922710660, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}