{"id": "fMdVvUGrl3", "number": 13829, "cdate": 1758223316675, "mdate": 1759897409859, "content": {"title": "Progressive Memory Transformers: Memory-Aware Attention for Time Series", "abstract": "Self-supervised learning has become the de‑facto strategy for time‑series domains where labeled data are scarce, yet most existing objectives emphasize \\emph{either} local continuity \\emph{or} global shape, seldom both.  We introduce \\textbf{Progressive Memory Transformer} (PMT), a lightweight transformer backbone that maintains a writeable memory bank across overlapping windows, allowing representations to accumulate evidence from short, medium, and long horizons without re‑reading the entire sequence.  On top of our proposed memory-aware attention, we formulate a hierarchical contrastive protocol that aligns embeddings at three complementary granularities---tokens, windows, and full sequences---through a token-window Gaussian loss, a memory‑state loss, and a global \\texttt{[CLS]} loss.  Together, PMT and these multi‑scale objectives yield a task‑agnostic model for time‑series data, providing strong features even when only $1$--$5\\%$ of labels are available.  We validate the approach on seven UCR/UEA/UCI benchmarks on classification tasks.", "tldr": "A hierarchical contrastive learning framework that effectively captures both local and global temporal patterns in time-series analysis by using a progressive memory attention architecture.", "keywords": ["Time-series analysis", "Statefull transformers", "Contrastiive Time-Series Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa39bfad4ac6d4107e55eccb3d95cce311ae77cb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Progressive Memory Transformer (PMT), a sliding-window Transformer architecture that maintains a writable memory bank for each window and layer. Memory states are updated via reset/carry gates and propagated across both time (window-to-window) and depth (layer-to-layer). The model is trained using three contrastive losses at different temporal scales: a hierarchical Gaussian contrastive loss (HGCL) for local structure, a progressive contrastive loss (PCL) for memory supervision, and an instance-level contrastive loss (ICL) for global alignment. Experiments on several small-scale time-series classification datasets (UCR/UEA/UCI) under low-label settings (1–5%) show moderate improvements over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written, with well-explained figures and masking diagrams.\n- The proposed memory mechanism is conceptually sound: instead of read-only caches (e.g., Transformer-XL), it introduces writable, gate-controlled slots aligned to each sliding window.\n- The design of contrastive losses targets local, intermediate, and global temporal structures.\n- Visualization of memory activations and ablations of loss weights are helpful to understand how each component behaves.\n- Computational analysis is provided."}, "weaknesses": {"value": "- The novelty needs to be clarified. The paper mainly combines writable memory (as in Transformer-XL, Compressive Transformer, and Titans) with hierarchical contrastive objectives.\n\n- Evaluation covers only seven small classification datasets with short sequences.\n\n- Transfer learning under in- and cross-domain scenarios is not tested."}, "questions": {"value": "- Could you show ablations replacing writable memory with a read-only cache (Transformer-XL style) to isolate the benefit of “writability”?\n\n- How sensitive is the model to patch size or window length?\n\n- Do the authors think any experiments on broader tasks are reasonable? i.e., softCLT is tested on forecasting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yRrB5tK8tg", "forum": "fMdVvUGrl3", "replyto": "fMdVvUGrl3", "signatures": ["ICLR.cc/2026/Conference/Submission13829/Reviewer_M7TR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13829/Reviewer_M7TR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761875478246, "cdate": 1761875478246, "tmdate": 1762924353313, "mdate": 1762924353313, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Progressive Memory Transformer (PMT), a lightweight transformer architecture for self-supervised time-series representation learning. Unlike conventional stateless transformers, PMT maintains a writable, hierarchical memory across overlapping temporal windows, allowing representations to accumulate information progressively rather than repeatedly re-encoding past segments. In particular, the core of the proposed PMT is Progressive Memory Attention (PMA), which combines causal attention with adaptive gating and reset mechanisms, enabling selective retention, refinement, or overwriting of context. Although this design is reminiscent of an LSTM, the authors do not appear to draw any explicit connection to it. On top of this architecture, the authors propose a three-level contrastive learning framework to capture local nuance, mid-range motifs, and global semantics: i) Hierarchical Gaussian Contrastive Loss – enforces smoothness among nearby tokens and overlapping windows; ii) PMA Contrastive Loss (PCL) – supervises writable memory tokens to capture mid-range temporal motifs; iii) Instance Contrastive Loss (ICL) – aligns sequence-level `[CLS]` representations for global semantics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed memory-augmented transformer for time-series SSL is a timely and well-motivated innovation. The proposed PMT ticks a few boxes of the memory-augmented transformers by maintaining a writable, hierarchical memory across overlapping temporal windows. The authors further propose a multi-scale contrastive loss to capture local nuance, mid-range motifs, and global semantics in one framework.\n- Empirical evaluations on UCR/UEA/UCI benchmarks show promising performance, especially in the few-label regime. \n- The paper is, in general, well-structured. \n- The code is available."}, "weaknesses": {"value": "- The design of PMA bears a strong resemblance to LSTM architectures. However, the authors do not appear to explicitly appreciate or elaborate this connection. Similar to LSTM’s hidden and cell states, PMT maintains a persistent memory that is progressively updated across temporal windows, enabling earlier segments to influence subsequent ones. It also incorporates learnable gating mechanisms that determine how much prior memory is retained, refined, or reset—analogous to the forget, input, and output gates in LSTMs. However, unlike LSTM, PMT appears less effective at capturing long-range dependencies, (as evidenced by its weaker performance on the Ford A/B datasets) due to its dependency on patch size. \n\n- Following my previous point, it might also be interesting to see the ablations on different patch sizes, at least for Ford A/B datasets. \n- Comparisons to prior arts, e.g., Titans, Transformer-XL, on the same setups would consolidate the paper more.\n- The evaluations are limited to the classification task. What about other TS tasks, such as forecasting and anomaly detection ?\n- Although “lightweight,” the hierarchical PMA still appears to be heavy. It might be more interesting to see comparisons to lightweight CNN-based SSL baselines."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MefY271vhe", "forum": "fMdVvUGrl3", "replyto": "fMdVvUGrl3", "signatures": ["ICLR.cc/2026/Conference/Submission13829/Reviewer_iV7V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13829/Reviewer_iV7V"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925114141, "cdate": 1761925114141, "tmdate": 1762924352922, "mdate": 1762924352922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Progressive Memory Transformer (PMT), a self-supervised architecture for time-series representation learning.\nPMT introduces writable, hierarchical memory units that propagate contextual information across overlapping windows, enabling progressive context aggregation without re-encoding the entire sequence.\nThree complementary contrastive objectives are defined:\n(1) HGCL (Hierarchical Gaussian Contrastive Loss) – encourages local smoothness within windows.\n(2) PCL (PMA Contrastive Loss) – aligns intermediate-level memory semantics.\n(3) ICL (Instance Contrastive Loss) – preserves global sequence consistency.\nExperiments on seven benchmarks (HAR, Epilepsy, Wafer, FordA/B, POC, ElectricDevices, etc.) show strong results, especially under low-label (1–5%) regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Architectural novelty:\nThe proposed progressive memory attention (PMA) extends Transformer-XL with writable memory propagation across both temporal windows and model layers. The idea of dynamically updating memory states is original and technically sound.\n\n2. Hierarchical learning objective:\nThe combination of HGCL, PCL, and ICL aligns well with the model hierarchy (token → window → sequence), achieving coherent multi-scale supervision.\n\n3. Comprehensive evaluation:\nThe model is validated on seven diverse benchmarks and includes reasonable ablations on the loss components and qualitative memory visualizations.\n\n4. Interpretability attempt:\nVisualizations of middle-layer memory embeddings illustrate the progressive clustering of classes, partially supporting the claimed representational hierarchy."}, "weaknesses": {"value": "(1). Limited and inconsistent empirical superiority.\nPMT is not consistently better than strong baselines such as TS2Vec and SoftCLT.\nPerformance drops on datasets such as FordA, FordB, and ElectricDevices are attributed to \"patchification loss,\" yet this claim is speculative. No patch-size or stride sensitivity analysis supports it.\n\n(2). Relation to recent global-token approaches.\nRecent work, for example \"Sequence Complementor: Complementing Transformers for Time Series Forecasting with Learnable Sequences\" (AAAI 2025), introduces learnable global tokens that complement local context and achieve similar goals of long-range dependency modeling.PMT’s progressive memory resembles such global or complement tokens, yet the paper does not clearly distinguish whether writable memory offers capabilities beyond static learnable tokens or Perceiver-style latent arrays.\n\n(3). The proposed writable memories, overlap pooling, and gating introduce extra computation, but there is no quantitative comparison of FLOPs, memory footprint, or runtime versus other baseline methods (Transformer-XL or PatchTST)."}, "questions": {"value": "see weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "th2kMW4ef8", "forum": "fMdVvUGrl3", "replyto": "fMdVvUGrl3", "signatures": ["ICLR.cc/2026/Conference/Submission13829/Reviewer_xeyo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13829/Reviewer_xeyo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13829/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979946817, "cdate": 1761979946817, "tmdate": 1762924352499, "mdate": 1762924352499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}