{"id": "jwJkPowRTl", "number": 8162, "cdate": 1758071351020, "mdate": 1759897802656, "content": {"title": "BloomQA: Automated Benchmark Generation from Domain Guidelines Using Bloom's Taxonomy", "abstract": "Open-ended question answering (QA) refers to evaluation tasks where models must generate responses that integrate domain knowledge, contextual understanding, and pedagogical clarity, rather than simply retrieving fixed facts. Such tasks are especially difficult for large language models (LLMs), and existing benchmarks often rely on exam banks or narrow factual datasets that fail to capture performance across various levels in practice-based domains. We introduce BloomQA, a novel framework for automated benchmark generation from domain guidelines using Bloom’s Taxonomy. BloomQA extracts expert-curated practices, converts them into violation scenarios, and expands them into multiple-choice questions (MCQs) and dialogues scaffolded by Remember, Understand, Apply, and Analyze. Applied to teaching and dietetics, our method produces 20k MCQs and 5k dialogues per domain. Psychometric-informed evaluation shows that BloomQA capture difficulty and discrimination, separate strong and weak models, and identify question items that could be problematic. Fine-tuning with dialogue data further improves performance, especially at higher Bloom levels. BloomQA provides a principled and extensible framework for benchmarking LLMs in applied domains with open-ended QA.", "tldr": "", "keywords": ["Bloom’s Taxonomy", "Food AI", "Pedagogy AI"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ad6f52ba5f85dae620217a305f29c9e3f130b10.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "BloomQA proposes a framework for generating domain-specific QA benchmarks from practice guidelines without relying on existing exam banks, demonstrating the approach in the teaching and dietetic domains to produce approximately 20,000 MCQs and 5,000 dialogs per domain. The three-stage pipeline extracts practices from guidelines using LLMs, generates violation scenarios, and expands them into Bloom's Taxonomy-aligned multiple-choice questions. Evaluation is performed with nine LLMs and shows performance separation, with top models like DeepSeek-V3 and Kimi-K2 achieving 69-75\\% accuracy while weaker models approach the chance level. \n\nHowever, the work suffers from fundamental weaknesses in empirical rigor and validation to be suitable for ICLR primary area \"datasets and benchmarks\". The framework is constructed on arbitrary choices lacking justification or ablation studies: quality thresholds (clarity $\\geq 4$, similarity $\\leq 2$), scenario length constraints (80-120 words), dialog turns (20-30) and discrimination thresholds ($\\Delta \\geq 0.2$) are presented without empirical support, and no alternative approaches are explored or compared. Most critically, only four of six Bloom levels are implemented. THe levels Evaluate and Create are omitted without explanation, thus, undermining the core claim of assessing cognitive depth and even the name of BloomQA, particularly since these highest-order skills define expert practice. Validation is severely limited: experts reviewed only 5\\% of scenarios without inter-rater reliability reported, and crucially, no human baseline exists to establish whether the benchmark actually measures expertise as claimed. Psychometric evaluation relies exclusively on GLMM without comparing alternative methods such as IRT or Rasch models. The approach exhibits circular validation where LLMs generate and evaluate their own outputs, propagating systematic biases (five diet items failed due to gender bias discovered only post-hoc). Generalizability remains unsubstantiated, demonstrated only on two domains using U.S. centric guidelines, while the 60\\% generation success rate and removal of nine problematic practices suggest fundamental scalability challenges. \n\nOverall, the paper presents an interesting conceptual framework, but requires substantial empirical strengthening, human validation studies, and systematic comparison of design alternatives before its contributions can be considered robust."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The topic of the work is interesting and timely. As identified in the paper, it would indeed be helpful to make LLM's generate high quality MCQ in practice domains on the basis of guidelines alone. \n\n2. The authors have attempted to use the principles of psychometric tests to evaluate their benchmark. This idea is good.\n\n3. The use of 4 levels of Bloom's taxonomy is a strength."}, "weaknesses": {"value": "1. The paper uses only four of six Bloom's Taxonomy levels by omitting Evaluate and Create levels without justification. These highest-order cognitive skills distinguish expert from novice practice, involving judging, critiquing, designing, and constructing novel solutions—essential for real-world teaching and dietetics practitioners. This omission fundamentally undermines the core claim of assessing ``cognitive depth\" and limits the benchmark's ability to probe advanced reasoning. The MCQ format could accommodate Evaluate questions, and dialogues naturally suit Create-level tasks, making this exclusion particularly puzzling and unjustified for a framework explicitly designed around Bloom's cognitive hierarchy.\n\n2. The paper never establishes how human subject matter experts perform on generated MCQs, creating a critical validation gap. Without human baseline data, there is no evidence that the benchmark measures domain expertise versus LLM-specific response patterns. No criterion in validity studies correlates benchmark scores with real-world competence. This absence is particularly concerning, since LLMs generated all content. More rigorous human validation would confirm whether items discriminate actual expert from novice understanding, whether difficulty levels are appropriate, and whether Bloom-level distinctions reflect genuine cognitive differences rather than mere phrasing variations.\n\n3. The framework generates exclusively violation-based scenarios without empirical justification for the superiority or sufficiency of this approach. Real expert training also involves learning from positive examples such as effective teaching demonstrations and successful diet interventions. No evidence shows that violation-recognition correlates with practice competence or that models identifying violations can recognize correct implementations. This one-sided approach may assess only error-detection rather than full expertise scope: knowing correct actions, understanding why approaches work, and designing novel solutions. Real practitioners must choose among multiple valid approaches for contexts, not simply avoid wrong choices.\n\n4. In addition to the above design choice, several other choices appear arbitrary (neither sufficiently justified or ablated over). Quality thresholds, scenario lengths, dialogue turns, and discrimination thresholds lack ablation studies demonstrating necessity or optimality. No comparison with alternatives are provided. Is the five W framework applicable in all domains? What are the other options? A rich discussion is missing. \n\n5. The entire pipeline exhibits circular validation where LLMs generate practices, scenarios, questions, and answers, thereby, propagating systematic biases through every stage unchecked. The clearest evidence appears in five diet practices flagged for ``male-centered framing\" in women-specific nutrition recommendations, discovered only post-hoc through statistical analysis rather than proactive bias auditing. I appreciate the authors for this discussion in the paper. An independent evaluation and identification on the number of examples to be human evaluated would add value. It is possible that the closed loop creates pattern matching to LLM generation styles rather than genuine domain knowledge assessment, as the same model class producing content also evaluates it.\n\n6. The paper claims framework generalizability \"to any domain with practice guidelines\" but demonstrates only two domains (teaching and dietetics) that have characteristics that may not transfer, such as, well-defined guidelines from authoritative sources, stable best practices, and clear violation consequences. Applicability remains unproven for contested practices, rapidly-evolving fields, domains without formal guidelines, or international contexts. Moderate cross-domain correlation suggests substantial domain-specific effects, yet no analysis identifies enabling characteristics. The U.S.-centric focus (American dietary guidelines, college teaching contexts) further limits generalizability without international validation."}, "questions": {"value": "1. How can one quantitatively estimate whether the benchmark measures domain expertise rather than LLM-specific patterns?\n2. Why were evaluate and create excluded?\n3. Why were violation-based scenarios only considered and not positive exemplars?\n4. What steps are necessary to ensure generalization to any domain?"}, "flag_for_ethics_review": {"value": ["Yes, Discrimination / bias / fairness concerns"]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}, "details_of_ethics_concerns": {"value": "The dataset is US-centric. Appropriate measures to include all ethnic and linguistic groups would be ideal for such research."}}, "id": "VUL6CN8F30", "forum": "jwJkPowRTl", "replyto": "jwJkPowRTl", "signatures": ["ICLR.cc/2026/Conference/Submission8162/Reviewer_uZA6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8162/Reviewer_uZA6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459973538, "cdate": 1761459973538, "tmdate": 1762920127116, "mdate": 1762920127116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes BloomQA, a framework for automatically constructing psychometrically valid evaluation benchmarks from domain knowledge guidelines, without relying on existing exam banks. Guided by Bloom’s Taxonomy, the method performs (1) LLM-assisted extraction and structuring of actionable domain practices, (2) generation of violation scenarios for quality-controlled assessment contexts, and (3) expansion into Bloom-aligned multiple-choice questions and scaffolded dialogues to support both evaluation and fine-tuning.\n\nTo validate the framework, the authors adopt a “model-as-student” paradigm, showing that BloomQA benchmarks exhibit clear cognitive-level difficulty hierarchies, effectively differentiate models with varying capabilities, and achieve strong reliability and validity. The work further releases large-scale datasets spanning multiple professional domains and demonstrates the extensibility of this approach to building practice-grounded evaluation resources."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes an automated framework that extracts domain practices and converts them into implicit violation scenarios, effectively reducing dependence on existing question banks and expert-crafted items. This provides a scalable solution for constructing psychometrically meaningful benchmarks in practice-grounded domains.\n\n2. The evaluation methodology is rooted in psychometric principles, offering reliable measurements of difficulty, discrimination, and bias. Such design ensures that the benchmark can robustly distinguish models of different capability levels while maintaining data quality and fairness.\n\n3. The empirical analysis yields a valuable observation that LLMs exhibit cognitive-level performance patterns distinct from humans when aligned with Bloom’s hierarchy. This insight highlights the importance of domain-specific cognitive evaluation and informs future work on model analysis and improvement."}, "weaknesses": {"value": "1. Although the benchmark creation process is grounded in real-world guideline documents, heavy reliance on LLMs introduces potential generation bias and factual inaccuracies that expert checks (limited to only 5% of scenarios) may not sufficiently capture. Moreover, the paper does not report quantitative findings from expert validation, leaving uncertainty about the actual extent and nature of residual errors.\n\n2. The work lacks a direct comparison with existing domain-specific benchmarks, making it difficult to evaluate the practical advantages of BloomQA in terms of discrimination, coverage, or predictive relevance to real-world performance. Additional experiments contrasting model behaviors on BloomQA versus prior resources would better justify the necessity and contributions of this new benchmark.\n\n3. While the framework is positioned as domain-agnostic and extensible, its empirical scope is restricted to psychometrically-validated MCQs in two domains. Including supplementary demonstrations in other fields (e.g., law, medicine, engineering) would provide stronger evidence for generalizability, especially given the general framing of the paper’s claims."}, "questions": {"value": "1. The document indicates that dialogue data used for model fine-tuning can improve the performance of LLMs on higher Bloom’s taxonomy levels (e.g., Apply, Analyze). How is the generation quality of this dialogue data (such as logical coherence and domain expertise) evaluated? Has a corresponding quality scoring system been established?\n2. How were the clarity and similarity thresholds in Equation 1 determined?\n3. Regarding the thresholds where α = 0.05 and $\\Delta_{Bloom}$ above 0.2 are often considered very meaningful, how were these thresholds determined?\n4. What does β represent in Line 300?\n5. The in-text equations between Line 298 and Line 318 are not numbered and need to be corrected."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "liWscuwSTc", "forum": "jwJkPowRTl", "replyto": "jwJkPowRTl", "signatures": ["ICLR.cc/2026/Conference/Submission8162/Reviewer_4f9y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8162/Reviewer_4f9y"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477000191, "cdate": 1761477000191, "tmdate": 1762920126603, "mdate": 1762920126603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "As a reviewer from the field of education, I appreciate the authors’ attempt to integrate Bloom’s taxonomy with multiple-choice questions (MCQs). However, I believe the manuscript would be stronger if it focused more clearly on educational aspects, such as applications across different disciplines, rather than broadly discussing the generation of MCQs. Therefore, I suggest removing the sections related to food and nutritionist domains from Chapter 2. The remaining parts should also be streamlined to avoid thematic inconsistency. Alternatively, the authors could focus specifically on the food and nutritionist domain instead of quickly shifting between different fields."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The integration of Bloom’s taxonomy with multiple-choice questions (MCQs) is an interesting and meaningful exploration. It holds practical value in education and could help teachers reduce their workload."}, "weaknesses": {"value": "Although the study appears to aim for a broad exploration of QA generation (at least as suggested by the title), the target domains are confusing and the overall content lacks focus."}, "questions": {"value": "It is recommended that the authors either concentrate their investigation on training for food and nutrition professionals or broaden their focus to include multiple disciplines within education."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The parts of the study involving human scoring or evaluation may require ethical review or approval. The authors are advised to clarify whether such procedures were reviewed by an ethics committee and to include relevant information in the manuscript if applicable."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AtwDJvUgZz", "forum": "jwJkPowRTl", "replyto": "jwJkPowRTl", "signatures": ["ICLR.cc/2026/Conference/Submission8162/Reviewer_tvNd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8162/Reviewer_tvNd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638478169, "cdate": 1761638478169, "tmdate": 1762920126229, "mdate": 1762920126229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Bloom QA is a benchmark for open-ended question answering in applied domains, particularly focusing on education and food and diet. It has three contributions: 1) BloomQA framework that transforms domain guidelines into psychometrically-validated MCQs, 2) two datasets of Teach-QA and Diet-QA, each of which is validated in consultation with domain experts, and 3) empirical validation that rigorously evaluated the quality of the benchmark by looking into 1. alignment with Bloom’s taxonomy 2. identification of weak vs strong models (fair test in the sense of difficulty) 3. balance of the benchmark (equal percentage of different Bloom tasks/areas)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper took a novel initiative in turning LLM evaluation into a role-play scenario where models are students who are cognitively involved in some applied domain. The choice of education and Bloom's taxonomy inspiration has been interesting. The paper has a clear and psychometrics-informed evaluation in the later sections of the paper and looks at different combinations of scenarios such as Model × Domain, Model×Practice, and Model×Practice×Bloom.\nBloom-aligned multiple-choice options help identify different cognitive depth and engagement within the same scenario."}, "weaknesses": {"value": "Although the paper has an innovative way of defining and evaluating the benchmark, there are still some questions that can be addressed.\n\n- The paper should state early (Abstract/Intro) that MCQs are auto-graded, not by an LLM judge. Right now, this is only explicit in the evaluation appendix and dataset description. Also, it is unclear how the MCQ and multi-turn scenarios are evaluated. There are some mentions of rejection policies, but how does the LLM-human collaboration work together in this approach?\n- Scenarios/dialogues are generated with GPT-4o-mini. The paper should discuss whether using an OpenAI family model to author data could advantage (or disadvantage) related families at evaluation time. A short bias-scan is useful, but a family-holdout (“author one family, test on others”) would strengthen the claim.\n- Only four levels of Bloom's taxonomy, of all six, are used. It also does not analyze how performance at lower levels conditions performance at higher ones, or justify excluding Evaluate/Create for the benchmark. Also, evaluation across different Bloom taxonomy levels is conducted in a flat, rather than hierarchical, structure.\n- The discrimination thresholds (∆model 0.20/0.50; ∆bloom 0.10/0.30) are labeled “heuristic”, but the practical significance choices need stronger justification.\n- Minor: Some acronyms appear before the definition or are repeated."}, "questions": {"value": "Most of the concerns are listed in the earlier sections, but clarification on the following points would be helpful:\n\n- Since GPT-4o-mini authored scenarios/dialogues, did you run any “author-family vs. non-author-family” robustness checks?\n- Why did the authors choose 4 out of 6 Bloom levels? How would you be able to use the hierarchical structure of the taxonomy in your evaluation and benchmark? I would like to know how one can infer mastery in a lower level of the taxonomy if the student fails at a higher level; there should be some connection between Bloom levels in the dataset.\n- What empirical or pedagogical rationale supports ∆model≈0.20 and ∆bloom≈0.30 as “meaningful” cutoffs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XI10jryXKo", "forum": "jwJkPowRTl", "replyto": "jwJkPowRTl", "signatures": ["ICLR.cc/2026/Conference/Submission8162/Reviewer_cyjw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8162/Reviewer_cyjw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8162/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059457099, "cdate": 1762059457099, "tmdate": 1762920125835, "mdate": 1762920125835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}