{"id": "c6iHKR4zTX", "number": 17578, "cdate": 1758277790612, "mdate": 1759897166636, "content": {"title": "Latent-Informed Energy-Based Models with collaborative generator training", "abstract": "Energy-based models (EBMs) have established a distinct niche in generative modeling through their architectural flexibility and expressive density estimation. However, they have yet to achieve mainstream adoption due to their training challenges. \nIn this paper, we propose training latent-variable EBMs that leverage self-supervised representation learning to derive informative target latent variables. This joint space optimization enables the energy function to capture both data distribution and semantic manifold geometry. To avoid long-run MCMC sampling, we introduce an auxiliary generator  with specialized training designs for effective energy-generator collaboration. Experiments show our approach significantly boosts the generation performance compared to current EBMs with fewer MCMC steps and smaller networks. We also demonstrate the capabilities of our model across multiple tasks, including out-of-distribution detection, conditional sampling, and zero-shot image restoration.", "tldr": "", "keywords": ["Energy-based model", "latent representation", "generator", "collaborative training"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/998b43ee471c67168a6bc8a6ce51f5a05800ff74.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a Latent-Informed Energy-Based Model (LIEBM) that integrates a pretrained self-supervised encoder to a en EBM (as in CLEL (lee et al. 2023)) and uses cooperative training approach to improve training and sampling efficiency of EBMs (as in Xie et al. 2020). \nSpecifically, it formulates a latent-variable EBM where the latent codes are derived from a fixed self-supervised representation, enabling the energy function to capture both data distribution and semantic structure. The model is trained jointly with an auxiliary generator that initializes MCMC sampling through a single-step transformation, reducing the need for long-run chains. Two generator training strategies are explored — energy distribution matching (EM) and energy-real distribution matching (ERM) — designed to promote stable and collaborative optimization between the energy model and generator. The approach is empirically evaluated on several datasets for image generation, out-of-distribution detection, conditional sampling, and zero-shot image restoration, showing competitive quantitative performance compared to prior EBM variants."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-motivated by a clear practical challenge in training energy-based models, namely the inefficiency of long-run MCMC sampling. It offers an interesting empirical exploration of combining pretrained self-supervised representations with cooperative training to stabilize and accelerate EBM learning. \nThe experimental section is extensive and covers diverse downstream tasks — including unconditional generation, OOD detection, and zero-shot restoration — which demonstrates the versatility of the proposed framework in practice. \nThe integration of a semantic latent space into the EBM objective is conceptually appealing and aligns with current trends in using pretrained representations to enhance generative modeling. \nThe paper is also well-organized, with several quantitative comparisons to illustrate the potential benefits of the approach despite its methodological overlap with prior work."}, "weaknesses": {"value": "- **Novelty:** The idea barely uses cooperative training which already relies on a generator to accelerate MCMC sampling from training an EBM. The only difference here is that they augment the observed sample X with a \"latent\" variable Z, which in fact is simply an encoded version of X using an encoder and a random augmentation. Hence, if one thinks about the augmented variable \\tilde{X} = (X,Z) as the new observation. there is really no difference with cooperative sampling of dual-MCMC already introduced in prior works. Additionally, the idea of using latent constructed using a pre-trained encoder + augmentation is exactly the one from CLEL (lee et al. 2023). This makes the methodological contribution very weak. \n\n- **Clarity:** The description of the generator training in section 3.2 is not clear enough. It heavily relies on the prior works in  Xie et al. 2020 (section 3.2.1)  on cooperative learning and Cui & Han (2023) on dual-MCMC (section 3.2.2) without providing enough context for the reader not familiar with these approaches. Notably, it remains unclear how the conditional p_{\\theta}(x|m) is defined and whether, in equation 9, the initial sample of the chain x_t^0 is obtained using x_i^0 = G(m_i) or using  another randomly sampled value for m.  While the original papers provide clear explanations, the present one is hard to understand without reading those papers first. Providing a precise algorithmic description could help. \n\n- **Erratic choice of baselines from an experiment to another:** Dual-MCMC appears in the table 1,2, 3 and 5 but not in table 4 and 6. Same for CLEL, it appears in table 1, 4, 5 but not in 2, 3 and 6. These are the most important baselines, other baselines exhibit the same treatment, they seem to randomly appear in some tables but not in others without clear justification."}, "questions": {"value": "- In equation 9 is x_t^0 initialize using x_i^0 = G(m_i) or using another randomly sampled m? How is the conditional p_{\\theta}(x|m) defined?\n\n- What are the ground truth images in table 5? \n- Why some important baselines such as Dual-MCMC and CLEL appear in some tables and not others?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IhrQKhsZqH", "forum": "c6iHKR4zTX", "replyto": "c6iHKR4zTX", "signatures": ["ICLR.cc/2026/Conference/Submission17578/Reviewer_xbAd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17578/Reviewer_xbAd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761556057242, "cdate": 1761556057242, "tmdate": 1762927438640, "mdate": 1762927438640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the issues such as mode collapse and exact alignment in generative prior with the utilization of pre-trained self-supervised representations as independent variables to guide the energy functions. The authors claim that the method achieves good quality across multiple tasks with lightweight architectures."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The paper addresses the prior-hole problem in latent EBMs with self-supervised pretrained spherical latent encoder, thus decoupling the semantic latent space from the generative prior..\n3. The paper addresses that they achieved good quality performance in multiple tasks with lightweight architectures."}, "weaknesses": {"value": "1. Limited novelty and incremental improvement: The proposed LIEBM addresses the  mode collapse and exact alignment in generative prior with the utilization of pre-trained self-supervised representations as independent variables. However, frameworks such as Energy Matching [1], VAPO [2] and Action Matching [3] eliminate the need for auxiliary models and avoid adversarial instability by formulating training as a variational PDE problem. These methods provide cleaner and general formulations, but not discussed and referenced in detail, which weakens the fundamental formulation of the proposed framework. \n2. Incomplete ablation study: The authors claim that spherical latent posterior is better than gaussian posterior. But there’s no experimental evidence found to support this statement.\n3. Validation of lightweight claims: There’s no results of computational metrics (cost/time/etc) that demonstrate the lightweight characteristics of the proposed model.\n4. Incomplete comparison of EBMs baselines: The work does not provide a comprehensive comparison with other EBM-based models such as Energymatching and VAPO. Lack of engagement with these SOTA limits the impact of the proposed framework.\n\nReferences:\n[1] Balcerak, Michal, et al. \"Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling.\" NeurIPS 2025.\n\n[2] Loo, Junn Yong, et al. \"Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching.\" TMLR 2025.\n\n[3] Neklyudov, Kirill, Daniel Severo, and Alireza Makhzani. \"Action Matching: A Variational Method for Learning Stochastic Dynamics from Samples.\" ICLR 2023."}, "questions": {"value": "Please consider addressing the weaknesses noted above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lw51tK4doZ", "forum": "c6iHKR4zTX", "replyto": "c6iHKR4zTX", "signatures": ["ICLR.cc/2026/Conference/Submission17578/Reviewer_BDpZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17578/Reviewer_BDpZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969049937, "cdate": 1761969049937, "tmdate": 1762927437939, "mdate": 1762927437939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a method for learning a latent EBM. During sampling, a generator network provides an initialization of the sample, which is then further refined through MCMC sampling for a few steps. To learn such a model, ideas are adapted from recent EBM works CLEL, Cooperative Learning, and Dual MCMC. The joint distribution of observed and latent variables follows CLEL and obtains z|x using a random augmentation on x and encoding it to a z on the unit sphere using a pretrained contrastive model. Learning the latent energy function uses the CLEL closed-form distribution of z|x that measure the alignment between z and a learnable contrastive encoding of x, which is also used to define the energy function. A generator is used to initialize MCMC samples and trained in an alternating manner with the energy function using a reconstruction loss to push generator samples closer to the energy refinement. Finally, a inference network is introduced to also encourage the generator to match the distribution of the training data with a variational loss based on the Dual MCMC EBMs. Experiments show strong performance on CIFAR-10 and Celeb-A relative to other EBM methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This work achieves strong generative and among EBMs.\n* The method brings together several different directions in EBM learning to synthesize their strengths into a single model.\n* The proposed model has good OOD performance relative to other EBM methods."}, "weaknesses": {"value": "* There is not much methodological novelty in the paper. Learning the energy function uses the framework of CLEL, except that a pretrained encoder is used rather than a learned encoder. Cooperative learning has been used in the same way as this paper in a variety of previous works. The work is essentially a straightforward combination of CLEL and cooperative learning.\n* The method requires a large number of interacting models: a pretrained SimCLR encoder, a learned encoder $f_\\phi$ and $g_\\psi$, a generator network $G$, and a separate inference network. This contrasts with the relatively simple setup of diffusion models.\n* The empirical results still lag behind GAN and diffusion models. There is no investigation into whether the proposed methods can scale well with high dimensional and more complex distributions like high resolution ImageNet.\n* The OOD comparisons might not be entirely fair, because the OOD calculation uses a network $h$ that is pretrained on large scale data as a contrastive model, rather than using only models learned from the generative process and target dataset as done in prior works.\n* The distribution $p_{data} (z | x)$ in its current form is somewhat arbitrary/heuristic since it depends on a set of hand-crafted augmentations"}, "questions": {"value": "* Does the parameter count in Figure 2 include the energy function and generator only? I assume it does not include models that are unused for inference, like the pretrained encoder and variational network. How many parameters are used for the energy, and how many for the generator?\n* Can you compare the inference time rather than parameter count for the methods listed?\n* What function is used for $g$ in the formula $g(f_\\phi (x))$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1recMxf7Ef", "forum": "c6iHKR4zTX", "replyto": "c6iHKR4zTX", "signatures": ["ICLR.cc/2026/Conference/Submission17578/Reviewer_kJhT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17578/Reviewer_kJhT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762217823152, "cdate": 1762217823152, "tmdate": 1762927437444, "mdate": 1762927437444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an energy-based model (EBM) framework that effectively trains a latent-variable EBM by leveraging (i) pretrained self-supervised models and (ii) cooperative learning with an auxiliary generator. The pretrained model helps the EBM efficiently capture semantic information, while the auxiliary generator accelerates MCMC sampling. Experimental results demonstrate the superiority of the proposed method over existing EBMs in unconditional image generation, and further highlight its versatility across various downstream tasks, including out-of-distribution detection, conditional sampling, and image restoration."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper effectively integrates existing EBM frameworks, including CLEL, cooperative learning, and Dual-MCMC.\n- The proposed framework shows competitive or superior performance compared to EBM baselines on several benchmarks, such as CIFAR, ImageNet-32, and CelebA.\n- The paper provides various analyses and ablation studies that demonstrate the effectiveness of the proposed EBM."}, "weaknesses": {"value": "- Although this integration itself can be viewed as a contribution, the lack of methodological innovation may weaken the novelty of the work. I found that LIEBM is essentially identical to CLEL except for the use of pre-trained SSL models; the negative sample augmentation strategy has already been proposed in EBM-CD; and the EM process follows cooperative learning. Beyond integrating these components, what additional contribution does this paper make?\n- CDRL-large achieves 3.68 FID on CIFAR-10, which should be reported in Table 1. With this result, the proposed EBM outperforms other EBMs on CelebA-HQ-256 and ImageNet-32, but not on CIFAR-10 or CelebA-64. This also weakens the claimed contribution.\n- Scalability should be further tested. The proposed method has been evaluated only on high-resolution but less diverse data (i.e., CelebA-HQ) and on diverse but low-resolution data (i.e., ImageNet-32). Is there any empirical evidence that this model can scale to both diverse and high-resolution datasets, such as ImageNet-256? This point is particularly important nowadays, as many generative models have successfully demonstrated such potential.\n- Minor comments and questions\n  - Comparisons of training efficiency (e.g., training time and memory consumption) should include not only other EBMs but also other classes of generative models such as diffusion models.\n  - Is it possible to leverage marginal MCMC (Eq. 9) and joint MCMC (Eq. 10) simultaneously to achieve both faster training and the ability to learn multimodal distributions? For example, one can use joint MCMC first at the early stage, and then use marginal MCMC for improving diversity.\n  - I am also curious about the training stability. Since MCMC sampling often causes instability in EBM training, did you ever observe any issues (e.g., NaN) while training LIEBM?\n  - After training the generator $G$, it is directly usable for generation. How is the generation quality?\n- Typos and editorial comments\n  - Most EBM literature uses the \"negative\" unnormalized log-likelihood for energy functions. So I recommend using $p_\\theta(x,z)\\propto\\exp(-E_\\theta(x,z))$.\n  - In L116-117, is $v$ a random augmentation? $v$ and $\\mathcal{V}$ appear for the first time there, but no explanation is given.\n  - In Eq. (5), $g$ is defined as a mapping from $f_\\phi(x)$ to the energy value. What exactly is $g$? Like CLEL, is it the norm of $f_\\phi(x)$? Furthermore, the same notation $g$ is reused in Eq. (7), which is confusing. I suggest using a different symbol for clarity.\n  - In L180, should it be $x_i^0=G(m)$ or $x_i^0=G(m_i)$?\n  - In Eq. (13), I think $G(m)$ should be replaced by $x$.\n  - In Eq. (14), I think $\\log$ is omitted.\n  - The energy functin $E_\\theta$ and the inference model $q_\\alpha$ are parameterized by $\\theta$ and $\\alpha$, respectively, but $G$ is not. Why?\n  - There is no overall figure or and algorithmic description of the proposed method. Also, it is unclear how the models (i.e., energy function $E_\\theta$, generator $G$, and inference model $q_\\alpha$) are exactly trained. For example, are they trained alternatively or jointly? Which architectures are used for the models $E$, $G$, and $q$? Since implementation details are crucial in EBM training, they should be clearly described."}, "questions": {"value": "See the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GGZwwxkItn", "forum": "c6iHKR4zTX", "replyto": "c6iHKR4zTX", "signatures": ["ICLR.cc/2026/Conference/Submission17578/Reviewer_igVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17578/Reviewer_igVn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17578/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762443235595, "cdate": 1762443235595, "tmdate": 1762927437089, "mdate": 1762927437089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}