{"id": "zXHiyA8OGq", "number": 21187, "cdate": 1758314709891, "mdate": 1759896937094, "content": {"title": "Looking beyond the next token", "abstract": "The most natural way to model language is rarely autoregressive. The structure of causal language model training assumes that each token can be predicted from prior context, a process that contrasts with humans’ natural writing and reasoning process, which is often non-linear and hierarchical. While this mismatch is well-documented, the working assumption has been that architectural changes are needed to address it. \nWe argue that by simply rearranging and modifying the training data, models can more accurately imitate some aspects of the true data-generating process without any changes to the architecture or training infrastructure. We introduce Trelawney, a purely data-centric method that modifies the training data by interleaving sequences with special lookahead tokens that contain future information. This simple data augmentation, requiring no changes to model architecture or training infrastructure, equips models to both condition on future goals and generate them. We present representative results on high-entropy tasks like path planning, algorithmic reasoning, zebra puzzles, and controllable generation, demonstrating improved performance on tasks with branching paths or long-horizon planning. Finally, our method enables the generation of plausible long-term goals at no additional cost, potentially opening doors to new capabilities beyond the current language modeling paradigm.", "tldr": "Data centric, model agnostic approach to better planning and controllable generation.", "keywords": ["Data centric", "Next token prediction", "Controllable generation"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/348e0914d0b38be6d1254646e60b9522f1d09690.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TRELAWNEY, a data-centric method designed to enhance the planning capabilities of autoregressive language models without altering the model architecture or training infrastructure. The core idea is to augment the training data by interleaving original sequences with special lookahead tokens that contain information about future parts of the sequence. The motivation stems from the limitations of the standard next-token prediction (NTP) objective, which struggles with tasks requiring long-range dependencies or non-linear reasoning, often failing to learn crucial early decision points due to teacher forcing. By exposing the model to future sub-goals or segments during training, TRELAWNEY aims to improve performance on tasks like path planning, algorithmic reasoning, and controllable generation, and potentially enable models to generate their own long-term goals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes an interesting and notably simple approach to potentially improve LLM planning abilities solely through data augmentation. Modifying the data factorization to include future information is an intuitive way to address known limitations of autoregressive NTP.\n\n- The motivation is well-presented and easy to understand. The discussion of NTP's drawbacks (like the \"Clever Hans Cheat\" and \"Indecipherable Token Problem\") effectively highlights why alternative training signals might be beneficial.\n\n- The method requires no changes to the underlying model architecture or complex training objectives, making it potentially easy to integrate into existing pre-training or fine-tuning pipelines."}, "weaknesses": {"value": "- The primary concern revolves around the method's scalability and applicability to more general and complex tasks beyond the specific benchmarks tested. While the results on path planning, algorithmic reasoning (SCC), Zebra puzzles, and the relatively simple TinyStories generation task are encouraging, these are still quite structured or constrained domains.\n\n- It remains unclear whether this data augmentation strategy would be effective for tasks requiring more sophisticated reasoning, such as complex mathematical problem-solving or nuanced long-form text generation, where identifying meaningful \"future\" segments to insert might be non-trivial or less impactful.\n\n- Since the paper's contribution is almost entirely empirical, resting on the performance gains shown in these specific tasks, the lack of strong evidence for broader applicability makes the overall impact uncertain."}, "questions": {"value": "- Could the authors elaborate on how the TRELAWNEY augmentation strategy might be applied to more general reasoning tasks, such as solving mathematical problems? What kind of \"future\" information (e.g., intermediate steps, final answer structure) could be inserted, and would it provide a meaningful signal?\n\n- In the story generation task (Section 4.4), the specific format used for the lookahead token is \"I want the [k]-th sentence from here to be [s]\". This phrasing seems quite specific to writing tasks. Is there a particular reason for this choice? Does this specific, natural language phrasing risk overfitting to the TinyStories dataset structure or this particular type of instruction, potentially limiting generalization?\n\nRegarding the experimental details:\n\n- In Figure 5 and Table 2 (Appendix), what does the \"Draw\" category represent in the GPT-4 evaluation? (Presumably, cases where GPT-4 rated the TRELAWNEY and baseline outputs as equal)\n- In Table 2, what exactly does the \"Few-shot\" condition entail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "on1Zio2SvM", "forum": "zXHiyA8OGq", "replyto": "zXHiyA8OGq", "signatures": ["ICLR.cc/2026/Conference/Submission21187/Reviewer_t56p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21187/Reviewer_t56p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761477649872, "cdate": 1761477649872, "tmdate": 1762941594773, "mdate": 1762941594773, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a data augmentation procedure called Trelawney which aims to make models better at learning long-range dependency information. This is an effort to address previously identified issues with in models trained auto-regressively with teacher forcing to solve certain kinds of planning tasks. During training Trelawney injects information from later in the sequence into the context, between two special tokens. This aims to have the model learn to represent information about the future earlier during generation - the authors make an analogy between this and planning. Experiments show this approach can improve performance on 3 different toy tasks aimed at assessing this kind of goal-directed planning. Additionally authors show when leveraged during pre-training this augmentation doesn't impair language modelling performance and can improve a particular kind of controllability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall the Paper is relatively well written, and the results are clearly presented. Additionally the approach introduced is general and can be applied off the shelf to existing decoder-only models with only a modification to include the special tokens indicating when future information is presented. The authors make an effort to show results on small tasks as well as pre-training. While the models are relatively small a 1B model seems sufficient to show the approach can scale."}, "weaknesses": {"value": "Overall, the main weaknesses appear to be in terms of the paper's novelty and evaluation. To highlight two parts of the evaluation that are concerning:\n\nFirst, the results in table 1 show that inserting random future sequences (z) is more effective than inserting sequences that are deliberately selected to be relevant to the goal. The authors don't offer a compelling explanation for this, despite the fact that it would appear to undermine their argument that what makes their approach work is the inclusion of goal-relevant information in z. Additionally the condition that comprises the last two rows of table 1 is one where during inference the model is given part of the solution as a \"user specified\" z. It should not be surprising that this improves performance, and the authors should make that clear.\n\nSecond. The evaluation in section 4.4 relies on using GPT4 to evaluate how well a model generates a story according to a goal like \"I want the 4th sentence from here to be \"Hello little frog.\"\" They show that models trained with Trelawney are better at complying with these explicitly stated goals than models trained without explicit goal supervision. This appears to be done by injecting a \"goal\" subsequence z during generation for both the Trelawney model and the baseline (according to the example shown in the appendix). This goal injection is exactly the Trelawney training objective used to train the author's model and not the baseline. Therefore these results show if you train on an objective, you will do better on it during evaluation --- a finding that lacks novelty. By contrast the results in table 1 do not suffer from this - they show training on the Trelawney objective improves performance on a task objective. In section 4.4 the task objective appears to be almost indistinguishable from the Trelawney objective. I have similar concerns for the \"conditional generation\" results in section 4.5 which the authors say follows the same protocol used in 4.4. \n\n\nAs some minor points: the authors appear to conflate next token prediction and teacher forcing. They refer to baselines as \"NTP\" despite the fact that the objective with Trelawney is still next token prediction just with augmented data. I may have missed it but I don't think the authors introduce the different between the implicit and explicit condition before referring to it."}, "questions": {"value": "Why does the random condition out perform fixed in table 1?\n\nIn the Path Planning results why does Trelawney perform better on higher degrees than it does on longer paths?\n\nIs the \"spec.\" condition in table 1 just giving part of the solution to the model - if so what should we conclude from an increase in performance?\n\nDoes Trelawney pre-training improve instruction following on any other tasks that do not explicitly resemble the Trelawney objective?\n\nAt line 048 in the introduction you state your approach \"embed[s] inductive biases directly.\" What inductive biases does your approach embed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "waJnfBSqL6", "forum": "zXHiyA8OGq", "replyto": "zXHiyA8OGq", "signatures": ["ICLR.cc/2026/Conference/Submission21187/Reviewer_vA7g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21187/Reviewer_vA7g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758011702, "cdate": 1761758011702, "tmdate": 1762941594046, "mdate": 1762941594046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve the conventional next-token prediction paradigm in language model training, which restricts models to predict each token solely based on past context. It argues that humans generate language with awareness of future goals rather than relying only on previous information. To address this limitation, the authors propose TRELAWNEY, a simple data augmentation approach that inserts special tokens $\\langle T\\rangle \\ldots \\langle \\/T\\rangle$ containing future text segments representing goals or upcoming content into training sequences. Using this method, models are trained on a mixture of original and augmented data. Experiments on synthetic planning tasks, algorithmic reasoning, and story generation demonstrate that TRELAWNEY enhances performance on tasks requiring foresight without degrading standard language modeling ability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Please find the strengths below:\n1. The paper enhances the foresight capability of language models by introducing explicit future markers $\\langle T\\rangle \\ldots \\langle \\/T\\rangle$ instead of modifying the model architecture as in prior approaches.\n2. The experiments cover multiple dimensions, including synthetic planning tasks (star-graph), algorithmic reasoning tasks (strongly connected components), and natural language story generation, providing thorough and diverse validation as well as comparisons across different augmentation strategies.\n3. The paper is clearly written, with well-motivated problem statements and detailed, reproducible descriptions of the proposed method."}, "weaknesses": {"value": "Please find the weaknesses below:\n1. The experiments are diverse across different types of datasets but are conducted on relatively small models and mostly synthetic or simplified data.\n2. The insertion of future tokens $\\langle T\\rangle \\ldots \\langle /T\\rangle$ and the choice of where to place them rely on manually defined or heuristic rules. There is no theoretical guidance on how to modify the training data effectively.\n3. The connection between the proposed augmentation and established notions such as planning, goal inference, or information flow is discussed only at an intuitive level.\n4. For open-ended or creative text generation, it remains unclear how the insertion of future tokens can be meaningfully applied."}, "questions": {"value": "The questions are related to the weaknesses:\n1. Is it possible to provide any theoretical guidance or justification for the insertion of future tokens and the choice of where to place them?\n2. Is it possible to conduct a more rigorous analysis of the connection between the proposed augmentation and established notions such as planning, goal inference, or information flow?\n3. Since the experiments only involve small-scale fine-tuning on a single model, is it possible to extend the evaluation to more advanced and larger language models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aH1q7LFWLQ", "forum": "zXHiyA8OGq", "replyto": "zXHiyA8OGq", "signatures": ["ICLR.cc/2026/Conference/Submission21187/Reviewer_cJtC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21187/Reviewer_cJtC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789448906, "cdate": 1761789448906, "tmdate": 1762941593091, "mdate": 1762941593091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for data manipulation in training a LLM to include specific “lookahead” fragments. The idea is that by training the model to make some longer distance predictions, it can exhibit more reasoned planning behaviour in its next-token prediction to reach these goals. This is demonstrated to be helpful in various settings, from path planning to puzzles to short story generation. \n\nOverall the paper is interesting, the method simple but seemingly effective, and overall well evaluated (but see my many questions.) The approach may be overly simple, in comparison to other papers in this space attempting a more challenging and general problem of intra-token thinking."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The method is very simple, which adds to its appeal as it can most likely be adapted to other situations.  \n\nThe paper is very well written, and the ideas clearly motivated and presented in an easily accessible manner.\n\nThe evaluation is thorough, hitting a range of very different tasks. The diversity of tasks employed helps to demonstrate the generality of the method, and the results are uniformly strong."}, "weaknesses": {"value": "The method is very heuristic, through the manner of thinking supervision predicting the [nth] following sentence/token, rather than something more abstract or learnable from data. Models do learn to solve complex generation problems, and while some of these capabilities are explicitly trained, many emerge from next-token prediction pre-training or from reinforcement learning, e.g., of thinking models. \n\nRelated to the above, the paper neglects to cite “quiet-star” [1] which includes thinking between tokens during training, but fit using RL training. This might be learning a similar type of lookahead, but based on their paper, it looks to be much deeper reasoning and other thinking behaviours. Quiet-star is much more ambitious than this work, and I was left a bit underwhelmed by this paper as a consequence. However, I would be very interested to see whether a) empirically you can show an improvement over quiet-star on your evaluation sets, b) you can compare what’s learned in your method vs theirs, or c) you could cook something up to combine both methods, e.g., initialise quiet-star with your heuristic, and then learn more general thinking behaviour.\n\nSome experimental aspects of the paper were unclear, see questions below. The pre-training section was particularly unclear, and I’m not sure how much this adds to the paper. \n\n[1] Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking; Eric Zelikman et al, 2024"}, "questions": {"value": "1. See weakness above, I’d appreciate a detailed comparison to quiet-star.\n1 .What are Zebra puzzles? What do the colours mean in Fig 1? Note that this figure is truncated on the right, the whole story is not visible.\n1. How does supervising special “futures” based on copying future tokens resolve exposure bias / teacher forcing? This part of the narrative is not made explicit.\n1. The method for adding a long sentence (line 156) between tokens looks very expensive computationally, likely requires an instruction tuned model to benefit, and it blows out the sequence length. Can you discuss these and other limitations?\n1. Masking <T> generation – if this were not masked, then the model would be free to include this style of thinking during generation. Can you comment on whether this might be useful, and how this might compare to dynamic thinking (or perhaps the runtime consequences would be serious?)\n1. Path Planning (Fig 3) – are the graphs provided in the dataset, and the start/goal nodes randomly selected?\n1. When generating from the model, what happens if no <T> token is supplied, i.e., the generation does not include any thinking at inference time? As I understand it, all evals were run with thinking on.\n1. For the story generation problem, can commercial LLMs generate stories that honor the <T> constraint, e.g., 8th sentence is X as per Figure 4? I wonder if this is within the capabilities of current models.\n1. 413 “stories are shuffled” – I don’t follow why this is done, and is this at word or sentence level? Either way, it appears to corrupt the dataset to the extent that it may be useless.\n1. 453 did the evaluation of pre-trained models use <T> decoding? I don’t know how to interpret the numbers in Table 2, can you clarify if they are perplexity results, and if do, does this show your method is considerably worse than Draw?\n\n# Comments on prose:\n1. 086 “exposure bias” sentence should reference on- vs off- policy learning.\n1. 096 you may want to consider diffusion models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cnqUaoDEjn", "forum": "zXHiyA8OGq", "replyto": "zXHiyA8OGq", "signatures": ["ICLR.cc/2026/Conference/Submission21187/Reviewer_5D2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21187/Reviewer_5D2n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21187/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861597057, "cdate": 1761861597057, "tmdate": 1762941592302, "mdate": 1762941592302, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}