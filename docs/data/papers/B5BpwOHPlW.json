{"id": "B5BpwOHPlW", "number": 19675, "cdate": 1758298247832, "mdate": 1763046023791, "content": {"title": "Decoder Only Transformer for Physics Informed Neural Networks", "abstract": "Physics-Informed Neural Networks (PINNs) approximate PDE solutions by embedding physical constraints into training, yet MLP-based backbones often suffer from instability and loss of fidelity on long horizons. Recent sequence models (e.g., Transformers) alleviate some of these issues, but their encoder–decoder design adds parameters and memory pressure with limited benefit for autoregressive pseudo-sequences.\nWe introduce \\textbf{DoPformer}, a \\emph{decoder-only} Transformer tailored to physics-informed learning. DoPformer consumes short spatio–temporal pseudo-sequences, uses multi-head self-attention with WaveAct activations, and applies a sequential physics loss across the window. Removing the encoder and cross-attention yields a lighter model while preserving long-range temporal coupling through self-attention.\nTo further boost spectral accuracy, we explore two optional modules: (i) a Fourier \\emph{neural-operator} branch (\\textit{DoPformer+NO}) that improves oscillatory regimes and long-horizon rollouts; and (ii) a compact \\emph{KAN}-based feed-forward replacement (\\textit{DoPformer+KAN}) that drastically reduces parameters while maintaining strong accuracy.\nAcross convection, reaction, wave, and 2D Navier–Stokes equations, DoPformer consistently improves PINN accuracy and stability; the NO and KAN variants deliver additional gains depending on stiffness and spectral content. Our numerical results show that on these benchmarks DoPformer attains state-of-the-art accuracy among physics-informed models while using substantially fewer parameters.", "tldr": "", "keywords": ["Physics Informed Neural Network", "PINN", "Neural Operators"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/378167ec8283d0e6a6688ce474c642f46e313c89.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper propose DoPformer, a decoder-only Transformer for Physics-Informed Neural Networks that processes short spatio-temporal pseudo-sequences with self-attention and a sequential physics loss, eliminating encoder/cross-attention to cut parameters while preserving long-range coupling. It can be optionally augmented with a Fourier neural-operator branch to recover high-frequency content and a compact KAN-based feed-forward module to boost token-wise expressivity with only a few thousand parameters. Across convection, reaction, wave, and 2D Navier–Stokes benchmarks, DoPformer variants match or surpass prior sequence PINNs while using substantially fewer parameters."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. On multiple PDEs, DoPformer variants match or surpass strong baselines while being markedly smaller.\n\n2. The decoder-only design removes redundant encoder/cross-attention, cutting parameters/FLOPs and activation memory—clean motivation tied to PINN data geometry."}, "weaknesses": {"value": "1. [**Key Weakness**] It is a known conclusion that sequence-based PINNs do not require an encoder-decoder architecture. The author can refer to the open-source code of RoPINN [1]. Also the proposed decoder-only architecture has no difference compared to PINMMamba's macro-architecture [2], the only difference is the backbone models used (MHSA vs SSM). \n\n\n2. The paper introduces a \"base\" DoPformer, but the strongest results, particularly on the 2D Navier-Stokes benchmark, are achieved by the DoPformer+NO+KAN variant. This \"kitchen-sink\" approach obscures the core contribution. It is impossible to determine if the performance gains come from: (a) The decoder-only architecture (the stated novelty) or (b)the strong spectral bias of the Fourier Neural Operator (+NO) or (c) the expressive, learnable nonlinearities of the KAN module (+KAN). The paper fails to disentangle these effects, leaving the central claim—that decoder-only is the key—unproven.\n\n3. The model's performance is likely critically dependent on the pseudo-sequence hyperparameters: window length k and time-stride Δt. The paper merely states these are \"fixed\" and \"aligned with prior work\". This is insufficient. A sensitivity analysis is required. How does the model perform if Δt is mismatched with the characteristic timescale of the PDE? Or if k is too short to capture relevant dynamics? The authors acknowledge this as a limitation, but it is a central weakness of the current study.\n\n4. The choices of NO and KAN are not well motivated, while no evidence shows the necessity of these components. The author should give a clear receipt of training. Lack ablation study, key NO/KAN choices are fixed (e.g., D=32, heads=2, k=7; KAN B-spline grid settings) and “same recipe” is reused; there isn’t a sweep showing robustness to these knobs. Also the sensitivity to layer's of block is all not tested.\n\n\n\n\n[1] Wu H, Luo H, Ma Y, et al. Ropinn: Region optimized physics-informed neural networks. NeurIPS 2024.\n\n[2] Xu C, Liu D, Hu Y, et al. Sub-sequential physics-informed learning with state space model. ICML 2025."}, "questions": {"value": "1. How about just replacing the Window Mixer (MHSA) with an MLP since the PINN doesn't have a distinct source/target stream that would necessitate cross-attention?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "k1Ag1evCIC", "forum": "B5BpwOHPlW", "replyto": "B5BpwOHPlW", "signatures": ["ICLR.cc/2026/Conference/Submission19675/Reviewer_ZryJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19675/Reviewer_ZryJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761785840002, "cdate": 1761785840002, "tmdate": 1762931520861, "mdate": 1762931520861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "qw9wyM0K6l", "forum": "B5BpwOHPlW", "replyto": "B5BpwOHPlW", "signatures": ["ICLR.cc/2026/Conference/Submission19675/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19675/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763046022768, "cdate": 1763046022768, "tmdate": 1763046022768, "mdate": 1763046022768, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DoPformer, a decoder-only Transformer for physics-informed neural networks. Unlike prior encoder–decoder or MLP-based PINNs, DoPformer uses only self-attention over short spatio-temporal pseudo-sequences, reducing redundancy and computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The decoder-only structure removes unnecessary encoder–decoder components while maintaining temporal coupling. Especially, Fourier and KAN modules address different PDE regimes effectively.\n2. The proposed model achieves strong accuracy with drastically fewer parameters. Evaluations cover multiple PDE types with fair comparisons to strong baselines."}, "weaknesses": {"value": "1. No deep explanation of why the decoder-only design improves stability.\n2. Few visualizations of dynamic behaviors or spectral recovery.\n3. While ablations cover variants (+NO, +KAN), more analysis on how window size or attention heads affect accuracy would be informative.\n3. Although the model is lighter in parameters, training with L-BFGS and auto-differentiation for PDE residuals may still be expensive; runtime comparisons are missing.\n4. It remains unclear how well DoPformer scales to 3D, multi-physics, or data-driven hybrid scenarios."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WP5UVKb3Tw", "forum": "B5BpwOHPlW", "replyto": "B5BpwOHPlW", "signatures": ["ICLR.cc/2026/Conference/Submission19675/Reviewer_duFp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19675/Reviewer_duFp"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986519111, "cdate": 1761986519111, "tmdate": 1762931520454, "mdate": 1762931520454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a decoder-only Transformer framework, named DoPformer, for physics-informed neural networks (PINNs). The key idea is to simplify the PINNsFormer encoder–decoder design by using a lightweight decoder-only structure that retains temporal coupling through self-attention, while reducing the number of parameters. To enhance spectral fidelity and efficiency, the model integrates two optional modules: a Fourier neural-operator branch (DoPformer+NO) and a feed-forward block based on the Kolmogorov–Arnold network (KAN) (DoPformer+KAN). The paper evaluates these variants on canonical PDEs and reports that the models achieve competitive or better accuracy with fewer trainable parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The model achieves accuracy with a smaller number of parameters, showing potential for light-weight physics-informed architectures.\n\n2. The decoder-only design simplifies the architecture while maintaining competitive performance, making it practical for resource-limited PDE simulation."}, "weaknesses": {"value": "1. The integration of a Fourier neural operator within a physics-informed framework is conceptually confusing. Neural operators typically rely on supervised input–output mappings, while PINNs operate in a semi-supervised or unsupervised setting. This ambiguity requires clarification, particularly regarding how neural operators operate without paired data.\n\n2. The work does not address scalability issues. Memory consumption and out-of-memory (OOM) behavior common in PINNsFormer and PINNMamba are not discussed or benchmarked, which is crucial for higher-dimensional or more complex PDEs. Please see the last table in the appendix of PINNsMamba paper [1] for a detailed list of problems where PINNsFormer and PINNMamba face the issue of OOM. \n\n3. The range of problems tested remains limited. Highly oscillatory or strongly nonlinear PDEs are missing, making it unclear whether the Fourier or KAN augmentations are truly beneficial beyond canonical cases.\n\n4. Important architectural and implementation details are underspecified. For instance, initialization schemes for KAN, specific basis functions, and exact parameterization strategies are not described, making reproducibility challenging.\n\n5. The presentation could be improved. Some tables show abrupt jumps in errors (e.g., rMAE and rRMSE for the Navier–Stokes case with DoPformer+KAN) without explanation. Moreover, the discussion of why parameter reduction is so drastic compared to MLPs is insufficient.\n\n[1] Xu, Chenhui, et al. \"Sub-Sequential Physics-Informed Learning with State Space Model.\" Forty-second International Conference on Machine Learning."}, "questions": {"value": "1. How is the Fourier operator integrated within the physics-informed loss? If the neural operator branch relies on paired mappings, how does it remain consistent with the PINN formulation?\n\n2. What happens if the encoder block is retained (i.e., a hybrid of PINNsFormer with KAN)? Would it improve accuracy by being parameter-efficient?\n\n3. It is interesting to explore a Chebyshev-based KAN formulation [2] and report its comparative performance.\n\n4. How would the proposed model handle complex or irregular geometries, where token formation and windowing become nontrivial?\n\n5. Please provide computational time comparisons to demonstrate the claimed efficiency.\n\n6. How is initialization handled for the KAN variant, and are there recommended basis configurations or hyperparameters that influence performance?\n\n7. Please explain why the Navier–Stokes case for DoPformer+KAN shows a sharp rise in rMAE and rRMSE? Is this a typo or due to instability?\n\n8. The paper mentions a drastic reduction in parameters when replacing MLPs with KANs. Please elaborate on whether the baseline MLP has already overfitted the problem and how fair the comparison is across architectures?\n\n[2] Shukla, Khemraj, et al. \"A comprehensive and FAIR comparison between MLP and KAN representations for differential equations and operator networks.\" Computer Methods in Applied Mechanics and Engineering 431 (2024): 117290."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dPrBOMJ9Zx", "forum": "B5BpwOHPlW", "replyto": "B5BpwOHPlW", "signatures": ["ICLR.cc/2026/Conference/Submission19675/Reviewer_frNH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19675/Reviewer_frNH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19675/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992733748, "cdate": 1761992733748, "tmdate": 1762931519916, "mdate": 1762931519916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}