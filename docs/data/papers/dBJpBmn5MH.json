{"id": "dBJpBmn5MH", "number": 23623, "cdate": 1758346448779, "mdate": 1759896804310, "content": {"title": "Building Neural Networks that are Robust to Adversarial Examples Using Probabilistic Loss Function", "abstract": "Adversarial examples are an Achilles heel of deep neural networks, robbing them of their functional performance in mission-critical applications. This work proposes a novel method of making deep neural networks robust to adversarial training by modifying the loss function to a soft version of cross-entropy loss for classification problems. For regression problems, the data distribution is examined using Bayesian techniques (Gaussian Mixture Model) and the loss function is modified using a posterior probability distribution. The approach is justified using mathematical derivation and is supplemented by applying it on MNIST and Imagenet classification problems to demonstrate its robustness to FGSM and Carlinii-Wagner L2 attacks. This approach alleviates the overhead of training an additional model inherent in adversarial-distillation based methods.", "tldr": "Building Neural Networks that are Robust to Adversarial Examples Using Probabilistic Loss Function for classification and regression", "keywords": ["Adversarial Training", "Deep Neural Networks", "FGSM", "Defensive Distillation"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7110194046da62c4d98d93db7f3b953029567b67.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a method for defense against adversarial attacks based on probabilistic loss functions. They evaluate their method against simple gradient-based attack like FGSM and show improvements on MNIST, but they fail to evaluate against stronger iterative attack methods like PGD."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is clearly written and the main ideas are clearly presented."}, "weaknesses": {"value": "- The evaluation used in this paper is not up to the current standards in evaluating adversarial robustness. The authors do not use PGD-based attacks nor do they compare against state-of-art defense methods (or even just adversarial training). \n\n- The authors don't seem to be familiar with the important works in adversarial defense and fail to cite or compare against them (e.g. adversarial training [Madry et al 2017] or the TRADES algorithm [Zhang et al 2019]. \n\nMadry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017).\n\nZhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" International conference on machine learning. PMLR, 2019.\n\n- For a more thorough evaluation, the authors should also go beyond MNIST and include more datasets."}, "questions": {"value": "- I would suggest the authors review the literature suggested and also try their method against more modern attack packages such as AutoAttack[Croce & Hein 2020]. \n\nCroce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" International conference on machine learning. PMLR, 2020."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Q8SQsBTh6w", "forum": "dBJpBmn5MH", "replyto": "dBJpBmn5MH", "signatures": ["ICLR.cc/2026/Conference/Submission23623/Reviewer_3B48"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23623/Reviewer_3B48"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761749167009, "cdate": 1761749167009, "tmdate": 1762942737116, "mdate": 1762942737116, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to improve adversarial robustness by replacing one-hot training targets with probabilistic targets. For classification, it introduces a loss that subtracts a weighted sum of non-target cross-entropy terms and a Bayesian variant that estimates soft class posteriors via a Gaussian mixture model (GMM) fitted using the EM algorithm, then trains against those posteriors. For regression, it proposes distance-weighted squared losses using either global or cluster-conditioned statistics, as well as a GMM-weighted variant. Experiments briefly mention MNIST and ImageNet under FGSM and CW attacks, providing qualitative examples but no quantitative robustness tables, baselines, or strong attacks. Overall, the approach resembles label smoothing, soft-label training, and density-based reweighting, but the empirical support and novelty positioning are insufficient."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper presents a simple and unified framework that covers both classification and regression tasks, which could make it straightforward and efficient to implement. Its motivation is intuitive: by softening the training targets, the method may regularize decision boundaries, reduce overfitting, and potentially improve robustness. Moreover, the attempt to introduce a Bayesian alternative through GMM-based soft labels is interesting, as it could, if more rigorously developed, provide a useful connection to uncertainty calibration."}, "weaknesses": {"value": "1. The paper lacks a standard robustness evaluation. No PGD-$k\\ (k \\ge 10)[1]$, AutoAttack$[2]$, Square$[3]$, Boundary$[4]$, or transfer attacks are reported; only FGSM and CW are mentioned. Robust accuracy versus $\\epsilon$ is not provided for MNIST or ImageNet.\n\n2. There are no quantitative results or baseline comparisons. The paper omits tables benchmarking against widely used baselines such as adversarial training (PGD-AT)$[1]$, TRADES$[5]$, MART$[6]$, label smoothing, distillation-based defenses, or confidence-calibrated losses: all essential to substantiate the claimed improvements.\n\n3. Several methodological and notation issues are present. Equation numbering is missing on line $99$; the EM updates use inconsistent indices; and the notation for the number of clusters is inconsistent ($C$ on line $184$ versus $N$ in Algorithm $2$).\n\n4. The novelty positioning is inadequate. The relationship to label smoothing or soft-target methods, robust regression (Huber/Tukey), heteroscedastic modeling, and uncertainty-aware training is not discussed, and there are no ablations that disentangle the effect of soft labels from that of the proposed GMM procedure. Moreover, the paper offers almost no discussion of related work.\n\n5. The experimental protocol is insufficient. In Section $3$, the authors write that “a summary of the model architecture is available upon request,” but such a summary should have been included in the appendix or supplementary material. The MNIST experiment uses only five training epochs and lacks any clean-versus-robust trade-off analysis. Statements such as “the model incorrectly classifies an image of digit $7$ as $2$” are not an appropriate way to present results in a scientific paper. Full quantitative results on the MNIST test set should be provided. It is insufficient to assert that “we obtain similar results for the Carlini-Wagner attack and on the ImageNet dataset” without presenting those results.\n\n$[1]$ Madry, Aleksander, et al. \"Towards Deep Learning Models Resistant to Adversarial Attacks.\" International Conference on Learning Representations. 2018.\n\n$[2]$ Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" International conference on machine learning. PMLR, 2020.\n\n$[3]$ Andriushchenko, Maksym, et al. \"Square attack: a query-efficient black-box adversarial attack via random search.\" European conference on computer vision. Cham: Springer International Publishing, 2020.\n\n$[4]$ Brendel, Wieland, Jonas Rauber, and Matthias Bethge. \"Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.\" International Conference on Learning Representations. 2018.\n\n$[5]$ Zhang, Hongyang, et al. \"Theoretically principled trade-off between robustness and accuracy.\" International conference on machine learning. PMLR, 2019.\n\n$[6]$ Wang, Yisen, et al. \"Improving adversarial robustness requires revisiting misclassified examples.\" International conference on learning representations. 2019."}, "questions": {"value": "1. Please report robust accuracy versus $\\epsilon$ on MNIST and CIFAR-$10/100$ using PGD-$20/100$ and AutoAttack, including mean $\\pm$ std over at least three random seeds, along with clean accuracy. How does your method compare to PGD-AT, TRADES, MART, and label smoothing (with and without adversarial training)?\n\n2. What is the effect of $\\beta$ in Equation $6$ and of prior initialization (e.g., $0.9$ vs. other values) in Algorithm $1$? Please provide ablation and sensitivity analyses for these parameters.\n\n3. Could you correct equation references and index notation, and fully specify the EM update procedure (dataset used, stopping criteria, normalization)?\n\n4. You claim to obtain “similar results” on ImageNet. Please provide the corresponding quantitative results: robust and clean accuracy, model architecture details, and attack configurations (norm, $\\epsilon$, and number of steps)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NAdQv8k0nG", "forum": "dBJpBmn5MH", "replyto": "dBJpBmn5MH", "signatures": ["ICLR.cc/2026/Conference/Submission23623/Reviewer_H6Pk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23623/Reviewer_H6Pk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842315654, "cdate": 1761842315654, "tmdate": 1762942736880, "mdate": 1762942736880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is an unfinished manuscript."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "NA"}, "weaknesses": {"value": "NA"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["Yes, Unprofessional behaviors (e.g., unprofessional exchange between authors and reviewers)"]}, "details_of_ethics_concerns": {"value": "It's not professional to submit unfinished manuscript to ICLR."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2SsVO9kYcB", "forum": "dBJpBmn5MH", "replyto": "dBJpBmn5MH", "signatures": ["ICLR.cc/2026/Conference/Submission23623/Reviewer_riQB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23623/Reviewer_riQB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948498648, "cdate": 1761948498648, "tmdate": 1762942736663, "mdate": 1762942736663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes modifying the loss function to improve the adversarial robustness of models against adversarial examples. The experiment only focuses on MNIST with FGSM on a custom model."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "n/a"}, "weaknesses": {"value": "The paper is difficult to follow and weakly presented throughout. Key experimental details and results are missing. The empirical evaluation is limited to MNIST with a non-standard model, and although the authors claim results on ImageNet and under CW attacks, these are neither described nor reported."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3JMykVkJaa", "forum": "dBJpBmn5MH", "replyto": "dBJpBmn5MH", "signatures": ["ICLR.cc/2026/Conference/Submission23623/Reviewer_2Q9n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23623/Reviewer_2Q9n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762267221369, "cdate": 1762267221369, "tmdate": 1762942736490, "mdate": 1762942736490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}