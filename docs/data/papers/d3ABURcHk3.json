{"id": "d3ABURcHk3", "number": 15669, "cdate": 1758253704948, "mdate": 1759897290014, "content": {"title": "From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization", "abstract": "The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.", "tldr": "We propose the Adaptive Local Preference Optimization (ALPO) method to improve expressiveness and vividness of subtitle translation.", "keywords": ["Large Language Model", "Subtitle Translation", "Preference Optimization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9270e371f319097ab6eed586eddc29e026adb292.pdf", "supplementary_material": "/attachment/41087eb52b3b161043a5cdd84ec32dd07e3d0454.zip"}, "replies": [{"content": {"summary": {"value": "The paper describes in detail a method to adapt LLMs for subtitle translation. The paper includes a number of preliminary studies that motivate the specific challenges of this task (such as that vividity and naturalness are more important than literalness), and the use of LLM as a judge to assess vividity. It then puts all these insights into practice into a well developed multi-step process, including a new preference optimization method. This is a very well written paper."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Very well written and motivated work, well executed solution.\n\nInsightful analysis into what makes subtitle translation different from more factual translation challenges such as in the news/legal domain."}, "weaknesses": {"value": "The correlation assessment would benefit from a second human assessment to get a sense of inter-annotator agreement, and how that relates to the correlation numbers you report.\n\nThe method does not deal with other constraints if subtitle translation, such as length limits"}, "questions": {"value": "You point out the contrast between chat and reasoning LLMs. But are you actually using reasoning LLMs to \"think\"? The prompt in the appendix asks just for a straight translation.\n\nIn Figure 4, you start with two differerent datasets - but are they actually different?\n\nThe reinforcement learning approach only tries to optimize vividness, or do I misunderstand something here?\n\nThere seem to be three variations in your preference optimization approach: (1) filtering out of small differences (Eqn2), importance scoring based on size of the set (Eqn3) and a dynamic beta. Which of these things make the method \"adaptive\" - I could not quite connect the name with the approach.\n\nDo I read Table 4 right that your method wins against the human translation on all language pairs / metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ZIHrRW2su2", "forum": "d3ABURcHk3", "replyto": "d3ABURcHk3", "signatures": ["ICLR.cc/2026/Conference/Submission15669/Reviewer_i1xn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15669/Reviewer_i1xn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950319214, "cdate": 1761950319214, "tmdate": 1762925924759, "mdate": 1762925924759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets subtitle translation, arguing that good subtitles often require “liberal” renderings rather than literal word-for-word output. The authors propose a segment-level preference-optimization setup (ALPO): for each subtitle line, they sample multiple candidates, use an LLM-as-judge to pick local winners/losers, and train the model with a contrastive DPO-style loss; a few engineering choices (segment gating, prefix mixing, adaptive weighting) aim to stabilize learning and encourage fluency. Evaluation emphasizes three dimensions—accuracy, naturalness, and vividness—with both LLM-judged metrics and a small human study. Across several language directions, the ALPO-trained model improves over its SFT base and is competitive with strong chat/reasoning LLMs, particularly on lower-resource directions. Ablations suggest the local preference signal and the auxiliary knobs contribute to the gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles a real-world application scenario—subtitle translation often benefits from more liberal renderings—and sets up a clean evaluation around that goal. The method is straightforward and well engineered: segment-level preference optimization with an LLM-as-judge, some human validation, and clear axes (accuracy/naturalness/vividness). The ablations are useful and suggest the components (e.g., segment gating, prefix mixing) actually matter. Empirically, the ALPO-trained model shows consistent gains over its SFT baseline and is competitive with strong chat/reasoning baselines, with gains on lower-resource directions."}, "weaknesses": {"value": "Novelty and Missing references and baselines:  \n\nThe work feels largely application-driven, and the methodological novelty is unclear beyond packaging known components (segment-level preference optimization with an LLM-as-judge) for subtitles. In particular, there’s no head-to-head comparison against fine-grained preference-learning baselines that operate below sequence level, and skips fine-grained baselines that are directly comparable to a segment-level method. \n\nAt minimum, it should include some of the following methods: \n\nFor fine-grained DPO:\n\n(i) “Token-level Direct Preference Optimization” — Yongcheng Zeng et al., arXiv, 2024\n\n(ii) “TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization” — Aiwei Liu et al., ICLR, 2025 \n\n(iii) “SDPO: Segment-Level Direct Preference Optimization for Social Agents” — Aobo Kong et al., ACL (Long), 2025. \n\nFor MT specifically, a fine-grained SFT baseline like: \n\n“Learning from Others’ Mistakes: Finetuning Machine Translation Models with Span-Level Error Annotations” — Lily H. Zhang et al., ICML, 2025,\n\nshould also be in scope. Adding these would test whether the gains come from segment-/token-level supervision itself rather than the particular packaging here."}, "questions": {"value": "Fine-grained baselines. Why no head-to-head with fine-grained DPO (e.g., token- or segment-level) and span-level MT training (TWA)? Please add at least one strong token-level DPO baseline and a TWA-style span-supervised SFT baseline on the same data.\n\nCredit assignment. How exactly are segment preferences aggregated across a subtitle to affect the full translation? Any cases where segment-level wins yield worse whole-sequence coherence? An ablation on seq- vs seg-only vs mixed would help.\n\nJudge calibration. How sensitive are results to the choice/prompting of the LLM-as-judge? Please report inter-judge agreement, bias by style (literal vs liberal), and a calibration curve vs. human wins. Also, was any method used to overcome the LLM-as-judge biases (e.g. for label bias, ordering bias, etc.) or would that make a difference in the results.\n\nStatistical rigor. For main tables, can you include confidence intervals and a simple permutation/bootstrap test for deltas? Also report the number of items per direction to assess power.\n\nGating and prefix-mixing. These look important. Could you share the exact gating rule, thresholds, and failure cases? What happens if you remove prefix mixing or replace it with random context shuffling?\n\nCost/efficiency. What is the end-to-end training and inference cost vs. your strongest baseline, and the quality-per-dollar or per-second trade-off?\n\nError analysis. Please include a brief breakdown of where ALPO wins/loses (e.g., idioms, wordplay, cultural references, timing constraints). This would make the “liberal translation” claim more concrete."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7IBQR2OLaU", "forum": "d3ABURcHk3", "replyto": "d3ABURcHk3", "signatures": ["ICLR.cc/2026/Conference/Submission15669/Reviewer_w6RR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15669/Reviewer_w6RR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951039406, "cdate": 1761951039406, "tmdate": 1762925924132, "mdate": 1762925924132, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscirpt focus on the problem of subtitle translation using LLM. Subtitle translation refers to the machine translation task in the scenarios such as visual media and movie (visual information is not used), where the emotion and atomosphere should be considered. The authors first investigate the subtitle translation by examine exisiting LLMs with comparing the literal and liberal translation in different domain. Then, the authors proposed ALPO, a preference optimization framework that considers the contexts and segmentation specifically needed for subtitle translation. The effectiveness of the method is evaluated by LLM-as-a-judge and human measurement on unclear number of examples."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The manuscript is well-written and I could follow the logic and understand the background.\n2. Comprehensive benchmarks and exisiting LLMs are evaluated in the experiments.\n3. The problem of subtitle translation (maybe in a more general sense, machine translation in diverse tones and scenarios), is an important and emergent problem as SOTA LLMs has now reached a very strong performance on literal translation."}, "weaknesses": {"value": "1. Although the topic is interesting and problem is important, the evaluation method is a bit weak and lack of soundness. See my questions for specific points below.\n2. The technical challenges of subtile translation are context-dependency and segmentation, etc., which are not discussed in depth. Intead, the point of liberal translation that the authors highlighted is vague in this manuscirpt: for example, there is a gap between lower BLEU and hihgher liberal translation, this could be simply caused by mis-translation from the models. More justification would be helpful in Section 3.2.\n3. In Section 5, the experiments use LLM-as-a-judge to evaluate the models assuming that LLM has already understand the \"emotion\" and \"atomospeher\", then why those LLM still need optimization for those two aspects? Although the BLEU correlation is shown in Section 3 but it's not directly related to the scope in vividness evaluation. \n\n\nMinor comments:\n1. In Figure3 caption: DS-R1 is the abbr. for Deepseek-R1 can be stated, though most people could guess the meaning. In addtion, chat-models and reasoning-models can be somehow visually diffrentiated, as you're comparing the patterns for those two groups.\n2. A previous work about word-level preference reward for machine translation [1], could be related this work as subtitle translation needs sentence-level context.\n\n[1] Word Alignment as Preference for Machine Translation.EMNLP 2024."}, "questions": {"value": "1. Can I understand ALPO as a context-aware sampling (step 2) + DPO (step 3)?\n2.This is related to limitation 3, is it possible to add an ablation experiments that expilictly asking reasoning LLM to consider vividness, emotion and atomospher withouth ALPO optimization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gRzK1GT6WO", "forum": "d3ABURcHk3", "replyto": "d3ABURcHk3", "signatures": ["ICLR.cc/2026/Conference/Submission15669/Reviewer_VFQE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15669/Reviewer_VFQE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997803216, "cdate": 1761997803216, "tmdate": 1762925923646, "mdate": 1762925923646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper looks at translating subtitles for visual media. They introduce a novel Preference Optimization method (ALPO) that looks at local outputs instead of the full ones. Overall, the paper covers a lot of things in depth, but I have two main questions (in question section below) that I would like to see answered that will really impact my review scores."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper has a lot of positive contributions. The analysis of domains on literal vs liberal translations is quite nice. The numerous experiments on recent models encompassing a lot of modern SOTA LLMs. A potentially interesting new PO algorithm. Empirically showing Table 3 that we cannot always beat humans (which makes sense since they likely use more modalities to translate as well). A human evaluation in section 5.3"}, "weaknesses": {"value": "The proposed method is a novel PO algorithm, but the comparisons are only to other models, not other PO methods. The authors claim that local is better, and thus propose ALPO as opposed to using full sequence outputs such as DPO. However, this is never empirically shown (as far as I can tell).\n\nDifferentiating the subtitle translation task from OpenSubtitles (Tiedemann 2016; Lison and Tiedemann 2016). It is mentioned as a core claim, but is only mentioned in section B.2.2"}, "questions": {"value": "Normally, PO tasks are done on full sequence outputs which is often touted as a benefit. I guess you could have preferences for partial outputs and that is not a problem, but it seems that in many ways, that is more similar to SFT with a slightly different loss - since you just update at every token autoregressively. Am I understanding this correctly? Regardless, this is a semantic definition that really does not impact the paper too much. My main question is how would a sequence level method (DPO, CPO, KTO, SimPO, etc. - choose one) compare to your method? Essentially, I am asking for another row in Table 3 with one of these.\n\nYour first claim at the end of your introduction states \"We introduce the visual media subtitle translation task for the first time...\" It isn't clear to me that this is the first time subtitle translation is done. You even cite OpenSubtitles in B.2.2. How is your claim here different that OpenSubtitles? Is this just a new dataset? If so, that is fine, but the claim should be about that - and not that this is a novel task. Or am I missing something key to your task that is new?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l8hsweEwtc", "forum": "d3ABURcHk3", "replyto": "d3ABURcHk3", "signatures": ["ICLR.cc/2026/Conference/Submission15669/Reviewer_Wkwm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15669/Reviewer_Wkwm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15669/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762318494699, "cdate": 1762318494699, "tmdate": 1762925923148, "mdate": 1762925923148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}