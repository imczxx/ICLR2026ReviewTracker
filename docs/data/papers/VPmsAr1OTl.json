{"id": "VPmsAr1OTl", "number": 6215, "cdate": 1757958782064, "mdate": 1759897929437, "content": {"title": "Textual Bayes: Quantifying Uncertainty in LLM-Based Systems", "abstract": "Although large language models (LLMs) are becoming increasingly capable of solving challenging real-world tasks, accurately quantifying their uncertainty remains a critical open problem—one that limits their applicability in high-stakes domains. This challenge is further compounded by the closed-source, black-box nature of many state-of-the-art LLMs. Moreover, LLM-based systems can be highly sensitive to the prompts that bind them together, which often require significant manual tuning (i.e., prompt engineering). In this work, we address these challenges by viewing LLM-based systems through a Bayesian lens. We interpret prompts as textual parameters in a statistical model, allowing us to use a small training dataset to perform Bayesian inference over these prompts. This novel perspective enables principled uncertainty quantification over both the model’s textual parameters and its downstream predictions, while also incorporating prior beliefs about these parameters expressed in free-form text. To perform Bayesian inference— a difficult problem even for well-studied data modalities—we introduce Metropolis-Hastings through LLM Proposals (MHLP), a novel Markov chain Monte Carlo (MCMC) algorithm that combines prompt optimization techniques with standard MCMC methods. MHLP is a turnkey modification to existing LLM pipelines, including those that rely exclusively on closed-source models. Empirically, we demonstrate that our method yields improvements in both predictive accuracy and uncertainty quantification (UQ) on a range of LLM benchmarks and UQ tasks. More broadly, our work demonstrates a viable path for incorporating methods from the rich Bayesian literature into the era of LLMs, paving the way for more reliable and calibrated LLM-based systems.", "tldr": "We quantify LLM uncertainty by replacing prompt engineering with Bayesian inference using a novel MCMC method over text", "keywords": ["LLMs", "large language models", "uncertainty quantification", "Bayes", "Bayesian", "Bayesian inference", "Metropolis-Hastings", "prompt engineering", "autoprompting", "TextGrad"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ac9fb5473ae89c393bd9b75abe0c645155f38560.pdf", "supplementary_material": "/attachment/17d591f646412669a30796871132a10912f79dfa.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes to see the prompts as \"textual variables\" for LLMs.\\\nThey approximate the posterior distribution of the textual variable with MH technique.\\\nIn MH technique, prior is defined with human constraints, and proposal distribution is defined as Update, which is TextGrad in this paper.\\\nAfter this process, we can sample multiple textual variables (i.e., prompts) from the posterior distribution.\\\nLastly, by using these multiple prompts, we can approximate the posterior predictive distribution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-motivated.\n- The large language models indeed show some vulnerability depending on the prompt, and prompt engineering is widely adopted.\n\n2. The proposed method to approximate the posterior distribution is sound and solid.\n- MH algorithm is a widely adopted technique to sample from an arbitrary distribution.\n- The textual prior defined in Eq.7 is understandable.\n- The proposal distribution defined with TextGrad is very interesting and makes sense for me."}, "weaknesses": {"value": "1. Bayesian lens\n- The title of this paper is Title: \"TEXTUAL BAYES: QUANTIFYING UNCERTAINTY IN LLM-BASED SYSTEMS\".\\\nHowever, in my opinion, the paper only deals with \"prompt uncertainty\"\n- A true Bayesian lens would be the posterior over \"model parameter\", rather than over textual variables.\\\nFor example, if a user inputs the question with his own prompt, then this method cannot quantify the uncertainty.\\\nIn this sense, I think the prompt is more of an input rather than a parameter.\n\n\n2. \"To the best of our knowledge, we are the first both to quantify the uncertainty associated with prompts in LLM-based systems.\" could be an over-claiming.\n- [A] and [B] also quantify the uncertainty associated with prompts in LLM-based systems.\\\nIndeed, [B] defines \"prompt uncertainty\" in their paper, and tries to quantify it.\n- [A] Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis, EMNLP'22\n- [B] Uncertainty Quantification and Decomposition for LLM-based Recommendation, WWW'25\n\n\n3. Method section\n- It ends with samples ($\\theta$). What's next?\n- It would be better to link how those samples can be utilized for the posterior predictive distribution in Eq.6.\n\n\n4. Experiment\n-  A direct evaluation of \"uncertainty\" of the predictive distribution (Eq.6) would be needed.\\\nWe can compute the entropy of Eq.6 and adopt metrics like Kendall's tau to evaluate how well the uncertainty is related to the performance.\n- I think the calibration is more like a post-processing process."}, "questions": {"value": "1. Is there any warm-up steps for prompt sampling?\\\nIn MH, by TextGrad, the prompt is gradually adapted to the true distribution.\n\n2. $\\\\mathcal{D}$ is a single sample or an entire dataset?\\\nI think it should be an entire dataset.\\\nCan you elaborate more on how TextGrad updates a single prompt for the entire dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4BvOyiFRVK", "forum": "VPmsAr1OTl", "replyto": "VPmsAr1OTl", "signatures": ["ICLR.cc/2026/Conference/Submission6215/Reviewer_j29K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6215/Reviewer_j29K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761444871690, "cdate": 1761444871690, "tmdate": 1762918552779, "mdate": 1762918552779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method called Textual Bayes, which treats a set of prompts as textual parameters $\\theta$ in a Bayesian modeling $p(y|x, \\theta)$. Specifically, $\\theta$ contains a set of prompts, and the paper proposes an algorithm to optimize this set of prompts in order to maximize $p(D | \\theta)$, where D is the data, so that we view the optimized set of prompts $\\theta$ as faithful to the original prior (e.g., human-defined constraints), and best for model's performances on data $D$. The authors claim that using these prompts, we can better estimate the model's internal confidence by computing the expected probability over all prompts. \n\nThe core of this optimization algorithm is called Metropolis–Hastings through LLM Proposals (MHLP), which uses an LLM-powered optimization step (TextGrad) and the Metropolis–Hastings algorithm, which is a novelty of the paper. The paper also introduced semantic ECE, which can be used to calibrate free-form generations of LLMs. Experiments showed that the proposed Textual Bayes method led to better expected calibration error and better task performance across several datasets (AIME/SimpleQA/QASPER), and more reliable factual claims."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The overall setup offers a novel perspective on existing discussions in prompt selection, multi-step reasoning (e.g., self-reflection and TextGrad), and calibration, by treating textual prompts as Bayesian parameters. The authors provided a working recipe for this setup and demonstrated relatively promising results on calibration and factual correctness experiments. The paper is well organized, with a background section that helped me understand the overall setup in a short time. This paper may inspire future works to think more about abstracting LLM components into probabilistic frameworks."}, "weaknesses": {"value": "1. The experiments are a little weak for the paper to be well-perceived as an uncertainty quantification work. The experiments are on three QA datasets with small sample counts. Some gains are modest and may not be statistically decisive (especially considering the variances). The baselines are a little weak, where the authors should consider prompt‑ensemble and decoding‑ensemble baselines (e.g., self‑consistency with diverse system messages/instruction styles, retrieval‑augmented ensembles, or perturbation and entropy-based methods). To showcase the effectiveness of the proposed prompt optimization, the authors should consider demonstrating examples and a human study, as well as comparing against simple baselines (e.g., having an LLM to \"decompose\" the original prompt into a diverse set of candidate system prompts that consider multiple angles).\n\n2. The authors should cite other papers that similarly abstract textual factors into Bayesian parameters in LLM settings for training, inference, and uncertainty quantification that share high-level motivations. Example: BIRD: A TRUSTWORTHY BAYESIAN INFERENCE\nFRAMEWORK FOR LARGE LANGUAGE MODELS"}, "questions": {"value": "See weaknessness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0U8q5ur2cI", "forum": "VPmsAr1OTl", "replyto": "VPmsAr1OTl", "signatures": ["ICLR.cc/2026/Conference/Submission6215/Reviewer_dut8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6215/Reviewer_dut8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709333898, "cdate": 1761709333898, "tmdate": 1762918552343, "mdate": 1762918552343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims at uncertainty quantification in LLM-based systems. Specifically, The authors interpret prompts as Bayesian textual parameters in a statistical model and perform Bayesian inference over these prompts with introduction of  Metropolis-Hastings through LLM Proposals (MHLP). These prompts are then used to calculate uncertainty. They demonstrate improved predictive accuracy and uncertainty quantification (UQ) on question-answering tasks and also in a conformal prediction setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a novel approach to quantify the uncertainty associated with prompts through Bayesian inference.\n\nThe narratives are generally clear and easy to follow."}, "weaknesses": {"value": "The empirical improvements in Tables 1–3 are relatively small compared to the substantial computational overhead reported in Appendix Table 4. Since the evaluation is also conducted on very small datasets, this further weakens the strength of the empirical evidence.\n\nI have concerns regarding the final sampled prompts. For example, I do not understand why the proposed prompt in step 20 is rejected. Another baseline that should be tested is just paraphrasing the prompt while keeping the question the same. The authors should also providing a full set of sampled prompts for at least one example which would help clarify this issue.\n\nThe justification regarding semantic entropy (lines 314–320) is not convincing. If multiple answers are semantically equivalent, they should be treated as a single answer. Similarly, in Tables 1 and 3 the evaluation should incorporate semantic clustering so that semantically equivalent answers are not counted as distinct ones."}, "questions": {"value": "1. How exactly do the authors use suggestions from LLMs to propose values of θ that implement the guidelines? (Line 254) Could the authors show the actual prompt?\n\n2. How are the textual constraints on prompts encoded in the prior, and how is it determined what constraints a valid prompt should satisfy? (Line 232)\n\n3. In line 817, what does “substitute the final LLM call in our LLM-based system with an open-source model” specifically mean? \n\n4. Missing citations: https://aclanthology.org/2024.naacl-long.390.pdf, https://arxiv.org/pdf/2412.09572"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UBp96GzB2o", "forum": "VPmsAr1OTl", "replyto": "VPmsAr1OTl", "signatures": ["ICLR.cc/2026/Conference/Submission6215/Reviewer_D3Rb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6215/Reviewer_D3Rb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877644034, "cdate": 1761877644034, "tmdate": 1762918551983, "mdate": 1762918551983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose Textual Bayes to quantify uncertainty in black-box LLM systems by treating prompts as Bayesian textual parameters. It introduces a new MCMC algorithm, MHLP, which uses the LLM to generate prompt proposals improving uncertainty quantification and predictive accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The central idea of treating the prompt ($\\theta$) as a textual parameter in a formal Bayesian model ($p(\\theta|\\mathcal{D})$) is interesting. It provides a principled, statistical approach to modeling the brittleness of LLM systems, namely their sensitivity to prompt engineering.\n- The proposed MHLP algorithm is theoretically well motivated. It repurposes prompt optimization techniques as MCMC proposal distributions for UQ even when using closed-source, black-box APIs."}, "weaknesses": {"value": "**Motivation:** The claim \"we are the first both to quantify the uncertainty associated with prompts in LLM-based systems\" (line 79) is not correct! There exist multiple works that have already considered this, such as Abbasi-Yadkori et al. [1], Tonolini el al. [2], or Zhang et al. [3].\n\n**Method**: The authors introduce a sophisticated and complex theoretical methodology. However, the proposed practical implementation relies on crude approximations that compromise the Bayesian guarantees and that questions whether a much simpler, cheaper heuristic would perform just as well:\n1. Correction Factor: The required Hastings Correction is ignored (set to 1) because black-box LLMs provide no probabilities. This transforms the method from a theoretically sound sampler into an ad-hoc Metropolis algorithm.\n2. Acceptance Metric: The core quality measure (the likelihood term) is calculated by a different surrogate model. Moreover, to make the method computationally feasible, it is approximated over a singe sample (batch size set to 1 as stated in in line 283).\nThe complex MCMC process ultimately reduces to accepting a new prompt if a single sample yields a higher log-likelihood score under a surrogate model. \n\n**Evaluation**: \n1. The uncertainty being quantified is primarily over prompt choice rather than epistemic or aleatoric uncertainty in the traditional sense. This should be made more clear and also reflected in the title as it might be misleading.\n2. The authors claim that \"all methods use the same number $m$ of LBS calls during inference to ensure a fair comparison from a computational perspective\" (line 312). This is technically true but highly misleading: the methods use the same inference budget (10 calls per test instance) but they do *not* use the same total budget. MHLP gets 180-300 extra calls that CoT doesn't get.\n\n\n---\n[1] Y. Abbasi-Yadkori, Ilja Kuzborskij, A. György, and C. Szepesvari. To believe or not to believe your LLM: iterative prompting for estimating epistemic uncertainty. NeurIPS, 2024.\n\n[2] F. Tonolini, N. Aletras, J. Massiah, and G. Kazai. Bayesian Prompt Ensembles: Model Uncertainty Estimation for Black-Box Large Language Models. ACL, 2024.\n\n[3] Z. Zhang, A. Verma, F. Doshi-Velez, B. Low. Understanding the Relationship between Prompts and Response Uncertainty in Large Language Models. arXiv preprint arXiv:2407.14845, 2024."}, "questions": {"value": "- The argument for not computing an uncertainty score (such as SE), namely that the \"method is a means of generating a diverse answer set\" (line 314), is not clear. Why does MHLP lead to diverse answers? And why can't an uncertainty score be computed that is then compared to SE and other UQ methods?\n- Re. Evaluation point 2: What if CoT gets the same overall budget of MHLP? Would MHLP with matched compute still underperform this baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JL8zlndsNY", "forum": "VPmsAr1OTl", "replyto": "VPmsAr1OTl", "signatures": ["ICLR.cc/2026/Conference/Submission6215/Reviewer_jbfF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6215/Reviewer_jbfF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910132994, "cdate": 1761910132994, "tmdate": 1762918551547, "mdate": 1762918551547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a Textual Bayes, a Bayesian framework for uncertainty quantification in LLM-based systems. The key idea of this work is how to treat prompts for LLMs as training textual parameters and perform Bayesian inference over them. For this, authors introduce a novel algorithm Metropolis–Hastings through LLM Proposals (MHLP), which leverages LLM-driven prompt optimization to generate proposal prompts and sample from a posterior distribution. It allows to estimate uncertainty over both prompts and outputs.\n\nExperimental results on QA and factuality benchmarks show that proposed framework, Textual Bayes, improves the following metrics: predictive accuracy, calibration (via lower Expected Calibration Error and a new semantic ECE metric). This method also improves factual reliability in conformal prediction settings by reducing the number of removed claims. Overall, the authors introduce novel integration of Bayesian reasoning with black-box LLM-based systems, enhancing uncertainty quantification in generative AI."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Recasts prompts as Bayesian textual parameters in \\( p(y \\mid x, \\theta) \\) and performs inference over prompts \\(\\theta\\).\n2. Introduces MHLP, an MCMC scheme with LLM-driven proposals – both novel and well-motivated.\n3. Clear prior construction via free-form textual constraints.\n4. Proposal mechanism targets high-posterior prompts by respecting priors and improving data likelihood \\( p(D \\mid \\theta) \\).\n5. Demonstrates consistent gains in accuracy and calibration across QA benchmarks."}, "weaknesses": {"value": "1. High computational cost due to MCMC with large LLMs. It is worth considering lighter models.\n2. The paper explicitly uses a cold/hot posterior and surrogate/open-source likelihoods for closed-source models, but doesn’t show sensitivity to these choices, which could bias the posterior and calibration.\n3. Evaluation limited to single-prompt systems; no empirical tests with multiple textual parameters \\((\\theta_1, \\ldots, \\theta_k)\\).\n4. Limited study of prior quality and initialization (limited analysis of sensitivity to priors and initialization)"}, "questions": {"value": "1.\tThe paper mentions applying a “cold/hot posterior” using a likelihood temperature \\( \\tau \\) (or the equivalent scaling factor \\( \\beta \\)), but does not explain the choice or its impact. Could you clarify what specific temperature values were used, how they were selected or tuned, and whether model accuracy or calibration metrics (e.g., ECE) are sensitive to different temperature settings?\n2.\tCould you report standard diagnostics for the MHLP sampling process, such as acceptance rates, effective sample sizes, or any convergence checks (e.g., monitoring of likelihood or posterior stability)? Additionally, did you test whether using longer Markov chains (more MHLP steps) materially changes the results in terms of accuracy or calibration?\n3.\tHow do outcomes change with weaker/stronger or mis-specified textual priors \\\\(p(\\theta)\\\\)? Did you run any ablations that vary the prior constraints versus the data likelihood \\\\( p(D \\mid \\theta) \\\\) to assess sensitivity?\n4.\tCan MHLP handle multiple textual parameters \\\\( \\theta = (\\theta_1, \\ldots, \\theta_k) \\\\) within a single system pass, and have you tested this capability in agentic or multi-stage LLM pipelines?\n5.\tThe paper references the use of a “cold/hot posterior” and defines a tempered likelihood scaling factor \\\\( \\beta := \\frac{n}{\\tau b} \\\\), but the description appears inconsistent – Section 3 mentions a cold posterior, while Appendix A.1 describes an effective “hot” behavior when \\\\( \\beta < n \\\\).  Please clarify whether the posterior is cold, hot, or generally tempered; explicitly report the values of \\\\( \\tau \\\\) (or \\\\( \\beta \\\\)) used in experiments and how they were selected.  In addition, please interpret these quantities relative to the mini-batch size \\\\( b \\\\), not to the total dataset size \\\\( n \\\\), to make the scaling consistent and reproducible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kLXo6tIXCM", "forum": "VPmsAr1OTl", "replyto": "VPmsAr1OTl", "signatures": ["ICLR.cc/2026/Conference/Submission6215/Reviewer_wYh1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6215/Reviewer_wYh1"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6215/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997910252, "cdate": 1761997910252, "tmdate": 1762918550513, "mdate": 1762918550513, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}