{"id": "2tHt7cMkCF", "number": 14915, "cdate": 1758245484281, "mdate": 1759897341627, "content": {"title": "KEIC: A Framework and Dataset to Self-Correcting Large Language Models in Conversations", "abstract": "Large language models (LLMs) are adept at generating coherent and fluent responses within conversational contexts. Recent studies also demonstrate that LLMs can follow the user preference in an extremely long-term setting. Nevertheless, there is still lack of comprehensive research exploring LLMs to dynamically update their knowledge in response to corrections of misinformation provided by users during dialogue sessions. In this paper, we present a unified framework termed Knowledge Editing In Conversation (KEIC), along with a 1,781 human-annotated dataset, devised to assess the efficacy of LLMs in aligning the user update in an in-context setting, wherein the previous chat containing a false statement that conflicts with the subsequent user update. Through systematic investigations on more than 25 LLMs using various prompting and retrieval-augmented generation (RAG) methods, we observe that the contemporary LLMs exhibit a modicum of proficiency in this task. To enhance their self-correction abilities, we propose a structured strategy to handle the information update in a multi-turn conversation. We demonstrate that our approach is effective and suggest insights for research communities in this emerging and essential issue.", "tldr": "We present and formalize the KEIC task for LLMs to update their knowledge based on user corrections, construct a 1,781 human-labeled dataset, and propose a structured approach to this task, including an iterative algorithm for self-correction.", "keywords": ["LLMs", "self-correction", "zero-shot", "framework and systematic evaluation for misinformation correction"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1bc4e300babf3c2c4b23ec68fbc98e1d7f8a6b3.pdf", "supplementary_material": "/attachment/8cb0db3831593dbcc98b771791c9ff2dc2e415e5.zip"}, "replies": [{"content": {"summary": {"value": "This work focuses on a question in the context of multi-turn conversations, that is *if LLMs can dynamically update their knowledge in response to corrections of misinformation provided by users.*. This task is also formulated as `Knowledge Editing In Context (KEIC)`.\n\nUnder this topics, this work has made three contributions:\n\n1. This work proposes a unified framework `KEIC` to decomposes dialogues into four disjoint phases, which can standardize the evaluation.\n\n2.Derived from the CoQA dataset, this work has developed a human-annotated dataset of 1781 instances.\n\n3. This work has designed four methods for simulating the user corrections in LLMs.\n\n4. Experiments on massive LLMs have shown the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed KEIC framework can well formalize the in-context knowledge editing task for LLMs in conversations, which allows us to better process the dynamic knowledge updating. Especially the decomposition of different types of utterances.\n\n2. Constructs a high-quality 1,781-instance human-annotated dataset  with clear definitions (such as  \"effective new fact\" ).\n\n3. Four practical and model-agnostic correction methods are proposed (OTC, Verification, Reiteration, Deletion), which can be adopted in various scenarios.\n\n4. Extensive experiments are conducted and a set of key insights are also investigated.  This research can help the future studies in the community."}, "weaknesses": {"value": "1. The dataset is only limited to  YN (Yes or No) questions and is also limited to some specific domains. The generalizability to more diverse real conversational scenarios has not been well-proven.\n﻿\n2. The major contribution is the task definition, dataset definition (sec 2). However, the novelty of the methodology part is limited.  Such methods use some pre-defined prompts, which may show some biases. Meanwhile, it seems like the `Deletion` is very costly.\n﻿\n3. The baseline setting is not very strong. Thus, the experiments can show the positive effectiveness but is hard t show how well it is."}, "questions": {"value": "1. It would be better to discuss the application of Sec 2.2 more.\n\n2. Please summarize the most highlighted findings of this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AcRv3vDmCk", "forum": "2tHt7cMkCF", "replyto": "2tHt7cMkCF", "signatures": ["ICLR.cc/2026/Conference/Submission14915/Reviewer_bEWG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14915/Reviewer_bEWG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722194849, "cdate": 1761722194849, "tmdate": 1762925257968, "mdate": 1762925257968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formalizes Knowledge Editing In Conversation (KEIC)—updating misinformation within an ongoing dialogue without changing parameters—and introduces a unified framework plus a 1,781-instance human-annotated dataset (CoQA-derived, Yes/No). It benchmarks 25+ LLMs with prompting/RAG and proposes four model-agnostic strategies (including Reiteration and an iterative external correction). Results indicate modest KEIC proficiency overall and report improvements from structured strategies on subsets of models/tasks, with evidence primarily based on the released dataset and analyses centered on GPT-3.5. The work frames an emerging problem and offers initial methodology and empirical observations rather than a definitive solution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear definition of KEIC as non-parametric conversational knowledge updating, and a four-phase decomposition that makes the task structured and testable.\n2. A 1,781-instance human-annotated dataset with minimally edited new facts provides a practical foundation for controlled evaluation and future benchmarks.\n3. The work proposes model-agnostic strategies with graded intervention strength, including OTC, Verification, Reiteration, and Deletion, combining prompting with NLI/RAG components while considering cost and practicality.\n4. The study addresses a capability of practical importance for long-horizon dialogue and offers actionable guidance such as favoring explicit negation, placing updates near the test turn, and using reiteration for greater stability."}, "weaknesses": {"value": "1. The paper lacks dedicated Related Work and Limitations sections in the main text and instead places them in the appendix, which is misaligned with ICLR standards and makes it difficult to assess novelty, positioning, and scope; a concise comparison to prior work and a clear limitations discussion should be moved into the main body.\n\n2. The dataset is relatively small and skewed toward Yes/No and CoQA-derived narratives, limiting coverage of open-ended answers, multi-hop reasoning, numerical or temporal updates, non-English conversations, and realistic misinformation distributions; expanding scope and diversity and adding harder update scenarios would strengthen claims.\nThe evaluation relies heavily on template-based corrections and automatic Y/N scoring, with limited human assessment of coherence, persona consistency, side effects, and long-horizon stability; incorporating blinded human judgments, persistence tests across many subsequent turns, and measurements of latency and cost would improve robustness.\n\n3. The proposed methods are not universally effective across models. In Figure 4 the OTC intervention does not consistently improve performance across models, and subsequent validation centers on GPT‑3.5, limiting representativeness; report per-model results with confidence intervals or significance tests and extend validation to multiple model families and sizes, including contemporary frontier models.\n\n4. The oracle analysis for Reiteration in Figure 4c is used to hypothesize that auto-generated rewrites would perform similarly to human-written ones without direct evidence, so the authors should run a controlled experiment using auto-generated rewrites under the same protocol and compare against human versions with matched controls and statistical testing."}, "questions": {"value": "1. Why does the main text omit Related Work and Limitations while devoting substantial space to analyzing a single model’s (GPT‑3.5) behavior?\n2. Can you explain why Yes/No is used as the primary evaluation rubric and how you handle partially correct or ambiguous answers (including AE-produced N/A or ties)?\n3. Beyond oracle (human) rewrites, can you run a matched-protocol comparison on a representative Dval subset between Reiteration with model-generated rewrites and the human version; given that different models vary in rewrite quality, how will you control for this, and do your conclusions still hold across models?\n4. Why is the comprehensive analysis focused on GPT‑3.5, and how do you substantiate the generalizability of key conclusions (e.g., template sensitivity, Reiteration outperforming OTC)?\n5. The CAM vs. CBA proximity effect is currently observed on GPT‑3.5; how will you ensure or verify that the same phenomenon holds for other models (families/sizes)?\n6. Deletion effectively keeps only correct history, so improvement is intuitive but costly; how do you define and measure cost and determine when it is worth using, and what happens if Deletion is imperfect (e.g., fails to remove all incorrect content or removes useful context)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0fpO6qXImG", "forum": "2tHt7cMkCF", "replyto": "2tHt7cMkCF", "signatures": ["ICLR.cc/2026/Conference/Submission14915/Reviewer_ZfZe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14915/Reviewer_ZfZe"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915838954, "cdate": 1761915838954, "tmdate": 1762925257610, "mdate": 1762925257610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a critical and challenging task: Knowledge Editing In Conversation (KEIC), aimed at evaluating and enhancing LLMs' ability to dynamically update their knowledge and correct misinformation in multi-turn dialogues. The authors formalize the KEIC framework, segmenting conversations into false, update, test, and other phases, and precisely defining what constitutes an \"effective new fact.\" To address this task, four model-agnostic user correction methods are proposed: One-Turn Correction (OTC), Verification, Reiteration, and Deletion. Reiteration and Deletion are identified as particularly effective. A high-quality, human-annotated KEIC dataset, comprising 1,781 instances derived from CoQA, is constructed to support this research, covering factual and non-factual narrative stories."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper discusses a crucial research question. The capability of in-context KE is highly demanded in realistic LLM products. Different from other knowledge conflict work, the paper focuses on user profiling.\n2. The dataset  CoQA contains 1781 instances with human annotation, and has a potential impact for related research.\n3. The evaluation is extensive, covering closed and open-sourced LLM families."}, "weaknesses": {"value": "1. My major concern can be the lack of comparison with existing methods. The paper primarily investigates in-context correction strategies within multi-turn dialogues. Some related methods deserve credit and comparison.\na. parameter-editing KE methods (e.g., MEND, ROME) \nb. retrieval-based approaches when used specifically for knowledge correction (Memory-Based Model Editing at Scale)\nc. methods that address knowledge conflict (ConflictBank)\n\n2. Another concern involves the top-performing Deletion method. While it effectively removes \"false knowledge,\" this information is not entirely useless, as the process of correction itself can provide valuable context.\n\n3. A clearer illustration of the novelty of the paper will help enhance the persuasiveness."}, "questions": {"value": "Please see Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nSmqTATQLC", "forum": "2tHt7cMkCF", "replyto": "2tHt7cMkCF", "signatures": ["ICLR.cc/2026/Conference/Submission14915/Reviewer_aTjc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14915/Reviewer_aTjc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14915/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984421625, "cdate": 1761984421625, "tmdate": 1762925257202, "mdate": 1762925257202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}