{"id": "Y05UeKa0XC", "number": 6872, "cdate": 1757999126568, "mdate": 1759897886638, "content": {"title": "FlexibleLLM: Making Low-Bit Quantization for Large Language Models More Flexible and Efficient", "abstract": "Low-bit quantization is crucial for deploying Large Language Models (LLMs) on resource-constrained hardware. However, existing Post-Training Quantization (PTQ) methods are limited by a monolithic view of outliers, failing to address their dual spatial distribution (both discrete and clustered) and overlooking \"attribute outliers\"—weights that are sensitive to quantization but not numerically large. Furthermore, these methods generally ignore the critical issue of quantization errors accumulating and amplifying across layers. To overcome these challenges, we introduce FlexibleLLM, a novel finetuning-free, weight-only PTQ framework founded on a new theoretical analysis of outliers. FlexibleLLM holistically addresses the outlier problem through three synergistic components: (1) To handle clustered outliers, the Self-Adaptive Block-Level Greedy Bit Search (SBGBS) module enables highly flexible, fractional-level bit-width allocation (e.g., 2.1 bits), optimizing the trade-off between hardware utilization and model accuracy. (2) For discrete outliers, the Discrete Outlier Suppression and Aware (DOSA) module employs a dual strategy: it innovatively uses Hadamard transforms for computationally efficient suppression of numerical outliers and a Hessian-aware mechanism to precisely handle overlooked \"attribute outliers”. (3) To combat error propagation, the Layer-Level Feedback and Denoising (LFD) module introduces a dynamic correction mechanism that mitigates the accumulation of ``activation noise'' from a global, cross-layer perspective. Extensive experiments demonstrate that FlexibleLLM achieves state-of-the-art performance, significantly outperforming not only existing finetuning-free methods but also many finetuning-based approaches, all while requiring substantially fewer computational resources. Code is available at https://anonymous.4open.science/r/FlexibleLLM.", "tldr": "FlexibleLLM, a finetuning-free weight-only Post-Training Quantization framework, makes low-bit quantization more efficient and flexible by focusing on the spatial distribution and intrinsic attributes of outliers as well as the noise they introduce.", "keywords": ["Large Language Models", "Model Compression", "Post-Training Quantization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd2c8ddbb70d98b8529769e3ce0a54440b42d47c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents FlexibleLLM, a weight-only post-training quantization method. At its core, three components are introduced: (1) A mixed-precision within each layer, (2) an outlier reduction mechanism with Hadamard transform, (3) asymmetric calibration. Unfortunately, I haven't found any of these to appear novel, given that they are already proposed in other works."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The name of each submodule looks fancy."}, "weaknesses": {"value": "My major concern for this work is its novelty. \n\n+ For SBGBS, the sailency metric has been proposed in SliM-LLM [1], in which the Hessian diagonal values and the weight norm are used. In this work, the authors used the weight norm, not the quantization error norm, which makes less sense given that it is not pruning. In the meantime, this metric is not ACCURATE since the weights are updated after every iteration. This problem has been discussed in the SparseGPT Algorithm, but I do not see any deep discussion in this paper. \n\n+ For NOR, this is the same as QuaRot. And more ironically, even the code implementation is the same as QuaRot without citing them in Section 3.2.1. Apparently, the authors know what they are doing. \n\n+ For LFD, the core idea and methodology are exactly the same as GPTAQ [2]. Another proof is that they used the code from GPTAQ by slightly modifying the function name. Yet they did not compare the GPTAQ in the experiments. \n\nI would reconsider my rating if the authors could acknowledge they used the implementation of QuaRot and GPTAQ, clarify the difference, if any, and provide the file `flexiblellm_utils.py` for a detailed evaluation. \n \n## Minor weakness\n\n+ The notation system in this work is very chaotic. Hessian is represented with both $H$ and $\\mathrm{H}$. \n\n+ The evaluation is primarily about perplexity, which is less trustworthy in the current LLM community. I would like to see MMLU and reasoning task accuracy. \n\n## Reference\n\n[1] SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models, ICML 2025. \n\n[2] GPTAQ: Efficient Finetuning-Free Quantization for Asymmetric Calibration, ICML 2025."}, "questions": {"value": "None."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Nvrvkt1xJ6", "forum": "Y05UeKa0XC", "replyto": "Y05UeKa0XC", "signatures": ["ICLR.cc/2026/Conference/Submission6872/Reviewer_zVui"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6872/Reviewer_zVui"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760808958518, "cdate": 1760808958518, "tmdate": 1762919127639, "mdate": 1762919127639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the challenge that existing post‑training quantization (PTQ) methods fail to adequately handle both “discrete outliers” and “attribute outliers,” and tend to ignore error accumulation across Transformer layers. To address this, the authors propose FlexibleLLM, a finetuning‑free, weight‑only PTQ framework, comprised of three key modules: SBGBS (self‑adaptive block‑level greedy bit search) for flexible fractional bit allocation, DOSA (discrete outlier suppression & awareness) combining Hadamard transforms and Hessian signals, and LFD (layer‑level feedback & denoising) to counter cross‑layer error propagation. Experimental results on LLaMA / LLaMA‑2 / LLaMA‑3 models under 2‑ and 3‑bit settings show that FlexibleLLM significantly outperforms both finetuning-free and many finetuning-based baselines in perplexity and zero-shot accuracy, while using substantially lower computation cost. The results validate that more flexible bit allocation, better outlier handling, and cross-layer error correction lead to more robust and efficient low-bit quantization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies key limitations in existing PTQ methods, including discrete outliers, attribute outliers, and cross-layer error accumulation, making the motivation very clear.\n\n- Introducing SBGBS allows adaptive, fractional bit allocation at the block level, improving precision without full finetuning. DOSA and LFD further enhance robustness by handling discrete outliers and mitigating cross-layer errors.\n\n- Evaluations span LLaMA, LLaMA‑2, and LLaMA‑3 models, under extremely low-bit settings (2-bit and 3-bit), demonstrating consistent improvements in perplexity and zero-shot tasks.\n\n- FlexibleLLM is weight-only and finetuning-free, achieving better accuracy than many finetuning-based methods while keeping computational cost low, making it practical for large-scale LLM deployment."}, "weaknesses": {"value": "- Combining SBGBS, DOSA, and LFD involves multiple stages and careful bookkeeping, increasing implementation difficulty.\n\n- Block-level bit search and Hadamard-based outlier suppression may require tuning for different models or layers.\n\n- Although finetuning-free, some steps like block-level greedy bit search and Hadamard transforms introduce extra computation compared to simpler PTQ methods.\n\n- While the paper emphasizes the advantage of being finetuning-free, in practice the cost of light finetuning is often acceptable even for large models. \n\n- The baselines used for comparison are relatively outdated and do not include more recent and stronger methods such as OSTQuant[1] or SpinQuant[2], which limits the significance of the claimed improvements.\n\n- Anonymous code links seem to be inaccessible.\n\n[1] Hu, Xing, et al. \"Ostquant: Refining large language model quantization with orthogonal and scaling transformations for better distribution fitting.\" arXiv preprint arXiv:2501.13987 (2025).\n\n[2] Liu, Zechun, et al. \"Spinquant: Llm quantization with learned rotations.\" arXiv preprint arXiv:2405.16406 (2024)."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eizMNdxhMo", "forum": "Y05UeKa0XC", "replyto": "Y05UeKa0XC", "signatures": ["ICLR.cc/2026/Conference/Submission6872/Reviewer_YATu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6872/Reviewer_YATu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830272014, "cdate": 1761830272014, "tmdate": 1762919126794, "mdate": 1762919126794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FlexibleLLM, a finetuning-free, weight-only Post-Training Quantization (PTQ) framework designed to make low-bit quantization of Large Language Models (LLMs) more accurate, flexible, and efficient, especially for deployment on resource-limited devices. It rethinks the nature of outliers in quantization, builds a modular finetuning-free framework with adaptive and fractional bit-width control, and sets a new benchmark for low-bit, high-accuracy LLM quantization."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Contribution of the Paper：The paper makes an original and technically solid contribution to the low - bit quantization of LLMs. It introduces FlexibleLLM, a finetuning - free PTQ framework that holistically addresses key limitations of existing methods through three modules.\n\n2. Modular design and High integration：FlexibleLLM use Self-Adaptive Block-Level Bit Search (SBGBS) module, Discrete Outlier Suppression and Awareness (DOSA) module, and Layer-Level Feedback Denoising (LFD) module to factilitate integtation into LLMs.\n\n3. Theory and Concept:The work is notable for its conceptual originality. It offers a new theoretical perspective on outliers by distinguishing between discrete, clustered, and “attribute” outliers.It also models the cumulative effects of quantization noise across layers.\n\n4. Methodology: The methodology is rigorous and empirically well - supported across multiple model families. It delivers strong performance improvements over SOTA baselines with minimal computational overhead."}, "weaknesses": {"value": "1. Lack of Direct Validation:The experimental results do not directly validate these specific theoretical claims.\n\n2. Impressive but Unclear Performance Improvements: The reported performance improvements are impressive but could be due to general optimization and calibration effects rather than the specific theoretical mechanisms proposed.\n\n3. Missing Controlled Experiments:There are no controlled experiments that:\n\n-  Isolate the influence of “attribute outliers” on quantization error.\n\n- Visualize or quantify the dual distribution of outliers.\n\n- Explicitly measure cross-layer noise accumulation before and after the LFD module."}, "questions": {"value": "1. The paper claims that outliers exhibit both discrete and clustered distributions. Could the authors present quantitative or visual evidence (e.g., density plots or clustering metrics) supporting this claim?\n\n2. Can the authors show how quantization noise propagates before and after applying LFD? For instance, is there a measurable reduction in activation variance or output deviation at deeper layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dnfpSqeHP4", "forum": "Y05UeKa0XC", "replyto": "Y05UeKa0XC", "signatures": ["ICLR.cc/2026/Conference/Submission6872/Reviewer_LcMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6872/Reviewer_LcMU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993699299, "cdate": 1761993699299, "tmdate": 1762919126353, "mdate": 1762919126353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FlexibleLLM, a finetuning-free, weight-only PTQ framework for large language models. It combines three key components: SBGBS for adaptive bitwidth allocation， DOSA for outlier suppression using a Hadamard transform and Hessian-aware calibration, and LFD for cross-layer error correction. The method targets both clustered and discrete outliers and reports consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated: it focuses on real problems—mixed outlier types and error accumulation—that most PTQ works overlook.\n- The integration of SBGBS, DOSA, and LFD forms a complete and coherent pipeline, not just a single heuristic.\n- Experimental coverage across LLaMA and Qwen models shows strong potential."}, "weaknesses": {"value": "- The inverse-Hessian correction might be heavy for large models. How is $H^{-1}$ approximated?\n- Many recent PTQ works (e.g., SpinQuant, QuaRot) also perform rotation or error compensation. The novelty boundary between DOSA and these methods should be better articulated."}, "questions": {"value": "- Weakness 1&2\n- What’s the absolute per-token latency and throughput with and without DOSA and LFD on A100/H100?\n-  Can you show empirical per-layer error amplification curves after applying LFD?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Bet7xYfdux", "forum": "Y05UeKa0XC", "replyto": "Y05UeKa0XC", "signatures": ["ICLR.cc/2026/Conference/Submission6872/Reviewer_ka1L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6872/Reviewer_ka1L"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994384708, "cdate": 1761994384708, "tmdate": 1762919124898, "mdate": 1762919124898, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FlexibleLLM, a finetuning-free, weight-only PTQ framework for LLMs. It tackles (i) clustered vs. discrete outliers and (ii) cross-layer error accumulation via three components:\n\n- SBGBS: a self-adaptive, block-level greedy bit search that allocates fractional bit widths (e.g., 2.1 bits) based on importance.\n\n- DOSA: a two-part module—(NOR) numerical outlier reduction via Hadamard transforms, and (HGC) a Hessian-guided mechanism to address “attribute outliers.”\n\n- LFD: layer-level feedback/denoising to mitigate activation-noise propagation with an error-aware objective.\n\nExperiments show improvements over several PTQ baselines on some metrics and models; the method is presented clearly with ablations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The framework is easy to follow; writing is organized and readable.\n\n2. Reasonable benchmarks and ablations; measurable gains on some tasks.\n\n3. Attention to both numerical and “attribute” outliers: Explicit handling is practically relevant."}, "weaknesses": {"value": "1. Limited novelty / incremental combinations: Each module largely extends known ideas with modest variations:\n\n- SBGBS: Importance-guided mixed-precision allocation is well-studied; the greedy search and block granularity feel incremental.\n\n- NOR (in DOSA): Hadamard transforms for outlier suppression/energy spreading are mature and previously explored.\n\n- HGC (in DOSA): Using Hessian information (or approximations) to guide quantization or error reconstruction is established.\n\n- LFD: Using output-error/activation-noise–aware objectives echoes prior PTQ works (e.g., QDrop-style criteria and related error-aware calibration).\n\nOverall, the contribution reads more as a package of small updates than a distinct conceptual advance.\n\n2. Many moving parts: Four submodules and several hyperparameters risk over-engineering, making it harder to attribute gains to any single idea and increasing deployment complexity.\n\n3. Ablation depth: While ablations exist, they don’t isolate what is truly new versus what prior techniques would already deliver under matched calibration/tuning.\n\n4. Theory claims: The “new theoretical analysis of outliers” is not clearly distinguished from existing analyses; formal novelty remains unclear."}, "questions": {"value": "Do gains persist across different seeds, calibration set choices/sizes, and task distributions (generation vs. perplexity vs. alignment tasks)? Any evidence of overfitting to the calibration set despite being “finetuning-free”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x7dUvKV7p9", "forum": "Y05UeKa0XC", "replyto": "Y05UeKa0XC", "signatures": ["ICLR.cc/2026/Conference/Submission6872/Reviewer_yusG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6872/Reviewer_yusG"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission6872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996877271, "cdate": 1761996877271, "tmdate": 1762919124338, "mdate": 1762919124338, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}