{"id": "dWeKnWqmMZ", "number": 20405, "cdate": 1758305595311, "mdate": 1759896979260, "content": {"title": "HEART: Emotionally-driven test-time scaling of Language Models", "abstract": "Test-time scaling has shown considerable success in improving the performance of language models on complex reasoning tasks without requiring fine-tuning. However, current strategies such as self-reflection primarily focus on logical or structural refinement. They do not leverage the guiding potential of affective feedback. Inspired by psychological research showing that emotions can modulate cognitive performance, we introduce \\textit{HEART}--a novel framework that uses emotionally-driven prompts for iterative self-correction. \\textit{HEART} provides feedback on a model's incorrect response using a curated set of concise, emotionally charged phrases based on the six universal emotion categorized by Dr. Paul Ekman. By systematically varying the emotional tone of the feedback across iterations, our method guides the model to escape flawed reasoning paths and explore more promising alternatives. We evaluate our framework on challenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam, and SimpleQA. Our results reveal a significant new phenomenon: when guided by an oracle verifier, this affective iteration protocol unlocks significantly deeper reasoning, leading to consistent and substantial increases in accuracy over state-of-the-art baselines with the same verifier. However, we also identify a critical bottleneck for practical deployment. In a verifier-free setting, it struggles to harness these gains consistently, highlighting as a key challenge for future work. Our findings suggest that the next frontier in machine reasoning may lie not just in refining logic, but also in understanding and leveraging the `\\textit{HEART}' of the models.", "tldr": "", "keywords": ["Large Language Models", "Prompt Engineering", "Reasoning", "Affective Computing", "Iterative Refinement"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8de5704fa6aedb24026a3f72f51cea4732c4f5ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "* self-reflection + emotional style prompting\n* setting 1: oracle verifier -> works\n* setting 2: no oracle -> doesn't work"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* important problem. the relation between emotion&language is under-exploited. Emotional filled content is abundant in pretraining data but the current production pipeline aren't exploting this\n* candidly reporting the results in both setting\n* clear writing"}, "weaknesses": {"value": "* perf diff. too small without confidence interval. claiming success by saying 98.72>98.04 without conf. interval lacks substantial rigor\n* if this is a prompting-based technqiue, I don't see why the author selects such a subset of benchmarks. should at least ~9 benchmark across different domain to show robustness. It's okay if HEART doesn't win everyone, but some pattern is what's valuable (e.g., is there no performance benefit in fact-based or niche-knowledge benchmark)?\n* The author should think about what the right problem setup task+dataset is that is the most reasonable setting to demonstrate the core message: emotional style matters. Self-verification doesn't seem like the right task"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YcVXtA0Vnb", "forum": "dWeKnWqmMZ", "replyto": "dWeKnWqmMZ", "signatures": ["ICLR.cc/2026/Conference/Submission20405/Reviewer_YaeC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20405/Reviewer_YaeC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761278907763, "cdate": 1761278907763, "tmdate": 1762933850731, "mdate": 1762933850731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HEART, a test-time scaling framework that guides LLMs through iterative self-correction using emotionally charged prompts inspired by Paul Ekman’s six basic emotions. By alternating emotional valence, the method aims to help models escape flawed reasoning paths and explore new ones. Experiments on reasoning benchmarks show improvement under oracle-guided settings, though performance remains inconsistent without a verifier."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths\n\n- The paper is well written and easy to follow.\n\n- Proposes an interesting idea of incorporating affective feedback into test-time scaling through emotionally driven prompts.\n\n- Builds a clear motivation, connecting affective science and cognitive performance to model reasoning.\n\n- Provides systematic ablation showing that dynamic alternation of emotional valence yields stronger effects than static emotions."}, "weaknesses": {"value": "Weaknesses and Questions\n\n- Limited technical contribution: The method remains a prompt-engineering approach without novel algorithmic or optimization components. Its contribution is primarily conceptual rather than methodological.\n\n- Strong dependence on oracle verifier: As the authors pointed out, the performance gains heavily rely on oracle-based candidate selection (S1), while the verifier-free setting (S2) shows inconsistent improvement. Could the authors explore reinforcement learning or internal uncertainty signals to reduce this dependence?\n\n- Ambiguous theoretical interpretation: The link between psychological theories (e.g., opponent-process theory) and actual model behavior is metaphorical rather than mechanistic. Is there any theoretical reasoning or educated guess to explain why affective cues influence reasoning dynamics inside LLMs?\n\n- Unclear source of improvement: It remains unclear whether the observed gains stem from the affective valence itself or simply from increased prompt diversity. Could the authors quantify how much of the improvement arises purely from linguistic variety rather than emotional content?\n\n- Unclear usage of additional emotions (Fear / Disgust): The paper notes these emotions as “additional possible values for G⁻” but never specifies when or how they were used in experiments. Were they sampled in certain iterations or merely included conceptually? Clarifying this is important for reproducibility.\n\n- Computational inefficiency due to sequential dependency: Unlike existing test-time scaling methods that support parallel generation or intra-step self-verification, HEART must wait for each full output before applying the next emotional cue, which increases latency and cost. Have the authors considered partial-step feedback or batching strategies?\n\n- Formatting inconsistency with ICLR standards: The manuscript deviates from the official template in several concrete ways: the top/bottom margins are incorrect, and table captions are placed below the tables rather than above.\n\n- Vulnerability to misguidance during iteration: In real-world scenarios where ground-truth verification is unavailable, repeated iterations could cause the model’s answers to drift rather than converge. As feedback accumulates without correctness signals, HEART might reinforce or amplify earlier reasoning errors, leading to systematically wrong conclusions. An analysis of whether such cases occurred—and under what conditions these misguidance patterns emerged—would be valuable to assess the stability and reliability of the method."}, "questions": {"value": "See above section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GS6xkp93Rj", "forum": "dWeKnWqmMZ", "replyto": "dWeKnWqmMZ", "signatures": ["ICLR.cc/2026/Conference/Submission20405/Reviewer_Z88G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20405/Reviewer_Z88G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971398485, "cdate": 1761971398485, "tmdate": 1762933850433, "mdate": 1762933850433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents HEART, a test-time framework that guides large language models through emotionally charged prompts for iterative self-correction. Drawing inspiration from psychology on emotion–cognition interactions, HEART introduces affective feedback (e.g., disappointment, encouragement) using six universal emotions to steer reasoning. When paired with an oracle verifier, HEART often boosts accuracy on reasoning benchmarks like Humanity’s Last Exam and OlympiadBench. The study shows that dynamic emotional tone can nudge models away from flawed reasoning paths, frequently outperforming neutral, logic-based prompts under oracle selection. However, in verifier-free settings, performance gains diminish, highlighting challenges in autonomous emotional prompting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles a novel and timely topic at the intersection of psychology and LLM reasoning, backed by substantial experiments.  \n- The overall writing is clear, accessible, and well-structured."}, "weaknesses": {"value": "- **Clarification on opponent-process theory** In line 180, the paper states that feedback alternates between positive and negative emotions inspired by opponent-process theory. However, for readers unfamiliar with this theory, it would be helpful if the authors provided additional background. Based on the introduction, opponent-process theory seems to describe selecting the most context-appropriate emotional response, not strictly alternating between opposing emotions. Please clarify how the alternating feedback mechanism aligns with or extends the original psychological framework.  \n- **Dependence on oracle verification** Most key results rely on the oracle-assisted S1 setting, where a verifier chooses the best candidate at each iteration. In the more realistic verifier-free S2 regime, performance gains are smaller or inconsistent, sometimes underperforming strong baselines. This suggests that HEART’s benefits depend heavily on oracle selection. It would strengthen the paper to explain why the S1 regime may still be realistic or valuable in real-world scenarios where partial verification or heuristic scoring may exist.  \n- **Missing vanilla baseline and prompt examples** Please include the results of a vanilla baseline that uses a single reasoning pass (without multiple rounds). Also, provide the full prompt templates for all baselines, including examples. For the CoT baseline, it is unclear whether the model was told that its previous answer was incorrect. The description is somewhat vague—did the CoT prompt simply append “think step-by-step” without referencing prior outputs? Clarifying whether CoT repeats or overwrites previous answers would help interpret its advantage over the vanilla baseline.  \n- **Under-specified selection in verifier-free setting** In S2, the “generative synthesis” step aggregates multiple candidate outputs into a final answer, but the procedure is not clearly described. Please elaborate on this scenario—specifically, provide the full prompt template, describe how candidates are presented to the synthesizer model, and include an example of the input–output format. This will help readers understand how the final output is produced in verifier-free conditions.  \n- **Missing modern test-time baselines** The comparisons omit recent plug-and-play test-time optimization methods (e.g., SRGen, SLOT) that dynamically adjust internal computations during decoding. Since HEART similarly improves reasoning at inference time without training, including these would make the evaluation more complete. Furthermore, works mentioned in the related work section—whether they involve static interventions or external supervision—should also be included as comparative baselines to contextualize HEART’s effectiveness.  \n- **Inference cost comparison** Please report the inference cost (number of tokens) in all result tables, and clarify whether Figure 3 evaluates all methods under identical inference budgets. This will help assess efficiency and fairness across methods.  \n- **Hyperparameter setting (temperature)** It would be useful to report results using temperature = 1 (greedy decoding) in addition to the current setup, as it may reveal whether performance improvements depend on stochasticity."}, "questions": {"value": "Refer to the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "itTFxMrsti", "forum": "dWeKnWqmMZ", "replyto": "dWeKnWqmMZ", "signatures": ["ICLR.cc/2026/Conference/Submission20405/Reviewer_n92z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20405/Reviewer_n92z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022522956, "cdate": 1762022522956, "tmdate": 1762933849835, "mdate": 1762933849835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HEART, a framework for emotionally-driven test-time scaling, where large language models (LLMs) iteratively self-correct using emotionally charged prompts inspired by Ekman’s six basic emotions. The approach aims to combine structured reasoning (e.g., CoT, Self-Refine) with affective modulation to guide reasoning in more human-like ways. Experiments on OlympiadBench, Humanity’s Last Exam, and SimpleQA show strong gains in an oracle-guided setting (up to +10–15% over CoT or Self-Reflection baselines), suggesting that alternating emotional valence helps escape reasoning traps. However, the method’s effectiveness sharply declines in a verifier-free setting, revealing a selection bottleneck."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The conceptual framing is quite novel, and the intersection between affective psychology and LLM reasoning control is interesting. \n- The writing is pretty clear and the method is easy to understand."}, "weaknesses": {"value": "- The success seems quite reliant on the oracle supervision. The main gains depend on access to a ground-truth verifier; without it, performance is a bit inconsistent. \n- It remains unclear why emotional phrasing improves reasoning — whether due to linguistic diversity, novelty, or implicit temperature control. It would be nice for the authors to further investigate some qualitative samples and hypothesize why this might be the case. \n- Further, is emotion control the special piece of the prompt, or are there other forms of intermediate feedback that also helps with the reasoning performance of these models?"}, "questions": {"value": "see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSSIZuwlKp", "forum": "dWeKnWqmMZ", "replyto": "dWeKnWqmMZ", "signatures": ["ICLR.cc/2026/Conference/Submission20405/Reviewer_2NEu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20405/Reviewer_2NEu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20405/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141949663, "cdate": 1762141949663, "tmdate": 1762933849371, "mdate": 1762933849371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}