{"id": "zAjcuqN31s", "number": 12304, "cdate": 1758206954484, "mdate": 1759897518828, "content": {"title": "SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models", "abstract": "Diffusion models (DMs), lauded for their generative performance, are computationally prohibitive due to their billion-scale parameters and iterative denoising dynamics. Existing efficiency techniques, such as quantization, timestep reduction, or pruning, offer savings in compute, memory, or runtime but are strictly bottle-necked by reliance on fine-tuning or retraining to recover performance. In this work, we introduce SlimDiff, an automated activation-informed structural compression framework that reduces both attention and feedforward dimensionalities in DMs, while being entirely gradient-free. SlimDiff reframes DM compression as a spectral approximation task, where activation covariances across denoising timesteps define low-rank subspaces that guide dynamic pruning under a fixed compression budget. This activation-aware formulation mitigates error accumulation across timesteps by applying module-wise decompositions over functional weight groups: query–key interactions, value–output couplings, and feedforward projections — rather than isolated matrix factorizations, while adaptively allocating sparsity across modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff achieves up to 35% acceleration and ~100M parameter reduction over baselines, with generation quality on par with uncompressed models without any backpropagation. Crucially, our approach requires only about $500$ calibration samples, over 70X fewer than prior methods. To our knowledge, this is the first closed-form, activation-guided structural compression of DMs that is entirely training-free, providing both theoretical clarity and practical efficiency.", "tldr": "", "keywords": ["Text-to-image models", "training-free", "structural compression"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3f6b5b15c28d68a8b8da804b34d4f2c72254842a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a training-free, activation-guided structural compression pipeline for diffusion models. It first builds a compact SlimSet with around 500 prompts to estimate per-timestep activation correlations; then aggregates these with a Spectral Influence weighting; and applies whitening-SVD to QK/VO and a Nystrom approximation to FFN, with an automatic rank allocator under a global budget. The experiment shows speedups and around 100M params removed, while keeping FID/CLIP and human-preference scores near SD v1.4/1.5 baselines with no backpropagation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Training-free, closed-form pipeline with tiny calibration, yet competitive quality. \n\n2. Principled, module-aligned design: QK/VO via whitening-SVD, FFN via Nystrom; decisions anchored by Spectral Influence and aggregated timestep-aware correlations. \n\n3. Real efficiency wins (with reduced MACs and latency) with near-baseline human-preference scores (HPS v2.1, ImageReward, Pick-a-Pic)."}, "weaknesses": {"value": "1. Lack of end-to-end memory profiling. The paper reports MACs and latency (GPU/CPU) but not peak VRAM / host RAM in GiB under standard batch sizes/schedulers, which matters for deployment. \n\n2. Missing perceptual-consistency metrics. Since outputs are not identical to the uncompressed model, the paper should also report LPIPS to quantify per-image perceptual closeness to the uncompressed model, beyond FID/CLIP and human-prefs. \n\n3. Narrow backbone coverage. Experiments/ablations focus on SD v1.4/v1.5; there's no DiT/FLUX evaluation here, and no comparison with similar purposed methods such as SVDQuant / Q-Diffusion (structural vs quantization is claimed complementary but not demonstrated empirically; a simple compose-and-compare such as \"SlimDiff on top of SVDQuant\" would strengthen the claim)."}, "questions": {"value": "1. Weighting ablations are compelling, can you replicate them on a second backbone (e.g., SD3.5/DiT) to show the Spectral Influence advantage holds beyond SD v1.x? \n\n2. How does this method perform on larger models on video generation, such as WAN 2.2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Fp87t7jx6W", "forum": "zAjcuqN31s", "replyto": "zAjcuqN31s", "signatures": ["ICLR.cc/2026/Conference/Submission12304/Reviewer_EYvd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12304/Reviewer_EYvd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761460001007, "cdate": 1761460001007, "tmdate": 1762923235530, "mdate": 1762923235530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SlimDiff, a novel framework for compressing diffusion models (DMs) without the need for fine-tuning or retraining. SlimDiff achieves this by leveraging activation-guided structural compression, which dynamically prunes the model's dimensions based on activation covariances across denoising timesteps. The framework includes several key components: Spectral Influence Scoring to measure module importance, SlimSet for efficient calibration, timestep-aware correlation modeling, and Module-Aligned Data-Aware Compression (MADAC) for functional weight group compression. The method is shown to reduce parameters and FLOPs significantly while maintaining high-quality generation across various benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper introduces a new perspective on diffusion model compression by focusing on activation-guided structural compression. The idea of reframing DM compression as a spectral approximation task is novel and provides a principled way to handle the non-uniform geometry of diffusion trajectories.\nQuality: The methodology is robust, with detailed proofs and algorithms for the compression techniques. The experiments are thorough, covering multiple datasets and models, and the results are consistently strong across various metrics.\nClarity: The paper is well-organized and clearly written. The authors effectively communicate complex ideas through well-structured sections, clear diagrams, and detailed explanations.\nSignificance: The contributions are significant for the field, as they provide a practical solution for deploying DMs in resource-constrained environments. The ability to achieve high compression ratios without retraining is particularly valuable."}, "weaknesses": {"value": "While the paper demonstrates impressive results on specific diffusion models, it is unclear how well the framework scales to larger models or different architectures. The authors could provide more discussion on the potential limitations and scalability of SlimDiff. Although the paper compares SlimDiff with several baselines, a more comprehensive comparison with other state-of-the-art compression techniques (e.g., quantization combined with pruning) could strengthen the claims further."}, "questions": {"value": "How does SlimDiff compare with quantization techniques when used in combination? Can the authors provide insights into the potential benefits of stacking SlimDiff with quantization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0IUvbJiqgF", "forum": "zAjcuqN31s", "replyto": "zAjcuqN31s", "signatures": ["ICLR.cc/2026/Conference/Submission12304/Reviewer_Y7F6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12304/Reviewer_Y7F6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923686681, "cdate": 1761923686681, "tmdate": 1762923232880, "mdate": 1762923232880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SlimDiff, a method for reducing the size of diffusion models without backpropagation and using only a small calibration set. The authors create a representative subset of prompts through clustering and then use activation statistics to guide block-wise architectural slimming. The dimensionality reduction can be solved analytically, eliminating the need for finetuning. Experiments show that SlimDiff maintains generation quality and performs competitively or better than existing techniques, while requiring far fewer calibration samples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work targets a timely and practically important problem: the high computational burden of large diffusion models and their multi-step sampling processes. A post-training slimming strategy that boosts inference efficiency has broad relevance across pretrained diffusion systems.\n\n2. The method has a sound theoretical basis. The block-wise slimming is derived from a straightforward formulation and yields a closed-form solution, making the approach efficient and more principled than many heuristic methods.\n\n3. Experimental results demonstrate that SlimDiff achieves performance close to, or matching, gradient-based pruning methods, while greatly reducing post-processing time."}, "weaknesses": {"value": "1.  The paper does not fully justify that the constructed SlimSet is sufficiently representative. While the sampling strategy selects prompts far from cluster centroids or with high embedding distances, cluster centers may also correspond to common and semantically important patterns. Additional qualitative results or visual inspection of SlimSet coverage would help validate its representativeness.\n\n2. From Table 6, it seems that the block-wise quantization cannot provide too much performance gain. Can you show the gap in a more challenging case? For example, with a more aggressive pruning setting."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ng3oBkYXfH", "forum": "zAjcuqN31s", "replyto": "zAjcuqN31s", "signatures": ["ICLR.cc/2026/Conference/Submission12304/Reviewer_3m4w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12304/Reviewer_3m4w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961455957, "cdate": 1761961455957, "tmdate": 1762923232555, "mdate": 1762923232555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript presents SlimDiff, a training-free, activation-guided framework for structurally compressing diffusion models (DMs), aiming to enable faster and lighter deployment without fine-tuning. The authors point out that slimming DMs is challenging due to module coupling (QK/VO/FFN), timestep-dependent activation geometry, and multi-step error propagation. To address these issues, they propose three key techniques: Module-aligned decomposition, Data- and process-aware compression, and SlimSet calibration. Experiments on MS-COCO, LAION Aesthetics, ImageReward, and PartiPrompts show promising results."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The approach is highly efficient in both training and data usage.\n2. The proposed framework is methodologically coherent and well-aligned with the diffusion model computation graph."}, "weaknesses": {"value": "1. The manuscript is poorly organized. On one hand, it claims to propose a pruning method, yet the actual approach is matrix decomposition. These two concepts should be clearly distinguished. On the other hand, the paper often introduces strategies before explaining their rationale, making it difficult to grasp the underlying motivation and to justify why the proposed strategy is the most appropriate solution.\n2. The method does not explicitly address error accumulation. While the authors claim that compression errors accumulate across timesteps and strongly affect diffusion model inference, the proposed method only performs timestep-wise metric evaluation without a joint calibration analysis across timesteps.\n3. The evidence for generalization is limited, as the experiments are conducted almost exclusively on Stable Diffusion v1.5.\n4. The choice of baselines is confusing. As a matrix decomposition-based lightweighting method, the paper should at least compare against the most naïve low-rank decomposition baseline, as well as demonstrate advantages over state-of-the-art matrix decomposition methods. However, some selected baselines, such as BK-SDM, are not directly comparable since they involve modular layer pruning rather than matrix factorization."}, "questions": {"value": "1. How does the method perform when the compression ratio is further increased? At what point does performance begin to collapse?\n2. Why does the compression process still require 4 days? Could the authors provide a breakdown of time costs across the different stages?\n3. Can the method generalize to recent state-of-the-art DiT models, including text-to-image and text-to-video tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DbnK2mBNaC", "forum": "zAjcuqN31s", "replyto": "zAjcuqN31s", "signatures": ["ICLR.cc/2026/Conference/Submission12304/Reviewer_n1XJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12304/Reviewer_n1XJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984298934, "cdate": 1761984298934, "tmdate": 1762923232282, "mdate": 1762923232282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SlimDiff, a backpropagation-free approach for slimming diffusion models using only a small calibration dataset. The authors first construct a representative calibration set through prompt clustering and then leverage activation statistics to guide block-wise structural slimming. The resulting dimensionality reductions are obtained in closed form, requiring no fine-tuning. Experimental results demonstrate that SlimDiff effectively preserves generative quality, achieving competitive or superior performance while relying on significantly fewer calibration samples than prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Interesting and practical problem: The paper addresses the efficiency bottleneck of modern diffusion models, which are computationally expensive due to both large model sizes and multi-step sampling. Developing post-processing methods that enhance inference efficiency is highly relevant and broadly applicable across pre-trained diffusion models.\n\n* Principled and theoretically grounded method: The proposed block-wise slimming strategy is formulated with theoretical justification, yielding a closed-form solution that makes the procedure both principled and computationally efficient.\n\n* Strong empirical performance: The experiments show that the proposed method performs comparably to gradient-based slimming techniques while requiring substantially less post-processing time."}, "weaknesses": {"value": "* Representativeness of samples: It remains unclear how the SlimSet construction guarantees inclusion of representative samples. While the selection strategy favors samples distant from cluster centroids or with large pairwise distances in embedding space, cluster centers may also capture semantically important and frequently occurring patterns. Visualizations or qualitative analyses demonstrating that SlimSet adequately captures data diversity would strengthen the paper.\n\n* Missing comparison with gradient-free pruning baselines: The paper does not compare against existing gradient-free structural pruning methods such as [1, 2]. If these approaches achieve similar generative quality under comparable parameter constraints, the claimed advantage of the proposed module-aligned, data-aware compression would be less convincing.\n\n* Marginal improvement from block-wise slimming: Table 6 suggests that the improvements from block-wise slimming are modest. It would be informative to include an additional baseline without block-wise estimation to isolate and quantify the specific contribution of this technique.\n\n* Clarity and notation issues: The paper introduces many variables without sufficiently clear definitions, which may impede readability. A dedicated subsection summarizing all notations is recommended. Specific points of confusion include: (1) Line 102: What is $L_{qual}$? (2)\nLine 180: $X$ is not defined in the momentum computation. (3) Line 182: What is the definition of $\\tilde{C}$? What is the relationship between it and $\\hat{C}$ in Line 180? Do they refer to the same variable as in Equation (2)?\n\n[1] Wang, Hongjie, et al. Attention-driven Training-free Efficiency Enhancement of Diffusion Models. CVPR 2024.\n\n[2] Nova, Azade, Hanjun Dai, and Dale Schuurmans. Gradient-free Structured Pruning with Unlabeled Data. ICML 2023."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8To29hTbpR", "forum": "zAjcuqN31s", "replyto": "zAjcuqN31s", "signatures": ["ICLR.cc/2026/Conference/Submission12304/Reviewer_UBzf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12304/Reviewer_UBzf"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12304/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762649451759, "cdate": 1762649451759, "tmdate": 1762923232034, "mdate": 1762923232034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}