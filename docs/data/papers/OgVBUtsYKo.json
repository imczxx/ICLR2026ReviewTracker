{"id": "OgVBUtsYKo", "number": 18730, "cdate": 1758290494139, "mdate": 1763031023186, "content": {"title": "PLuG-Attention: Unleashing the Potential of Attention via Plug-in Pairwise Logit Gating", "abstract": "Despite its widespread success on vision tasks, standard attention employs a shared dot-product mechanism that uniformly scores all query–key interactions before applying softmax. In this paper, we hypothesize that explicitly controlling the amplification or suppression of individual query-key token-pair interactions can lead to more expressive and discriminative representations. To this end, we propose \\textbf{Pairwise Logit Gating (PLuG)} attention, a simple yet effective plug-in approach that introduces a learnable gating mechanism operating on each token-pair to modulate attention logits prior to softmax. This gating enables the model to selectively amplify informative interactions and suppress spurious ones through gating coefficient matrix, improving its ability to capture spatial and semantic relationships critical for vision tasks. Experimental results demonstrate that PLuG can be seamlessly integrated into various attention mechanisms and attention-based architectures, including ViTs and Mask2Former, as well as multi-scale deformable attention in Deformable DETR, without requiring architectural redesign or hyperparameter tuning. These results highlight the effectiveness of PLuG as a general-purpose plug-in enhancement broadly applicable to attention-based vision tasks.", "tldr": "We introduce Pairwise Logit Gating (PLuG) attention, a simple yet effective plug-and-play mechanism that introduces a learnable gating mechanism that operates on each token pair to modulate attention logits prior to softmax.", "keywords": ["Attention Mechanisms", "Vision Transformers", "Deformable DETR", "Pairwise Logit Gating"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/481b432d590030e5f0548626fe0f0a90e1855a52.pdf", "supplementary_material": "/attachment/2fb1b6c4e98c125be3fde041e2e8203d8f8b3aed.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces PLuG (Pairwise Logit Gating), a plug-in mechanism designed to enhance attention mechanisms in vision models by modulating attention logits at the token-pair level before softmax. Unlike standard attention that uniformly scores query–key interactions, PLuG introduces learnable gates that selectively amplify or suppress specific token-pair interactions. The authors demonstrate that PLuG can be seamlessly integrated into various architectures—including ViTs, Mask2Former, and Deformable DETR—without architectural redesign or hyperparameter tuning. Extensive experiments across classification, segmentation, and detection tasks show consistent performance improvements with slight computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* This paper is well-organized and easy to follow.  \n* The method is validated across diverse architectures and tasks, consistent improvements are reported across multiple benchmarks (ImageNet-1K, ADE20K, COCO), with slight parameter and FLOP increases."}, "weaknesses": {"value": "* Lack of insight. The paper focuses heavily on empirical validation but offers limited insight (e.g., theoretical grounding or intuitive explanation) for why logit-level gating works. Specifically,\n  - Why cannot the attention logits itself learn the scoring and an extra gating module is necessary for that? How do you define \"model capacity\" (Sec. 5) and why the designed module increases it for, and only for small models?\n  - Could the performance gain simply comes from extra parameters or FLOPs, instead of the concrete design of the gating logic?\n\n* The design of Gating Modulation Layer (GML) is redundant and arbitrary. Sequentially applying linear projection, splitting and elementwise multiplication should be simply equivalent to a square activation function, why do you make it so complex?"}, "questions": {"value": "1. How does PLuG interact with other attention optimizations like FlashAttention or sparse attention—are there synergies or conflicts? \n2. Could the authors elaborate on why PLuG performs poorly when applied to decoder self-attention (Table 7), and whether this limitation could be mitigated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9aB3xHDABn", "forum": "OgVBUtsYKo", "replyto": "OgVBUtsYKo", "signatures": ["ICLR.cc/2026/Conference/Submission18730/Reviewer_CV4G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18730/Reviewer_CV4G"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760619023976, "cdate": 1760619023976, "tmdate": 1762928438883, "mdate": 1762928438883, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "8xWDxi2jNz", "forum": "OgVBUtsYKo", "replyto": "OgVBUtsYKo", "signatures": ["ICLR.cc/2026/Conference/Submission18730/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18730/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763031022485, "cdate": 1763031022485, "tmdate": 1763031022485, "mdate": 1763031022485, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an enhanced attention mechanism called PLuG-Attention (Pairwise Logit Gating). By introducing a learnable gating mechanism, PLuG modulates the attention logits of every token pair prior to softmax normalization, enabling the model to selectively amplify informative interactions and suppress spurious interactions, thereby improving its ability to capture spatial and semantic relationships. The paper demonstrates performance improvements across various architectures and tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. PluG has a small number of parameters and computational complexity, and barely increases inference time. \n2. PluG can simultaneously enhance and suppress interactions between tokens. \n3. PluG is applicable to various attention mechanisms and tasks."}, "weaknesses": {"value": "1. Although the paper compares with other methods in the appendix, it lacks quantitative results, which is very important for verifying the effectiveness of the method compared with previous work.\n2. The method appears to achieve performance gains only on smaller-scale models. Since small models are inherently lightweight, the benefit of employing a lightweight attention enhancement mechanism on them is also limited."}, "questions": {"value": "1. Have the authors attempted to conduct experiments on models with more than 200M parameters? I would like to know whether performance gains are still achieved when the model scale is larger.\n\n2. I don't fully understand the mechanism behind the Gating Modulation Layer (GML). Why is the raw gating matrix projected into two factors that are then multiplied? Can the authors provide more intuition or explanation beyond just the ablation study results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "81sm3YIHUT", "forum": "OgVBUtsYKo", "replyto": "OgVBUtsYKo", "signatures": ["ICLR.cc/2026/Conference/Submission18730/Reviewer_J3q1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18730/Reviewer_J3q1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760938185488, "cdate": 1760938185488, "tmdate": 1762928438345, "mdate": 1762928438345, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PLuG Attention, a lightweight gating mechanism that modulates pre-softmax attention logits with a learned token-pair coefficient matrix. The design is simple, integrates seamlessly into ViTs, Mask2Former, and Deformable DETR, and achieves small but consistent improvements across classification, segmentation, and detection benchmarks."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Very clear and easy-to-implement mechanism.\n2. Consistent improvements across multiple vision tasks.\n3. Solid ablation studies and visual analyses that help explain the effect.\n4. Pseudocode and mathematical description are easy to follow."}, "weaknesses": {"value": "1. Novelty is limited. The method is closely related to existing gated attention variants and the conceptual overlap is strong. Importantly, the paper does not cite or compare against DGSA (Differential Gated Self-Attention) where already introduced input-dependent pairwise gating, but applied post-softmax to fuse excitatory/inhibitory maps. PLuG differs mainly in where the gating occurs and at the usage of single softmax branch. FoX introduces additive, input-dependent pre-softmax biases (suppressive only). PLuG’s multiplicative form can be seen as a proportional bias term AijGij. Overall, PLuG feels like another variant in a well-explored space rather than a fundamentally new mechanism.\n2. Modest empirical gains, possibly within variance. The gains are relatively modest (+0.3–0.5% top-1 on ImageNet, +0.2–0.4 AP in DETR). The paper does report that each configuration was run three times and averaged. However, no standard deviations or confidence intervals are given. For small deltas (e.g. +0.1 mIoU), it is impossible to judge if they are meaningful. Reporting only the mean hides the possible overlap between baseline and PLuG runs. Improvements diminish with larger backbones.\n3. No head-to-head comparisons with closest prior work. While Talking-Heads, ConVit, and FoX are mentioned, there are no direct baselines under the same settings. Without such comparisons, it is difficult to judge if PLuG is truly more effective than existing input-dependent gating approaches.\n4. Unanalyzed failure cases. Applying PLuG to Deformable DETR decoder self-attention reduces AP, but no explanation is provided. This undermines the claim of “universality” and suggests limits that deserve deeper analysis."}, "questions": {"value": "1. How do you formally distinguish PLuG from DGSA (Differential Gated Self-Attention), which also applies input-dependent pairwise gating (albeit post-softmax with excitatory/inhibitory maps)? Could you elaborate on what fundamentally new modeling capacity PLuG enables beyond simply moving the gate pre-softmax and apply on a single softmax attention channel? \n2. Can you clarify whether your proportional bias view A'ij=Aij+AijGij is essentially equivalent to FoX with a logit-dependent bias? If not, what key expressive differences make PLuG more powerful?\n3. How does PLuG compare empirically to DGSA, FoX, or ConVit in the same backbones? Since these are the most closely related mechanisms, empirical head-to-head evaluation would greatly clarify PLuG’s contribution. \nWhy does PLuG hurt performance in decoder self-attention? Can you provide an analysis of this failure (e.g., attention distribution sharpness, redundancy, or training instability)?\nGiven that performance gains diminish or vanish for larger backbones, do you see PLuG as mainly a small-model regularizer, or is there an intuition why scalability is limited?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kM8FKW94LF", "forum": "OgVBUtsYKo", "replyto": "OgVBUtsYKo", "signatures": ["ICLR.cc/2026/Conference/Submission18730/Reviewer_b9GB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18730/Reviewer_b9GB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756748971, "cdate": 1761756748971, "tmdate": 1762928437384, "mdate": 1762928437384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PLuG-Attention, a plug-in mechanism that introduces Pairwise Logit Gating to modulate attention logits before the softmax operation. The method learns a token-pair-specific gating coefficient matrix that selectively amplifies informative query–key interactions and suppresses spurious ones, aiming to enhance representational expressiveness and interpretability. PLuG is simple to implement, architecture-agnostic, and evaluated across a wide range of vision transformer architectures. Experiments show consistent performance improvements with minimal parameter and FLOP overhead."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple and Generalizable Design: PLuG can be easily integrated into existing architectures without redesign or tuning.\n\n2. Broad Empirical Validation: Comprehensive experiments on classification, detection, and segmentation tasks confirm consistent gains.\n\n3. Interpretability: Visualization and CKA analyses effectively demonstrate how PLuG modulates attention structure and focuses on semantically meaningful regions.\n\n4. Clear written and visualization.\n\n5. Practicality: The approach achieves improvements with negligible computational overhead, making it appealing for deployment."}, "weaknesses": {"value": "1. Lack of Theoretical Foundation: The method is largely empirical, with limited theoretical justification for why pairwise gating enhances expressiveness or stability.\n\n2. Limited Scaling and Generalization Analysis: The method is only tested on moderate-scale models; it’s unclear if gains persist at very large scales\n\n3. Potential Redundancy: The added gating projections may overlap functionally with existing attention head mechanisms or MLP mixing."}, "questions": {"value": "1. The baselines reported in Table 1 appear somewhat outdated; including comparisons with more recent vision backbones would strengthen the empirical evaluation.\n\n2. It would be interesting to know whether PLuG can be applied to linear attention variants, such as PolaFormer [1] or FLatten Transformer [2], and if similar gains can be observed there.\n\n3. The paper would benefit from additional analysis or intuition showing how the gating mechanism affects gradient flow or attention entropy, to better justify its stabilizing effect.\n\n4. Finally, clarification is needed on whether PLuG introduces any training instability or noticeable memory overhead when applied to high-resolution or large-scale vision tasks.\n\n\nThis is an interesting work and I will raise my score if authors address all my concerns.\n\n[1] Meng, W., Luo, Y., Li, X., Jiang, D. and Zhang, Z., 2025. PolaFormer: Polarity-aware linear attention for vision transformers. arXiv preprint arXiv:2501.15061.\n\n[2] Han, Dongchen, et al. \"Flatten transformer: Vision transformer using focused linear attention.\" Proceedings of the IEEE/CVF international conference on computer vision. 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9fTnbsAE9X", "forum": "OgVBUtsYKo", "replyto": "OgVBUtsYKo", "signatures": ["ICLR.cc/2026/Conference/Submission18730/Reviewer_8rjb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18730/Reviewer_8rjb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928212461, "cdate": 1761928212461, "tmdate": 1762928436862, "mdate": 1762928436862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PLuG (Pairwise Logit Gating) attention, a simple plug-in mechanism that operates before the softmax and at the token-pair level, modulating the attention logits with a learnable gating matrix. The gate is produced from separate query/key projections (Q₍gate₎, K₍gate₎), passed through a lightweight Gating Modulation Layer (GML), and finally applied multiplicatively so that the original attention is preserved and only amplified/suppressed where needed. The method is dropped into several vision backbones (DeiT, TinyViT, XCiT, PVT, GCViT, Mask2Former, Deformable DETR) and consistently improves performance, especially on small/medium models, with very small parameter/FLOPs overhead. The paper is very well written, the related work is rich and up-to-date, and the qualitative visualizations make the idea easy to follow."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. **Novel but simple idea.** Logit-level, token-pair gating is an under-explored spot between “talking heads” / row-wise temperatures and post-attention gating. Here it is done in the minimal way (extra Q/K, small GML, residual 1+G) and is easy to implement.\n2. **Plug-in nature.** Authors actually show it on ViTs, Mask2Former, and Deformable DETR without surgery in the rest of the model, which supports the claim that it is a general mechanism and not tailored to one backbone.\n3. **Clear writing + good figures.** The mechanism is explained in Section 3 with a clean decomposition (main path vs gating path vs GML) and the figures (Fig. 1–3, 6–7) really help.\n4. **Solid experimental section.** ImageNet-1K classification across many small ViT-style models, ADE20K segmentation with Mask2Former, COCO detection with Deformable DETR, plus thorough ablations on gating dimension, GML variants, activations, head-specific vs shared gating. This is above average for this line of work.\n5. **Honest limitations.** They themselves say gains are more pronounced on small models and become modest as capacity increases (Sec. 5). This matches the results tables."}, "weaknesses": {"value": "1. **Effect mainly on small models.** This is already acknowledged, but it means the contribution is more about expressivity for small vision transformers than about scaling to very large models.\n2. **Attention analysis is mostly qualitative.** Fig. 7 and the rollout maps are nice, and the CKA analysis (Fig. 6) shows reduced redundancy in mid-layers, but it stays at a descriptive level. This is exactly where one extra quantitative experiment would strengthen the story.\n3. **Broadcasted gate across heads.** They justify why head-shared is better (Table 6), but this also means we don’t fully see whether PLuG creates complementary heads or just reshapes a shared pattern. A small quantitative probe could clarify that."}, "questions": {"value": "- Right now, the paper gives qualitative attention maps (e.g. Fig. 7) but not a quantitative look at how PLuG changes attention diversity across heads. How complementary are the attention maps of the PLuG-augmented model compared to the original model? A simple way to do this is to borrow the complementarity / head-diversity metric from [1] and present the results.\n\n[1] Psomas, Bill, et al. “Attention, Please! Revisiting Attentive Probing for Masked Image Modeling.” arXiv:2506.10178 (2025).\n\nMoreover:\n- Take a small model (e.g. DeiT-S with and without PLuG)\n- For a fixed validation subset (ImageNet-1K val is fine), compute attention maps for all heads in selected layers\n- Compute the complementarity / dissimilarity between heads within the baseline and within the PLuG model, and also the cross-model comparison (baseline head vs PLuG head).\n- Report a small table showing whether PLuG increases head diversity or, alternatively, whether it re-organizes attention compared to the original model.\n\n---\n\nOverall recommendation: Accept\n\nJustification: Method is novel yet simple, clearly written, broadly validated across tasks, and authors are transparent about limitations. A tiny quantitative attention experiment would polish the paper and provide some insights."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SGEztHNINa", "forum": "OgVBUtsYKo", "replyto": "OgVBUtsYKo", "signatures": ["ICLR.cc/2026/Conference/Submission18730/Reviewer_udxS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18730/Reviewer_udxS"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18730/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939456900, "cdate": 1761939456900, "tmdate": 1762928436212, "mdate": 1762928436212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}