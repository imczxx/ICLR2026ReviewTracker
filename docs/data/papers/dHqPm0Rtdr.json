{"id": "dHqPm0Rtdr", "number": 13466, "cdate": 1758218234443, "mdate": 1763619378010, "content": {"title": "When Does Multimodality Lead to Better Time Series Forecasting?", "abstract": "Recently, there has been growing interest in incorporating textual information into foundation models for time series forecasting. However, it remains unclear whether and under what conditions such multimodal integration consistently yields gains. We systematically investigate these questions across a diverse benchmark of 16 forecasting tasks spanning 7 domains, including health, environment, and economics. We evaluate two popular multimodal forecasting paradigms: aligning-based methods, which align time series and text representations; and prompting-based methods, which directly prompt large language models for forecasting. Our findings reveal that the benefits of multimodality are highly condition-dependent. While we confirm reported gains in some settings, these improvements are not universal across datasets or models. To move beyond empirical observations, we disentangle the effects of model architectural properties and data characteristics, drawing data-agnostic insights that generalize across domains. Our findings highlight that on the modeling side, incorporating text information is most helpful given (1) high-capacity text models, (2) comparatively weaker time series models, and (3) appropriate aligning strategies. On the data side, performance gains are more likely when (4) sufficient training data is available and (5) the text offers complementary predictive signal beyond what is already captured from the time series alone. Our study offers a rigorous, quantitative foundation for understanding when multimodality can be expected to aid forecasting tasks, and reveals that its benefits are neither universal nor always aligned with intuition.", "tldr": "We identify when multimodal text information improves time series forecasting from both modeling and data perspectives.", "keywords": ["multimodal time series forecasting"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f1b5511d3814d96f18046ac502513062b315f635.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates whether and under what conditions the textual modality benefits time series forecasting. Aligning-based methods and prompting-based methods are summarized for experiments. Experimental results reveal that the benefits of textual modality are highly condition-dependent for time series forecasting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper explores the contribution of textual modality and large language models (LLMs) to time series forecasting, with particular attention to the effect of model size.\n\n2. The experimental evaluation is extensive and well-organized.\n\n3. The paper covers a wide range of domains, enhancing the comprehensiveness of the analysis."}, "weaknesses": {"value": "1. The existing work CM2TS [1] has already investigated similar questions regarding cross-modality modeling for time series. However, this paper does not provide a proper citation or discussion to clarify how its contributions differ from or extend CM2TS.\n\n2. Only text/language is involved as an external modality. Thus, this study is a dual-modality or cross-modality analysis, rather than a multimodality analysis. Please ensure the authenticity.\n\n3. Some experimental results (e.g., Sections 4.2 and 4.5) are interesting but appear to depend heavily on hyperparameter configurations of methods. As such, the observed improvements may correspond to local optima. It would be better to combine the theoretical proof and the absolutely global optimal experimental results to verify your assumption.\n\n4. The code and implementation details are not available, which limits reproducibility and independent validation of the findings.\n\n[1] Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era, IJCAI 2025."}, "questions": {"value": "1. It is difficult to understand Table 1. How are PatchTST, DLinear, and Chronos aligned with LLMs? Which alignment strategies were applied in each case?\n\n2. For each LLM-based method, have you fine-tuned the best hyperparameters individually, or did you use a unified setting across models? Please clarify the procedure.\n\n3. The paper discusses the benefit of multimodality, but it seems that CM2TS [1] has already explored a similar topic. Could the authors clarify how this work differs from or extends the contributions of CM2TS?\n\n[1] Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era, IJCAI 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aSe4O0smqy", "forum": "dHqPm0Rtdr", "replyto": "dHqPm0Rtdr", "signatures": ["ICLR.cc/2026/Conference/Submission13466/Reviewer_GVvr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13466/Reviewer_GVvr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761375120732, "cdate": 1761375120732, "tmdate": 1762924085031, "mdate": 1762924085031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We appreciate all reviewers for the feedback. We are glad that reviewers find our **experiments and analysis comprehensive** (**KtuZ, v77y, JcvS, GVvr**), our **writing clear** (**KtuZ, v77y**) and our **findings insightful** (**KtuZ, v77y, JcvS**). We have addressed reviewers’ concerns in separate responses, and the major ones are summarized as follows.\n\n* We tested with **image modality** (TS+image and TS+text+image) which confirmed our core findings;\n* We added experiments for another recent paradigm based on **code generation**, and found it performs worse than aligning-based paradigm;\n* We added the PixelRec dataset for **ultra-long time series**, confirming consistent results on extreme lengths;\n* We added experiments using LLM as a judge, trying to **quantify the correlations** between text and time series; \n* We **clarified that our definition** of multimodal forecasting follows the **established convention** in the time series community; \n* We added experiments using **vision language model** which confirmed our core findings."}}, "id": "gvSI0FuHrs", "forum": "dHqPm0Rtdr", "replyto": "dHqPm0Rtdr", "signatures": ["ICLR.cc/2026/Conference/Submission13466/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13466/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13466/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763596761166, "cdate": 1763596761166, "tmdate": 1763596811256, "mdate": 1763596811256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors evaluate various existing models for multimodal time series (MMTS) forecasting across 16 datasets under two paradigms: alignment-based and prompt-based methods. The paper provides detailed analyses that can inform the broader understanding of this research field. However, the most critical aspect—namely, the definition of “multimodal time series” -- is not clearly explained."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors evaluate multiple existing methods using two pipelines (alignment-based and prompt-based) across 16 diverse datasets.\n- The paper provides a comprehensive analysis of different modeling strategies and offers detailed experimental results and insights."}, "weaknesses": {"value": "### 1. **The Definition of “Multimodal” Time Series**\n\nI am very concerned about the formulation of **multimodal** time series in this paper.\n\nIn Lines 034–044, the authors categorize six “MMTS” methods into two types: (I) alignment-based and (II) prompt-based. \n\nHowever, I believe that **multimodal learning inherently implies semantic alignment** between modalities. For instance, I think it difficult to perceive any semantic alignment between numerical time series data and a textual statement such as *“Tomorrow there will be a meeting between the US and Canada.”* These two sources do not describe the same underlying content; rather, they convey fundamentally different types of information. Hence, such data and methods are better described as **multi-source** or **multi-factor** forecasting approaches, rather than traditional multimodal learning.\n\n**Speech–text** is a good example to illustrate what true semantic alignment means. The alignment between modalities presupposes that they represent the *same underlying content* (e.g., the same utterance in two modalities) rather than *different kinds of information*. In contrast, most time series language models, e.g., [4],  do not share this semantic correspondence. \n\nFor multimodal time series (MMTS), recent works [1, 2, 3] have explored more semantically grounded formulations by transforming raw time series into frequency, visual, or textual representations—e.g., spectrograms or pattern images—and learning alignment in these shared spaces. These directions capture the essence of multimodality much better.\n\nIf the authors intend to redefine or extend the concept of MMTS, I strongly suggest that they **explicitly discuss what constitutes a multimodal time series and what does not**. As it stands, I disagree with the paper’s implicit definition of “multimodal time series,” which appears more closely related to multi-source or multi-factor data integration rather than genuine multimodal learning.\n\n\n[1] Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting\n\n[2] Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives\n\n[3] GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images\n\n[4] One Fits All: Power General Time Series Analysis by Pretrained LM\n\n### 2. **Benchmark**\n\nConsidering the conceptual gap between conventional multimodal learning and the “MMTS” formulation in this paper, I believe it is necessary for the authors to provide a more comprehensive clarification of the semantic relationship between time series and textual modalities in the chosen datasets. Moreover, recent studies have demonstrated that vision-language models (VLMs) can effectively comprehend and reason over time-series data, highlighting the importance of discussing these conceptual distinctions in greater depth."}, "questions": {"value": "For my question, please refer to the “Weaknesses” section.\n\nAdditionally, although the authors provide extensive experimental details in Appendices A, B, and C, following ICLR’s reproducibility guidelines, it is recommended that the authors include a dedicated “Reproducibility Statement” section before the References."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mBE29n2DuN", "forum": "dHqPm0Rtdr", "replyto": "dHqPm0Rtdr", "signatures": ["ICLR.cc/2026/Conference/Submission13466/Reviewer_JcvS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13466/Reviewer_JcvS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931766544, "cdate": 1761931766544, "tmdate": 1762924084684, "mdate": 1762924084684, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work systematically addresses the fundamental question of when and why multimodality improves forecasting, from both model and data perspectives. By evaluating 16 benchmarks and two key paradigms (alignment-based and prompting-based), the study offers essential insights into choosing encoders and fusion strategies for building powerful multimodal forecasting models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research is underpinned by a clearly defined and compelling motivation.\n\n2.  The work itself is pioneering and addresses a problem of considerable importance.\n\n3. The article is exceptionally well-written, and the experimental section is systematically conducted, with results presented in a clear and convincing manner."}, "weaknesses": {"value": "Please refer to the **Questions**."}, "questions": {"value": "The paper validates through synthetic and real-world datasets: MMTS is effective only when the text provides complementary predictive signals not contained in the time series. However, this conclusion is based on static evaluation scenarios (i.e., the \"complementarity\" between text and time series in experiments is fixed, such as whether the text contains trend shift information in synthetic data being a predefined condition). In real-world scenarios, the complementarity of text often changes dynamically (for example, in economic time series forecasting, a piece of news may contain complementary information before a policy is released but becomes redundant after the policy is implemented; in medical monitoring, the complementarity of clinical notes dynamically evolves with the patient's condition stability). Existing MMTS models all adopt fixed fusion strategies (such as fixed late fusion or early fusion) and are unable to determine in real time whether the text possesses complementarity and adjust the fusion intensity. Therefore, how can we design an MMTS model that can dynamically quantify the complementarity intensity between text and time series (e.g., based on metrics such as novelty in temporal patterns or semantic relevance of text) and adaptively switch fusion strategies (e.g., increasing text weight when complementarity is high, or reverting to unimodal mode when complementarity is low)? Can such a dynamic mechanism break through the performance ceiling of existing static fusion models in real-world scenarios (such as real-time economic forecasting or intensive care time series monitoring)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EBgjhgOiqu", "forum": "dHqPm0Rtdr", "replyto": "dHqPm0Rtdr", "signatures": ["ICLR.cc/2026/Conference/Submission13466/Reviewer_v77y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13466/Reviewer_v77y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969863416, "cdate": 1761969863416, "tmdate": 1762924084149, "mdate": 1762924084149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper \"When Does Multimodality Lead to Better Time Series Forecasting?\" conducts a systematic investigation into the effectiveness of multimodal time series (MMTS) forecasting by integrating textual information. The authors evaluate two dominant paradigms: alignment-based methods (fusing time series and text representations) and prompting-based methods (directly using LLMs for forecasting). Through a comprehensive benchmark spanning 16 datasets across 7 domains (e.g., health, economics), the study reveals that multimodal improvements are highly conditional and not universal. Key contributions include:\n\nDemonstrating that MMTS methods do not consistently outperform unimodal baselines, challenging common assumptions.\n\nProviding insights into how model capacity (e.g., text encoder size, time series model strength) and data characteristics (e.g., training data size, text complementarity) influence performance.\n\nOffering data-agnostic guidelines via controlled experiments, such as synthetic data analyses, to generalize findings beyond specific benchmarks.\n\nThe paper emphasizes that multimodality is most beneficial when text provides complementary signals not captured by time series alone, and it encourages more cautious, data-driven approaches in future MMTS research."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Quality: The experimental design is thorough, covering 16 datasets, multiple model families (e.g., Chronos, BERT, LLMs), and diverse alignment strategies. The synthetic data approach is particularly strong for isolating key variables.\n\nClarity: The writing is accessible, with clear explanations of methods and results. Visualizations (e.g., scatter plots showing performance trends) effectively communicate complex findings.\n\nSignificance: The paper provides actionable guidelines for researchers and practitioners, potentially reducing wasted effort on ineffective multimodal integrations. Its focus on data characteristics beyond model architecture broadens the impact."}, "weaknesses": {"value": "The study is limited to text and time series; excluding other modalities (e.g., images in retail forecasting) may reduce generalizability to broader multimodal settings.\n\nWhile datasets are diverse, they may not capture all real-world challenges (e.g., ultra-long sequences or low-resource domains). Including more extreme cases could strengthen the conclusions.\n\nThe evaluation of prompting-based methods relies on current LLMs (e.g., GPT-4, Claude), which evolve rapidly; however, this is mitigated by testing multiple models and versions.\n\n\nSome recent work addresses these problems by converting text into code or by involving human intervention to improve alignment (e.g., https://arxiv.org/abs/2505.15354, https://arxiv.org/pdf/2506.13705). We encourage future studies to demonstrate and benchmark such approaches as well."}, "questions": {"value": "How might the inclusion of other modalities (e.g., images or audio) affect the conclusions? Could the guidelines be extended to multimodal settings beyond text?\n\nCould the findings apply to streaming or online learning scenarios where data arrives incrementally?\n\nBased on the results, are there specific domain invariants (e.g., healthcare vs. finance) where multimodality is consistently beneficial or ineffective?\n\nSome recent work addresses these problems by converting text into code or by involving human intervention to improve alignment (e.g., https://arxiv.org/abs/2505.15354, https://arxiv.org/pdf/2506.13705). Please also include these kinds of methods—and any others that have been overlooked—to ensure a more complete and rigorous evaluation. I will raise additional points as further work and study are incorporated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GKJYFfhymx", "forum": "dHqPm0Rtdr", "replyto": "dHqPm0Rtdr", "signatures": ["ICLR.cc/2026/Conference/Submission13466/Reviewer_KtuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13466/Reviewer_KtuZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137666826, "cdate": 1762137666826, "tmdate": 1762924083759, "mdate": 1762924083759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper \"When Does Multimodality Lead to Better Time Series Forecasting?\" conducts a systematic investigation into the effectiveness of multimodal time series (MMTS) forecasting by integrating textual information. The authors evaluate two dominant paradigms: alignment-based methods (fusing time series and text representations) and prompting-based methods (directly using LLMs for forecasting). Through a comprehensive benchmark spanning 16 datasets across 7 domains (e.g., health, economics), the study reveals that multimodal improvements are highly conditional and not universal. Key contributions include:\n\nDemonstrating that MMTS methods do not consistently outperform unimodal baselines, challenging common assumptions.\n\nProviding insights into how model capacity (e.g., text encoder size, time series model strength) and data characteristics (e.g., training data size, text complementarity) influence performance.\n\nOffering data-agnostic guidelines via controlled experiments, such as synthetic data analyses, to generalize findings beyond specific benchmarks.\n\nThe paper emphasizes that multimodality is most beneficial when text provides complementary signals not captured by time series alone, and it encourages more cautious, data-driven approaches in future MMTS research."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Quality: The experimental design is thorough, covering 16 datasets, multiple model families (e.g., Chronos, BERT, LLMs), and diverse alignment strategies. The synthetic data approach is particularly strong for isolating key variables.\n\nClarity: The writing is accessible, with clear explanations of methods and results. Visualizations (e.g., scatter plots showing performance trends) effectively communicate complex findings.\n\nSignificance: The paper provides actionable guidelines for researchers and practitioners, potentially reducing wasted effort on ineffective multimodal integrations. Its focus on data characteristics beyond model architecture broadens the impact."}, "weaknesses": {"value": "The study is limited to text and time series; excluding other modalities (e.g., images in retail forecasting) may reduce generalizability to broader multimodal settings.\n\nWhile datasets are diverse, they may not capture all real-world challenges (e.g., ultra-long sequences or low-resource domains). Including more extreme cases could strengthen the conclusions.\n\nThe evaluation of prompting-based methods relies on current LLMs (e.g., GPT-4, Claude), which evolve rapidly; however, this is mitigated by testing multiple models and versions.\n\n\nSome recent work addresses these problems by converting text into code or by involving human intervention to improve alignment (e.g., https://arxiv.org/abs/2505.15354, https://arxiv.org/pdf/2506.13705). We encourage future studies to demonstrate and benchmark such approaches as well."}, "questions": {"value": "How might the inclusion of other modalities (e.g., images or audio) affect the conclusions? Could the guidelines be extended to multimodal settings beyond text?\n\nCould the findings apply to streaming or online learning scenarios where data arrives incrementally?\n\nBased on the results, are there specific domain invariants (e.g., healthcare vs. finance) where multimodality is consistently beneficial or ineffective?\n\nSome recent work addresses these problems by converting text into code or by involving human intervention to improve alignment (e.g., https://arxiv.org/abs/2505.15354, https://arxiv.org/pdf/2506.13705). Please also include these kinds of methods—and any others that have been overlooked—to ensure a more complete and rigorous evaluation. I will raise additional points as further work and study are incorporated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GKJYFfhymx", "forum": "dHqPm0Rtdr", "replyto": "dHqPm0Rtdr", "signatures": ["ICLR.cc/2026/Conference/Submission13466/Reviewer_KtuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13466/Reviewer_KtuZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13466/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137666826, "cdate": 1762137666826, "tmdate": 1763691317894, "mdate": 1763691317894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}