{"id": "v23Pqcm6qp", "number": 8298, "cdate": 1758077883806, "mdate": 1759897793566, "content": {"title": "SumRA: Parameter Efficient Fine-tuning with Singular Value Decomposition and Summed Orthogonal Basis", "abstract": "Parameter-efficient fine-tuning (PEFT) aims to adapt large pretrained speech models using fewer trainable parameters while maintaining performance. Low-Rank Adaptation (LoRA) achieves this by decomposing weight updates into two low-rank matrices, $A$ and $B$, such that $W'=W_0+BA$. Previous studies showed that freezing $A$ and only updating $B$ can reduce trainable parameters and achieve performance close to standard LoRA, where $A$ is initialized using the principal singular vectors of $W_0$ obtained via singular value decomposition (SVD). However, because $A$ is typically initialized with only the leading singular vectors, its representation capacity is confined to a narrow subspace of the model’s knowledge. To overcome this limitation, we propose SumRA, which initializes each row of $A$ as a sum of multiple singular vectors chosen from beyond the leading components, thereby enabling $A$ to influence a larger portion of the model’s knowledge space. Experiments on multilingual automatic speech recognition (ASR) tasks show that by adapting Whisper to five new languages from Common Voice with only 10 hours of data each, our method improves word error rate from 14.42\\% to 12.41\\% over LoRA baselines while using 50\\% less trainable parameters.", "tldr": "The paper introduces SumRA, a new PEFT method that expands LoRA’s representational capacity by initializing low-rank matrices with sums of multiple singular vectors instead of only the leading ones.", "keywords": ["low rank adaptation", "automatic speech recognition", "model adaptation", "parameter efficient fine tuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/651c24107980f38a829eab58a1346298178e0345.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SumRA, a parameter-efficient fine-tuning (PEFT) method that improves upon PiSSA by initializing each row of the LoRA matrix A as a sum of multiple singular vectors from the SVD of pretrained weights, rather than using only the leading singular vectors. The key insight is that PiSSA's use of only top-r singular vectors restricts adaptation to a narrow knowledge subspace. By summing multiple singular vectors per row (with proposed interleave-sum and greedy-sum strategies), SumRA aims to influence a broader portion of the model's knowledge space. The method is evaluated on multilingual ASR tasks using Whisper, showing improvements over LoRA baselines while using 50% fewer trainable parameters by freezing matrix A."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear motivation**: The limitation of PiSSA using only top singular vectors is well-articulated, and the proposed solution is intuitive.\n\n2. **Comprehensive baselines**: The paper compares against multiple LoRA variants (LoRA, LoRA-FA, PiSSA, CorDA, DoRA, VeRA).\n\n3. **Consistent improvements**: Results show consistent WER improvements across multiple languages and model sizes (up to 16% relative improvement).\n\n4. **Mathematical rigor**: The proof of greedy sum optimality (Appendix A.1) is mathematically sound, even if its connection to performance is unclear.\n\n5. **Reproducibility**: Training details and hyperparameters are clearly specified."}, "weaknesses": {"value": "1. **Weak theoretical foundation**: The paper lacks evidence that singular vectors encode vocabulary-specific knowledge. This is the central premise but is not validated. The authors should:\n   - Provide empirical analysis of what different singular vectors represent\n   - Show correlation between singular vector indices and vocabulary subsets\n   - Validate the \"concept encoding\" hypothesis with interpretability experiments\n\n2. **Incomplete experimental analysis**:\n   - No analysis of which singular vectors are being summed and whether they truly represent complementary knowledge\n   - Missing ablation: What if A is trained instead of frozen?\n   - No visualization of learned representations to validate the broader knowledge influence claim\n   - Limited to speech; NLP experiments needed to claim generality\n\n3. **Unfair comparisons**: \n   - SumRA uses SVD initialization while baseline LoRA uses random initialization\n   - Should compare against: (a) PiSSA with frozen A, (b) SumRA with trained A\n   - Storage cost comparison (Figure 4) is misleading—the savings come from freezing A, not from summing\n\n4. **Statistical significance**: No error bars, confidence intervals, or multiple runs reported. With improvements sometimes <1% WER, statistical testing is essential.\n\n5. **Limited scope**: \n   - Only evaluated on low-resource (10h) settings; unclear if benefits hold with more data\n   - Only one model architecture family\n   - No analysis of computational cost during training/inference\n\n6. **Interference analysis missing**: The paper mentions \"destructive interference\" between summed vectors but provides no quantitative analysis of this phenomenon or how much information is actually lost."}, "questions": {"value": "1. What happens if you train A instead of freezing it? Is the improvement from initialization or from freezing?\n\n2. Can you provide empirical evidence that different singular vectors encode different vocabulary subsets in speech models?\n\n3. How does performance scale with the amount of training data (10h vs. 100h vs. full data)?\n\n4. What is the computational overhead of SVD during initialization, especially for large models?\n\n5. How do you choose which singular vectors to sum? Is there an automatic way to determine optimal subsets?\n\n6. Can you show t-SNE or other visualizations demonstrating that SumRA representations cover broader knowledge space than PiSSA?\n\n7. Why does greedy sum sometimes perform worse than interleave sum (Table 3, languages mhr and kmr)?\n\n8. How sensitive is the method to the rank r? What happens with very small (r=1) or very large (r=64) ranks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J8lIprXNj8", "forum": "v23Pqcm6qp", "replyto": "v23Pqcm6qp", "signatures": ["ICLR.cc/2026/Conference/Submission8298/Reviewer_VaDY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8298/Reviewer_VaDY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761207363459, "cdate": 1761207363459, "tmdate": 1762920227162, "mdate": 1762920227162, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper can be seen as an upgrade of PiSSA and LoRA.\nTo address the issue in PiSSA that “A is typically initialized with only the leading singular vectors, which limits its representational capacity to a narrow subspace of the model’s knowledge,”\nthe paper proposes SumRA, which initializes each row of A as a sum of multiple singular vectors selected beyond the leading components, while B is initialized to zero, as in LoRA.\nExperimental results on multilingual automatic speech recognition tasks show that SumRA outperforms many state-of-the-art fine-tuning methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of initializing each row of A as a sum of multiple singular vectors chosen from beyond the leading components—thereby enabling A to influence a larger portion of the model’s knowledge space—is quite intuitive.\n\n2. The writing of the paper is very fluent and easy to read.\n\n3. In terms of experimental results, SumRA achieves significant improvements compared to various other methods."}, "weaknesses": {"value": "1. This paper conducts experiments only on ASR tasks using the Whisper series of models.\nHowever, the compared methods such as LoRA, DoRA, PiSSA, and CorDA mostly focus on fine-tuning large language models (LLMs).\nTherefore, it is necessary to validate the effectiveness of SumRA on LLMs to enhance the credibility of the experimental results.\n\n2. Although the motivation behind SumRA is quite intuitive, the paper does not sufficiently reveal the underlying mechanism that explains why it works.\nIt would be helpful to include some theoretical justification or experimental analysis to support the claim that\ninitializing each row of A as a sum of multiple singular vectors chosen from beyond the leading components\nindeed enables A to influence a larger portion of the model’s knowledge space.\n\n3. In Figure 1, the depiction of PiSSA’s initialization of W_0 is incorrect; this initialization would break the model’s capability at the start."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9FBzFTghOX", "forum": "v23Pqcm6qp", "replyto": "v23Pqcm6qp", "signatures": ["ICLR.cc/2026/Conference/Submission8298/Reviewer_Q2k7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8298/Reviewer_Q2k7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761883147466, "cdate": 1761883147466, "tmdate": 1762920226778, "mdate": 1762920226778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SumRA, a parameter-efficient fine-tuning method that improves upon PiSSA by initializing each row of the LoRA A matrix as a sum of multiple singular vectors from the SVD decomposition of pretrained weights, rather than using only the top-r singular vectors. The authors freeze matrix A during training and only update matrix B, reducing trainable parameters by 50% compared to standard LoRA. They introduce three summation strategies (block sum, interleave sum, and greedy sum) to distribute singular vectors across rows while minimizing interference between important components. Experiments on multilingual ASR adaptation using Whisper show that SumRA achieves 12-16% relative WER improvement over LoRA baselines across five low-resource languages with only 10 hours of training data each, while using half the trainable parameters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated approach with clear theoretical foundation. The paper provides strong intuition for why summing multiple singular vectors enables broader knowledge coverage compared to PiSSA's approach of using only top-r vectors (Figure 2D and Section 3.1). The connection to model averaging in Section 4 further strengthens the conceptual framework, and the formal proof of greedy sum optimality in Appendix A.1 adds mathematical rigor to the method.\n2. Comprehensive experimental validation with consistent improvements. The experiments systematically compare SumRA against multiple baselines (LoRA, LoRA-FA, VeRA, DoRA, PiSSA, CorDA) across two model sizes (Whisper-small and Whisper-large-v2), two ranks (2 and 32), and five languages. SumRA consistently outperforms all baselines, achieving improvements across nearly all experimental configurations while using 50% fewer trainable parameters, demonstrating both effectiveness and practical efficiency."}, "weaknesses": {"value": "1. Limited evaluation domains. The paper exclusively evaluates on multilingual ASR adaptation with only 10 hours of training data per language from Common Voice. This narrow experimental setting raises concerns about generalizability. No experiments are provided to validate this claim or explore other domains such as natural language understanding, vision tasks, or even ASR with different data scales (e.g., 100 hours, 1000 hours). The evaluation should be expanded to include at least one additional modality (e.g., text-based tasks using LLaMA or similar models) and different data regimes to demonstrate broader applicability.\n\n2. Incomplete comparison with related work. The paper lacks comparisons with several directly relevant PEFT methods and omits discussion of recent theoretical advances that would contextualize the contributions. Experimentally, the paper does not compare against AdaLoRA [1], which adaptively allocates ranks and could demonstrate whether fixed summation outperforms adaptive strategies, or LoRA+ [2], which improves LoRA through differential learning rates for A and B. Theoretically and methodologically, the paper misses several pertinent works: Hu et al. [3] establish computational limits of LoRA that could inform when summation strategies are beneficial; SORSA [4] uses orthonormal regularization of singular vectors, directly related to this work's orthogonality claims (lines 264-265); NdLinear [5] addresses preserving multi-dimensional structure in parameter-efficient methods, relevant to the claimed broader knowledge coverage; and recent theoretical foundations on fine-tuning limits [6,7] could strengthen the motivation for summing singular vectors. These omissions make it difficult to assess whether SumRA advances beyond current state-of-the-art or whether existing methods achieve similar benefits.\n\n3. Insufficient analysis of the summation mechanism. While the paper provides intuition about reducing \"destructive interference\" between singular vectors (lines 270-271, 283), there is no empirical analysis of what actually happens when vectors are summed. \n\n# Reference \n\n[1]: Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao: AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning. ICLR 2023.\n\n[2] Hayou, Soufiane, Nikhil Ghosh, Bin Yu: Lora+: Efficient low rank adaptation of large models. arXiv preprint arXiv:2402.12354.\n\n[3] Jerry Yao-Chieh Hu, Maojiang Su, En-Jui Kuo, Zhao Song, Han Liu:\nComputational Limits of Low-Rank Adaptation (LoRA) Fine-Tuning for Transformer Models. ICLR 2025.\n\n[4] Yang Cao, Zhao Song: SORSA: Singular Values and Orthonormal Regularized Singular Vectors Adaptation of Large Language Models. arXiv:2409.00055.\n\n[5] Alex Reneau, Jerry Yao-Chieh Hu, Zhongfang Zhuang, Ting-Chun Liu, Xiang He, Judah Goldfeder, Nadav Timor, Allen G Roush, Ravid Shwartz-Ziv: NdLinear: Preserving Multi-Dimensional Structure for Parameter-Efficient Neural Networks. arXiv:2503.17353.\n\n[6] Timothy Chu, Zhao Song, Chiwun Yang: Fine-tune Language Models to Approximate Unbiased In-context Learning. arXiv:2310.03331.\n\n[7] Jerry Yao-Chieh Hu, Wei-Po Wang, Ammar Gilani, Chenyang Li, Zhao Song, Han Liu: Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency. \tarXiv:2411.16525."}, "questions": {"value": "1. Have you conducted any preliminary experiments on text-based fine-tuning like instruction-tuning LLMs or vision tasks that you could share? \n\n2. Can you provide experimental evidence for this multi-task scenario? What is the actual performance when you train one shared A matrix with task-specific B matrices across all 5 languages compared to task-specific (A,B) pairs? \n\n3. Have you measured the actual information loss when summing singular vectors, for example by comparing reconstruction error?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ROakoBubjP", "forum": "v23Pqcm6qp", "replyto": "v23Pqcm6qp", "signatures": ["ICLR.cc/2026/Conference/Submission8298/Reviewer_ckmZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8298/Reviewer_ckmZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8298/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942300694, "cdate": 1761942300694, "tmdate": 1762920226498, "mdate": 1762920226498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}