{"id": "Z0DypVJKNj", "number": 5532, "cdate": 1757918258307, "mdate": 1763355539318, "content": {"title": "Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning", "abstract": "Most recently, Reinforcement Learning (RL) has empowered frontier Large Language Models (LLMs) to solve challenging math, science, and coding problems. This paper consentrates on RL on data without explicit labels for reasoning tasks in LLMs. The core challenge of the problem is reward estimation during inference in absense of ground-truth information. In this work, we propose COMPASS: Composite Path and Answer Self-Scoring -  a novel method for training LLMs using RL on unlabeled test data. COMPASS consists of Dual-Calibration Answer Reward (DCAR) and Decisive Path Reward (DPR), which enables self-evolution of LLMs by fully utilizing the priors in the pre-trained models as intrinsic rewards. We find that by simultaneously reinforcing the trustworthy consensus answers and chains of thought that yield high model desiciveness on its generated responses, the model improves its reasoning ability. Our experiments demonstrate that COMPASS consistently improves performance across a variety of tasks and models, marking a further step of learning from continuous streams of experience.", "tldr": "", "keywords": ["Test-Time Reinforcement Learning; Self-rewarding Mechanism; Process Reward; Outcome Reward"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/222818772fde1fb149cd8b520efc0cc9555ce2fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Based on TTRL, this paper further explores the direction of RLIF and proposes the COMPASS method. To address the reward design for reasoning tasks without labels, COMPASS introduces solutions from two dimensions: **outcome** and **process**.  \n\n1. **Outcome Reward**: It adopts **Confidence-Calibrated Self-Consistency**, where the *topkdiff* metric is used to characterize the model's confidence level. Additionally, **Credibility-Calibrated Pseudo-Labels** are applied to eliminate abnormal cases.  \n\n2. **Process Reward**: It employs the **Decisive-Path-Reward** to represent the value degree of each generated token.  \n\nFurthermore, small-scale experiments are conducted to verify that COMPASS outperforms the TTRL method, and additional ablation studies are also carried out to validate the effectiveness of each component in COMPASS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is composed in a clear and fluent style, making it easy to read and understand.\n2. The proposed method is simple and clearly defined, facilitating comprehension.\n3. The approach to designing rewards from both **outcome** and **process** perspectives shows a degree of innovation.\n4. The experimental results indicate that the method achieves observable performance improvements compared to TTRL."}, "weaknesses": {"value": "1. The paper presents numerous **hypotheses** throughout, yet lacks experimental validation or theoretical derivation to substantiate their validity;\n\n2. The connection between **Decisive-Path-Reward** and **process reward** appears somewhat tenuous—assigning rewards to tokens does not inherently qualify as a process reward mechanism;\n\n3. The experimental scope is limited to models up to **7B parameters**, which undermines the persuasiveness of the findings; additionally, the authors could strengthen their work by incorporating comparisons with *rubric-based* approaches or methods that leverage stronger models for evaluation.\n\nThe remaining points will be addressed in the **Questions** section."}, "questions": {"value": "1. Why should \"more confident responses contribute more significantly to the final decision\"? This claim requires substantiation through either theoretical derivation or empirical evidence;\n\n2. Why can **std of topk_diff** effectively characterize **confidence**? Tokens representing logical relationships inherently exhibit large topk-diff values; if such tokens appear more frequently, the resulting lower confidence seems counterintuitive and lacks theoretical justification;\n\n3. Why is \"a consensus derived from high-confidence responses more reliable than one based on diverse low-confidence outputs\"? This assertion necessitates validation through theoretical analysis or experimental results;\n\n4. Why is \"decisiveness more valuable during moments of high uncertainty\"? Please provide theoretical derivation or empirical evidence to support this claim;\n\n5. Why are comparisons with *rubric-based* approaches or methods leveraging stronger models for judgment absent? These alternatives should also be viable solutions to the RLIF problem;\n\n6. The ablation studies require more in-depth analysis. For instance, why does **Credibility-Calibrated Pseudo-Labels** yield almost no positive effect? Why does **Decisive-Path-Reward** contribute minimally to performance improvement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y5MDjZwyGc", "forum": "Z0DypVJKNj", "replyto": "Z0DypVJKNj", "signatures": ["ICLR.cc/2026/Conference/Submission5532/Reviewer_NWaM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5532/Reviewer_NWaM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761386692744, "cdate": 1761386692744, "tmdate": 1762918116928, "mdate": 1762918116928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "8CwTlYa6fE", "forum": "Z0DypVJKNj", "replyto": "Z0DypVJKNj", "signatures": ["ICLR.cc/2026/Conference/Submission5532/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5532/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763355538347, "cdate": 1763355538347, "tmdate": 1763355538347, "mdate": 1763355538347, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to solve two key problems in existing work, Test-Time Reinforcement Learning (TTRL), a setting for LLM self-evolution on unlabeled test data : 1) the fragility of pseudo-labels and sparse rewards from majority voting , and 2) the neglect of reasoning process quality. It introduces COMPASS, a composite reward mechanism that combines a \"Dual-Calibration Answer Reward\" (DCAR) for more robust outcome scoring and a \"Decisive Path Reward\" (DPR) to reward the reasoning journey itself. Experiments demonstrate that this approach generally improves performance over the TTRL baseline across several reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses the highly significant and challenging problem of LLM self-evolution using unlabeled data in a test-time setting.\n\n2. The proposed method is a direct and effective improvement over the TTRL baseline, thoughtfully addressing its key weaknesses in reward sparsity and process-agnosticism ."}, "weaknesses": {"value": "1. Fragility of the DPR Heuristic: The process reward (DPR) is shown to be brittle; it fails and degrades performance on the LLaMA3.2-1B model, as the paper admits its guiding heuristic (rewarding decisiveness in high-entropy states) reinforces \"fundamental confusion\" in less capable models .\n\n2. Extensive Reward Engineering: The method's success appears to rest on a complex combination of specific, fine-tuned heuristics (e.g., the exact formulas for confidence and credibility), which are not well-justified over simpler alternatives (see Question 1).\n\n3. Narrow Scope and Limited Comparison: The contribution feels more like an incremental \"patch\" for TTRL rather than a general method. The experiments narrowly compare only against this single baseline, failing to benchmark against other RLIF approaches (like RENT or EMPO) that also tackle self-rewarding.\n\nOverall, I am at a borderline. My main concern is whether the contribution is broad enough to be more than a \"patch for TTRL.\" Given the potential, I would consider raising my score after the rebuttal."}, "questions": {"value": "1. Confidence Metric Justification: Can you provide more details from the \"empirical analysis\" mentioned Line 220, that led to defining confidence as exp(standard deviation of topk diff)? Why was this specific metric chosen over simpler, more common ones like average sequence log-likelihood or the mean topk_diff? How sensitive is performance to this exact formulation?\n\n2. DPR vs. Entropy Minimization: Your DPR uses high entropy as a positive weight to reward decisiveness. This philosophy seems to directly contradict other RLIF work like RENT, which aims to minimize entropy (Line 134). Could you comment on this apparent contradiction and justify why your approach is superior to simply rewarding low-entropy (high-certainty) outputs overall?\n\n3. Computational Cost: The paper claims \"relatively low computational costs\" (Line 340); compared to what? It appears that this method will cost more compared to other test-time methods. Could you please quantify this additional cost relative to the TTRL baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G1E3qkPgSg", "forum": "Z0DypVJKNj", "replyto": "Z0DypVJKNj", "signatures": ["ICLR.cc/2026/Conference/Submission5532/Reviewer_cW87"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5532/Reviewer_cW87"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901821641, "cdate": 1761901821641, "tmdate": 1762918116515, "mdate": 1762918116515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes COMPASS, a method for training LLMs using RL on unlabeled test data. COMPASS introduces (1) Dual-Calibration Answer Reward (DCAR) to calibrate majority voting with confidence measures, and (2) Decisive Path Reward (DPR) to optimize the quality of the thought process. Specifically, DCAR uses a confidence-calibrated self-consistency score to obtain the pseudo-label, and further propose a metric to assess the credibility of this pseudo-label. DPR is specifically designed to evaluate the model’s reasoning pathway, encouraging decisive actions at critical junctures.\n\nThe authors claim DCAR enables the model to prioritize learning from high-credibility consensus, while DPR provides a direct and dense supervisory signal for optimizing the reasoning path."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The research on RL without explicit labels for reasoning is a promising direction.\n\n2. This paper attempts to address the fragility of pseudo-labels and evaluates the process quality, which is a good idea.\n\n3. The design of DPR is very interesting, but I believe it may lack some empirical evidence to support it.\n\n4. The results outperform baseline TTRL."}, "weaknesses": {"value": "I find the claim/method of this paper is not very convincing, and the evaluation is limited. I discuss relevant weaknesses below.\n### Method\nThe biggest problem is that the author makes many hypothetical claims without supporting empirical evidence or literature, which makes the argument unconvincing.\n\n1. In line 218 and 241, the authors claim that \"*We hypothesize that more confident responses should contribute more significantly to the final decision*\" and \"*Our underlying hypothesis is that a consensus derived from high-confidence responses is more reliable than one based on diverse low-confidence outputs*\".\nBut it's not clear whether this hypothesis applies to both simple and difficult questions. Moreover, some studies [1][2] figure out that LLMs may \"make mistakes with confidence\". I suggest providing evidence to support those hypotheses.\n\n2. In line 298, the authors claim that \"Our central hypothesis is that decisiveness is more valuable during moments of high uncertainty\". This also lacks some empirical evidence, and it's unclear whether it's related to the difficulty of questions. The author argues that tokens with higher certainty are better at this point and should be assigned a higher score. But will this impair the model's exploratory ability in RL and encourage overconfidence, which causes faster entropy collapse?\n\n### Experiments\n1. Concerns on baseline comparison. Only one baseline TTRL was compared. I suggest adding the comparison with supervised baselines to see the effectiveness of COMPASS, such as GRPO with ground truth labels.\n\n2. Concerns on evaluation benchmarks. The evaluation benchmarks are limited on math and physics, I suggest adding the results on general reasoning domains like MMLU-Pro and StrategyQA to see the effectiveness of the proposed method.\n\n2. Concerns on the experimental setting. Although this paper follows the pattern of Test Time Reinforcement Learning proposed by TTRL, I don't think it's reasonable. **Evaluating on the training set is hard for me to understand.** Recent studies reveal that there may be potential data contamination in the Qwen model of popular benchmarks like MATH and AMC [7]. Consequently, training and testing on the same contaminated benchmarks on Qwen2.5 series may be unreliable.\n\n3. Concerns on evaluation results. I notice some evaluation results are inconsistent with those provided by the official Qwen and Llama. For example, Qwen2.5-7B achieves 36.4 accuracy on GPQA [8], which is even better than the performance of COMPASS reported in this paper. Besides, the official performance of Llama3.2-1B-Instruct on MATH and GPQA is 30.6 and 27.2 respectively [9], which seems inconsistent with the reported 24.7 and 23.8 in the paper.\n\n### Presentation\nThe coordinate axes and fonts in Figure 2, 3 are small, which may affect the reading experience.\n\n### Missing References\nI suggest citing relevant works that also explore unsupervised reinforcement learning [3][4][5][6].\n\n---\n\n[1] Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models. arXiv preprint:2502.11028\n\n[2] Why Language Models Hallucinate. arXiv preprint:2509.04664\n\n[3] Learning to Reason without External Rewards. arXiv preprint:2505.19590\n\n[4] Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning. arXiv preprint:2506.08745\n\n[5] Reinforcing General Reasoning without Verifiers. arXiv preprint:2505.21493\n\n[6] Can Large Reasoning Models Self-Train? arXiv preprint:2505.21444\n\n[7] Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination. arXiv preprint arXiv:2507.10532\n\n[8] Qwen2.5 Technical Report. arXiv preprint:2412.15115\n\n[9] https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"}, "questions": {"value": "1. In Line 219, the authors claim \"Through empirical analysis, we found that the standard deviation of topk_diff across a generation trajectory correlates strongly with the correctness of the final answer\". I want to see the empirical analysis and evidence.\n\n2. In Eq. (3), how is $std_t$ calculated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TdzQ3EKEYE", "forum": "Z0DypVJKNj", "replyto": "Z0DypVJKNj", "signatures": ["ICLR.cc/2026/Conference/Submission5532/Reviewer_nvai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5532/Reviewer_nvai"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979577109, "cdate": 1761979577109, "tmdate": 1762918115941, "mdate": 1762918115941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The paper proposes COMPASS, a test-time reinforcement learning method that lets large language models self-improve on unlabeled data using intrinsic rewards.\n- It introduces two reward components: Dual-Calibration Answer Reward (DCAR) for more reliable pseudo-labels and Decisive Path Reward (DPR) for rewarding confident, high-quality reasoning steps.\n- Experiments show that COMPASS consistently outperforms prior test-time RL methods, e.g., TTRL, across multiple reasoning benchmarks and model sizes."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- COMPASS effectively enables self-improvement without labeled data, making reinforcement learning scalable to unlabeled reasoning tasks.\n- The proposed DCAR and DPR components are well-motivated and complementary, jointly addressing reward reliability and reasoning quality."}, "weaknesses": {"value": "- The method still depends on self-consistency assumptions, which can reinforce systematic model biases or errors.\n- The paper should include more challenging and diverse benchmarks to better demonstrate the generality of COMPASS.\n- Error bars or statistical significance tests are missing, making it hard to assess the reliability of reported improvements.\n- The performance gains over TTRL are relatively modest, raising questions about the practical impact of the proposed method.\n- The paper provides limited analysis of failure cases, which could clarify when and why the method underperforms."}, "questions": {"value": "- Can the authors clarify how the confidence and credibility metrics in DCAR interact when they disagree (for example, high confidence but low credibility cases)?\n- Do the intrinsic rewards correlate with ground-truth correctness when labels are available?\n- How does the method perform if the base model's confidence calibration is poor or systematically biased?\n- Could the authors report the variance or standard deviation across multiple runs to assess the reliability of the reported improvements?\n- What happens if the model generates reasoning paths of varying lengths?\n- In Figure 4, the performance curves of different methods appear very similar, suggesting that the claimed improvements may be marginal or not clearly demonstrated in this comparison.\n- In Line 16, consentrates -> concentrates\n- In Line 18, absense -> absence\n- In Line 24, desiciveness -> decisiveness\n- In Line 215, algorithm -> Algorithm\n- In Line 324, Specifically,we -> Specifically,(SPACE)we\n- Equation (9) is not presented in a standard mathematical form; the use of a large opening parenthesis makes the expression unclear.\n- In the caption of Figure 3, comparision -> comparison\n- The reference formatting and citation usage need improvement; the authors should update the BibTeX entries for accuracy and consistently use \\citep and \\citet.\n- Figures should use larger font sizes to improve readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There is no particular ethical concern."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSvmPwmkuY", "forum": "Z0DypVJKNj", "replyto": "Z0DypVJKNj", "signatures": ["ICLR.cc/2026/Conference/Submission5532/Reviewer_7VGZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5532/Reviewer_7VGZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5532/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762028666684, "cdate": 1762028666684, "tmdate": 1762918115284, "mdate": 1762918115284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}