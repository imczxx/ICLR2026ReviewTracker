{"id": "BNOz93d1UM", "number": 6092, "cdate": 1757952548922, "mdate": 1759897935796, "content": {"title": "HILDA: Hessian-Implicit Langevin with Damping and Adaptation for Diffusion Sampling", "abstract": "Diffusion models have achieved remarkable success in high-quality image generation through learning score functions of noise-corrupted data distributions. Contemporary sampling acceleration techniques predominantly focus on optimizing denoising trajectories along the temporal dimension, yet still rely on first-order Langevin dynamics for updates at individual noise levels. As the denoising process advances, curvature disparities along different principal directions of the target distribution become increasingly severe, resulting in pronounced anisotropic behavior. Methods that depend exclusively on first-order gradient information suffer from zigzag sampling trajectories in such regimes, thereby constraining effective step sizes and compromising sample quality. To address this limitation, we introduce HILDA—a training-free diffusion sampler that implicitly incorporates second-order geometric information at each noise level by employing Hessian-vector products combined with conjugate gradient methods to capture complete geometric information along coupled directions without explicitly constructing the Hessian matrix. To handle numerical ill-conditioning arising from strong anisotropy in later stages, we develop an adaptive damping coefficient λt based on condition number estimates and a spectral radius normalization factor ct, constructing a unified geometric operator Mt = ct(Ht + λtI)^(-1) that applies consistently to both drift and diffusion terms. HILDA functions as a plug-and-play geometric enhancement module that integrates seamlessly with existing ODE solvers, including DPM-Solver and UniPC. Experimental validation across multiple pre-trained diffusion models demonstrates that HILDA substantially mitigates zigzag artifacts and enhances both detail preservation and overall image quality under comparable or reduced sampling steps.", "tldr": "", "keywords": ["Diffusion Models", "Hessian-Vector Product", "Conjugate Gradient", "Training-free Sampling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2be1e27d7b87aa784ac293993e7cac4efdaed4ff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes HILDA, a training-free diffusion sampling framework that implicitly integrates second-order geometric information into diffusion samplers. By employing Hessian-vector products and conjugate gradient methods, HILDA approximates Hessian-guided Langevin dynamics without explicitly forming the Hessian matrix. The adaptive damping and spectral normalization components make the system numerically stable and efficient. Empirical results show consistent improvements in FID scores and smoother trajectories that mitigate the zigzag phenomenon during diffusion sampling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written and addresses an interesting direction by attempting to include geometric curvature information in diffusion models. The empirical results are solid and demonstrate clear quantitative and qualitative improvements, while the proposed method is computationally practical and can be plugged into existing samplers without retraining."}, "weaknesses": {"value": "1. The paper’s technical novelty is somewhat limited. The approach primarily combines well-known techniques HVP and CG within the diffusion framework, and the underlying idea of implicit preconditioning via second-order information has been studied in related optimization and dynamics contexts. Please correct me if I am wrong about this.\n\n2. Some statements look weird. See questions below."}, "questions": {"value": "1. At the bottom of page 1, the paper states that “the forward diffusion transforms from Gaussian to images,” which seems reversed since the forward process typically maps data to noise, and the reverse process reconstructs images from Gaussian noise. Could you clarify if this is an error or an intentional reinterpretation of the forward/reverse process?\n\n2. the paper attributes the severity of the zigzag phenomenon to the non-uniform curvature of the data manifold, but this explanation may not be fully accurate. Since $p_t$ ends to become smoother with increasing $t$, the zigzag behavior’s variation over timesteps cannot be entirely explained by manifold curvature. Could you provide a more rigorous justification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "uoUjZHaEEn", "forum": "BNOz93d1UM", "replyto": "BNOz93d1UM", "signatures": ["ICLR.cc/2026/Conference/Submission6092/Reviewer_FFUg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6092/Reviewer_FFUg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887065149, "cdate": 1761887065149, "tmdate": 1762918460052, "mdate": 1762918460052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HILDA a new method for accelerating sampling on diffusion models. They provide insights from preconditioned langevyn dynamics and observe that trajectories in diffusion models have a curved nature, which makes simulation less efficient. Such problem can be corrected by including second order information into the sampling. To this purpose, they introduce a preconditioner into the backwards diffusion process. They then introduce an algorithm for approximating the preconditioner by using automatic differentiation, the conjugate gradient and Lanczos algorithm. Their algorithm demonstrates good results when compared with other diffusion samplers without a significant extra overhead in computation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- They identify the \"zigzag\" phenomenon in diffusion \n- They propose a preconditioner $M_t = c_t ( H_t + \\lambda_t I)^{-1}$ to address the issue as well as natural algorithms to approximate $c_t, H_t, \\lambda_t$\n- They demonstrate benefits of their method when compared to standard baselines like DPMSolver"}, "weaknesses": {"value": "- The method adds new hyperparameters, $\\kappa_*$ as well as the number of iterations for Lanczos algorithm. When compared to other methods this adds an extra layer of complexity\n- In practice they leverage a frozen preconditioner that is independent of the position, but it is not clear how this is chosen, is it frozen for each $t$? If so, how does one compute it during sampling if it is independent of $x$\n- There is no proof that including this preconditioning keeps the terminal distribution the same as $p_0$, as diffusion is not exactly Langevin it does require a separate proof\n- All experimental results are done in a unconditional fashion, which limits the interpretability in the conditional setting. $M_t$ would require a different value for each condition. Could the authors provide some clarification and perhaps experimental validation on the conditional setting?\n- The base models seem to have very weak performance not up to date with modern standards. For instance, the unconditional CIFAR-10 results have FID of around 20-30 but EDM already has achieved FID of 2."}, "questions": {"value": "- It is unclear how despite the extra (m+k) differentiations HILDA still results in faster wall clocks than methods that don't have this overhead"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zwGrtzE8B7", "forum": "BNOz93d1UM", "replyto": "BNOz93d1UM", "signatures": ["ICLR.cc/2026/Conference/Submission6092/Reviewer_Ceja"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6092/Reviewer_Ceja"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941557207, "cdate": 1761941557207, "tmdate": 1762918459651, "mdate": 1762918459651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HILDA, a novel sampler for diffusion models. HILDA uses second-order information (implicit HVP) and conjugate gradient (CG) solver to derive a corrected sampling step. To ensure numerical stability, it uses some adaptive normalization schemes. The method can be applied to existing ODE solvers like DPM-Solver."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The primary contribution of this work is its novel integration of second-order, Hessian-free optimization with diffusion sampling. It also empirically found that first-order methods can produce inefficient trajectories (zigzag), and their method could alleviate this. Experiments showed overall performance gain on CelebA and CIFAR-10."}, "weaknesses": {"value": "- The experiments are insufficient. It is an overstatement that the authors describe their experiments as \"extensive\". The validation is limited to small-scale, unconditional generation on CIFAR-10 and CelebA, using older model architectures. The method needs to be benchmarked more, for example, ImageNet64 or higher-resolution tasks. Experiments on larger models are also encouraged (I couldn't find the size of networks used in the paper)\n- The lack of up-to-date baselines. The paper only compares HILDA to methods up to 2023 (e.g., DPM-Solver, UniPC). The current state-of-the-art in fast sampling, such as Consistency Models and MeanFlow, is missing.\n- The wall-clock time comparisons in Table 1 lack standard deviations, making it hard to assess them. We usually expect the computing time to scale (super)linearly with steps, but HILDA is the fastest in 5 and 50 NFEs. Considering the overhead of HVP, it raises question.\n- The visualizations in Figure 1 are not helpful; the authors claim they show a reduction in zigzag behavior, but the difference in the PC2/PC1 ratio plot (middle) appears marginal. The left plot only shows one instance (and one instance could have multiple ways of projections), which also raises question. It is also suggested that the authors make a Pareto frontier plot about FID and NFE.\n- The theory is not well explored. The problem formulation is not well written. The proof needs to be evaluated carefully. For example, Lemma 2 considers a \"frozen-time\" SDE, while diffusion is not \"frozen-time\". There are many design choices that are not shown to be optimal."}, "questions": {"value": "Many are mentioned in Weaknesses. Plus,\n- For adaptive damping, when condition numbers are extremely high, the preconditioner degenerates to scaled identity and loses second-order information. How frequently does this occur, and doesn't this undermine the core motivation?\n- The paper lacks failure case analysis. When does HILDA fail? How does performance degrade with poorly conditioned Hessians after damping? Ablation studies on sensitivity are encouraged."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OHeoGzkBFT", "forum": "BNOz93d1UM", "replyto": "BNOz93d1UM", "signatures": ["ICLR.cc/2026/Conference/Submission6092/Reviewer_3QDu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6092/Reviewer_3QDu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968608059, "cdate": 1761968608059, "tmdate": 1762918458609, "mdate": 1762918458609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HILDA, a training-free sampler for diffusion models. It aims to address the \"zigzag phenomenon\" in sampling trajectories. The method uses a geometric preconditioner that incorporates second-order Hessian information. This preconditioner features adaptive damping and spectral normalization. HILDA is designed as a plug-and-play module for existing ODE solvers like DPM-Solver. Experiments on CIFAR-10 and CelebA-HQ show improved FID scores over several baseline samplers."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "*   The identification and analysis of the \"zigzag phenomenon\" is an interesting and well-motivated starting point for the work.\n*   The proposed HILDA method is theoretically grounded, with a comprehensive design of the geometric preconditioner, including Hessian symmetrization, adaptive damping, and spectral normalization."}, "weaknesses": {"value": "*   **Contradictory Claims on Computational Overhead:** There is a significant contradiction regarding the computational cost of HILDA, which severely undermines the credibility of the method. In the methodology section (Lines 342-345), it is stated that each timestep requires \"6 ∼ 11 symmetrized HVPs\" and that \"each symmetrized HVP costs roughly twice a score evaluation.\" This implies a **12x to 22x** increase in computational cost per step, which is substantial and cannot be ignored. However, in the experiment section (Lines 427-428), it is claimed that the \"additional computational cost incurred by our HILDA method is virtually negligible,\" and Table 1 shows wall-clock times similar to baselines. This contradiction must be clarified and resolved.\n*   **Weak and Potentially Flawed Baselines:** The experimental baselines and reported results are not competitive with the current state-of-the-art, making it difficult to assess the method's true effectiveness.\n    *   Regarding Table 2 (CelebA-HQ 256x256 with a Latent Diffusion Model), the best FID reported is over 60, which is vastly inferior to the FID of 5.15 reported in the original LDM paper [1]. The generated samples in Figure 3 also exhibit visible artifacts. While the number of function evaluations (NFE) used here (5-50) is lower than in [1] (500 NFE), the results at 50 NFE should not be this poor, raising concerns about the implementation or sampling configuration.\n    *   Regarding Table 3 (CIFAR-10), the best FID is above 5, which is much worse than the FID of 1.97 achieved by EDM [2]. Comparisons with more recent and stronger baselines are lacking.\n\n[1] Rombach, Robin, et al. High-resolution image synthesis with latent diffusion models. CVPR 2022.\n\n[2] Karras, Tero, et al. Elucidating the design space of diffusion-based generative models. NeurIPS 2022."}, "questions": {"value": "*   Line 91 mentions experiments on \"text-guided conditional generation,\" but I could not find these results in the paper. If I have overlooked them, could the authors please point out their location? If they are indeed missing, could the authors provide these results, as they are important for demonstrating the generality of the method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "olaYE9S7Q8", "forum": "BNOz93d1UM", "replyto": "BNOz93d1UM", "signatures": ["ICLR.cc/2026/Conference/Submission6092/Reviewer_Ypwz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6092/Reviewer_Ypwz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6092/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991230017, "cdate": 1761991230017, "tmdate": 1762918458246, "mdate": 1762918458246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}