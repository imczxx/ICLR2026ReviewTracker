{"id": "U5Fm5ZSbSD", "number": 14242, "cdate": 1758231059114, "mdate": 1759897381784, "content": {"title": "Memory-Efficient Differentially Private Training with Gradient Random Projection", "abstract": "Differential privacy (DP) protects sensitive data during neural network training, but standard methods like DP-Adam suffer from high memory overhead due to per-sample gradient clipping, limiting scalability. We introduce DP-GRAPE (Gradient RAndom ProjEction), a DP training method that significantly reduces memory usage while maintaining utility on par with first-order DP approaches. DP-GRAPE is motivated by our finding that privatization flattens the gradient singular value spectrum, making SVD-based projections (as in GaLore Zhao et al. (2024)) unnecessary. Consequently, DP-GRAPE employs three key components: (1) random Gaussian matrices replace SVD-based subspaces, (2) gradients are privatized after projection, and (3) projection is applied during backpropagation. These contributions eliminate the need for costly SVD computations, enable substantial memory savings, and lead to improved utility. Despite operating in lower-dimensional subspaces, our theoretical analysis shows that DP-GRAPE achieves a privacy-utility trade-off comparable to DP-SGD. Our extensive empirical experiments show that DP-GRAPE can reduce the memory footprint of DP training without sacrificing accuracy or training time. In particular, DP-GRAPE reduces memory usage by over 63% when pre-training Vision Transformers and over 70% when fine-tuning RoBERTa-Large as compared to DP-Adam, while achieving similar performance. We further demonstrate that DP-GRAPE scales to fine-tuning large models such\nas OPT with up to 6.7 billion parameters, a scale at which DP-Adam fails due to\nmemory constraints.", "tldr": "", "keywords": ["Differential Privacy", "Random Projection", "Memory Efficient Training"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/32f5ade518f7059ee03dbe2769041ff07776d3fc.pdf", "supplementary_material": "/attachment/292ae5bb5d848c83a9b807910a6a5594d8ba9833.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces DP-GRAPE, a method to reduce the memory overhead of DP training by using random projections."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is supported by a theoretical analysis of its privacy and utility guarantees.\n\n2. Empirical results show it effectively reduces memory usage while maintaining model accuracy."}, "weaknesses": {"value": "The choice of the projection dimension r is a key hyperparameter, but the paper provides little guidance on how to set it. The trade-off between memory compression and model utility needs a systematic analysis."}, "questions": {"value": "1. After projecting the gradient, how is the noisy gradient used to update the model parameters?\n\n2. In the memory comparison (Table 2), why is DP-GRAPE's cost $n_l$ and not $m_ln_l$? Don't you need to compute the full gradient before projecting it?\n\n3. The Theorem says DP-GRAPE achieve comparable trade-off to DP-SGD. Do you have any experimental results? Also, what is the memory usage of DP-SGD?\n\n4. For the DP-Adam experiments, it is important to compare against the correct baseline, DP-AdamBC [1], which accounts for bias correction. Why was this comparison omitted?\n\n[1] DP-AdamBC: Your DP-Adam Is Actually DP-SGD (Unless You Apply Bias Correction), AAAI'24"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pI8QqPql9f", "forum": "U5Fm5ZSbSD", "replyto": "U5Fm5ZSbSD", "signatures": ["ICLR.cc/2026/Conference/Submission14242/Reviewer_gYPz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14242/Reviewer_gYPz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760629415743, "cdate": 1760629415743, "tmdate": 1762924696947, "mdate": 1762924696947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DP-GRAPE, a memory-efficient DP training method that projects each per-sample gradient into a low-dimensional random subspace, then performs clipping and Gaussian privatization after projection. This design cuts per-sample gradient and optimizer-state memory from full dimension to the projection dimension r, enabling large-model DP training without materializing full per-sample gradients. The authors argue SVD-based subspaces are unnecessary in the DP regime because privatization flattens the singular-value spectrums of gradients, so cheap Gaussian projections suffice and avoid expensive SVDs or storing projectors. Theoretically, under standard assumptions, the algorithm achieves (\\epsilon,\\delta)-DP and the expected stationarity gap matches that of DP-SGD up to log factors when the number of layers are considered as a constant. Empirically, it maintains accuracy comparable to first-order DP baselines while substantially reducing memory and scaling to larger models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality. The paper advances DP training by coupling project-then-privatize gradient handling with random low-rank projections, motivated by the observation that privatization flattens the gradient spectrum.\nQuality. The paper provides rigorous theoretical guarantees and offers reproducible implementation details and hyperparameter guidance.\nClarity. Figures, tables, and the presentation of the algorithm are clear with consistent notation that makes the method easy to follow. \nSignificance. DP-GRAPE substantially reduces the memory usage of DP training while preserving comparable accuracy."}, "weaknesses": {"value": "Limited novelty (main concern).\nAlgorithmically, the core move—projecting gradients into a low-dimensional subspace and then privatizing—is a direct transplant of low-rank / random-projection ideas into the DP setting; the paper does not introduce a fundamentally new optimization principle. On the theory side, the guarantees largely read as an incremental generalization of standard DP-SGD analyses to the projected case.\n\nMissing head-to-head experiments with the methods surveyed in Table 1.\nTable 1 contrasts DP-SGD-JL[1], Ghost Clipping[2], and Book-Keeping[3], but the paper does not reproduce them under the same models/hardware/privacy accounting—leaving the table’s claims unsupported in this setting. In the zeroth-order line, only DPZero is included while DP-ZO[4] is omitted, which is a notable gap given the scarcity and relevance of Zeroth-order DP work.\n\nOpaque memory attribution.\nThe comparisons do not decompose where memory is saved or spent—parameters, gradients, optimizer states, and activations—nor do they separate forward/backward/communication peaks. As a result, readers cannot tell whether the gains are dominated by optimizer-state shrinking, gradient tensor compression, or interactions with activation checkpointing.\n\n[1]  Fast and memory efficient differentially private-sgd via jl projections. Advances in Neural Information Processing Systems, 34:19680–19691, 2021.\n[2] Large language models can be strong differentially private learners. arXiv preprint arXiv:2110.05679, 2021.\n[3] Differentially private optimization on large model at small cost. In International Conference on Machine Learning, pp. 3192–3218. PMLR, 2023.\n[4] Private fine-tuning of large language models with zeroth-order optimization. arXiv preprint arXiv:2401.04343, 2024."}, "questions": {"value": "Questions\nWill you add DP-SGD-JL, Ghost Clipping, Book-Keeping, and DP-ZO under identical models and hardware, reporting peak memory, throughput, and wall-clock to a fixed validation target, so Table-1 claims are empirically supported?\n\nCan you include a stacked memory breakdown separating parameters/gradients/ optimizer states/activations and phase-specific peaks (forward/backward/communication)?\n\nSince the analysis only covers SGD-type convergence now, can you provide an Adam-style convergence guarantee of DP-GRAPE?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9EHXeP3VBd", "forum": "U5Fm5ZSbSD", "replyto": "U5Fm5ZSbSD", "signatures": ["ICLR.cc/2026/Conference/Submission14242/Reviewer_dFp7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14242/Reviewer_dFp7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704326240, "cdate": 1761704326240, "tmdate": 1762924696579, "mdate": 1762924696579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose projecting per-sample gradients onto low-dimensional random Gaussian subspaces before privatization, thus reducing memory and optimizer state size while maintaining DP guarantees.\nThe work is motivated by an empirical observation that differential privatization flattens the singular value spectrum of gradients, making SVD-based projections (e.g., GaLore) unnecessary. DP-GRAPE instead uses random Gaussian projections computed on-the-fly"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The observation about spectral flattening is novel and provides a principled reason to abandon SVD-based projections.\n\nThe authors provide a theoretical privacy and convergence analysis for DP-GRAPE, which is non-trivial due to the introduction of random projections.\n\nEvaluations cover both CV (ViT pre-training) and NLP (RoBERTa, OPT). Achieves large-scale DP training (OPT, 6.7B).\n\nMemory savings in training are considerable: it cuts memory by over 63% in Vision Transformer training and 70% in RoBERTa fine-tuning compared to DP-Adam."}, "weaknesses": {"value": "The privacy guarantee under random projections with unbounded entries is described informally. A more rigorous sensitivity or RDP proof sketch is needed.\n\nDP-GRAPE’s algorithm is more complex to implement than vanilla DP-SGD/DP-Adam. I'm not sure how practical would be to implement it. No code mentioning."}, "questions": {"value": "Have you done any ablation of projection dimension r versus accuracy/privacy?\n\nHow does the projection dimension r influence the effective privacy budget?\n\nHow does DP-GRAPE interact with existing memory-saving techniques like ghost clipping or even simple gradient accumulation?\n\nDP-MERF, Harder at al. uses random features to create embeddings. It does not do it with memory efficiency as a goal but does not do it as a by-product?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "q5xQEvS3dc", "forum": "U5Fm5ZSbSD", "replyto": "U5Fm5ZSbSD", "signatures": ["ICLR.cc/2026/Conference/Submission14242/Reviewer_3fv9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14242/Reviewer_3fv9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762359945895, "cdate": 1762359945895, "tmdate": 1762924696156, "mdate": 1762924696156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents a new approach to memory-efficient DP training using random projections instead of SVD-based subspaces, which is motivated by a “flattened” singular value spectrum after privatization. DP-GRAPE (Gradient RAndom ProjEction) employs three key components: (1) random Gaussian matrices replace SVD-based subspaces, (2) gradients are privatized after projection, and (3) projection is applied during  backpropagation. The experiments show that DP-GRAPE can reduce the memory footprint of DP training without sacrificing accuracy or training time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- using random projections (DP-GRAPE) instead of SVD-based projections, which is memory efficient.\n- DP-GRAPE (Gradient RAndom ProjEction) achieves a privacy-utility trade-off comparable to DP-SGD.\n- The margins in the experiments are significant, in terms of the memory reduction, while preserving the accuracy."}, "weaknesses": {"value": "- Comparisons asre not sufficient with SOTA methods, and other subspace methods.\n- The robustness analysis for failure cases is missing."}, "questions": {"value": "- The differences between the DP-GRAPE and existing subspace methods, such as LoRA, etc.\n- The robustness analysis for failure cases is missing.\n- Hyperparameters, 'somewhat extensive hyperparameter searches', sensitivity analysis is necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZDyqIAcvH7", "forum": "U5Fm5ZSbSD", "replyto": "U5Fm5ZSbSD", "signatures": ["ICLR.cc/2026/Conference/Submission14242/Reviewer_uZQ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14242/Reviewer_uZQ1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14242/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762538382485, "cdate": 1762538382485, "tmdate": 1762924695769, "mdate": 1762924695769, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}