{"id": "dm42omwep1", "number": 1840, "cdate": 1756951156439, "mdate": 1759898182958, "content": {"title": "MEM-$\\alpha$: LEARNING MEMORY CONSTRUCTION VIA REINFORCEMENT LEARNING", "abstract": "Large language model (LLM) agents are constrained by limited context windows, necessitating external memory systems for long-term information understanding. Current memory-augmented agents typically depend on pre-defined instructions and tools for memory updates. However, language models may lack the ability to determine which information to store, how to structure it, and when to update it—especially as memory systems become more complex. This results in suboptimal memory construction and information loss. To this end, we propose Mem-$\\alpha$, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback. We also construct a specialized training dataset spanning diverse multi-turn interaction patterns paired with comprehensive evaluation questions designed to teach effective memory management. During training, agents process sequential information chunks, learn to extract, store, and update the memory system. The reward signal derives from downstream question-answering accuracy over the full interaction history, directly optimizing for memory construction. To illustrate the effectiveness of our training framework, we design a memory architecture comprising core, episodic, and semantic components, equipped with multiple tools for memory operations. Empirical evaluation demonstrates that Mem-$\\alpha$ achieves significant improvements over existing memory-augmented agent baselines. Despite being trained exclusively on instances with a maximum length of 30k tokens, our agents exhibit remarkable generalization to sequences exceeding 400k tokens—over 13× the training length, highlighting the robustness of reinforcement learning for memory management. Code and data will be released upon publication.", "tldr": "Learning agent memory construction via reinforcement learning", "keywords": ["memory agent", "reinforcement learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/84b195754f5a425454f70a545ce1e22ee38834db.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "They propose Mem-α, a reinforcement learning framework that trains agents to effectively manage complex memory systems through interaction and feedback."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- They propose an algorithm for the agent to learn to maintain its memory through reinforcement learning - which is an important research question.\n- The algorithm is clearly explained.\n- Empirical performance over multiple benchmarks have been shown."}, "weaknesses": {"value": "- There are multiple related works in this research direction, but novelties compared with them are not so clear.\n- The novelty mainly shows in (1) the design of the memory architecture; (2) the design of the action space and reward function, which is relatively limted.\n- Several baselines mentioned in the last paragraph of Section 2 are not covered in the experiment part."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "w4aZNmqWsV", "forum": "dm42omwep1", "replyto": "dm42omwep1", "signatures": ["ICLR.cc/2026/Conference/Submission1840/Reviewer_JTQ1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1840/Reviewer_JTQ1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445863239, "cdate": 1761445863239, "tmdate": 1762915906186, "mdate": 1762915906186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Mem-α, a reinforcement learning framework for teaching LLM agents to construct and manage multi-component memory systems. The method formulates memory construction as a sequential decision-making problem, where the agent learns through interaction which information to store, update, or delete across core, semantic, and episodic memory modules. The reward combines downstream QA accuracy with auxiliary signals for tool usage, compression, and content quality. Experiments on MemoryAgentBench and long-context benchmarks show that Mem-α consistently outperforms baselines, and generalizes from 30k-token training sequences to over 400k tokens at test time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a highly relevant problem for the community: scalable and trainable memory management in LLM agents, which remains a key bottleneck in long-context reasoning. Framing memory construction as a reinforcement learning problem with interpretable reward components is conceptually clean, well-motivated, and methodologically sound.\n2. The separation of memory into core, semantic, and episodic components provides flexibility and aligns well with cognitive theories of human memory. This structured design enhances interpretability and facilitates more effective memory operations.\n3. The proposed method demonstrates consistent improvements across diverse benchmarks and exhibits remarkable generalization to sequences over 400k tokens, highlighting the robustness and scalability of the approach."}, "weaknesses": {"value": "1. **Limited novelty over recent RL-based memory works.** While the framework is technically solid, it mainly extends prior RL-based memory systems (e.g., Memory-R1, MEM1) rather than introducing a fundamentally new paradigm. The paper’s two novel aspects: the reward function design and the memory architecture The former feels more like an engineering enhancement, while the latter, on its own, may not constitute a sufficiently strong contribution for a conference paper.\n2. **Lack of analysis on memory components.** The paper includes ablation studies on reward design but does not perform quantitative analyses isolating the effects of core, semantic, or episodic memory modules. As a result, it remains unclear how much each component contributes to the overall performance gain."}, "questions": {"value": "1. How do the individual memory components—core, semantic, and episodic—contribute to the final performance? It would be helpful to see either quantitative or qualitative analysis demonstrating the role of each component in improving retrieval accuracy or long-range reasoning.\n2. How is the granularity of each memory unit (e.g., 512 tokens for the core memory, atomic factual entries for semantic memory) determined? Could this level of granularity be learned automatically rather than predefined? Furthermore, how general is the proposed method across different tasks—does the optimal granularity depend on the specific task distribution or domain?\n3.  In your formulation, each turn (or chunk) appears to be treated as an individual training sample, whereas VeRL and similar frameworks treat the entire trajectory as a single training instance for policy optimization. Could you elaborate on how this design choice is implemented in your RL pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rCdGBpm738", "forum": "dm42omwep1", "replyto": "dm42omwep1", "signatures": ["ICLR.cc/2026/Conference/Submission1840/Reviewer_5HwK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1840/Reviewer_5HwK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761453457647, "cdate": 1761453457647, "tmdate": 1762915905792, "mdate": 1762915905792, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Mem-$\\alpha$, a reinforcement learning framework designed to train LLM agents to manage complex external memory by effectively using tools. The agent is trained using a composite reward signal including downstream QA accuracy, tool call format, memory compression, and a llm-as-a-judge for memory content quality. The method demonstrates significant improvements over existing benchmarks and shows strong generalization capability to sequences much longer than those seen during training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. When designing the framework of RL, this paper comprehensively includes reward reflecting key aspects of a successful memory system, including effectiveness (through QA acc and tool call format) and efficiency (through the compression score).\n\n2. This paper constructed a diverse training dataset, spanning multiple tasks with long-context. This trains the model to learn a more general-purpose memory management strategy, which is then evaluated on even longer contexts.\n\n3. Comprehensive evaluation and ablation studies show the effectiveness of the proposed framework. Especially the generalization shows the agent is learning a robust policy for memory curation, rather than merely overfitting to patterns within the fixed training length."}, "weaknesses": {"value": "1. In Table 1, the final memory size for Mem-$\\alpha$ is often comparable to the Long-Context baseline. The most significant compression gains appear on the BookSum task, which involves synthetic \"conversation history\" created by chunking a book. It's unclear how the structured episodic memory provides a meaningful advantage in this non-conversational scenario?\n1. The learned memory-writing policy is tightly coupled to a fixed RAG pipeline (BM25 retriever, Qwen3-32B generator). It is unclear if this memory structure would be as effective for a different generator model that was not part of the RL training loop. Since this can show the generalization capability of your memory management.\n1. Lack of detailed comparison with related work. You've mentioned recent work also use reinforcement-learning to enhance the model's memory management capability, such as Memory-R1 (Yan et al.), what's the main novelty of mem-$\\alpha$, apart from the data differences you mentioned around line 140. \n1. Ablation study on $\\gamma$  does not efficiently support the choice of $\\gamma = 0.1$, since only present results on $\\gamma =\\{0, 0.1\\}$ , why not try larger $\\gamma$ which may lead to memory with higher quality?\n1. Small typos, such as percentage missing around line 359, before (3)."}, "questions": {"value": "1. The case study in Table 5 shows GPT-4.1-mini failing to record assistant behavior or consolidate same-timestamp events. Given this model's  instruction-following capability, could these \"failures\" be solved with better prompting? I did not found prompt for GPT-4.1-mini, making it difficult to assess if this is a fair comparison. \n2. The reward from the QA accuracy is relatively sparse, given QAs are only conducted with the final $\\mathcal{M}_n$. Why we could not use QA reward for all $M_i$ ? How effectively can this signal be back-propagated to credit or penalize individual memory operations from much earlier in the sequence? To what extent are the denser, step-wise rewards ($r_2$ and $r_4$) responsible for driving the performace improvement, compared to the sparse $r_1$?\n3. Just for discussion, the current framework trains a memory manager but uses a fixed \"reader\" (the RAG generator). And you've used llm-as-a-judge to directly measure the quality of memory by a 0/1 reward. Since the agent is learning to organize memory in a specific way, would it be beneficial to also train the \"reader\" model to co-evolve with the memory structure? A dual-training approach might teach the generator model to better leverage the specific format of the memory being created by the memory manager."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M1aPqBVR8D", "forum": "dm42omwep1", "replyto": "dm42omwep1", "signatures": ["ICLR.cc/2026/Conference/Submission1840/Reviewer_iBxE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1840/Reviewer_iBxE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829525606, "cdate": 1761829525606, "tmdate": 1762915905262, "mdate": 1762915905262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Mem-$\\alpha$, a reinforcement learning framework that teaches an agent to operate a complex external memory via function calls. The memory consists of Core, Episodic, and Semantic components. Evaluated on long-range understanding and question answering, the approach performs on average better than naive RAG methods, full context llms and other RL fine-tuned agentic approaches with external memory (MEMAgent and MEM1)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Extensive experiments across diverse datasets and tasks. A solid set of ablation studies on key design choices.\n\n* The method outperforms simpler RL baselines with flat-memory ."}, "weaknesses": {"value": "* Presentation quality requires further work. It is unclear how the advantage is computed in the GRPO variant because rewards vary not only across trajectories but also across time steps $t$ and action components $k$, leaving ambiguous what set of rewards is averaged in the formula on L240. L283 says Figure 3 illustrates the memory components and their interactions, yet the figure shows zero interactions between these components. The interfaces for write, delete, and update memory functions are underspecified; an in-text example or simple illustration would help.\n\n* The level of novelty appears limited. The combination of a complex memory architecture with RL is composed of known ideas, and their integration feels straightforward.  Сurrent ablations (showing usefulness of complex memory and RL fine-tuning) support the design choices but adds limited new insight. Deeper ablations could strengthen novelty, e.g., analyzing the utility of each memory component by disabling one component and observing which tasks degrade, toggling components during training vs. only at inference, measuring access frequency to each component during generation, and tracking how usage shifts as context length grows. \n\n* Comparisons to stronger external-memory agents are missing. Many agents do not use fine-tuning yet employ complex memory structures, including dynamic knowledge graphs, combined episodic and semantic memory, and RAG systems (e.g., Search-o1, AriGraph, GraphRAG, and works cited in Section 2). Such comparisons would clarify whether to invest in RL fine-tuning for tool-based memory management or prioritize designing richer memory structures without fine-tuning."}, "questions": {"value": "* What exactly is used to compute the advantage $A_t$ at step $t$ for the action component $a_t$? How are the reward mean and standard deviation computed? Which rewards enter mean and std computation: all components across all time steps and trajectories, all components across all trajectories but only at the same time step $t$, or some other aggregation?\n* Regarding the Correctness Reward: it appears to be computed for a single final memory state against a set of questions. Since LLMs can hallucinate and also answer correctly without memory due to internal knowledge, per-question rewards can be noisy. Averaging across multiple questions likely reduces noise, but how does the final agent’s performance depend on the number of end-of-trajectory questions?\n* Please address the weaknesses noted above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "og5UxJbIMN", "forum": "dm42omwep1", "replyto": "dm42omwep1", "signatures": ["ICLR.cc/2026/Conference/Submission1840/Reviewer_Tknh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1840/Reviewer_Tknh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1840/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995084426, "cdate": 1761995084426, "tmdate": 1762915905122, "mdate": 1762915905122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}