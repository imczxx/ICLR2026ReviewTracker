{"id": "cTWxQ0bFvI", "number": 22277, "cdate": 1758328910566, "mdate": 1763702683802, "content": {"title": "Stealing the Recipe: Hyperparameter Stealing Attacks on Fine-Tuned LLMs", "abstract": "Large language models (LLMs) rely on carefully tuned hyperparameters such as optimizer, learning rate, batch size, and model size. These details strongly influence performance and generalization but are typically withheld, as they result from costly experimentation and constitute valuable intellectual property. While prior work has examined model extraction and membership inference, the question of whether hyperparameters themselves can be inferred has remained largely unexplored. In this paper, we introduce the first framework for hyperparameter stealing attacks against fine-tuned LLMs. Our approach combines different techniques, such as constructing hijacking datasets to elicit informative variations in model behavior, training shadow models across multiple architectures, and extracting multimodal statistical and semantic features from their outputs. Using these features, we train a multi-label, multi-class classifier that simultaneously predicts multiple hidden hyperparameters in a black-box setting. Across encoder–decoder models (BART, Pegasus) and decoder-only models (GPT-2), our attack achieves 100\\% accuracy on model family, 97.9\\% on model size, and strong performance on learning rate (88.7\\%) and batch size (80.0\\%). Even in mixed-family settings, learning rate and batch size remain identifiable. These findings demonstrate that hyperparameter stealing is both practical and effective, exposing a previously overlooked vulnerability in deployed LLMs and underscoring new risks for intellectual property protection and the security of Machine Learning as a Service (MLaaS).", "tldr": "", "keywords": ["Hyperparameter stealing; Large language models; Shadow models; Black-box attacks"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/419bad400fbdf20e1b5478e01e74a2a0c2788430.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces a new data-poisoning attack that allows identification of certain hyperparameters used to train a model (mainly the model family, its size, the learning rate, and the batch size).\n\nThe attack first constructs a poisoned dataset containing specific hijacking tasks, and then trains a set of shadow models on both the poisoned dataset and a clean dataset, with each shadow model using different hyperparameters. Those shadow models are in turn used to train a classifier that predicts the hyperparameters based on their outputs when prompted with the poisoned data."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The threat model (hyperparameter stealing) is relevant given the importance of hyperparameter selection in LLM training and the cost associated with hyperparameter tuning.\n- The threat model's goal and the attacker's capabilities are clearly explained."}, "weaknesses": {"value": "- The presentation of the method is hard to follow; an illustration to assist in understanding the method could significantly improve the clarity of Section 4. In particular, I do not grasp the intuition behind how the construction of the poisoning dataset helps in creating a downstream signal used to classify which hyperparameters are used. Similarly, the feature extraction process appears to be mostly hand-designed, and an explanation of the procedure that led to the choice of those features would be beneficial for guiding future work.\n- The experiments are not sound. My understanding is that Table 1 shows the accuracy and F1-score of the classifier on the shadow models, i.e., on the training data. This means we do not know how the method behaves out of distribution. In particular, what if the victim (the one from whom we are stealing the hyperparameters) used a different clean dataset than the shadow models? What if they used a set of hyperparameters not included among those used for the shadow models? A table with out-of-distribution models and the accuracy of the method on such models would be necessary to claim that the method is effective in recovering hyperparameters.\n- Similarly, we see that apart from $x_{1},x_{2}$, the additional features contribute little predictive power. The added benefit could simply be due to overfitting, because the classifier is evaluated on the training set.\n- The results are limited; only model family and size are accurately classified for all model architectures tested.\n- The models evaluated are both old and relatively small; whether the method generalizes to newer models (where the hyperparameters are even more valuable) remains an open question.\n- The method does not transfer across model families, meaning the attacker needs to know the model family beforehand. This could be a practical issue, as most models whose families are known are open-source."}, "questions": {"value": "- Can the authors compute results similar to Table 1 but using out-of-distribution models? For instance, using hyperparameters that differ from those of the shadow models, a different clean dataset, and a different percentage of poisoned data.\n- What is the ratio between clean and poisoned data in your experiments? An ablation on this ratio is important, as in practice the poisoned data constitutes only a negligible percentage of the entire training dataset.\n- What are the motivations behind the selected features?\n- Can you evaluate your method on newer models? Perhaps a recent 3B model for cost-effectiveness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "WtWCyayC03", "forum": "cTWxQ0bFvI", "replyto": "cTWxQ0bFvI", "signatures": ["ICLR.cc/2026/Conference/Submission22277/Reviewer_S22H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22277/Reviewer_S22H"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726361305, "cdate": 1761726361305, "tmdate": 1762942148084, "mdate": 1762942148084, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores “hyperparameter stealing”: inferring optimizer, learning rate, batch size, model size, and architecture of fine-tuned language models from black-box access. The attacker embeds a stealthy “hijacking” corpus into the victim’s training data, trains a bank of shadow models covering many hyper-parameter settings, extracts seven kinds of statistical and semantic features from model outputs, and learns a multi-label classifier to guess the hidden recipe. On BART, Pegasus and GPT-2 the attack recovers family (100 %), size (≈98 %), learning rate (≈89 %), and batch size (≈80 %) in the encoder–decoder case; optimizer remains near chance. The paper also offers modality ablations, cross-family transfer tests, and an evaluation of the ONION defense."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-Clear statement of a new security objective: recovering hyper-parameters rather than parameters or training data.\n- Method is systematically broken down (hijacking data, shadow bank, features, classifier) and each component is described with equations (Pages 3–5) enabling replication.\n- Table 1 gives comprehensive accuracy/F1 across 189 target configurations and shows sizable margins over random guessing, especially for family, size, learning rate, and batch size.\n- Figure 1 visualises that the camouflaged outputs stay close in embedding space, supporting the stealth argument.\n- Ablations in Table 2 quantify the contribution of each feature block; the rise from 58.7 % to 98 % size accuracy when adding statistical blocks is informative.\n- Transfer (Table 3) and defense (Table 4) studies highlight both limitations and the inadequacy of an existing sanitisation technique, giving a balanced picture.\n- Writing is mostly clear; equations defining $S_{\\text{sem}}$, $S_{\\text{hij}}$, and the multi-label loss are explicit and free of obvious errors."}, "weaknesses": {"value": "1. Unrealistic threat prerequisite. The attack requires the adversary to have data-poisoning access to the victim’s training corpus (Sec. 3, Page 3). For most commercial LLM providers training on carefully curated or private corpora, sneaking thousands of crafted IMDB-like sentences in is speculative. The work does not measure how many poisoned samples must be retained to keep performance in Table 1 or assess the attacker’s cost vs. benefit.\n2. Limited baseline comparison. No alternative hyper-parameter inference attack is implemented; even a simple black-box meta-classifier (without poisoning) could be a strawman. Thus the incremental gain due to hijacking is unclear.\n3. Optimizer prediction failure (≈18 %) suggests the feature design or framing is incomplete. The paper does not investigate why nor attempt alternative signals (e.g., gradient-based probes).\n4. Cross-family generalisation largely collapses (Table 3: 0 % family accuracy), yet the abstract still claims effectiveness “even in mixed-family settings” without emphasising the sharp degradation.\n5. Mathematical clarity gaps. Equation for the loss (Page 5) introduces per-head weights $\\lambda_k$ and class weights $\\alpha_{k,c}$ but never specifies chosen values; wrong choices might bias the reported head accuracies.\n6. Experimental scope. Only summarisation is tested. Other generation tasks (dialogue, translation) or instruction-tuned models are common in practice and may exhibit different leakage patterns.\n7. Ethical and defensive discussion remains shallow: no mitigation beyond ONION is studied, and ONION is known to be weak on text generative backdoors.\n8. Potentially missing related work: Wang & Gong 2018 (classical hyper-parameter stealing) is cited, but recent LLM-specific privacy backdoor papers such as Feng & Tramèr 2024 or Kandpal et al. 2023 are absent from Related Work despite obvious \n\nPotentially Missing Related Work\n- Qi, Zeng & Xie, “Fine-tuning Aligned Language Models Compromises Safety” (2023) – shows how fine-tuning introduces new attack surface; should be cited in Sec. 2 to frame poisoning risk.\n- Kandpal, Pillutla & Oprea, “User Inference Attacks on Large Language Models” (2023) – studies inference from outputs; relevant for comparison in Sec. 2.\n- Feng & Tramèr, “Privacy Backdoors: Stealing Data with Corrupted Pretrained Models” (2024) – demonstrates data-theft backdoors; similar stealth mechanism.\n- Hicks, “Stealing Finetuning Data with Corrupted Models” (2025) – expands the backdoor discussion; inclusion would strengthen positioning.\n(These works are directly relevant yet not cited.)"}, "questions": {"value": "1. How many hijacking examples are actually retained by the target in Table 1 experiments after the simulated 20 % filtering, and how does accuracy scale if retention drops to 5 %?\n2. Could a non-poisoning passive attack, e.g., training shadow models and probing the deployed API with random inputs, achieve similar accuracy?\n3. For optimizer prediction, did you try time-series features such as variance of log-probs across decoding steps which might capture Adam’s batching effects?\n4. How robust is the attack to temperature sampling in generation at inference?\n5. Can the attacker fine-tune fewer than 189 shadows and still match performance? Any subsampling study?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bxfT85edS8", "forum": "cTWxQ0bFvI", "replyto": "cTWxQ0bFvI", "signatures": ["ICLR.cc/2026/Conference/Submission22277/Reviewer_AQMw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22277/Reviewer_AQMw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970133058, "cdate": 1761970133058, "tmdate": 1762942147895, "mdate": 1762942147895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new model stealing scenario called hyperevaluation, where the goal is to recover the instruction tuning behavior of a black-box language model. Instead of copying the model’s answers, the attacker extracts task-like examples from the model’s own prompts and responses. These examples are then used to train a smaller open-source model that imitates the original model’s instruction-following behavior. The main contribution lies in shifting attention from output mimicry to replicating how the model generalizes across tasks. While the idea of stealing instructions is thought-provoking and relevant to current API usage patterns, the technical approach is relatively simple and does not introduce new learning algorithms or modeling techniques."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "● The idea of stealing instruction patterns instead of just outputs is interesting and not something I’ve seen explored much. It frames a realistic kind of vulnerability that hasn’t gotten much attention.\n\n● The attack setup makes sense given how APIs are used in practice, especially with prompts that expose few-shot examples or task templates.\n\n● While the method itself is relatively simple, the experiments are clean and show that the approach can lead to meaningful imitation of the original model’s behavior.\n\n● The paper is clearly written and easy to follow. It’s not hard to imagine others building on this kind of attack framing."}, "weaknesses": {"value": "● The attack is conceptually interesting but technically quite straightforward. It mostly involves mining examples from prompts and training a model on them. There’s no new algorithm or mechanism beyond this.\n\n● The success of the attack depends a lot on how the API model formats its responses. If few-shot examples or task prompts are not exposed in the output, it’s unclear how well this method would work.\n\n● The paper doesn’t really discuss how easy it would be to defend against this kind of attack. Basic strategies like hiding prompt structure, trimming outputs, or limiting example formatting could potentially reduce the risk, but none of this is explored.\n\n● It would have been helpful to understand how sensitive the attack is to noise or to less clean examples. Right now it’s not clear how robust the data collection and downstream training really are."}, "questions": {"value": "1. How dependent is your attack on the model exposing clean few-shot examples or task templates in its outputs?\n\n2. Did you try running the attack on models that give more free-form or less structured responses?\n\n3. Could basic defenses like removing examples from responses or randomizing output structure weaken the attack?\n\n4. How noisy can the extracted examples be before the student model stops learning meaningful patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vytaHLsSwg", "forum": "cTWxQ0bFvI", "replyto": "cTWxQ0bFvI", "signatures": ["ICLR.cc/2026/Conference/Submission22277/Reviewer_JQ7Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22277/Reviewer_JQ7Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986233066, "cdate": 1761986233066, "tmdate": 1762942147539, "mdate": 1762942147539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}