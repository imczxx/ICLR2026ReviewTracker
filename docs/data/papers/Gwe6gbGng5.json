{"id": "Gwe6gbGng5", "number": 14900, "cdate": 1758245316384, "mdate": 1759897342690, "content": {"title": "Cautious Weight Decay", "abstract": "We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic modification that applies weight decay only to parameter coordinates whose signs align with the optimizer update. Unlike standard decoupled decay, which implicitly optimizes a regularized or constrained objective, CWD preserves the original loss and admits a bilevel interpretation: it induces sliding-mode behavior upon reaching the stationary manifold, allowing it to search for locally Pareto-optimal stationary points of the unmodified objective. In practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon, requiring no new hyperparameters or additional tuning. For language model pre-training and ImageNet classification, CWD consistently improves final loss and accuracy at million- to billion-parameter scales.", "tldr": "", "keywords": ["optimization", "regularization", "weight decay", "decoupled", "lyapunov", "training", "deep learning"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46912f6bea26cf709d99b0df502e307fb4654c93.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a novel way of weight decay, which only implements weight decay on the coordinates that the direction aligns with the gradient. The new method is flexible to work with any optimizer and can be implemented easily and efficiently. It can converge to the minimizer manifold, unlike decoupled weight decay will add some additional constraint and can’t reach the minimizer.  The paper theoretically shows the converged point is a Pareto optimal point in the manifold. The experiments demonstrate it empirically works better than decoupled weight decay or no weight decay."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The theoretical analysis on the continuous part shows the advantage of cautious weight decay, which can be an improvement over decoupled weight decay. The converged point of cautious weight decay has some good implicit bias. \n2. The extensive experiment results show the advantage of weight decay is promising. \n3. The comparison between different weight decay is very clear with the visualization of trajectories."}, "weaknesses": {"value": "The statement of theorem 1 and the noise assumption are problematic. The four constants $K_1, K_2, K_3, K_4$ should depend on $L, \\sigma$ and initial loss gap, which needs to be shown explicitly to check whether the convergence rate is valid. When I read the proof, more problems arise. \n1. The noise assumption depends on the batch size, which is unconventional. In other papers that show convergence rate, the variance of the stochastic gradient is fixed $\\sigma^2$. Then they can still show $T^{-1/2}$ convergence rate. If this paper’s result is presented with the standard noise assumption, the convergence rate will have a constant term $\\sigma$ and therefore is even $\\Theta(1)$ for the dependence on $T$. \n2. I don’t understand why the iterates are bounded in Lemma 2. Also I don’t know the exact meaning of $G$. If it is a constant independent of $T$ and other problem parameters such as $L$ and $\\sigma$, then you are basically cheating with this lemma because you essentially assume the loss function is $G$-Lipschitz rather than $L$-smooth, which is a much stronger assumption. And you also assume the region is bounded, i.e., $||x_t||_\\infty \\leq R$, which is used in the proof of lemma 6. These should be mentioned explicitly in the main text because they are stronger assumptions than smoothness and noise variance. If $G$ can have some dependence on $T$, then $K_1, K_2, K_3, K_4$ in theorem 1 are not constant and the final convergence rate will have a much worse dependence on $T$. \n3. The constants $K_2, K_3, K_4$ are $O(d)$, which means the convergence rate has a very bad dependence on $d$. This is not ideal when $d$ can be much larger than $T$ in reality. Also they are $O(1/\\epsilon)$ so the dependence on $\\epsilon$ is also not ideal. But this issue is much less serious than the two issues above. It’s good to improve the dependence on $d$ and $\\epsilon$ but I can accept the current dependence as long as the proof fixes issue above."}, "questions": {"value": "1. Can you address the issues about convergence rate as I mentioned in weaknesses?\n2. I don’t understand the interpretation of the second term in remark 1. Are you suggesting that it will converge to a Pareto front? This claim can only be rigorously shown by a proof similar with Xie and Li 2024. \n3. This kind of cautious weight decay makes SGD not rotation-invariant. Have you thought about global cautious weight decay, which implement the weight decay when its inner product with gradient is positive?\n\nThere are some possible typos or things that are not clear enough to me:\n1. Page 4 line 181: the set of stationary points **is** a union of connected\n2. Page 5 line 263: the order comparison between two vectors is a bit ambiguous. I suggest write $y_i$ and $x_i$ explicitly in the scalar order\n3. Figure 3: left function should be $f(x,y)=((y-3)^2+(x-3)^2-1)^2$ in the caption. It’s also better to indicate that purpler region is global minimizers. \n4. Page 22 line 1153: the last line seems wrong. It seems you are using $||a+b||_2^2 \\leq ||a||_2^2 +||b||_2^2$ here. It can be correct if multiplied by 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TQixSwrO5H", "forum": "Gwe6gbGng5", "replyto": "Gwe6gbGng5", "signatures": ["ICLR.cc/2026/Conference/Submission14900/Reviewer_yQsL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14900/Reviewer_yQsL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761158069784, "cdate": 1761158069784, "tmdate": 1762925244733, "mdate": 1762925244733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a simple modification to weight decay, only applying the decay if the optimizer updates and the weights are aligned. The show mathematically that this leads to optimizing the original loss rather than a regularized loss. Through extensive empirical work, the authors demonstrate: 1) Adding the fix to Adam/muon/lion all improves convergence speed for LLM training without retuning hyperparameters. (fig4) 2) The performance is consistently better across levels of weight decay (fig1), 3) ImageNet accuracy is also improved."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is very simple\n- The empirical results are very convincing, especially since they use multiple codebases\n- The paper is well written and easy to follow"}, "weaknesses": {"value": "- Personally, I think the theoretical results are distracting. I'd prefer that they be moved to the appendix and that the main paper is focused on the empirical results. That is what most people will care about.\n- There is some missing related work (e.g., \"Understanding decoupled and early weight decay\")"}, "questions": {"value": "1. Are you able to provide scaling plots of final-loss-vs-model size for the baseline and your method? It would be interesting to see if the \"boost\" in performance is retained as the model is scaled."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VeBETOeQbq", "forum": "Gwe6gbGng5", "replyto": "Gwe6gbGng5", "signatures": ["ICLR.cc/2026/Conference/Submission14900/Reviewer_hDqM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14900/Reviewer_hDqM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761344752839, "cdate": 1761344752839, "tmdate": 1762925244100, "mdate": 1762925244100, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Cautious weight decay describes a one-line modification of weight decay such that it is applied only on parameters that are aligned with the update direction.\nThe paper shows that this is related to minimizing the unregularized loss, with limiting point such that all weights have minimal magnitude locally.\nLarge-scale experiments show the effectiveness of the method.\n\n**Overall,** the contributions and evaluations in this paper are of highest quality and the presented technique is likely to be adapted in practice quickly."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper presents a seemingly simple idea that can be implemented with changing one line of code change. The effects of this modification are extremely well explained, and justified through experiments on industry-scale problems for language model training; the method also performs well on standard classification models for Imagenet. Several ablations are provided that justify the exact construction of cautious weight decay.\n\nBesides the clarity and convincing experiments, the paper also contains convergence analysis of the method, and extensive motivation and background through Lyapunov analysis."}, "weaknesses": {"value": "The paper has no major weaknesses. The only minor point/question is on the motivation of CWD through Lyapunov analysis: the design of CWD is not explicitly reflected in the choice of its Lyapunov function. This leaves open the question whether other modifications of weight decay would allow for the same Lyapunov function, and if these alternatives would be interesting/competitive methods. From the current presentation, I could not find an argument why this modification of weight decay would be the singular one to yield improvements."}, "questions": {"value": "* Did you test the effect of CWD for different learning-rate schedules (e.g. cosine and WSD)? Did you test whether CWD is compatible with AdamC by Defazio, 2025, and if there is a compound improvement of using both?\n* Does the floating-point format have an effect on CWD, in the sense that using low-precision formats might introduce a harmful bias in the direction/weights?\n* Figure 3 left: why does the CWD trajectory not stop once it reaches the Pareto set? \n* Assumption 1 states that the iterates are bounded due to coercivity. Can you provide a short proof for this, and also clarify in which probabilistic sense (as the iterates are random variables)?\n\nMinor:\n\n* line 220: why does $m$ necessarily decay to zero? In the stochastic case, with non-vanishing noise, this is not clear to me.\n* The entire paper uses the notation $(\\nabla f(x_t) x_t)^+$, but this is missing the transposition operator on the gradient."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lll59v7uYB", "forum": "Gwe6gbGng5", "replyto": "Gwe6gbGng5", "signatures": ["ICLR.cc/2026/Conference/Submission14900/Reviewer_Jtg5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14900/Reviewer_Jtg5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818444924, "cdate": 1761818444924, "tmdate": 1762925243580, "mdate": 1762925243580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Cautious Weight Decay (CWD) — a simple, optimizer-agnostic modification that applies weight decay only when the update direction matches the parameter sign. Unlike standard weight decay, CWD preserves the original objective and admits a bilevel/sliding-mode interpretation. It requires no extra hyperparameters and works as a drop-in replacement for AdamW, Lion, and Muon. Experiments on large-scale language model pretraining and ImageNet classification show consistent improvements in loss and accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Well written, easy to follow, and clearly presented.\n2. Theoretical foundations are solid and supported by extensive experiments across both vision and language tasks."}, "weaknesses": {"value": "While the proposed method is elegant and broadly applicable, its novelty may be somewhat limited. Conceptually, it appears closely related to Selective Projection Decay (SPD) [1], although applied in a different setting. SPD selectively applies regularization to layers whose updates are inconsistent, determined by the sign of the inner product between the negative gradient direction $(-g_t)$ and the accumulated parameter change $(\\theta_{t-1} - \\theta_0)$. A positive inner product indicates progress toward lower loss, whereas a negative value implies potential movement toward higher loss, prompting SPD to impose stronger penalties on such layers. The main difference is that SPD is designed for robust fine-tuning and therefore regularizes the $L_2$ distance between pretrained and finetuned weights $(\\theta_{t-1} - \\theta_0)$, while CWD regularizes the $L_2$ norm of the weights themselves $(\\theta_{t-1})$. Under this substitution, the core mechanism becomes mathematically similar, making the originality of CWD somewhat questionable, despite the fact that this paper provides stronger theoretical justification and demonstrates wider applicability across optimizers and domains.\n\n[1] Tian, Junjiao, Chengyue Huang, and Zsolt Kira. \"Rethinking weight decay for robust fine-tuning of foundation models.\" Advances in Neural Information Processing Systems 37 (2024): 22418-22440."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4jKpS3To33", "forum": "Gwe6gbGng5", "replyto": "Gwe6gbGng5", "signatures": ["ICLR.cc/2026/Conference/Submission14900/Reviewer_XuM6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14900/Reviewer_XuM6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14900/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974546092, "cdate": 1761974546092, "tmdate": 1762925242977, "mdate": 1762925242977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}