{"id": "iaPEM00wEs", "number": 140, "cdate": 1756729508534, "mdate": 1759898274386, "content": {"title": "When Large Multimodal Models Confront Evolving Knowledge: Challenges and Explorations", "abstract": "Large Multimodal Models (LMMs) store vast amounts of pretrained knowledge but struggle to remain aligned with real-world updates, making it difficult to avoid capability degradation when acquiring evolving knowledge. Furthermore, most current work focuses on exploring static textual knowledge injection, neglecting dynamic multimodal evolving knowledge injection, leaving the potential of LMMs for multimodal knowledge injection as an open question. To address this, we first propose a pipeline to construct MMEVOKE, a benchmark for evaluating LMMs' ability in multimodal evolving knowledge injection. MMEVOKE contains 9,422 samples spanning 159 subtypes. Then, based on extensive experiments with MMEVOKE, we reveal challenges such as poor injection performance and capability degradation in existing knowledge injection methods through knowledge injection tests and general capability tests. Finally, to tackle these challenges, we introduce knowledge augmentation and knowledge retention methods, finding that knowledge-aware augmentation strengthens knowledge injection performance, and that Data Replay and MoE methods effectively mitigate capability degradation.", "tldr": "This work introduces MMEVOKE benchmark to reveal challenges in knowledge injection and explores potential solutions.", "keywords": ["Evolving Knowledge Injection; Large multimodal model; Benchmark and Dataset"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ead32a34a2e8384b4ec6416340a5adc1c9dac4c0.pdf", "supplementary_material": "/attachment/cc444dd86dee4991fd0dc74ca7884492d602377e.pdf"}, "replies": [{"content": {"summary": {"value": "This paper addresses a crucial but underexplored challenge for LMMs, i.e., their ability to acquire and retain evolving multimodal knowledge. The authors argue that while LMMs possess vast pretrained knowledge, they struggle to remain consistent with the dynamically changing world. To tackle this, they introduce MMEVOKE, a large-scale benchmark comprising 9,422 multimodal samples spanning 159 subfields, collected from evolving entities and news since 2024. The authors further propose knowledge-aware augmentation (versus naive data augmentation) and retention strategies (e.g., replay and MoELoRA) to mitigate these challenges. They demonstrate that knowledge-aware augmentation improves both adaptation and retention, and replay/MoELoRA effectively alleviate degradation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work addresses a critical gap in current LMM research, i.e., the handling of evolving multimodal knowledge, which is becoming increasingly relevant as real-world data changes rapidly.\n2. The MMEVOKE benchmark is well designed, covering diverse modalities (text and images) and dynamic sources (CNN and Wikipedia). The pipeline is largely automated, reproducible, and designed for continuous updates.\n3. The paper evaluates a broad spectrum of existing paradigms (SFT, LoRA, RAG, Web Search, Commercial AI systems) and general capabilities across 12 benchmarks in 7 dimensions, offering a panoramic view of how LMMs behave under evolving knowledge injection."}, "weaknesses": {"value": "1. The proposed “knowledge-aware augmentation” and “retention” methods largely adapt existing paradigms (e.g., replay, MoE) rather than introducing fundamentally new algorithms. Their novelty lies in application and synthesis, not algorithmic innovation.\n2. Despite the automated data pipeline, Step 4 (manual selection) introduces subjectivity and may constrain full automation or large-scale scalability.\n3. Evolving knowledge has been widely studied in the context of LLMs, but there is few discussion in the related works and literature review, such as RealtimeQA, DyKnow, EvoWiki, etc."}, "questions": {"value": "1. Besides CEM/F1, could you provide qualitative or human evaluation on reasoning consistency, hallucination reduction, or factual grounding after injection?\n2. Could you elaborate on how GPT-4o’s summarization and augmentation process ensures semantic fidelity and avoids introducing bias?\n3. The conclusion hints at multi-stage or hybrid strategies. Have you conducted any preliminary experiments combining augmentation and replay/MoELoRA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2bgI9WMe5j", "forum": "iaPEM00wEs", "replyto": "iaPEM00wEs", "signatures": ["ICLR.cc/2026/Conference/Submission140/Reviewer_SWqj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission140/Reviewer_SWqj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761547301796, "cdate": 1761547301796, "tmdate": 1762915456842, "mdate": 1762915456842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MMEvoke, a benchmark for evaluating LMMs’ ability to incorporate evolving multimodal knowledge. MMEvoke is built via an automated pipeline with manual curation, using images and accompanying text scraped from Wikipedia and CNN across 159 subfields. It evaluates adaptation and retention for SFT, RAG, AI web agents, and Sufficient Context. The authors show that MMEvoke is challenging for current knowledge-injection methods, which also degrade general capabilities (e.g., instruction following, multi-turn QA). Additional studies indicate that knowledge-aware augmentation substantially improves adaptation, whereas knowledge-agnostic augmentation harms it; and that Replay/MoE-LoRA mitigates degradation better than EWC/LwF."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MMEvoke leverages real-world data to benchmark LMM’s abilities to adapt to evolving knowledge. The benchmark is comprehensive, containing 9,422 knowledge and covering 159 subfields.\n2. The paper conducts a comprehensive evaluation (12 benchmarks) spanning training- and retrieval-based methods, e.g., SFT (Full/LoRA) to RAG, commercial agents, and sufficient context, yielding useful cross-method comparisons.\n3. The analysis of knowledge-aware vs knowledge-agonistic augmentation and Replay/ MoELoRA provides practical insights into improving knowledge adaption while preserving  general capabilities.\n4. The writing quality is good. It is easy to follow the benchmark pipeline and evaluation details."}, "weaknesses": {"value": "1. It is unclear why injecting images of new knowledge is necessary, since the text often suffices to answer questions once the model recognizes the person and recalls textual knowledge. In both examples of “Geoffrey Hinton + Nobel Prize” and “Donald Trump + Assassination”, the text provides all the information needed to answer the question. Or, in the case of news for “Region” or “Business” categories, the images often don’t provide closely related information to the text.\n2. The work does not differentiate among types of knowledge injection, e.g., (1) **new entities** (Xiaomi SU7), (2) **known entities with new facts** (Geoffrey Hinton + Nobel Prize), and (3) **known entities with conflicting facts** (Lee Jae-myung: party leader vs. president).\n3. The pipeline cannot guarantee that knowledge is truly new to the pre-trained LMM, since the article date is not necessarily the first online appearance (e.g., iPhone 16, Xiaomi SU7 existed online before 2024; links below).\n4. For RAG, AI web agents, and Sufficient Context, performance is surprisingly low without an accompanying error analysis; failures could stem from (1) failure to recognize the image, (2) incorrect images from Google, or (3) inability to leverage the provided context.\n\nLinks:\n- https://carnewschina.com/2023/11/15/xiaomis-first-ev-revealed-in-china-to-be-called-xiaomi-su7/\n- https://www.forbes.com/sites/davidphelan/2023/11/03/apple-iphone-16-pro-will-bring-remarkable-upgrade-report-claims/"}, "questions": {"value": "1. Among the 9,422 knowledge updates, how many correspond to *known entities* versus *new entities* for the evaluated LMMs?\n2. Do you provide a baseline where the LMM is trained **only on text-based knowledge updates** (without images)?\n3. How do you ensure that knowledge updates scraped from CNN or Wikipedia do not already exist in the LMM’s **parametric knowledge**, given the counterexamples mentioned?\n4. How are the **knowledge-aware variants** constructed for both text and images?\n5. What are the causes of the **failure cases** for RAG, AI Web Agent, and Sufficient Context? Can you provide more details for error analysis.\n6. How is **Sufficient Context** constructed? Please provide more methodological details on this setup."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KyNnVEAACx", "forum": "iaPEM00wEs", "replyto": "iaPEM00wEs", "signatures": ["ICLR.cc/2026/Conference/Submission140/Reviewer_fBUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission140/Reviewer_fBUi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761846285087, "cdate": 1761846285087, "tmdate": 1762915456721, "mdate": 1762915456721, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper，the authors proposed MMEVOKE for multimodal evolving knowledge, which, serves as a evaluation dataset to measure LMMs’ evolving knowledge injection capabilities.  The authors conduct knowledge injection tests with Supervised FineTuning, Retrieval Augmented Generation, Web Search Engine, and Sufficient Context on MMEVOKE. Based on the experimental results, the authors find that existing methods exhibit poor knowledge adaptation performance and the performance of LMMs remains imperfect even with sufficient context."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. In this paper，the authors proposed MMEVOKE for multimodal evolving knowledge, which, serves as a evaluation dataset to measure LMMs’ evolving knowledge injection capabilities.  \n2. The authors conduct knowledge injection tests with Supervised FineTuning, Retrieval Augmented Generation, Web Search Engine, and Sufficient Context on MMEVOKE. Based on the experimental results, the authors find that existing methods exhibit poor knowledge adaptation performance and the performance of LMMs remains imperfect even with sufficient context."}, "weaknesses": {"value": "1. In my view, the authors overlook a type of method, knowledge editing, such as ROME ( Locating and Editing Factual Associations in GPT), AnyEdit (AnyEdit: Edit Any Knowledge Encoded in Language Models.) and MEMIT (Mass-Editing Memory in a Transformer). \n2.  In Benchmark construction, the authors compare offline versions of Wikipedia at different time points to identify new entries. But such a way cannot cannot guarantee that these entities will be unfamiliar to LMMs, since LMMs are pre-triained on a much larger dataset than Wikipedia."}, "questions": {"value": "See in Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3DUHoWO8hk", "forum": "iaPEM00wEs", "replyto": "iaPEM00wEs", "signatures": ["ICLR.cc/2026/Conference/Submission140/Reviewer_kvMY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission140/Reviewer_kvMY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913252285, "cdate": 1761913252285, "tmdate": 1762915456593, "mdate": 1762915456593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the knowledge retention and adaptation capabilities of LMMs in evolving knowledge. A benchmark named MMEVOKE is constructed to evaluate the performance. Two types of evaluations are conducted: 1) knowledge injection tests, which assess the ability of models to acquire newly introduced knowledge. 2) general capability tests, which examine the ability to preserve previously learned knowledge. Several approaches, including Supervised Fine-Tuning, Retrieval-Augmented Generation (RAG), Commercial AI Web Search Engines, and Sufficient Context Provision, are evaluated for their effectiveness in knowledge injection. Experimental results show that current methods perform poorly on the MMEVOKE benchmark, and LMMs still tend to produce incorrect answers even when sufficient contextual information is provided. Knowledge-aware augmentation demonstrates a clear improvement in knowledge injection performance. Regarding knowledge retention, the study finds that the general capabilities of LMMs degrade after knowledge injection. Methods based on Replay and MoELoRA are shown to effectively mitigate this degradation and help maintain the overall performance of LMMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed MMEVOKE benchmark serves as the first evaluation dataset designed to measure the evolving knowledge injection capabilities of Large Multimodal Models.\n2. This work systematically evaluates a wide range of approaches for their effectiveness in knowledge injection, including Supervised Fine-Tuning, Retrieval-Augmented Generation (RAG), Web Search Engines, and Sufficient Context Provision. The results indicate that knowledge augmentation substantially enhances model comprehension and adaptability in dynamic knowledge environments.\n3. This work further examines multiple approaches for knowledge retention, encompassing Replay-based methods, two classical continual learning techniques (EWC and LwF), and MoELoRA. The findings reveal that Direct Rehearsal (Replay) and Structured Separation (MoELoRA) effectively preserve previously acquired knowledge through retraining on historical data and isolating newly injected knowledge, respectively."}, "weaknesses": {"value": "1. It appears that the proposed concept of evolving knowledge injection is essentially a continual learning problem. The connection between evolving knowledge injection and continual learning remains unclear. A more detailed discussion of this relationship should be included in the Introduction or Related Work sections to better position this study within the broader research context.\n2. In the field of continual learning, several recent studies have explored the potential of large multimodal models, e.g., Large Continual Instruction Assistant (ICML 2025), Generative Multi-modal Models are Good Class-Incremental Learners (CVPR 2024). The CL methods evaluated in this work, such as EWC and LwF, are relatively outdated. It would be valuable to evaluate more recent and advanced approaches on the proposed MMEVOKE benchmark and to analyze whether their results align with or challenge the current findings."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dCgmfyMZy9", "forum": "iaPEM00wEs", "replyto": "iaPEM00wEs", "signatures": ["ICLR.cc/2026/Conference/Submission140/Reviewer_PjCG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission140/Reviewer_PjCG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979999993, "cdate": 1761979999993, "tmdate": 1762915456454, "mdate": 1762915456454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}