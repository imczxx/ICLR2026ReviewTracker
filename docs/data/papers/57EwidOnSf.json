{"id": "57EwidOnSf", "number": 1067, "cdate": 1756832645858, "mdate": 1759898229835, "content": {"title": "VideoMind: A Chain-of-LoRA Agent for Temporal-Grounded Video Reasoning", "abstract": "Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in text-based reasoning with large language models, multi-modal reasoning – especially for videos – remains limited. In this work, we fill this gap by introducing VideoMind, a novel video-language agent for temporal-grounded video reasoning. Our method involves two key innovations: (1) We identify four essential capabilities for grounded video reasoning and propose a role-based agentic workflow, comprising a planner to coordinate roles, a grounder for temporal event localization, a verifier to assess event candidates, and an answerer for question answering. (2) To efficiently integrate these roles during inference, we propose a novel Chain-of-LoRA mechanism, where a unified base model with multiple LoRA adapters is leveraged to enable seamless role switching, balancing efficiency and flexibility. Extensive experiments on 14 benchmarks across 3 tasks, including Grounded VideoQA, Video Temporal Grounding, and General VideoQA, demonstrate the effectiveness of the proposed scheme in advancing video agent, test-time scaling, and long-form video reasoning. Code, models, and data will be publicly available.", "tldr": "An agentic solution for long video understanding and video temporal grounding through LoRA switching.", "keywords": ["Multi-modal Agent", "Video Understanding", "Video Temporal Grounding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2198010499d34b1c22f15d59ade1838ae929830b.pdf", "supplementary_material": "/attachment/e7d4cbf42d92dd4caf650ded6b176b4655b595dd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes VideoMind, a video-language agent for temporal-grounded reasoning in long-form videos. The method's core is an agentic workflow comprising four distinct roles—Planner, Grounder, Verifier, and Answerer—that systematically decompose and address complex video queries. To efficiently integrate these roles, the authors introduce a novel \"Chain-of-LoRA\" mechanism, which leverages a unified base model with lightweight, switchable LoRA adapters for each role. The paper demonstrates the effectiveness of this approach through extensive experiments on benchmarks across three tasks: Grounded VideoQA, Video Temporal Grounding, and General VideoQA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The \"Chain-of-LoRA\" mechanism is a key strength, enabling a single model to efficiently switch between specialized roles. This design achieves a strong balance between performance and memory efficiency, making it a more practical alternative to deploying multiple, full models.\n\n* The paper is supported by a thorough and comprehensive evaluation across 14 diverse benchmarks spanning three different video understanding tasks. The claims are well-substantiated by detailed ablation studies that validate the contributions of individual components and the overall framework design."}, "weaknesses": {"value": "* The paper lacks a granular, independent evaluation of each module's reliability. For instance, there is no isolated analysis of the Planner's accuracy in task decomposition, the Grounder's precision, or the Verifier's classification performance. This makes it difficult to pinpoint sources of error or understand the individual limitations of each component.\n\n* The design of the Timestamp Decoder appears overly complex, and the paper fails to provide sufficient justification for this complexity. There is no direct, apple-to-apple comparison against simpler, more common alternatives (e.g., directly predicting timestamps through language modeling), making it difficult to assess the true benefit of the proposed decoder architecture.\n\n* The framework's reliance on an external model (GPT-4o) for query rephrasing undermines its contribution as a self-contained solution. This dependency introduces potential reproducibility issues and reliance on a proprietary API.\n\n* The paper overlooks several recently published and highly relevant papers in video temporal grounding [1-4].\n\n[1] Universal Video Temporal Grounding with Generative Multi-modal Large Language Models\n\n[2] TRACE: Temporal Grounding Video LLM via Causal Event Modeling\n\n[3] Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models\n\n[4] LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding"}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vaQveh8Sd0", "forum": "57EwidOnSf", "replyto": "57EwidOnSf", "signatures": ["ICLR.cc/2026/Conference/Submission1067/Reviewer_ocUG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1067/Reviewer_ocUG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760488224410, "cdate": 1760488224410, "tmdate": 1762915668779, "mdate": 1762915668779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VideoMind, a unified multimodal agent framework for long-video temporal grounding and question answering. It decomposes capabilities into four roles—Planner, Grounder, Verifier, and Answerer—and employs a Chain-of-LoRA strategy to efficiently invoke different roles by switching LoRA adapters on the same base large language model. The model is evaluated on 14 datasets spanning Grounded VideoQA, Video Temporal Grounding, and general VideoQA, reporting state-of-the-art or competitive results on multiple long-video benchmarks. The authors also commit to releasing the code and models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe method is structurally clear and engineering-efficient: it modularizes planning, temporal localization, verification, and answering, and realizes one model with multiple roles via LoRA switching, which reduces GPU memory while keeping flexible composition—an appealing design. \n2.\tThe Grounder includes a decoder specialized for temporal boundaries: a <REG>-triggered timestamp decoder and a multi-scale temporal pyramid, avoiding reliance on language tokens alone for time prediction; the technique is solid.\n3.\tThe Verifier adopts zoom-out recheck plus boundary markers: it expands both sides of a candidate segment and inserts <SEG-START/END>, performing binary validation to improve IoU and robustness, with clear training and inference definitions. \n4.\tEfficiency–effectiveness trade-off: compared with multi-model dispatch, Chain-of-LoRA achieves comparable accuracy while significantly reducing memory usage (4.2 GB vs. 16.6 GB), validating the design intent."}, "weaknesses": {"value": "1.\tNovelty boundary: While the multi-role agent and LoRA switching are practical, their difference from existing modular tool-based agents with vision APIs/components lies mainly in implementation form rather than a new paradigm; stronger theoretical grounding or unified learning evidence is needed to highlight originality.\n2.\t Joint training and error propagation: The authors acknowledge in limitations/future work that better joint optimization is required. The current pipeline-style chaining of roles may accumulate upstream errors, and the paper lacks quantitative analysis of error propagation. \n3.\tReproducibility details and open-source dependencies: Although the appendix lists training hyperparameters and dataset splits, implementation details for data reuse/annotation across roles remain brief. It is recommended to provide role-level prompts, sampling, and filtering scripts in the supplementary material to ensure others can reproduce the key results in Tables 2/3/5. \n4.\tChoice of baselines and strength of claims: Comparisons to GPT-4o and Gemini-1.5-Pro emphasize advantages on portions of long-video benchmarks, but input/context configurations and sampling strategies for these closed models are not fully aligned. Controlled comparisons with equal frame sampling, total frame count, and resolution are suggested. \n5.\tCross-domain generalization and failure cases: On domain-specific datasets such as Ego4D-NLQ, performance lags behind strong pre-trained specialists; the paper provides only brief explanations and lacks targeted analyses and error visualizations."}, "questions": {"value": "1.\tHave you attempted multi-task training that simultaneously optimizes the Planner/Grounder/Verifier on a shared backbone? If such attempts were unsuccessful, please share the main challenges and stabilization techniques. \n2.\tError sensitivity: Could you provide a sensitivity curve from the Grounder’s mIoU to the final QA accuracy, or bucket results on NExT-GQA by candidate-segment IoU to quantify the Verifier’s corrective gain? \n3.\tResource–performance trade-off: While Chain-of-LoRA has low VRAM usage, what is the time cost of repeated “zoom-out rechecks” and multi-candidate verification during inference? Can you report wall-clock comparisons and throughput curves on CG-Bench?\n4.\t Fair comparison with closed-source LMMs: Are frame sampling for long videos, maximum frame count, subtitle usage, temperature/sampling steps, etc., aligned? Could you include strictly controlled settings in the appendix to substantiate the stronger claim that the 2B/7B variants surpass GPT-4o/Gemini?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UEt5BQeo9j", "forum": "57EwidOnSf", "replyto": "57EwidOnSf", "signatures": ["ICLR.cc/2026/Conference/Submission1067/Reviewer_3Ldi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1067/Reviewer_3Ldi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760621620568, "cdate": 1760621620568, "tmdate": 1762915668566, "mdate": 1762915668566, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VideoMind, a novel video-language agent designed for temporal-grounded video reasoning. The authors identify a key limitation in existing models: while strong at text-based reasoning, they struggle to explicitly localize, revisit, and reason about specific temporal moments in long videos."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes the \"Chain-of-LoRA\", which is a highly practical solution to a major challenge in agentic AI. It directly addresses the need for multi-skilled agents without the prohibitive memory overhead of loading multiple full models.\n2. The paper's claims are backed by a comprehensive set of experiments across 14 benchmarks. The results are consistently strong."}, "weaknesses": {"value": "1. While the 'Chain-of-LoRA' mechanism is shown to be highly memory-efficient, how does the total wall-clock inference time of the full, multi-step VideoMind pipeline (Planner $\\rightarrow$ Grounder $\\rightarrow$ Verifier $\\rightarrow$ Answerer) compare to an end-to-end baseline model that performs reasoning in a single forward pass? Does the paper provide any latency benchmarks (e.g., in seconds per video)?\n2. How dependent are the results on the Qwen2-VL architecture? Could the 'Chain-of-LoRA' training approach be applied to other strong open-source VideoLLMs (e.g., InternVL3 or Qwen2.5-VL) to validate that this agentic pipeline provides a consistent benefit across different base models?\n3. The workflow itself, separate from the LoRA implementation, could be a significant contribution. Could this multi-step reasoning pipeline (plan $\\rightarrow$ ground $\\rightarrow$ verify $\\rightarrow$ answer) be simulated via a series of prompts on closed-source models like GPT-4o, Gemini 1.5 Pro or newer models? This would help isolate the performance gain of the agentic workflow from the specific model training."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LR2UzAg3Ud", "forum": "57EwidOnSf", "replyto": "57EwidOnSf", "signatures": ["ICLR.cc/2026/Conference/Submission1067/Reviewer_RtrU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1067/Reviewer_RtrU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468093419, "cdate": 1761468093419, "tmdate": 1762915668443, "mdate": 1762915668443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces VideoMind, a novel video-language agent designed to address the challenges of temporal-grounded reasoning in long-form videos. The authors identify that existing methods struggle to explicitly localize and revisit relevant video segments, a process that is natural for human cognition. The system decomposes the complex task of video reasoning into four distinct roles: a Planner to coordinate the workflow, a Grounder to localize temporal moments, a Verifier to assess and refine these moments, and an Answerer to synthesize the final response. This structured approach mimics a human-like process of breaking down problems, identifying evidence, and confirming details. To efficiently implement this multi-role system, the authors propose a novel inference-time strategy. A single base Large Multimodal Model (LMM) is equipped with multiple, role-specific Low-Rank Adapters (LoRAs). During inference, the agent can seamlessly and efficiently switch between roles (Planner, Grounder, Verifier) by activating the corresponding LoRA adapter, avoiding the significant memory overhead of loading multiple full models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The core idea of decomposing video reasoning into an agentic workflow of Planner, Grounder, Verifier, and Answerer is highly intuitive and effectively addresses the limitations of monolithic, end-to-end models. It formalizes a \"re-watching\" and \"self-verification\" loop that is crucial for complex video understanding.\n\n2. The Chain-of-LoRA mechanism is a standout contribution. It provides a highly practical solution to the problem of test-time scaling for multi-capability agents. By caching LoRA adapters in memory, it achieves the performance and flexibility of a distributed, multi-model system while maintaining the memory efficiency of a single model. The ablation study in Table 7 provides compelling evidence for this, showing it matches the performance of the \"All-Distributed\" approach with a fraction (≈1/4) of the memory cost.\n\n3. The individual components are well-designed. The Grounder's timestamp decoder, which moves beyond simple text-based timestamp generation, and the Verifier's \"zoom-in\" strategy with special boundary tokens (<SEG-START>, <SEG-END>) are thoughtful architectural choices that contribute directly to the model's strong grounding performance."}, "weaknesses": {"value": "1. The Planner currently relies on selecting one of three pre-defined reasoning plans based on the query. This approach may be brittle and lack flexibility when faced with novel tasks or complex queries that require a hybrid or dynamically generated sequence of steps. The paper does not explore how the system would perform on tasks that do not fit neatly into these templates.\n\n2. The roles are trained separately on curated datasets, with the Answerer role not being fine-tuned at all. This multi-stage training process, while effective, may be complex to replicate and sensitive to the quality and composition of the data for each stage. A deeper discussion on the challenges of this approach versus a more integrated, joint-training strategy would be insightful.\n\n3.The pipeline is inherently sequential. An error in an early stage can propagate and lead to a complete failure. For example, if the Planner mischaracterizes the query and chooses the wrong plan, or if the Grounder fails to identify any plausible moments, the subsequent steps are rendered useless. The paper could benefit from a discussion of the system's robustness and any potential fallback mechanisms to handle such cascading errors."}, "questions": {"value": "Why the authors miss the comparison with other video reasoning works, such as LongVILA-R1-7B[1], VIdeo-R1-7B[2]\n\n[1] Scaling RL to Long Videos\n[2] Video-R1: Reinforcing Video Reasoning in MLLMs"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AVl77zcb9T", "forum": "57EwidOnSf", "replyto": "57EwidOnSf", "signatures": ["ICLR.cc/2026/Conference/Submission1067/Reviewer_vJ8R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1067/Reviewer_vJ8R"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1067/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531251662, "cdate": 1761531251662, "tmdate": 1762915668334, "mdate": 1762915668334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}