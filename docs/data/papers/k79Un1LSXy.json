{"id": "k79Un1LSXy", "number": 362, "cdate": 1756736575858, "mdate": 1759898265320, "content": {"title": "Mitigating Mismatch within Reference-based Preference Optimization", "abstract": "Direct Preference Optimization (DPO) has become the de facto standard for offline preference alignment of large language models, but its reliance on a reference policy introduces a critical tension. \nDPO weighs each update relative to a reference, which stabilizes the training by regularizing the updates within a trusted region. This reliance becomes problematic for pessimistic pairs, where the reference model prefers the rejected response. For these pairs, DPO prematurely attenuates the gradient as soon as the policy margin ($\\Delta_\\theta$) merely beats the reference margin ($\\Delta_{\\mathrm{ref}}$) even if the policy is still wrong ($\\Delta_{\\theta}<0$). We name this failure premature satisfaction, which is a concrete form of the training–inference mismatch.\nReference-free objectives remove this mismatch by optimizing the absolute margin, but at the cost of discarding the stabilizing signal of the reference. We resolve this tension with Hybrid-DPO (HyPO), a drop-in modification to DPO that applies reference conditionally: HyPO behaves exactly like DPO when the reference is optimistic or neutral, and it treats the reference as neutral when it is pessimistic by replacing $\\Delta_\\theta-\\Delta_{\\mathrm{ref}}$ with $\\Delta_\\theta-\\max\\(0,\\Delta_{\\mathrm{ref}}\\)$. This one-line change strictly strengthens per-example learning signals on pessimistic pairs while preserving DPO’s objective form and computational cost. By conditionally debiasing the pessimistic reference signal, HyPO mitigates premature satisfaction; empirically, across preference alignment, HyPO improves inference-aligned metrics and achieves higher pairwise win rates. Our results provide evidence that direct preference alignment could be enhanced by conditionally debiasing the reference signal, rather than discarding it.", "tldr": "", "keywords": ["machine learning", "language models", "alignment", "preference optimization", "offline preference alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6be01f0c09aa62d8d35eaf9aa1ef69b3a282f806.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a critical issue in DPO --- the standard method for aligning large language models.\n\nDPO relies on a reference policy that can introduce a \"training-inference mismatch.\" Specifically, when the reference model incorrectly prefers the rejected response (pessimistic pairs, $\\delta_{\\text{ref}} < 0$), DPO prematurely weakens the learning signal even when the policy is still wrong, a phenomenon the authors call \\textbf{\"premature satisfaction\"}.\n\nThe authors introduce \\textbf{Hybrid Preference Optimization (HyPO)}: Behaves like standard DPO when reference is helpful ($\\delta_{\\text{ref}} \\geq 0$). Nullifies the reference when pessimistic ($\\Delta_{\\text{ref}} < 0$), reverting to absolute margin optimization.\n\nEmpirically, HyPO consistently and significantly outperforms both various DPO and strong reference-free baselines across a range of models and benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Core Innovation: First formalization and proof of the \"premature satisfaction\" phenomenon -- when the reference model is pessimistic ($\\Delta_{\\text{ref}} < 0$), DPO's gradient weight prematurely decays even when the policy is still wrong ($\\Delta_\\theta < 0$): $\\|\\nabla_\\theta \\ell_{\\text{DPO}}\\| \\leq \\beta e^{(-z)} \\|\\nabla_\\theta \\Delta_\\theta\\|$\n\n- Elegant Solution: Proposes a one-line code modification $\\Delta_\\theta - \\max\\{0, \\Delta_{\\text{ref}}\\}$ that mathematically guarantees two key properties: (1) when $\\Delta_{\\text{ref}} \\geq 0$, $w_{\\text{HyPO}} = w_{\\text{DPO}}$ maintains stability; (2) when $\\Delta_{\\text{ref}} < 0$, $w_{\\text{HyPO}} \\geq w_{\\text{DPO}}$ strengthens learning signal; also provides smooth version (Eq. 13) ensuring differentiability\n\n- Visualization Insights (Figure 1): Innovative heatmap visualization showing gradient weight distribution in $(\\Delta_\\theta, \\Delta_{\\text{ref}})$ space, intuitively revealing DPO's systematic problem in the pessimistic region\n\n- Empirical Support: Demonstrates that even SimPO-aligned strong reference still has $\\approx 45\\%$ pairs that are pessimistic, proving this is a pervasive issue that cannot be fully solved by \"better reference\" alone"}, "weaknesses": {"value": "1. Circular Dependency on Better Reference Model\n- **Page 7, Line 324-327 & Table 2**: The paper uses a SimPO-aligned model as the \"better reference,\" creating a fundamental circular dependency:\n  - To train HyPO effectively, you first need a strong aligned model (trained with SimPO or similar)\n  - This undermines the claim of HyPO being superior to reference-free methods like SimPO\n  - **Ablation shows severe degradation** (Table 2): Without better reference, performance drops by 4.1%, reducing the advantage over standard DPO from 8.9% to only 4.8%\n  - This suggests HyPO's gains may primarily come from using a better reference, not from the clipping mechanism itself\n- **Missing Critical Experiment**: No comparison of \"HyPO with SFT reference\" vs \"SimPO with no reference\" on equal footing\n\n2. **Training-Inference Mismatch Only Partially Addressed**\n- **Fundamental Issue** (Page 1, Line 50-53 & Page 4, Line 199-201): The paper claims to resolve training-inference mismatch, but:\n  - **At training**: HyPO still uses $Δθ - max{0, Δref}$, conditioning on reference model\n  - **At inference**: Uses only $Δθ$ with no reference whatsoever\n  - The mismatch is only **reduced** for pessimistic pairs, not eliminated\n- **On optimistic pairs** ($Δref ≥ 0$, which is ~55% of data per Figure 2): HyPO behaves identically to DPO, thus inheriting the exact same mismatch problem\n- **Contradiction with Main Claim**: \n  - Page 1, Abstract claims HyPO \"mitigates premature satisfaction\" and \"resolves tension\"\n  - But optimistic pairs still suffer from training-inference mismatch where reference is used in training but not inference\n  - This is inconsistent with the paper's critique of DPO\n- **Missing Analysis**: No comparison of how much mismatch remains (e.g., agreement rate between implicit reward ranking and likelihood ranking, as mentioned in Page 1, Line 52 for DPO's ~50%)\n\n3. **Theoretical Justification for Clipping Operation Lacks Rigor**\n- **Page 5, Eq. 11**: The choice of $\\max\\{0, Δref\\}$ appears ad-hoc without principled justification:\n  - Why clip at exactly 0? Why not clip at other values or use different functional forms?\n  - Why is $\\max\\{·\\}$ the optimal operation? What about soft-clipping, or learned thresholds?\n- **No Optimality Proof**: Unlike DPO which derives its objective from first principles (KL-regularized RL → Gibbs form → DPO loss), HyPO's clipping is presented as an intuitive fix without showing it's optimal\n- **Alternative Approaches Not Explored**:\n  - Adaptive clipping: $\\max\\{f(Δref), Δref\\}$ where f is learned\n  - Weighted combination: $αΔθ + (1-α)\\max\\{0, Δref\\}$ where $α$ depends on confidence\n  - Smooth transitions instead of hard clipping\n- **Missing Theoretical Analysis**: No proof that this specific form of clipping leads to convergence to the desired policy, or that it's better than alternatives\n- **Page 6, Eq. 13**: While softplus smoothing is provided, the paper doesn't analyze whether smoothness parameter $α$ matters (not in Figure 4's sensitivity study)"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QsqXZyyHiD", "forum": "k79Un1LSXy", "replyto": "k79Un1LSXy", "signatures": ["ICLR.cc/2026/Conference/Submission362/Reviewer_MKDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission362/Reviewer_MKDX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997670763, "cdate": 1761997670763, "tmdate": 1762915503366, "mdate": 1762915503366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce a one-line change to DPO, by replacing the reference model margin $\\Delta_{ref}$ with a relu-regularized margin $\\max(0, \\Delta_{ref})$.\nThis change is motivated by the problem that: when  $\\Delta_{ref} < 0$, the margin $\\Delta_\\theta$ cannot deviate too far from the reference model, resulting in unintended optimization result: $0 > \\Delta_\\theta > \\Delta_{ref}$.\nBy applying the RELU function, the learned margin will be encouraged to be positive.\nEmpirical results on different models and benchmarks demonstrate the effectiveness of this simple modification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Well Motivated Solution: The paper clearly identifies a potential issue of reference model in DPO optimization and provides a simple yet effective solution ($\\max(0, \\Delta_{ref})$) to address it.\n2. Stable Empirical Improvement: The proposed method is empirically validated on different models and benchmarks to improve over several existing DPO variants. The downstream task evaluation further demonstrates the proposed method avoids performance degredation."}, "weaknesses": {"value": "1. **Introduced Hyperparameter**: In practice, the objective is implemented with $\\Delta_\\theta - \\max(\\Delta_{ref}, \\gamma) - h$ which introduces two additional hyperparameters $\\gamma$ and $h$ to DPO. This complicates the hyperparameter tuning process in practice.\n\n2. **Concerns about Ablation Results**: The motivation of the proposed HyPO method is based on the \"premature satisfaction\" problem, which the authors solve by applying the relu function to the reference margin $\\max(\\Delta_{ref}, \\gamma)$.\nHowever, Figure 4 shows that the empirical result is not sensitive to the choice of $\\gamma$. A more reasonable explanation is needed to justify the effectiveness of the relu function.\\\nMoreover, the ablation study on each component in Table 2 misses the setting of DPO + relu on $\\Delta_{ref}$, without better reference and home advantage. It is unclear how much improvement comes from the relu function itself."}, "questions": {"value": "Does adding an SFT loss on the chosen response helps mitigate the premature satisfaction problem? It would be interesting to compare with this alternative solution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4trzkx3nA3", "forum": "k79Un1LSXy", "replyto": "k79Un1LSXy", "signatures": ["ICLR.cc/2026/Conference/Submission362/Reviewer_obfj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission362/Reviewer_obfj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762165733299, "cdate": 1762165733299, "tmdate": 1762915503185, "mdate": 1762915503185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a key limitation in Direct Preference Optimization (DPO), termed **“premature satisfaction”**, where pessimistic reference models weaken learning signals too early. To resolve this, the authors propose **Hybrid-DPO (HyPO)** — a simple yet principled modification that conditionally removes the negative influence of the reference while maintaining DPO’s stability. Extensive experiments show that HyPO consistently improves alignment performance across diverse settings, confirming both its effectiveness and scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-organized, moving naturally from the identification of DPO’s weakness to the proposal of a concise and theoretically grounded fix, with clear motivation.\n\n2. HyPO establishes a thoughtful balance between reference-based and reference-free optimization, combining their respective advantages while eliminating their key drawbacks.\n\n3. The authors provide strong evidence that pessimistic samples are a real and significant issue, and this observation directly supports their design and problem formulation.\n\n4. The authors conduct systematic experiments under various configurations, demonstrating that the proposed method is both effective and robust without adding computational overhead."}, "weaknesses": {"value": "1. The paper uses sequence-level log-likelihood difference to decide “pessimistic”, but explicitly “does not length-normalize”. Although evaluation uses length-controlled metrics (e.g., AlpacaEval LC), the training-time pessimism criterion remains length-sensitive.\n\n2. The paper fails to distinguish cases where a negative reference margin reflects genuine “pessimism” from cases where it arises due to noisy or incorrect preference labels, in which the reference may in fact be the more reliable anchor.\n\n3. HyPO’s hard threshold switches the objective at the sample level. Such hard switching can induce non-smooth optimization landscapes. Does the mixed objective’s fixed point correspond to an interpretable KL-regularized or f-divergence–constrained optimum? \n\n4. Lacking some recent related work, e.g., AlphaDPO [1].\n\n[1] Junkang Wu, et. al. AlphaDPO: Adaptive Reward Margin for Direct Preference Optimization. In ICML 2025."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tv2QHNrhLI", "forum": "k79Un1LSXy", "replyto": "k79Un1LSXy", "signatures": ["ICLR.cc/2026/Conference/Submission362/Reviewer_Af2K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission362/Reviewer_Af2K"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762695984714, "cdate": 1762695984714, "tmdate": 1762915503019, "mdate": 1762915503019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Hybrid-DPO (HyPO), a variant of DPO that aims to mitigate a key failure mode termed premature satisfaction, where DPO prematurely attenuates gradients for pessimistic pairs (when the reference model prefers the rejected response). The proposed modification clips the reference margin (replacing $\\Delta_{ref}$ with $\\max\\\\{0, \\Delta_{ref}\\\\}$) to conditionally remove the influence of pessimistic references. Empirical results across multiple benchmarks (AlpacaEval and Arena-Hard) show improvements over existing benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work is well-motivated. The paper identifies a concrete limitation of DPO and proposes a simple yet effective idea to fix it. \n\n2. Strong numerical behavior. The experiments on Mistral and Llama are reported with significant improvements over existing baseline methods (e.g., DPO and SimPO)."}, "weaknesses": {"value": "1. The Qwen series models are missing. It would be better to also include one of them to run the tests.\n\n2. The sample code is not provided. It would be better to share more details on experimental configurations."}, "questions": {"value": "Please see the weakness part.\n\nOverall, I find the paper borderline but leaning toward acceptance. I'm open to reevaluating this work according to the further discussions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k7c3RoMgeU", "forum": "k79Un1LSXy", "replyto": "k79Un1LSXy", "signatures": ["ICLR.cc/2026/Conference/Submission362/Reviewer_vgnX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission362/Reviewer_vgnX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission362/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762767244063, "cdate": 1762767244063, "tmdate": 1762915502921, "mdate": 1762915502921, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}