{"id": "BoOu1X1fkT", "number": 20525, "cdate": 1758307076933, "mdate": 1759896973457, "content": {"title": "Calibration-Guided Quantile Regression", "abstract": "Obtaining finite-sample guarantees for predictive models is crucial for settings where decisions have to be made under uncertainty. This has motivated works where models are trained, then recalibrated to yield coverage guarantees. However, doing so often significantly increases model entropy, i.e., it becomes less sharp, making the model less useful. To mitigate this, recent works have increasingly attempted to achieve a balance between good calibration and sharpness. However, these methods often involve deriving completely new, poorly understood loss functions or employing a complex and computationally intensive training pipeline. Moreover, the trade-off between sharpness and calibration is frequently unclear for these methods. In this work, we argue for making the trade-off explicit by choosing the sharpest model subject to some pre-set miscalibration tolerance. To achieve this, we present a simple yet effective approach that combines two established metrics in a novel fashion: we minimize the pinball loss while controlling for calibration using a held-out dataset. Coincidentally, our method motivates a hitherto unexplored analysis: we explicitly compute the Pareto front achieved across methods in terms of sharpness and calibration, and compare performance against this Pareto front. Our approach consistently outperforms various state-of-the-art methods in terms of various Pareto front-related metrics, even though the competing methods are more complex and computationally expensive.", "tldr": "Calibration-guided quantile regression generates the best approximation of the calibration-sharpness Pareto front.", "keywords": ["Deep learning", "quantile regression", "frequentist statistics", "calibration", "pareto front"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b291b8888299cee1a818671d24aa036282ebc21f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors emphasize the importance of calibration in quantile regression and propose to minimize the quantile loss subject to the constraint that the calibration error on a validation subset remains below a specified threshold. However, the algorithm and accompanying theory do not appear to actually solve this constrained optimization problem, and the procedure as written provides no guarantee that it will converge to a solution that satisfies the constraint. The authors also claim that their method performs fitting and calibration simultaneously and may therefore improve upon two-stage approaches that first train a model and then apply post-hoc calibration. As presented, however, this claim is not convincingly supported by either the algorithmic design or the theoretical results."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The authors recognize that calibration is important in quantile regression and propose an objective that is to minimize the quantile loss under the constraint that the solution is calibrated up to some threshold."}, "weaknesses": {"value": "1. The claim and key motivation of the paper that post-hoc calibration necessarily worsens `\"sharpness'' or \"discrimination'' is not well justified. Standard calibration methods—such as Platt scaling in classification—fit a secondary model that minimizes a proper loss (e.g., log-likelihood) on validation data, and therefore aim to improve predictive quality rather than degrade it. In the context of quantile regression, an analogous calibration step could be performed using the quantile loss itself. While calibration may increase interval width when the original model is overconfident, this is a correction for miscalibration rather than a flaw of post-hoc calibration methods.\n\n\n\n2. In Algorithm~1, it is unclear why the procedure would ever converge to a valid solution satisfying the constraint of objective (12). The gradient descent update to the parameter $\\theta^\\*$ occurs only when the validation ECE falls below $\\varepsilon$  and the sharpness decreases, but there is no guarantee that this condition will ever be met. If it is not, the algorithm simply returns the initial parameter $\\theta$, regardless of how many gradient steps are taken. For small values of $\\varepsilon$, there is no reason to expect gradient descent to produce a model that satisfies both constraints, and the algorithm provides no theoretical or practical mechanism ensuring feasibility, termination, or even monotonic improvement.\n\n3. The proposed objective in (12) seeks to minimize the loss subject to the constraint that the calibration error is at most $\\varepsilon$. However, Algorithm~1 does not actually optimize this constrained objective. Instead, it performs unconstrained gradient descent on the loss and then selects, from what may be an empty set, the iterates that happen to satisfy the calibration constraint. This is not equivalent to solving the constrained optimization problem in (12). A principled approach would require directly minimizing the loss over the feasible region---that is, performing a true constrained optimization in which the calibration constraint is enforced throughout the procedure. Although this seems to be the conceptual goal of the paper, it is neither implemented nor addressed algorithmically, and carrying it out in practice is nontrivial.\n\n4. Theorem 3.2 does not appear to be useful in its current form, and it may even be incorrect. It is unclear whether $m(\\varepsilon)$ is intended to denote the number of gradient descent steps required for convergence, but as noted above, the procedure may not converge at all, in which case the theorem would be false as stated. Even if convergence were guaranteed, $m(\\varepsilon)$ is likely to be extremely large in practice, making the stated high-probability guarantee effectively vacuous: the error term $\\gamma$ would need to be so large for the bound to hold with high probability that the result provides no meaningful assurance about the returned model. Substantially stronger theoretical development is needed. Algorithm~1 is too generic to support the type of guarantee claimed in Theorem 3.2. A useful theory would need to answer basic questions such as: under what conditions does the method converge, and at what rate? In general, no such convergence can be expected without additional assumptions, and any rigorous result will likely require specifying a concrete optimization procedure (e.g., gradient descent with fixed step size on a convex objective) and proving guarantees in that setting."}, "questions": {"value": "My questions are in direct reference to the weaknesses discussed above:\n\n1. Can the authors justify the claim that post-hoc calibration necessarily harms sharpness or discrimination? Are there empirical or theoretical results demonstrating this effect specifically in quantile regression?\n\n2. What guarantees, if any, ensure that Algorithm~1 will produce a parameter satisfying the calibration constraint rather than simply returning the initialization? In practice, does the early-stopping condition ever trigger?\n\n3. How is the constrained optimization problem in (12) being solved, given that Algorithm~1 performs only unconstrained gradient descent and then filters iterates after the fact?\n\n4. In Theorem~3.2, what is the precise meaning of $m(\\varepsilon)$, and under what assumptions is convergence to such a point guaranteed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ApF3RaRGrU", "forum": "BoOu1X1fkT", "replyto": "BoOu1X1fkT", "signatures": ["ICLR.cc/2026/Conference/Submission20525/Reviewer_t1Nn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20525/Reviewer_t1Nn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956172204, "cdate": 1761956172204, "tmdate": 1762933944506, "mdate": 1762933944506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CGQR, addressing the fundamental trade-off between calibration and sharpness. The core insight is: instead of the standard train and calibrate approach, which hinders sharpness, CGQR introduces a regularizer lambda*L_calib and an early stopping criterion to trade off between calibration and sharpness by allowing miscalibration up to some pre-specified tolerance. After training, the model is frozen, and a recalibration step is applied. The authors use the Pareto frontier to evaluate. The claim is that CGQR produces better sharpness-calibration trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple solution to trace a pareto frontier. Based on my understanding, the method uses lambda as a knob to control how quickly the model learns calibration, which in turn implicitly trades off sharpness by determining the early-stopping point. This is a novel and powerful method for tracing a Pareto frontier. \n2. Strong and extensive empirical results, albeit on small UCI datasets."}, "weaknesses": {"value": "1. The methodology needs to be clearer. For example, calibration on the validation dataset and then doing recalibration with a separate calibration set was not immediately clear and was frustrating to navigate. The presentation of the main method should be cleaner and not use the same terminology for separate procedures.\n2. Based on my understanding, the entire method hinges on a key assumption that regularizing the loss function with L_calib on D_train will improve performance. This may work on small UCI datasets, but potentially in larger and more multi-dimensional learning, a model could overfit without that learning transferring to D_val. A more comprehensive discussion and analysis of how model overfitting would affect the performance would significantly improve the quality of the work.\n3. One of the keys to the method is this lambda*L_calib term as a regularizer. The paper seems to be missing an ablation study comparing the existing method to lambda=0 with the early stopping rule. This ablation would be crucial for isolating the true benefit of your lambda*L_calib term. It may be possible that the D_val stopping rule by itself provides most of the benefit."}, "questions": {"value": "1. Based on weakness 2, your method relies on L_calib on D_train being a better proxy for the true calibration on D_val. Have you ever observed this proxy fail? Is it possible for a model to overfit and minimize L_calib on D_train without this improvement ever translating to D_val, thus causing the early-stopping rule to never be met?\n2. In Table 1, IGD for CGQR is 0.03±0.04. Is this a typo? The IGD appears to be quite small compared to other baselines. Furthermore, why is Cali in the IGD row bolded?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N9plPSGcJH", "forum": "BoOu1X1fkT", "replyto": "BoOu1X1fkT", "signatures": ["ICLR.cc/2026/Conference/Submission20525/Reviewer_ALqY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20525/Reviewer_ALqY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966164994, "cdate": 1761966164994, "tmdate": 1762933944063, "mdate": 1762933944063, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Calibration-Guided Quantile Regression (CGQR), a simple procedure that explicitly trades-off between calibration and sharpness. The procedure is conducted by (1) minimizing the simultaneous pinball loss during training and (2) on a held-out validation set, keeping the sharpest checkpoint whose empirical ECE is below a specified tolerance. A standard finite-sample analysis is provided. The paper evaluates models by computing Pareto fronts in the calibration-sharpness trade-off(ECE vs. average 95% interval length) and reports hypervolume (HV), GD, and IGD metrics across real datasets. Empirically, CGQR outperforms other methods within Pareto-frontier-related metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The simultaneous pinball loss is simple and easy to implement.\n\n2. Theoretical guarantee is provided.\n\n3. Empirical results show the effectiveness of reaching Pareto frontiers and obtaining good trade-offs."}, "weaknesses": {"value": "1. The Pareto optimality of this method is the result of always keeping the sharpest checkpoint with respect to 95\\% coverage interval length. But if any other sharpness measurement (e.g., 75\\% coverage interval length) is to be considered in evaluation, we might not expect the checkpoints are still at the Pareto frontiers or outperforming other methods.\n\n2. The implementation details of how to do gradient descent on the simultaneous pinball loss should not be omitted from the main text. I think that procedure involves an integration with respect to $\\tau$ from 0 to 1, which is non-trivial.\n\n3. Theoretical method is standard and not novel."}, "questions": {"value": "1. I don't know if the word \"trade-off\" best describes the method; the method cannot directly control which level of ECE it will get. Consider we are training the model for infinitely long time. The whole training trajectory is fixed no matter how you select $\\epsilon$. The role of $\\epsilon$ is just setting the filtering threshold of checkpoints, and the improvement highly relies on the spread/randomness of checkpoints. I would like to see the overall trend along the training trajectories, which I think is the critical property of this method.\n\n2. The final implementation includes three ingredients: (1) the loss, (2) the optimizer, and (3) the model structure. The authors spend almost all the efforts in explaining the first one. But what if you change the optimizer (so that the training trajectory is changed)? Why the optimizer matters: Imagine there is a strong enough oracle optimizer that can reach the minimum of the loss in just one step, then your method will definitely fail in that sense (only one checkpoint now). What if you change the model structure? I think more discussions should be included.\n\n3. Typo: $f(q(X, \\tau)+|X)$ in equation (8); Table 1 CALI IGD should not be bold."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WGVPsD6dNL", "forum": "BoOu1X1fkT", "replyto": "BoOu1X1fkT", "signatures": ["ICLR.cc/2026/Conference/Submission20525/Reviewer_WBvh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20525/Reviewer_WBvh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762067091340, "cdate": 1762067091340, "tmdate": 1762933943615, "mdate": 1762933943615, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method (CGQR) for regression uncertainty quantification that aims to explicitly balance calibration and sharpness. The approach trains a quantile regression model using the pinball loss and selects the sharpest model that satisfies a user specified calibration error threshold (ECE \\leq \\epsilon), thereby tracing out the Pareto front between calibration and sharpness. The authors provide finite sample guarantees and evaluate their method on several standard regression datasets and a nuclear fusion prediction task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is clearly written and well structured.\n2. The motivation of balancing calibration and sharpness in predictive uncertainty is relevant for practical applications.\n3. The method is simple, easy to implement, and provides explicit control over the calibration sharpness trade-off.\n4. The empirical evaluation is thorough, covering a range of datasets and comparing with several baselines."}, "weaknesses": {"value": "1. The main contribution is a straightforward combination of standard quantile regression (pinball loss minimization) with a validation set based selection for calibration. The Pareto front analysis is more diagnostic than methodological. There is no new loss function, model architecture, or significant algorithmic contribution.\n2. The theoretical results are direct applications of standard concentration inequalities and union bounds, and do not provide new insights or tight guarantees. \n3. The method does not enforce non crossing quantiles, which can be problematic for interpretability. The choice of miscalibration tolerance (\\epsilon) is left to the practitioner, with little guidance or analysis of sensitivity."}, "questions": {"value": "1. The paper frames the calibration sharpness trade-off as a constrained optimization problem, but the solution is essentially a validation-set-based selection. Is there a principled way to jointly optimize for both objectives, perhaps via a Lagrangian or multi-objective optimization framework? How does the proposed approach compare, in theory, to such alternatives?\n2. The method does not enforce non-crossing quantiles, which can lead to interpretability issues. Are there theoretical guarantees or practical strategies to ensure monotonicity of quantile functions within the CGQR framework? How would enforcing non crossing constraints affect the calibration and sharpness trade off?\n3. The method relies on a user specified miscalibration tolerance. Can the authors provide theoretical or empirical guidance on how to select \\epsilon? How sensitive are the results to this parameter, and what are the implications for model selection in practice?\n4. The approach is built around the pinball loss for quantile regression. How would the theoretical guarantees and Pareto front analysis extend to other uncertainty quantification frameworks? Are there limitations to the generality of the proposed method?\n5. How does the finite-sample coverage guarantee of CGQR compare theoretically to conformal prediction methods, which provide distribution-free coverage guarantees? Can the authors clarify the advantages or limitations of their approach in this context?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cEb6ZS2wqO", "forum": "BoOu1X1fkT", "replyto": "BoOu1X1fkT", "signatures": ["ICLR.cc/2026/Conference/Submission20525/Reviewer_NdLJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20525/Reviewer_NdLJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762691456593, "cdate": 1762691456593, "tmdate": 1762933942836, "mdate": 1762933942836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}