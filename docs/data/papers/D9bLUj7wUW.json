{"id": "D9bLUj7wUW", "number": 12746, "cdate": 1758210009090, "mdate": 1759897489826, "content": {"title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "abstract": "Trustworthy language models should provide both correct and verifiable answers. However, citations generated directly by standalone LLMs are often unreliable due to hallucinations. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during (continual) pretraining, without test‑time retrieval, by revising the training process. To study this, we construct **CitePretrainBench**, a benchmark that mixes real‑world corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and probes both short‑form (single fact) and long‑form (multi‑fact) citation tasks. Our approach follows a two-stage process: (1) Continual-pretraining to index factual knowledge by binding it to persistent document identifiers; (2) Instruction tuning to elicit citation behavior. We introduce **Active Indexing** for the first stage, which creates generalizable, source-anchored bindings by augmenting training with synthetic data that (i) restate each fact in diverse, compositional forms and (ii) enforce bidirectional training (source$\\to$fact and fact$\\to$source). This equips the model to both generate content from a cited source and attribute its own answers, improving robustness to paraphrase and composition. Experiments with Qwen‑2.5‑7B and 3B show that Active Indexing consistently outperforms a Passive Indexing baseline, which simply appends an identifier to each document, achieving citation precision gains of up to 30.2\\% across all tasks and models. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16× the original token count. \nFinally, we show that internal citations complement external ones by making the model more robust to retrieval noise.", "tldr": "We introduce Active Indexing, a training strategy that enables language models to provide reliable internal citations without test-time retrieval, supported by our new CitePretrainBench benchmark.", "keywords": ["Large Language Models; Knowledge Attribution; Interpretability and explainable AI; Citations"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3dc24b1a6b5681362edb6b61a41ffff780dada9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies whether large language models can produce reliable, verifiable citations to the documents they were (continually) pretrained on, without using test-time retrieval. It introduces a two-stage framework, including continual pretraining with “Active Indexing” plus citation-style instruction tuning and a new benchmark, CitePretrainBench, to evaluate both short- and long-form citation over mixed sources such as Wikipedia, Common Crawl, arXiv, and synthetic unseen documents."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem is well-motivated and distinct from standard RAG: it targets internal, training-time attribution rather than test-time retrieval.\n2. The proposed Active Indexing pipeline yields clear, reported gains over passive / repetition baselines on multiple QA-style tasks."}, "weaknesses": {"value": "1. A first concern is cost and practicality: the gains come from heavy, LLM-generated active augmentation, including intra-document QA, cross-document synthesis, bidirectional objectives (scaling to 2.75B extra tokens), which means the approach improves because we invest substantial additional training signal, not necessarily because it is an inherently lightweight replacement for retrieval. A deployment-oriented reader will wonder how often such augmentation must be repeated when the corpus changes.\n2. The evaluation benchmark is authored by the paper itself: although it is diverse, it is still tailored to documents with clean, human-readable titles and to a closed identifier space; it remains unclear how well the method transfers to messier enterprise or web-scale settings with noisy IDs, partial documents, or no stable titles.\n3. Many of the reported improvements start from very low baselines (e.g., citation precision ≈2–5% for passive or instruction-only models), so relative lifts of “up to 30 points” overstate the absolute reliability; even the best models still lag behind their own answer correctness, and long-form settings (ELI5, ASQA) remain far from robust attribution.\n4. Comparison with stronger external-citation / RAG baselines is informative but not exhaustive: the paper shows internal and external are complementary and that a hybrid is best, yet the hybrid still leaves a noticeable gap to the “oracle” combination, suggesting that the proposed training does not fully solve conflict resolution between memorized and retrieved evidence."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hR1tOBwznL", "forum": "D9bLUj7wUW", "replyto": "D9bLUj7wUW", "signatures": ["ICLR.cc/2026/Conference/Submission12746/Reviewer_icLP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12746/Reviewer_icLP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868372149, "cdate": 1761868372149, "tmdate": 1762923565264, "mdate": 1762923565264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how to make Large Language Models (LLMs) reliably cite the specific documents from their pretraining data without needing an external retrieval system (e.g. RAG) at inference time. The authors aim to solve the problem of citation hallucination and create a retrieval-free alternative to RAG. The core contribution is a training strategy called **Active Indexing**. The authors contrast this with what's called **Passive Indexing** that has been studied in prior work, where a document identifier (like its title) is simply appended to the end of the text during pretraining. The authors constructed a training corpus by collecting common factual sources such as wikipedia,  then created the CitePretrainBench for evaluation by using four existing question-answering datasets (ASQA, ELI5, SciQAG, and RepliQA) whose answers are grounded in that specific corpus. Their results find supporting evidence of active indexing being a superior method than passive indexing (subject to token replay)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper systematically conduct empirical analysis on knowledge attribution methods that moves beyond proposing a new method. By directly contrasting simple \"Passive Indexing\" with their more sophisticated \"Active Indexing,\" the work provides the community with a clear understanding of how different training strategies for attribution directly impact a model's citation accuracy. Furthermore, their scaling experiments offer crucial insights, demonstrating that this citation capability consistently improves with the amount of augmented data, which helps quantify the trade-offs and charts a path for future work in this domain."}, "weaknesses": {"value": "**Novelty Concern**: The proposed training pipeline still largely mirrors Khalifa et al. (2024): (i) continue pretraining on a corpus where each document is tagged with a unique identifier, and (ii) instruction-tune the model to answer questions and emit supporting identifiers, i.e., to ground answers in specific training documents. The paper positions “Active Indexing” as the main advance beyond this baseline. However, much of Backward Active Indexing looks like a scaled-up version of Khalifa’s fact→source supervision: the model is trained on synthetic instructions whose answers integrate facts from multiple documents, and each generated factual statement is annotated with the supporting set of document titles.\n\nConceptually, this extends Khalifa’s single-document attribution setup to multi-document synthesis and per-claim citation sets, which is present in prior work. See Patel et al. (Towards Improved Multi-Source Attribution for Long-Form Answer)\n\n**Scope and stability of CitePretrainBench**: CitePretrainBench is described as a benchmark that “mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents” and evaluates both short-form (single-fact) and long-form (multi-fact, multi-source) citation. The “novel, unseen” part is primarily RepliQA, which the paper characterizes as short-form QA over “fictional, synthetic documents created post-training cutoff (Monteiro et al., 2024).” \n\nIn other words, the “new knowledge” is largely imported from an existing dataset that was written after the model’s cutoff, not newly created here. This raises two issues:\n- The benchmark’s claim to test the model’s ability to “learn and cite new knowledge” depends on RepliQA still being out-of-distribution for the underlying models. That claim will become weaker as newer base models inevitably ingest RepliQA-like synthetic corpora (or RepliQA itself) during pretraining, which is outside the authors’ control.\n\n- Aside from that post-cutoff synthetic set, the rest of CitePretrainBench reuses existing QA benchmarks (ASQA, ELI5, SciQAG, RepliQA) and established correctness metrics such as Exact Match Recall, Claim Recall, and FreshEval-style LLM grading, plus citation precision/recall that check whether cited titles actually entail each generated claim. \n\nThe unification work — consolidating Wikipedia / Common Crawl / arXiv / synthetic docs under a single deduplicated title space, constraining decoding so the model can only cite that space, and evaluating multi-document, per-claim attribution in long-form answers — is valuable infrastructure. That said, the benchmark is still largely built out of pre-existing datasets and inherited metrics, and its “novel knowledge” component is fragile over time. \n\n**Memorization vs. Generalization Claim**: In Section 5.3, you proposed to use FullDoc -> Partial Doc -> GoldQA -> ModelQA as tasks that shift from testing memorization to generalization. This framing seems to assume that “memorization” = being able to associate a document ID with its own content, and “generalization” = being able to recover the same ID when you only see downstream QA-style text. But I’m not convinced that this setup is actually isolating generalization, as opposed to just increasing task difficulty under progressively less evidence."}, "questions": {"value": "1. Your experiments only included Qwen3B and Qwen7B. Does other model families and scales exhibit similar scaling behavior when trained with active indexing? For example, the paragraph \"Model Size Matters.\" in Section 5.2 needs much more than two models to make claims about patterns on model size.  \n\n2.  Section 5.1 suggests that when combined together, Active Indexing dataset is much larger than Passive Indexing baseline. Can you reveal how big is the passive indexing dataset in Section 5.1, or make a table to include comparison to how big the instruction tuning baseline dataset is.  \n\n3. Building on question 2, the comparison between Passive Indexing and Active Indexing is currently unfair. Despite adding Replay, Passive Indexing training has less fresh tokens than Active Indexing. Do you observe Active Indexing having consistent advantage over passive indexing when the dataset is truncated into the same size as Passive Indexing?  \n\n4. Section 5.4 poses an interesting question about the advantage of internal citation when external citation retrieval quality differs. In Figure 3, the x-axis represents retrieval quality. Can you use a paragraph or a figure to explain how we can quantitatively interpret what's called \"retrieval quality\" here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QLmkxcKNNm", "forum": "D9bLUj7wUW", "replyto": "D9bLUj7wUW", "signatures": ["ICLR.cc/2026/Conference/Submission12746/Reviewer_3iUa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12746/Reviewer_3iUa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881661875, "cdate": 1761881661875, "tmdate": 1762923564832, "mdate": 1762923564832, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CitePretrain, a method for enabling LLMs to perform reliable internal citations—attributing answers to documents seen during continual pretraining without requiring test-time retrieval. The authors construct CitePretrainBench, a benchmark mixing real-world corpora (Wikipedia, Common Crawl, arXiv) with novel documents for both short-form and long-form citation QA tasks. Their approach uses two-stage training: (1) continual pretraining to index factual knowledge by binding it to document identifiers, and (2) instruction tuning to elicit citation behavior. They propose Active Indexing, which augments training with synthetic data that restates facts in diverse forms and enforces bidirectional training (source→fact and fact→source), significantly outperforming Passive Indexing (simply appending identifiers). Experiments on Qwen-2.5-7B/3B show Active Indexing improves citation precision by up to 30.2%, with performance scaling with augmentation data (up to 16×) and model size. The paper also demonstrates that internal and external citations are complementary, with hybrid approaches performing best across varying retrieval quality."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.  Unlike prior work limited to synthetic biographies, CitePretrainBench introduces the first rigorous benchmark emphasizing real-world complexity across Wikipedia, Common Crawl, arXiv, and novel documents, with both short-form and long-form citation tasks.\n\n2. Active Indexing is conceptually elegant and addresses passive indexing's core limitation: failure to ground non-verbatim content. By generating diverse synthetic QA pairs and enforcing bidirectional training (source→fact, fact→source), it achieves substantial improvements (up to 30.2% citation precision gains). Performance scales favorably—continuing to improve at 16× augmentation without saturation, indicating significant headroom.\n\n3. Thorough ablations reveal that both fact variation and active supervision are essential. The paper demonstrates that internal citations excel under poor retrieval, while external citations perform better with strong retrieval, with hybrid approaches achieving the best overall performance, establishing practical value for real-world deployment."}, "weaknesses": {"value": "(A) The core motivation is weak. The paper leans on the idea that RAG is too expensive or impractical, yet in real deployments RAG is the default pattern to ground models on fast-changing enterprise data. With reranking, caching, and short contexts, teams control tokens and latency, and every major cloud ships managed RAG stacks. If the goal is to argue for internal citation on cost, the comparison needs to be against optimized, production RAG baselines rather than token budget.\n\n\n(B) The claim that the method “goes beyond memorization” is overstated. Inference is constrained to titles or identifiers, which functions as an ID proxy. The paper itself notes title-level memorization on Wikipedia and infeasibility where stable IDs are weak. Bidirectional training may help usage, but the supervision still steers toward ID cues, which limits granularity.\n\n(C) The paper does attempt to answer why the method works with some ablations and scaling results, but lacks intuitive ablation experiments like ablating title distinctiveness or using near-duplicate titles.\n\n(D) Baselines and model diversity are thin. Main results are on Qwen-2.5 3B and 7B, plus a prompt-only GPT-4.1 check. There is no head-to-head against LongCite-8B or LongCite-9B or other long-context citation-tuned systems, despite long-form citation being a stated focus.\n\n(E) The hybrid story is sensible engineering, but does not address the most important failure case. The paper shows Internal-only, External-only, and Hybrid setups and average complementarity across retrieval quality, but it does not isolate the conflict slice where internal is wrong and external is correct. That is the production reality that needs to be quantified, with an arbitration policy that shows when the system will switch, abstain, or defer."}, "questions": {"value": "Q1 Why are GPT-4.1 scores not bolded where it has the best performance in Table 1, given the caption says “best results are bolded”?\n\n Q2 What were the reasons behind choosing only Qwen models for evaluation?\n\n Q3 Why are models like LongCite-8B and LongCite-9B, which are fine-tuned for long-context citation, not included as baselines when long-form citation is a major theme?\n\n Q4 What happens when internal citations are wrong but retrieved external context is correct and contains the right citation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ro9juUl8Rm", "forum": "D9bLUj7wUW", "replyto": "D9bLUj7wUW", "signatures": ["ICLR.cc/2026/Conference/Submission12746/Reviewer_m27s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12746/Reviewer_m27s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887959769, "cdate": 1761887959769, "tmdate": 1762923564333, "mdate": 1762923564333, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CitePretrain, a framework that teaches large language models to cite the documents they learned from during pretraining without RAG. The goal is to make models more transparent and trustworthy by letting them show where their knowledge originally came from. The work builds on Khalifa et al. 2024, extending it from synthetic to real-world data like Wikipedia, Common Crawl, and arXiv. The main addition is Active Indexing, a continual pretraining step that links each document to an identifier and trains the model in two directions: Forward indexing: generate facts when conditioned on a document ID and Backward indexing: cite the correct document given a fact. Experiments on Qwen-2.5-3B and 7B show that Active Indexing improves citation precision over baselines, while keeping answer accuracy stable."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Well-motivated:**  Making LLMs capable of citing their pretraining data is an exciting direction that’s still largely unexplored. Also, this paper extends prior work in a meaningful way. Moving from synthetic experiments to real-world data and introducing Active Indexing feels like a natural next step.\n\n2. **Active Indexing works well:**  The dual training setup (forward + backward) leads to solid citation gains. The authors also provide detailed ablations showing how much each component contributes, which helps clarify *why* the method works.\n\n3. **Thorough evaluation:** The experiments cover a range of QA datasets and both correctness and citation metrics. \n\n4. **deep analysis and insights**: The discussion on scaling, memorization vs. grounding, and the effects of dataset size is nice to see."}, "weaknesses": {"value": "1. **Heavy additional training cost:**  The method needs a full active indexing phase with two objectives and lots of synthetic QA generation. That’s quite resource-heavy, and it might be hard to apply at the scale of large, from scratch, pretraining runs.\n\n2. **Forward indexing might be unnecessary:**  The results show that backward indexing (fact → doc) already gives most of the gains. The extra forward indexing adds complexity and cost for very little improvement, which makes me question whether it’s worth the extra training. This could be explained by that in forward indexing, teaching the model to go from document ID → facts feels artificial. It’s not something the model will ever be asked to do at inference time, so it might encourage memorization rather than true grounding.\n\n4. **Shortcut risk via document titles:**  Using real document titles as identifiers can leak information. The model might just learn to guess the title instead of genuinely retrieving it. For example, if the question mentions “Barack Obama,” it could easily predict `<s> Barack Obama</s>` as the citation without any retrieval.\n\n6. **Limited OOD evaluation**  \n   The authors do not evaluate the LM out-of-domain after their active indexing. Khalifa et al., 2024 report increase in perplexity over natural text due to training on document IDs. Do you observe the same? Can you  alsoevaluate the model on OOD tasks e.g., instruction following or QA before and after the indexing training?"}, "questions": {"value": "- Do you make sure that you're evaluating citation on totally unseen documents data as in Khalifa et al.? Or could the same document be included in both Active indexing and in the test set? \n- Have you tried anonymizing or hashing document titles to make sure the model isn’t just predicting them directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vz1QMherqK", "forum": "D9bLUj7wUW", "replyto": "D9bLUj7wUW", "signatures": ["ICLR.cc/2026/Conference/Submission12746/Reviewer_grma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12746/Reviewer_grma"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956102464, "cdate": 1761956102464, "tmdate": 1762923563941, "mdate": 1762923563941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}