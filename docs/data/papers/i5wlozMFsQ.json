{"id": "i5wlozMFsQ", "number": 15454, "cdate": 1758251522374, "mdate": 1759897306014, "content": {"title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation", "abstract": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap\", stemming from data scarcity and embodiment heterogeneity. To address this, we pioneer \"pointing\" as a unified, embodiment-agnostic intermediate representation, defining four core embodied pointing abilities that bridge high-level vision-language comprehension with low-level action primitives. We introduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed for embodied reasoning and pointing. We use a wide range of embodied and general visual reasoning datasets as sources to construct a large-scale dataset, Embodied-Points-200K, which supports key embodied pointing capabilities. Then we train Embodied-R1 using a two-stage Reinforced Fine-tuning (RFT) curriculum with specialized multi-task reward design. Embodied-R1 achieves state-of-the-art performance on 11 embodied spatial and pointing benchmarks. Critically, it demonstrates robust zero-shot generalization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5% across 8 real-world XArm tasks without any task-specific fine-tuning, representing a 62% improvement over strong baselines. Furthermore, the model exhibits high robustness against diverse visual disturbances. Our work shows that a pointing-centric representation, combined with an RFT training paradigm, offers an effective and generalizable pathway to closing the perception-action gap in robotics.", "tldr": "We propose EmbodiedR1, a VLM using reinforced fune-tuning, achieving 87.5% zero-shot success in robotic manipulation across diverse scenarios.", "keywords": ["Embodied Reasoning", "Vision-Language Model", "Embodied AI", "Reinforcement Learning", "Zero-shot Generalization"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ddf9e43daa1967d43ef162ed44c2a9cc14f7dfff.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the author proposed a pipeline of RL post-training of VLM models to perform embodied related perception tasks. The core innovation is \"pointing\" as a unified, embodiment-agnostic intermediate representation that bridges high-level vision-language comprehension with low-level action primitives. The authors define four embodied pointing abilities: Referring Expression Grounding (REG), Region Referring Grounding (RRG), Object Functional Grounding (OFG) and Visual Trace Generation (VTG). To train the model, the authors curate Embodied-Points-200K, a 200,000-sample dataset structured as \"question-verification\" pairs to handle the multi-solution nature of pointing tasks. It draws from diverse sources like RefCOCO, RoboRefIt, HandAL, and synthetic Isaac Gym simulations, augmented by GPT-4o for question rewriting."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The evaluation benchmarks are comprehensive.\n3. The proposed RL method improves the performance of zero-shot reasoning tasks."}, "weaknesses": {"value": "1. Lack of novelty. For the whole pipeline, I think it is highly similar to the Seg-Zero [1] and VisionReasoner [2]. The authors finetuned Qwen2.5-VL-3B using Reinforced Fine-tuning (RFT) approach, closely mirroring the training paradigms introduced the two papers. From my perspective, the authors essentially repurposed the reasoning detection, segmentation, and point prediction tasks from prior papers, adapting them into object-manipulation-level pointing prediction tasks. Apart from introducing a few additional reward functions, the overall framework remains fundamentally unchanged. Most importantly, I noticed that the authors did not cite these two papers at all, despite the fact that their publicly released anonymous code is highly similar to the open-source implementations of those two papers. I would like to remind the authors of the principle of academic integrity. Also, treating point or affordance as a unified representation is discussed broadly in previous works.\n\n2. Insufficient literature review. Several highly similar works are neither compared nor discussed, such as Affordance-R1 [3]. \n\n3. Lack of comparisons of scalability. What would happen if you scale up the model size from 3B to 7B or larger ones?\n\n4. Meaningless comparisons. In the VLAs experiments, the authors compared the Embodied-R1 directly with VLAs, where the Embodied-R1 outputs the point prediction and utilizes another planner to solve the path planning. However, the VLAs directly output all-step actions in relatively high control frequency, which is intrinsically more difficult to complete manipulation tasks.  Hence, such kind of comparisons are meaningless from my perspective.\n\n5. Insufficient experiments. How about the dual-arm experiments, does the Embodied-R1 perform good?\n\n6. Overall, I think the paper's motivation is not strong enough, that we know the RL post-triaing strengthens the perception capabilities as the authors claimed. However, how to prove such boost of performance will lead to improvements of real manipulation tasks? In other words, the object-level pointing paradigm is limited compared to the end-to-end VLA paradigm, which I think limits the meaning of this paper. To be more specific, the claimed paradigm and model are useless in manipulation tasks that are not object-centric.\n\n[1] Seg-Zero: Reasoning-Chain Guided Segmentation via Cognitive Reinforcement\n\n[2] VisionReasoner: Unified Reasoning-Integrated Visual Perception via Reinforcement Learning\n\n[3] Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model"}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zdnhY8RVFz", "forum": "i5wlozMFsQ", "replyto": "i5wlozMFsQ", "signatures": ["ICLR.cc/2026/Conference/Submission15454/Reviewer_LhSQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15454/Reviewer_LhSQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761587611043, "cdate": 1761587611043, "tmdate": 1762925738656, "mdate": 1762925738656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper summarizes prior works with point-based affordances, redefines the concept of \"pointing\" as an intermediate representation, and extends it to four specific interpretations: Referring Expression Grounding (REG), Region Referring Grounding (RRG), Object Functional Grounding (OFG), and Visual Trace Generation (VTG). It collects and relabels datasets from multiple previous works, curates and re-annotates portions of them, specifically enhancing capabilities in these four areas to improve the accuracy of operations using pointing. The pointing format remains the classic xyz-coordinate point; however, by training on data with different semantic meanings, this representation acquires diverse types of knowledge. Downstream motion planners can more effectively utilize points enriched with these four types of information. To address the limitation of generalization when using supervised fine-tuning (SFT) with pointing, the paper employs GRPO for training."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper systematically examines previous point-based works and explicitly categorizes the information required in embodied tasks into four distinct types. Although the representation itself is not particularly novel, the proposed classification scheme and the approach of constructing training data according to these four categories are relatively innovative.\n2.\tThe work is solid and thorough, involving extensive data collection and cleaning, as well as comprehensive experimental validation.\n3.\tThe application of RLVR methods in embodied AI is still rare. This paper takes a clever approach by not training end-to-end directly; instead, it uses reinforcement learning to train a perception module, and then passes the perception outputs to a motion planner for action execution."}, "weaknesses": {"value": "1.\tBaseline coverage could be broader. Although the authors compare to strong baselines like FSD and RoboPoint, some recent VLA models (e.g., VLA-RL, ThinkAct) are only cited but not experimentally compared, which slightly limits the comprehensiveness.\n2.\tUnclear differentiation from prior “point-based” or affordance representations. Although the paper repeatedly emphasises the novelty of the “pointing” representation, it does not clearly delineate how this differs conceptually and functionally from existing notions, such as affordance points, grasp points, or visual trajectory signals, used in prior works (e.g., RoboPoint, FSD). The reader is left uncertain whether “pointing” represents a new learning formulation, a different abstraction level, or simply a unified terminology. A more explicit comparison — e.g., in a figure or a dedicated subsection — between pointing, affordance, and trajectory signals would clarify the actual contribution."}, "questions": {"value": "1.\tThe difference between the “pointing” representation and previous works should be further illustrated. For example, how do previous studies address the points and the issue of their dataset composition? \n2.\tIt would be better for authors to add the evaluation results for all four LIBERO suites for further comparison. In the ablations, there is only the LIBERO-spatial suite."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LRdwUls7HN", "forum": "i5wlozMFsQ", "replyto": "i5wlozMFsQ", "signatures": ["ICLR.cc/2026/Conference/Submission15454/Reviewer_4ftQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15454/Reviewer_4ftQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761890466425, "cdate": 1761890466425, "tmdate": 1762925738129, "mdate": 1762925738129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Embodied-R1, a 3B Vision-Language Model (VLM) designed to bridge the \"seeing-to-doing gap\" in robotics, a problem it attributes to data scarcity and embodiment heterogeneity. The authors pioneer \"pointing\" as a unified, embodiment-agnostic intermediate representation to connect high-level comprehension with low-level actions. They define four core embodied pointing abilities—Referring Expression Grounding (REG), Region Referring Grounding (RRG), Object Functional Grounding (OFG), and Visual Trace Generation (VTG)—and introduce a large-scale dataset, Embodied-Points-200K, to train them. A key contribution is the training methodology: a two-stage Reinforced Fine-tuning (RFT) curriculum with a specialized multi-task reward system. This RFT approach is argued to be superior to standard Supervised Fine-Tuning (SFT) as it enables flexible, free-form reasoning and resolves the \"multi-solution dilemma\" (e.g., many valid points for an \"empty space\") inherent in pointing tasks. Embodied-R1 achieves state-of-the-art performance on 11 spatial and pointing benchmarks and demonstrates robust zero-shot generalization, achieving a 56.2% success rate in SIMPLEREnv and 87.5% across 8 real-world XArm tasks without task-specific fine-tuning, a 62% improvement over strong baselines. The model also exhibits high robustness to diverse visual disturbances."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- It pioneers \"pointing\" as a unified and embodiment-agnostic intermediate representation to bridge high-level vision-language understanding with low-level robot actions. This approach is designed to solve the \"seeing-to-doing gap\" by overcoming data scarcity and the challenge of hardware (embodiment) heterogeneity.\n\n- The paper introduces Embodied-Points-200K, a large-scale, high-quality dataset curated from diverse sources to support four newly defined core pointing abilities: Referring Expression Grounding (REG), Region Referring Grounding (RRG), Object Functional Grounding (OFG), and Visual Trace Generation (VTG).\n\n- It proposes a novel \"Reinforced Fine-tuning (RFT)\" training curriculum, which is a key advantage over standard Supervised Fine-Tuning (SFT). The RFT method enables flexible, free-form reasoning beyond rigid templates and effectively resolves the \"multi-solution dilemma\" by reinforcing any correct solution rather than overfitting to one.\n\n- The resulting 3B parameter model, Embodied-R1, achieves state-of-the-art performance on 11 different spatial reasoning and embodied pointing benchmarks. It also demonstrates exceptional zero-shot generalization to robotic manipulation."}, "weaknesses": {"value": "- The paper's core motivation for using Reinforced Fine-tuning (RFT) over Supervised Fine-tuning (SFT) is to address the \"multi-solution dilemma\" (e.g., many valid points for an \"empty space\") . However, an SFT loss can also be adapted to handle this by, for example, minimizing the distance to the closest valid point in a target mask rather than a single (x, y) coordinate. Could the authors clarify how the \"Embodied-SFT\" baseline was implemented for these ambiguous tasks? Does this baseline already account for the multi-solution problem, or does it naively overfit to a single sampled ground-truth point? This is critical for validating that RFT is a necessary solution to this specific problem, rather than just one possible solution.\n\n- The paper pioneers \"pointing\" as an embodiment-agnostic representation. While this is effective for localization and trajectories, the authors acknowledge it may be insufficient for tasks requiring force, twisting, or interaction with deformable objects . How does the model currently handle instructions for which \"pointing\" is a fundamentally poor representation? For example, if given the command \"unscrew the jar lid\" or \"wipe the table,\" would the model attempt to generate a point or trace, or would its reasoning process allow it to recognize the task's incompatibility with its action space?\n\n- The preliminary results for the Embodied-R1-RGBD variant are concerning. The paper notes that on complex relational tasks, the 3D model performs worse than the 2D RGB-only model, a degradation attributed to potential \"hallucinations\" from the depth map . This seems to contradict the premise that richer, more relevant sensory information should improve performance. Could the authors elaborate on this finding? Does this suggest a fundamental flaw in the model's fusion architecture, where adding modalities can confuse the reasoning process, and how does this impact the future viability of extending this \"pointing\" paradigm into 3D?\n\n- The paper's title claims \"General Robotic Manipulation,\" yet the evaluations are limited to single-step instructions. The authors propose that a high-level planner could decompose long-horizon tasks as future work . While promising, this is a significant limitation. Could the authors provide any preliminary evidence, even a qualitative example, of Embodied-R1's ability to handle a simple, two-step instruction (e.g., \"pick up the strawberry and then move it to the bowl\") to demonstrate its feasibility as a robust execution module for such a hierarchical system?\n\n- The authors astutely note that the automated pipeline for generating the Embodied-Points-200K dataset \"inevitably introduces noise\". How does the RFT training process handle this label noise compared to SFT? Is it more or less robust to an incorrect ground-truth trajectory or point? A brief analysis of the dataset's noise level and its effect on the reward signal would strengthen the paper's claims about the robustness of the RFT methodology."}, "questions": {"value": "See the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i1ADui3YL0", "forum": "i5wlozMFsQ", "replyto": "i5wlozMFsQ", "signatures": ["ICLR.cc/2026/Conference/Submission15454/Reviewer_uNYS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15454/Reviewer_uNYS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901574559, "cdate": 1761901574559, "tmdate": 1762925736632, "mdate": 1762925736632, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Embodied-R1, a 3B Vision-Language Model (VLM) for general robotic manipulation. The core contribution is pioneering \"pointing\" as a unified, embodiment-agnostic intermediate representation to bridge the perception-action gap. The authors define four core embodied pointing abilities (REG, RRG, OFG, VTG) and train the model using a two-stage Reinforced Fine-tuning (RFT) curriculum on a custom large-scale dataset, Embodied-Points-200K. The RFT approach is crucial for handling the multi-solution nature of pointing tasks, which is superior to standard Supervised Fine-tuning (SFT). The model achieves state-of-the-art (SOTA) performance on spatial reasoning and strong zero-shot generalization in both simulation and real-world robot tasks."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The work is original for two main reasons. First, the formal definition and systematization of four distinct embodied pointing abilities (REG, RRG, OFG, VTG) as a unified intermediate representation is a novel and intuitive approach to abstracting robot control. Second, the use of RFT specifically to address the multi-solution dilemma inherent in embodied pointing (e.g., many points are valid for an \"empty space\").\n- The paper is written clearly and is easy to follow. The problem statement (\"seeing-to-doing gap\") is well-defined, and the proposed solution using \"pointing\" is logically introduced.\n- The RFT-based, pointing-centric approach provides a new, highly effective, and scalable pathway for training VLMs for robotics, proving that a relatively smaller VLM (3B parameters) can outperform larger, specialized models by focusing on robust reasoning and generalization rather than just mimicking specific actions."}, "weaknesses": {"value": "- Limited Exploration of Learning-Based Execution: The current main zero-shot robot evaluation (SIMPLEREnv and XArm) couples the VLM with a traditional CuRobo motion planner. While a promising preliminary experiment in Appendix F shows that the visual trace can boost a Diffusion Policy baseline, the core evaluation does not extensively explore this integration. Given that the paper emphasizes closing the \"seeing-to-doing gap,\" relying predominantly on a classical planner might limit the realization of the full potential of the visual trace output, especially for complex or dynamic actions where a learned policy excels.\n- Generalization Scope: The paper acknowledges that the \"pointing\" representation may be insufficient for complex manipulation requiring precise force control, wiping, or interaction with deformable objects. This limits the generality claim for the full spectrum of robotic manipulation."}, "questions": {"value": "- 2D to 3D Trace Accuracy: The Visual Trace Generation (VTG) outputs a 2D trajectory which is then mapped to 3D using the pinhole camera model and initial depth information. How robust is this 2D-to-3D transformation process, particularly in the context of fine-grained manipulation tasks?\n- I previously tested OpenVLA on the WidowX Robot in SIMPLEREnv using the same four tasks as in the paper, achieving an average success rate of around 40%. However, the paper reports 1.0%, which is a significant difference from 40%. Therefore, I am unsure whether this reported result is accurate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JCwbjhMQY5", "forum": "i5wlozMFsQ", "replyto": "i5wlozMFsQ", "signatures": ["ICLR.cc/2026/Conference/Submission15454/Reviewer_gZir"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15454/Reviewer_gZir"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15454/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921039477, "cdate": 1761921039477, "tmdate": 1762925736211, "mdate": 1762925736211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}