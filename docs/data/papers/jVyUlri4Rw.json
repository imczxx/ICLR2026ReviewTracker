{"id": "jVyUlri4Rw", "number": 21214, "cdate": 1758315022139, "mdate": 1759896934354, "content": {"title": "Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement", "abstract": "This research introduces the \\textbf{Judge's Verdict Benchmark}, a novel two-step methodology to evaluate Large Language Models (LLMs) as judges for response accuracy evaluation tasks. We assess how well 54 LLMs can replicate human judgment when scoring responses from RAG (Retrieval-Augmented Generation) or Agentic pipelines against ground truth answers. Our methodology progresses from traditional correlation analysis to comprehensive Cohen's Kappa analysis that measures actual agreement patterns. The two-step approach includes: (1) a correlation test that filters judges with strong alignment ($r \\geq 0.80$), followed by (2) a human-likeness test using z-scores to identify two distinct judgment patterns—\\textit{human-like} judgment ($|z| < 1$) that mimics natural human variation, and \\textit{super-consistent} judgment ($z > 1$) that exceeds typical human-to-human agreement levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1 performance: 23 models exhibit human-like patterns that preserve the nuances of human judgment, while 4 models demonstrate super-consistent behavior—a pattern that could indicate either enhanced reliability or oversimplification of complex judgments. Testing 43 open source models (1B-405B parameters) and 11 closed models (GPT, Gemini, Claude variants), we demonstrate that judge excellence is not solely dependent on model size but on specific training strategies. Our key contributions include: (1) establishing that correlation alone is insufficient for judge evaluation, (2) introducing a \"Turing Test for judges\" based on agreement patterns, and (3) providing a standardized benchmark for classifying LLM judges into distinct performance tiers for different evaluation needs.", "tldr": "", "keywords": ["LLM-as-a-Judge", "Judge's Verdict Benchmark", "Human-AI Agreement", "Cohen's Kappa Analysis", "RAG Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cacf8acb453088b2078df5fe750daa2510c3069b.pdf", "supplementary_material": "/attachment/57af66def3e70192bdbabfe6e9be491b2a28b1e5.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Judge’s Verdict Benchmark, a two-step framework to evaluate LLM-as-a-judge for response-accuracy tasks (e.g., RAG/agent outputs). Step-1 filters models by Pearson correlation r ≥ 0.80 with human consensus; Step-2 computes Cohen’s κ against human raters and performs a “Turing Test for judges” using κ-based z-scores to label models as human-like (|z|<1) or super-consistent (z>1). On 1,994 items with 3 expert annotators each across six datasets, the authors test 54 models, finding 27 Tier-1 judges (23 human-like; 4 super-consistent) and arguing correlation alone is insufficient for judge validation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Moves beyond correlation to agreement-aware validation (Cohen’s κ) and frames a human-likeness test via κ-based z-scores; this addresses systematic harsh/lenient biases that correlation can miss.\n- Broad evaluation: 54 judges, 6 datasets (including PDFs), 1,994 items, 3 expert raters per item; inter-annotator reliability is substantial (Fleiss’ κ≈0.79, Krippendorff’s α≈0.79).\n- Clear, actionable tiers (human-like vs super-consistent) that articulate a nuance–consistency trade-off for different application needs.\n- Transparent thresholds and sensitivity check for |z|, showing how classifications shift.\n- Concrete findings: 36 pass r≥0.80; 27 attain Tier-1; lists of top models by r and κ provided."}, "weaknesses": {"value": "- Grounding of the target task: “Answer Accuracy” is anchored to a RAGAS-style rubric (0/2/4 normalized), but human raters used 0/0.5/1.0—the mapping and potential scale incongruence between LLM judges and humans isn’t fully reconciled (could inflate/deflate κ).\n- Threshold choices (r≥0.80; |z|<1) are motivated but still somewhat ad-hoc; no principled decision-theoretic analysis or preregistration, and the baseline human κ=0.801 is treated as near-universal across datasets/labels. \n- Potential circularity: humans and models judge the same RAG outputs produced by a small set of models/prompts; this may bias in-distribution to specific answer styles or retrieval artifacts. Dataset generation stack (retrievers/rerankers/chunking) might leak patterns."}, "questions": {"value": "- How exactly are human 0/0.5/1 labels reconciled with the LLM-judge’s 0/2/4→[0,1] scale at scoring time?\n- Were judge prompts standardized across models (temperature, system instructions), and are there prompt-leakage risks from using similar instructions in RAGAS and human tasks?\n- Do you release code/dataset/leaderboard at review time (with anonymity preserved), and are the thresholds preregistered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "02Co0Vt4ra", "forum": "jVyUlri4Rw", "replyto": "jVyUlri4Rw", "signatures": ["ICLR.cc/2026/Conference/Submission21214/Reviewer_NLJK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21214/Reviewer_NLJK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761450798311, "cdate": 1761450798311, "tmdate": 1762941621894, "mdate": 1762941621894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper measures how LLMs can replicate human judgment for RAG-style tasks. To do so, the authors introduce a two-step evaluation protocol, first using Pearson correlation, then using Kohen's kappa. In doing so, they introduce a Turing test for LLM-based judges."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The evaluation is thorough, spanning 54 LLMs (43 open-source 11 closed-source models)."}, "weaknesses": {"value": "1. Many of the citations/references are wrong; they point to correct arXiv identifiers, but the title/authors are hallucinated. For instance, the authors cite \"arXiv:2505.16222\" as \"Testing the limits of llmbased code generation: How well do llms understand code semantics?\" by \"Erfan Ashouri, Rafael-Michael Karampatsis, and Charles Sutton.\" The actual article is titled \"Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation\" by \"Jiwon Moon, Yerin Hwang, Dongryeol Lee, Taegwan Kang, Yongil Kim, and Kyomin Jung.\" This is not an isolated issue, 8/14 citations have incorrect titles and/or author lists.\n2. The writing quality and organization can be greatly improved. For example, Sections 3 and 4 are difficult to distinguish, and Section 5.2 could appear earlier to help the reader understand the evaluation design.\n3. The thresholds the authors settle on feel arbitrary (e.g., r>=0.8 or |z|<1). The corresponding interpretations (e.g., \"super-consistent\" and \"human-link\") are not well supported."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6lpguzFXLi", "forum": "jVyUlri4Rw", "replyto": "jVyUlri4Rw", "signatures": ["ICLR.cc/2026/Conference/Submission21214/Reviewer_Futy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21214/Reviewer_Futy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953111023, "cdate": 1761953111023, "tmdate": 1762941621200, "mdate": 1762941621200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempts to address the problem of unreliable evaluation by LLM judges, where current methods rely on simple correlation with human scores that fail to capture true agreement. A model may correlate well yet consistently rate too high or too low. To address this, the authors propose the Judge’s Verdict Benchmark, a two-step method that first selects models with strong correlation (r ≥ 0.80) and then measures genuine alignment with humans using Cohen’s Kappa and z-scores. This approach distinguishes human-like judges from super-consistent judges, providing a clearer, standardized way to assess whether LLMs truly evaluate like humans."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a well-defined two-step evaluation method that combines correlation and agreement analysis to measure how closely LLMs align with human judgments. Its structured framework and thorough comparison across 54 models provide clear insights."}, "weaknesses": {"value": "1. No Analysis of Why Existing Methods Fail: The paper criticizes correlation-based approaches but provides no empirical analysis or case studies showing when or why those methods fail. Concrete examples comparing correlation-only evaluations to agreement-based results would make the motivation stronger.\n\n2. Limited Task Scope: Experiments focus mainly on response accuracy in QA/RAG settings. It’s unclear if the framework generalizes to subjective tasks like summarization, dialogue, or creative writing evaluation.\n\n3. Interpretation Ambiguity: The meaning of “super-consistency” is unresolved—does higher agreement mean better reliability or loss of nuance? The paper acknowledges this but doesn’t provide empirical evidence to decide.\n\n4. Presentation: Presentation could be clearer and more concise. The writing is dense, with repeated explanations that make it difficult to follow at times. Figures and tables are informative but not well integrated into the narrative, which reduces readability and flow. A more streamlined structure and clearer visual summaries would make the paper’s main ideas and results easier to grasp."}, "questions": {"value": "1. Could the authors provide more explanation of why previous correlation-based methods fail to meet evaluation requirements? What specific aspects of judgment quality do they overlook?\n2. Are there any tasks or datasets where z-scores are notably higher or lower? A breakdown of z-scores by task type would make the analysis more insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JhP7dIZAm4", "forum": "jVyUlri4Rw", "replyto": "jVyUlri4Rw", "signatures": ["ICLR.cc/2026/Conference/Submission21214/Reviewer_WVgQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21214/Reviewer_WVgQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21214/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135761513, "cdate": 1762135761513, "tmdate": 1762941620834, "mdate": 1762941620834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes a novel two-stage benchmark to robustly evaluate the agreement between\nhuman ratings and instruction-following large language models' judgements. The benchmark is\nbuilt upon open-access standard LLM benchmarks in an RAG setting. Empirically, 54 LLMs are\nevaluated and ranked into different tiers. The main contribution is the inclusion of Cohen’s\nKappa as a metric to capture the nuanced consistency with human ratings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed evaluation method is clear and sound, rooted in classic statistical analysis.\n\nThe experimentation comprehensively evaluates the frontier models, including 54 recent\nlanguage models.\n\nThe paper reveals a phenomenon where human annotators internally and naturally\ndisagree and discuss the subtle impact on llm evaluation."}, "weaknesses": {"value": "The novelty of the benchmark is limited and insufficiently argued. Although using\nCohen’s Kappa better captures human-llm agreement, the paper fails to address the\nimportance of adopting the new metric. The example of 2 scenarios presented online\n190 is claimed without concrete evidence showing how often it is observed.\n\nThe threshold sensitivity analysis fails to provide a rationale for the tier definition. It\nappears, the analysis in Sec 4.1 only measures the changes within the selected pool of\nlanguage models, without arguing that the chosen models are representative of the\ncurrent LLM landscape. There is a likelihood that the chosen thresholds would not\ngeneralize to other language models. Additionally, the definition of thresholds seems to\nbe based on the number of models falling into each tier, which seems arbitrary and\nsubjective.\n\nThe paper does not include a behavioral analysis to showcase the significance of\n“Human-like” and “Super-consistent” models. Models with high correlation scores are\nfurther classified into the two tiers without analysis of how exactly their behavior differs. It\nis unclear what being “super-consistent” actually implies about the language model’s\nability and how future model training pipelines could benefit from this metric.\n\nThe benchmark setup is not clear and has a limited scope. The questions included are\nquestion answering tests with or without documents with golden answers. A\ncomprehensive analysis should increase the task diversity and, importantly, include\nopen-ended creativity-focused tasks. Additionally, there is no analysis of the distribution of annotator scores, explaining how and where they agree or disagree. This provides\nevidence for the trade-off discussion in Sec. 5.1."}, "questions": {"value": "How large is the pool, and what is the demographic background of the human\nannotators.\nHow do the low-tier model (with r<.8), human-like model, and super-consistent models\ndiffer in behavior?\nCan you show an example where two models have similar correlation but clearly differ in\nagreement (κ)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2vSbKkRciz", "forum": "jVyUlri4Rw", "replyto": "jVyUlri4Rw", "signatures": ["ICLR.cc/2026/Conference/Submission21214/Reviewer_9vU8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21214/Reviewer_9vU8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21214/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763039351780, "cdate": 1763039351780, "tmdate": 1763039351780, "mdate": 1763039351780, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}