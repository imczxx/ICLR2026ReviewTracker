{"id": "Zbt44sC4tE", "number": 16804, "cdate": 1758268862600, "mdate": 1759897218632, "content": {"title": "What If TSF: A Multimodal Benchmark for Conditional Time Series Forecasting with Plausible Scenarios", "abstract": "Time series forecasting has long been constrained by history-bound, unimodal methods and benchmarks that fail to capture predictive, forward-looking context. Recent progress in large language models and multimodal alignment suggests richer possibilities, yet most existing multimodal benchmarks rely on textual descriptions that merely repeat historical patterns and can introduce misleading signals due to irrelevant context.\nTo advance research in this area, we introduce ``What If TSF (WIT)'', a benchmark constructed around expert-crafted what-if scenarios and explicit future events. WIT encourages models not only to match historical patterns but also to reason under uncertainty, evaluating their ability to integrate multimodal signals, anticipate plausible futures, and enable conditional forecasts.\nBy moving beyond historical pattern extraction, WIT establishes a principled testbed for scenario-guided multimodal forecasting.", "tldr": "Time series forecasting still relies on historical patterns, while multimodal gains remain limited by redundant text. The WIT Benchmark provides expert-authored “what-if” scenarios to test whether models go beyond pattern replication.", "keywords": ["Multimodality", "Time Series Forecasting", "Future Scenarios", "Counterfactuals"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c230de58aa7214b5c83c7c1c6413df3ef4b63bd5.pdf", "supplementary_material": "/attachment/44b1e741af1485bcfd57a601ef80f71f621eb5f9.zip"}, "replies": [{"content": {"summary": {"value": "### *Summary*\n\n- The authors present a novel benchmark for context-aided forecasting with what-if scenarios over 4 domains.  This benchmark distinguishes between static and dynamic context, and adds counterfactual future context. The authors measure using directional accuracy and MSE (no MSE for long-term forecasts). The authors evaluate a diverse subset of LLMs, TSFMs, Time-MQA and statistical methods (e.g. ARIMA).\n\n  ### *Contributions*\n\n- Novel benchmark for context-aided forecasting that distinguishes itself particularly by considering counterfactuals and evaluating directional accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the paper is well-structured, with evidence mostly supporting its claims (see weaknesses and questions). \n\n### *Originality*\n\n- Enlightening comparison against relevant previous work (Time-MMD and Context is Key (CiK), Table 1)., with introduction of interesting counterfactual scenarios.\n\n  ### *Quality*\n\n- Good set of baselines  \n- Diverse metrics\n\n  ### *Clarity*\n\n- Clear text\n\n  ### *Significance*\n\n- Important, timely topic"}, "weaknesses": {"value": "The main weaknesses of the paper lie in the validation of its original contributions.\n\n### *Originality*\n\n- The distinction from previous benchmarks (Time-MMD and Context is Key (CiK) ) is vague, and it’s unclear what analysis backs up table 1\\.   \n  - While the work states that in practice, Time-MMD has redundancies and incomplete texts, the authors do not back these claims up with an analysis of the dataset. A better understanding of the limitations  of Time-MMD would certainly be useful to the community.  \n  - The discussion in 4.2 would be greatly improved by actual comparisons to examples from the WIT benchmark to ground the claimed improvements.   \n  - While 4.3 discusses three components (time series, historical context and future information), figure 4 of CiK shows that their classification of context subsumes this categorization. Improving 4.2 with concrete examples from WIT that highlight examples would fix this issue.\n\n- Contemporaneous work that is highly related: https://openreview.net/forum?id=YRp4xqTs3n \n\n  ### *Quality*\n\n- If you measure using directional accuracy, how do you estimate overshooting or undershooting estimates? A decline in the dollar broad index of 10% vs 50% is a very different thing  \n- The analysis of why historical context does not improve accuracy is absent. Beyond the observed results, it would be important to understand *why* this is the case, especially since the benchmark itself is the main contribution of the article.  \n- An intact time series signal from before an LLM's knowledge cutoff can leak the future answer to an LLM that has been pre-trained on these time series, especially if the values are somewhat unique. How do you validate that your memorization mitigation strategy works?  \n- I’m not sure what the takeaway here is from the ablation, it’s unclear what this contributes to my understanding of the benchmark."}, "questions": {"value": "### *change your opinion*\n\nIn my mind, this paper is currently a 5: I wouldn't mind it being accepted. However, I think that additional validation is required to ensure that this benchmark will be useful to the context-aided forecasting community in its current form. The main things I would like to see to up my score are an assessment of the relevance of the historical context, validation of the memorization mitigation. Beyond that, adding additional baselines and analyses of forecasting instances could improve my score, after a rigorous validation of the relevance of the context. \n\n- In which cases is the historical context informative or uninformative? A human eval here would be useful to confirm whether the information is indeed relevant or not to the forecasting task, beyond it being merely logically compatible. Otherwise, what's the point of including it in the benchmark?\n\n  ### *clarify a confusion*\n\n- Where are the details of your human evaluation protocol for validating the relevance of the context (section 4.6?)\n\n  ### *address a limitation*\n\n- Have you considered estimating model abilities when context is less informative, e.g. removing the “likely to weaken significantly” part in the example of figure 1 to see if the model can deduce the impact of temperature on demand for natural gas itself?  \n- How do you ensure that the news is not actually updated after the fact? Many articles are first published, then continuously updated with relevant facts afterward, which may leak the answer.  \n- It’s not only the textual facts, but also the time series themselves which present potential leakage problems. LLMs are pretrained on time series data in the wild, even if it is input as text tokens. How do you prevent this form of leakage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "l7g3WTgaU8", "forum": "Zbt44sC4tE", "replyto": "Zbt44sC4tE", "signatures": ["ICLR.cc/2026/Conference/Submission16804/Reviewer_atme"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16804/Reviewer_atme"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672711532, "cdate": 1761672711532, "tmdate": 1762926836092, "mdate": 1762926836092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the WIT benchmark, a novel and valuable contribution to the field of multimodal time series forecasting. \nCompared to existing benchmarks, this paper also construct the countrefactual task and incorporate more datasets to build a comprehensive benchmark. This paper's work try to bring multimodal time series forecasting to next level."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of this paper is the comprehensive dataset, collecting many real-world data which is not included in other multi-modal time series benchmarks."}, "weaknesses": {"value": "1. Time Series Ground Truth in Counterfactual Forecasting (Task 3) is unclear. If text is constructed by mininal change, the time series should be changed too, but there is no clear description about how the time series can be change.\n\n2. This paper mentioned some advantages of this benchmark over CiK. However, there is something good in CiK which is not in this benchmark, like: ** Retrieval and Reasoning **. CiK constructs multiple types of text to test different Retrieval and Reasoning abilities which is lacked in this paper. Also, this paper said CiK doesn't included the  counterfactual tasks, which is not true, because CiK contruct the text and the corresponding time series manually, so it must include some counterfatucal tasks, only not very categorized this way.\nI think the real advantage of this paper over other benchmarks is the comprehensive real-world datasets instead of the counterfactual, because most of CiK data is not real in essence.\n\n3. About the metrics. Besides the mse mae or some accuracy metrics, there should be some metrics to measure how the prediction obey the text. For example, there may be some cases with same MAE and MSE, but with totally different directions in the prediction."}, "questions": {"value": "1. Counterfactual Ground Truth: For Task 3, what is the source of the ground truth time series used for evaluation after the textual context is modified? If the text is counterfactual, does the ground truth series remain the original (now misaligned) data, or is it also synthetically altered? This is unclear and critical for interpreting results.\n\n2. Benchmark Scope vs. CiK: CiK explicitly tests retrieval and reasoning with mixed-context inputs, a capability WIT does not evaluate. What is the rationale for not incorporating such a test? Do you consider the lack of a retrieval/reasoning component a limitation of WIT's design?\n\n3. Metrics for Textual Faithfulness: Beyond standard accuracy, how do you measure if a model's forecast directionally obeys the textual guidance (e.g., predicts \"rise\" when the text describes a \"surge in demand\")? Would a metric specifically for \"textual guidance adherence\" be more direct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wDKL9IqQp9", "forum": "Zbt44sC4tE", "replyto": "Zbt44sC4tE", "signatures": ["ICLR.cc/2026/Conference/Submission16804/Reviewer_WFsj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16804/Reviewer_WFsj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707666667, "cdate": 1761707666667, "tmdate": 1762926835701, "mdate": 1762926835701, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors develop a benchmark for multimodal forecasting, with what-if scenarios and future events. The authors build this benchmark with real-world time series and real-world events that are paired together, and further with counterfactual events constructed with an LLM. The authors evaluate LLMs, multimodal time series models and unimodal time series models such as time series foundation models and statistical models, for their forecasting performance (MSE) on tasks from the benchmark and directional accuracy on counterfactual events."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of constructing counterfactual events and verifying directional accuracy (in the absence of the counterfactual time series) is very interesting. Applying this idea in the context of multimodal forecasting brings clear novelty.\n* The de-identification rules that the authors apply on the data (mentioned in Sec 4.5) are well-thought out.\n* As rightfully mentioned, instead of the approach of writing scenarios and modifying time series as in prior work, the approach of aligning real-world time series and events is interesting and brings the setup closer to real-world scenarios.  \n* The authors provide substantial detail on the data sources used for the benchmark and the prompts used for generating the final textual contexts in the benchmark. I appreciate this."}, "weaknesses": {"value": "* Incorrect claims on related work: \n  * The authors state “CiK [1] emphasizes contextual grounding and event understanding, but its design primarily supports retrospective reasoning rather than explicit, scenario-based forecasting.” This is incorrect. CiK [1] has tasks that are historical and future-based. In fact, there are tasks that are very similar to the tasks presented in this benchmark (see [this task](https://servicenow.github.io/context-is-key-forecasting/v0/UnemploymentCountyUsingMultipleStateData.html)) \n  * The authors state “These limitations motivate our benchmark: we provide expert-authored future scenarios that articulate plausible upcoming events”. This is exactly what CiK [1] did as well; the work however claims novelty on this aspect which is incorrect.\n\n* Proposed static and dynamic types of context: This is already encapsulated in the 5 types of context that CiK [1] proposes (Intemporal, Historical, Future, Covariate, Causal information). The authors do not discuss the differences. It is not clear what value the proposed 2 types of context brings.\n\n* Insufficient detail: No examples are provided of the tasks/instances in the benchmark. E.g. I do not know how fine-grained the contexts are. CiK [1] has overly specific contexts, but how is this benchmark different?\n\n* Insufficient evaluation: The performance of LLMs without context is not provided in Table 3. So it is not clear if it is the context reasoning capabilities that is enabling the superior performance of the LLMs or if it is their forecasting capabilities. Further, it is not clear if they outperform the unimodal models when evaluated w/o context. This is important to know when LLMs are considered for use in forecasting in the real-world.\n  * The performance of LLMs without any context for each domain is available in the appendix in C.2.2 (I appreciate that the authors put this) but the unimodal models are absent from this table. Further this is an important evaluation that must be visible from the main text without having to dig into the appendix.\n  * The authors only evaluate LLMs with a prompting methodology similar to Direct Prompting as in prior work [1, 2, 3]. However this is not the only prompting methodology to use LLMs for forecasting; there also exists LLMP [4] / LLMTime [5] (prior work [1] evaluates the LLMP methodology). The authors must evaluate these strategies as well, at least with a subset of LLMs.\n\n* Adding to the above point, the authors present a very limited analysis of results (Less than a single page when 5.2 and 5.3 are put together). \n  *  CiK [1] showed that the largest LLMs (such as Llama-3.1-405B-Inst) perform the best, and that performance clearly differs with scale. Such analyses are not presented here. \n\n  *  Further, a very limited set of LLMs are assessed. It is unclear why these LLMs were picked. \n\n  *  The authors do not verify that the models obtain their performance due to the context reasoning capabilities and not their forecasting capabilities. This is related to the point about evaluating these models without context.\n\n  *  The paper provides no insight on the limitations of LLMs and how they can be improved.\n\n  *  And no examples are provided of the forecasts of LLMs with and without context, demonstrating how the context meaningfully changes the forecasts.\n\n  *  An issue that the CiK paper [1] highlighted is the high cost of LLMs. That is not discussed here.\n\n\n### Summary note\n\n* My main reasons for rejection are the misleading claims of novelty, insufficient evaluation and lack of rigorousness in the paper. The benchmark appears to be rigorous but the paper doesn’t do a good job at convincing the reader, so I’m still on the fence [This is reflected in my scores for Soundness, Presentation and Contribution] \n* My suggestion to the authors is to position their contributions accurately with respect to related work, and highlight precisely the novelty that they bring."}, "questions": {"value": "* What is the difference between a conditional statement and anticipated event? That is not discussed in Sec. 3.1 and I don’t see the need to differentiate them. To the model it makes no difference in conditioning between either. \n* The verification of the alignment and relevance of the text with the time series by human experts needs more explanation. This is crucial to the quality of the benchmark, as this is where prior works [6] have used different methodologies (such as an LLM judge). \n  * How many annotators were there? What exact instructions were they given? What was the inter-rater agreement achieved?\n  * Give examples of examples that were discarded after human verification. \n* I completely miss the point of the ablation study 5.3. The authors state “In constructing the historical context for WIT, we extract all significant events corresponding to the history time series without any restrictions.” Why is this important? What is the relevance of this?\n* A minor correction - “Yet, these models remain constrained by the quality of textual inputs, which in existing benchmarks are often descriptive or redundant, rather than predictive of future outcomes” → In this case isn’t their evaluation that remains constrained? The models themselves are not constrained.\n* Minor / grammar: Figure 1 → “enables”: shoudn’t this be “enables evaluation of” ?\n\n## References\n\n[1] Williams, Andrew Robert, Arjun Ashok, Étienne Marcotte, Valentina Zantedeschi, Jithendaraa Subramanian, Roland Riachi, James Requeima et al. \"Context is key: A benchmark for forecasting with essential textual information.\" ICML 2025.\n\n[2] Ashok, Arjun, Andrew Robert Williams, Vincent Zhihao Zheng, Irina Rish, Nicolas Chapados, Étienne Marcotte, Valentina Zantedeschi, and Alexandre Drouin. \"Beyond Na\\\" ive Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs.\" arXiv preprint arXiv:2508.09904 (2025).\n\n[3] Zhang, Xiyuan, Boran Han, Haoyang Fang, Abdul Fatir Ansari, Shuai Zhang, Danielle C. Maddix, Cuixiong Hu et al. \"Does Multimodality Lead to Better Time Series Forecasting?.\" arXiv preprint arXiv:2506.21611 (2025).\n\n[4] Requeima, James, John Bronskill, Dami Choi, Richard Turner, and David K. Duvenaud. \"Llm processes: Numerical predictive distributions conditioned on natural language.\" Advances in Neural Information Processing Systems 37 (2024): 109609-109671.\n\n[5] Gruver, Nate, Marc Finzi, Shikai Qiu, and Andrew G. Wilson. \"Large language models are zero-shot time series forecasters.\" Advances in Neural Information Processing Systems 36 (2023): 19622-19635.\n\n[6] Liu, Haoxin, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan Prabhakar Kamarthi, Aditya Sasanur, Megha Sharma et al. \"Time-mmd: Multi-domain multimodal dataset for time series analysis.\" Advances in Neural Information Processing Systems 37 (2024): 77888-77933."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9jdRRfGnlJ", "forum": "Zbt44sC4tE", "replyto": "Zbt44sC4tE", "signatures": ["ICLR.cc/2026/Conference/Submission16804/Reviewer_ALHr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16804/Reviewer_ALHr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761006838, "cdate": 1761761006838, "tmdate": 1762926835309, "mdate": 1762926835309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents \"What if TSF (WIT)\" benchmark to go beyond prediction to forecast under counterfactual scenarios. It combines expert crafted what if scenarios and future events with structured textual descriptions. For short term forecasting, MSE and trend accuracy (whether the trend will flip) is reported. For long horizon forecasting and in the counterfactual setting, just the direction of the trend is predicted and trend accuracy is reported based on whether the trend will flip or not. The benchmark contains 4 domains (politics, energy, society and economy) and the zero-shot performance of 6 models (Mistral 7B, Qwen2.5 7B, Qwen3-32B, GPT-4o and Gemma-3-27B, Time-MQA) are reported along."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a multimodal counterfactual what-if benchmark. This has received little attention in the TSF literature, and a benchmark dedicated solely to evaluating this is both interesting and novel."}, "weaknesses": {"value": "- **I am not sure if deidentification merely boils down to redacting company names and removing uniquely identifying URLs**. How can we be sure that memorization is not helping the model perform predictions after replacing <actual_company_name> with a placeholder like \"COMP_A\"? The news articles might still contain other identifying details. For example, \"Amazon made deals with XYZ\" can be anonymized to \"COMP_A made deals...\". However, it is not uncommon for news articles to contain details like \"the tech giant's CEO, Jeff Bezos\". Do the authors handle such cases? Or can they atleast provide empirical evidence from the prompts that go into the models?\n\n- For table 3, the authors should also report MSE for long term and counterfactual tasks, as well as for the tables in the appendix.\n\n- **The paper would benefit from added experiments on datasets released after cutoff dates of the LLMs used in the paper**. Even after the de-identification techniques used in the paper, it is possible the models might be able to pick up these closely related patterns from their pretraining data. However, this is not addressed by the authors. Given that this is a benchmarking paper, it is hard to comment on the rigor of the evaluations given an entirely LLM based pipeline. The gold standard to ensure there is no data leakage would be to evaluate on data released after the cutoff date.\n\n- Future outlook context is generated by prompting GPT5-mini to list the single most significant sub event among summaries. Real life scenarios involve multiple complicated event-event interactions, sometimes even involving directly conflicting information, yet this is boiled down to listing one event for the prediction task.\n\n- Other than the weaknesses already mentioned, the breadth of baselines (5 LLMs + Time-MQA) and tasks (4 domains) covered is also not particularly impressive for a benchmarking paper; the contributions are incremental/marginal. Even papers introducing methods (that also have to train model, rather than just evaluate zero-shot performance) typically evaluate on more baselines and tasks, despite not having benchmarking as the primary focus."}, "questions": {"value": "- \"To avoid confounding long term dynamics, counterfactual evaluation follows the short term forecasting setup of Task 1.\" – can the authors explain what they mean here?\n\n- It is not clear what the ground truth values are for the forecasted variables in the counterfactual setting. How are these numbers obtained? Is it by prompting GPT5-mini? To my understanding both the counterfactual text and the counterfactual data is generated by using an LLM (GPT5-mini?). This generally seems very simplistic from the examples given in the paper. For example, in table 15, the future outlook text says: \"Rising unemployment will sharply reduce consumer spending and push many households into financial distress.\". The model then predicts \"answer\": {\"future_timeseries\": [69.92], \"trend\": \"fall\"}. This seems like neither a challenging task nor a realistic one? Can the authors comment on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tHysctqLWU", "forum": "Zbt44sC4tE", "replyto": "Zbt44sC4tE", "signatures": ["ICLR.cc/2026/Conference/Submission16804/Reviewer_LVWN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16804/Reviewer_LVWN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16804/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129657469, "cdate": 1762129657469, "tmdate": 1762926834952, "mdate": 1762926834952, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}