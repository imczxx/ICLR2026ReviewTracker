{"id": "bGGv4WAGFJ", "number": 24356, "cdate": 1758356054173, "mdate": 1759896769842, "content": {"title": "RFC-GAN — Feedback-Conditioned GAN for Iterative Memory-to-Sketch Facial Translation", "abstract": "Eyewitness–to–sketch translation is traditionally performed by human artists, a process prone to bias and information loss. Recent work has applied conditional GANs to automate this task, yet existing models remain limited in their ability to iteratively refine coarse recollections into photorealistic faces. We propose a context-feedback training paradigm for image-to-image GAN: at each step, the generator and discriminator receive the most recent output as an auxiliary three-channel input, enabling the model to reason over its own predictions. Building on the Pix2Pix framework, we further investigate where to embed Self-Attention and Edge-Gating modules within the encoder–decoder and skip connections, systematically analyzing their effect on perceptual and adversarial loss. Experiments on synthetic-sketch and real eyewitness datasets demonstrate consistent improvements in FID, LPIPS, and human-rated realism, with the model producing sharper, more faithful faces and exhibiting stable iterative refinement at inference. These results suggest that feedback-conditioned GAN provide a principled path toward reliable facial reconstruction from memory.", "tldr": "", "keywords": ["Generative Adversarial Networks (GANs)", "Feedback-conditioned learning", "Iterative refinement", "Self-Attention", "Edge-Gating", "Image-to-image translation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0755a24c2298cd6579952f4dbda89e64414018fe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The approach essentially follows the pix2pix framework. In my assessment, the authors primarily apply existing techniques to the sketch synthesis task for testing and analysis. Therefore, the work demonstrates **limited novelty and advancement**."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The approach essentially follows the pix2pix framework. In my assessment, the authors primarily apply existing techniques to the sketch synthesis task for testing and analysis."}, "weaknesses": {"value": "### 1. Limited Novelty\n- The introduction of EG (Edge Guidance) is presented as a fine-tuning strategy, but the main text lacks a concrete mathematical formulation or detailed workflow.\n- The concept of EG itself is cited from Tang et al., further limiting the perceived novelty.\n\n### 2. Method Generality and Specificity\n- The proposed method does not demonstrate a clear, specific relationship to sketch synthesis.\n- It is unclear how the method would perform if applied to other image translation tasks.\n\n### 3. Literature Review\n- The references consist mainly of older works.\n- The survey of recent advancements in **GANs** and **sketch synthesis** is insufficient.\n\n### 4. Paper Organization\n- The structure and presentation of the paper are unusual and inconsistent with common practices, which hinders the understanding of its logical flow.\n\n### 5. Technical Concerns regarding Self-Attention (SA)\n- The paper uses SA only in later layers, while many efficient SA methods exist that could be applied in earlier layers. An analysis of using SA in earlier layers is necessary.\n- The novelty of introducing SA is questionable, as:\n    - SAGAN already introduced SA into GANs.\n    - Many subsequent works, including diffusion models, have incorporated SA into U-net architectures.\n- The significant increase in computational complexity due to SA raises concerns about its cost-effectiveness relative to any performance gains."}, "questions": {"value": "### Core Novelty\n1.  Please clearly state the core innovation of this work and its essential difference from the standard pix2pix framework.\n2.  Can you provide a precise mathematical definition or algorithmic workflow for the EG (Edge Guidance) strategy? What are the specific improvements compared to the method by Tang et al.?\n\n### Method Design\n3.  Why is the proposed method particularly suitable for sketch synthesis? Has its generality been verified on other image translation tasks?\n4.  Why is the self-attention mechanism only used in the deeper layers of the network? Were more efficient self-attention modules explored for the earlier layers?\n\n### Experimental Analysis\n5.  Does the performance gain from using self-attention justify its computational overhead? Please provide a detailed complexity vs. performance trade-off analysis.\n6.  Can comparisons with more recent sketch generation or image translation methods be added to better demonstrate the advantages of the proposed approach?\n\n### Suggestions\n*   **Literature Review**: It is recommended to supplement the review with the latest advances in GANs and sketch synthesis.\n*   **Paper Structure**: It is recommended to adjust the paper's structure to align with conventional academic logic for better readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lkDbxhboZd", "forum": "bGGv4WAGFJ", "replyto": "bGGv4WAGFJ", "signatures": ["ICLR.cc/2026/Conference/Submission24356/Reviewer_Mnia"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24356/Reviewer_Mnia"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761204280556, "cdate": 1761204280556, "tmdate": 1762943055494, "mdate": 1762943055494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RFC-GAN, a feedback-conditioned generative adversarial network designed to iteratively refine facial reconstructions from sketches or eyewitness recollections. The model extends the Pix2Pix framework by introducing a Recursive Feedback Conditioning (RFC) mechanism in which both the generator and discriminator incorporate the previous output as an auxiliary input, enabling iterative self-improvement across inference steps. Two complementary modules, Self-Attention (SA) to enhance global feature consistency and Edge-Gating (EG) to preserve contour fidelity, are examined individually and in combination. Experiments on the CelebAMask-HQ synthetic sketch dataset and the CUHK hand-drawn sketch dataset demonstrate consistent gains across standard perceptual and structural metrics including FID, KID, LPIPS, SSIM, and PSNR, with the RFC-only configuration yielding the most stable and accurate reconstructions. The study positions feedback conditioning as a viable strategy for human-like iterative refinement in generative facial translation with potential applications in forensic reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The feedback-conditioning mechanism is well-motivated as a human-analogous process of iterative self-correction.\n\n* Decent ablations (RFC, SA, EG, and their combinations) are conducted, isolating the effect of each module.\n\n* The RFC-only variant shows improvements across multiple perceptual metrics.\n\n* The study addresses a socially meaningful application (forensic sketch translation), potentially encouraging follow-up research in this niche."}, "weaknesses": {"value": "* The RFC design closely resembles known self-conditioning and recurrent refinement paradigms, offering limited conceptual advancement.\n\n* Both datasets are small or synthetic. The model’s generalizability to unconstrained, hand-drawn sketches or diverse domains is untested.\n\n* No comparisons are made against modern diffusion-based or transformer-based translators, which are now dominant in image synthesis tasks.\n\n* The recursive conditioning lacks formal analysis of convergence or potential instability over multiple iterations.\n\n*  The combination of SA and EG underperforms compared to the RFC-only model, suggesting redundancy and limited synergy.\n\n* Occasional verbosity and inconsistent terminology reduce readability in sections describing architecture and results."}, "questions": {"value": "1. How does RFC-GAN compare against diffusion-based iterative refiners or self-conditioning methods like SR3 or SelfRefine?\n\n\n2. Did the authors test beyond the face-sketch domain (e.g., general image restoration or object sketches) to validate cross-domain adaptability?\n\n\n3. Can the authors provide quantitative evidence of iterative convergence (e.g., change in FID per iteration) to support claims of refinement stability?\n\n\n4. What are the computational implications of recursive conditioning? Does the feedback loop significantly increase training time or memory usage?\n\n\n5. Could longer-term feedback (multiple previous outputs rather than one) be incorporated without instability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aPTcYjrsk7", "forum": "bGGv4WAGFJ", "replyto": "bGGv4WAGFJ", "signatures": ["ICLR.cc/2026/Conference/Submission24356/Reviewer_x4GL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24356/Reviewer_x4GL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786038375, "cdate": 1761786038375, "tmdate": 1762943054970, "mdate": 1762943054970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RFC-GAN, a conditional image-to-image generative adversarial network that reconstructs photorealistic faces from eyewitness sketches through iterative refinement. The key idea is Recursive Feedback Conditioning, at every training and inference step the previous generator output is concatenated to the sketch and passed back into both generator and discriminator."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "This paper proposes a feedback-conditioned generative framework and offers a principled approach to reliable memory-based facial reconstruction. The results show improvements in metrics such as FID."}, "weaknesses": {"value": "1)Lack of motivation: Many diffusion-based methods, such as ControlNet[1], already perform well. The paper lacks discussion of these related works and does not clearly justify the significance of using Pix2Pix as the baseline for improvement.\n2)Lack of comparison: Only vanilla Pix2Pix is evaluated. Stronger, task-specific baselines (e.g., diffusion or transformer models) are missing, making the practical advantage of RFC-GAN unclear.\n3)Poor writing quality: The writing is disorganized, and the figures are unclear. It's really hard to follow.\n[1] Zhang L, Rao A, Agrawala M. Adding conditional control to text-to-image diffusion models[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 3836-3847."}, "questions": {"value": "Please refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "joOVtuKpJK", "forum": "bGGv4WAGFJ", "replyto": "bGGv4WAGFJ", "signatures": ["ICLR.cc/2026/Conference/Submission24356/Reviewer_hXX8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24356/Reviewer_hXX8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998201958, "cdate": 1761998201958, "tmdate": 1762943054727, "mdate": 1762943054727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes a context-feedback training paradigm for image-to-image GANs. It is built on top of the Pix2Pix framework with Self-Attention and Edge-Gating modules. Experiments demonstrate improvements in FID and LPIPS."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "It is an interesting topic that could have real life impact."}, "weaknesses": {"value": "- It is unclear if this work is about memory-to-sketch or sketch-to-photo. It is expected to be a work for memory-to-sketch by looking at the title and the abstract. But it appears to be a sketch-to-photo work by looking at the adopted method (Sec. 4) and the evaluation metrics.\n- It is unclear to me what feedback is used as a condition in the GAN model. \n- It lacks an overview of the proposed method. It is not clear how the RFC, SA,  and EG modules are incorporated in the Pix2Pix model. The motivation and design details of these modules are missing as well.\n- Figures (Figs 1 & 2). The texts are too small to be recognised.\n-  It is hard to tell that the proposed method is effective by looking at the results."}, "questions": {"value": "- see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XuEnyMdGKm", "forum": "bGGv4WAGFJ", "replyto": "bGGv4WAGFJ", "signatures": ["ICLR.cc/2026/Conference/Submission24356/Reviewer_fQWo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24356/Reviewer_fQWo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762383361489, "cdate": 1762383361489, "tmdate": 1762943054192, "mdate": 1762943054192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}