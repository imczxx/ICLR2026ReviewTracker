{"id": "OgiLGrPgw5", "number": 15607, "cdate": 1758253094382, "mdate": 1759897295941, "content": {"title": "Structured Visual Landscape: Generating Preferred Representations in Multi-modal Biological and Artificial Neural Networks", "abstract": "Understanding how neurons responding to visual stimulus inputs is an important question in both deep learning and neuroscience. It has significant implications in enhancing the interpretability of black-box artificial neural networks and understanding the visual representation in biological neural networks. We proposed a structured visual representation landscape and design an activation score based prior that allows effectively regularizing the landscape with either activations from a brain region or units in neural networks. Our model Vis-Lens integrates a variational auto-encoder and diffusion model as an image generative model. It allows generation of natural realistic preferred images with directly modifying the activation-regularized latents, which avoids the tedious optimization procedure. We demonstrate the effectiveness of our framework in both artificial neural networks and biological neural networks with multi-modal response data derived from human visual cortex, including functional Magnetic Resonance Imaging (fMRI) and electroencephalography (EEG). Our framework outperforming state-of-the-art method on generating visual representations of those networks.", "tldr": "Develop a structured visual representation landscape constrained by activations to generating preferred representations in biological and artificial neural networks", "keywords": ["visual representation", "preferred images", "fMRI", "EEG", "generative models"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a060c8d37a62a15824529f3d6792c2bd690a510a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce VisLens which uses activation-regularized VAEs to generate images that drive or suppress activations in artifitial and biological NNs. This method learns a structured latent space that supports the generation of realistic and controllable stimuli without iterative optimization. The evaluation uses the NSD fMRI and Things-EEG datasets and different networks. Results show that compared to baseline methods such as BrainACTIV the proposed method generates images that better modulate activations, provides better cross-subject generalization and produces more realistic images."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "I was able to easily read and follow the paper, it flows well and I found no typography errors or awkward phrasing.\n\nThe results were properly tested on a wide variety of datasets, including EEG, fMRI, and multiple neural network activations.\n\nThe authors committed to releasing all the models and data\n\nCross subject generalization: This important aspect is rarely tested. I was happy to see the authors thoroughly examine this."}, "weaknesses": {"value": "Limited novelty, see contribution and soundness sections\n\nThe FID is a somewhat contrived metric. The distribution of the generated images might match the distribution of the original data more closely pixel wise. However, to examine that it is indeed more realistic a small human preference survey might be necessary.\n\nPrevious papers such as \"driving and suppressing the human language network\" recorded brain activity for the generated stimuli that is believed to maximally drive / suppress different ROIs. This might be excessive, but is an idea that might be worth exploring, especially for thingsEEG.\n\nThe methods seem to be sound as far as the proposed framework working. However, I believe the improvements in FID scores and other results are mainly due to the constraints of the regularized VAE inducing structure to the latent space that \"stabilize\" sampling during image generation. In other words, to generate images vectors are sampled from a latent space that was constrained to have a linear shape, and due to this constrain the risk of the data points being \"out-of-distribution\" (leaving the manifold of natural images in the latent space) is minimized, resulting in more coherent / realistic images. In other words, the improved performance seems to be due to better sampling during image generation, with the cost being that we have to learn a transformation into a more structured latent space. It is unclear to me that this method captures anything about the data that BrainACTIV does not, or that the results can not be beat by more sophisticated sampling in BrainACTIV pipeline. \n\nThe contribution as far as I can tell is that the authors learned an extra transformation into more structured space, allowing for better sampling during generation. It is unclear to me that this method captures anything about the data that BrainACTIV does not, or that the results can not be beat by more sophisticated sampling in BrainACTIV pipeline. More analysis would be needed in this direction."}, "questions": {"value": "I would suggest randomly picking N images generated by BrainActiv and the proposed method and examining if humans actually see any significant difference between them.\n\nPrevious papers such as \"driving and suppressing the human language network\" recorded brain activity for the generated stimuli that is believed to maximally drive / suppress different ROIs. This might be excessive, but is an idea that might be worth exploring, especially for thingsEEG."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R7RrvIG2TE", "forum": "OgiLGrPgw5", "replyto": "OgiLGrPgw5", "signatures": ["ICLR.cc/2026/Conference/Submission15607/Reviewer_ZZhw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15607/Reviewer_ZZhw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816702616, "cdate": 1761816702616, "tmdate": 1762925877787, "mdate": 1762925877787, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel two-stage method to generate minimally/maximally activating images for functional selectivity visualization of units or regions in biological (fMRI, EEG) and artificial neural networks. In the first stage, a VAE is trained to reconstruct CLIP embeddings while being regularized to a neural activation prior, creating a structured latent space where embeddings are organized from minimally to maximally activating. In the second stage, a reference image is encoded in this latent space, and interpolated there towards min or max activation; the interpolated embedding is then passed through the decoder, and the output, now in CLIP space, is used to guide the manipulation of the reference image with a diffusion model SDEdit. Compared to a recent prior baseline that does not have this structured latent space, the proposed method gives better results for minimizing brain activations, as well as for maximizing activations in some brain areas with mixed selectivity. Authors claim that the method is comparable to the baseline in the highly selective regions, and that it outperforms it when minimizing/maximizing activations in ANNs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers a novel method that improves performance of prior methods in certain respects (better activation minimization, maximization for mixed-selectivity), although not across the board. It is methodologically solid, and its advantage in minimizing activations can be clearly linked to the method, as stated in “our method produces greater decrease in activation in all regions, demonstrating effectiveness of defining an independent negative prior when constructing landscape rather than simply reverse the maximal direction in baseline”. Finally, the method is explained in a fairly clear manner, and is compared against recent state-of-the-art methods.\nHowever, presentation quality is quite poor, and some of the claims are not adequately supported with evidence."}, "weaknesses": {"value": "Listed from more major to more minor:\n1. Results (section 4.3.1) do not support the claim that the proposed method *generally* outperforms the Garcia et al. method used as baseline. Specifically, results across all ROIs (including those in the Appendix) show that it outperforms the baseline in the “Activation Decrease” but not consistently in the “Activation Increase”. This benefit for the “Decrease” is by itself an important contribution and should be presented as the main contribution of the paper.\n\n    For the “Activation Increase”, it is claimed that the method outperforms the baseline in mix-selectivity regions like IPS4, SPL1, 31a, TE2p, IP0, and IP1 while it is comparable in highly selective regions like FFA, OFA, EBA, and OPA. However, in OFA, FFA, and EBA, the method actually *underperforms* the baseline, if we are to judge the score difference by the same standards as in the other regions. Out of all the regions (in the main paper), it seems the ones the two methods do perform comparably on are TE2p, IP0, and OPA (2 mixed selectivity and 1 high), although it is hard to tell without statistical significance. A more accurate account would be that the method outperforms the baseline in 4/6 mixed-selectivity regions, and underperforms the baseline in 3/4 high-selectivity regions. Thus, the argument that the method is beneficial for “Activation Increase” in mixed-selectivity regions could potentially also stand as a secondary contribution of the paper, but it needs to also extend to the (high-level) ROIs in the Appendix. The disadvantage of the method for highly selective regions should also be made clear as a limitation. \n\n    Finally, the usefulness of generating maximally or minimally activating images for specific EEG electrodes (which do not have good spatial resolution and cannot be claimed to exclusively correspond to specific brain regions) is debatable. Also, EEG results do not show clear advantage against the baseline in decreasing or increasing activations.\n2. Similarly the evidence does not support the claim well enough in the results concerning preferred representations of ANNs instead of brain (section 4.3.2), where the improvement shown is not convincing at all with missing statistical tests, and the differences (esp. in B) looking very marginal. It is also unclear how the specific ANN units are chosen; would it not be better to show results across many units? Also, could this analysis not have been performed with the same images as the ones in NSD and THINGS instead of switching to ImageNet?\n3. No statistical significance is computed for the score differences between proposed method and baseline, making it hard to discern between meaningful differences and marginal ones. Also, no standard deviation is reported in Table 1, even though it exists in Figure 3B as vertical bars. The origin of the deviation shown in those bars is also not explained. Are they across all reference images from the test set? Also in Figure 4 no statistical test is performed between the box plots. Additionally, the reason for picking the specific ROIs for the main paper while leaving the rest to the Appendix should be better motivated.\n4. Results section 4.3.3 is overall hard to understand. Specifically, what is the “certain unit” that controls the latent space activated by and how is it chosen? Why is CIFAR used, which has really tiny and hard to interpret images? It would be good to also show how similar to the original are the images generated in this setting by the baseline method of Garcia et al.\n5. In the paper title and in many instances throughout the paper the word “Multimodal” is mentioned - it is not clear at all what this refers to. The datasets NSD and THINGS-EEG are not multimodal (they show visual stimuli to the subjects, multimodal would be if they showed other modalities of stimuli like multimodal video with sound, or text-image pairs).  If it refers to having both fMRI and EEG, the word “multimodal” is not commonly used to refer to this and is misleading, could be replaced with “multiple neural brain imaging methods” or similar for clarity. Same goes if “multimodal” refers to dealing with both neural data and images, the word would be misleading. If it refers to the embedding space coming from CLIP, this does not seem enough to justify such focus on multimodality, as only the visual encoder is used. Thus, it is suggested that the word “Multimodal” be removed.\n6. There are many issues in the Presentation quality of the paper. \n    - The writing (grammar, tenses, style) is very poor with numerous errors throughout the paper, which would be impossible to outline all of them here. In order to match the quality of the venue it needs to be read and corrected by a native or fluent English speaker, perhaps a colleague of the authors.\n    - The flow of ideas does not always make sense and some things are mentioned out of nowhere (i.e. do not follow from prior text), examples: “which avoids the tedious optimization procedure” (abstract - what is this procedure?), “brain computer interface applications” (line 106, 485 - like what?), “avoiding falling into local minimum in the optimization process” (line 161 - not mentioned as a problem of the baseline before), “pass it to a frozen SDEdit diffusion model” (line 312 - this model is not introduced in the section above, and the gamma parameter not defined). \n    - Contextualization relative to prior work is too historical (not relevant enough) in the first two paragraphs, it is not clear how GANs are related or the early work in feature visualization except from a purely historical perspective. In the Introduction with the current order of paragraphs 2 and 3, it is not made clear that the methods cited in paragraph 2 are also used for the same goal as the proposed method in paragraph 3. \n    - There are various problems with the paper figures. In Figure 2 it looks like posterior=prior in the top box, which is not true. Labels are missing from x, y, and z axes of the latent space. SDEdit is used but not explicitly shown in the method figure. Figure 3B does not show absolute scores clearly enough, they are very hard to discern with no ticks or intermediate values on the y axis. Figure 4 is missing titles for the figures on the right (should be Sim-CLR and ViT for top and bottom respectively)."}, "questions": {"value": "For the paper to be considered for acceptance (rating of 6 maximum), all of the above weaknesses need to be addressed with a concrete change in the manuscript.\n\nList of additional questions for clarification:\n- How are the two latent dimensions shown in Figure 1 obtained? Are they principal components of the latent space?\n- How is the scalar activation r computed? Is it a scalar across all voxels inside an ROI? Additionally, is it also averaged across stimuli?\n- Were absolute EEG amplitudes considered for r? As a (highly) negative response is also a high activation in EEG.\n- How are the prior means mu_pos and mu_neg “predefined”? How are they set?\n- In Table 2, is the held-out subject only considered for a single fold, or is it averaged across multiple folds of held-out subjects (cross-validated)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tkOG6MbDBC", "forum": "OgiLGrPgw5", "replyto": "OgiLGrPgw5", "signatures": ["ICLR.cc/2026/Conference/Submission15607/Reviewer_4H6N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15607/Reviewer_4H6N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924238046, "cdate": 1761924238046, "tmdate": 1762925877397, "mdate": 1762925877397, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for linking visual stimuli to neural activity by learning structured latent spaces that can generate images predicted to elicit specific activation patterns in biological and artificial neural networks. The approach integrates generative modeling and neural encoding to explore how visual features map onto activity within different regions of interest. The innovation lies in regularizing both high- and low-activity spaces, aiming to capture the full spectrum of meaningful neural variation. While the conceptual framework is interesting, the empirical validation is limited, with several aspects of the evaluation lacking quantification and clarity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important problem in computational neuroscience and interpretable AI—how to generate and analyze visual stimuli that meaningfully modulate neural activity. Its main innovation is to learn a latent space that is explicitly regularized for both high and low neural activity, capturing the full spectrum of neural tuning rather than focusing solely on activation increases. The inclusion of both increased and decreased activation directions is novel as far as I know and aligns with recent findings that low-activity patterns can also carry significant information. The technical implementation, combining a variational autoencoder with a diffusion model, is appropriate for the stated goals. The use of CLIP embeddings and pre-trained diffusion models provides a solid and practical foundation, even if not particularly innovative."}, "weaknesses": {"value": "- Despite the above strengths, the paper falls short in quantitative validation and clarity. The authors claim that their method helps to alleviate the effects of noise in neural data, but it remains unclear why this would be the case, and empirically it is not clear whether it actually is. \n\n\n- The paper does not show that the approach is more robust for particularly noisy data compared to data with higher signal-to-noise ratios, nor does it quantify how the proposed regularization contributes to stability. Beyond this, several technical aspects of the evaluation require clarification. \n\n\n- In Table 1, the reported activation changes are not accompanied by statistical measures, making it difficult to determine whether the improvements exceed inter-subject variability. The qualitative neuron examples in Figure 4 are interesting but insufficient to assess generality across the population. \n\n\n- The discussion section is very short and would benefit from additional depth, especially regarding the implications of the findings and how they relate to prior methods. The related work section on feature visualization also misses some recent and relevant studies, for example Fel et al. (2023) [1] which would help situate the contribution more clearly within current literature. \n\n\n- Another limitation is the lack of clarity on how much data is required to train the latent space for each region or neuron. If this is done on a per-neuron basis, the method may become data-intensive or impractical for broader application. The supplementary figures, while extensive, do not provide a clear sense of which method performs better or why. In addition, the generated images appear to have systematically higher contrast, which could itself drive neural responses; it is unclear whether contrast or similar low-level image properties were controlled for during the analysis.\n\n\n[1] https://arxiv.org/abs/2306.06805"}, "questions": {"value": "1. Can the authors provide quantitative evidence that their method is more robust to noise in neural data, and clarify why the proposed regularization should lead to such robustness? \n\n\n2. In Table 1, can they include measures of variability to demonstrate that the reported activation changes exceed inter-subject variance? \n\n\n3. Regarding Figure 4, how consistent are the neuron-level effects across the population, and are the differences shown in the plots statistically significant? It would also be helpful to clarify how low-level image properties, such as contrast, were controlled for, given their strong influence on neural activity. \n\n\n4. Finally, how much data is required to train the latent space, and does the data requirement scale with the number of neurons or regions of interest? Including predicted activations—for example using the DINOv2 encoding model—for the supplementary examples would also make the comparison between methods more interpretable. (edited)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aydOFXVwpn", "forum": "OgiLGrPgw5", "replyto": "OgiLGrPgw5", "signatures": ["ICLR.cc/2026/Conference/Submission15607/Reviewer_dETa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15607/Reviewer_dETa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957491829, "cdate": 1761957491829, "tmdate": 1762925876862, "mdate": 1762925876862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Vis-Lens, a novel framework for generating preferred visual representations in biological and artificial neural networks (ANN). The core contribution is the proposed activation-regularized prior for Variational Auto-Encoder (VAE), leverages activations r from a target brain region or ANN unit to organize the VAE's latent space. By sampling from this structured space and passing the modified latent embeddings to a frozen diffusion model, Vis-Lens can generate new images that effectively modulate target activations without requiring per-image optimization. Experiments on human brain data and image data demonstrate model effectiveness on brain and artificial neural networks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-\tA new, simple and efficient prior for VAE to learn structured visual latent landscape, conditioned on the brain activation.\n-\tThe proposed method shows advantageous performance compared to baseline, on multi-modal human brain data (fMRI, EEG) and image data in both activation modulation and image realism (FID). \n-\tPotentials for neuroscience analysis and brain visual understanding."}, "weaknesses": {"value": "1) The contributions seem limited.  The author does not provide intuition, explanation or theoretical support to justify why the proposed prior is effective. Why \\sigma_{prior} is defined as Eq2?\n2) Several technical details remain unclear:\n2.1)What is activation r? Does that value come from brain signal data like fMRI and EEG? What does it represent? How is it used for ANN?\n2.2)What are \\mu_{pos} and \\mu_{neg}?\n3)What are metrics Activation Increase and Decrease? What is its meaning and how is it calculated? \n4)The discussion of prior work seems limited; there have been several VAE and related machine learning approaches for similar problem (\"Generative decoding of visual stimuli.\" In International Conference on Machine Learning, pp. 24775-24784. PMLR, 2023; \"Hierarchical VAEs provide a normative account of motion processing in the primate brain.\" Advances in Neural Information Processing Systems 36 (2023))\n5) Ablation studies (Table 3) show that using structured landscape with latent modification (main method) obtain lower \\delta_r than standard VAE (0.046 vs 0.067), raising concern about method effectiveness. What are the results on brain data?\n6) Lacking parameter studies. How hyperparameters such as \\lambda, \\alpha, are selected and how they contribute to the performance?\n7) How training/inference time of Vis-Lens compared to baseline?"}, "questions": {"value": "1) The contributions seem limited.  The author does not provide intuition, explanation or theoretical support to justify why the proposed prior is effective. Why \\sigma_{prior} is defined as Eq2?\n2) Several technical details remain unclear:\n2.1)What is activation r? Does that value come from brain signal data like fMRI and EEG? What does it represent? How is it used for ANN?\n2.2)What are \\mu_{pos} and \\mu_{neg}?\n3)What are metrics Activation Increase and Decrease? What is its meaning and how is it calculated? \n4)The discussion of prior work seems limited; there have been several VAE and related machine learning approaches for similar problem (\"Generative decoding of visual stimuli.\" In International Conference on Machine Learning, pp. 24775-24784. PMLR, 2023; \"Hierarchical VAEs provide a normative account of motion processing in the primate brain.\" Advances in Neural Information Processing Systems 36 (2023))\n5) Ablation studies (Table 3) show that using structured landscape with latent modification (main method) obtain lower \\delta_r than standard VAE (0.046 vs 0.067), raising concern about method effectiveness. What are the results on brain data?\n6) Lacking parameter studies. How hyperparameters such as \\lambda, \\alpha, are selected and how they contribute to the performance?\n7) How training/inference time of Vis-Lens compared to baseline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mDktuKfSgl", "forum": "OgiLGrPgw5", "replyto": "OgiLGrPgw5", "signatures": ["ICLR.cc/2026/Conference/Submission15607/Reviewer_QtiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15607/Reviewer_QtiM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15607/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762233354437, "cdate": 1762233354437, "tmdate": 1762925875901, "mdate": 1762925875901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}