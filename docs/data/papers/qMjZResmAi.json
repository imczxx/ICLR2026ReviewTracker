{"id": "qMjZResmAi", "number": 427, "cdate": 1756739092486, "mdate": 1762953238922, "content": {"title": "MT-Video-Bench:  A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues", "abstract": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI’s ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce \\textbf{MT-Video-Bench}, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.", "tldr": "", "keywords": ["Multimodal Large Language Models", "Multi-turn Video Dialogue", "Benchmark Evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/460f343452c1e58dc28f30bbfb12eb1060435027.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this work, the authors propose a MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, they carefully collect and annotate 987 multi-turn dialogues in six core capacities of perceptivity and interactivity. Finally, they evaluate various state-of-the-art MLLMs to show the potential of proposed benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clarity\n\nThe paper is well-written with good structure. Hence, the clarity is basically good.\n\n* Significance\n\nThis paper focuses on evaluating multi-turn video dialogue capacity of MLLMs, which is an important and practical problem for video understanding. Hence, the significance is basically OK for video research community."}, "weaknesses": {"value": "* Capacity Evaluation\n\nIn this work, the authors propose to evaluate six core capabilities of perceptivity (object reference, memory recall, content summary) and \ninteractivity (answer refusal, topic shifting, proactive interaction). However, the reason why to choose these tasks or capacities is not well explained. I do understand that, in general, it is necessary to evaluate perceptivity and interactivity, especically for long videos. \n\n1) Then, why to choose these six tasks? It that sufficient to evaluate both perceptivity and interactivity? For example, in the perception test benchmark [ arXiv:2305.13786], the tasks are designed, inspired by human perception screening tests in developmental psychology or medicine. \n\n2) Please show various capacities by visualizing more videos. It makes these capacities more understandable.\n\n* Reference\n\nThe recent work [VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos, ICCV 2025] proposes the similar topic for video understanding. Please clarify the key difference.\n\n* Method Insight\n\nIt woule be more interesting to investigate or indicate how to design MLLMs to tackle the tasks in this benchmark.\n\n* Small Size\n\nThe authors collected only 135 original videos from online platforms. The small number of videos would restrict the generalization of this  benchmark."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "erSZlHZjib", "forum": "qMjZResmAi", "replyto": "qMjZResmAi", "signatures": ["ICLR.cc/2026/Conference/Submission427/Reviewer_qhiz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission427/Reviewer_qhiz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793791453, "cdate": 1761793791453, "tmdate": 1762915517836, "mdate": 1762915517836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "TJBQIjFLKF", "forum": "qMjZResmAi", "replyto": "qMjZResmAi", "signatures": ["ICLR.cc/2026/Conference/Submission427/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission427/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762953238272, "cdate": 1762953238272, "tmdate": 1762953238272, "mdate": 1762953238272, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MT-Video-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in multi-turn video-grounded dialogues. The benchmark spans six core capabilities—three perceptual (Object Reference, Memory Recall, Content Summary) and three interactive (Answer Refusal, Topic Shifting, Proactive Interaction)—comprising 987 dialogues across 135 videos. The authors conduct extensive evaluations of 20 open-source and closed-source MLLMs, revealing significant performance gaps and challenges, particularly in cross-scene reasoning and interactive tasks. While the benchmark addresses an important gap in video-dialogue evaluation, the paper suffers from methodological omissions, insufficient ablation studies, and ambiguities in novelty claims, which undermine its overall contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper introduces MT-Video-Bench which covers diverse video categories and tasks, with balanced data distribution (Figure 3).\n2. Extensive experiments on 20 models provide a holistic performance analysis, highlighting limitations."}, "weaknesses": {"value": "1. In Figure 2, The data construction lacks details on human annotation guidelines.\n2. Reliance on Gemini-based checklists risks circularity, as the top-performing model (Gemini 2.5 Pro) may align better with Gemini-generated metrics.\n3. The metric of accuracy (ACC) based on binary yes/no questions oversimplifies the nuanced requirements of multi-turn dialogues. For example, tasks like \"Proactive Interaction\" involve open-ended reasoning but are reduced to checklist-based scoring, which may not capture conversational fluency or coherence. And the paper does not report inter-annotator agreement for human validation of checklists, undermining the reliability of the evaluation protocol.\n4. The uniform sampling of 32 frames (Section 4.1) may not optimally represent long videos (e.g., those exceeding 15 minutes in Figure 3c).  The paper does not justify this choice or explore adaptive sampling methods, potentially disadvantaging models designed for long-context processing. \n5. Models like InternVL3.5 and InternVideo2.5 use different resolutions (448×448 vs. 728×728), but the impact of resolution variations on performance is not analyzed.  This inconsistency introduces confounding factors in comparisons.\n6. The contribution and completeness are insufficient; it would be better if there were a simple, adaptable baseline that could partially address the benchmark challenge."}, "questions": {"value": "1. Were ablation studies conducted to compare Gemini-generated metrics with those from other LLMs (e.g., GPT-4o) or human? If not, how can the authors rule out model-specific bias?\n2. Did the authors explore other sampling strategies?\n3. How can fair comparisons be made when resolution varies across models? \n4. Is there a plan to open source the code? This is very important for reproducing the results of the paper.\n5. Although the paper claims that its method significantly improves performance, it lacks sufficient experimental data and significance analysis to support this conclusion, and the results in Table 1 show only a slight improvement.\n6. Could the authors provide more visualization samples?\n7. Given the complexity of the benchmark, will the code for data generation, evaluation checklists, and model inference be open-sourced?  This is critical for community adoption and validation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cLUyEDdl59", "forum": "qMjZResmAi", "replyto": "qMjZResmAi", "signatures": ["ICLR.cc/2026/Conference/Submission427/Reviewer_Wc4u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission427/Reviewer_Wc4u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879952811, "cdate": 1761879952811, "tmdate": 1762915517650, "mdate": 1762915517650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. The benchmark comprises 987 carefully curated dialogues across 135 videos. Based on MT-Video-Bench, the authors provide a detailed evaluation of both open-source and closed-source models, highlighting the current limitations and performance discrepancies in different abilities."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose a new video understanding benchmark for evaluating MLLMs in multi-turn dialogues.\n2. Various open-source and close-source models are evaluated."}, "weaknesses": {"value": "1. The comparison to existing video understanding benchmarks is problematic. For example, SVBench has 49,979 QAs while the authors claim that SVBench only has 7,374 QAs in Table 1. Moreover, the authors define long videos as videos of more than 10 min. However, the sample only has 5.88 turns in dialogue. Such a number of turns cannot fully leverage information in a \"long video\". Moreover, while SVBench has cross-scene dialogues, the authors further define \"cross-scene\" as more than scenes. That's not a good definition as \"cross-scene\" is already defined in English.\n2. The novelty of this benchmark compared to SVBench is weak. SVBench is also a video understanding benchmark for evaluating MLLMs in multi-turn dialogues, even with far more QAs and videos.\n3. Important information about data is missing, e.g., the source of the videos and the distribution of video categories.\n4. The evaluation is inappropriate. While the average video length is greater than 10 min, the authors only resample 32 frames of every video. Most video information is lost in the input.\n5. The evaluation will be unavailable to reproduce in the future. Since Google continually deprecates the API of old Gemini models, Gemini 2.5 Flash used in the evaluation will be unavailable in the future. \n6. This benchmark uses only Gemini 2.5 Flash to generate video descriptions and dialogues, which can bring model bias."}, "questions": {"value": "Please reply to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TtKtfgs53t", "forum": "qMjZResmAi", "replyto": "qMjZResmAi", "signatures": ["ICLR.cc/2026/Conference/Submission427/Reviewer_ujFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission427/Reviewer_ujFi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903719233, "cdate": 1761903719233, "tmdate": 1762915517533, "mdate": 1762915517533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new video understanding benchmark with multi-turn dialogues for MLLMs. The benchmark mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. With MT-Video-Bench, the authors evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed benchmark contains 987 meticulously curated multi-turn dialogues from diverse domains, which assesses capabilities rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring.\n2.\tThe experimental results reveal that current MLLMs are still weak in multi-turn dialogues for videos.\n3.\tVarious MLLMs with different sizes are evaluated in experiments."}, "weaknesses": {"value": "1.\tThis paper excessively downplays the advantages of SVBench and overstates the novelty of the proposed benchmark. SVBench already contains multi-turn dialogues for videos across multiple scenes. Moreover, the information of SVBench in Table 1 is inconsistent with which in the paper of SVBench.\n2.\tThe source and language of the collected data is not provided.\n3.\tThe videos are overly compressed in the evaluation. Only 32 frames of each video is input into the evaluated MLLMs, while the authors claim that videos in their benchmark have a average length of 10 minutes.\n4.\tThe impacts of video resampling strategies should be investigated.\n5.\tThe results of human is not provided in experiments, which is important for assessing the abilities of MLLMs.\n6.\tThis benchmark only adopts Gemini 2.5 Flash in data annotation and evaluation. The trustworthiness of employing this model instead of other MLLMs and humans should be investigated.\n7.\tThe authors do not provide new methods for video understanding in multi-turn dialogues."}, "questions": {"value": "1.\tWhat is the novelty of the proposed benchmark compared to SVBench?\n2.\tWhat’s the source and language of your videos?\n3.\tWhat are your criteria of video sampling in evaluation?\n4.\tHow well can humans perform on your benchmark?\n5.\tIs Gemini 2.5 Flash reliable in data annotation and evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yyXcj0OOW8", "forum": "qMjZResmAi", "replyto": "qMjZResmAi", "signatures": ["ICLR.cc/2026/Conference/Submission427/Reviewer_Gg8Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission427/Reviewer_Gg8Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission427/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910112703, "cdate": 1761910112703, "tmdate": 1762915517430, "mdate": 1762915517430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}