{"id": "FLXZOIDrli", "number": 22782, "cdate": 1758335365977, "mdate": 1759896846446, "content": {"title": "RAD3D-Prefix:  Anomaly-Aware Prefix Learning on Frozen LLM for 3D CT Image to Report Generation", "abstract": "Recent advances in multimodal learning, including large language models (LLMs) and vision-language models (VLMs aka foundational models), have demonstrated strong adaptability to natural images. However, extending their use to the medical domain, particularly for volumetric (3D) images, is challenging due to high computational complexity and the need to model volumetric dependencies. The significant misalignment between visual and textual features further limits the ability to leverage the strength of LLMs, and naively fine-tuning these models on limited medical data often leads to overfitting and underperformance on downstream tasks. In this study, we address these challenges for volumetric radiology scans (specifically CT) report generation by introducing a simple, lightweight approach that minimizes the need for extensive parameter training. Our solution, called RAD3D-Prefix, employs a novel anomaly-aware prefix learning module that effectively aligns visual features from 3D images with textual features. This module integrates image embeddings with multi-label diagnostic classification logits, preserving critical clinical details while bridging the vision-language gap. By keeping the LLM frozen, our method requires minimal trainable parameters and mitigates the risk of overfitting on small, domain-specific datasets. Across four different evaluation criteria, RAD3D-Prefix outperforms existing similar-sized models and performs comparably to larger models that have more than five times the number of trainable parameters. Our approach demonstrates superior clinical relevance and out-of-domain generalization, highlighting the effectiveness of our lightweight, anomaly-aware prefix projection module.", "tldr": "", "keywords": ["Report generation", "3D CT scans", "medical image analysis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f065c46a08cd104501804710faaeff8e1f2dfe0b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper addresses automatic clinical report generation from 3D CT volumes by aligning volumetric visual features with domain-specific text, and avoiding overfitting when fine-tuning large LLMs on small medical datasets.\n\nThe authors propose RAD3D-Prefix, a lightweight, anomaly-aware prefix learning module that keeps the LLM frozen. They concatenate 3D image embeddings with multi-label diagnostic classification logits and pass this through a small transformer “projector” to produce a fixed-length prefix token sequence that conditions the LLM to generate reports. This aims to bridge the vision-language gap while training only a small number of parameters.\n\nThe specific claimed contributions are:\n\n(1) anomaly-aware 3D to text prefix projector that fuses image features with diagnostic logits for frozen LLMs; \n(2) a systematic study of prefix design and LLM tuning regimes across sizes; \n(3) use of medical-specific metrics to ensure clinical relevance; \n(4) state-of-the-art or comparable performance to larger or domain-specialized systems with far fewer trainable parameters."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper proposes a lightweight prefix projection module that enables parameter-efficient report generation by keeping the large language model (LLM) frozen. \n\nThe authors adapt prefix-based conditioning to the 3D CT imaging domain. The integration of anomaly classification logits potentially improves alignment between visual findings and textual descriptions.\n\nDespite using a small 1B parameter LLM, the proposed RAD3D-Prefix model achieves competitive or superior scores on GREEN, a metric designed to measure clinical factuality in radiology reports. \n\nThe authors present some ablation study comparing prefix-only (V-2), fine-tuned (V-1), and anomaly-aware prefix (V-3) variants across two datasets. \n\nThe model is tested not only on CT-RATE (chest CT reports), but also on INSPECT, a dataset focused on pulmonary embolism."}, "weaknesses": {"value": "1. Limited novelty presented in this paper. The core idea of projecting image features into a frozen LLM using prefix tokens is not new. It follows the “Prefix Tuning” paradigm first established by Mokady et al. (2021) [1] where visual tokens are projected into the text decoder via a learned projection network. More recently, Wang et al. (2023) [2] adapted this idea to radiology, using a learned prefix from visual features for report generation with frozen LLMs. The paper simply adds on anomaly logits, a marginal extension, not a conceptual innovation. \n\n2. The experiments are limited in scope. It is already well-established in previous works (e.g., LLaVA, BLIP-2) that large frozen models benefit from learned conditioning tokens. The LLM tuning strategy (e.g., small models benefit from fine-tuning, large models from freezing)  has been established in parameter-efficient tuning (e.g LoRA). There is no new interpretation or theory relating to the proposed model. \n\n3. The model performance on out-of-domain dataset INSPECT is exceptionally low. This raises serious concerns on the applicability to clinical settings. \n\n4. The claimed inclusion of multi-label logits only marginally improves the performance, as shown in Table 4. Furthermore, the paper states that the logits are obtained from a pretrained classifier and not updated along with the model. This is somewhat surprising and raises further questions such as how are these logits normalized and handled to match the scale of the visual features? There is no learning signal to tell the model how or when these logits help generate better reports.\n\n5. Why is there not even a basic analysis of model training/inference costs in the ablation study?"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3c98MmqTKK", "forum": "FLXZOIDrli", "replyto": "FLXZOIDrli", "signatures": ["ICLR.cc/2026/Conference/Submission22782/Reviewer_bop9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22782/Reviewer_bop9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760649010974, "cdate": 1760649010974, "tmdate": 1762942385995, "mdate": 1762942385995, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RAD3D-Prefix proposes a lightweight model that aligns pretrained CT image encoders with text features from generic pretrained frozen LLMs to improve CT image VLM analysis. Specifically, RAD3D-Prefix proposes a transformer-based projection network that aligns the two pretrained models by combining information an image encoder and a multi-anomaly classification model to produce a prefix that can be appended to a text prompt for an LLM. The authors evaluate the approach on a variety of report generation tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated problem. Efficient domain adaption of LLMs seems like an important area for reducing costs of training large models.\n- Certain results seem promising if evaluated more comprehensively."}, "weaknesses": {"value": "- **Text organization/clarity needs to be improved**\n    - Line 206 and 239: The authors repeatedly use the term “bridge the gap”. However it is vague what this is referring to. I think you mean aligning two modalities here, but it is unclear from the text they are referring to alignment or something else.\n    - Concepts are introduced in an order that makes the text difficult to follow. For instance, V1, V2 and V3 are defined in the intro. But are not used until section 4. This makes readers have to jump between the beginning and end of the paper to understand what’s happening. \n    - Table 1 is placed on page 6 but not referred to until page 8.\n- **Unclear description of technical framework**\n    - Line 248: it is unclear how the “linear transformation” is applied onto z_i. It is also unclear what is “structured sequence” referring to? How is the structure defined? What exactly is the final output from this self-attention layer?\n    - Line 251: “capture complex dependencies”. What kinds of complex dependencies is it supposed to capture? Typically self-attention captures relationships between tokens\n    - Line 213: Can we formalize where $l_i$ comes from? It is stated that it comes from a separate classification head on image encoder. Does this require labels for training then? \n    - Line 253: where does the $\\hat{R}$ embeddings come from? it is not clear.\n    - Line 241: how does mapping visual features to sequence of LLM tokens make it anomaly aware? The connection is not very clear to me. Does anomaly aware refer to the classification logits\n    - Equation 2: its unclear to me what are the $\\theta$’s being optimized. Why are we doing next embedding prediction? does the training process not require the LLM at all? If not, then how can it learn the best tokens with respect to a specific LLM? \n    - Figure 3: Im not sure what these learnable constants are. Why do they not show up in the equations anywhere? \n- **Unclear and inconsistent notation**: \n    - Line 247: does plus dot mean concatenate?\n    - I dont understand what $(C.t_p.p_1.p_2)$ means in line 226. Similarly $(B.T)$ in line 229\n    - Equation 1:  i recommend the authors to be consistent with bold notation. Currently, $z_i$ is a vector and is bolded. However, later $l_i$ is also a vector but not bolded. Are vectors bolded or not in this paper? \n- **Limited Evaluations**:\n    - Can the authors clarify why the V1, V2, and V3 baselines are relevant with respect to the rest of the literature. \n    - Figure 4: Can you explain why the largest model (1.6B) has the worst performance during fine-tuning? Is this model fine-tuned correctly?\n    -  Table 2: why did you only evaluate RAD3D-prefix on only one LLM backbone? Shouldn’t you try to evaluate across multiple backbones to show generality of the method? Does this approach scale to larger backbones like R2GenGPT does? Also which variant is reported in Table 2 for RAD3D-Prefix?\n    - Table 3: Why not also compare with the baselines from table 2? \n    - Figure 5: can you show examples that compare between V1, V2, and V3?"}, "questions": {"value": "1. How would this method generalize to unseen anomalies? It seems like the proposed approach relies on having predefined labels and a curated dataset for training. \n2. In table 4, why do the contributions work for some scores and datasets, but not others? The performance seems to be dataset and metric dependent.\n3. a core motivation seems to be reducing computational costs. Can the authors quantify how much training efficiency has improved as a result of this approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IktI8vaE43", "forum": "FLXZOIDrli", "replyto": "FLXZOIDrli", "signatures": ["ICLR.cc/2026/Conference/Submission22782/Reviewer_BUQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22782/Reviewer_BUQA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760899141026, "cdate": 1760899141026, "tmdate": 1762942385725, "mdate": 1762942385725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RAD3D-Prefix, a lightweight framework for radiology report generation from 3D CT scans. The method keeps a pretrained LLM frozen and learns a small anomaly-aware prefix projection module that maps 3D visual embeddings and multi-label abnormality logits into the LLM token space. The approach aims to reduce computational cost and overfitting while improving clinical alignment. Experiments on CT-RATE and INSPECT show competitive text-generation metrics and higher GREEN (clinical accuracy) scores compared to prior models such as CT2Rep and E3D-GPT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and motivated by practical concerns in medical imaging, namely the high cost of fine-tuning large LLMs on limited 3D data. The idea of using prefix learning for frozen LLMs is simple and parameter-efficient, and the inclusion of anomaly logits provides some domain relevance. Experiments are extensive and cover both in-domain and out-of-domain datasets, with reasonable baseline comparisons."}, "weaknesses": {"value": "- Despite solid engineering execution, the conceptual novelty is limited. Prefix-tuning and lightweight alignment for frozen LLMs are already well-established in both natural-image and medical domains. The proposed “anomaly-aware” extension is a concatenation of abnormality logits to the prefix embedding, which offers little methodological innovation or theoretical insight. The paper does not clearly articulate why this integration constitutes a new learning mechanism rather than a routine design choice.\n\n- The empirical evidence is also weak in supporting strong claims. Reported gains in BLEU or ROUGE are small and often inconsistent across datasets. The primary metric improvement lies in GREEN, but the calculation and clinical validation of this metric are insufficiently described. Statistical significance, variance, and inter-observer consistency are missing, leaving uncertainty about the robustness of the results.\n\n- The experimental scope is narrow. Both datasets are chest CT–based, so generalization to other modalities (MRI) or anatomical regions is untested. The model’s reliance on CT-CLIP and pretrained classifiers also confounds attribution of performance gains—improvements may stem from better visual encoders rather than the prefix design. \n\n- The claimed anomaly awareness and interpretability are largely unsubstantiated. There is no qualitative analysis or visualization showing that the prefix module meaningfully encodes pathology patterns or improves factual correctness. The prefix mechanism acts as a generic adapter, not a clinically interpretable reasoning step."}, "questions": {"value": "Please refer to the Weaknesses section.\n\n**I am willing to raise my score according to the rebuttal.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FN3ptNoaEh", "forum": "FLXZOIDrli", "replyto": "FLXZOIDrli", "signatures": ["ICLR.cc/2026/Conference/Submission22782/Reviewer_ws6g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22782/Reviewer_ws6g"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684640558, "cdate": 1761684640558, "tmdate": 1762942385422, "mdate": 1762942385422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses 3D CT report generation, identifying that fine-tuning large LLMs on small medical datasets leads to overfitting. To overcome this issue. The authors proposes RAD3D-Prefix as a solution. RAD3D-Prefix is a transformer-based prefix projector that fuses a 3D image embedding (from CT-CLIP) with multi-label abnormality logits and learned “prefix tokens” to a frozen LLM (LLaMA-3.2-1B) with the goal is to align volumetric CT features with textual report space while training only the projector, hence reducing overfitting on small medical datasets. Different variants of the architecture are proposed. Experiments on CT-RATE and INSPECT show that freezing larger LLMs with the proposed prefix improves report metrics compared to linear projection baselines and several prior systems (e.g., R2GenGPT, CT2Rep)"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This is a well-executed paper. The core idea—to create an \"anomaly-aware\" prefix by concatenating visual features with multi-label classification logits is a simple but effective. The frozen encoder (CT-CLIP), lightweight projector, and frozen LLM are . The anomaly-aware logit fusion is simple and clinically motivated. \n2. The paper has good structure and is relatively easy to follow."}, "weaknesses": {"value": "1. Missing radiologist (human expert) assessment, no error taxonomy. Although several metrics have been used to evaluate the report quality, these N-gram metrics are known to be weak proxies for radiology quality. The authors are recommended to include a small evaluation set from domain expert to make sure that the proposed method actually work in the real-world setting. \n2. The core idea (prefix learning) is known here, I feel the novelty here is mainly 3D + anomaly-logit fusion into a multi-token prefix with a frozen LLM. This is a useful contribution indeed, but largely incremental relative to linear-projector baselines so I recommend the authors to sharpen what is fundamentally new"}, "questions": {"value": "1. I wonder how do performance and compute trade off as you vary the learned prefix length and the projector’s capacity (layers/hidden size)? Could you show this.\n\nI'd be happy to increase my score if the authors can address my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "YayannoTsL", "forum": "FLXZOIDrli", "replyto": "FLXZOIDrli", "signatures": ["ICLR.cc/2026/Conference/Submission22782/Reviewer_NL8p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22782/Reviewer_NL8p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22782/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975923254, "cdate": 1761975923254, "tmdate": 1762942385211, "mdate": 1762942385211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}