{"id": "WPJnLvQrP3", "number": 10243, "cdate": 1758164969386, "mdate": 1759897663961, "content": {"title": "Prompt and Parameter Co-Optimization for Large Language Models", "abstract": "Prompt optimization and fine-tuning are two major approaches to improve the performance of Large Language Models (LLMs).\nThey enhance the capabilities of LLMs from complementary perspectives: the former through explicit natural language, and the latter through implicit parameter updates. \nHowever, prior work has typically studied them in isolation, leaving their synergistic potential largely underexplored. To bridge this gap, in this paper, we introduce MetaTuner, a novel framework that jointly integrates prompt optimization and fine-tuning for LLM training.\nSpecifically, we introduce two neural networks to generate prompts and parameters, respectively, while allowing them to share a common bottom encoding layer to enable knowledge sharing.\nBy the guidance of the final supervised signals, our framework is optimized to discover the optimal combinations between the prompts and parameters.\nGiven that prompt learning involves discrete optimization while fine-tuning operates in a continuous parameter space, we design a supervised regularization loss to train our framework effectively.\nExtensive experiments across diverse benchmarks show that our method consistently outperforms the baselines.  To benefit the research community, we have released our project at https://anonymous.4open.science/r/metatuner.", "tldr": "This paper proposes MetaTuner, a novel framework that jointly optimizes prompts and parameters of LLMs through a knowledge-sharing mechanism and a supervised regularization loss, achieving superior performance across multiple benchmarks.", "keywords": ["prompt-parameter co-optimization", "shared-private parameterization", "supervised regularization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cb6af0e88fcfa0745a20b48fc6e7263d503fd5c6.pdf", "supplementary_material": "/attachment/32d1437a8c7dae8caf4c1072041924c56dea74b5.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes MetaTunner which is a framework that jointly optimizes both prompts and model parameters for finetuning LLMs. They design a prompt generator that outputs discrete prompts and a parameter generator that produces LoRA style updates, this two modules are trained jointly, with an additional ad-hoc supervised regularization loss on prompt. Experimetns on various QA datasets such as MATH, GSM8K show their proposed method outperforms various baselines including prompt tuning, LoRA tuning, and the closely related hybrid BetterTogether approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper address an important direction that the prompt for a downstream tasks need to be engineereed and might not be optimial, finetuning both prompt and model offers a more unified task adaptation framework.\n2. The propose architecture is clear and experiment results are good."}, "weaknesses": {"value": "1. The proposed framework is essentially a composition of existing components: prompt generation + LoRA + shared encoder, which is incremental and lack clear conceptual novelty. The idea of optimizing both prompt and model itself is well-explored in studies like BetterTogether, which the authors themselves cite but weakly differentiate from.\n2. Instead using soft prompts that can truely trained jointly end-to-end, it is questionable why this paper choose discrete prompts generated by a separate LLM, although it may help interpretability. More insights or analysis on this should be discussed for why discrete prompts are preferable. \n3. The supervised regularization loss to handle discrete-continuous issues feels ad hoc and unstable, it is based on an auxiliary “expert prompt” dataset, and the algorithm itself has no analysis of convergence.\n4. The evaluation is limited to 4 QA benchmarks and a single Qwen model family with two different sizes, the method's generalizability to other domains and models remains untested."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q9LDPJACzC", "forum": "WPJnLvQrP3", "replyto": "WPJnLvQrP3", "signatures": ["ICLR.cc/2026/Conference/Submission10243/Reviewer_LrBX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10243/Reviewer_LrBX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762450550, "cdate": 1761762450550, "tmdate": 1762921602403, "mdate": 1762921602403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a joint framework for LLM prompt optimization and parameter training. Clear gains are demonstrated over prompt optimization and fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear and concise description of the approach \n- The evaluation covers a very good range of prompt optimization and fine-tuning methods\n- Strong and consistent gains over the baselines across 4 datasets and 2 model sizes\n- Interesting and well-designed ablation studies"}, "weaknesses": {"value": "- A major benefit of prompt optimization is infrastructure simplicity and efficiency - serving the same frozen model for multiple tasks is much cheaper than multiple dedicated models. This benefit is lost here.\n- The whole system design is quite complex, but the strong gains and comprehensive ablations make up for it\n- Given the otherwise broad experimental setup, I find it a bit unfortunate that - iiuc - fine-tuning is limited to LoRA. An isolated small-scale experiment with full fine-tuning of the 3B model would tell us more about the head room and would help to better understand the gains of MetaTuner.\n\nMinor comments:\n- L107: \"In LLMs, we typically work with input-output pairs\" is too simplistic/general - e.g. think LLM pre-training etc.\n- Fig. 1 font size is too tiny"}, "questions": {"value": "- Can you say something about the computational training costs compared to regular prompt optimization / fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3dv119YGGz", "forum": "WPJnLvQrP3", "replyto": "WPJnLvQrP3", "signatures": ["ICLR.cc/2026/Conference/Submission10243/Reviewer_RbJU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10243/Reviewer_RbJU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942773558, "cdate": 1761942773558, "tmdate": 1762921602013, "mdate": 1762921602013, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, a framework to jointly optimize prompt and parameter is proposed. The framework is named after metaTuner. During training time, the initial prompt is given to the meta encoder to rewrite with a language model A. In the meantime, there is a language model B to generate the parameter which is used to optimize the downstream actor model. The actor model takes the generated parameter and the the rewritten prompt as input to generate the answer. The response is used to compare with the ground truth and get the task specific reward, which is used to update the model A and B."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed approach is easy to follow and understand. \n- The proposed approach has been verified in multiple SFT datasets."}, "weaknesses": {"value": "- Concerns about the generalization of the model trained with the proposed approach. The model is trained to optimize for several specific task verticals, including prompt rewrite and parameter generation. one concern is about catastrophic forgetting: is the model still able to generalize to other unseen dataset during finetuning? Some OOD evaluation will helps.\n- One hypothesis of the performance win is that the introduction of metaTuner actually increased the total number of parameters of the whole system and the scaling up of the model itself is the source of improvement. Is it possible to validate this point so we can confirm that the wins are from the approach instead of the model scaling up?"}, "questions": {"value": "- Concerns about the generalization of the model trained with the proposed approach. The model is trained to optimize for several specific task verticals, including prompt rewrite and parameter generation. one concern is about catastrophic forgetting: is the model still able to generalize to other unseen dataset during finetuning? Some OOD evaluation will helps.\n- One hypothesis of the performance win is that the introduction of metaTuner actually increased the total number of parameters of the whole system and the scaling up of the model itself is the source of improvement. Is it possible to validate this point so we can confirm that the wins are from the approach instead of the model scaling up?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "r24EIKS29a", "forum": "WPJnLvQrP3", "replyto": "WPJnLvQrP3", "signatures": ["ICLR.cc/2026/Conference/Submission10243/Reviewer_ttpV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10243/Reviewer_ttpV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975930805, "cdate": 1761975930805, "tmdate": 1762921601590, "mdate": 1762921601590, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MetaTuner, a unified framework for the joint optimization of prompts and model parameters in large language models (LLMs). Traditional approaches typically treat prompt optimization, discrete and external, and fine-tuning, continuous and internal, as separate processes. MetaTuner bridges these two paradigms through a shared meta-encoder that feeds into two complementary decoders: a prompt decoder that generates discrete natural language prompts and a parameter decoder that produces continuous LoRA-based parameter updates. To address the challenge of non-differentiability in prompt generation, the framework incorporates a supervised regularization loss that leverages an “expert dataset” of prompts, partly generated using GPT-4o. This mechanism stabilizes training and allows efficient co-optimization of both prompt and parameter spaces. Experiments on MATH, GSM8K, HotpotQA, and CosmosQA, using Qwen2.5-3B and Qwen2.5-7B backbones, demonstrate that MetaTuner achieves consistent performance gains over prompt-only, fine-tuning, and hybrid baselines such as BetterTogether."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Unified Framework: Introduces MetaTuner, a single architecture that jointly optimizes prompts and model parameters, integrating discrete prompt learning and continuous fine-tuning.\n\nDual-Decoder Design: Proposes a shared meta-encoder with separate prompt and parameter decoders that generate textual prompts and LoRA-based parameter updates in a coordinated manner.\n\nSupervised Regularization: Employs a regularization loss guided by an expert prompt dataset to stabilize training and address non-differentiability in prompt optimization.\n\nProper Evaluation: Demonstrates consistent performance gains across multiple reasoning and QA benchmarks, supported by detailed ablations and generalization analyses.\n\nIn addition, the paper is well-written and proper examples are provided."}, "weaknesses": {"value": "The paper could more clearly differentiate MetaTuner from sequential prompt–fine-tuning pipelines such as Chain-of-Thought distillation (Finlayson et al., 2025; Wei et al., 2022). A stronger conceptual contrast would help clarify the advantages of end-to-end co-optimization over staged approaches. Also a comparison with Self-Instruct (Wang et al., 2022) would strengthen the paper’s positioning. While both integrate prompting and fine-tuning.\n\nGenerating both prompts and LoRA parameters for each query introduces latency and computational cost, which may limit scalability and deployment in real-time settings.\n\nThe supervised regularization relies on GPT-4o-generated expert prompts, creating a dependency on an external model. The impact of weaker or smaller dataset on performance is not analyzed.\n\nDespite strong empirical results, the paper provides little theoretical explanation of why shared representations improve generalization."}, "questions": {"value": "What are the computational and latency trade-offs of generating both prompts and LoRA parameters at inference time, and how could these be mitigated for large-scale or real-time use?\n\nHow sensitive is MetaTuner’s performance to the quality and size of the GPT-4o-generated expert dataset, and how does it generalize when trained with weaker or smaller teachers?\n\nDo the learned meta-prompts exhibit shared structures or transferable patterns across related tasks?\n\nHow model-specific are the learned meta-prompts, would fine-tuning different base models yield similar or divergent prompt behaviors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7Gp0UoT5IP", "forum": "WPJnLvQrP3", "replyto": "WPJnLvQrP3", "signatures": ["ICLR.cc/2026/Conference/Submission10243/Reviewer_6Sni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10243/Reviewer_6Sni"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762009111478, "cdate": 1762009111478, "tmdate": 1762921601213, "mdate": 1762921601213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}