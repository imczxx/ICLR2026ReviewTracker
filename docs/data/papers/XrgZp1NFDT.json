{"id": "XrgZp1NFDT", "number": 11428, "cdate": 1758198754541, "mdate": 1759897576189, "content": {"title": "Latent Space Uniformization in Generation", "abstract": "Latent space transformation is a core topic in generative model research and is crucial for understanding and controlling the generative process.  This study proposes the concept of latent space uniformization inspired by Coulomb's law, named ULatent. This concept provides a canonical representation of the latent space and facilitates sampling and aligning elements in both single-domain and cross-domain generative scenarios. Specifically, we model data points in a two-dimensional latent space as charged particles driven by Coulomb forces (electrostatic dynamics). The repulsive forces between them form a uniform distribution in the latent space, simulating the phenomenon where equally charged particles reach equilibrium. The uniformization of the original data enhances latent space structure, particularly by eliminating gaps between isolated clusters. For semantically overlapping clusters, pre-translation operations are required. By integrating geometric mapping techniques, we achieve precise alignment of uniformly distributed data across both single-modal and multi-modal domains, thereby simultaneously improving sampling efficiency and generation accuracy. All these conclusions are validated through multi-dataset experiments and ablation studies.", "tldr": "", "keywords": ["latent space representation", "generation", "alignment", "transformation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4f9d40d8c4fe132d4a864f6dee798bea015d41e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper maps latent codes into 2D, applies a Coulomb-inspired repulsion to make the space more uniform, and then maps back the shifts to high dimensions for generation. The goal is to achieve smoother and more consistent cross-domain translations by regularizing the latent geometry."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Creative and unconventional use of physical analogies**  \n   The idea of applying Coulomb‚Äôs law to model latent-space uniformization is genuinely creative. It‚Äôs rare to see such a direct, physically inspired formulation in modern generative modeling. This originality reflects open and out-of-the-box thinking, and the field benefits from more attempts to connect physical intuition with geometric latent design.\n\n2. **Conceptually sound latent-space reasoning**  \n   The focus on operating in latent space, rather than image space, is a solid design decision. The canonical space representation and harmonic registration modules are conceptually coherent and help maintain semantic consistency across domains, which is supported by clear ablation evidence.\n\n3. **Demonstrated improvement over immediate baselines**  \n   Results show moderate but consistent quantitative and qualitative improvements over prior methods such as GMapLatent and CycleGAN. FID and accuracy metrics, as well as ablations, demonstrate that each module (canonical representation and harmonic registration) contributes meaningfully to better semantic alignment and smoother translations.\n\n4. **Coherent geometric interpolation mechanism**  \n   The curve-to-curve generation results effectively illustrate the intended advantage of smooth, continuous mappings through latent space. This geometric interpretation of interpolatio, particularly the spherical (Slerp-based) variant, successfully maintains semantic continuity and avoids abrupt mode changes during translation.\n\n5. **Avoiding 2d bottleneck is elegant**\nThe fact that uniformization happens in the 2d space, but that the 2d representations are not the actual representations but only proxies for the uniformization considerations is elegant. Mapping the change to the higher dim rather than mapping the representation itself is a good useful solution."}, "weaknesses": {"value": "1. **Fundamental non-scalability**  \n   The method by its own definition cannot scale.  \n   To be clear, this is not about the paper not showing higher-scale results. The algorithm itself depends on pairwise Coulomb-like repulsion and KD-tree construction in every iteration, with no batching or locality reduction.  \n   The complexity $O(T_{\\max}(N \\log N + kN))$ per cluster makes it fundamentally unsuitable for large datasets.   \n  \n2. **Reliance on small, predefined class counts**  \nThe paper never clarifies how clusters are obtained. From the description and examples, it appears that semantic labels are used directly, meaning the method is supervised rather than unsupervised. The method assumes a small number of semantic clusters that are known in advance. The angular sector layout and barycentric formulation both rely on these being fixed manually. This limits applicability to datasets with few discrete classes and cannot naturally extend to many or continuous domains. It seems there is a restriction allowing only annotated datasets. \n\n3. **Angular-only uniformization bias**  \n   The ‚Äúuniformization‚Äù process equalizes only the angular component, leaving the radial distribution unbalanced. Because the area element in polar coordinates scales as $r\\,dr\\,d\\theta$, the resulting density grows roughly as $1/r$, producing a strong bias toward the center.  \n   This effect is visible even in the paper‚Äôs own illustrations, where samples appear clearly denser near the origin. I couldn't find any comment regarding this or even awareness. Basically this contradicts the premise of the paper because this is not uniform density, it is radial dependent density.\n\n4. **Complexity omissions**  \n   While the paper provides complexity expressions for the repulsion updates, it omits the cost of clustering (when applicable) and likely undercounts repeated KD-tree rebuilds per cluster. The analysis therefore understates the actual computational cost.\n\n5. **Presentation of Coulomb‚Äôs law**  \n   A very minor and friendly remark: ‚ÄúTheorem 3.1 (Coulomb‚Äôs Law)‚Äù is presented as if it were an original result.  \n   This is not how such known laws are usually included in ML papers‚Äîunless, of course, the author happens to be Charles-Augustin de Coulomb üôÇ.  \n\n6. **Unclear definition of contributions**  \n   The ‚ÄúContributions‚Äù paragraph does not clearly describe what was actually introduced or achieved relative to prior work.  \n   A contribution section should explicitly state what you did that was not done before. The current phrasing mixes broad intentions with unclear tasks, which makes it difficult for readers to identify the paper‚Äôs concrete novelty.  \n   I know this sounds picky, but here it was genuinely important for understanding what the authors consider their contribution.\n\n7. **Weak and outdated results**  \n   The reported FID scores are modest (around 13.7 for digits and 81 for animals) and the claimed improvements over prior work are marginal well within noise.  \n   Comparisons are limited to older GAN-based methods (CycleGAN, MUNIT, StarGAN) and to Zeng et al. (2025).  \n   No modern diffusion-based or transformer-based translation models are included, despite these being the true state of the art.  \n   Therefore, the claim of ‚Äústate-of-the-art‚Äù performance is outdated and not substantiated. The evaluation also relies on small, low-resolution datasets with no variance analysis, further limiting credibility."}, "questions": {"value": "1. Could you do it in batches? of course the nearest-neighbor would become weird but this should be a key direction towards a feasible method.\n\n\nI find the use of physics compelling, there is an elegant side of uniforming by density. \nHowever, this paper seems to not be ready. To my understanding, currently the method is not feasible beyond toy data. Comparisons are only to old works and it feels like the writing of the paper is not great. Unfortunately, I don't think this paper should be accepted. I do encourage to keep perusing this direction and try to get it to something scalable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bgTBPuAanz", "forum": "XrgZp1NFDT", "replyto": "XrgZp1NFDT", "signatures": ["ICLR.cc/2026/Conference/Submission11428/Reviewer_FYGM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11428/Reviewer_FYGM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761563887573, "cdate": 1761563887573, "tmdate": 1762922544389, "mdate": 1762922544389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method to build an auto-encoder with a regularized and uniformly distributed latent space. This is done by a method that models latent points as charged particles driven by Coulomb forces, and that seeks equilibrium on this ensemble of particles. The authors apply experiments on single-domain and cross-domain generative models using their proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Propose a novel method based on Coulomb potentials to make latent space uniformly distributed."}, "weaknesses": {"value": "* The paper is not well written and lacks crucial details. It is vague about core components of the method. There is no algorithm of the overall method, no code provided, nor no details (e.g. hyper-parameters) on neural networks' trainings. For example, how is the encoder/decoder training combined with the latent space uniformization proposed by the authors. How does the cross-domain setting differs from the single-domain setting? Where do the categories come from in the barycentric translation: datasets' classes?\n\n* There is also a lack of precise mathematical notation. There is no harmonized notations between sections. And many sections lack precise mathematical definitions of the operations that are performed, making the explanations of the methods very unclear. Overall, the quality of presentation is far from reaching the bar of ICLR's standards. \n\n* Experimental results are not particularly strong. The method does not show strong advantages compared to concurrent methods."}, "questions": {"value": "* Is the method limited to 2D latent spaces? \n\n* What is the complexity and computational cost of latent space uniformization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WQTZMORxkx", "forum": "XrgZp1NFDT", "replyto": "XrgZp1NFDT", "signatures": ["ICLR.cc/2026/Conference/Submission11428/Reviewer_ZRL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11428/Reviewer_ZRL3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730075564, "cdate": 1761730075564, "tmdate": 1762922543907, "mdate": 1762922543907, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method to map a latent variable to a canonical manifold structure. This is motivated by the idea that the space from several different models can be aligned to support cross-domain generation and that having a simple structured latent representation will allow for simpler and more intuitive transformations. The approach taken in the paper is to first map the latent representation to two dimensions then transform this data to a clustered representation based on categorical information. This representation is then made uniform into a canonical representation.\n\nExperimental results are shown in simple MNIST style examples comparing the proposed approached with a GAN approach."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The paper is somewhat original especially in the context of modern generative models. The task that the authors try to address is important."}, "weaknesses": {"value": "This paper suffers from a rather chaotic disposition and it is hard to extract what exactly the authors are proposing and even after a few reading it a few times I find the paper confusing. The main issue I have with the paper is that I don't find any justification for what is proposed except for that it makes intuitive sense and that the results shows that it works. Sadly the experimental evaluation in a paper with these complex models will never be sufficient to draw conclusions from. To that end it is really hard to draw conclusions from this work. \n\nMinor thing, but as you correctly say, diffusion models do not reduce the dimensionality of the observed data, still you refer to latent generative models as models that map from a low-dimensional latent representation."}, "questions": {"value": "Can you clarify the use of UMAP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TzmXZwqEsT", "forum": "XrgZp1NFDT", "replyto": "XrgZp1NFDT", "signatures": ["ICLR.cc/2026/Conference/Submission11428/Reviewer_zKDr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11428/Reviewer_zKDr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774743682, "cdate": 1761774743682, "tmdate": 1762922543497, "mdate": 1762922543497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes using uniformization and canonical representation to enable cross-domain generation. The idea is novel and interesting. The authors conduct experiments on both MNIST (mapping Chinese numerals to Arabic numerals) and on color images (transferring from one animal species to another). However, I have practical concerns and would like the authors to clarify the motivations as well as demonstrate the performance improvements achieved by their method. Please see below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core idea of the paper is novel and interesting, particularly the uniformization and alignment components.\n\n2. The proposed algorithms do not require training an additional cross-domain generator, and the transfer process is geometric. This may improve computational efficiency.\n\n3. The structure provides some explainability. Since the alignment is achieved through convex partitions and harmonic maps, it becomes possible to reason about where a specific sample will be mapped, which contrasts with the black box of large conditional diffusion models.\n\n4. The paper is clearly written and easy to follow."}, "weaknesses": {"value": "1. The algorithm heavily depends on a 2-D projection. However, in image generation, a 2-D representation is often insufficient to capture the full information of high-dimensional images. When projecting an original 256*256 space into only two dimensions, different categories of images may overlap significantly. In such cases, the algorithm may fail to properly distinguish classes during uniformization.\n\n2. The improvement of ULatent over the existing method GMapLatent appears limited, as shown in Table 1. Although the authors mention that GMapLatent has higher computational complexity, the paper does not provide comparisons of training or inference time to support this claim.\n\n3. Although the method does not require training a separate cross-domain generator in the latent space, it still requires training separate VAEs for each image class (based on my understanding). If this is true, then when the number of classes becomes large, the computational cost may increase significantly and offset the claimed efficiency benefits."}, "questions": {"value": "Following up the weaknesses section, I have some questions that I want the authors to clarify:\n\n1. Could the authors justify whether the proposed algorithm can still succeed when the number of classes is large and when the image dimensionality increases? Please explain why, and under what conditions, the approach would remain effective.\n\n2. How does this method compare to conditional diffusion models operating in the latent space of a VAE, such as Stable Diffusion or the more recent conditional diffusion/flow approaches trained over VAE latent spaces ([1]‚Äì[3])? These methods only require training a single VAE across all classes and an additional conditional diffusion model in the latent space.\n\n3. Could the authors provide training and sampling efficiency comparisons against other baselines, especially GMapLatent?\n\n\n[1] Xu, Chen, Xiuyuan Cheng, and Yao Xie. \"Computing high-dimensional optimal transport by flow neural networks.\" arXiv preprint arXiv:2305.11857 (2023).\n\n[2] Batzolis, Georgios, et al. \"Conditional image generation with score-based diffusion models.\" arXiv preprint arXiv:2111.13606 (2021).\n\n[3] Melistas, Thomas, et al. \"Benchmarking counterfactual image generation.\" Advances in Neural Information Processing Systems 37 (2024): 133207-133230."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YvCFDFRIXV", "forum": "XrgZp1NFDT", "replyto": "XrgZp1NFDT", "signatures": ["ICLR.cc/2026/Conference/Submission11428/Reviewer_oB2n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11428/Reviewer_oB2n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11428/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762285307293, "cdate": 1762285307293, "tmdate": 1762922542858, "mdate": 1762922542858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper's core idea, ULatent (Latent Space Uniformization), uses Coulomb's Law to get the messy latent space organized. They treat all the data points as identical charged particles that repel each other until they are spread out uniformly in a canonical structure, eliminating useless empty areas. The benefit is huge: sampling is better (every random spot gives a meaningful output), and cross-domain translation (like changing a horse to a zebra) is more accurate because they use a Barycentric Translation trick to precisely align features. The big question, though, is that they only proved this dynamic, particle-repulsion method works in a 2D space, making it unclear if it can handle the much higher dimensions used by modern, serious AI models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using Coulomb's Law to fix a deep learning problem is interesting. It makes the whole complex math easy to grasp: they just make the points hate each other until they're perfectly spaced.\n\n2. The paper is aiming to fix the two biggest annoyances in generative models: making sure you can sample anywhere and still get a decent picture (no more dead zones), and making cross-domain translations (like cat to dog) reliable.\n\n3. The writing is good. E.g. The main algorithm (A.1) is laid out step-by-step. It's easy to look at and figure out how to rebuild it yourself."}, "weaknesses": {"value": "1. The paper only modeled this whole repulsion thing in a 2D latent space. This could make the entire method feel like a toy problem or just a cool concept that might completely fail due to the Curse of Dimensionality when scaled up to real-world complexity.\n\n2. Their goal is purely geometric which make the points evenly spread out. But forcing points away from their neighbors doesn't mean they'll land in a semantically useful area. They might get pushed into a noise zone or an area that generates a bad image, just so they can maintain distance. The method might actually break the subtle semantic structure the model learned."}, "questions": {"value": "1. The work only showed results in a 2D latent space. What happens when we jump to practical dimensions (128D/512D)? Will the method even scale, or will the k-NN cost and the Curse of Dimensionality totally break the uniformization process?\n\n2. Why the weird, custom force formula? It‚Äôs not standard Coulomb's Law, and it relies on arbitrary-looking $\\alpha$ and $\\beta$ parameters. Can you provide a strong theoretical reason for this specific modification and prove its stability?\n\n3. The proposed method is purely geometric repulsion. How do you ensure this uniform spreading doesn't push points into dead zones or break the semantic structure learned by the original encoder/decoder? Does uniform geometry actually hurt content integrity?\n\n4. Is this slow, iterative process a one-time post-processing step? If so, how do you handle the encoder? If it's a regularizer during training, how much does it slow down model convergence? The cost seems prohibitive either way."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "a69iI2f1Zk", "forum": "XrgZp1NFDT", "replyto": "XrgZp1NFDT", "signatures": ["ICLR.cc/2026/Conference/Submission11428/Reviewer_MVQz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11428/Reviewer_MVQz"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11428/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762922938167, "cdate": 1762922938167, "tmdate": 1762922938167, "mdate": 1762922938167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}