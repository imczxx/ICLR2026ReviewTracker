{"id": "ylr6WArKQN", "number": 9529, "cdate": 1758126196936, "mdate": 1763738532521, "content": {"title": "DAG-Math: Graph-Guided Mathematical Reasoning in LLMs", "abstract": "Large Language Models (LLMs) demonstrate strong performance on mathematical problems when prompted with Chain-of-Thought (CoT), yet it remains unclear whether this success stems from search, rote procedures, or rule-consistent reasoning. To address this, we propose modeling CoT as a certain rule-based stochastic process over directed acyclic graphs (DAGs), where nodes represent intermediate derivation states and edges encode rule applications. Within this framework, we introduce **logical closeness**, a metric that quantifies how well a model’s CoT trajectory (i.e., the LLM's output) adheres to the DAG structure, providing evaluation beyond classical PASS@$k$ metrics. Building on this, we introduce the *DAG-MATH* CoT format and construct a benchmark that guides LLMs to generate CoT trajectories in this format, thereby enabling the evaluation of their reasoning ability under our framework. Across standard mathematical reasoning datasets, our analysis uncovers statistically significant differences in reasoning fidelity among representative LLM families—even when PASS@$k$ is comparable—highlighting gaps between final-answer accuracy and rule-consistent derivation. Our framework provides a balance between free-form CoT and formal proofs systems, offering actionable diagnostics for LLMs reasoning evaluation.", "tldr": "We propose a new pipeline by modeling CoT on directed acyclic graphs (DAGs), introduce the concept of logic closeness, and then precisely evaluates the mathematical reasoning ability of LLMs via the proposed DAG-MATH format.", "keywords": ["LLMs", "mathematical reasoning", "directed acyclic graphs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfcad10c8f6f0da25f5a2f5e0e04ede98db95c70.pdf", "supplementary_material": "/attachment/6c8e735e81bec51432b023d1dc6ac8af462528f7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a DAG-based framework for representing CoT reasoning and introduces the concept of logical closeness, enabling fine-grained evaluation of LLM mathematical reasoning beyond final-answer accuracy. Its approach assesses the coherence and consistency of logical dependencies along a CoT trajectory, rather than focusing solely on whether the solution is correct. Furthermore, the authors construct a benchmark of DAG-formatted mathematical problems derived from existing datasets and provide empirical analyses linking graph-level characteristics—such as size, density, and branching complexity—to problem difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work formalizes the notion of logical closeness and proposes a metric, the perfect reasoning rate, based on this notion to measure LLMs' logical consistency beyond the final output. This indeed addresses an important question of whether an LLM arrives at a correct answer through genuine logical reasoning or mere pattern matching.\n- The formalization in Sections 2 and 3 is clearly presented and easy to follow.\n- Section 5 and Appendix B offer several interesting insights, such as the correlation between graph structure and problem difficulty, and how a correct final answer may still arise from unclosed or flawed reasoning."}, "weaknesses": {"value": "- The DAG-MATH benchmark presented in the paper is validated using symbolic correctness and an LLM-as-Judge approach. I assume that SymPy is employed to verify mathematical equivalence, while the logical dependencies between nodes (i.e., whether an edge should exist) are assessed by the LLM-as-Judge. However, as the paper itself demonstrates, LLMs can often produce superficially consistent but logically inconsistent reasoning. While using LLMs for judgment is a practical solution, it would be helpful for the authors to further justify the reliability of this dataset construction and evaluation methodology. \n- Building on this point, reliable automation of the logical closeness check appears challenging, if not infeasible, since formalizing a DAG from natural-language CoT inherently involves subjective interpretation, particularly for more complex problems. For instance, reasonable disagreement could arise over whether a given edge should connect two specific nodes."}, "questions": {"value": "- How can we trust an LLM-as-Judge to reliably evaluate the logical coherence of DAG constructions, particularly when edges are intended to represent valid inference paths? Have the authors conducted any analyses or validation studies to assess the consistency or accuracy of these judgments?\n- At first glance, Figure 4 being a line plot was somewhat confusing. Do the authors plot accuracy against varying levels of logical correctness rates and then smooth the resulting curve? A brief clarification in the caption or text might help.\n- I also wonder whether different node segmentation choices could exist for the same reasoning trajectory. If so, how sensitive are the proposed approach and the PRR metric to such segmentation differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3qoCulNMAJ", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Reviewer_GdSj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Reviewer_GdSj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761597727832, "cdate": 1761597727832, "tmdate": 1762921093434, "mdate": 1762921093434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Reply 5 on reliable automation of the logical closeness check"}, "comment": {"value": "The DAG identification of our framework is compatible to interface with autoformalization and theorem-prover via LLM.\n\nThe core idea is that each step in a CoT can be treated as a natural-language statement to be autoformalized into a formal Lean statement then proven in Lean. This process would inherently perform a canonicalization, stripping away linguistic variation and leaving a unambiguous logical object. \n\nThe autoformalization and proof verification provide a critical advantage for dependency analysis: a formally formalized and proven step must explicitly include its premises from the available hypotheses and previous results. In this formalized setting, the question of \"whether an edge should connect two nodes\" is not a matter of subjective interpretation but of **syntactic necessity**, similar to the computational graph of programming language.\n\nTherefore, autoformalization can provide the path to reliable automation of logical closeness. While high-accuracy autoformalization remains a challenging research frontier (e.g., Goedel-Prover-V2, Kimina-Prover, [1]) and was out of our compute budget, our DAG-MATH framework established a foundation to leverage its future success.\n\nAlternatively, future research could leverage the DAG-MATH framework to map token-level probabilities to edge-level confidence scores, investigating whether models are well-calibrated not just on final answers, but on the intermediate logical leaps required to reach them.\n\n**Reference:**\n\n[1] Liu, Qi, et al. \"Bootstrapping Hierarchical Autoregressive Formal Reasoner with Chain-of-Proxy-Autoformalization.\" NeurIPS 2025."}}, "id": "ATlVPc0Uq1", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763737695408, "cdate": 1763737695408, "tmdate": 1763740807068, "mdate": 1763740807068, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to model mathematical reasoning’s CoT traces as a rule-based stochastic process over task-specific DAGs, where nodes represent reasoning states and edges encode inference rules or justifications. Building on this formulation, the paper introduces logical closeness as a new metric to evaluate the model's reasoning trajectory. It also presents a benchmark of gold-standard DAG-MATH graphs with verified logical structures and statistical analyses relating graph properties to problem difficulty. Finally, the paper evaluates several large language models by prompting them to produce formatted CoT reasoning on mathematical benchmarks and examines how their reasoning abilities correlate with the proposed DAG-MATH framework."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is clearly written and well-organized. \n- The idea of representing CoT reasoning with DAG-MATH is interesting and novel. The proposed metrics are also new and conceptually sound. \n- The empirical results are informative, showing how graph structures reflect problem difficulty and reasoning quality."}, "weaknesses": {"value": "- Enforcing the DAG-MATH format may degrade the natural reasoning flexibility of LLMs. It would help to include an analysis or ablation comparing performance with and without this formatting constraint. Furthermore, if the few-shot examples are drawn from a specific model family, models of the same family might have an advantage because their reasoning patterns are similar.\n- The analysis is primarily quantitative. Some qualitative examples or case studies of the generated DAG-MATH graphs, especially highlighting common reasoning errors or structural failures, would strengthen the insights.\n- In Section 2.2, the paper mentions that thinking LLMs can be viewed as “an exploration of the task-specific DAG with self-correction or backtracking, but its final output … is still consistent with our transition rule.”, but empirical results did not include reasoning models, which could strengthen the value of the paper. \n- DAG-MATH has limited scalability. Complex problems with very long and multiple reasoning traces are computationally expensive to construct. Those with cyclic reasoning or backtracking are hard to capture in a strictly acyclic form."}, "questions": {"value": "- How exactly is the branching of reasoning paths determined?\n- Since the canonicalization turns reasoning steps into SNF, does it limit the type of mathematical questions that DAG-MATH can apply to? \n- In lines 63-64, what does it mean that the other works “fail to capture long-range and cross-branch dependencies, as well as the goal-directed, absorbing-state nature of CoT”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jCMw85o5vc", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Reviewer_61B1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Reviewer_61B1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976943700, "cdate": 1761976943700, "tmdate": 1762921093125, "mdate": 1762921093125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Reply 4 on practical paths to improve LLM reasoning"}, "comment": {"value": "Our proposed metrics would provide a principled foundation for future work aimed at improving both search-based inference and LLM's post-training.\n\n**1. Guiding Search Policies (MCTS/ToT):**\nOur framework suggests that **logical closeness** can serve as a reward to guide graph-based search algorithms like MCTS or Tree-of-Thoughts. Instead of relying solely on random branching or value function, search policies could **prioritize reasoning paths that maintain high logical coherence**. For instance, by favoring branches that close open dependencies or by pruning highly speculative trajectories early. This offers a promising avenue to make search more efficient and goal-directed, steering it toward not just correct but also well-structured solutions.\n\n**2. Informing Reward Design for RL:**\nThe AUC curves inspire a curriculum learning strategy for training reasoning models via RL. One could progressively increase the required level of logical closeness during training, starting from a regime that rewards basic correctness and gradually guiding the model toward the \"sweet spot\" of perfect reasoning identified by our framework.\n\nWe have included this dicussion in **Appendix J** of the updated version and leave the experimental improvement as future work."}}, "id": "OSdD34WH9v", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763737754858, "cdate": 1763737754858, "tmdate": 1763737754858, "mdate": 1763737754858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DAG-MATH, which is a framework to evaluate mathematical reasoning of LLMs through modeling Chain-of-Thought as stochastic process over directed acyclic graphs. The authors propose metric called \"logical closeness\" for distinguishing between when model solves problem by search versus real logical inference. They make benchmark with 2,894 gold-standard DAGs and test five LLMs, discovering that although PASS@1 scores vary much between models, their perfect reasoning rates stay quite similar, which suggests search is inflating the accuracy metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles really important problem about understanding whether LLMs achieve correct answers through systematic search or through genuine logical reasoning, which is fundamental question for the field. The DAG-based formalization is quite novel approach that sits nicely between completely free-form CoT and very formal systems like LEAN verification, making it more practical to use. The logical closeness metric gives us insights that go beyond simple PASS@k metrics that everyone uses.\n\nThe empirical analysis is quite comprehensive, showing interesting patterns about how DAG statistics like number of nodes, edges, density and branching correlate with problem difficulty. It reveals that harder problems create larger and more sparse graphs with higher branching, which makes sense intuitively. The finding that search and exploration inflate PASS@1 while actual reasoning ability measured by PRR stays comparable across different models is really actionable insight that changes how we should think about evaluating these systems. Also good that authors released the benchmark and code for others to use."}, "weaknesses": {"value": "There is concerning circularity in how the benchmark was constructed - using GPT-4 and Qwen to create the \"gold standard\" DAGs means the benchmark is essentially created by same type of models that are being evaluated, which introduces obvious biases. The theoretical justification feels not enough developed. Why should we believe this specific DAG formalization captures what \"true\" reasoning means? The stochastic process described in Equation 1 seems somewhat arbitrary choice without proper justification.\n\nThe statistical analysis lacks rigor with only 32 samples per problem and no significance testing provided for the differences claimed between PASS@1 and PRR. There is no analysis about robustness - what happens when same problem can be formulated with different but equivalent DAG structures? The scope is quite limited, restricting to mathematics problems with difficulty below 6, and missing comparisons with other approaches like process reward models that also try to do step-level verification. The three-stage prompting methodology might be imposing particular reasoning patterns that are not universal. Most importantly, there is no human validation beyond what the models themselves produce, which is problematic for claiming these are \"gold standard\" solutions."}, "questions": {"value": "How did you validate that the DAGs generated by GPT and Qwen actually represent correct reasoning structures? It seems crucial to have human experts verify at least subset of these DAGs to ensure the benchmark quality. Many mathematical problems have multiple valid solution approaches - how does the logical closeness metric handle cases where completely different but valid DAG structures could exist for same problem?\n\nCould you provide proper statistical significance tests for the differences you claim between PASS@1 and PRR? How sensitive are all these results to the specific prompting strategies you used? If you changed the prompts slightly, would the DAG structures and evaluation results change significantly?\n\nWhat is the practical path from these evaluation insights to actually improving LLM reasoning capabilities? The paper identifies interesting patterns but doesn't suggest how to use this knowledge for making better models. How does this framework compare empirically with recent work on process reward models from OpenAI and others that also try to verify reasoning at step level?\n\nWhy did you choose to require exactly one assertion per node - this seems quite restrictive and arbitrary? And why make logical closeness binary measure instead of having gradient that could capture partial correctness better? Finally, have you considered that the models might be following completely different internal reasoning process and the DAG structure is just post-hoc rationalization that we impose on their outputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gWqKiSfTN2", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Reviewer_utCQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Reviewer_utCQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762336395196, "cdate": 1762336395196, "tmdate": 1762921092850, "mdate": 1762921092850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Reply 3 on the acyclicity assumption"}, "comment": {"value": "The acyclic assumption is fair to charecterize thinking and non-thinking models' final outputs. The LLM may reason imperfectly during its thinking process, e.g., dead-ends, iterative refinement, and backtracking, but is expected to output only the finalized, perfect, correct reasoning results to the user.\nBesides, this assumption is naturally examplified by certain cases.\n\n1. **Rationale of Acyclicity:** A mathematical derivation/proof is a finite sequence of well-formed formulas where each formula is either an axiom or is derived from earlier formulas by a rule of inference. If we treat each formula occurrence as a node and draw a directed edge from each premise inferred from it, the dependency graph we obtained is indeed a directed acyclic graph. This is also supported by formal finitary system such as Zermelo–Fraenkel set theory or type theory. Strategies like *proof-by-contradiction* are rules of inference (e.g., `¬P → ⊥ ⊢ P`), which creates a clear, acyclic dependency: the conclusion `P` depends on the sub-derivation that starts with the assumption `¬P` and ends with a contradiction `⊥`. This is not a cycle but a conditional branch that is closed, which can be covered by DAG-MATH.\n\n2. **Compatibility with Planning:** A high-level plan can be viewed as a sketch of the solution's structure and seamlessly integrated as an initial part of the DAG-MATH, which is fully compatible with our framework.\n\nWe have updated the discussion for Assumption 1 in **Appendix A.4** of the updated version."}}, "id": "tscV9S6Iss", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763737801107, "cdate": 1763737801107, "tmdate": 1763737801107, "mdate": 1763737801107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DAG-MATH, a framework designed to formalize and evaluate the Chain-of-Thought (CoT) trajectories generated by Large Language Models (LLMs) in mathematical reasoning. The CoT is modeled as a rule-based stochastic process over a Directed Acyclic Graph (DAG), where nodes are intermediate states. The core proposal is the Logical Closeness metric, which quantifies the fidelity of an LLM's path against a \"gold standard\" DAG, yielding the Perfect Reasoning Rate (PRR) and AUC scores. The authors claim this provides a superior diagnostic tool compared to simple final-answer metrics like PASS@k. The resulting DAG-MATH benchmark is built using LLM-generated structured outputs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The underlying idea of treating CoT as a DAG traversal is fundamentally sound and offers a pathway for structured reasoning analysis beyond token-level checks. This is the paper's primary and most important strength.\n2. The authors have created impressive few-shot prompts to enforce their complex output format, which is a valuable demonstration of structured generation control in LLMs. The visual examples of the DAGs are convincing.\n3. The metric correctly isolates failure modes like speculative branching and imperfect reasoning, which are invisible to simple PASS@k."}, "weaknesses": {"value": "1. The PRR/AUC metric confuses adherence to the authors' custom template with true logical reasoning ability. The paper must provide evidence that this metric holds up when applied to non-formatted, naturally generated CoT.\n2. A critical omission is the lack of comparison with or contextualization against MCTS or similar graph-based search methods. If the goal is to improve reasoning, how does the DAG-MATH diagnosis inform or relate to these established LLM search strategies?\n3. The use of LLMs to generate the ground truth DAGs for their own evaluation introduces a circular dependency. This casts significant doubt on the objectivity and reliability of the Logical Closeness scores.\n4. The Acyclicity Assumption restricts the framework to simple forward derivation, excluding crucial reasoning patterns like planning, iterative refinement, or proof by contradiction, thereby limiting its general applicability."}, "questions": {"value": "1. Can you demonstrate the utility of PRR/AUC by heuristically parsing DAGs from unconstrained, free-form CoT outputs (without the DAG-MATH template) on a subset of problems? If the metric collapses here, it confirms the dependency on the template is too strong.\n2. Please elaborate on the relationship between DAG-MATH's diagnostic insights and existing MCTS/Tree-of-Thought techniques. How can the PRR/AUC scores be used to guide the search policies or reward functions in such systems?\n3. Given the reliance on LLM-generated ground truth, what specific human review or verification process was applied to the 2,894 gold-standard DAGs to ensure their canonical logical structure? What were the human agreement statistics on the logical decomposition?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LVOje2f8ST", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Reviewer_wXKz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Reviewer_wXKz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762350781699, "cdate": 1762350781699, "tmdate": 1762921092359, "mdate": 1762921092359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Reply 2 on Human evaluation of gold-standard benchmark"}, "comment": {"value": "We thank the reviewers for raising the critical issue of potential circularity and the need for rigorous human validation of our gold-standard DAGs.\n\nTo address this, we have conducted a rigorous **Human Evaluation** (detailed in the newly added **Appendix E**) on a random sample of 50 problems. Crucially, to eliminate the subjectivity on dependency judgement in human reviews, we designed an **Algorithmic Human Evaluation Pipeline**. This approach treats the logical structure of a solution as isomorphic to a computational graph:\n\n1.  **Methodology:** Human manually maps every natural language step of the sampled DAGs into executable **SymPy code**.\n2.  **Verification:** \"Dependencies\" were rigorously defined as the flow of variables between lines of code. If `step_B` mathematically requires `step_A`, the code for `step_B` must reference the variable `step_A`.\n3.  **Results:**\n    *   **Execution Correctness:** 100% (50/50) of the stepwise conclusions and final answers are correct.\n    *   **Dependency Accuracy:** 98% (49/50) of the DAGs perfectly matched the human-coded computational graph. The single anomaly was a weak narrative link rather than a logical failure.\n\nThis evaluation demonstrates that while LLMs were used to scale the construction of the benchmark, the resulting logical structures are still reliable under our strict verification desgin in **Appendix E**. Human evaluation confirms the high reliability of our DAG-MATH benchmark as a gold standard for further use such as high-quality data for post-training."}}, "id": "xwaiT6drId", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763737861710, "cdate": 1763737861710, "tmdate": 1763737861710, "mdate": 1763737861710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Reply 1 on robustness, statistical significance, generalizability of DAG-MATH"}, "comment": {"value": "We performed extensive ablation studies (**Appendices K**) to show that our framework is robust to prompt variation and model's inductive bias (non-circularity). Also, we condcuct statistical tests to verify the significance of our finding claim and show the generalizability of DAG-MATH to natural CoT.\n\n*   **Resilience to Prompting and Model Bias (Appendices K.2-4):** We confirmed that our metric and framework are robust to prompt & model variations. \n    * **Prompt Perturbation:** Using Two One-Sided Tests (TOST) in **Appendix K.2**, we establish statistical equivalence in DAG structures generated by reformatted (tables vs. bullets) or rephrased prompts comparing to our originial one.\n    * **Cross-Model DAG Parsing:** **Appendix K.3** uses multiple LLMs from different families to re-parse DAG structures of formatted CoT (also natural CoT) and clearly show that there are **no circularity** in DAG-MATH evaluation. The metrics remain highly close (from 24.9% to 25.5%) and nearly identical AUC trends. Furthermore, we use TOST to show statistical equivalence of DAG strcutres.\n    * **Cross-Family Few-shot Demonstrations:** **Appendix K.4** addresses concerns about model-specific inductive bias; we show that using cross-family few-shot demonstrations (e.g., prompting GPT-4.1 with Gemini-generated trajectories) yields consistent relative rankings and metric behavior, also achieve statistical equivalence on DAG structures via TOST, proving the evaluation is not dependent on matching the generator’s own reasoning style.\n\n*   **Statistical Significance of the Reasoning Gap (Appendix K.1):** We verified that the observed disparity between final-answer accuracy (PASS@1) and reasoning rigor (PRR) is not due to sampling noise. By scaling the evaluation to 128 samples per problem, we computed tight 95% confidence intervals for this gap. The results consistently exclude zero with non-trivial values (from 9.0% to 36.3%), statistically confirming that the gap between finding the correct answer and adhering to a valid deductive path is a substantial and genuine phenomenon in current LLMs.\n\n*   **Generalizability to Natural CoT (Appendix K.3):** Our evaluation metrics (PRR and AUC) are not limited to the structured DAG-MATH format. By parsing free-form **Natural CoT** via both **internal & external models**, we found that logical closeness remains **consistent** across different DAG parsings. The metrics successfully capture fundamental reasoning patterns in natural text, achieving AUC scores comparable to the structured format, which validates the framework’s utility beyond our specific benchmark format."}}, "id": "Yp1Gt72RiD", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763737901755, "cdate": 1763737901755, "tmdate": 1763738890065, "mdate": 1763738890065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision summary"}, "comment": {"value": "We deeply thank for the reviewers' insightful comments and suggestions. We address the common concerns and then respond each reviewer's specific question/weakness point-by-point. We have updated the paper (highlight in red) to include these insightful suggestions:\n\n**Major:**\n\n\\+ **Appendix E** Human evaluation of gold-standard DAG-MATH benchmark\n\n\\+ **Appendix K** Extended experimental studies on:\n- Statistical significance test of reasoning gap with extended sample size\n- Ablation studies on the sensitivity of prompt via perturbation\n- Generalization of DAG-MATH format to natural CoT, and ablation studies on non-circularity of DAG-MATH evaluation\n- Ablation studies on robustness of evaluation using cross-family few-show demonstrations\n\nWe summarize the key findings from above studies into one new discussion section in **Section 5.2**.\n\n**Minor:**\n\n\\+ **Appendix A.4** Add discussion on rationale of acyclicity assumption.\n\n\\+ **Appendix D** Add more details of validation procedure in benchmark construction and filter statistics.\n\n\\+ **Appendix J** Add discussions on future improvements from our evaluation insights."}}, "id": "zuHly60pAv", "forum": "ylr6WArKQN", "replyto": "ylr6WArKQN", "signatures": ["ICLR.cc/2026/Conference/Submission9529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9529/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9529/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763737960522, "cdate": 1763737960522, "tmdate": 1763737960522, "mdate": 1763737960522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}