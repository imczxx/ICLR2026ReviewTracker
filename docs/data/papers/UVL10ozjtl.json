{"id": "UVL10ozjtl", "number": 4872, "cdate": 1757781349907, "mdate": 1759898007877, "content": {"title": "Diversity-aware Training for Test-time Scaling", "abstract": "Test-time scaling for large language models (LLMs) is a recognized effective approach to improving performance. However, when increasing test-time computation, the performance gains grow progressively smaller. This is largely due to the tendency of independent reasoning attempts to collapse into similar incorrect solutions. Existing approaches to enhancing reasoning diversity mainly focus on token-level diversity, which fail to capture reasoning-level diversity and introduce hallucinations. To this end, we introduce RePrism, a novel framework designed to act like a Reasoning Prism, guiding models to explore a spectrum of distinct and valid reasoning paths from a single input. First, we construct training data where each prompt is associated with multiple diverse yet correct answers. Additionally, we introduce noise embeddings into special tokens as implicit diversity signals, teaching the model to recognize these embeddings as indicators of diverse reasoning paths. We validate RePrism on 9 challenging benchmarks across Math, Code, and Agent tasks, where it increases the models' pass@N accuracy by up to 6.4\\%, 1.1\\%, and 0.5\\% on Math, Code, and Agent tasks, respectively. Moreover, we demonstrate that the reasoning diversity instilled by RePrism provides a superior foundation for Reinforcement Learning (RL). RePrism not only furnishes a richer exploration space that leads to enhanced performance gain from RL, but also prevents the collapse of reasoning diversity during RL training.", "tldr": "This paper introduces a novel training framework that uses diverse training data and noisy embeddings to guide LLMs, significantly improving test-time scaling performance on math, code, and agent tasks.", "keywords": ["large language mode", "test-time scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b955d58e1a21b8ee5d62921cee85929ef89c6191.pdf", "supplementary_material": "/attachment/86e15fa9046bd7ea6698c058c95844bd4487e3fe.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces REPRISM, a training framework designed to improve the test-time sampling efficiency of LLMs by addressing the issue of diminishing returns caused by a lack of diversity in generated solutions. The method trains models to produce a diverse set of correct reasoning paths using two core components: (1) a specialized training dataset, curated with submodular optimization, that contains multiple diverse and correct solutions for each prompt, and (2) the injection of Gaussian noise into special token embeddings to act as an implicit control signal for diversity. The authors validate REPRISM across 9 math, code, and agent benchmarks, demonstrating minor improvements in pass@k accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Extensive empirical studies are done for an in-depth understanding of the proposed method.\n2. Figures/tables are clear and well designed"}, "weaknesses": {"value": "This paper has numerous weaknesses. Notably, (1) the empirical results are concerning, and (2) the main contribution of this paper is unclear as having more diverse data (quality), and more data (quantity) is already well known in the literature. Noise injection has also been studied extensively.\n\n1. Empirical gains stated in the abstract are minimal, and could very well be from noise\n\n> We validate REPRISM on 9 challenging benchmarks across Math, Code, and Agent tasks, where it increases the models’ pass@N accuracy by up to 6.4%, 1.1%, and 0.5% on Math, Code, and Agent tasks, respectively\n\n2. Significant concern on empirical results. When we deploy a model in the real-world pass@K only works on queries (x) with verifiable answer (y). Therefore, real-world performance is most closely proxied by pass@1. Consider pass@1 results in fig. 1 where the proposed method commonly performs worse than regular SFT. To show that it can be deployed in the real-world for any potential query from a user, it would have been  better to show self-consistency (majority voting) results over pass@k\n\n3. Experiments might be incomplete. Why is Fig 1’s first curve missing data points at x = 128, 256. Why does the last curve (Agent) have no post-RL results? \n\n4. Empirical gains could be from an increased train data set size in terms of total tokens. Authors should provide clear comparison experiments.\n\n5. Are the authors sure that there are no additional baselines? Their only baseline is SFT\n\n6. Figure descriptions are commonly unclear. e.g.\n\nFig. 1 it is more clear to have said comparison of SFT and RePrism as displayed in the order in the image\n\n> Figure 1: (Left) Comparison of REPRISM and SFT\n\nand some figures/tables would help to have more description. \n\n7. Figure 1 is missing information: no x-axis label is provided. \n\n8. Typos should be fixed. E.g. Fig. 1 agerage → average\n\n9. Writing needs revision. Why does the paper sometime use pass@k and pass@N, arent these referring to the same thing? Let me know if i am mistaken.\n\n10. Writing is overselling the objective empirical results. E.g.\n\n> Beyond improving test-time sampling, we find that the diversity fostered by REPRISM serves as a powerful catalyst for RL.\n\n>  exceptional foundation for RL\n\nThe empirical gains are too minor to call it a “powerful catalyst” and “exceptional foundation”\n\n11. Human evaluations do not have IRB approval or other information related to how it was set-up. This can raise ethical concerns like lack of appropriate compensation.\n\n12. Hyperparameters (line 241) seem arbitrary. Why pick these? Are results sensitive to hyperparameters? Because this can be a huge headache for practitioners to tune this hyperparameter.\n\n13. The experiments only use a single LLM (Llama-3.1-8B) making generalization to other model families unclear. This is concerning as there are other highly popular open-src model families like Qwen series"}, "questions": {"value": "1. There are a lot of noise injection related literature that improves performance like NEFTune: Noisy Embeddings Improve Instruction Finetuning. Are these not relevant baselines?\n\n2. What is the cost overhead of this method, and how does its overheads compare to existing methods? This seems to be a critical discussion point that i can not find (or have missed)\n\ncould be from data curation, or compute, or other economic costs. This should be discussed to get a clear view of the contribution."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "Human subjects are involved. \n\nRequires a check if the human experiments are sound.\n\nUnclear if they were fairly compensated."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AOIngAYXjs", "forum": "UVL10ozjtl", "replyto": "UVL10ozjtl", "signatures": ["ICLR.cc/2026/Conference/Submission4872/Reviewer_Ms5P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4872/Reviewer_Ms5P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760852649627, "cdate": 1760852649627, "tmdate": 1762917625002, "mdate": 1762917625002, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose REPRISM, a novel algorithm used on LLM training with SFT and RL to enchance the generation diversity of LLMs by adding noise on embedding space. The algorithm is tested on Llama on math, coding and agent task to show the proposed algorithm can provide a benefit compared to the baselines compared."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The problem of increasing generation diversity in LLM is important."}, "weaknesses": {"value": "- The clarity of the paper can be improved. While the paper include many mathematical notations, a lot of them lack a definition and is unclear what is exactly referring to in the context. Please refer to my questions for some examples.\n- The idea is not completely novel, and the related works are greatly missing. Similar idea of manupilating latent space has been explored [1, 2], improving diversity in LLM training without prompt have been explored [3]. \n- The experiments are on Llama only. More recent models need to be tested.\n\n[1]. Geiping J, McLeish S, Jain N, et al. Scaling up test-time compute with latent reasoning: A recurrent depth approach[J]. arXiv preprint arXiv:2502.05171, 2025.\n\n[2]. Zhang Z, He X, Yan W, et al. Soft thinking: Unlocking the reasoning potential of llms in continuous concept space[J]. arXiv preprint arXiv:2505.15778, 2025.\n\n[3]. Chen W, Zhang Z, Liu G, et al. Flaming-hot Initiation with Regular Execution Sampling for Large Language Models[C]//Findings of the Association for Computational Linguistics: NAACL 2025. 2025: 7118-7127."}, "questions": {"value": "The current paper needs significant improvement in all dimensions mentioned in the weakness section. Especially, additional experiments on more models and compared to different baselines are needed.\n\nSpecifically for clarity:\n1. What is L? iIs it the number of propmts per batch? Or length of prefix?\n2. Is the noise sequence $n$ pre-defined and fixed throughout different prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SLk32fXmPv", "forum": "UVL10ozjtl", "replyto": "UVL10ozjtl", "signatures": ["ICLR.cc/2026/Conference/Submission4872/Reviewer_wGCG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4872/Reviewer_wGCG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761081383175, "cdate": 1761081383175, "tmdate": 1762917624556, "mdate": 1762917624556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes REPRISM, a training-time framework to improve test-time scaling of LLMs by increasing reasoning-level diversity. It (1) constructs per-prompt training sets with multiple diverse-but-correct solutions via submodular sampling, (2) injects Gaussian noise into selected special-token embeddings as an implicit diversity signal during training and inference, and (3) provides theoretical analysis of stability and the training objective decomposition. Experiments on 9 benchmarks show improved sampling efficiency / pass@N gains and more robust RL fine-tuning behaviour."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel integration of submodular optimization for diverse data sampling and noise embeddings for implicit diversity control;\n2. Comprehensive evaluation across 9 benchmarks in math, code, and agent tasks, with consistent improvements in Pass@N;\n3. Theoretical analysis showing noise injection acts as regularization and enables controlled diversity;\n4. Demonstrates enhanced RL performance and prevents diversity collapse."}, "weaknesses": {"value": "1. Insufficient Reproducibility: Key details are missing, including the random seed, data partitioning, specific rules for selecting special tokens, and explicit settings for the number of sampling trajectories m per prompt (the appendix only lists some hyperparameters).\n2. Insufficient Ablation Experiments: Although several variants are reported, they do not adequately demonstrate the contributions of different components across various datasets and sampling budgets k.\n3. Insufficient Robustness Analysis: While pass@1 decreases in some cases, there is a lack of detailed analysis regarding the failure scenarios.\n4. Strong Dependence on Feature Representations: The approach relies on n-gram/AST features, which may limit sampling quality if these features fail to sufficiently capture the diversity of reasoning.\n5. Limited Manual Annotation Scale: Table 1 includes annotations for only 32 queries, indicating a relatively small sample size."}, "questions": {"value": "1. Candidate set construction: for each prompt you mention collecting up to m trajectories. What is the typical m used per dataset / per prompt in experiments, and how sensitive are results to m?\n2. Special tokens / noise schedule: which tokens are considered “special tokens” and how were their positions chosen? Is σ=0.001 fixed for all experiments — did you sweep σ and report sensitivity? \n3. Reproducibility: authors state code is in supplementary—will the code include the full data-sampling / feature extraction (n-gram, AST) pipeline, LLM prompts, and random seeds? Appendix references but please clarify. \n4. Negative impact cases: can you provide example instances where REPRISM reduces pass@1 and analyze why (e.g., overly promoting exploratory but low-probability valid solutions)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G9MXnc21rC", "forum": "UVL10ozjtl", "replyto": "UVL10ozjtl", "signatures": ["ICLR.cc/2026/Conference/Submission4872/Reviewer_CKUo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4872/Reviewer_CKUo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761434458357, "cdate": 1761434458357, "tmdate": 1762917624104, "mdate": 1762917624104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents two complementary strategies for enhancing inference performance in mathematical reasoning, code generation, and agent-based decision tasks. The first leverages submodular optimization to promote diversity among generated outputs, while the second improves model robustness by injecting noise into the input embeddings during training. Empirical results demonstrate consistent gains in accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The method employs a principled submodular approach to select a diverse set of trajectories.\n2. Injecting noise into the input embeddings appears to yield practical performance improvements."}, "weaknesses": {"value": "1. There is no significant improvement in pass@N performance for math and code. Standard deviation numbers are not given in those experiments.\n2. While n-gram–based diversity metrics are standard for identifying redundant text, they do not necessarily reflect the deeper logical diversity present across trajectories.\n3. The inherently sequential nature of submodular optimization may limit the scalability of this approach. This may limit potential gains on more complex tasks.\n4. Injecting noise into embeddings also requires applying noise at inference, which may be undesirable. A more suitable approach may be needed."}, "questions": {"value": "1. What is the ceiling performance when you scale up the inference compute / your sampling budget is around infinite?\n2. While n-gram diversity may not capture the underlying logical structure of mathematical reasoning, can you provide intuition for why a bag-of-n-grams representation is still a reasonable proxy for diversity among solution trajectories?\n3. What is the baseline in Table 1? I.e. randomly sample 4 outputs w/o using submodularity?\n4. Can you provide the algorithmic complexity of sumbodular optimization? Assuming you are selecting K trajectories, each with N bag-of-words?\n5. It is not entirely clear why perturbing the embeddings during training improves performance. Is the effect primarily due to the regularization it introduces?\n6. If perturbing the embeddings during training improves performance, then based on the theory in (6), would applying explicit regularization achieve a similar effect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AnMabsUt94", "forum": "UVL10ozjtl", "replyto": "UVL10ozjtl", "signatures": ["ICLR.cc/2026/Conference/Submission4872/Reviewer_Sixx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4872/Reviewer_Sixx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4872/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014417516, "cdate": 1762014417516, "tmdate": 1762917623775, "mdate": 1762917623775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}