{"id": "2bgDXzaYqN", "number": 18345, "cdate": 1758286681550, "mdate": 1759897109585, "content": {"title": "What Is Missing: Interpretable Ratings for Large Language Model Outputs", "abstract": "Current Large Language Model (LLM) preference learning methods such as Proximal Policy Optimization and Direct Preference Optimization rely on direct rankings or numerical ratings of model outputs as a way to learn human preferences. These rankings are subjective, and a single numerical rating chosen directly by a judge is a poor metric to quantify a complex system such as human language. This paper introduces the What Is Missing (WIM) rating system to create better rankings for preference learning methods. WIM is a straightforward method that can be integrated into existing training pipelines, combined with other rating techniques, and used as the input to any preference learning method without changes. To create a WIM rating, natural language feedback for a model output is given by a human or LLM judge. Both the output and the feedback are passed through a sentence embedding model and the cosine similarity between the high dimensional vectors is calculated. Theoretical benefits in the distribution of WIM ratings, compared to numerical ratings, translate into lower loss throughout training, better reward advantage scaling, and better performance in a trained task. Importantly, WIM is interpretable as the reason for the chosen ranking can be discovered easily. WIM provides an alternate way to think about preference learning by shifting the focus away from the algorithms themselves and onto the improvement of the preference data generation pipeline", "tldr": "Improving preference learning with interpretable ratings.", "keywords": ["Large Language Models", "preference learning", "reinforcement learning", "sentence embeddings"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f221b26fa29bb0132bac9dd601481062e850f7f0.pdf", "supplementary_material": "/attachment/c05be2b8999390fbefe10156e2d9b1cda739d3d2.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the What Is Missing (WIM) rating system as an improvement over traditional preference learning methods for LLMs. Current methods like PPO and DPO rely on subjective direct rankings or numerical ratings (e.g., 1-10 scales) to learn human preferences. The authors argue that these approaches are problematic because a single numerical rating poorly captures the complexity of human language, ratings lack interpretability, and discrete rating systems frequently produce duplicate scores that prevent generating meaningful learning signals. The WIM system addresses these shortcomings while remaining compatible with existing training infrastructure."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- 1. Ratings are directly derived from human-readable natural language feedback explaining what is missing. Unlike opaque numerical scores, anyone can understand exactly why a rating was assigned, which enables easy debugging and identification of model weaknesses or rating errors.\n\n- 2. WIM produces a continuous-like distribution instead of clustered discrete values, reducing duplicate ratings from 42.78% (numerical systems) to only 2.00%. This generates 47.82% higher average rating deltas between outputs.\n\n- 3. WIM is algorithm-agnostic and works with any preference learning method (PPO, DPO, GRPO, etc.) without modifications. It can be plugged into existing training pipelines saving engineering time and costs, supports multiple judge configurations (human, separate LLM, or self-judging), and can be mixed with other rating methods through a tunable parameter."}, "weaknesses": {"value": "- The authors propose the concept of \"interpretable rating\" but fail to provide a clear definition of this notion. The interpretability of language models is a highly complex concept that demands a well-defined frameworkâ€”specifying which type of interpretability is being pursued, e.g., mechanistic explanation as in mechanistic interpretability [1] or a theoretical perspective from training dynamics [2]. \n\n- Rating alone means nothing for interpretability because LLMs can still exhibit behaviors like sycophancy [3] or deception [4] to generate unintended responses (which also occurs with benign inputs [5]). For example, they may give relatively positive responses for a bad example when evaluating certain inputs, and thus semantic cosine similarity cannot serve as a good metric.\n\n- Much more literature review is needed. In terms of improving the accuracy and performance of LLM-as-a-judge, there are many alternative solutions such as natural language as reward [6] and improved versions of LLM-as-a-judge (please refer to https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge for more details). As for algorithmic improvements of DPO, there are also many variants that align LLMs with further alignment from representation engineering or feature level (e.g., FPO).  There are almost no baselines comparing against these two fields, which severely undermines the paper's contribution.\n\n- The experiments cannot adequately support the results. Only LLaMA-3 is used, and although I understand that research papers with limited resources should not be required to conduct excessive experiments, relying on only one model with one parameter size cannot demonstrate effectiveness across different model architectures (Qwen performs really differently from LLaMA in tasks like reasoning) and scaling (will this work for larger models?).\n\n[1] https://transformer-circuits.pub/2025/attribution-graphs/biology.html\n\n[2] https://arxiv.org/html/2505.17646v1\n\n[3] https://arxiv.org/abs/2310.13548\n\n[4] https://arxiv.org/abs/2501.16513\n\n[5] https://arxiv.org/abs/2508.06361\n\n[6] https://www.arxiv.org/abs/2506.03637"}, "questions": {"value": "1. Can the authors explain what kind of interpretability they have achieved in this paper?\n\n2. There is no explicit performance gain in benchmarks like MMLU. Are there any reasons for this?\n\n3. Please provide more baselines, e.g., natural-language-as-reward [6] and other improved LLM-as-a-judge methods.\n\n4. Have the authors tested this method on other model architectures (e.g., Qwen, Mistral) to verify generalizability?\n\n5. What are the individual contributions of each component in your proposed method? Please provide ablation studies to demonstrate which parts are essential for the claimed improvements."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vW3yKicYZ3", "forum": "2bgDXzaYqN", "replyto": "2bgDXzaYqN", "signatures": ["ICLR.cc/2026/Conference/Submission18345/Reviewer_c1ep"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18345/Reviewer_c1ep"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727553232, "cdate": 1761727553232, "tmdate": 1762928054273, "mdate": 1762928054273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors introduce \"What is Missing (WIM)\", an approach to rate responses of an LLM and use this ratings for preference optimization. \nThe method employs a Judge (LLM or human) to produce what is missing from a candidate response. Then the cosine similarity is calculated between the candidate response and the Judge's output, and finally the score is mapped to a 1-10 (continuous) rating scale.\nThe main advantages of the method are the interpretable nature of the ratings --the Judge's output can be directly inspected-- and the continuous (rather than discrete) nature of the rating score, allowing the method to model potentially more expressive ratings.\nThe main idea is clearly conveyed and some preliminary experiments are presented, showing the promise of the method. However, many of the arguments and claims made throughout the paper need further rigorous experimentation."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1. The framework proposed is flexible enough to be adapted to a plethora of preference optimization (PO) methods, with the possibility to use either human or LLM-based judges.\nS2. The method tackles the lack of interpretability and expressiveness in preference ratings, a well-known limitation in PO methods."}, "weaknesses": {"value": "The paper mainly lacks rigorous experiments to support the claimed benefits of the model, such as how it compares to strong, simpler baselines (PPO, DPO) in well-stablished experimental setups for preference optimization (e.g. HH, TL;DR, AlpacaEval2, among many others)\nAs such, it is difficult to make grounded conclusions about the contributions of this paper."}, "questions": {"value": "- One of the main assumptions the paper makes is that the cosine similarity of sentence embeddings can be used as an indicator of semantic overlap between two responses. However, this assumption needs to be confirmed experimentally for each use case. Previous work has found that contextual embeddings of tokens and sentences might not be appropriate for semantic similarity comparisons, since a few dimensions might dominate the representation, leading to misleading or inflated cosine similarities [1,2]. \nPlease consider doing a preliminary, sanity-check experiment to confirm that this is not the case, if you haven't already done so.\n- Could you please elaborate why an online optimization algorithm is more appropriate for your methodology rather than an offline one?\n\nThe experimental setup could greatly benefit from the following\n- Direct comparison against other candidate ranking or selection strategies, using basic PO methods such as PPO, DPO, and compared over standard PO benchmarks, e.g. AlpacaEval2.\n- Usage of available PO datasets with ratings, e.g. Argilla DPO Mix 7K\n- Training model on full precision, even models as small as 1B\n- Experiments over a range of preference optimization scenarios well investigated in previous work, such as helpfulness and harmfulness, summary quality control (TL;DR), toxicity, etc. See [3,4]. \n- An analysis on the influence of the difference in length between a response and the Judge output. Ideally, WIM will not show significant bias against extremely short or extremely long responses.\n\n- Table 1 could be explained directly in the text body and save space.\n- Claims and sections that would need experimental evidence: \n  - Section 3.3, \"WIM could improve training of reward models\"\n  - L352, \"WIM can be integrated into existing training pipelines\"... currently tested only for online DPO. Integration on other PO methods would give us a better picture.\n\n\n[1] https://aclanthology.org/2021.emnlp-main.372.pdf\n[2] https://aclanthology.org/2020.emnlp-main.733/\n[3] https://arxiv.org/abs/2305.18290\n[4] https://aclanthology.org/2024.findings-acl.592.pdf"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7t9XhhyoA9", "forum": "2bgDXzaYqN", "replyto": "2bgDXzaYqN", "signatures": ["ICLR.cc/2026/Conference/Submission18345/Reviewer_WnUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18345/Reviewer_WnUb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919621089, "cdate": 1761919621089, "tmdate": 1762928053842, "mdate": 1762928053842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is motivated by the ambiguity and subjectivity of human preference over model outputs, specifically as expressed by boolean or continuous rankings. It instead proposes an automatically inferred ranking, wherein a judge (human or LLM) provides feedback on which information is missing from model outputs, and the ranking is inferred through the cosine similarity between feedback and the original outputs.\n\nThe paper is unfortunately very difficult to follow and the efficacy of its proposed method is not well supported. The theoretical advantages are not clearly demonstrated, and in experiments the proposed method is not significantly better than a randomized preference metric."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well motivated, in that an automatic method to determine preferences would be beneficial."}, "weaknesses": {"value": "- The premise of the paper is counter-intuitive. The proposed metric seems to be limited to instances were preference can be determined through key missing information. However, in those cases the human provided preferences should not be ambiguous nor subjective. In contrast, in cases where there is low human agreement, e.g. preference over stylistic choices in language, there would be no missing information for this metric to capture. \n- The qualitative analysis is vague and based on undisclosed human-provided rankings. The paper additionally provides no examples of the rankings or the generated feedback.\n- The experimental results clearly show that the proposed method is not significantly different from a random baseline. The same is shown for the numerical rankings, which casts doubt on the experimental procedure and/or the rankings themselves. The modest improvements in training loss or reward advantage by themselves cannot support the efficacy of the method.\n- The paper could benefit from intensive proofreading to improve its presentation, including confusing claims, citations used in the wrong format, and numerous typos."}, "questions": {"value": "- Please elaborate on how the numerical rankings were obtained in Section 3. Provide information on who the annotators were and their relation to the paper. Were the annotators familiar with the task and aware of the missing information in each output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KR8dgYTsq1", "forum": "2bgDXzaYqN", "replyto": "2bgDXzaYqN", "signatures": ["ICLR.cc/2026/Conference/Submission18345/Reviewer_x7hE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18345/Reviewer_x7hE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992521126, "cdate": 1761992521126, "tmdate": 1762928053453, "mdate": 1762928053453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel way to provide a score to a summary written by an LLM. In particular, it asks a judge LLM to provide what is missing in the summary and take the cosine similarity between the summary and the response from the judge LLM. The authors provide evidence that support their claim that their WIM score improves the task performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The strength is the novelty of the idea that tries to set up an adversary-like LLM."}, "weaknesses": {"value": "Overall, I feel that the paper has a nice idea but the evidence is on the weak side and the evaluation is far from comprehensive.\n\n- There is a frequent use of passive voice, which makes it difficult to immediately catch if the subject is the authors or existing literature or someone else. There are excellent web articles about why passive voice can make the writing less clear. Please consider switching to active voice to improve readability.\n- Figure 3 does not provide a fair comparison. The problem is that the frequency of WIM rating looks smaller because the binning is thinner. Please plot it again with 10 bins for the WIM rating. The frequency will surely increase.\n- Section 3 could've been more comprehensive by showing that the same kind of results are reproduced across various models.\n- Section 4.1.1 Training loss: I am really not convinced that this is providing a meaningful signal because here, the loss function is using a different reward model. It depends on beta. Ideally, for each method, we could tune beta and report the best. I did not really catch the point trying to make here. \n- Section 4.1.3 reward advantage: I am not sure what signals we are catching here. Large reward advantage just means that we are going away from piref. Simply using a larger step size would achieve this, so it is not clear what the plot signals.\n\t- Also, the authors say \"WIM changing judge increasing logarithmically\", but the curve looks like an upside-down U shape. Why do they think it is a logarithmic curve? Also, the right way to verify if it is logarithmic is to make the x-axis logarithmic and verify if it forms a line..\n\t- Also, it is unclear how much the fitting was accurate. How do we know if the same trend will persist when run with a different random seed? I would love to see the original data (training dynamics) used for fitting the polynomial curve.\n- Overall, there is no explanation on why WIM changing judge vs fixed judge makes a difference\n\n\nminor comments\n- L218: in what specific sense does it help interpretability?\n- I was not able to find the definition of $\\hat r$ in Eq (7), but maybe I could have missed it. I found it later in L415, but it was not clear to me that it was exactly that one. I think we would have to multiply $\\beta$ to the definition of $\\hat r$ therein.\n- is moving judge = changing judge?"}, "questions": {"value": "See the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OTmH5qoGOz", "forum": "2bgDXzaYqN", "replyto": "2bgDXzaYqN", "signatures": ["ICLR.cc/2026/Conference/Submission18345/Reviewer_txzA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18345/Reviewer_txzA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132638628, "cdate": 1762132638628, "tmdate": 1762928053127, "mdate": 1762928053127, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}