{"id": "bRVJcc89Em", "number": 18612, "cdate": 1758289492811, "mdate": 1759897091914, "content": {"title": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty", "abstract": "Generative video models demonstrate impressive text-to-video capabilities,\nspurring widespread adoption in many real-world applications. However, like\nlarge language models (LLMs), video generation models tend to hallucinate, pro-\nducing plausible videos even when they are factually wrong. Although uncertainty\nquantification (UQ) of LLMs has been extensively studied in prior work, no UQ\nmethod for video models exists, raising critical safety concerns. To our knowl-\nedge, this paper represents the first work towards quantifying the uncertainty of\nvideo models. We present a framework for uncertainty quantification of generative\nvideo models, consisting of: (i) a metric for evaluating the calibration of video\nmodels based on robust rank correlation estimation with no stringent modeling\nassumptions; (ii) a black-box UQ method for video models (termed S-QUBED),\nwhich leverages latent modeling to rigorously decompose predictive uncertainty\ninto its aleatoric and epistemic components; and (iii) a UQ dataset to facilitate\nbenchmarking calibration in video models, which will be released after the review\nprocess. By conditioning the generation task in the latent space, we disentangle\nuncertainty arising due to vague task specifications from that arising from lack\nof knowledge. Through extensive experiments on benchmark video datasets, we\ndemonstrate that S-QUBED computes calibrated total uncertainty estimates that are\nnegatively correlated with the task accuracy and effectively computes the aleatoric\nand epistemic constituents.", "tldr": "This paper introduces a uncertainty quantification method for video world models, utilizing latent modeling to decompose total uncertainty into its aleatoric and epistemic components.", "keywords": ["video models", "world models", "uncertainty quantification"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad6870b6530b0ba3834519a496326a3dd34f619c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Text-to-video models are quickly improving and creating excitement both among researchers and users of AI.  However, like LLMs, these models are prone to hallucinate details of their output, especially when the input prompt is underspecified or underrepresented in training data.  To address this challenge, this work presents the first (to their knowledge) study of uncertainty quantification in text-to-video models.  They propose a black-box UQ method based on the epistemic/aleatoric decomposition to help identify when a text-to-video model is likely to hallucinate, and also plan to release a dataset of 40K videos for benchmarking UQ."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Effective uncertainty quantification is a central pillar in creating trustworthy AI systems.  While most focus on UQ in deep learning has been in image classification and more recently LLMs, it is important that these tools are extended to other fields and application areas, for example robotics or other generative media besides text.  This paper aims to take the first step towards developing a framework and tools for UQ in text-to-video systems.  This is a very solid motivation, and creates the potential for a significant contribution."}, "weaknesses": {"value": "The main weakness I find is that this paper does not carefully treat the concepts of epistemic and aleatoric uncertainty, in particular by treating them primarily in terms of the input prompt rather than as properties that depend on the interaction between the model, its capacity, and the data distribution. Aleatoric uncertainty is described as randomness from prompt vagueness, while epistemic uncertainty is tied to a lack of model knowledge. This framing assumes these uncertainties are intrinsic to the prompt, but in practice, they are model- and data-dependent. For instance, if the entire training set consists of videos of cats napping on purple beds in the backs of pickup trucks, then the prompt “a cat napping on a purple bed in the back of a pickup truck” would still display high aleatoric uncertainty, not because the prompt lacks specificity, but because the data distribution itself is highly variable in that region. By focusing almost entirely on prompt semantics, the paper overlooks the fact that the distinction between epistemic and aleatoric uncertainty depends fundamentally on the model and the data it has seen.\n\nThis conceptual problem extends directly into the method. The decomposition in Equation (3) is presented as a principled separation between epistemic and aleatoric uncertainty, but in practice both quantities depend on the behavior and biases of the specific models used to estimate them. The authors estimate aleatoric uncertainty by prompting an LLM to generate refined textual variants and epistemic by sampling multiple videos from the same generative model. Both steps produce variability that arises from model architectures and training data of the various models, not from isolated intrinsic uncertainty types. What they call aleatoric uncertainty reflects the LLM’s own distribution, while their epistemic uncertainty reflects the video embedding model’s representation space, making the split depend on implementation choices rather than underlying epistemic principles. As a result, the decomposition is not theoretically or empirically meaningful.\n\nBeyond these conceptual issues, the method relies on untested and implausible assumptions. The independence assumption discards dependence between the text prompt and the generated video, which is unlikely to hold in text-to-video generation. The estimation of entropy in embedding spaces further introduces arbitrary geometric distortions, since the embedding dimensions and projection have a major effect on the computed entropies. The authors provide no sensitivity analysis or justification for these choices, leaving the reported uncertainty values largely uninterpretable.\n\nThe experimental evaluation generally lacks rigor. The decision to use CLIPScore as the primary accuracy metric is based on a small 10 sample correlation study, an inadequate basis for methodological justification. The subsequent experiments that claim to disentangle aleatoric and epistemic uncertainty depend on opaque subsetting of data where one component is deemed zero according to the authors’ own estimators, introducing circular reasoning. These experimental protocols make the reported calibration and correlation results difficult to trust.\n\nOverall, the proposed decomposition lacks solid conceptual grounding, the implementation does not meaningfully separate uncertainty types, and the empirical evaluation does not convincingly support the claims."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FvrOo1N1RR", "forum": "bRVJcc89Em", "replyto": "bRVJcc89Em", "signatures": ["ICLR.cc/2026/Conference/Submission18612/Reviewer_Mt95"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18612/Reviewer_Mt95"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760873901341, "cdate": 1760873901341, "tmdate": 1762928327761, "mdate": 1762928327761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- The authors introduced a framework to measure the uncertainty of video generative models. \n- The framework consists of a metric for evaluating the calibration of video models based on robust rank correlation estimation.\n- They also introduce S-QUBED, a black-box UQ method for video models. S-QUBED effectively distinguishes between uncertainty arising from ambiguous prompts and uncertainty stemming from the model's lack of knowledge.\n- They will also release a dataset of 40K videos across diverse tasks to help benchmark calibration in video models.\n- The authors used their method to disentangle and understand aleatoric and epistemic misunderstandings of the video generation models. For example, to assess epistemic misunderstanding, they generated multiple videos for the same prompt and embedded them. Then, they measured the embeddings' spread, with wider spread indicating higher epistemic uncertainty.\n- For the main result of their work, they further study the correlation between accuracy and the different uncertainties. They find that when uncertainty is higher, accuracy tends to be lower. This holds for both overall uncertainty and aleatoric/epistemic misunderstanding."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Uncertainty quantification of LLMs is well studied, but not studied at all for video generation models. This work was novel in that it studied uncertainty quantification of video generation models.\n- The black box approach makes it accessible to evaluate any video generation model.\n- The authors presented the material well, providing the necessary background to understand the motivation and importance of this work, which is especially important given its novelty."}, "weaknesses": {"value": "- I would like to see empirical results and to validate S-QUBED on other open (non-API) video models, given that it is a black-box approach. The authors mentioned that different models were considered but not evaluated due to access and compute constraints. However, I believe there should be multiple open text-to-video models to evaluate S-QUBED on (e.g., OpenSora).\n- Typical metrics (e.g., CLIP, PSNR) for evaluating text-to-image and text-to-video models often do not align with human judgment. Would like to see the correlation of uncertainty with human judgment metrics."}, "questions": {"value": "No questions as the background, motivation, and results were presented well."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "54Z0cYrdkN", "forum": "bRVJcc89Em", "replyto": "bRVJcc89Em", "signatures": ["ICLR.cc/2026/Conference/Submission18612/Reviewer_nQDP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18612/Reviewer_nQDP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683152976, "cdate": 1761683152976, "tmdate": 1762928327305, "mdate": 1762928327305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is (to the authors’ knowledge) the **first study of uncertainty quantification (UQ) for text-to-video models**, proposing a three-part framework: (i) a **calibration metric** based on robust rank correlation between uncertainty and task accuracy, (ii) a black-box UQ method, **S-QUBED**, that uses a **latent-space factorization** to **decompose total predictive uncertainty** into **aleatoric** (prompt vagueness) and **epistemic** (model ignorance) components, and (iii) a ~**40K-video UQ dataset** for benchmarking. Experiments on VidGen-1M and Panda-70M show that S-QUBED’s total uncertainty is **significantly negatively correlated** with semantic accuracy (CLIP score), and its decomposition yields calibrated aleatoric/epistemic trends on subsets where the other source of uncertainty is minimal."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Positions UQ for video generation as a first-class problem; formal **entropy decomposition** (h(V|\\ell)=h(V|Z)+h(Z|\\ell)) cleanly maps to epistemic vs. aleatoric sources. \n* **S-QUBED** operates without model internals, aligning with many **closed-source video models**. \n* Uses **Kendall’s τ** and demonstrates **significant negative correlation** between S-QUBED uncertainty and **CLIP accuracy**, with visuals that match the trend.  \n* Empirical **disentangling** of aleatoric vs. epistemic uncertainty shows expected behavior on curated subsets. \n* Plans to release a **~40K-video UQ dataset** covering diverse tasks."}, "weaknesses": {"value": "* Calibration hinges primarily on **CLIP similarity**; other perceptual metrics (SSIM/PSNR/LPIPS) show weak or insignificant correlations, raising concerns about **metric sensitivity** and potential semantic-evaluator bias. \n* Estimating **epistemic uncertainty** requires **multiple generations per latent prompt**, which the authors acknowledge as a limitation. \n* Main experiments use **Cosmos-Predict2** and two datasets; broader **model diversity** and real-world perturbations (codecs, length, audio conditions) are not deeply explored."}, "questions": {"value": "1. Beyond CLIP, what **additional accuracy signals** (e.g., human semantic judgments, video-text retrieval scores, physics consistency probes) are necessary to **validate calibration** and mitigate evaluator bias? \n2. What **sampling schedules** (fewer latent prompts/videos, adaptive stopping) or **latent-space proxies** would you require to deem S-QUBED **computationally practical** without sacrificing epistemic resolution? \n3. Which **additional models/datasets** or **deployment artifacts** (compression, prompt styles, audio/no-audio) would most convincingly demonstrate that the **aleatoric/epistemic decomposition** remains **stable and calibrated** in the wild?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "0uYIykKYYi", "forum": "bRVJcc89Em", "replyto": "bRVJcc89Em", "signatures": ["ICLR.cc/2026/Conference/Submission18612/Reviewer_RL8t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18612/Reviewer_RL8t"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818582451, "cdate": 1761818582451, "tmdate": 1762928326870, "mdate": 1762928326870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a black-box framework that lets text-to-video models express uncertainty by decomposing predictive uncertainty into aleatoric (prompt vagueness) and epistemic (model ignorance) components. The framework is evaluated with a rank-correlation-based calibration metric, and a 40K-video UQ benchmark is released."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-presented, well-written, and the motivation is justified.\n2. The research topic of principled evaluation of synthetic videos is very timely and important.\n3. The proposed dataset will be valuable."}, "weaknesses": {"value": "1. The method’s evidence of **general** video-model UQ almost entirely depends on one text-to-image-to-video pipeline (Cosmos-Predict2). While I appreciate that authors state the API/compute constraints, it will be more convincing if the paper proposes potential solutions or fixes to overcome the challenge. That being said, the practicality and calibration of stronger video models shall be evaluated.\n2. Please fix salient typos such as \"video modes\" (Page 3) and \"peak signal-to-noise ration\" (Page 13)."}, "questions": {"value": "While there are several weaknesses stated above, I believe this paper will be contributive and will provide new insights to the community. I therefore have the initial rating of 6 for this paper. Please note that my final rating will be conditioned on the soundness of the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cmAqDaA81k", "forum": "bRVJcc89Em", "replyto": "bRVJcc89Em", "signatures": ["ICLR.cc/2026/Conference/Submission18612/Reviewer_eJzE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18612/Reviewer_eJzE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762031145587, "cdate": 1762031145587, "tmdate": 1762928326274, "mdate": 1762928326274, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}