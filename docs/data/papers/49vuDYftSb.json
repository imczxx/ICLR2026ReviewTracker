{"id": "49vuDYftSb", "number": 4946, "cdate": 1757816784180, "mdate": 1759898003525, "content": {"title": "Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow Models", "abstract": "We present a mechanism to steer the sampling diversity of denoising diffusion and flow matching models, allowing users to sample from a sharper or broader distribution than the training distribution. We build on the observation that these models leverage (learned) score functions of noisy data distributions for sampling and show that rescaling these allows one to effectively control a 'local' sampling temperature. Notably, this approach does not require any finetuning or alterations to training strategy, and can be applied to any off-the-shelf model and is compatible with both deterministic and stochastic samplers. We first validate our framework on toy 2D data, and then demonstrate its application for diffusion models trained across five disparate tasks -- image generation, pose estimation, depth prediction, robot manipulation, and protein design. We find that across these tasks, our approach allows sampling from sharper (or flatter) distributions, yielding performance gains e.g., depth prediction models benefit from sampling more likely depth estimates, whereas image generation models perform better when sampling a slightly flatter distribution.", "tldr": "We proposed a training-free temperature sampling technique for diffusion and flow models, improving performance of off-the-shelf models across various domains.", "keywords": ["Diffusion sampling", "flow matching", "score-based model"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fef81ed95b1d9c460721dc31a5c5990efaff866d.pdf", "supplementary_material": "/attachment/0378602e85bcbbe8bdf0747935eb62ee63c8bd4a.zip"}, "replies": [{"content": {"summary": {"value": "The authors consider the problem of temperature rescaling. That is, given a diffusion model (or equivalently a flow matching model) sampling the density $p$ they aim to sample from $p_k(x)\\propto \\exp(1/k \\log p(x))$. To this end, they rescale the score functions by a time-dependent factor, where the time-dependence is derived from the isotropic Gaussian case. The method is tested on a wide range of applications."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea is very simple, clearly presented and the paper includes experiments for plenty of applications."}, "weaknesses": {"value": "There is no justification why the proposed method should make sense despite the trivial case of an isotropic Gaussian (or seperated Gaussian mixtures, which are quite the same). More generally, I am quite sure that the generated distribution with TSR does not coincide with the target distribution $p_k$ whenever $p$ is not an isotropic Gaussian. The paper does not include any analysis or bound of this error. The claim that the method has the intended effects is purely empirical.\n\nThe rest of the paper is a tour through different applications, each of them using a well-tuned diffusion model taken from the literature. While slight adaptions can lead to minor improvements, it reduces the image quality drastically for larger adaptions. Most likely, the main reason is that changing the temperature on image datasets is not really a sensible task.\n\nIn summary, the paper consists out of a small implementation trick and I wouldn't consider the methological improvement over prior work as high enough."}, "questions": {"value": "I have a couple of questions regarding the experiments:\n\n- In the toy experiments, where the authors compare CNS with TSR using the same scaling parameter $k$. Is there any reason to believe that the scaling parameter $k$ in CNS corresponds to the $k$ in TSR (which corresponds to the distribution $\\propto \\exp(1/k \\log p(x))$)? \n\n- The example from Fig. 2 is missing descriptions. How exactly is TRS used for conditional sampling? Or is it taking already the conditional model and adjusting the conditional scores? In this case: Why is it relevant that it has been a conditional distribution in the first place?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u3mOW9DwaM", "forum": "49vuDYftSb", "replyto": "49vuDYftSb", "signatures": ["ICLR.cc/2026/Conference/Submission4946/Reviewer_1Biq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4946/Reviewer_1Biq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760611326276, "cdate": 1760611326276, "tmdate": 1762917786196, "mdate": 1762917786196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper study a test-time sampling approach for diffusion models, creating a sharper or broder distribution than the one at training to sample data from. Particularly, they propose to apply temperature sampling technique to diffusion model to control the diversity of the sampled output by rescaling the learned score functions. The method demonstrates the applications to both stochastic and deterministic samplers and validates the performances on 5 different tasks, such as image generation, pose estimation, depth prediction, robot manipulation, and protein design."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Though the idea of applying temperature scaling is not new, the authors propose an elegant way to effectively alter the learned score function without stripping away the modes and structure of data. The main idea is to rescale the variance of intermediate variables of diffusion model while preserving the mean (modes).\n2. The exposition is easy to follow."}, "weaknesses": {"value": "1. Since this method introduces additional two parameters $(k, \\sigma)$ for tuning, these are varied between tasks. It is better to have a conclusion on the range of them per task. \n2. There is no mention of a systematic way for choosing an optimal parameter set. This is extremely important for test-time scaling techniques these days.\n\n\n\n- L056: \"For example, ... we want the more likely estimate(s) as output\". \"more likely estimates(s)\" is not very clear, it is different than what model have been trained on?"}, "questions": {"value": "Q1. Look at Figure 10, the method seems not bring any performance gain (even worse), expecially for small inference steps (< 100). Any rationale behind this? \n\nQ2. Will the findings of optimal $(k, \\sigma)$ be hold for different model version like StableDiffusion vs Flux? \n\nQ3. I wonder if the finding is applicable to video domains since video is more dynamic and complex. This could be an useful test."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F25U1YcDzy", "forum": "49vuDYftSb", "replyto": "49vuDYftSb", "signatures": ["ICLR.cc/2026/Conference/Submission4946/Reviewer_RPgN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4946/Reviewer_RPgN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964304899, "cdate": 1761964304899, "tmdate": 1762917785843, "mdate": 1762917785843, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Temporal Score Rescaling (TSR), a method designed to steer the distribution of diffusion and flow models during inference.\nCompared to Constant Noise Scaling (CNS), TSR is less prone to mode collapse.\nUnder relatively strong assumptions, TSR provides bounded error guarantees with respect to the ideal Gaussian mixture distribution, whereas CNS fails to achieve true temperature scaling.\nThe authors validate the superiority of TSR over CNS through experiments on synthetic data and conduct extensive evaluations on real-world datasets.\nExperimental results show that across various quality–diversity trade-off scenarios, TSR consistently achieves a better Pareto frontier than CNS."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Bounded error: Under independence and well-separatedness assumptions, the temperature sampling estimated by TSR has exponential and polynomial error bounds, which vanish with the score estimation error.\n\n- Extensive experiments: TSR’s effectiveness is clearly demonstrated on synthetic data, and its superiority in achieving a better Pareto frontier is demonstrated on real-world datasets. The experiments span multiple domains and data modalities, confirming the general applicability of the method.\n\n- Clear presentation.\n\n- The idea is simple yet effective, with strong potential for broad applications."}, "weaknesses": {"value": "- Mode collapse: Although TSR claims to avoid mode collapse under strong assumptions (e.g., Gaussian mixture distributions with well-separated components), this claim lacks solid empirical support on high-dimensional real-world data. The paper provides no direct quantitative verification of mode preservation.\n\n- Sigma: In the theoretical derivation, σ originates from the variance of the underlying data distribution; however, in real-world experiments, it becomes a tunable hyperparameter. This deviation may weaken the theoretical interpretability of TSR and increases the tuning overhead across different tasks."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oKoyqQ5qf5", "forum": "49vuDYftSb", "replyto": "49vuDYftSb", "signatures": ["ICLR.cc/2026/Conference/Submission4946/Reviewer_GhEF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4946/Reviewer_GhEF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973829933, "cdate": 1761973829933, "tmdate": 1762917785205, "mdate": 1762917785205, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method to sample from tempered distribution of pretrained diffusion models.  The key method relies on re-scaling the score function with a factor depending on the variance of the data distribution and the temperature factor. This scaling is motivated from the mixture-of-gaussian data assumptions, and are empirically validated on real-world data. The method is benchmarked against a variety of tasks including synthetic 2D examples, text-to-image generation, protein generation, depth estimation, pose prediction. and robotic manipulation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method provides a training-free, plug-and-play inference-time method for tempered sampling in diffusion models (although in somewhat heuristic way). It is simple to implement, as it only involves rescaling the score function in certain ways.\n\n- In text-to-image generation tasks, the performance is improved with k slightly smaller than 1, and in robotic manipulation task, the performance is improved with k larger than 1 for most problems, when compared to the base sampling method."}, "weaknesses": {"value": "- While the proposed method is motivated by closed-form derivations under Gaussian assumptions, it remains a heuristic approach in the general case.\n\n- The juxtaposition with the Mixture of Gaussian case in Section 3.2 should be clarified in the main text. This case represents a more realistic approximation than the single Gaussian scenario. However, the current argument in the main text feels somewhat hand-wavy. In particular, it is unclear how the proposed reasoning applies when t lies in the mid-range of [0,1], since the arguments at the extreme points do not extend naturally to this region. I also found the structure in the relevant appendix section difficult to follow.\n\n- The presentation in Section 5.3 on depth estimation could be improved for clarity. Please refer to my detailed comments in the “Questions” section below.\n\n- The empirical results appear mixed. For instance, the proposed method consistently underperforms CNS in the pose prediction task. In Figure 6, the conclusions are also not straightforward: the best FID is achieved by CNS under certain settings, while the best designability is obtained by the proposed method under others. I also find the linear trend fits for CNS and the proposed method confusing; it is unclear how fitting a single linear line across results from different hyperparameter settings is justified.\n\n- It is difficult to assess the statistical significance of the results, as standard errors are not reported in the tables and error bars are missing from the plots."}, "questions": {"value": "- Can you explain more on the interpretation of $\\sigma$ in the first paragraph of Section 4.2? In particular, as $\\sigma$ corresponds to the variance of the data distribution, why is it that it \"indicates how early we want to steer the sampling process\"? In the same paragraph, I also found the term \"initial Gaussian distribution\" confusing. While from the previous context I understand that this refers to the data distribution under gaussian assumption, it can be overloaded with the initial gaussian of the reverse diffusion process.\n\n- Can you explain the task in Sec 5.3 depth estimation, how diffusion model is used and why a tempered sampling approach would be favorable? Also can you provide more context and interpretation of Figure 7?\n\n- Why is it that CNS is only benchmarked against in some of the tasks, and not others. For example, results for CNS are not included in Figure 4,5,7 and Table 3. \n\n- The following paper may be a good reference for tempered sampling and baseline to compare to. \n\nSkreta, Marta, et al. \"Feynman-kac correctors in diffusion: Annealing, guidance, and product of experts.\" arXiv preprint arXiv:2503.02819 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aOuq13hnzw", "forum": "49vuDYftSb", "replyto": "49vuDYftSb", "signatures": ["ICLR.cc/2026/Conference/Submission4946/Reviewer_SgqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4946/Reviewer_SgqF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4946/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762024878325, "cdate": 1762024878325, "tmdate": 1762917784754, "mdate": 1762917784754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}