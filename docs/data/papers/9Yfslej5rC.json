{"id": "9Yfslej5rC", "number": 15198, "cdate": 1758248904492, "mdate": 1759897322074, "content": {"title": "SO-Lazy-BiO: Accelerating Bilevel Optimization with Reduced Second-Order Information Computation", "abstract": "Bilevel optimization has attracted significant attention recently due to its applicability in various large-scale machine learning tasks (e.g., the large language model (LLM) pretraining-finetuning pipeline).\nIn the literature, one popular approach for solving bilevel optimization problems is to use hypergradient-based methods. \nHowever, computing the hypergradients requires evaluating second-order information (Hessians/Jacobians) of the lower-level objective function, which is computationally expensive.\nTo address this challenge, we propose SO-Lazy-BiO (**S**econd-**O**rder **Lazy** **Bi**level **O**ptimization), an algorithmic framework that significantly accelerates the state-of-the-art (SOTA) bilevel optimization methods by allowing *infrequent* evaluation of second-order information.\nWe theoretically establish the performance of SO-Lazy-BiO and show that, despite the additional errors incurred by the infrequent evaluations of second-order information, SO-Lazy-BiO *surprisingly* matches the computation complexity of existing non-lazy bilevel algorithms, while requiring *fewer* second-order information evaluations.\nThis leads to substantial savings in both computational cost and wall-clock running time.\nWe further conduct extensive experiments to demonstrate that SO-Lazy-BiO enjoys significant gains in numerical performance compared to SOTA, especially for large-scale tasks.\nTo our knowledge, this is the first work to employ infrequent second‑order computations while still guaranteeing the convergence of stochastic bilevel algorithms.", "tldr": "", "keywords": ["Bilevel optimization", "stochastic optimization", "lazy second-order information evaluation"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5cf012db7bb029b5a356b3b59c245212ea51d345.pdf", "supplementary_material": "/attachment/e51cbfb11608d7bfe83b6430a8f69721638218f8.zip"}, "replies": [{"content": {"summary": {"value": "This paper considers stochastic bilevel optimization. The authors introduce SO-Lazy-BiO (Second-Order Lazy Bilevel Optimization) — a new framework that accelerates bilevel optimization (BiO) by evaluating second-order information (Hessian/Jacobian-vector products) only infrequently, rather than at every iteration. Despite this “lazy” update, the method preserves convergence guarantees comparable to state-of-the-art (SOTA) bilevel algorithms while substantially reducing computational cost and wall-clock time.\n\nTheoretically, the authors prove the convergence rates as well as the sampel complexity of their proposed algorithms, and conduct experiments to compare different bilevel optimization algorithms to support their claims on reducing the number of Hessian-vector products needed in bilevel optimization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Theoreticall speaking, the authors provide a novel stochastic bilevel optimization algorithm that only updates the Hessian-related parameters every N steps where N is a hyperparameter to tune. They further provide a solid analysis of their algorithm to showcase the convergence rates and sample complexity. The results can be seen as direct generalization of existing bilevel optimization algorithms.\n\nEmpirically speaking, the authors conduct experiments ranging from computer vision to LLM alignment. The results align well with the theoretical findings. Source code is provided in the supplementary materials."}, "weaknesses": {"value": "Major:\n\n1. One limitation of the theory part is, the authors seem not to discuss the optimal choice of $N$. For example, the big-O bound in theorem 5.5 is about $\\sqrt{N/T} + 1/\\sqrt{NT}$, which obtains its minimum at $N=1$, and thus optimal $N$ should be $\\mathcal{O}(1)$ if we take the ignored constants into account -- this suggests that the potential improvement on reducing the number of Hessian-vector products is of order constant, not $\\epsilon$. They authors may want to discuss this in detail.\n\n2. The tricks of lazy-Hessian already exist in literature. Introducing them to bilevel optimization does not have too much novelty despite that the work is solid.\n\nSome other comments:\n\n1. The momentum update for $h$ in Eq. (9) is not novel and the authors may want to add a few discussions about MA-SOBA work or some other related papers that use this momentum in their algorithms\n\n2. The authors do not provide the motivation for using bilevel optimization for solving problems like RLHF and data reweighting."}, "questions": {"value": "1. Could the authors provide some discussions on how to choose the optimal $N$ from a theoretical perspective? Furthermore, are there empirical findings or guidance on setting $N$ in different experiments/settings.\n\n2. For experiments like RLHF and data reweighting, from eixsting literature we understand they could be reformulated as bilevel optimization problems. Could the authors share their insights/comments/discussions on why practitioners would choose bilevel optimization algorithms, which are typically hard to implement in large scale because of the Hessian-vector products, rather than classical optimization methods? For example RLHF was originally executed in two stages -- one reward model training and one policy model training, but bilevel optimization for RLHF seems to unify these two steps into a one-stage problem. Are there benefits of doing this, from an empirical perspective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "3MMwVVe0ob", "forum": "9Yfslej5rC", "replyto": "9Yfslej5rC", "signatures": ["ICLR.cc/2026/Conference/Submission15198/Reviewer_Hero"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15198/Reviewer_Hero"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761205799649, "cdate": 1761205799649, "tmdate": 1762925500637, "mdate": 1762925500637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies stochastic bilevel optimization where lower-level problem is strongly-convex and proposes a lazy Hessian strategy to reduce the computational overhead of second-order bilevel methods. The method keeps the same convergence rate, reduces Hessian/Jacobian-vector calculations by orders. This paper also proposed a momentum variant without the large-batch requirement. Numerical experiments on LLM fine-tuning validates the proposed algorithm."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper has strong theoretical guarantee which has the order-wise improvement for the Hessian and Jacobian-vector computations. \n2. The experiments on LLM fine-tuning task are innovative and solid."}, "weaknesses": {"value": "1. Corollary 5.6 did not specify the requirements for $N$ to achieve such computation complexity. Also as N is a key hyperparameter in the proposed algorithm, what is the guideline for choosing N in practice? \n2. The novelty appears limited: lazy updates are well studied in optimization, and the adaptations required for bilevel optimization appear to be standard combinations of bilevel tools with lazy-update strategies.  \n3. This result is not entirely surprising: order-wise savings in Hessian- and Jacobian–vector products via lazy updates are expected, given that fully single-loop methods already perform just one such product per round [1]. \n\n[1] Mathieu Dagr´eou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. NeurIPS, 2022."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mmiDvY6kXr", "forum": "9Yfslej5rC", "replyto": "9Yfslej5rC", "signatures": ["ICLR.cc/2026/Conference/Submission15198/Reviewer_d1on"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15198/Reviewer_d1on"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969245665, "cdate": 1761969245665, "tmdate": 1762925499488, "mdate": 1762925499488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SO-Lazy-BiO, a single-loop stochastic bilevel framework that reuses stale second-order information: the Hessian-inverse-vector product (HIVP) (via a one-step SGD solve) and the Jacobian-vector product (JVP) are updated only every $N$ steps, all other steps reuse the last values. The main theorem shows that under the nonconvex-strongly-convex bilevel setting, SO-Lazy-BiO requires $O(N\\epsilon^{-2})$ partial gradient evaluations and $O(\\epsilon^{-2})$ second-order information\nevaluations to reach an $\\epsilon$-stationary point. A momentum-free variant is also analyzed but achieves a weaker bound and requires larger batches. Experiments on (i) RLHF reward-model data-weighting, (ii) LLM-alignment data-weighting (Llama-3.2-1B), and (iii) hyper-representation with ResNet-20 report lower wall-clock and far fewer HVP/JVP calls than non-lazy baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The single-loop algorithm is described clearly with two laziness modes, and they are easy to implement.\n\n- The proposed method preserves $O(\\epsilon^{-2})$ second-order complexity while cutting HVP calls.\n\n-  Experimental results verify that the infrequent evaluations of second-order information lead to computational savings."}, "weaknesses": {"value": "- Corollary 5.8 requires large batch sizes of $\\Theta(\\epsilon\\^{-1})$ and $\\Theta(N\\epsilon\\^{-1})$, which is inconsistent with practical settings that typically use small batch sizes.\n\n- This work does not demonstrate clear advantages over the MA-SOBA method proposed in [1], which is an optimal single-loop algorithm achieving $O(\\epsilon^{-2})$ complexity with the same order of HVP/JVP calls. Moreover, [1] does not rely on large batch-size assumptions.\n\n\n[1] Chen, X., Xiao, T., & Balasubramanian, K. (2024). Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions. Journal of Machine Learning Research, 25(151), 1-51."}, "questions": {"value": "- The Corollary 5.8 for SO-Lazy-BiO-SGD specifies batch sizes of $\\Theta(\\epsilon\\^{-1})$ and $\\Theta(N\\epsilon\\^{-1})$. Were these batch-size settings actually used in the experiments?\n\n- Could you provide a detailed theoretical and technical comparison to clarify the differences and any advantages of SO-Lazy-BiO over MA-SOBA?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UNR4itNuxn", "forum": "9Yfslej5rC", "replyto": "9Yfslej5rC", "signatures": ["ICLR.cc/2026/Conference/Submission15198/Reviewer_khk4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15198/Reviewer_khk4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975434694, "cdate": 1761975434694, "tmdate": 1762925498916, "mdate": 1762925498916, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a bilevel optimization method that refreshes second-order information infrequently and reuses it for several iterations while keeping first-order gradients up to date. The analysis argues that the method attains a convergence rate comparable to standard second-order approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality comes from combining periodic reuse of Hessian and Jacobian vector products with a lightweight approximation of the Hessian inverse vector product, together with proofs that the lazy updates retain the same order of convergence."}, "weaknesses": {"value": "The paper repeatedly claims a good balance between accuracy and cost, but does not quantify it with a rigorous oracle breakdown at a fixed target stationarity. Add a table reporting counts of upper- and lower-level gradients, counts of Hessian and Jacobian vector products, and the mini-batch sizes required to reach a common tolerance across all methods."}, "questions": {"value": "The authors claim that the proposed algorithm strikes an effective balance in its use of second-order information. Could you quantify this more precisely relative to both fully first-order and standard second-order methods? For instance, the authors said that very large batch gradients are necessary in the fully first-order method. Is it possible to quantify the difference?\n\nOn the other hand, I understand the intuition, but the practical advantage remains unclear. The paper would be stronger if you could articulate an optimal or near-optimal policy for deploying the Hessian, or at least provide principled guidance on when and how often to use it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2gqzXA2pga", "forum": "9Yfslej5rC", "replyto": "9Yfslej5rC", "signatures": ["ICLR.cc/2026/Conference/Submission15198/Reviewer_pMDx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15198/Reviewer_pMDx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762362926659, "cdate": 1762362926659, "tmdate": 1762925498375, "mdate": 1762925498375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}