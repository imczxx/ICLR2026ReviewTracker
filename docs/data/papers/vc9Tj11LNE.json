{"id": "vc9Tj11LNE", "number": 13937, "cdate": 1758225504744, "mdate": 1763406809142, "content": {"title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game", "abstract": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader’s actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.", "tldr": "We propose a novel game-theoretic approach to RLHF by framing preference optimization as two-player sequential-move game.", "keywords": ["alignment", "rhlf", "preference optimization", "game theory", "human feedback", "test-time improvement"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83c5ba70d418be4753d4126a22dd444bcba24fd2.pdf", "supplementary_material": "/attachment/203dba4ee62db1ed0df22482650c5a769e71b1af.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Stackelberg Learning from Human Feedback (SLHF), a framework for preference optimization that models alignment as a sequential-move game between a Leader policy and a Follower policy. The authors propose StackelbergGDA, a two-timescale gradient descent-ascent algorithm to approximate the Stackelberg equilibrium. The framework naturally enables inference-time refinement through iterative sampling. Experiments on language models ranging from 0.5B to 8B parameters demonstrate that SLHF achieves strong alignment performance, with the Follower policy consistently outperforming RLHF and NLHF baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- **Strong motivation and intuition:** The paper provides excellent motivating examples, particularly the Condorcet paradox analysis in Section 4.1, which clearly illustrates how SLHF, RLHF, and NLHF differ in handling intransitive preferences.\n\n- **Practical value of inference-time refinement:** The Leader-Follower structure naturally supports inference-time improvement without additional training, which is valuable for LLM applications. The empirical results in Table 4 demonstrate impressive cross-model generalization.\n\n- **Comprehensive experiments:** The paper includes thorough empirical evaluation across multiple model scales (0.5B to 8B parameters) and provides detailed ablations and additional results in the appendix."}, "weaknesses": {"value": "### Major Weaknesses\n1. **Mischaracterization and Missing Literature on NLHF** \n\n**Issue:** The claim in the introduction that \"simultaneous play forces both players to optimize against a moving opponent which can hinder convergence\" is not accurate. Recent NLHF works have established polynomial or even linear convergence rates to Nash equilibrium: https://arxiv.org/abs/2312.00886, https://arxiv.org/abs/2401.04056, https://arxiv.org/abs/2410.16714, https://arxiv.org/abs/2503.08942. The paper should acknowledge these theoretical convergence guarantees rather than suggesting NLHF inherently struggles with convergence.\n\n**Critical omission:** A closely related work (https://arxiv.org/abs/2502.18099v2) that also studies Stackelberg games for LLM alignment was published over half a year ago and cannot be considered concurrent work. This significantly undermines the novelty claim. The authors must thoroughly discuss this work and clarify their contributions relative to it.\n\n2. **Problematic Characterization of Mixed Strategies**\n\n**Issue 1:** At the end of Section 3, the authors state: \"when no action is majority-preferred the equilibrium necessarily involves mixed strategies. This inherent stochasticity can be undesirable in applications where consistency and reliability are critical.\"\nThis characterization is misleading for several reasons: (1) In RLHF/NLHF/SLHF with KL regularization ($\\tau > 0$), the optimal policy is always a distribution over responses, not deterministic. (2) A \"deterministic policy\" in LLM is not well-defined, unless we sample with temperature 0, but the model still outputs a probability distribution.\n\n**Issue 2:** The claim before Section 4.1 that \"there exists a deterministic Stackelberg equilibrium\" suffers from the same conceptual problem. With regularization (which the paper uses throughout), policies must be stochastic. The best response is essentially the RLHF solution when viewing win-rate as reward.\n\n3. **Lack of Theoretical Analysis:** The paper provides no convergence guarantees for either Algorithm 1 or Algorithm 2. Key questions remain unanswered: Does StackelbergGDA converge to a Stackelberg equilibrium? What is the convergence rate? Under what conditions does convergence occur? Given that RLHF/NLHF methods now have established convergence theory, the lack of any theoretical analysis for SLHF is a significant weakness. The paper should at minimum discuss the challenges in proving convergence or provide experimental evidence of convergence behavior.\n\n4. **Algorithm Presentation Issues:** Inconsistency: Algorithms 1 and 2 are essentially different algorithms (analogous to OGDA vs. OMWU, https://arxiv.org/abs/2006.09517). For consistency and clarity, I suggest replacing Algorithm 1 with a theoretical version of Algorithm 2.\n\n### Minor Weaknesses and Questions\n\n5. **Baseline choice:** The paper uses Nash-MD-PG as the primary NLHF baseline, which has been shown to converge extremely slowly (see Section 5 of https://arxiv.org/abs/2503.08942). How would SLHF compare against faster NLHF algorithms like those mentioned above?\n\n6. The intransitivity analysis (57% of graphs contain cycles) is interesting but could be expanded—what is the typical cycle length? How does this compare to other datasets?"}, "questions": {"value": "Please address my concerns in the Weakness section. There is no other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bXLcbgtV8G", "forum": "vc9Tj11LNE", "replyto": "vc9Tj11LNE", "signatures": ["ICLR.cc/2026/Conference/Submission13937/Reviewer_Uhos"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13937/Reviewer_Uhos"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861552447, "cdate": 1761861552447, "tmdate": 1762924441186, "mdate": 1762924441186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Stackelberg Learning from Human Feedback (SLHF), a new framework that models alignment as a sequential two-player game between a Leader (the action proposer) and a Follower (the conditional refiner). Unlike RLHF (scalar rewards) or NLHF (simultaneous equilibria), SLHF exploits sequential asymmetry to capture richer preference structures and support inference-time refinement. The authors propose STACKELBERG-GDA to efficiently approximate equilibria and scale training to large LLMs (0.5B–8B). Empirical results show SLHF achieves strong alignment across diverse datasets, with Follower policies improving outputs even when transferred to unseen models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "This paper introduces an innovative game-theoretic Stackelberg structure for preference learning. \n\nThe proposal is rooted in the existence of intransitivity in pairwise preferences. It proposes a rational computational solution that replicates the logic with additional transparency into the learning and inference process.\n\nExperiments showed that it outperforms or matches RLHF/NLHF baselines across multiple datasets.\n\nSome theoretical foundations are discussed, i.e., qualitative connections to RLHF and NLHF, constructive conditions for numerical approximation, standard regularity assumptions, an equilibrium analysis, and an optimization algorithm (Stackelberg-GDA).\n\nSource code and assets are open."}, "weaknesses": {"value": "The two-policy framework, i.e., Leader policy and Follower policy, increases computational and training costs."}, "questions": {"value": "Despite the discussion of model non-transitivity in Appendix D1, can you elaborate further on the merits of intransitivity coverage, compared to real-valued reward models?\n\nIn the majority of RLHF literature, people rely on 'transitivity' assumptions for its simplicity, while in real-world datasets, binary reward models, e.g., the Bradley-Terry (BT) model, when explicitly used, are known to be subject to 'intransitivity' because they rely on scalar variables that assume all preferences are transitive.\n\nFor your information, the literature below studied representative preference datasets in the real world, where the 'transitive' relationship between preference annotations may not always hold. \n- https://arxiv.org/abs/2409.19325 (Duan et al., 2017)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cwwbOtucn2", "forum": "vc9Tj11LNE", "replyto": "vc9Tj11LNE", "signatures": ["ICLR.cc/2026/Conference/Submission13937/Reviewer_qE41"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13937/Reviewer_qE41"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985039954, "cdate": 1761985039954, "tmdate": 1762924440823, "mdate": 1762924440823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Stackelberg Learning from Human Feedback (SLHF), a new framework for aligning LLMs that models the problem as a sequential-move game between two policies: a Leader that commits to an initial response and a Follower that refines it. This sequential approach avoids the need for a single scalar reward model, unlike traditional RLHF, allowing it to better handle complex or intransitive preferences. The paper also proposes an algorithm, STACKELBERGGDA, to find the game's solution. A key advantage of this framework is its natural ability to perform inference-time refinement, where the Follower can be used to iteratively improve the Leader's output. Experiments demonstrate that the SLHF Follower policy not only improves upon its own Leader's outputs but also consistently refines and enhances the responses from other, independently trained models without any additional fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Unlike standard RLHF, SLHF optimizes directly over pairwise preferences without collapsing them into a single scalar reward, allowing it to handle complex and intransitive preference cycles.\n- The Leader-Follower structure naturally supports improving model outputs at inference time, as the Follower is explicitly trained to refine a given response, allowing for iterative improvement with more computation.\n- By decomposing the problem, the Follower solves a simpler refinement task against a fixed action rather than a non-stationary opponent, leading to more stable learning."}, "weaknesses": {"value": "- The method's success heavily relies on having a \"well-specified and representative pairwise preference function, which can be unavailable. \n- The experiments suggest the method can be sensitive to biases in the preference judge (in this case, an \"LLM-as-a-judge\"). The authors attribute the gap between standard and length-controlled win rates to the judge model's \"length bias,\" which the SLHF model may have learned to exploit."}, "questions": {"value": "- In the practical implementation, the Leader and Follower share parameters. Could this limit the follower's ability?\n- In Appendix D.3, this paper mentions that \"increasing κ leads to a gradual decline in the Leader’s performance. While\nthe Follower benefits from increasing κ from 1 to 5\". How to balance the performance of the leader and the follower? Which is more important in practice? \n- Regarding \"refining outputs from other models\", Does it imply the Follower learns a universal refinement rather than just a policy specific to its own Leader?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MGU1b0ojDo", "forum": "vc9Tj11LNE", "replyto": "vc9Tj11LNE", "signatures": ["ICLR.cc/2026/Conference/Submission13937/Reviewer_U2fY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13937/Reviewer_U2fY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989373273, "cdate": 1761989373273, "tmdate": 1762924440089, "mdate": 1762924440089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a stackleberg game formulation of the RLHF problem, in contrast to prior works which either use a bradley-terry model or search for a nash equilibrium.  The authors demonstrate that their stackleberg formulation is able to resolve common problems with BT-based models, i.e. cyclical preferences, while also allowing for further test-time adaptation using the follower model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is generally easy to follow and can be understood. The overview of differetn approaches is nice too!\n* The section demonstrating the different types of preference relationships that different models can address is very nice and useful! \n* The approach is well justified theoretically. \n* The experimental evaluation considers both preference dataset evaluation and general finetuning\n* to my knowledge, using a stackleberg game for learning from feedback is novel."}, "weaknesses": {"value": "* The experiment section lacks any ablations on the choices made. For example, how does the two-timescale schedule affect performance?\n* The method seems like it will be computationally more expensive. \n* I am not sure why the stackleberg formulation makes sense. I can see how the nash formulation can resolve ambiguities in preferences vs BT, but realistically when would I want to have a leader and follower? Using the follower will double inference costs. \n* the gains of the leader vs the nash models seem marginal at best. This seems to indicate that a lot of the performance gains might be coming from just using more compute / tokens for a response i.e. adding context.\n* The length bias seems really strong in the Alpaca results.\n\n\nNit:\n* In eq 5, the order might be more intuitive if the leader is the inner optimization and the then follower moves after? the notation for eq is also not super well defined -- and it would be nicer if the symbols for the leader and follower were more clearly introduced."}, "questions": {"value": "* How is the reference for the follower defined? Does this reference model even make sense if the prior model hasn't been trained as a follower?\n* For a lot of LLM methods, compute matters. Could the authors comment on any difference in compute requirement vs nash vs BT model + PPO? What do results look like at compute parity?\n* Baselines: could the authors comment on why a method like SPO was not relevant / considered as abaseline? It is also based on nash equilibrium?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5SoTeSM28W", "forum": "vc9Tj11LNE", "replyto": "vc9Tj11LNE", "signatures": ["ICLR.cc/2026/Conference/Submission13937/Reviewer_oDxv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13937/Reviewer_oDxv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13937/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065674902, "cdate": 1762065674902, "tmdate": 1762924439363, "mdate": 1762924439363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}