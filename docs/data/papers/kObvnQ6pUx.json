{"id": "kObvnQ6pUx", "number": 12005, "cdate": 1758205148943, "mdate": 1759897540016, "content": {"title": "Gradient Intrinsic Dimensionality Alignment：Narrowing The Gap Between Low-Rank Adaptation and Full Fine-Tuning", "abstract": "Parameter-Efficient Fine-Tuning (PEFT) techniques, such as Low-Rank Adaptation (LoRA) and its variants, have emerged as critical tools for adapting large pretrained models under limited computational resources. However, a notable performance gap persists between these LoRA methods and Full Fine-Tuning (FFT). \nIn this paper, we investigate a key yet overlooked cause of this gap: the relationship between LoRA's low-rank adaptation subspace and true effective update directions of FFT gradients, which we define as the **gradient intrinsic dimensionality**.\nTo systematically quantify this dimension, we first propose a novel entropy-based estimator, uncovering substantial discrepancies (up to more than 100x) between the rank of LoRA and the gradient intrinsic dimensionality. Motivated by this finding, we introduce **RaLoRA**, which adaptively aligns the ranks of LoRA adapters with layer-specific gradient intrinsic dimensions, without increasing the number of overall parameters. We further extend this approach into **RaLoRA-Pro**, integrating intra-layer rank alignment and inter-layer parameter reallocation guided by loss sensitivity, enabling finer-grained capacity relocation under comparable parameters. Extensive experiments demonstrate the effectiveness of our methods. Specifically, compared to vanilla LoRA, our methods achieve more than +5\\% improvement on GLUE, +0.57 on MT-Bench, +5.23\\% on GSM8K, +5.69\\% on HumanEval, and +1.58\\% on image classification, confirming consistent and substantial performance gains across diverse tasks and modalities.", "tldr": "", "keywords": ["PEFT", "LoRA", "Gradient Intrinsic Dimension", "Adaptive Alignment"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6781a131975b9658b345cc3b9e700f5a285fffa3.pdf", "supplementary_material": "/attachment/9eaba14a09d5cc314d1b86cccd8ad6dd0e94f2c1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a novel and compelling concept: the mismatch between the fixed, low-rank subspace of LoRA and the high intrinsic dimensionality (GID) of full fine-tuning gradients is a fundamental limitation of existing LoRA variants. To address this, the authors propose an entropy-based estimator for Gradient Intrinsic Dimensionality (GID) and build two new methods upon it: RaLoRA, which performs intra-layer rank alignment, and RaLoRA-Pro, which adds inter-layer parameter reallocation. The paper is well-structured, the motivation is clear and well-argued, and the experimental evaluation across NLP and vision tasks is extensive and generally demonstrates the superiority of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Novel and Well-Motivated Core Idea: The paper's central thesis—that aligning LoRA's structure with the intrinsic dimensionality of the full fine-tuning gradient is key to closing the performance gap—is both novel and persuasive. The analysis in Section 5.1 and Appendix E effectively characterizes this GID, providing strong empirical evidence for the problem statement.\n\nStrong Empirical Results: The experimental section is a major strength. The authors comprehensively evaluate their methods on a diverse set of tasks (NLU, NLG, Vision) and against a wide array of strong, recent baselines. The results consistently show that RaLoRA and RaLoRA-Pro outperform other LoRA variants and significantly narrow the gap with full fine-tuning, sometimes even surpassing it. The ablation studies (Section 5.2, 5.3, 5.4) are particularly valuable for disentangling the contributions of the different components.\n\nTheoretical Grounding: The paper provides a solid theoretical foundation. The derivation of LoRA as an implicit gradient compressor (Appendix A) is elegant and directly motivates the GID concept. The analysis of the approximation error for RaLoRA's block-diagonal structure (Appendix C) offers a principled explanation for why its architectural change can be more expressive than standard LoRA under a fixed parameter budget."}, "weaknesses": {"value": "**1. Computational Overhead of GID Estimation:** A significant practical concern is the computational cost of estimating GID. The method requires computing the SVD of the full gradient matrix for each target layer over several mini-batches (Algorithm 1). For very large models (e.g., LLaMA 70B), the cost of these SVD operations, even if done only once during a warm-up phase, could be non-trivial. The paper would be strengthened by a discussion or analysis of this overhead, perhaps quantifying the extra time/memory cost compared to standard LoRA initialization.\n\n**2. Clarity on the \"One-Shot\" vs. \"Dynamic\" Nature of GID:** The methodology section and Algorithm 1 suggest that the GID estimation and parameter reallocation are performed as a one-time initialization step using gradients accumulated from a few mini-batches. However, the discussion of GID dynamics in Section 5.1 shows that GID evolves during training. This raises questions:\n\n   - Why is a one-time estimation sufficient? Could performance be further improved by periodically re-estimating GID and adapting the structure during training?\n\n   - How sensitive are the results to the specific mini-batches used for the initial GID estimation?\nA discussion of these points and perhaps an experiment on the stability of the initial GID estimate would alleviate these concerns.\n\n**3. Hyperparameter Sensitivity and Tuning Cost:** While the proposed methods are shown to be effective, they introduce new hyperparameters: the maximum expansion factor n_max and the rank reallocation range (r_min, r_max). The results in Tables 5 and 6 show that performance is sensitive to these choices, and the authors mention using a grid search. This somewhat offsets the \"parameter-efficient\" and \"easy-to-use\" appeal of standard LoRA. A more in-depth discussion on guidelines for setting these hyperparameters or strategies to reduce the tuning cost would be very helpful for practitioners.\n\n**4. Comparison to Other High-Rank PEFT Methods:** The baselines are comprehensive within the LoRA family. However, it would be valuable to see a comparison with other PEFT methods that also aim for higher rank/more expressive updates but through different mechanisms, such as (IA)³ or perhaps a memory-efficient implementation of a method like DiffPruning. This would better situate the contribution within the broader PEFT landscape."}, "questions": {"value": "1. What is the computational overhead (in time and memory) of the GID estimation phase compared to the standard LoRA initialization? Is this cost negligible for very large models (e.g., >100B parameters)?\n\n2. The GID is estimated once at initialization. Given that Figure 3(c) shows GID evolves, have you experimented with dynamic rank adaptation during training? What were the results and challenges?\n\n3. It is better if the authors can discuss some recent works:\n\n   - RandLoRA: Full-rank parameter-efficient fine-tuning of large models\n   - Mlae: Masked lora experts for parameter-efficient fine-tuning\n   - Latent Space Factorization in LoRA\n\n4. How does the performance and efficiency of your method compare to simply using a standard LoRA with a much higher rank (e.g., r=64 or 128) to cover the high GID, even if it means a higher parameter count? The results in Appendix E.3 (Table 8) are a start, but a more systematic comparison on a equal-parameter or equal-performance basis would be insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gF3cSwxZUp", "forum": "kObvnQ6pUx", "replyto": "kObvnQ6pUx", "signatures": ["ICLR.cc/2026/Conference/Submission12005/Reviewer_4tTg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12005/Reviewer_4tTg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761793167121, "cdate": 1761793167121, "tmdate": 1762922995847, "mdate": 1762922995847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RaLoRA and RaLoRA-Pro, methods for adapting the rank of LoRA adapters by estimating a new quantity called Gradient Intrinsic Dimensionality (GID) using an entropy-based “effective rank” estimator. The idea is that LoRA’s fixed low-rank subspace can misalign with the true gradient subspace of full fine-tuning; thus, estimating GID per layer and adjusting ranks could better use the parameter budget. The paper reports strong improvements on GLUE, MT-Bench, GSM8K, HumanEval, and image classification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem motivation, mismatch between LoRA rank and true gradient structure, is interesting. \nThe core insight of this block-diagonal design hinges on the property that $\\text{Rank}(BA)= \\sum_{i=1}^{n_l} rank(B_iA_i)$. By employing block diagonalization, the authors aim to strategically augment the overall $\\text{Rank}(BA)$ to align with the estimated Gradient Intrinsic Dimension (GID).\n\n2. The proposed entropy-based GID estimator is conceptually clean and grounded in established ideas.\n\n3. Writing is mostly clear and well-organized."}, "weaknesses": {"value": "1. The paper estimates Gradient Intrinsic Dimensionality using spectral entropy, which typically requires SVD and significant computation. Is SVD performed for every layer at each step, or precomputed and frozen? What is the time complexity compared to other estimation methods? It would be helpful to show robustness under different batches to assess the stability of the gradient SVD.\n\n2. The design of the block diagonal matrix may restrict gradient flow and information sharing.  If the goal is merely to increase $\\text{Rank}(BA)$, other approaches such as introducing a Hadamard product (as in HiRA [1], Eq. (4)) could also achieve a higher rank. A comparative discussion would further elucidate the distinctions between these approaches.\n\n3. The visualization of importance estimation in RaLoRA-Pro is unclear. How does it differ from prior works that allocate LoRA ranks based on importance or sensitivity?\n\n4. The paper adopts a dynamic rank allocation strategy. Has it been fairly compared with other methods, for example, in terms of parameter count or efficiency?\n\n5. Typos: \n     1) Chapter “Disscusion” → “Discussion” \n     2) Inconsistent capitalization of “GSM8K” (e.g., “GSM8k” in Table 2). Please standardize. \n     3) All equation references should use “Equation XX” instead of “equation XX”.\n\n[1] HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models"}, "questions": {"value": "Q1: The paper estimates Gradient Intrinsic Dimensionality (GID) using spectral entropy, which requires SVD decomposition.\nIs SVD computed for every layer at each update step, or is it precomputed and then frozen?\nWhat is the time complexity compared with other baseline methods?\nQ2: The current block-diagonal design may restrict gradient flow and information sharing. Could the authors discuss or compare this design with HiRA, or provide further justification for choosing this particular formulation?\nQ3: The visualization of importance estimation in RaLoRA-Pro is unclear. How does it differ from prior works that allocate LoRA ranks based on importance or sensitivity?\nQ4: The paper proposes a dynamic rank allocation method. Has it been fairly compared with other rank adaptation approaches? Could the authors analyze the comparison in terms of parameter count or efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "D8ffZgpasE", "forum": "kObvnQ6pUx", "replyto": "kObvnQ6pUx", "signatures": ["ICLR.cc/2026/Conference/Submission12005/Reviewer_rn9v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12005/Reviewer_rn9v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881509743, "cdate": 1761881509743, "tmdate": 1762922995336, "mdate": 1762922995336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces **Gradient Intrinsic Dimensionality (GID)** to explain the performance gap between low-rank adaptation (LoRA) and full fine-tuning (FFT). The authors argue that LoRA’s fixed low-rank subspace fails to align with the true gradient subspace of FFT. To address this, they propose **RaLoRA**, which estimates each layer’s effective gradient rank via an entropy-based metric and dynamically allocates LoRA capacity accordingly. A further variant, **RaLoRA-Pro**, performs inter-layer reallocation guided by loss sensitivity. Experiments on diverse NLP, code, and vision tasks show consistent and sometimes significant gains over existing PEFT methods without increasing parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear and meaningful motivation.**\nThe paper addresses a known limitation of LoRA—its rigid and sometimes misaligned update subspace—through a theoretically motivated concept (GID). This adds a fresh geometric interpretation to the efficiency–performance trade-off in PEFT.\n\n2. **Methodologically elegant design.**\nThe entropy-based estimator and adaptive rank allocation are conceptually simple yet principled. The proposed dual-level alignment (intra- and inter-layer) is coherent and implementable without modifying the base model.\n\n3. **Comprehensive experimental evaluation.**\nResults span multiple domains and include comparisons to strong baselines such as DoRA, LoRA+, PiSSA, and MELoRA. The reported gains are consistent and statistically significant, and the ablation analyses are generally convincing.  The ablation studies and visualizations (e.g., GID heatmaps) support the empirical narrative.\n\n4. **Readable and reproducible.**\nThe algorithm is clearly described, and the proposed method could be implemented with minimal modification to existing LoRA frameworks. The method can potentially generalize to other PEFT frameworks and adaptive subspace modeling approaches, making it of interest to the broader community."}, "weaknesses": {"value": "**1. Incremental novelty.**  \nAlthough the paper introduces Gradient Intrinsic Dimensionality (GID) and rank alignment, the overall idea aligns closely with recent adaptive-PEFT variants such as GoRA, LoRA+, and DoRA. The contribution mainly reframes known intuitions in a new terminology without offering a fundamentally new optimization mechanism or theoretical insight.\n\n**2. Lack of theoretical rigor.**  \nWhile GID is intuitively appealing, the paper lacks formal analysis linking GID alignment to optimization dynamics or generalization improvement. No convergence guarantees, projection error bounds, or theoretical explanations are provided for why matching ranks to GID improves performance.\n\n\n**3. Computational overhead and scalability.**  \nThe GID estimator requires repeated singular-value decompositions (SVDs) on layerwise gradients, which can become expensive at scale. The paper omits concrete measurements of wall-clock overhead, GPU memory usage, or FLOP ratios compared with vanilla LoRA. It remains unclear whether RaLoRA scales to 70B-parameter LLMs or multi-GPU fine-tuning setups.\n\n**4. Interpretability and analytical depth.**  \nThe visualization of GID heatmaps provides little insight into model behavior. The work does not analyze whether GID correlates with layer function, curvature, or Fisher information. A deeper discussion of why certain layers have high or low GID would strengthen the scientific contribution."}, "questions": {"value": "1. How stable is the entropy-based GID estimator across mini-batch sampling, different random seeds, or noisy gradients?\n\n\n2. What is the computational overhead (in wall-clock time, GPU memory, or FLOPs) relative to standard LoRA?\n\n\n3. Does GID correlate with curvature or layer sensitivity measured via Fisher Information or Hessian spectra?\n\n\n4. Could the observed improvements of RaLoRA be attributed to implicit regularization effects rather than true gradient alignment?\n\n\n5. Have you tested dynamic rank scheduling during training instead of static pre-assignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QjkxGzeUK2", "forum": "kObvnQ6pUx", "replyto": "kObvnQ6pUx", "signatures": ["ICLR.cc/2026/Conference/Submission12005/Reviewer_keQx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12005/Reviewer_keQx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935460191, "cdate": 1761935460191, "tmdate": 1762922994614, "mdate": 1762922994614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper use the effective rank of the gradient matrix in full-parameters fine-tuning as the Gradient Intrinsic Dimensionality (GID) and based on it adaptively allocates the number of mini-LoRA to increase the overall rank without increasing the total parameter count.  Additionally, the paper integrates a loss-sensitivity-guided inter-layer reallocation strategy to dynamically redistribute trainable parameters across layers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is intuitive and effective, combining several existing techniques into a cohesive and well-motivated design. Extensive experiments across NLP and CV tasks validate its robustness and superior performance. \n\n2. The visualization of gradient intrinsic dimensionality is particularly insightful, revealing similar GID distributions across different tasks."}, "weaknesses": {"value": "1. The paper lacks efficiency comparisons, such as peak GPU memory usage and FLOPs, which is essential for a new PEFT method.\n\n2. Since the proposed methods is similar to the MELoRA with adaptive rank/number of mimi-LoRA for each layer, it would be helpful to include more MELoRA results with different configurations (e.g., *n* = 8, 16, 32) for a fairer comparison."}, "questions": {"value": "In Figure 2 (Part II, Step 3: *Post-Double Alignment*), why the output dimension of ( A_1 ) in layer N is defined as ( r_1 / n_1 )?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QRn528yH9F", "forum": "kObvnQ6pUx", "replyto": "kObvnQ6pUx", "signatures": ["ICLR.cc/2026/Conference/Submission12005/Reviewer_E7b3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12005/Reviewer_E7b3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12005/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972580093, "cdate": 1761972580093, "tmdate": 1762922994284, "mdate": 1762922994284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}