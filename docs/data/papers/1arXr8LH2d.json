{"id": "1arXr8LH2d", "number": 22546, "cdate": 1758332675337, "mdate": 1759896860437, "content": {"title": "Adaptive Text Transformations Defend Against Dataset Inference Attacks", "abstract": "There has been increasing legal interest in identifying possible copyright violations committed by Large Language Model (LLM) trainers. Many works developing individual membership inference attacks have recently been shown to be weaker than previously thought, due to implicit distributional shifts. To combat this, progress has been made by considering large datasets and aggregating multiple different membership attacks. A hidden assumption in these methods is that if the LLM has improperly used a dataset, the LLM was trained on that exact dataset. By challenging this assumption, we demonstrate, to our knowledge, the first failure of any large-scale dataset inference (DI) attack. In particular, we study LLM fine-tuning for both short and long text datasets. We adaptively transform the datasets before fine-tuning, enabling an increase in model performance while avoiding dataset inference. In the case of long texts, we find that text summarization followed by rephrasing substantially reduces the success probability of DI in our setting from over 95\\% to less than 5\\%. We also develop a new theoretical formulation of dataset inference specifically tailored to LLMs, which explains the effectiveness of our method and sheds light on how parameters, such as the number of training epochs, can affect dataset inference.", "tldr": "", "keywords": ["Membership Inference", "Dataset Inference", "LLM Defenses"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cda335485cfc70eb4444bcc979e7e71d989c35a1.pdf", "supplementary_material": "/attachment/25213605ae2e346ebc8475a9f38e043831afffaf.pdf"}, "replies": [{"content": {"summary": {"value": "This paper proposes methods to evade dataset inference (DI) attacks against large language models by transforming training data before fine-tuning. The authors develop two approaches: synonym substitution for short text and summarisation followed by rephrasing for longer text. They claim these transformations preserve model performance while reducing DI attack success rates from over 95% to less than 5%. The paper also presents a theoretical framework to explain DI attack dynamics and justify their approach. Experiments are conducted on GPT-2 and Pythia models using the Pile datasets."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper tackles a timely problem given ongoing copyright litigation involving major AI companies. The experimental design is reasonably comprehensive within its scope, testing across multiple datasets and model sizes. The authors identify that existing DI methods assume training occurs on exact dataset copies, which may not reflect real-world scenarios. The algorithms themselves are clearly presented and easy to follow."}, "weaknesses": {"value": "1. *Questioning the fundamental premise.* The authors frame dataset transformation as \"evasion\", but this may be mischaracterising the problem. If transformations are substantial enough to fool dataset inference attacks, they may constitute fair use or legitimate transformation that prevents memorisation. Detection failure would then be the appropriate outcome, not a vulnerability. The paper never grapples with this tension—when does their \"evasion\" actually represent responsible, legal use of copyrighted content?\n\n2. *Flaws in theory.* What's presented as theory appears to be post-hoc rationalisation. There are basic errors throughout: $\\mu$ is defined as \"a latent space\" but then used as a vector in $\\mathbb{R}^N$. Key assumptions like isotropic Gaussian distributions in latent space are never justified. Parameters like $\\epsilon$, $\\sigma_\\mathrm{obs}$, and $N$ have no clear connection to empirical quantities, yet specific numerical values are plugged in without explanation (how does one get from \"argument changes from 2 to 1\" to probabilities like 0.977 to 0.841?). The exercise feels like mathematical storytelling rather than genuine theoretical insight.\n\n3. *Questionable statistical methods.* The paper claims to compute \"p-values\" using Equation (3) without defining a null hypothesis or making a connection to proper statistical testing. This quantity appears to be a test statistic/score, not a p-value.\n\n4. *Presentation issues.* The theoretical section puts goals before problem formulation, creating confusion. Mathematical notation is inconsistent throughout (e.g., switching between \"pub\"/\"public\" subscripts). Figure 4 is difficult to interpret. Key symbols remain undefined (what is $m$, $\\sigma$, $\\Phi$ in Corollary 1?).\n\n5. *Missing reference.* The paper does not cite Zhang et al. (2025), who challenge whether membership/dataset inference attacks can provide statistically sound evidence for legal proceedings. This is unfortunate, given the paper's focus on copyright applications.\n\n6. *Limited evaluation.* Testing against a single attack method raises robustness concerns. What happens when attackers adapt to account for text transformations? This possibility is not discussed.\n\nMinor issues:\n1. The IID assumption for public/private splits is unconvincing (if they're truly identical, why steal the private data?)\n2. The learning rate formula $\\alpha = 1.5/\\sqrt{θ}$ appears ad-hoc and various claims about dimensionality reduction factors are presented as facts without evidence.\n3. True/false positives are not defined in the DI context.\n\n# References\nJ. Zhang, D. Das, G. Kamath and F. Tramer, \"Position: Membership Inference Attacks Cannot Prove That a Model was Trained on Your Data,\" in 2025 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)."}, "questions": {"value": "1. How do you distinguish between legitimate transformation and problematic evasion? If your methods prevent memorisation while preserving utility, isn't detection failure actually desirable?\n\n2. Can you justify the isotropic Gaussian assumption? How do the specific numerical examples in Section 4 connect to your theoretical parameters?\n\n3. What's the null hypothesis underlying Equation (3)? This doesn't appear to be a proper p-value.\n\n4. How would your approach fare against adaptive attacks designed to handle text transformations?\n\n5. How does your work relate to the SatML paper questioning whether MIAs/DIAs can provide legally sound evidence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aS9WNnSeG0", "forum": "1arXr8LH2d", "replyto": "1arXr8LH2d", "signatures": ["ICLR.cc/2026/Conference/Submission22546/Reviewer_QAsm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22546/Reviewer_QAsm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657973237, "cdate": 1761657973237, "tmdate": 1762942270966, "mdate": 1762942270966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes adaptive text transformations—semantic-preserving modifications such as synonym substitution for short texts and summarization plus rephrasing for long texts—as a defense against dataset inference attacks, which aim to determine whether a large language model was trained on specific copyrighted data. By fine-tuning models like GPT-2 and Pythia on these transformed datasets, the authors show that DI success rates drop dramatically while model performance remains largely intact. They also introduce a theoretical framework modeling DI success probability as a function of dataset size, model parameters, and noise, revealing a “tipping point” where larger or noisier data make inference harder."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and easy to follow.\n2. This is a timely and important topic.\n3. Demonstrates strong empirical results, reducing DI success from over 95% to under 5%."}, "weaknesses": {"value": "1. Experiments are limited to fine-tuning scenarios rather than full-scale pretraining, meaning the proposed defense is tested only on relatively small datasets and moderate-sized models such as GPT-2 and Pythia variants. As a result, it remains unclear whether the same degree of DI evasion and performance retention would hold when scaling up to foundation models trained on trillions of tokens or when integrating the transformations into the entire pretraining pipeline. Large-scale pretraining involves different optimization dynamics, data diversity, and parameter interactions, which could alter the balance between semantic preservation and effective noise injection. Without evidence at that scale, the scalability, efficiency, and generalizability of the proposed defense to real-world industrial settings remain uncertain.\n\n2. The approach relies on computationally intensive large language model rewriting, as each document in the dataset must be summarized and rephrased using powerful models like Mistral-7B. This process incurs significant compute, memory, and inference costs, especially when applied to large-scale corpora containing millions or billions of documents."}, "questions": {"value": "1. How feasible is it to apply your adaptive text transformation pipeline to web-scale pretraining corpora, where data volumes reach trillions of tokens? Could lightweight or rule-based approximations achieve similar defense strength?\n\n2. How do you ensure that summarization and rephrasing preserve the essential meaning and label consistency of training data, especially for tasks requiring fine-grained linguistic or factual precision?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9XBJhErXQb", "forum": "1arXr8LH2d", "replyto": "1arXr8LH2d", "signatures": ["ICLR.cc/2026/Conference/Submission22546/Reviewer_7MVM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22546/Reviewer_7MVM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702320048, "cdate": 1761702320048, "tmdate": 1762942270758, "mdate": 1762942270758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates defenses against dataset inference attacks, which attempt to determine whether a large language model (LLM) has been trained on a specific dataset. The authors challenge the standard assumption that an LLM, if guilty, is trained on the exact dataset in question. They propose adaptive text transformations to obscure training data while maintaining model utility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a new theoretical framework that explicitly relates DI success probability to latent-space variance, model size, and dataset properties, offering some interpretability to previously empirical findings.\n- The adaptive transformation approach (summarization and synonym replacement) is conceptually straightforward and can be implemented using off-the-shelf LLMs such as Mistral-7B."}, "weaknesses": {"value": "- The experiments lack statistical significance testing and robust metrics beyond “evaded/not evaded.” The Maini et al. (2024) DI attack is the only method tested; no comparison to newer or more adaptive DI variants is provided. Results rely on a single run per model per dataset, with limited reporting of variance or error bars.\n- The paper assumes access to the full dataset before training and the ability to summarize/rephrase it—conditions that may not hold for real-world LLM pretraining.\n- The derivations are abstract Gaussian latent-space approximations with many unverified assumptions (e.g., isotropic distributions, linear dependence of ϵ on training parameters).\n- Critical implementation details are missing: how “embedding similarity thresholds” are chosen, how summarization outputs are verified, what DI thresholds correspond to success/failure. The figures and tables are poorly formatted; e.g., Table 1 and Fig. 3 occupy nearly a full page with large blank spaces, reducing readability."}, "questions": {"value": "- How does the defense perform against distributionally robust or semantic-aware DI methods that use paraphrase-invariant embeddings rather than token-level similarity?\n- How are summarization and rephrasing quality controlled, does meaning drift affect benchmark performance or create biases?\n- Can the theoretical model be empirically verified (e.g., does DI success indeed follow the √m dependence predicted)?\n- How sensitive are results to the choice of summarization model (Mistral-7B-Instruct)? Would smaller or instruction-tuned models yield similar effects?\n- What is the trade-off between defense strength and training utility—are there tasks where summarization/rephrasing degrades fine-tuning benefits more severely?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PaeyMyjo9t", "forum": "1arXr8LH2d", "replyto": "1arXr8LH2d", "signatures": ["ICLR.cc/2026/Conference/Submission22546/Reviewer_fbzV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22546/Reviewer_fbzV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926570610, "cdate": 1761926570610, "tmdate": 1762942270282, "mdate": 1762942270282, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that dataset-inference (DI) methods—used to tell whether an LLM was trained on a specific corpus—are fragile when trainers apply meaning-preserving transformations before fine-tuning. It proposes an adaptive pipeline (synonym-based paraphrasing for short texts; summarize-then-rephrase for long texts) that minimally perturbs data until DI tests are statistically non-significant, and develops a latent-space theory explaining how added noise and reduced effective dimensionality undermine DI."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This work explores an important problem in LLM, i.e., dataset inference (or membership inference). \n2. This work provides a theoretical understanding of the proposed method."}, "weaknesses": {"value": "1. **Scenario–evidence gap.** The paper motivates with copyright-risk scenarios but never evaluates on truly copyrighted or restricted corpora, so external validity to the stated risk setting remains unproven.\n2. **Forgetting vs. distribution shift.** The approach relies on transformed-data fine-tuning that shifts the representation distribution; without explicit retention controls, this can induce catastrophic forgetting of unrelated capabilities and background knowledge.\n3. **No selective retention/erasure.** The method treats all examples uniformly. In practice, only sensitive content (copyrighted text, PII-bearing records, or data implicated by membership inference) should be targeted for removal or obfuscation; blanket transformation risks needless utility loss.\n4. **Limited novelty.** The core defense—summarize/paraphrase then fine-tune—is methodologically straightforward and overlaps with a large body of prior fine-tuning and data-augmentation work; the incremental contribution rests mostly on the evaluation framing.\n5. **Synthetic-data IP risk and teacher dependence.** When using an LLM to summarize/rephrase, compliance with intellectual-property constraints is not guaranteed. The method implicitly assumes access to a strong external model; if that teacher carries copyrighted material in its own training history, “no verbatim copying” is not a sufficient safeguard against derivative-work concerns.\n6. **Thresholding and stats choices are under-motivated.** The reliance on a fixed p-value cutoff (e.g., 0.1) and a specific embedding-similarity threshold lacks sensitivity analyses; results may be brittle to these knobs."}, "questions": {"value": "See **weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nE43Amnxks", "forum": "1arXr8LH2d", "replyto": "1arXr8LH2d", "signatures": ["ICLR.cc/2026/Conference/Submission22546/Reviewer_KS4J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22546/Reviewer_KS4J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762457134370, "cdate": 1762457134370, "tmdate": 1762942269940, "mdate": 1762942269940, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper challenges the assumption in prior work that, in the context of LLM dataset inference, the model is trained on the exact same copyrighted dataset without any modifications. It proposes two methods to attack dataset inference techniques: for short texts, substituting words with synonyms, and for long texts, summarizing and rephrasing them."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The topic of the paper is important for the community.\n\n- The idea of studying the effect of text transformations on dataset inference methods is interesting.\n\n- The paper includes a theoretical result that helps justify findings from previous work."}, "weaknesses": {"value": "- The authors claim that their attacks lead to improved model performance while avoiding dataset inference. However, it's unclear where this performance improvement is actually demonstrated in the paper. For example, in Figure 4 there is no comparison between the proposed methods and a baseline model trained on the original, unmodified dataset. Intuitively, model performance should not increase when trained on modified data with word substitutions or summaries.\n\n- There is relevant prior work on reducing memorization during training using the Goldfish Loss [1]. It's not clear whether the proposed methods outperform such prior defenses. It would be important to understand whether there is a better tradeoff between task performance and evasion performance for the proposed methods, especially since training on transformed data could be more time-consuming (because of the overhead of transforming the text) than using something like Goldfish Loss. A comparison of computational complexity would also strengthen the paper.\n\n[1] https://arxiv.org/pdf/2406.10209"}, "questions": {"value": "What does \"False Positive\" refer to in Figure 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "rhAYQdJYKB", "forum": "1arXr8LH2d", "replyto": "1arXr8LH2d", "signatures": ["ICLR.cc/2026/Conference/Submission22546/Reviewer_dQnK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22546/Reviewer_dQnK"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762890928497, "cdate": 1762890928497, "tmdate": 1762942269421, "mdate": 1762942269421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}