{"id": "Rip3EJc4nx", "number": 4434, "cdate": 1757680172183, "mdate": 1759898032569, "content": {"title": "High-Fidelity Pruning for Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks, yet their significant computational and memory requirements present major challenges for deployment. \nA common approach uses Taylor expansion on the loss function to estimate neuron importance. \nHowever, its reliance on one-hot cross entropy loss, a key limitation is that it narrowly assesses importance based only on the probability assigned to the single predicted next token, thereby ignoring the other potential predictions of the original model. \nAn intuitive solution to address this is to employ self distillation criterion for importance evaluation. \nHowever, this approach introduces significant computational overhead by requiring a separate teacher model for supervision. \nTo this end, we propose a simple but effective criterion, information entropy of the model's output distribution, to efficiently evaluate importance scores of neurons with Taylor pruning without requirement of additional teacher. \nCompared to plain cross entropy criterion, it provides a more holistic criterion for Taylor pruning to prune neurons with the least impact on the prediction of model in a global manner, thereby preserving the fidelity of the model's predictive capabilities. \nExperimental results on extensive zero-shot benchmarks demonstrate that our method consistently outperforms existing pruning methods across the LLaMA and Qwen series models. The source code and trained weights will be publicly available.", "tldr": "", "keywords": ["Prune", "LLM"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/326af0c400051142ac41bdacff7e9e91729a8e98.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a pruning criterion for large language models based on the information entropy of the model’s output distribution. Instead of using one-hot cross-entropy (which focuses on the single next token) or relying on a separate teacher for self-distillation, the method computes per-layer entropy-based importance scores within a Taylor-style framework and prunes parameters to better preserve the model’s global output distribution. The approach is label-free, aims to maintain fidelity after pruning, and is evaluated on zero-shot benchmarks for LLaMA and Qwen-family models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose a relatively simple yet reasonable approach for pruning large language models (LLMs), which uses the entropy of output distributions as an indicator of neuron importance, instead of relying solely on next-token cross-entropy. The method does not depend on additional teacher models or complex distillation procedures, making it practically appealing for real-world pruning applications. The experiments cover multiple model families (e.g., LLaMA, Qwen) and several zero-shot benchmarks, showing that the proposed approach consistently outperforms existing pruning baselines. This provides some evidence of general applicability and reliability."}, "weaknesses": {"value": "1. The dataset coverage is narrow and the evaluation tasks are of low difficulty, relying on a small set of relatively simple zero-shot benchmarks. This makes it difficult to assess robustness in areas such as instruction following, multi-step reasoning, long-context understanding, or multilingual and multi-domain settings.\n2. There may be a mismatch between the benchmarks and the pruned model components: if the tasks do not sufficiently engage the pruned submodules, the reported “distribution fidelity” may be overstated and external validity remains uncertain.\n3. Baseline and tuning transparency is limited, with missing comparisons to stronger or more recent pruning baselines, as well as a lack of systematic hyperparameter exploration under fair alignment conditions (e.g., matched training steps and learning-rate sweeps).\n4. The evaluation metrics are limited, focusing heavily on perplexity or simple zero-shot accuracy, without including direct generation results."}, "questions": {"value": "1. The evaluation datasets are insufficient — they’re all too simple. The pruned parameters might not cover the activation patterns of these models, so more diverse datasets should be added.\n2. In addition, the model sizes don’t seem sufficient either, for example models like Qwen3-8B.\n3. For reasoning models, does the entropy criterion remain reliable?\n4. The main contribution is the use of entropy for pruning, which feels rather limited."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nQImfR7KXS", "forum": "Rip3EJc4nx", "replyto": "Rip3EJc4nx", "signatures": ["ICLR.cc/2026/Conference/Submission4434/Reviewer_zpe3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4434/Reviewer_zpe3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761203409451, "cdate": 1761203409451, "tmdate": 1762917362692, "mdate": 1762917362692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces HFPrune (High-Fidelity Pruning), a structured pruning method for LLMs that aims to preserve model fidelity while reducing computational and memory costs, replacing the conventional loss-based Taylor pruning criterion with an information entropy–based criterion that measures the global prediction distribution of the model, instead of focusing only on the ground-truth token."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a label-free, holistic signal for neuron importance estimation. \n- HFPrune consistently outperforms strong baselines: LLM-Pruner, LoRAPrune, SDMPrune, on LLaMA and Qwen families.\n- Comprehensive ablation studies validate the entropy criterion’s role in preserving output distributions. \n- The algorithmic description is clear and reproducible. Implementation details are systematically reported."}, "weaknesses": {"value": "- The pruning ratio $\\rho{mlp}$  is fixed across all MLP layers, despite entropy potentially varying per layer, this could limit the functionality of HFPrune.\n- Lack a comparative discussion or empirical correlation analysis between entropy-based and Fisher-based importance scores.\n- Training-time FLOPs for fine-tuning (post-pruning recovery) are omitted."}, "questions": {"value": "- Can a per-layer entropy-based adaptive cutting ratio improve the trade-off between fidelity and compression?\n- How does the entropy gradient behave in the low entropy versus high entropy regions of the output distribution?\n- What are the effects of using entropy calculated from logits vs. softmax probabilities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "v9E8pRE0Cn", "forum": "Rip3EJc4nx", "replyto": "Rip3EJc4nx", "signatures": ["ICLR.cc/2026/Conference/Submission4434/Reviewer_oKVY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4434/Reviewer_oKVY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761263595106, "cdate": 1761263595106, "tmdate": 1762917362386, "mdate": 1762917362386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Strong Concern Regarding the Quality and Constructiveness of Reviews"}, "comment": {"value": "Dear ACs, SACs and PCs,\n\nWhile I fully respect the peer review process and welcome constructive criticism that strengthens our work, I find it necessary to point out that these reviews fail to meet the standards expected of a top-tier conference like ICLR.\n\n#### **1. On the Lack of Constructive Criticism** \n\nThe \"weaknesses\" cited in the review are predominantly trivial and tangential to the core contributions of this paper. They **focus on marginal details while neglecting the substantive technical novelty and experimental validity of the work**. These comments do **not identify any fundamental flaws in our logic, methodology, or results**. Instead, they appear to be **a collection of \"weaknesses\" manufactured solely to justify a rejection rating**. Such feedback is neither actionable nor constructive; it does not help us improve the manuscript, nor does it provide the Area Chair with a valid basis for decision-making.\n\n#### **2. A Broader Concern for the Machine Learning Community** \n\nAlthough the acceptance of this specific paper is ultimately a single decision, I am compelled to highlight a troubling trend reflected in this review: **the practice of rejecting papers based on specious or superficial reasons**. There is a growing number of reviews that seem designed to act as \"gatekeepers\" rather than \"peers.\" When reviewers search for reasons to reject—**magnifying insignificant flaws while ignoring significant contributions**—it does not uphold the quality of the conference. Instead, it stifles innovation and demoralizes researchers who have invested significant effort into their work. The goal of peer review should be to foster the healthy development of the field, not to suppress valid research through arbitrary or capricious standards.\n\n#### **3. A Call for a Healthy Research Environment** \n\nAs an active participant in the ML community who also serves as an Area Chair and Reviewer, I am deeply invested in the integrity of our review process. We must collectively strive to **identify and filter out low-quality reviews just as rigorously as we filter out low-quality papers**.\n\nI urge the Area Chair to weigh these reviews with the scrutiny it warrants. Furthermore, I hope this response serves as a reminder to all of us—authors, reviewers, and ACs alike—of our shared responsibility. **We must ensure that our judgments are technically sound, fair, and driven by a genuine desire to advance the state of machine learning**. Let us work together to cultivate a research atmosphere that **values substance over nitpicking** and **rigorous scientific debate over dismissive rejection**.\n\nSincerely,\n\nAuthors"}}, "id": "uDFIiIFDOu", "forum": "Rip3EJc4nx", "replyto": "Rip3EJc4nx", "signatures": ["ICLR.cc/2026/Conference/Submission4434/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4434/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4434/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763696019030, "cdate": 1763696019030, "tmdate": 1763696019030, "mdate": 1763696019030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HFPrune, a structured pruning method for LLMs that replaces traditional one-hot cross-entropy with information entropy as the criterion for Taylor-based neuron importance evaluation. The authors argue that entropy-based evaluation considers the full output distribution rather than just the ground-truth token, leading to better preservation of model capabilities. The method focuses on pruning MLP modules and demonstrates consistent improvements over existing methods across LLaMA and Qwen models on zero-shot benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The method avoids computational overhead of teacher models and resolves gradient initialization issues in self-distillation approaches, showing 3x speedup over SDMPrune with 31% less memory usage.\n\nDemonstrates improvements across multiple model families (LLaMA, Qwen) and sparsity levels, with some configurations even exceeding dense model performance after fine-tuning.\n\nThe approach is straightforward to implement, requiring only standard forward-backward passes without custom kernels or auxiliary models."}, "weaknesses": {"value": "The paper fundamentally lacks theoretical justification for why entropy-based importance should preserve model performance. This is not a minor omission, in my view it's a central issue that undermines the contribution's scientific rigor.\n\n**Limited Evaluation Scope**: \n- Exclusively focuses on zero-shot QA/classification tasks\n- No evaluation on reasoning, long-form generation, or conversational capabilities  \n- Largest model tested is only 7B parameters\n- Limited architectural diversity beyond LLaMA/Qwen families\n\n**Methodological Limitations**:\n- Uses uniform pruning ratios across layers, ignoring heterogeneous sensitivity\n- Limited sparsity range testing (only 20-30%)\n- The performance gains over dense models likely result from fine-tuning rather than pruning itself\n\n**Insufficient Analysis**: \n- No explanation of which types of neurons are pruned and why (recent works have revealed super neurons, super weights, super experts etc.)\n- No investigation of the relationship between entropy reduction and specific capabilities\n- Missing analysis of method sensitivity to calibration data selection\n\n**Questionable Claims**: The assertion that the method \"minimizes the change of global prediction distribution\" is not rigorously nor theoretically established, and the connection between this and performance preservation remains speculative."}, "questions": {"value": "1. Can you provide rigorous mathematical analysis of why minimizing entropy change should preserve model capabilities better than minimizing cross-entropy change?\n2. What is the information-theoretic justification for treating high-entropy neurons as more important?\n3. How does your approach relate to existing theories of neural network capacity and information flow?\n4. How does the method perform on reasoning tasks (GSM8K, BBH), long-context generation, and conversational AI beyond zero-shot classification?\n5. Can you evaluate on larger models (70B+) and more diverse architectures to support generalizability claims?\n6. What explains the performance improvements over dense models - is this due to fine-tuning effects rather than pruning benefits?\n7. Why use uniform pruning ratios instead of layer-sensitive approaches?\n8. How sensitive is the method to calibration data selection and size?\n9. How does the method perform under more aggressive pruning (40-70% sparsity)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NcMHoqRoGL", "forum": "Rip3EJc4nx", "replyto": "Rip3EJc4nx", "signatures": ["ICLR.cc/2026/Conference/Submission4434/Reviewer_Emas"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4434/Reviewer_Emas"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903326072, "cdate": 1761903326072, "tmdate": 1762917362119, "mdate": 1762917362119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HFPrune, a novel method for compressing LLMs through pruning of the MLP modules within transformer architectures. HFPrune introduces an information entropy-based criterion for evaluating neuron importance, offering a more holistic approach than traditional methods, which rely on one-hot cross-entropy loss. This entropy-based method minimizes the global prediction distribution change, effectively preserving model performance.  Extensive experiments on LLaMA and Qwen models demonstrate the effectiveness and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The proposed idea is simple, straightforward, and aligns well with intuition. From the engineering perspective, it is also very easy to implement.\n\nExtensive experiments were conducted on multiple LLM models, across diverse benchmarks, comparing the proposed method with several previous methods. The results demonstrate significant improvements in both performance and efficiency of the proposed method."}, "weaknesses": {"value": "**Explanation on the Choice of Baselines**: As discussed in the Related Work section, LLM pruning is a highly focused research area with a large body of work, which can be categorized into different approaches. However, in the experimental section, only a few methods such as LLM-pruner, LoRAPrune, and LoRAP are compared, and the rationale behind choosing these baselines is not explained. It remains unclear whether the state-of-the-art methods from each category are all covered by the baselines used in this paper. If they are, an explanation should be provided; if not, more baselines should be included (or a justification should be given for why they cannot be included for a fair comparison).\n\n**More Comparison on Efficiency**:  For Efficiency, the paper only compares the proposed method with SDMPrune, demonstrating that the proposed approach is more efficient than methods like SDMPrune, which require a teacher model. The efficiency of other methods (LLM-pruner, LoRAPrune, LoRAP) should also be compared.\n\n**Writing Needs Improvement**:\n- For example, in lines 013-014 of the Abstract: The statement \"A common approach uses Taylor Expansion\" does not clearly explain the field of research. It should at least mention that \"it is a common approach for LLM pruning.\"\n- In line 015 of the Abstract, the sentence \"However, its reliance on one-hot cross entropy loss, ...\" contains a grammatical error. It should be \"it relies on\" instead.\n- Many more problems on writing exists"}, "questions": {"value": "**Regarding whether \"focus on pruning MLP\" is the contribution of this paper**: In lines 039-043, when explaining why the paper focuses on pruning MLP, the authors use phrases like \"We find that\" and \"we observe that\" (and then prove this in Section 5.3.3 with experiments), but these statements are also referencing previous works. Is this an innovative contribution of the paper, or is it the findings from previous works?\n\n**considering a subset of tokens**：Between the extremes of \"only consider the ground-truth label token\" and \"consider all possible tokens\", an intermediate approach could be: only considering a subset of tokens (for example, the top K tokens with the highest probability) when evaluating neuron importance. Did the authors experiment with this approach? Experiments or discussions on this would make the paper more insightful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "oQLbsixoEN", "forum": "Rip3EJc4nx", "replyto": "Rip3EJc4nx", "signatures": ["ICLR.cc/2026/Conference/Submission4434/Reviewer_cwzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4434/Reviewer_cwzU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959291294, "cdate": 1761959291294, "tmdate": 1762917361834, "mdate": 1762917361834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}