{"id": "mXQslpfSU5", "number": 21134, "cdate": 1758314109753, "mdate": 1759896940303, "content": {"title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-agent Pipeline", "abstract": "While Language Models (LMs) have made significant progress in automating machine learning engineering (MLE), the acquisition of high-quality MLE training data is significantly constrained. Current MLE benchmarks suffer from low scalability and limited applicability because they rely on static, manually curated tasks that demand extensive time and manual effort to produce.\nWe introduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw datasets into competition-style MLE challenges through an efficient generate--verify--execute paradigm for scaling MLE tasks with verifiable quality, real-world usability and rich diversity. \nThe proposed multi-agent pipeline in MLE-Smith drives structured task design and standardized refactoring, coupled with a hybrid verification mechanism that enforces strict structural rules and high-level semantic soundness. It further validates empirical solvability and real-world fidelity through interactive execution.\nWe apply MLE-Smith to 224 of real-world datasets and generates 606 tasks spanning multiple categories, objectives, and modalities, demonstrating that MLE-Smith can work \neffectively \nacross a wide range of real-world datasets.\nEvaluation on generated tasks shows that the performance of eight mainstream and cutting-edge LLMs on MLE-Smith tasks is strongly correlated with their performance on carefully human-designed tasks, highlighting the effectiveness of the MLE-Smith in scaling up MLE tasks while maintaining task quality.", "tldr": "", "keywords": ["Machine Learning", "LLM Agents"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/55fe2fe90efef6fbe6721634b073ceb46e6304e6.pdf", "supplementary_material": "/attachment/622a95bedad248792eb6ada49ffc3574b300680a.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the scarcity of high-quality, scalable training data for Machine Learning Engineering (MLE) tasks. The authors propose MLE-Smith, a fully automated multi-agent pipeline designed to convert raw datasets into competition-style MLE challenges. The pipeline operates on a \"generate-verify-execute\" paradigm, utilizing specialized agents to create tasks. A hybrid verification mechanism is employed to ensure structural integrity, semantic soundness, and empirical solvability. The authors apply this pipeline to 224 real-world datasets, generating 606 new MLE tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important problem. The bottleneck in creating high-quality, large-scale benchmarks for MLE agents is a real obstacle to progress in the field."}, "weaknesses": {"value": "1. The contribution of the paper is limited. The proposed \"generate-verify-execute\" pipeline is a relatively common and established strategy for automated data synthesis and benchmark generation. Many prior works have employed similar paradigms, and the paper does not sufficiently articulate how this multi-agent application fundamentally differs from or improves upon those, beyond its application to the MLE domain. \n2. While the verification mechanism ensures that tasks are executable and solvable (i.e., an agent can run code and get a non-trivial score), it does not appear to guarantee the correctness of their reference solutions. It is possible for a generated task to pass all verification checks while containing subtle logical flaws, or for the intended solution path to be suboptimal. The validation relies on \"non-trivial predictive performance\" rather than a strong guarantee of ground-truth correctness, which could impact the quality of the benchmark for training.\n3. The experimental evaluation is lacking in-depth analysis. The results are largely aggregated, focusing on high-level Elo rankings and correlations. While this shows that the benchmark can rank models, it fails to provide insight into why models perform as they do or what specific challenges the new benchmark presents. The paper lacks a qualitative or fine-grained analysis of model failures. For example, what kinds of tasks are most difficult? What specific errors do top-performing agents make? Without this analysis, it is difficult to confirm the \"challenging\" nature of the benchmark, as it's unclear if it introduces new, harder problems or simply more of the same problems found in existing benchmarks."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5QLTkByDMg", "forum": "mXQslpfSU5", "replyto": "mXQslpfSU5", "signatures": ["ICLR.cc/2026/Conference/Submission21134/Reviewer_Z7gA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21134/Reviewer_Z7gA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761395029457, "cdate": 1761395029457, "tmdate": 1762941410687, "mdate": 1762941410687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Automating machine learning engineering (MLE) is an important task in LLM. However, the acquisition of high-quality MLE training data is difficult. In this work, the authors introduce MLE-smith, a fully automated multi-agent pipeline, to transform raw datasets into competition-style MLE challenges through an efficient generate–verify–execute paradigm for scaling MLE tasks with verifiable quality, real-world usability and rich diversity and a hybrid verification mechanism that enforces strict structural rules and high-level semantic soundness. The proposed benchmark utilizes 224 datasets to derive 606 tasks spanning multiple categories, objectives, and modalities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The writing is clear and easy to understand.\n2. The design of the pipeline is reasonable, detailed and economical. The pipeline incorporates a hybrid verification stack: deterministic assertions (format/structure-checks), semantic reviews (via agent), and execution-based validation (empirical solvability).  This provides multiple levels of guarantee that the generated tasks are structurally correct, semantically meaningful, and actually executable by agents while the cost is mere $2.11 per dataset. Given the cost, the scalability of this method is ensured.\n3. They conduct comprehensive experiments on 8 different language models to testify to the quality of their dataset and their strong correlation with human-designed tasks."}, "weaknesses": {"value": "1. I think the authors should include more details about the hybrid verification check in the Appendix, to ensure the reproducibility of this work."}, "questions": {"value": "1. The difference of Refactor and Assertions seems ambiguous to me. It looks like that they are both executing formal checks. I would appreciate it if the authors could further clarify their differences, e.g. Refractor is for execution while Assertions is for check only?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wzaaQxZ4Ml", "forum": "mXQslpfSU5", "replyto": "mXQslpfSU5", "signatures": ["ICLR.cc/2026/Conference/Submission21134/Reviewer_9fGm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21134/Reviewer_9fGm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761551741039, "cdate": 1761551741039, "tmdate": 1762941409760, "mdate": 1762941409760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical bottleneck in scaling the creation of high-quality Machine Learning Engineering (MLE) benchmarks, which are essential for developing and evaluating sophisticated AI agents. The key of this work is MLE-Smith, a fully automated, multi-agent pipeline designed to transform raw datasets into competition-style MLE challenges at scale. The authors demonstrate the efficacy of their system by applying MLE-Smith to 224 real-world datasets, successfully generating 606 fully verified tasks spanning a wide range of modalities, objectives, and domains."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThe manuscript is well-motivated and well-written. The paper tackles a problem of significant importance. The ability to automatically generate diverse, high-quality MLE benchmarks at scale would be a major catalyst for research in autonomous MLE agents. Moreover, the proposed generate-verify-execute pipeline is well-presented and can be easily understood.\n\n2.\tThe authors successfully demonstrate the system's capability at scale. They generate 606 tasks from 224 diverse datasets. The subsequent evaluation is extensive, involving eight different LLMs on a 100-task benchmark."}, "weaknesses": {"value": "1.\tThe paper's main justification for task quality is that its performance rankings correlate with the MLE-Dojo benchmark. While this is a useful check, it feels like a narrow definition of \"quality.\" Relying only on this metric makes it hard to judge other important aspects, like whether the tasks are truly novel, realistic, or test a wide range of skills. The paper's claims would be much more convincing if backed by more evidence, such as qualitative feedback from human MLE experts or an analysis showing that the tasks require diverse problem-solving strategies.\n\n2.\tThe paper proposes a sophisticated system with multiple components, but there are no ablation studies to show what each part is actually contributing. For example, how much does performance change if you alter the agent's prompt or remove a specific reasoning step? Without this analysis, it's hard for the reader to know which components are essential to the system's success and which might be less important."}, "questions": {"value": "1.\tCould you consider supplementing the benchmark correlation with qualitative feedback from human MLE experts to provide a more holistic validation of task quality?\n\n2.\tCould you provide further analysis on the diversity of skills or problem-solving strategies required by your tasks, beyond the performance ranking correlation?\n\n3.\tHow sensitive is the system's performance to changes in key components, such as the agent's prompt or the removal of a specific reasoning step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OkgrOT25el", "forum": "mXQslpfSU5", "replyto": "mXQslpfSU5", "signatures": ["ICLR.cc/2026/Conference/Submission21134/Reviewer_pQFv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21134/Reviewer_pQFv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761554750662, "cdate": 1761554750662, "tmdate": 1762941408609, "mdate": 1762941408609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MLE-Smith, a fully automated multi-agent system that converts raw datasets into competition-style machine learning engineering (MLE) tasks using a generate–verify–execute paradigm. Three specialized agents—Brainstormer, Designer, and Refactor—collaborate to create, standardize, and validate MLE tasks. Performance rankings on MLE-Smith tasks strongly correlate with those on human-curated benchmarks (MLE-Dojo), Pearson r ≈ 0.98, Spearman ρ ≈ 0.95, demonstrating high realism. Overall, MLE-Smith achieves scalable, diverse, and verifiable generation of MLE challenges for agent training and benchmarking."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Thorough evaluation – Quantitative correlation between synthetic and real tasks across multiple LLMs; diverse modalities and metrics.\n2. Strong empirical realism – Demonstrated high rank and score correlation with human benchmarks, suggesting the tasks are faithful surrogates for real-world ones.\n3. Reproducibility – Detailed description of environment, budgets, and execution setup; appendix lists datasets and code schema."}, "weaknesses": {"value": "1. Ablation missing: The pipeline is explicitly broken into three agent roles (Brainstormer, Designer, Refactor) and a three-layer verification mechanism (Assertions, Reviews, Execution-based validation). However, it does not provide an empirical ablation study to justify the necessity of each individual component.\n2. The pipeline was tested on 300 datasets from Kaggle. These datasets are typically well-structured and pre-cleaned for competition. It is unclear how MLE-Smith would handle \"rawer\" datasets (e.g., unstructured server logs, complex scientific data) that lack clear, pre-identified features or labels, and which may require significant domain expertise to formulate a task. In practice, those rawer data are even more useful since it requires good feature engineering strategies. \n3. Lack of in-depth discussion: The paper states 807 tasks were generated and 606 were \"fully verified\". This implies a failure rate of ~25% (201 tasks). The paper does not provide a breakdown of why these tasks failed. clearer framing of scientific insight (why this works) would strengthen the whole paper as well."}, "questions": {"value": "1. Could MLE-Smith handle domains outside Kaggle-like structured datasets?\n2. Are there known failure patterns or bottlenecks in verification throughput? How many failed at the Assertions, Reviews, and Execution-based Validation stages, respectively?\n3. What empirical benefit does the multi-agent separation provide versus a monolithic prompting approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "boxTiSF4T0", "forum": "mXQslpfSU5", "replyto": "mXQslpfSU5", "signatures": ["ICLR.cc/2026/Conference/Submission21134/Reviewer_5zJ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21134/Reviewer_5zJ5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21134/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968220038, "cdate": 1761968220038, "tmdate": 1762941407260, "mdate": 1762941407260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}