{"id": "WGqibe5H3W", "number": 18112, "cdate": 1758283994499, "mdate": 1763662381593, "content": {"title": "ResCP: Reservoir Conformal Prediction for Time Series Forecasting", "abstract": "Conformal prediction offers a powerful framework for building distribution-free prediction intervals for exchangeable data. \n    Existing methods that extend conformal prediction to sequential data rely on fitting a relatively complex model to capture temporal dependencies. \n    However, these methods can fail if the sample size is small and often require expensive retraining when the underlying data distribution changes. \n    To overcome these limitations, we propose Reservoir Conformal Prediction (ResCP), a novel training-free conformal prediction method for time series. \n    Our approach leverages the efficiency and representation learning capabilities of reservoir computing to dynamically reweight conformity scores. \n    In particular, we compute similarity scores among reservoir states and use them to adaptively reweight the observed residuals at each step. With this approach, ResCP enables us to account for local temporal dynamics when modeling the error distribution without compromising computational scalability. We prove that, under reasonable assumptions, ResCP achieves asymptotic conditional coverage, and we empirically demonstrate its effectiveness across diverse forecasting tasks.", "tldr": "", "keywords": ["Conformal prediction", "Time series", "Uncertainty quantification"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5a5b9f4affc3d9eaede615cf313515802cd2aed.pdf", "supplementary_material": "/attachment/f88d72dd66f0656f903a8812a8ee4f63825badf1.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ResCP (Reservoir Conformal Prediction), a novel training-free method for constructing conformal prediction intervals in time series forecasting. They use Echo State Networks (ESNs) - randomized recurrent neural networks - to compute data-dependent weights for conformal prediction without requiring expensive model training compared to existing works. The authors provide comprehensive theoretical and experimental support for the CP technique."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Excellent presentation. The motivation, algorithm, and results are all very clearly explained and easy to follow. \n\n- The algorithm is strong and fills a needed gap. In 2023 / 2024 there are a bunch of papers that use regression inside the CP algorithm in order to handle patterns in the residual (SPCI and HopCPT are prime examples), although they did achieve better efficiency and are inspiring, the methods are slow and data-hungry, negating some of the advantages of CP itself. Having a high performing training-free (almost non-parametric) alternative is great.\n\n- Strong Empirical Performance. Achieves competitive or superior results across multiple datasets, particularly outperforming HopCPT while being much faster."}, "weaknesses": {"value": "Not much. The only criticism is perhaps the assumptions for ResCP is quite limiting, making the algorithm to be sensitive to data conditions (assumption 3.1) and hyperparameter choices (assumption 3.5). The sensitivity analysis shows performance varies significantly with these choices."}, "questions": {"value": "- Strongly mixing data generation process was not an assumption by SPCI and HopCPT, I believe they both permit distribution shifts or regimes in the data generation process. I also do not think neither weather data (solar and Beijing) nor financial data is strongly mixing? I would love to see a discussion on what kind of data is NOT suitable for ResCP and it performs worse than SPCI for example. \n\n- Condition 3.5 (ii) is quite strong and perhaps need more discussion. How is this clustering property ensured? Is there evidence that the ESN hyperparameters (spectral radius, input scaling, etc.) produce a reservoir that maps similar dynamics to nearby states? Does it place a strong requirement that  the input sequence have strong recurring patterns? The paragraph below assumption 3.5 feels like 3.5(ii) can be a direct result of 3.5(i), which i do not think is the case."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eFTkG9b2ro", "forum": "WGqibe5H3W", "replyto": "WGqibe5H3W", "signatures": ["ICLR.cc/2026/Conference/Submission18112/Reviewer_3iSQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18112/Reviewer_3iSQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761274382621, "cdate": 1761274382621, "tmdate": 1762927880967, "mdate": 1762927880967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General comments and change list"}, "comment": {"value": "We thank all the reviewers for the valuable feedback. We addressed all the questions and concerns in the following. In addition, we made several modifications and additions to the paper, which we summarize below:\n- Following the suggestions of reviewer WWw1, we added the calibration curves of ResCP, ResCQR, CP-QRNN and NexCP over all datasets with the RNN baseline (Fig. 2, main body) and the calibration curves of ResCP and ResCQR over all datasets and baselines (Fig. 8). Moreover, Appendix D now includes tables with full results across additional coverage levels.\n- As suggested by reviewer SLWv, we added two new ablation studies assessing 1) the effectiveness of the design introduced in 3.1.2 to deal with non-stationarity (Section 4 and Appendix E), and 2) the use of exogenous variables (Appendix E).\n- As recommended by reviewer TvuY we added an experiment on a synthetic non-stationary time series generated by multiple AR processes with varying parameters (Appendix D).\n- By taking reviewers’ feedback into account, we updated the text to include clarifications when needed (updated text is in green)."}}, "id": "5gwEt1IrYV", "forum": "WGqibe5H3W", "replyto": "WGqibe5H3W", "signatures": ["ICLR.cc/2026/Conference/Submission18112/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18112/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18112/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763662538089, "cdate": 1763662538089, "tmdate": 1763662538089, "mdate": 1763662538089, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposed a conformal prediction method for time series forecasting using reservoir computing implemented with echo state network."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tThe study utilized reservoir computing implemented by echo state networks, which has not been studied before in the context of conformal prediction for time series.\n-\tThe study provided an asymptotic conditional coverage guarantee and the theoretical analysis was explained thoroughly.\n-\tDetailed backgrounds and motivation related to the existing studies."}, "weaknesses": {"value": "-\tIn Assumption 3.1, the author assumed stationarity of $(x_t, r_{t+H})$. I believe the author used residuals for $x_t$ (i.e., $x_t = r_t$) for ResCP, and I think assuming stationarity of residuals (i.e., $(r_t, r_{t+H})$) makes sense. However, in 3.1, the author claimed that “the input $x_t$ can include any set of endogenous and exogenous variables at time step t”.  I am concerned that in this case the assumption does not hold, since in general time series features are not guaranteed to be stationary, which makes conditional coverage somewhat restricted to ResCP with $x_t = r_t$ (since I guess ResCQR utilizes other variables except residuals).\n\n\n-\tThe author proposed and used ad-hoc methods, such as using time-dependent weights described in 3.1.2 and handling exogenous inputs (i.e., ResCQR) described in 3.2, on top of the main proposed method (ResCP). However, with the current experiment design, it is difficult to see what is the performance of the proposed method and what is the improvement due to the ad-hoc methods. Based on my understanding, the authors used time-dependent weights as the default to ResCP and used exogenous variables only for ResCQR. Ablation studies regarding these ad-hoc methods will be helpful for readers to understand and interpret the experimental results. For example, ablation study: ResCP vs. ResCP + time-dependent weights vs. ResCP + exogenous variables vs. ResCP + time-dependent weights  + exogenous variables vs. ResCQR.\n\n\n-\tIn line 328 under Remark section, the author claimed that “good approximation of F entirely depends on the ability of the ESN to encode all relevant information from the past in its state”. I am not sure what it means since ESN does not have trainable parameters and only has echo state property, which says it asymptotically forgets its initial state. Even if the claim is true, then what is the advantage of using ESN over RNNs or other sequence models? Since I believe RNNs have more capacity to learn “all relevant information from the past in its state” to compute better weights to approximate F (even tho it would not feasible to achieve the asymptotical conditional coverage theoretically as in the way the author derived using other sequence models).\n\n\n-\tThe experiment results did not show consistent results supporting the effectiveness of the proposed methods, and the manuscript did not contain enough details about the implementation of the method, experiment setup, to interpret the results (see questions)."}, "questions": {"value": "-\tIn 2.2 Echo State Networks, it is stated that when “$W_h$ and $W_x$ are properly initialized, …”, however throughout the paper, I can only find the condition about $W_h$ which is required to set to satisfy the spectral radius < 1. What is “proper initialization” for $W_x$?\n\n-\tIn line 217, “we approximate it through Monte Carlo sampling akin to Auer (2023)…”.  I believe its better to describe how the authors did this since its important things to construct the prediction intervals with some details: what sampling size did the authors used? Did authors used sampling size same for all datasets or different size for datasets?\n\n-\tIn line 225, the authors refined the intervals by selecting $\\beta$ that minimizes width. What are $\\beta$ candidates did authors used to obtain $\\beta^*$ that minimizes interval width?\n\n-\tIn Assumption 3.2, are there any assumptions need for $W_h$ and $W_x$? how did authors derive the bound for ESN based on $d_{\\text{fm}}$\n\n-\tWhat is CP-QRNN baseline? While the authors cited [1], there is no method entitled CP-QRNN in the cited paper. Additionally, the important details of CP-QRNN such as model architecture are missing in the main text and Appendix, which makes difficult to interpret the results since CP-QRNN is the baselines that showed the best performances in the experiments on Beijing and Solar datasets.\n\n-\tWhat exogenous variables are used for the experiments? which methods used exogenous variables?\n\n\n-\tIn sec 4.1, the author stated “CP-QRNN baseline achieves strong performance on the large datasets with informative exogenous variables,… but fails in achieving good results in the smaller datasets”. This claim does not sound convincing since we do not know whether exogenous variables are informative. It could be interpreted that in other two datasets where the proposed methods outperformed other baselines,  the exogenous variables are not informative? The authors need to provide details of exogenous variables or carefully designed experiments using synthetic dataset to support this claim. Additionally, what is the definition of the smaller datasets? Based on the dataset details provided in the appendix, the size of the ACEA dataset is the largest (solar data is 26k, Beijing air quality dataset is 35k, exchange dataset is 7.5k, and ACEA dataset is 137k), while the author stated that ACEA dataset is a small dataset.\n\n-\tIn line 60, the author claimed that “ResCP can be applied on top of any point forecasting model, since it only requires residuals from a disjoint calibration set”. However, isn’t this true for any CP methods? Since technically CP is base prediction model-agnostic.\n\n\n[1] Relational Conformal Prediction for Correlated Time Series, Cini et al., 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kLhqPfpksd", "forum": "WGqibe5H3W", "replyto": "WGqibe5H3W", "signatures": ["ICLR.cc/2026/Conference/Submission18112/Reviewer_SLWv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18112/Reviewer_SLWv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761804577907, "cdate": 1761804577907, "tmdate": 1762927880443, "mdate": 1762927880443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies prediction set construction for time-series data.\nThe proposed method builds on reservoir computing and learns the \nresidual distribution by dynamically weighting the calibration samples.\nThe method is evaluated on real datasets, showing advantages in \ncoverage and computation efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow. \n2. Integrating conformal prediction with reservoir computing demonstrates the potential to unite statistical efficiency and computational efficiency in predictive analysis of time-series data—achieving the best of both worlds."}, "weaknesses": {"value": "1. The theoretical results are relatively clean, but depend on a set of assumptions. \nIt would be helpful to evaluate the method in synthetic environments (where the data-generating process is fully known)\nand systematically investigate the method's sensitivity to violations of its assumptions.\n\n2. In the numerical results, it appears that CP-QRNN often achieves strong statistical performance in \nreasonable computing time, compared with ResCQR. I wonder if the authors could provide more discussion on \nthe comparison, to clarify better the advantage of ResCQR and how a practitioner should choose between the methods \nin different scenarios."}, "questions": {"value": "Please refer to the \"Weaknesses\" section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t492SGhK7E", "forum": "WGqibe5H3W", "replyto": "WGqibe5H3W", "signatures": ["ICLR.cc/2026/Conference/Submission18112/Reviewer_TvuY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18112/Reviewer_TvuY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933138164, "cdate": 1761933138164, "tmdate": 1762927879276, "mdate": 1762927879276, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work proposes a novel method that utilises Reservoir Computing to reweight conformity scores. This helps account for the local temporal dynamics, and under some assumptions, the method can achieve conditional coverage. Empirical evidence for the method's efficacy is provided across various forecasting tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea is quite interesting to use Reservoir Computing to adapt the residuals. The background is well-written and thorough. The experiments are also conducted with various datasets, baselines, and models. The experiments are repeated across several runs, giving a good picture of the method's stability."}, "weaknesses": {"value": "The biggest weakness is the comparison with only one significance level. To best judge a conformal method, it is imperative to compare its performance across different significance levels, and plotting a calibration curve is particularly helpful.\n\nThe notations are unclear at times and the presentation could be improved.\n\nSee \"Questions*\" below for more"}, "questions": {"value": "1. Line 078: If I understand correctly, y_t is a scalar observation. How would the method incorporate the cases where we have multidimensional time series forecasting?\n\n2. Line 081: If I understand correctly, y_hat_{t+H} is a vector corresponing to H steps of prediciton. Since both times, a small y is used, it becomes notationally confusing to follow the stuff here. \n\n3. In equation 2, shouldn't it be the absolute residuals, since they are used as the nonconformity scores for regression? \n\n4. Line 102: \"To address heteroskedasticity\": does it refer to heteroskedasticity over the values, or time index, or both?\n\n5. Line 191: Is x_t supposed to be the residual, not the history i.e y_{t-1}...\n\n6. Line 196: Is x the time series now instead of y, and not the exogenous variables or residuals?\n\n7. Equation 6: Shouldn't s be upper-bounded by T and not T-H?\n\n8. Equation 11: If we want symmetric intervals, can we work with absolute residuals and adapt them accordingly?\n\n9. Assumption 3.1: Earlier, it was alluded to that under mild assumptions, the coverage is satisfied; however, strongly mixing seems to be a rather strong condition. Would any of the real-world time series even satisfy it?\n\n10. \"Assumption 3.2 tells us that the reservoir state evolves in a stable and predictable manner\"  This also seems a strong assumption, especially if there is a change of distribution. \n\n11. Line 338 \"do not include any positional encoding\": Could having a positional encoding help the algorithm?\n\n12. Regarding Table 1: Firstly, it is interesting that adding exogenous variables is hurting the performance, as apparent from RESCQR. As for RESCP, it is not clear if there are huge advantages; in most cases, across learning and non-learning, the average width is better for Learning methods, whereas many of the learning methods still have better Winkler score.\n\n13. The target confidence is set as high as 0.9, which is okay. However, it is necessary to demonstrate the performance of the proposed method with varying significance levels. A calibration curve may be helpful in determining if the proposed method works effectively in all cases.\n\n14. The theoretical statements are there for conditional coverage, but there is no empirical evidence for the same."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mawJfYMZt0", "forum": "WGqibe5H3W", "replyto": "WGqibe5H3W", "signatures": ["ICLR.cc/2026/Conference/Submission18112/Reviewer_WWw1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18112/Reviewer_WWw1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18112/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762027986329, "cdate": 1762027986329, "tmdate": 1762927878487, "mdate": 1762927878487, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}