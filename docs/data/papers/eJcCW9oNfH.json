{"id": "eJcCW9oNfH", "number": 12093, "cdate": 1758205655720, "mdate": 1759897534181, "content": {"title": "EVLP: Learning Unified Embodied Vision-Language Planner with Reinforced Supervised Fine-Tuning", "abstract": "In complex embodied long-horizon manipulation tasks, effective task decomposition and execution require synergistic integration of textual logical reasoning and visual-spatial imagination to ensure efficient and accurate operation. Current methods fail to adopt a unified generation framework for multimodal planning, leading to inconsistencies in multimodal planning. To address this challenge, we present EVLP (Embodied Vision-Language Planner), an innovative multimodal unified generation framework that jointly models linguistic reasoning and visual generation. Our approach achieves multimodal planning for long-horizon tasks through a novel training pipeline incorporating dynamic pretraining and reinforced alignment. Our core innovations consist of three key components: 1. Unified Multimodal Generation Framework: For understanding, we integrate semantic information with spatial features to provide comprehensive visual perception. For generation, we directly learn the joint distribution of discrete images for one-step visual synthesis, enabling coordinated language-visual modeling through learnable cross-modal attention mechanisms. 2. Dynamic Perception Pretraining: We propose a bidirectional dynamic alignment strategy employing inverse dynamics tasks and forward dynamics tasks, effectively strengthening multimodal correlations within a unified feature space. 3. Reinforced Supervised Fine-Tuning: While conducting instruction-based fine-tuning in the unified generation space, we construct a reinforce loss to align the spatial logic between textual actions and generated images, enabling the model to acquire spatio-aware multimodal planning capabilities.Comprehensive evaluations on multiple complex tasks demonstrate that EVLP significantly outperforms competitive baselines in both instruction execution accuracy and task success rate, benefiting from its unified multimodal architecture and well-designed training pipeline. Extensive ablation studies further validate the rationality of our framework design.", "tldr": "An unified multimodal planning framework for complex long-horizon tasks with dynamic learning and reinforced alignment", "keywords": ["Embodied intelligence", "multimodal large language models", "reinforcement learning", "long-sequence planning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f153cac2d15c42da2b3f7a41cd93a790850de0cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an innovative multimodal unified generation framework (EVTP), which enhances model consistency in problem-solving through dynamic perception preprocessing and reinforcement supervised fine-tuning (RSFT). In terms of the unified multimodal generation framework, it integrates semantic understanding and spatial encoding (visual tower design) to achieve more comprehensive visual perception; in dynamic preprocessing, the bidirectional alignment strategy strengthens the correlation between models; and in reinforcement supervised fine-tuning, it ensures the stability of generated images."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "See Summary"}, "weaknesses": {"value": "1. The paper treats reinforcement supervision fine-tuning (RSFT) as one of the core methodological innovations, and its combination of maximum likelihood supervision with strategy gradient optimization is effective. However, the current discussion of RSFT in the paper is mainly limited to comparisons with GPG and GRPO-onpolicy (Table 7), which does not fully place it in a broader LLM alignment research context. In recent years, the paradigm of combining reinforcement learning with supervised learning to optimize generative models has become a research hotspot, such as the combination of instruction fine-tuning (SFT) with reinforcement learning from human feedback (RLHF), and alignment algorithms like direct preference optimization (DPO) that do not require explicit reward models. It is recommended that the authors delve deeper into the differences and similarities in motivation, optimization objectives, and implementation mechanisms between RSFT and these mainstream alignment technologies (such as PPO-based RLHF, DPO, etc.) in the methodology section or related work section.\n2. The dynamic alignment reward function in the paper is the cornerstone of the RSFT framework, and its quality directly determines the optimization effect of the reinforcement learning stage. However, in Section 2.3 of the main text, its description is merely \"measuring whether the dynamics of the generated image is consistent with the real dynamics,\" which is too vague. The specific definition is placed in Appendix C.1, which weakens the integrity and readability of the methodology in the main text. Considering the importance of this reward function, it is strongly recommended that the authors move the key information from Appendix C.1, at least the simplified version of its core ideas and mathematical forms, to Section 2.3 of the main text. Specifically, the function should be clearly explained in the main text on how it calculates the reward by detecting dynamic regions, using the Hungarian algorithm for region matching, and combining IoU (spatial alignment) and MSE (pixel consistency).\n3. The experiments in the paper mainly focus on tabletop manipulation scenarios, such as LoHoRavens and Meeting Preparation. Although these are standard benchmarks in the field, they represent relatively restricted environments. The paper prospects for \"more open embodied scenarios\" in the conclusion but does not delve into the potential challenges that the EVLP framework may face when extended to more complex environments. For example, in complex 3D scenarios involving mobile navigation, is the single-step generation of complete visual sub-goals (a 2D image) still applicable? How should 3D spatial states or free-viewpoint images be characterized and generated? Moreover, when there are severe occlusions and partial observability in the environment, is the dynamic perception pretraining based on two frames sufficient? It is recommended that these potential extension issues be discussed in depth in the conclusion or a newly added \"Limitations and Future Work\" section, and a prospective analysis of the EVIP framework be conducted, exploring the improvements needed to deal with more complex scenarios (such as 3D navigation, interaction with deformable objects, etc.).\n4. The paper introduces the composition of the \"Vision Tower\" in Section 2.1, which combines a Siglip for semantic understanding with a trainable \"low-level visual encoder\" for capturing spatial details. The description of this low-level encoder is somewhat lacking, merely mentioning \"pretrained through image reconstruction loss.\" In Appendix C.2, it is mentioned that this encoder is the tokenizer's encoder of MAGVIT2, a key piece of information that should be explicitly stated in the main text.\n5. The paper emphasizes the sampling efficiency of EVLP during the inference stage, which is a significant advantage for its application in reinforcement learning. However, the paper does not mention the overall training cost. From the description, it appears that its two-stage training process (especially the pre-training stage with a batch size of 2048) requires a high amount of computational resources. It is suggested that a brief discussion of the training cost of EVLP (such as the required GPU model, number, and training duration) be included in the experimental section or the appendix, and compared with the main baseline models (especially PERIA, which also requires a generative model). Although fast inference speed is a significant advantage, a high training cost may limit its widespread application in both academia and industry.\n6. There are significant issues with Figure 1 in the paper as an overall framework diagram. Firstly, in the inverse dynamics part of the dynamic perception pre-training, there are two action description text inputs, which is very confusing and does not match the text description and formulas in the paper. Secondly, the red arrow (representing the backpropagation process) in the \"Reinforcement Supervised Fine-tuning\" section points to a rather chaotic direction. It seems that \"Reinforce Loss\" directly acts on \"Image Logits,\" but it is actually calculated after evaluating the reward function on \"Sampled Images.\" It is suggested to redraw Figure 1 to clearly and accurately show the process of the paper's model.\n7.The model is overly dependent on data preprocessing, limiting its effectiveness in application domains. In the dynamic preprocessing step shown in Figure 1, real-world object information needs to be fed into model training through the bidirectional alignment strategy. However, challenges arise when handling naturally occurring objects that have not been previously studied or discovered, and further research is needed to address this limitation.\n8.In the benchmark tests, key experimental metrics are evaluated under simulation modes, lacking real-world manifestation. This inevitably introduces data biases. Additionally, comparisons with advanced models such as VLIA-U are absent, which should be added to strengthen the persuasiveness of the experimental results.\n9.For image generation involving multiple movement processes, unified quantitative processing should be implemented, with clear comparative metrics provided to ensure the clarity and consistency of reasoning logic and experimental results.\n10.The paper compares models such as CLIPort, SuSIE, PERIA, and EmbodiedGPT in Table 1, but does not cover embodied multimodal systems such as RoboDreamer, Octo, and RT-Trajectory that have been publicly released in recent years. It is suggested that the authors supplement comparisons with these models or at least perform qualitative comparisons in the revision to demonstrate the leading position of EVLP in the research context of 2024-2025."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wBwHoWFJdJ", "forum": "eJcCW9oNfH", "replyto": "eJcCW9oNfH", "signatures": ["ICLR.cc/2026/Conference/Submission12093/Reviewer_inNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12093/Reviewer_inNa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536515458, "cdate": 1761536515458, "tmdate": 1762923062644, "mdate": 1762923062644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents EVLP, a unified framework for embodied vision-language planning that jointly generates textual actions and visual sub-goals.\nThe authors propose: A Unified Multimodal Generation Framework that integrates language and image token generation within a single model; A Dynamic Perception Pre-training stage based on inverse and forward dynamics prediction; A Reinforced Supervised Fine-Tuning (RSFT) scheme that combines SFT with reinforcement learning for task-specific optimization.\nExperiments on multiple embodied simulation benchmarks (e.g., LoHoRavens, Meeting Preparation) demonstrate improved task success rates compared to several recent baselines such as CLIPort, EmbodiedGPT, and PERIA."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of generating both visual sub-goals and language actions within a single autoregressive framework is elegant and practically appealing for long-horizon embodied tasks.\n2. The paper provides comprehensive experiments and ablations, showing consistent improvements over strong baselines.\n3. Dynamic perception pre-training is a valuable design choice that improves model understanding of environment dynamics."}, "weaknesses": {"value": "1. The proposed “Reinforced Supervised Fine-Tuning” is essentially a re-implementation of existing methods, without introducing new algorithmic insights or theoretical contributions. Its novelty is mostly limited to application in the embodied context.\n2. Experiments are confined to simulation. The paper does not discuss challenges in transferring the proposed unified planner to physical robots or real-world sensory noise.\n3. The method relies on a large unified transformer architecture; computational cost and scalability under constrained hardware settings are not analyzed."}, "questions": {"value": "No"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oHEPy0ruvr", "forum": "eJcCW9oNfH", "replyto": "eJcCW9oNfH", "signatures": ["ICLR.cc/2026/Conference/Submission12093/Reviewer_yA48"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12093/Reviewer_yA48"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904386370, "cdate": 1761904386370, "tmdate": 1762923062192, "mdate": 1762923062192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents EVLP, a unified multimodal framework for long-horizon robo manipulation tasks. The approach combines three components: (1) a dual-tower vision architecture integrating SigLIP for semantic understanding with MAGVIT2 discrete tokenization for one-step image generation, (2) dynamic perception pretraining using inverse and forward dynamics prediction tasks, and (3) RSFT that combines maximum likelihood with policy gradients to align spatial consistency between language actions and generated images. Experiments on LoHoRavens and Meeting Preparation benchmarks show improvements over baselines including language-only (PAR, EmbodiedGPT), vision-only (SuSIE, CoTDiffusion), and multimodal (PERIA) planning methods."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- This is a well motivated problem with clear practical value. Addressing an important gap in embodied AI: the separation of language and vision.\n- Table 6 shows dramatic speed ups from one-step generation  (0.15s vs 21.37s for 7B model), which is a truly practical advantage\n- I find the idea to be quite interesting and innovative. One-step generation is important for an application like robotics. I like the idea of unifying language and visual planning.\n- There were thorough ablations and a detailed appendix. I also like the thorough multimodal evaluation."}, "weaknesses": {"value": "- I believe there to be major writing quality issues which hinder presentation. There are numerous grammatical errors throughout (\"lead to inconsistent in\", \"spatio-awared\"). I think the main paper is missing key technical details like the architecture of the \"low-level visual encoder\", the \"image reconstruction loss\", the dataset of the vision tower. For the RSFT algorithm, K is never defend and it isn't explained how the advantage A_k is computed. Also missing the optimizer, learning rate, training steps, etc. A lot of this should be moved to the main paper. Notation is used inconsistently throughout the paper. For instance, model parameters θ appear explicitly in some equations (Equation 1: log P(a_t|...; θ)), as subscripts in others (P_θ(x|c)), and are completely omitted in Algorithm 2 despite describing the same probability distributions, making it difficult to track dependencies on trainable parameters.\" Minor presentation issues are fine but in this case, it makes it to difficult understand the method.\n- I think another major issue is the lack of real-robot validation. Real-world evaluation is limited to an offline dataset (LA/LPIPS) rather than physical execution with success rates on hardware; this limits claims about sim-to-real.\n- The main gains are shown on Ravens-style and \"Meeting Preparation\" simulations. I would like to see more diverse, open-world household benchmarks (multi-room, distractors, etc) which would help strengthen the claims. \n- I think an important baseline that is missing is state-of-the-art Vision-Language-Action (VLA) policies, like OpenVLA, Pi-0.5, etc."}, "questions": {"value": "- The 'one-step generation' claim is somewhat misleading—the model generates 256 tokens in parallel, which is more accurately described as parallel decoding rather than a fundamentally different generation paradigm. Is framing it as 'parallel token generation' more accurate?\n- Your dynamic alignment reward (Equation 7) relies on frame differencing to detect moving regions. How does this work when the background is not static (for example with a moving camera) or if multiple objects are moving at the same time?\n- How much data is used for dynamic perception pertaining, and where does this data come from? Have you tested how pretraining data quantity affects performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kqIoNctQdR", "forum": "eJcCW9oNfH", "replyto": "eJcCW9oNfH", "signatures": ["ICLR.cc/2026/Conference/Submission12093/Reviewer_c8Tb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12093/Reviewer_c8Tb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008365069, "cdate": 1762008365069, "tmdate": 1762923061716, "mdate": 1762923061716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "At its core, this paper proposes EVLP, a unified multimodal planner that jointly generates (i) step‑wise language actions and (ii) visual subgoal images for long‑horizon manipulation. On LoHoRavens, it beats language‑only, vision‑only, and prior multimodal planners (avg. ~6% over SOTA), and it samples images much faster than diffusion/AR baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Unified generation that is practically faster and simpler than AR/diffusion.\nThe paper’s well-motivated one‑step generator lets the LLM model the full image‑token distribution directly. Sampling throughput is compelling: EVLP reports 0.15 s for one image and 0.40 s for eight images, compared with 21.37 s and 172.96 s for autoregressive baselines.\n\n1. Consistent gains across long‑horizon tasks. EVLP beats PERIA and CoTDiffusion by healthy margins across sub-tasks. \n\n1. Strong ablations. While the paper has a somewhat involved architecture (SigLIP + image encoder + quantized codebook), the ablations are thorough and cover the choice of encoder ('vision tower'), RL algorithm, etc."}, "weaknesses": {"value": "1. Novelty vs. prior “unified” generators is under-explained. The paper cites Janus-style unified models and diffusion‑plus‑LLM hybrids, but the precise algorithmic novelty for one‑step image sampling (beyond “learnable image tokens + direct p(x|c) modeling”) could be spelled out more crisply. Concretely, for example, what prevents degenerate modes in your quantized codebook? This is a common issue in training codebook-based models."}, "questions": {"value": "1. The paper does not include compute budgets (GPUs, training time per stage, memory), and code/model checkpoints are not clearly committed. Please include them and outline your code/model weight policy clearly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OJ6QUnYkxo", "forum": "eJcCW9oNfH", "replyto": "eJcCW9oNfH", "signatures": ["ICLR.cc/2026/Conference/Submission12093/Reviewer_SJjD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12093/Reviewer_SJjD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12093/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147169780, "cdate": 1762147169780, "tmdate": 1762923060918, "mdate": 1762923060918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}