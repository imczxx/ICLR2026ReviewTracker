{"id": "92d74WdgtG", "number": 18587, "cdate": 1758289282182, "mdate": 1759897093992, "content": {"title": "A New Initialization to Control Gradients in Sinusoidal Neural Networks", "abstract": "Proper initialization strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialization parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialization for networks with sinusoidal activation functions such as $\\texttt{SIREN}$, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialization of the parameters, differing from the original $\\texttt{SIREN}$ scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling gradients prevents the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialization strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark $\\texttt{SIREN}$ with the proposed initialization against the original scheme and other baselines on function fitting and image reconstruction. The new initialization consistently outperforms state-of-the-art methods.", "tldr": "We propose a closed form expression for parameters initialization in SIREN networks to control gradient", "keywords": ["Initialization Strategy", "Deep Neural Networks", "Sinusoidal Activations", "Gradient Control", "Implicit Neural Representations", "Neural Tangent Kernel"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f94c06580a7eb4821b5b437dff521831f884c959.pdf", "supplementary_material": "/attachment/7d503c39f18b6b5f1a0211ef8270b92a7168dd69.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the problem of vanishing/exploding gradient of sinusoidal neural networks at initialization. Based on theoretical analysis, it argues that the prior standard initialization scheme should be adjusted by an additional factor to prevent the vanishing/exploding gradient issue. Its empirical results on an image-fitting problem and NTK analysis support this new initialization strategy."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1: The new initialization method is theoretically justified to be better in stabilizing the gradient when network depth is large. \n\n2: The efficacy of the new method is empirically verified in a real-world problem, and is also supported by the numerical analysis of the NTK matrix.\n\n3: The new initialization method is simple to implement and requires small extra computation.\n\n4: The paper is well-presented and is easy to follow."}, "weaknesses": {"value": "1: The paper claims a causal relation between the gradient control and (spurious) high-frequency modes. For example, in abstract (Line 21) and introduction (Line 50, 55). However, I don’t quite see a proof or analysis of this causal relation in the paper. It seems the most related is Eq. 12, but this equation only shows a correlation, not causality. In addition, this correlation is loose: a stable gradient not necessarily tied with no spurious high frequency, as it also depends on $\\omega_0$.\n\n2: The scope of the paper is confined to the setting of $sin$ activation function. Does it extend to commonly used activation functions, such as ReLU? I know there are some empirical results on the image-fitting problem in Figure 2 for other activations. I’d like to know whether it extends beyond this problem, such as image classification, image generation where ReLU (or similar) are much more often used than $sin$. In addition, I’d like to see whether the theory extends.\n\n3: It would be better if there is more empirical evidence, in addition to the image-fitting problem.\n\n4: The paper exceeds the page limit by approximately one-third of a page and includes an acknowledgment section that may partially reveal the authors’ identities.\n\nIf the above are addressed, I am happy to increase the rating"}, "questions": {"value": "See comments in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8zUHV3XoDu", "forum": "92d74WdgtG", "replyto": "92d74WdgtG", "signatures": ["ICLR.cc/2026/Conference/Submission18587/Reviewer_Le3d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18587/Reviewer_Le3d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588359407, "cdate": 1761588359407, "tmdate": 1762928305991, "mdate": 1762928305991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new initialization for INRs that rely on sinusoidal activation functions. Choosing the parameters of this new initialization is theoretically motivated by avoiding vanishing/exploding gradient variances of the network with respect to the network depth and width. This is done by a series of theoretical results relying on NTK and assuming it is constant at the beginning of training. The result is the avoidance of spurious frequencies and other quality degradation that occurred otherwise when increasing the depth of the network using previous types of initialization. Empirical evaluations show better generalization when using the proposed initialization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s strengths are:\n- The paper studies a relevant problem which is theoretically motivated and has practical applications.\n- The paper’s motivation and contributions are overall clear.\n- The paper makes good connections with existing literature in both its applied and theoretical derivations."}, "weaknesses": {"value": "I have a series of things to point out about the paper, which are the reason for the current score assigned to the paper.\n\n**>>About the results:**\n- It is my understanding that the SIREN initialization does not work well for large depths, and it is argued that the initialization by the authors (equation (7)) does not suffer from it. Well, this is really hard to know *for sure*: I understand that the theoretical results were derived in the large depth limit and using the NTK at initialization, but in practice, we are not in the NTK regime across training, so things could be very different. It would be important to try experiments where one keeps increasing the network’s depth using the authors’ initialization and see if there is a **tipping point** after which larger networks lead to deterioration on generalization tasks (due to spurious high frequencies or another cause). This is largely unexplored in the paper. If such tipping point is found, it will establish an important limitation on the practicality of the authors’ method. One could compare such a tipping point to the value of depth over which SIREN leads to deterioration—would such tipping point be x2, x10, etc. magnitude larger?\n- There is something which is not clear at all to me: the meaning of the expression “limit of large $N$” in Theorem 3.2. When talking about limits, either you (i) have $N\\to C$ where $C$ is a constant that is much larger than some other parameters, or you (ii) have $N\\to\\infty$. I believe Theorem 3 refers to item (i), since item (ii) does not make much sense because the equation of $\\sigma_g^2$ depends on $N$. Then, what are the other parameters that are less than the constant $C$ that I defined above? I hope my question makes sense. The expression “limit of large $N$” **makes no sense** unless **you specify what makes $N$ “large”**. This is important to know for the paper. \n- As a follow-up to the previous point: is equation (12) also under the “limit of large $N$”? If so, it would be good to make it explicit in the text and respond to my previous point — otherwise, a reader **can’t know what makes $N$ large enough for these expression to hold**.\n- Another problem is that Theorem 3.2’s  results are under the same assumptions as Theorem 3.1.—i.e., assuming the **random initialization** proposed by the authors. Thus, the results of Theorem 3.2 such as the expression of $\\sigma_g^2$ **must hold under some probability**. Can the authors clarify the probability under which these results hold? This is also very important for the formality of the results—until now, the reader can’t know if the results hold with probability $1$ or some probability dependent on problem parameters (such as the network width $n_0$, for example).\n\n**>>Motivation:**\n- In the last paragraph of the contributions in Section 1.3, it is mentioned that various results from the paper “will be valuable beyond the specific case of INR with sinusoidal activations”. Can the authors list examples of other cases beyond “INR with sinusoidal activations” for which their results are valuable and informative? All the results derived in the paper are highly specific to the setting of INR with sinusoidal activations, how is it then that the paper’s results can be transferred to other application domains?\n- Lines 175-178: why is it important to ensure differentiability in the applications mentioned therein?\n- The paper focuses on INR and SIREN, nonetheless, there is not much mention about areas (with citations) where it has been successfully applied. I see that PDE-related applications are mentioned (without providing a citation), but I am sure there are more applications of interest. This will strengthen the motivation of the paper.\n- In Section 4, a considerable assumption is made: that the NTK remains constant during training. Nonetheless, this, to my best understanding, is not something that will commonly hold in practice. The paper seems to claim this is the case, since it mentions “the early training dynamics is fully determined by the spectral properties of the NTK at initialization”. Can the authors provide references/citations or a more formal justification for such a claim? If I were to make a guess, networks with very large width are the ones that more likely will preserve the spectral properties of the NTK given at initialization after a few iterations of gradient descent with a small learning rate (but this is a guess, the authors should look at the literature or provide a good formal justification).\n- In the last paragraph of Section 5 it is mentioned that the “pre-activation variance ($\\sigma_a^2=1$) may be adapted and optimized in future work”. However, how could anyone decide to change the value of $\\sigma_a^2$ when this could possibly lead to vanishing/exploding gradients (at least close to initialization, if I understood correctly)?\n\n**>>Clarification:**\n- Line 049-051: it seems to state that it is known that there is a frontier between vanishing-gradient and exploding-gradient that one needs to fit the training in. Do you have a reference for it? How is it known (e.g., from previous literature) that “spurious high-frequency artifacts” can lead to both vanishing and exploding gradients during training?\n- Line 051: it is said that we seek a regime where “gradients remain stable”. Does “stable” mean that the norm of the gradient will be uniformly bounded below and above by a non-zero constant?\n- Figure 1: for each vertical bar, what does the difference between the two colors (dark vs light gray) represent?\n- In the example described in the paragraph starting at line 345: how many input points are used for training?\n\n**>>Other clarifications:**\n- About Section 1.2, first paragraph: it is said that SIREN has the tunable parameter $w_0$ to control the frequency range of the network—however, this is only guaranteed at initialization, nothing is known during training, correct? (Unless you are in the NTK domain). Please, clarify this in the text.\n- About Section 1.2, first paragraph: it is said that SIREN is related to other alternatives such as positional encoding and Random Fourier features. Why would somebody use SIREN instead of these other alternatives? Is it all about the calibration of $w_0$? If so, why is this so important compared to other approaches that allow for frequency control?\n- In Theorem 3.1, clarify in the last sentence that the convergence is as $L \\to \\infty$.\n\n**>>Other things:**\n- Line 039: please, provide examples to better understand “signals of high-frequency content”. I imagine learning functions that correspond to images with considerable texture could be one example, is that correct?\n- Please, specify that you consider the $l_2$-norm, e.g., in equation (4).\n- In line 254: indicate after $W_l$ that $2\\leq l \\leq L$.\n- For consistency, use “decay” instead of “converge” when referring to the small eigenvalues below line 377.\n- Please, proofread the paper for typos and grammatical errors which **should have not happened** at this point. Examples:\n  - Delete “have” and the “-d” at the end of “converged” in the last line of page 4 (below line 215).\n  - Add parenthesis after “Hinton (2010)” in line 242. Also, “Nair & Hinton (2010)” should be “(Nair & Hinton, 2010)” since it is referring to the paper itself, not the authors as subjects. For this, we can use “\\citep” in Latex. More instances of this mistake are found throughout the paper.\n  - Title of section 3.1: it says “PRE-CTIVATION”, it should be “PRE-ACTIVATION”.\n  - end of line 254: it should say “$W_1$ is sampled” instead of “let $W_1$ be sampled”.\n  - line 255: add “and” before “$b_l$”.\n  - The first word in Remark 3.2 should be “As”.\n  - Line 290: the beginning should be “Under the same assumptions as Theorem 3.1,\""}, "questions": {"value": "Please, see the Weaknesses section for most questions. One additional question:\n- What happens if in equation (7) one decides to keep the bias uniformly sampled (as in equation (6)), instead of Gaussian sampled, while keeping the rest of equation (7) the same. I wonder what kind of performance will result from it—it could be a good ablation to try."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "mYaZY2eXNh", "forum": "92d74WdgtG", "replyto": "92d74WdgtG", "signatures": ["ICLR.cc/2026/Conference/Submission18587/Reviewer_1Ux1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18587/Reviewer_1Ux1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761812763246, "cdate": 1761812763246, "tmdate": 1762928303553, "mdate": 1762928303553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "\\paragraph{Summary}\nThe paper proposes a closed-form initialization for SIREN-style sinusoidal networks. Hidden-layer weights and biases are set via two scalars \\((c_w, c_b)\\) chosen so that pre-activations converge to a fixed variance and the layerwise Jacobian entry variance satisfies\n$\n\\sigma_g^2 = \\frac{1}{N}.\n$\nSolving these constraints yields\n$\nc_w = \\sqrt{\\frac{6}{1+e^{-2}}}, \\qquad\nc_b = \\sqrt{\\frac{1}{3}}\\, c_w e^{-1}\n\\quad \\text{(Eqs.~(7)--(8)).}\n$\nExperiments on 1D/2D/3D synthetic functions and a single image-fitting task suggest improved “generalization” (via upsampled reconstructions) over baselines. NTK measurements indicate the mean NTK eigenvalue grows roughly linearly with depth under the proposed initialization, versus exponential growth."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A simple, analytical initialization for sine activations; easy to implement (Eqs. (7)–(8)). \n\n- Clear fixed-point analysis for pre-activation variance using Lambert-W; explicit gradient-variance target. \n\n- Useful NTK-based depth scaling narrative; the paper measures trace/eigenvectors and links depth to speed. \n\n- Repeated small-scale experiments show consistent wins on their metrics."}, "weaknesses": {"value": "- The Jacobian-variance factorization assumes $W_\\ell \\perp z_\\ell$ even though $z_\\ell = W_\\ell h_{\\ell-1} + b_\\ell$. Without a large-width justification (e.g., leave-one-out / tensor-program) the claim that targeting $\\sigma_g^2 = 1/N$ stabilizes gradients is fragile.\n- Matching entrywise Jacobian variance does not ensure favorable singular-value spectra of end-to-end Jacobians (dynamical isometry). No SVD/condition-number results are reported.\n- No correlation recursion or $\\chi$-coefficient/fixed-point correlation study to substantiate an EoC claim beyond gradient-variance targeting.\n- Mostly small synthetic INR tasks and a single image; no NeRF/PINN or noisy/irregular sampling settings. Image evaluation uses upsampled reconstructions instead of standard PSNR/SSIM."}, "questions": {"value": "- Can you justify or bound $\\mathrm{Cov}(W_{\\ell,ik}, z_{\\ell,i})$ as width $N$ grows? A tensor-program or leave-one-out derivation—or empirical covariance vs.\\ $N$—would strengthen Theorem~3.2.\n - Please report singular-value distributions (or spectral norms/condition numbers) of end-to-end Jacobians across depths and widths for multiple inputs, to demonstrate stability beyond entrywise variance.\n- Beyond the trace, provide the NTK eigenvalue spectrum vs.\\ depth (including tails and condition numbers) and analyze eigenvector frequency alignment to support claims about depth-wise learning dynamics.\n - Add kernel-bandwidth SIREN inits and residual/skip/normalized SIREN variants. Include realistic INR settings (e.g., NeRF or PINNs) with standard metrics (PSNR/SSIM/relative $L_2$).\n- Evaluate under noisy/irregular sampling, distribution shift, and different input scalings to test whether the fixed-point targeting remains effective.\n- Will you release code, seeds, and exact training logs? A concise “How to initialize” box in the main text would aid adoption."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "62Eeumlgmh", "forum": "92d74WdgtG", "replyto": "92d74WdgtG", "signatures": ["ICLR.cc/2026/Conference/Submission18587/Reviewer_s653"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18587/Reviewer_s653"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761823651335, "cdate": 1761823651335, "tmdate": 1762928303046, "mdate": 1762928303046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a theoretically grounded initialization scheme for SIRENs that enforces:\n (1) the preservation of the pre-activation distribution across layers, and\n (2) stable gradients throughout the network (avoiding vanishing or exploding gradients).\n\nThe authors evaluate their method on image-fitting tasks and demonstrate stable training in deep SIRENs. Furthermore, they leverage Neural Tangent Kernel (NTK) theory to show that the proposed initialization avoids the introduction of spurious high-frequency components at the start of training.\n\nTraining deep SIRENs is a challenging problem with several applications in implicit neural representations (INRs). I tested the proposed initialization on image-fitting experiments and found it promising. However, a straightforward extension to SDF fitting did not yield good results, suggesting that this adaptation may not be trivial."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and well organized.\n\nThe proposed initialization is supported by NTK theory, which provides a solid theoretical foundation.\n\nThe empirical results indicate robustness in fitting tasks.\n\nImproving the trainability of deep SIRENs has significant implications for a wide range of INR applications."}, "weaknesses": {"value": "[L53–54] It is unclear whether the authors refer to (1) the gradient of the loss function with respect to network parameters or (2) the derivative of the network output with respect to input coordinates. In L212, it seems to refer to (2). This point is addressed in Eqs. 10–11.\nCould the authors include a specific example where gradients explode under the standard SIREN initialization but remain stable with the proposed one? In Fig. 1, both methods seem to yield noisy reconstructions. Including analytical gradient visualizations (as in TUNER) would provide a clearer comparison.\n\n[L107] TUNER [1] is a closely related work proposing frequency-aware initialization for SIRENs. It also offers a formula that could explain the emergence of high-frequency artifacts when increasing network depth [L43]. I recommend including it in the comparisons. Additionally, FINER [3] is another recent method addressing similar initialization and spectral control challenges.\n\n[L132] Evaluation is restricted to RGB image fitting with an MSE loss (Eq. 4).\n\n[L144] INRs are generally trained to represent low-dimensional signals (e.g., 2D images, SDFs, occupancy, or radiance fields), often including a regularization term (e.g., the Eikonal constraint for SDFs).\nIt would strengthen the paper to show results for SDF fitting or at least discuss how the proposed initialization behaves under such regularization. I attempted to implement the initialization for SDF reconstruction using the repository from “Exploring Differential Geometry in Neural Implicits” and observed poor convergence.\n\n[L90] Yüce et al. [2] should be cited, as they also analyze SIRENs through the NTK framework.\n\n[L159] Mention explicitly that \\theta denotes the union of all \\theta_i​’s.\n\n[Fig. 2] A 10-layer MLP with 256 neurons per layer is quite large for image fitting. Testing smaller architectures (e.g., 16 or 32 neurons) would help assess scalability."}, "questions": {"value": "[L25] When stating that the new initialization “consistently outperforms state-of-the-art methods,” which methods are considered SOTA? If only Xavier, Kaiming, and SIREN, this comparison is limited, these are not typically used for signal representation. Please also consider TUNER [1], FINER [3], and [4].\n\n[L47] Does the proposed initialization accelerate training convergence?\n\nHow does the method perform on high-frequency audio fitting, where deeper architectures might be more beneficial?\n\n[L188] Does the initialization assume identical hidden-layer widths (same N) across the network?\n\n\n[1] Novello et al., Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks, CVPR 2025.\n\n[2] Yüce et al., A Structured Dictionary Perspective on Implicit Neural Representations, CVPR 2022.\n\n[3] Liu et al., FINER: Flexible Spectral-Bias Tuning in Implicit Neural Representation by Variable-Periodic Activations, CVPR 2024.\n\n[4] Yeom et al., Fast Training of Sinusoidal Neural Fields via Scaling Initialization, ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bVtzSkswPL", "forum": "92d74WdgtG", "replyto": "92d74WdgtG", "signatures": ["ICLR.cc/2026/Conference/Submission18587/Reviewer_K6uc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18587/Reviewer_K6uc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18587/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937707778, "cdate": 1761937707778, "tmdate": 1762928302633, "mdate": 1762928302633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}