{"id": "svyjoTT47M", "number": 6312, "cdate": 1757966860371, "mdate": 1759897922870, "content": {"title": "A Benchmark for Vericoding: Formally Verified Program Synthesis", "abstract": "We present and test the largest  benchmark for *vericoding*, LLM-generation of formally verified code from formal specifications --- in contrast to *vibe coding*, which generates potentially buggy code from a natural language description. Our benchmark contains 12,504 formal specifications, with 3,029 in Dafny, 2,334 in Verus/Rust and 7,141 in Lean. Of these, 6,174 are new unseen problems. We find vericoding success rates of 27\\% in Lean, 44\\% in Verus/Rust and 82\\% in Dafny using off-the-shelf LLMs. Adding natural-language descriptions does not significantly improve performance. We also find that LLM progress has improved progress on pure Dafny verification from 68\\% to 96\\% over the past year.", "tldr": "We present and test a benchmark for *vericoding*, AI-generation of formally verified code from formal specifications --- in contrast to *vibe coding*, which generates potentially buggy code from a natural language description.", "keywords": ["Formal Verification", "Program Synthesis", "Benchmark", "LLM", "Verus", "Dafny", "Lean"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/62bed8ce9c92410224921b317f33dcd82872787c.pdf", "supplementary_material": "/attachment/6b9422a9c8fb5beba0ce8706118d5d033d667f64.zip"}, "replies": [{"content": {"summary": {"value": "The paper's main contribution is to introduce a benchmark for \"vericoding\" (defined in this paper as generating code + proof given a formal spec). Data examples in this benchmark were curated by pulling together existing vericoding benchmarks in different formal languages (Lean, Dafny, Verus) and using LLMs to translate between them, as well as autoformalizing coding benchmarks such as APPS. The authors also evaluated many off-the-shelf LLMs such as GPT-5 on the new benchmark and performed analysis on the results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "I agree with the high-level motivation of this work. Generating formally verifiable code is important and will be more important in the near future as LLMs become better at code generation. Well-constructed benchmarks in this space will unleash research progress."}, "weaknesses": {"value": "However, I'm not convinced by the quality of the proposed benchmark and the method for constructing it. Specifically:\n\n* Most examples in the benchmark were either directly from existing benchmarks in different languages (Dafny, Lean, Verus) or obtained by using LLMs to translate between them. It is not clear what value the new benchmark brings or why would a subsequent work evaluate on this new benchmark instead of on multiple existing benchmarks individually.\n\n* The quality of the benchmark is not clear. Only a subset of the translated problems are manually inspected by humans. Many translated specs may be too weak (Line 370). The authors argue that perfect translation is not required; as long as a spec is syntactically valid, it can potentially be a useful task for training or evaluation. I agree the translation doesn't need to be perfect, but it needs to above a certain level for the benchmark to be useful. \n\n* The new benchmark's data sources (CLEVER, Verina, HumanEval, etc.). For example, CLEVER translated HumanEval into Lean. It's not clear why the authors attempted to translate HumanEval again. \n\n* This paper uses \"Vibe coding\" to refer to the dominant approach to AI code generation that does not involve formal methods. For example, in Line 24, the authors claim that Google is using \"vibe coding\" to generate over 30% of its software, which sounds astonishing. However, what the cited report actually says is over 30% of the code was generated by AI and accepted by human engineers. The research on AI for code generation has a long history and shouldn't be referred to as \"vibe coding\" (a term coined by Andrej Karpathy in a tweet in 2025 and is often used in the context of LLM coding agents such as Claude Code) in a serious academic publication.\n\n * The paper introduced a new term \"vericoding\" to refer to generating code and proof given a formal spec. This task is usually called verified (or verifiable) code generation in recent papers (CLEVER, FVAPPS, Verina, etc.). Before that, program synthesis from formal specification is a well-established task in the PL community."}, "questions": {"value": "* Why is FVAPPS categorized as a \"vibe coding\" benchmark instead of vericoding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lGxEGb0dnk", "forum": "svyjoTT47M", "replyto": "svyjoTT47M", "signatures": ["ICLR.cc/2026/Conference/Submission6312/Reviewer_JR7g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6312/Reviewer_JR7g"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760840511009, "cdate": 1760840511009, "tmdate": 1762918610928, "mdate": 1762918610928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark set for vericoding, which is the task of synthesizing formally verified code (including proofs) from formal specifications. The benchmark set contains benchmarks for Dafny, Verus, and Lean. The benchmarks are generated from multiple sources:\n1. cross translation into target languages from existing verification benchmarks; \n2. auto-formalizing vibe-coding benchmarks;\n3. auto-formalizing documentation of libraries such as NumPy and BigNum.\n\nThe paper describes several ways employed to ensure the quality of the benchmarks. The paper also includes a thorough evaluation of a range of LLMs on this benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- A comprehensive vericoding benchmark set is good to have, given that there has been significant research interest in this area.\n- The paper uses iterative refinement coupled with verification to improve the success rate of the translation.\n- The paper identifies several additional potential issues that may hamper the quality of the benchmark, and describes ways to mitigate them.\n- The experimental evaluation is quite thorough and gives a nice overview of the current landscape of LLM capability in vericoding."}, "weaknesses": {"value": "- The language-to-language generation process is LLM-based, which is a little sloppy and itself error-prone. I understand that translating proofs between theorem provers can be challenging (and, anyway, verification can ensure the soundness of the proof), but I wonder whether translating the specifications can be done in a more rigorous manner, using analytical approaches. \n- Although the paper seems to have spent non-trivial effort to ensure the quality of the benchmarks, the quality of the benchmarks is still not close to being perfect (9% weak specs and 15% incorrect translation)."}, "questions": {"value": "Could you discuss the challenge of translating specifications between theorem provers in a formal manner?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o3H0h8RKse", "forum": "svyjoTT47M", "replyto": "svyjoTT47M", "signatures": ["ICLR.cc/2026/Conference/Submission6312/Reviewer_vsVC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6312/Reviewer_vsVC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959392539, "cdate": 1761959392539, "tmdate": 1762918610421, "mdate": 1762918610421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a substantial vericoding benchmark, encompassing 12,504 formal specifications. These specifications are divided among Dafny (3,029), Verus (2,334), and Lean (7,141), with 6,174 being newly developed problems. Vericoding distinguishes itself from end-to-end generation by requiring Large Language Models (LLMs) to generate both implementations and their corresponding proofs from existing formal specifications, rather than inferring the specifications themselves. Current frontier LLMs demonstrate varying success rates: 82% for Dafny, 44% for Verus, and 27% for Lean."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Unprecedented Scale: With 12,504 tasks, this benchmark is two orders of magnitude larger than existing verification benchmarks, addressing a significant gap in the field.\n- Comprehensive Language Coverage: It includes both Automated Theorem Provers (ATPs) like Dafny and Verus, and Interactive Theorem Provers (ITPs) such as Lean, thereby enabling cross-paradigm comparisons."}, "weaknesses": {"value": "- Duplication Concerns: Figure 5 highlights the presence of near-duplicate tasks, which are essentially the same problem with minor variations. This issue artificially inflates the benchmark's size and reduces its overall diversity.\n- Limited Quality Validation: The study only examines 5 tasks per model and source, indicating a lack of systematic evaluation regarding the quality of the benchmark tasks.\n- Insufficient Evaluation and Analysis: Figure 4's analysis is restricted to length metrics, which are obvious predictors. The paper lacks investigation into crucial aspects such as: factors contributing to task difficulty (e.g., proof complexity, theorem depth), reasons behind different models' varying performance across languages."}, "questions": {"value": "- Is there a plan to release a cleaned version of the benchmark that addresses the 24% of problematic specifications?\n- How is the inclusion of 98.4% near-duplicate tasks justified, and shouldn't these be deduplicated?\n- Is every task in the benchmark provable, meaning each has a ground-truth code and proof?\n- Can the manual inspection process be expanded beyond the current 5 samples, which represent less than 1% coverage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Lo4CUbEo6H", "forum": "svyjoTT47M", "replyto": "svyjoTT47M", "signatures": ["ICLR.cc/2026/Conference/Submission6312/Reviewer_4LEx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6312/Reviewer_4LEx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762043558055, "cdate": 1762043558055, "tmdate": 1762918609891, "mdate": 1762918609891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a comprehensive and large benchmark for the automatic generation of formally verified programs given formal specifications. The benchmark spans three verification frameworks (Dafny, Verus, and Lean), covering both automated theorem provers (ATPs) and interactive theorem provers (ITPs). It aggregates and translates tasks from multiple prior sources (e.g., DafnyBench, Verina, FVAPPS, HumanEval) into a unified suite of 12,504 tasks, of which 6,174 are new. The authors evaluate a broad range of LLMs (GPT-5, Claude, Gemini, etc.) on this benchmark, achieving best results in Dafny (82.2% success) and lower rates for Verus (44.3%) and Lean (26.8%). The study aims to provide a foundation for measuring LLM progress in generating provably correct code."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Introduces a large-scale dataset on a timely and impactful topic—AI-assisted formal verification.\n2. Covers diverse verification frameworks (Dafny, Verus, Lean), enabling cross-system comparisons between ATP and ITP paradigms.\n3. Provides comprehensive LLM evaluations, revealing quantitative differences across languages and models.\n4. Transparent methodology with anti-cheating validation and manual inspection reports.\n5. Clear writing."}, "weaknesses": {"value": "1. Conceptual limitation of the “vericoding” definition: The paper defines vericoding narrowly as code and proof generation from formal specifications, explicitly excluding specification generation. However, specification writing is the main bottleneck in formal verification and a key component of prior benchmarks such as CLEVER and VERINA. By ignoring spec generation, this benchmark captures only a partial and arguably easier slice of the full verified-coding problem. This weakens the practical and conceptual impact of the work.\n\n2. I find the benchmark quality to be weak, for reasons detailed below. This is a major limitation, especially because the work targets formal verification—a domain that demands the highest level of rigor. If the benchmark itself includes incomplete or incorrect specifications, it cannot faithfully measure models’ verification capabilities and may even lead to misleading or unsafe outcomes if such models are later used in production settings.\n    - The spec translation and validation rely on an LLM judge to assess semantic equivalence, which may not guarantee correctness. The verifier only ensures syntactic validity.\n    - I appreciate the authors’ manual inspection and being upfront about weak specification or other problems with the benchmark. However, the description reads vague to me: “Verus showed *many* weak specs due to spec translation issues, e.g. deviations from the original BigNum specs in Dafny. Lean displayed *occasional* redundant lemma additions during vericoding, and had *some* weak specs from the original sources.”. Could you provide quantitative measures? \n    - The authors also did not seem to talk about how specifications identified as low quality are processed. Are they still included in the benchmark, or are they fixed? My impression is that they are still included.\n    - The current quality score of the benchmark is a weighted average of several defined criterias, which only consider obvious cases (including defaults, sorry defs, ghost types, duplication), but do not measure the semantic quality of the specification.\n\n3. The evaluation settings are unclear and lack important baselines.\n    - The paper does not specify how many refinement iterations are allowed, what error messages are fed back, or whether these differ across verification frameworks. Moreover, how do these choices impact performance?\n    - Only general-purpose LLMs are tested. No experiments with proof-specialized approaches (e.g., DeepSeek-Prover-V2 [1], Goedel-Prover-V2 [2], SAFE [3], AutoVerus [4], AlphaVerus [5]) are included.\n\n4. Overall, the benchmark emphasizes scale, but size alone does not determine effectiveness. Beyond quality, a benchmark should be diverse enough to cover a broad spectrum of difficulty levels to meaningfully differentiate model capabilities. In fact, without such diversity, a large benchmark can even become a costly burden for evaluation, obscuring rather than clarifying model progress. In this regard, I think the paper is doing a good job in including both ATPs and ITPS. However, In Table 2, although multiple benchmark sources are combined, the paper does not discuss the degree of overlap among these sources or the relative difficulty of the tasks they contribute. It is therefore unclear whether the enlarged dataset truly expands task diversity or merely aggregates similar problems, which raises questions about how well the benchmark can measure real advances in verification performance. For example, in Table 3, the performance trend on the Verina subset closely mirrors the overall average, suggesting limited incremental value. If that is the case, why not evaluate directly on the Verina subset instead?\n\n[1] Ren, Z. Z., et al. \"Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition.\" arXiv preprint arXiv:2504.21801 (2025).\n\n[2] Lin, Yong, et al. \"Goedel-prover-v2: Scaling formal theorem proving with scaffolded data synthesis and self-correction.\" arXiv preprint arXiv:2508.03613 (2025).\n\n[3] Chen, Tianyu, et al. \"Automated proof generation for rust code via self-evolution.\" ICLR 2025.\n\n[4] Yang, Chenyuan, et al. \"Autoverus: Automated proof generation for rust code.\" Proceedings of the ACM on Programming Languages 9.OOPSLA2 (2025): 3454-3482.\n\n[5] Aggarwal, Pranjal, Bryan Parno, and Sean Welleck. \"Alphaverus: Bootstrapping formally verified code generation through self-improving translation and treefinement.\" ICML 2025."}, "questions": {"value": "Please address my comments in the “Weakness” section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SKotKqCKG6", "forum": "svyjoTT47M", "replyto": "svyjoTT47M", "signatures": ["ICLR.cc/2026/Conference/Submission6312/Reviewer_dMe1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6312/Reviewer_dMe1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6312/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248095435, "cdate": 1762248095435, "tmdate": 1762918609618, "mdate": 1762918609618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}