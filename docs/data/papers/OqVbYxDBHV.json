{"id": "OqVbYxDBHV", "number": 22072, "cdate": 1758325603130, "mdate": 1759896887962, "content": {"title": "Where do Large Vision-Language Models Look at when Answering Questions?", "abstract": "Large Vision-Language Models (LVLMs) have shown promising performance in vision-language understanding and reasoning tasks. However, whether they truly understand the input image remain underexplored. A fundamental question arises: to what extent do LVLMs rely on visual input, and which image regions contribute to their responses? It is non-trivial to interpret the free-form generation of LVLMs due to their complicated visual architecture (e.g., multiple encoders and multi-resolution) and variable-length outputs. In this paper, we extend existing heatmap visualization methods for classification tasks to support LVLMs for open-ended visual question answering. We propose a method to select visually relevant tokens that reflect the relevance between generated responses and the input image.\nFurthermore, we conduct a comprehensive analysis of state-of-the-art LVLMs on benchmarks designed to require visual information to answer. Our findings offer several insights into LVLM behavior, including the relationship between focus region and answer correctness, differences in visual attention across architectures, and the impact of LLM scale on visual understanding. The code and data will be\nreleased", "tldr": "", "keywords": ["VLM", "interpretation"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4355162476e9728b00b5555188b568686c0c8317.pdf", "supplementary_material": "/attachment/76927e020451520af77819ec0747c0642709b96d.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces a general, architecture-aware interpretability framework for LVLMs generation. It first identifies vision-relevant tokens in the generated answer via a log-likelihood ratio between the real image and a blurred “no-vision” baseline, then optimizes a single image-space mask for heatmap. Evaluated with insertion/deletion metrics across several benchmarks and model families, the method yields stronger, more faithful attributions and reveals several interesting insights."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Empirical study is thorough. Strong baselines, clearly defined insertion/deletion metrics, and targeted ablations that convincingly attribute gains to the proposed components.\n\n2. Architecture-aware methodology. Unified pre-encoder masking and differentiable multi-resolution handling, with a single image-space iGOS++ mask that works across LVLM variants.\n\n3. The insights are quite provocative with research motivation and questions clearly defined and solved. Pattern analysis among different LVLMs are also discussed."}, "weaknesses": {"value": "> Experiments\n\nThe paper relies on a single metric family (insertion/deletion). Please clarify its robustness—in particular, sensitivity to sampling, baseline type, etc. Thus, Table 2 should report mean and standard deviation to prove superiority.\n\nIn addition, is this evaluation metric causal? By this I mean if it's possible to add some experiments to show what parts are actually causally emphasized in LVLM generation?\n\nAll current comparative baselines treat the LVLM as a white box. It would strengthen the evaluation to include black-box, perturbation-based baselines (e.g., D-RISE [1])\n\n[1] Petsiuk, Vitali, et al. \"Black-box explanation of object detectors via saliency maps.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021."}, "questions": {"value": "1. Why the result in Appendix A.2 is not aligned with human attention? It seems not intuitive.\n\n2. Given your finding that visual grounding does not improve with larger LLMs, what method can solve this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ktKRnfAGTE", "forum": "OqVbYxDBHV", "replyto": "OqVbYxDBHV", "signatures": ["ICLR.cc/2026/Conference/Submission22072/Reviewer_b64Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22072/Reviewer_b64Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761493719754, "cdate": 1761493719754, "tmdate": 1762942049214, "mdate": 1762942049214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper adapts existing decision localization/visualization methods (GradCAM, iGOS++) for use with large vision-language models (LVLMs) in open-ended visual question answering, and leverages them to study how LVLMs attend to images when answering questions. Since these methods require a single prediction score, the paper proposes a token selection strategy which selects the tokens with high loglikelihood ratio (normalized by their loglikelihood given an neutral input image), and then computes the sum of the loglikelihood of these selected tokens as the prediction score. The paper also introduces a regularization term to the optimization objective of iGOS++. Several experiments are provided to compare the different decision localization methods, and explore how well LVLMs look at the correct regions in images."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper adapts existing decision localization/visualization methods (GradCAM, iGOS++) for use with large vision-language models (LVLMs) in open-ended visual question answering, and leverages them to study how LVLMs attend to images when answering questions. Since these methods require a single prediction score, the paper proposes a token selection strategy which selects the tokens with high loglikelihood ratio (normalized by their loglikelihood given a neutral input image), and then computes the sum of the loglikelihoods of these selected tokens as the prediction score. The paper also introduces a regularization term to the optimization objective of iGOS++. Several experiments are provided to compare the different decision localization methods and explore how well LVLMs look at the correct regions in images."}, "weaknesses": {"value": "1. The comparison in Table 2 (and consequently the choice of iGOS++ and deletion/insertion metrics for the rest of the experiments) is incorrect. The problem is that iGOS++ directly optimizes the deletion/insertion metrics on test images, and then compares these values to methods that do not have this privilege (Grad-CAM etc). Note that directly optimizing any metric on test images will result in better values for that metric, but what actually matters is whether any generalization can be achieved beyond the optimized metric. For example, the paper's proposed optimization for the insertion metric could easily discover adversarial patterns on any test image that when inserted on a blurry background image will boost the likelihood of the LLM's prediction. In such cases, the \"adversarial\" interpretation would be of little use/meaning despite the high selection metric score, and so the paper must show improvement on an independent metric (for example object localization metrics, or overall accuracy as in [1]).\n\n2. The paper misses several very relevant published papers in its related works. [1,2] have studied whether LVLMs attend to images. [3,4] have provided methods for decision localization/visualization in LVLMs.\n\n3. The paper’s main findings are already established in previous work (“Q1: Do LVLMs rely on the input image when answering visual questions?” is answered in papers [1,2,3] and “Q2: Where do different LVLMs attend when generating variable-length responses? Q3: What is the relationship between answer correctness and focus region?“ is answered in [3,4]), so the paper’s novelty is limited to its proposed token selection method.\n\n4. The paper provides no statistical confidence intervals for the metrics, so it is unclear how meaningful the small differences are. Some p-values are reported in the last section (lines 470-473), but the paper does not discuss how these values are computed (what statistical test, sample size, etc).\n\n[1] The Instinctive Bias: Spurious Images lead to Illusion in MLLMs. EMNLP 2024.\n\n[2] Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly. CVPR 2025.\n\n[3] MLLMs Know Where to Look: Training-Free Perception of Small Visual Details with Multimodal LLMs. ICLR 2025.\n\n[4] V*: Guided Visual Search as A Core Mechanism in Multimodal LLMs. CVPR 2024."}, "questions": {"value": "1. I suggest using a simpler object detection or VQA metric for comparing and validating the correctness of the localization methods.\n2. Clarify novelty compared to several related published papers mentioned in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mAk4fec5gi", "forum": "OqVbYxDBHV", "replyto": "OqVbYxDBHV", "signatures": ["ICLR.cc/2026/Conference/Submission22072/Reviewer_m8ag"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22072/Reviewer_m8ag"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790022841, "cdate": 1761790022841, "tmdate": 1762942048759, "mdate": 1762942048759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method to extend traditional heatmap visualization techniques (e.g., GradCAM, iGOS++) to support Large Vision-Language Models (LVLMs) in open-ended visual question answering tasks. The key contribution is a visually relevant token selection mechanism that identifies tokens most dependent on image content through a log-likelihood ratio (LLR) between visual and blurred-image inputs. This approach enables the generation of interpretable visual heatmaps and facilitates analysis of how LVLMs attend to image regions when producing answers. The authors analyze multiple state-of-the-art LVLMs, like LLaVA-1.5, LLaVA-OneVision, and Cambrian across recent benchmarks including CV-Bench, MMStar, and MMVP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper is clearly written, well structured, and easy to follow. \n2.\tLVLM interpretability is a critical and underexplored topic, especially considering the success of models such as LLaVA, InstructBLIP, Gemini, and GPT-4o, and the concerns raised by benchmarks like MMVP, MERLIM, AMBER, POPE, and HallusionBench regarding hallucinations and insufficient visual grounding. The proposed tool can help diagnose whether such errors stem from limited grounding capacity or architectural constraints that prevent LVLMs from fully leveraging visual information.\n3.\tThe approach effectively identifies which tokens rely on visual grounding and visualizes how this information is used via interpretable heatmaps. The paper includes several qualitative examples that illustrate the findings, supported by the corresponding mathematical derivations.\n4.\tThe study extends and compares multiple heatmap-based interpretability methods, including GradCAM, T-MM, IIA, and iGOS++, providing a useful empirical reference for future LVLM analysis."}, "weaknesses": {"value": "1.\tMy main concern is the lack of novelty. Although this paper addrees a relevant topic, the method primarily combines existing components (heatmap visualization + token selection) rather than introducing a fundamentally new paradigm. The LLR-based token selection is a natural extension of standard likelihood analysis, and while the integration is non-trivial, it could be seen as incremental."}, "questions": {"value": "1. It would be insightful to analyze the visual attention (heatmap) associated with hallucinated or spurious tokens to better understand whether these hallucinations arise from incorrect grounding or over-reliance on textual priors. Could you provide such an analysis?\n2.\tThe hyperparameters α and λ seem potentially architecture-dependent, but the paper does not report how they behave across different LVLMs. Have you studied their sensitivity or generalization across models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xdgkf8z6aB", "forum": "OqVbYxDBHV", "replyto": "OqVbYxDBHV", "signatures": ["ICLR.cc/2026/Conference/Submission22072/Reviewer_crB2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22072/Reviewer_crB2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852162062, "cdate": 1761852162062, "tmdate": 1762942048534, "mdate": 1762942048534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper dives into a key question about large vision-language models: where do they focus on images when answering questions? Since existing heatmap visualization methods mostly work for classification tasks, the authors extended these tools to handle VLMs' variable-length, autoregressive outputs. The authors propose a method to identify \"visually relevant tokens\" from model responses. Using this method, they tested state-of-the-art LVLMs like LLaVA-1.5, LLaVA-OV, and Cambrian on vision-centric datasets. The results showed that these models do rely on images for over 75% of cases, different vision architectures lead to distinct attention patterns, and simply making the LLM bigger doesn’t change how the model looks at images much. They also found that even when models get answers wrong, their focus regions often relate to the question, and sometimes, correct answers come from irrelevant image parts. These findings further highlight generalization challenges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method fixes a gap for interpreting VLMs at the sentence level by filtering out the language stuff that doesn’t need images; the heatmaps make way more sense. \n\n2. The authors conduct experiments on different LVLMs with different setups and multiple vision-heavy datasets. These experiments make their conclusion more reliable. \n\n3. The generalization of the proposed method works with different heatmap tools. This makes it easy for others to follow."}, "weaknesses": {"value": "1. Although the authors provide several models in the experiment part, they lack the results on the frontier open-source model, like Qwen3-VL. Showing that these models maintain the same property as previous models is important for others to understand these models. Also, results on larger models should be discussed.\n\n2. The authors mainly provide results on some perception benchmarks. I would also like to know the phenomenon of these models on some reasoning benchmarks and high-resolution perception benchmarks. Incorporating these results will strengthen the overall paper.\n\n3. More analysis of visual encoder should be included. New visual encoders do not use fixed resolution, like in Qwen2-VL and newer model. Will these new models perform differently as they improve visual perception?"}, "questions": {"value": "As stated in the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jVHYtkQpAp", "forum": "OqVbYxDBHV", "replyto": "OqVbYxDBHV", "signatures": ["ICLR.cc/2026/Conference/Submission22072/Reviewer_eJZt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22072/Reviewer_eJZt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22072/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995227733, "cdate": 1761995227733, "tmdate": 1762942048318, "mdate": 1762942048318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}