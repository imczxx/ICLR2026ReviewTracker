{"id": "UNeL5NdLzc", "number": 10528, "cdate": 1758174700786, "mdate": 1759897645620, "content": {"title": "Arbitrary-Shaped Image Generation via Spherical Neural Field Diffusion", "abstract": "Existing diffusion models excel at generating diverse content, but remain confined to fixed image shapes and lack the ability to flexibly control spatial attributes such as viewpoint, field-of-view (FOV), and resolution. \nTo fill this gap, we propose Arbitrary-Shaped Image Generation (ASIG), the first generative framework that enables precise spatial attribute control while supporting high-quality synthesis across diverse image shapes (e.g., perspective, panoramic, and fisheye).\nASIG introduces two key innovations: (1) a mesh-based spherical latent diffusion to generate a complete scene representation, with seam enforcement denoising strategy to maintain semantic and spatial consistency across viewpoints; and  (2) a spherical neural field to sample arbitrary regions from the scene representation with coordinate conditions, enabling distortion-free generation at flexible resolutions. \nTo this end, ASIG enables precise control over spatial attributes within a unified framework, enabling high-quality generation across diverse image shapes. Experiments demonstrate clear improvements over prior methods specifically designed for individual shapes.", "tldr": "ASIG learns a spherical latent with diffusion and uses a spherical neural field to control FOV, viewpoint, and resolution, enabling high-quality perspective/panorama/fisheye/irregular synthesis from a single model.", "keywords": ["Diffusion Models", "Image Generation", "Spherical Neural Field"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/840786222f9ba2468a81704eedaa0ec645a25279.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Arbitrary-Shaped Image Generation (ASIG), whose core idea is to use the latent of an implicit field representation as the denoising target, enabling resolution- and field-of-view-independent image generation. The method introduces Mesh-based Spherical Latent, Seam-aware Padding, and Seam Enforcement Denoising for the design of the diffusion representation, model, and network architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes an interesting framework for resolution-independent image generation. Both the paper and the framework are easy to follow and understand.\n2. Although using implicit representations to handle varying resolutions is an effective and commonly adopted approach, the representation, network, and model design proposed in this paper are well-motivated and meaningful.\n3. The paper provides detailed and comprehensive quantitative experiments to demonstrate the effectiveness of the model, achieving state-of-the-art performance on multiple tasks, including panoramic, perspective, and fisheye image generation."}, "weaknesses": {"value": "1. Considering that DiT-based architectures are currently more mainstream and have demonstrated superior performance, using a U-Net–style architecture might be somewhat outdated. The authors could further explore attention-based operators under their proposed representation to enrich the paper’s content and potentially improve model performance.\n2. One major advantage of implicit representations, beyond enabling resolution-independent inference, is that during training they could also leverage diverse types of images (e.g., standard perspective images) to augment the dataset and improve model generalization. The paper currently lacks exploration in this direction, but I believe this could be a promising avenue for further performance improvement.\n3. Although the operators designed for the spherical grid are reasonable, and the authors cleverly convert them into regular tensors, I question how practical these methods are for real-world implementation and deployment, especially since the authors have not released the model code."}, "questions": {"value": "Please see the weaknesses.\n\nOverall, my assessment of this paper is positive. I look forward to the authors’ responses to my concerns and remain open to the possibility of raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CejcQ32MM4", "forum": "UNeL5NdLzc", "replyto": "UNeL5NdLzc", "signatures": ["ICLR.cc/2026/Conference/Submission10528/Reviewer_6hfe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10528/Reviewer_6hfe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761574478313, "cdate": 1761574478313, "tmdate": 1762921807352, "mdate": 1762921807352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASIG, a diffusion-based framework for arbitrary-shaped image generation, which unifies viewpoint, field-of-view (FOV), and resolution control within a single model. ASIG combines two main components: (1) Mesh-based spherical latent diffusion representing the scene over a subdivided icosahedron mesh with a seam-aware padding (SAP) mechanism to ensure spatial consistency. (2) Spherical neural field (SNF) – decoding the spherical latent representation into images with arbitrary projections (perspective, panoramic, fisheye) via coordinate-conditioned sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Solid Technical Design. The authors designed a new representation for the spherical diffusion model and achieved arbitrary viewpoint and resolution generation through improved VAE.\n\n2. The unification of perspective, panorama, and fisheye generation in one framework is interesting."}, "weaknesses": {"value": "1. Seam-aware padding is a clever design, but it seems to significantly increase the complexity of the model during computation.\n\n2. Although SAP and SR blocks are ablated, the contribution of the mesh subdivision is not studied. One thing I'm curious about is what advantages this representation method has compared to many commonly used Cube representation methods? It seems that many of the claimed advantages are due to the addition of control signals in the VAE decoding process, rather than this representation method itself.\n\n3. Lack of comparison with some of the latest methods, such as LayerPano3d[1] and PAR[2] :\n\n[1] LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation\n[2] Conditional panoramic image generation via masked autoregressive modeling"}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RnBglpSuec", "forum": "UNeL5NdLzc", "replyto": "UNeL5NdLzc", "signatures": ["ICLR.cc/2026/Conference/Submission10528/Reviewer_L4Gf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10528/Reviewer_L4Gf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825269199, "cdate": 1761825269199, "tmdate": 1762921806895, "mdate": 1762921806895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ASIG, a novel diffusion framework designed to overcome the limitations of existing models by enabling explicit control over viewpoint, FOV, and resolution in a unified system. The method first generates a complete spherical scene representation via a mesh-based latent diffusion, and then uses a spherical neural field to sample and render arbitrary-shaped images (e.g., perspective, panoramic). The quantitative and qualitative results shows clear improvements over current methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. ASIG enables explicit control over viewpoint, FOV, and resolution within a unified system. Both quantitative and qualitative results confirm that the method outperforms existing approaches. \n\n2. The authors' methodology is reasonable. ASIG constructs a spherical latent space by linking UNet principles with an $L$-subdivided icosahedron, and the proposed seam-aware padding (SAP) and spherical neural field components are effective in achieving excellent results."}, "weaknesses": {"value": "1. Some ablation results are confusing.\n\n\ta. The reported results for ASIG in Table 2 are inconsistent with the corresponding results in Table 1.\n\n\tb. Furthermore, the ASIG results presented in Table 2 also differ from those in Table 3 and Table 4.\n\n2. The Sampler component is underspecified.\n\n\ta. The Sampler appears to integrate two distinct information sources: a 'remapped mesh-structure' and 'information processed by SAP'. It is unclear how these sources are fused or prioritized. Is there conflict between them and how they merge together?\n\n\tb. The process for inputting the 'sampling region' information is not detailed. Please elaborate on how this region is defined and fed into the sampler.\n\n3. The manuscript's clarity needs significant improvement, as several sections are difficult to understand.\n\n\ta. The explanation for Equation 3 + line 204 is insufficient. The authors should provide a more detailed derivation or justification.\n\n\tb. The description accompanying Equation 5 (line 248), particularly the phrase \"F guided by...\" (or \"F guided by $\\pi$\"?), is ambiguous. It is not clear what this operation entails or how the guidance mechanism works.\n\n4. Some important details are missing.\n\n\ta. Whether the sampling strategy requires an additional stitching mechanism or post-processing step to ensure seamless transitions between different sampled patches?\n\n\tb. In SAP component, transforming rectangular inputs into squares is unclear. How is this achieved (e.g., resizing?), and does this step introduce unwanted distortion?\n\n\tc. What is the meaning of \"out-of-distribution\" (OOD) (Figure 14&15).\n\n\td. How to apply different projection functions π(·)?"}, "questions": {"value": "1. Why the 'Receptive Field Remap' is needed, given that the VAE encoder already processes features using 'SAP'?\n\n2. Where do the weights of the VAE encoder come from? Are the other models that need to be trained trained from scratch?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TP2lXyfwy3", "forum": "UNeL5NdLzc", "replyto": "UNeL5NdLzc", "signatures": ["ICLR.cc/2026/Conference/Submission10528/Reviewer_73SH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10528/Reviewer_73SH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839095885, "cdate": 1761839095885, "tmdate": 1762921806316, "mdate": 1762921806316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for generating images with arbitrary shapes (different aspect ratios, etc.) and projections (e.g. panoramic, fisheye) form a single model. It defines a 2D latent diffusion over a sphere around the camera, and rasterises from this to create the final image. Standard U-net architectures are applied on the sphere by discretising it as a modified icosahedron, subdividing triangles, and noting that pairs of triangles form diamonds having a grid topology. Beyond this the method is a standard latent diffusion. The model is initialised from SDXL, and fine-tuned on the Matterport3D dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach is novel; latent diffusion over a discretised sphere is an elegant approach to learning a generative model over 2D visual content that is invariant to the camera projection type and parameters. The specific approach to mapping the sphere to grid topology (so pretrained UNets) can be used is also interesting.\n\nResults are shown using a single model to generate diverse images – perspective at diverse aspect ratios, panoramic, and fisheye. Quality is comparable for all image types, with no visible degradation e.g. in highly deformed regions.\n\nCompared with several baselines for panoramic/large/multiview image generation, the proposed method achieves stronger results for three projection types across seven metrics; FID in particular is significantly lower than any other methods.\n\nThere is a fairly detailed ablation study, measuring the benefit of several of the introduced components, including the spherical residual block, and use of a spherical representation instead of direct deformation of planar generations.\n\nThe model is shown to retain some of the open-domain characteristics of the original SDXL model it is fine-tuned from (despite that fine-tuning only using the single-domain Matterport3D dataset) – in particular, it is still possible to generate out-of-distribution images given text conditioning, even under fisheye/etc projections."}, "weaknesses": {"value": "The overall technical innovation is rather small. The tricks required to apply the diffusion UNet on the sphere are not particularly noteworthy (e.g. there is significant discussion of what simply amounts to circular convolution). Beyond this, the model is essentially a standard latent diffusion model.\n\nIt is unclear how far the ability to generate arbitrary scenes (as opposed to those in-distribution for Matterport3D) is retained. While one qualitative example is shown, in addition there should be a quantitative comparison (e.g. using CLIP score) on a broad range of prompts, and this compared against the vanilla SDXL.\n\nThe claim of \"arbitrary resolution\" is rather strong. The method is trained using a fixed discretisation of the sphere, and while this can be sampled at arbitrarily high spatial frequencies, the fineness of synthesised details is bounded by the discretisation (and the training data) – unlike truly \"infinite resolution\" models like Generative Powers of Ten. The present work is similar to training a very high resolution 2D diffusion then cropping out an ROI of the desired resolution (though of course the spherical representation allows other lens types etc.)."}, "questions": {"value": "Please address the points mentioned under \"Weaknesses\" above – in particular, measure more thoroughly how well open-domain generation works with the fine-tuned model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mgGNdiLkjJ", "forum": "UNeL5NdLzc", "replyto": "UNeL5NdLzc", "signatures": ["ICLR.cc/2026/Conference/Submission10528/Reviewer_mcAc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10528/Reviewer_mcAc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761950430725, "cdate": 1761950430725, "tmdate": 1762921805228, "mdate": 1762921805228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}