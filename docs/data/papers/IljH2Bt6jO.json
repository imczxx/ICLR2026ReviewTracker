{"id": "IljH2Bt6jO", "number": 18228, "cdate": 1758285405798, "mdate": 1759897117906, "content": {"title": "Training a Vision-Language Model for Diverse Exploration in Open GUI World", "abstract": "Vision-language models have emerged as capable computer-use agents, showing increasing potential to automate a wide range of computer tasks through graphical user interfaces. However, their effectiveness remains bounded by a fundamental limitation: current LLM- or VLM-based agents struggle to generalize to unfamiliar applications and remain heavily dependent on large-scale, human-curated datasets. To address this, we introduce ScreenExplorer, a novel VLM-based agent designed for autonomous exploration in real, dynamic, open-ended GUI environments. Through end-to-end training with an exploration-driven objective, our approach enables sustained interaction and diverse discovery without relying on predefined task structures. Specifically, we introduce a world model-inspired curiosity reward that helps the agent to overcome the cold-start phase of exploration, coupled with state-change-based exploration rewards to encourage agent's intrinsic motivation for venturing into novel states. Additionally, an experience stream distillation mechanism is designed to systematically accumulate and refine exploratory policies, enabling continual learning from gathered experiences. Extensive evaluations demonstrate that ScreenExplorer achieves remarkable generalization and diverse exploration capabilities in unseen applications, significantly outperforming static deployment baselines. This work establishes a new paradigm for GUI agents to progressively learn through autonomous exploration, moving beyond static dataset dependency toward adaptive, lifelong learning in complex digital worlds.", "tldr": "", "keywords": ["Vision–Language Model", "World Model", "Computer Use Agent"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/96d6d36654d2d8699963f2f0a67575c7caaec318.pdf", "supplementary_material": "/attachment/b47d854e22070f20321e6bc28c8184e65ebb0429.zip"}, "replies": [{"content": {"summary": {"value": "This study proposes a reinforcement learning agent that autonomously explores GUI environments using a vision-language model. The proposed agent employs a reward function composed of nine components, including immediate, subsequent, and alignment rewards, together with a World Model–based curiosity reward to effectively explore unseen interfaces. To achieve more stable learning under the sparse and high-variance nature of GUI environments, the authors adopt Group Relative Policy Optimization (GRPO), demonstrating that it enables increasingly diverse exploration as training progresses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The study appropriately defines the observation and action spaces, which are essential aspects when applying reinforcement learning to web environments, and provides detailed explanations along with a plausible experimental setup.\n- It conducts extensive experiments, including ablations, to analyze how visual reward signals and the world model influence exploration.\nDefining a reward function in GUI environments is inherently challenging, but this work provides more combinations of the rewards than just simple single-step return rewards by introducing additional components such as the subsequent change reward, and empirically examines their effects on exploration through ablation studies.\n- Given the sparse and highly fluctuating rewards characteristic of GUI environments, the choice to use GRPO instead of PPO to reduce variance is reasonable."}, "weaknesses": {"value": "- This work uses visual/text diversity at the trajectory and group levels as its primary metrics, but it remains unclear whether these metrics truly correspond to meaningful web exploration rather than simply capturing random or superficial behavioral differences.\nFor instance, in Figure 5, the agent consistently selects the web browser across all episodes, and this could be due to the browser’s inherently higher visual diversity rather than intentional exploration, suggesting that the agent may have converged to a local optimum without exploring other diverse applications.\n\n- The authors does not provide quantitative evaluations of the World Model’s prediction accuracy or qualitative visualizations comparing predicted versus actual screens.\nGiven that fine-grained predictive understanding is critical in such settings, presenting only the finding that the World Model helps during the cold-start phase, without evidence of its predictive fidelity, feels somewhat incomplete.\n\n- Since several proposed reward components are directly tied to the evaluation metrics themselves (e.g., World Model–based reward), there is a risk that the agent’s exploration behavior becomes overly dependent on these self-referential metrics, potentially leading to biased or misleading exploration if the World Model is inaccurate."}, "questions": {"value": "- Is there any way to verify the quantitative results of the World Model?\n- In the GUI environment, does a sufficiently converged agent always choose the web browser?\nIf so, would this behavior correspond to a local optimum?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IsZTkXhjK4", "forum": "IljH2Bt6jO", "replyto": "IljH2Bt6jO", "signatures": ["ICLR.cc/2026/Conference/Submission18228/Reviewer_u3BX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18228/Reviewer_u3BX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800920880, "cdate": 1761800920880, "tmdate": 1762927967432, "mdate": 1762927967432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Explaining Our Problem Setting and System-Level Focus Beyond Benchmark Task Performance"}, "comment": {"value": "We thank all reviewers for their careful evaluations and constructive feedback. We first clarify the **core goal and problem setting** of this work: ScreenExplorer is not primarily designed to “directly improve task success rates on existing GUI benchmarks.” Instead, it targets a long-neglected aspect in current GUI agent research: **how to enable large models to autonomously explore and interact stably in open environments.**\n\nMost existing GUI works rely on manually designed tasks and annotated trajectories, and are evaluated on relatively closed benchmarks. In contrast, when humans face a new system or application, they typically begin with exploration to identify “reachable goals” and “reusable interaction structures,” and only then move on to solving specific tasks. We argue that, for large models to truly acquire the ability to “adapt to any new interface,” we first need a mechanism that allows them to **continuously self-explore and accumulate experience in real GUI environments**, rather than only performing supervised or RL fine-tuning on a fixed set of tasks.\n\nIn practice, we attempted task-level reinforcement learning directly in environments such as OSWorld. However, due to limited base model capability and extremely sparse rewards, it was almost impossible to roll out successful trajectories and obtain stable positive returns, so end-to-end optimization did not converge. This practical bottleneck directly motivated the setting in this paper: we instead design multiple exploration rewards and a world‑model‑based curiosity signal, and **prioritize the “precondition problem” of whether the agent can establish effective interaction with the environment and continuously discover new states**, laying the behavioral and data foundation for later introducing task-oriented rewards and constructing large-scale task spaces.\n\nTherefore, we position this work as a **problem-setting and system-level exploration**: we propose and validate a feasible framework that enables a VLM to perform self-driven exploration in real GUI environments, systematically analyze how different exploration rewards and curiosity signals shape behavior patterns, and build a closed loop from long-horizon exploration to model updates via experience stream distillation. We believe this line of work is complementary to research that “directly improves task success rates on existing benchmarks”: the former provides the fertile ground for scalable adaptation of large models in open-world settings, while the latter can then build on this foundation by adding concrete tasks and evaluations."}}, "id": "z0L1aNOfMN", "forum": "IljH2Bt6jO", "replyto": "IljH2Bt6jO", "signatures": ["ICLR.cc/2026/Conference/Submission18228/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18228/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18228/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763711804060, "cdate": 1763711804060, "tmdate": 1763711804060, "mdate": 1763711804060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents ScreenExplorer, a vision-language agent designed for exploration and interaction within real, dynamic, and open-ended GUI environments. The framework integrates a curiosity-driven reward mechanism that leverages a learned world model to enhance exploratory behavior; a reinforcement learning pipeline based on RL; and an experience stream distillation procedure that improves adaptation and reduces reliance on manually curated datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe combination of RL, a learned world model, and experience stream distillation forms a coherent, reproducible pipeline for self-supervised GUI exploration.\n2.\tThe world model, trained on paired image-text state transitions, introduces an intrinsic curiosity reward that improves cold-start exploration and advantage variance.\n3.\tThe paper analyzes reward components, showing that removing the world model or alignment rewards degrades performance."}, "weaknesses": {"value": "1.\tAlthough the paper claims that the agent is “rewarded for both successful interaction and exploration novelty,” the implementation shows that “successful interaction” merely refers to producing syntactically valid JSON actions that alter the GUI state. There is no external or task-based success criterion (e.g., reaching a goal, executing a correct function, or completing a workflow). As a result, the learned policy optimizes purely intrinsic objectives without assurance that these behaviors translate into useful or goal-directed interactions. This limits the practical significance of the reported improvements in exploration diversity and weakens the claim that ScreenExplorer enhances “interaction capability.”\n\n2.\tThe system is designed almost entirely around exploration incentives. Curiosity-based rewards and diversity metrics are maximized without complementary exploitation or goal conditioning. Consequently, the policy may overfit to visually novel yet semantically meaningless actions (a form of the “noisy-TV” problem acknowledged by the authors). This imbalance raises doubts about whether the learned behaviors can generalize to structured GUI tasks requiring planning or consistency. The paper would benefit from experiments showing how ScreenExplorer behaves when explicit goals or extrinsic feedback are introduced.\n\n3.\tDespite the “open-world” framing, all reported experiments appear confined to a limited set of GUI applications with similar layouts and interaction patterns. There are no results demonstrating generalization to unseen interfaces, visual themes, or operating-system contexts. Without evaluation on held-out GUI types, the claim of “generalizable exploration” remains speculative. Additionally, the paper does not discuss potential domain adaptation techniques or cross-environment fine-tuning strategies.\n\n4.\tThe paper defines nine reward components spanning format validity, visual and textual novelty, intent alignment, and world-model curiosity. However, their relative weighting, normalization, and mutual interference are underexplored. The only detailed ablation concerns the world-model term; other components (e.g., intent alignment or diversity scores) lack sensitivity analyses. Without systematic tuning or normalization (beyond GRPO’s implicit standardization), there is a risk of reward hacking, where the model exploits specific reward structures instead of learning genuinely diverse behaviors."}, "questions": {"value": "1.\tCould the authors clarify whether any extrinsic (task-success) reward or evaluator signal exists, and how the agent’s utility is measured beyond diversity?\n2.\tHow sensitive is performance to the weighting of the nine reward components? Have normalization or coefficient ablations been tested?\n3.\tWhat prevents the curiosity reward from degenerating into the “noisy-TV” effect (repeating visually novel but meaningless actions)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L5oRtlA60p", "forum": "IljH2Bt6jO", "replyto": "IljH2Bt6jO", "signatures": ["ICLR.cc/2026/Conference/Submission18228/Reviewer_WPCF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18228/Reviewer_WPCF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901968153, "cdate": 1761901968153, "tmdate": 1762927966998, "mdate": 1762927966998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ScreenExplorer introduces a VLM-based agent trained via reinforcement learning in real GUI environments to enable autonomous exploration without relying on predefined task structures. The key innovation is a hierarchical reward system combining state-change rewards, world model-based curiosity signals, and intent-state alignment rewards for GRPO. Experiments demonstrate improved exploration diversity compared to static baselines, with the model improving from worst to best performer through RL training."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses the important but under-explored challenge of autonomous GUI exploration in open-ended environments, moving beyond task-specific training.\n- The multi-faceted reward function elegantly combines immediate feedback (format, instant change), long-term diversity (subsequent change), curiosity (world model predictions), and grounding (intent-state alignment).\n- Clear improvements in exploration diversity metrics, with ScreenExplorer-7B achieving 0.55 average diversity compared to 0.25 for GPT-4o and 0.43 for Qwen2.5-VL-72B.\n- The world model curiosity reward successfully addresses the exploration cold-start problem by increasing advantage variance (Figure 4), enabling the 3B model to overcome initial learning barriers."}, "weaknesses": {"value": "- The paper critically lacks evaluation on actual GUI tasks. While exploration diversity is measured extensively, there's no evidence that this exploration improves performance on established benchmarks like WebArena, VisualWebArena, or Mind2Web. This is a fundamental gap. Exploration is only valuable if it improves task performance.\n- The diversity metrics (visual/textual sequence diversity) don't clearly correlate with useful exploration. The agent might be exploring irrelevant states (e.g., clicking random news articles) without learning transferable skills.\n- Evaluation is restricted to a single Linux desktop environment. No evidence of generalization to other platforms (Windows, macOS, mobile) or complex web applications.\n- While ablations show which rewards contribute to diversity, they don't demonstrate which exploration behaviors actually help downstream task learning. The \"noisy TV problem\" is mentioned but not thoroughly addressed.\n- The filtering process for experience streams relies on GPT-4o-mini or manual curation, but there's no analysis of what makes exploration trajectories valuable for learning."}, "questions": {"value": "- What is the performance on established benchmarks? How does ScreenExplorer perform on WebArena, VisualWebArena, or Mind2Web after exploration pre-training? Does exploration diversity correlate with task performance? Can you show that models with higher exploration diversity actually perform better on downstream GUI tasks?\n- How does the approach handle task-specific fine-tuning? After exploration pre-training, how should the model be adapted for specific tasks?\n- What prevents meaningless exploration? How do you ensure the agent explores task-relevant states rather than just clicking randomly to maximize state changes?\n- How does exploration transfer across environments? Does exploration in Linux desktop environments transfer to web or mobile applications?\n- What is the quality of distilled behaviors? Do distilled models learn meaningful exploration strategies or just memorize trajectories?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W03SfYu2Ub", "forum": "IljH2Bt6jO", "replyto": "IljH2Bt6jO", "signatures": ["ICLR.cc/2026/Conference/Submission18228/Reviewer_YR9L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18228/Reviewer_YR9L"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012927076, "cdate": 1762012927076, "tmdate": 1762927966570, "mdate": 1762927966570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ScreenExplorer, a vision-language model trained via reinforcement learning for open-world GUI exploration. The agent learns to interact with real desktop interfaces without predefined goals, driven by multi-term rewards that capture both state changes and semantic alignment. A world-model-based curiosity signal promotes novelty, and experience stream distillation helps consolidate diverse experiences for continual improvement. Experiments in a Linux GUI environment show consistent gains in exploration diversity and novelty compared to strong VLM baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper explores a fresh and timely problem (open-world GUI exploration), moving beyond fixed task datasets toward agents that can learn to explore software interfaces on their own through curiosity-driven interaction.\n- It proposes a well-designed framework that combines a world-model-based curiosity signal, multi-part state-change rewards, and GRPO optimization, with experience stream distillation helping the agent gradually improve through self-collected experience, this is intuitive and effective.\n- Experiments in a realistic Linux desktop environment show noticeable gains in exploration diversity and novelty compared to strong VLM baselines, and the analyses clearly demonstrate how the curiosity signal and alignment rewards contribute to the results.\n- The paper is clearly written and well-organized."}, "weaknesses": {"value": "- The evaluation is done only in a custom Linux GUI environment with a small set of apps and layouts. While this makes the study controlled and clean, it doesn’t reflect the variety and complexity of real-world interfaces like web or multi-window systems. It’s therefore unclear if the exploration policy and curiosity module would still work well in broader or more realistic GUI settings.\n\n- The ablation studies focus on removing reward terms or curiosity signals but don’t test the effect of experience stream distillation, reward weights, or world-model design. As a result, it’s hard to tell which parts of the system actually drive the performance gains, rather than the whole pipeline working together.\n\n-  While the framework is well-designed, many parts (like the curiosity module, GRPO training, and distillation) are adapted from existing methods. The paper mainly integrates these components rather than introducing new techniques, so the overall novelty feels more like a solid system combination than a new algorithmic idea."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zeTYrHMsgw", "forum": "IljH2Bt6jO", "replyto": "IljH2Bt6jO", "signatures": ["ICLR.cc/2026/Conference/Submission18228/Reviewer_oiXs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18228/Reviewer_oiXs"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18228/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762080509614, "cdate": 1762080509614, "tmdate": 1762927966139, "mdate": 1762927966139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}