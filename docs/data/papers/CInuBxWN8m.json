{"id": "CInuBxWN8m", "number": 12356, "cdate": 1758207260528, "mdate": 1759897515043, "content": {"title": "DiffusionReward: Enhancing Blind Face Restoration through Reward Feedback Learning", "abstract": "Reward Feedback Learning (ReFL) has recently shown great potential in aligning model outputs with human preferences across various generative tasks.\nIn this work, we introduce a ReFL framework, named *DiffusionReward*, to the Blind Face Restoration task for the first time.\nDiffusionReward effectively overcomes the limitations of diffusion-based methods, which often fail to generate realistic facial details and exhibit poor identity consistency. \nThe core of our framework is the Face Reward Model (FRM), which is trained using carefully annotated data.\nIt provides feedback signals that play a pivotal role in steering the optimization process of the restoration network.\nIn particular, our ReFL framework incorporates a gradient flow into the denoising process of *off-the-shelf* face restoration methods to guide the update of model parameters.\nThe guiding gradient is collaboratively determined by three aspects: (i) the FRM to ensure the perceptual quality of the restored faces; (ii) a regularization term that functions as a safeguard to preserve generative diversity; and (iii) a structural consistency constraint to maintain facial fidelity. \nFurthermore, the FRM undergoes dynamic optimization throughout the process. It not only ensures that the restoration network stays precisely aligned with the real face manifold, but also effectively prevents reward hacking.\nExperiments on synthetic and wild datasets demonstrate that our method outperforms state-of-the-art methods, significantly improving identity consistency and facial details. The source codes and models are available\nat: https://anonymous.4open.science/r/DiffusionReward-D02F", "tldr": "", "keywords": ["Blind face restoration", "Reward feedback learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a7267f406393e7cf14c4c37d0b6162f7b2ae23f3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces DiffusionReward, a framework applying Reward Feedback Learning (ReFL) to the Blind Face Restoration (BFR) task. The key idea is to train a Face Reward Model (FRM) that provides preference-based feedback to guide diffusion model optimization. The FRM is dynamically updated to avoid reward hacking. The framework is evaluated using DiffBIR and OSEDiff as base models and claims consistent improvements in both synthetic and real-world datasets in terms of identity preservation and perceptual quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper extends reinforcement-style reward optimization into the restoration domain, which is a cross-domain adaptation of alignment techniques.\n\n2. Introducing online updating of the FRM to mitigate reward hacking is conceptually strong and practically meaningful.\n\n3.The paper is easy to follow, with clear figures and a logical explanation of methodology."}, "weaknesses": {"value": "1. Lack of analysis on reward accuracy and sparsity: Although the Face Reward Model (FRM) is central to the proposed framework, the paper lacks an in-depth analysis of reward reliability (accuracy with respect to human preference) and reward sparsity or noise sensitivity. The authors mention a hybrid human–SVM annotation pipeline, yet there is no quantitative study of how accurate or consistent the FRM’s feedback is compared to real human judgment, especially after dynamic updates. Moreover, ReFL optimization critically depends on reward signal density — sparse or unstable reward gradients can easily lead to optimization collapse or reward hacking.\n\n2. Lack of convergence and training dynamics analysis，The authors claim that introducing the Face Reward Model (FRM) can provide effective gradient feedback to guide optimization. Intuitively, this should accelerate convergence or stabilize training. However, the paper does not present any training dynamics analysis, such as loss curves, reward evolution, or convergence speed comparisons with baseline models.\n\n3. Diffusion models are known to be slow. The paper does not report inference speed or resource cost, nor analyze trade-offs introduced by ReFL fine-tuning.\n\n4. The proposed ReFL framework introduces supervision at the image (decoder) level by backpropagating through the image reward model. This incurs significant training-time and memory overhead, as gradients must flow through the decoder and multiple denoising steps. Although the authors later mention truncated backpropagation (N=1), there is no quantitative analysis of the actual training cost compared with the base models. This raises concerns about scalability and practicality.\n\n5. The related work section overlooks several important prior approaches:\n\n[1].Face Super-Resolution Guided by 3D Facial Priors\n\n[2]. Rethinking Deep Face Restoration"}, "questions": {"value": "CodeFormer and related transformer-based BFR methods already demonstrate strong perceptual quality and robustness. The current ReFL framework is tied to the diffusion process, where gradient feedback is injected during denoising. It remains unclear whether the proposed reward learning paradigm is architecture-agnostic or only applicable to diffusion-based models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P3BpXAKc7J", "forum": "CInuBxWN8m", "replyto": "CInuBxWN8m", "signatures": ["ICLR.cc/2026/Conference/Submission12356/Reviewer_HVjR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12356/Reviewer_HVjR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798773340, "cdate": 1761798773340, "tmdate": 1762923270568, "mdate": 1762923270568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies that pre-trained diffusion models for blind face restoration (BFR) often lack face-specific priors, leading to results deficient in detail and identity preservation. The authors propose a novel solution by adapting Reward Feedback Learning (ReFL) to BFR. Their key innovation involves fine-tuning a restoration model using a tailored, dynamically updated face reward model, combined with structural consistency and weight regularization constraints. This approach enhances the visual fidelity of restored faces while ensuring identity consistency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Effective Adaptation:​​ The proposed application of Reward Feedback Learning (ReFL) to Blind Face Restoration (BFR) is novel and compelling. It presents a computationally efficient fine-tuning strategy that demonstrably enhances the performance of powerful pre-trained diffusion models, as evidenced by the clear improvements in both visual quality and quantitative metrics reported.\n\n2. Well-Designed Reward Model:​​ The construction of the Face Reward Model (FRM) is a significant contribution. The meticulous data collection and annotation process for training the FRM is well-motivated and interesting.\n\n3. ​Good Reproducibility:​​ The release of the source code, coupled with the extensive details provided in the appendix, facilitates future research and practical application, adhering to good practice in the research community.\n\n4. ​Clarity and Readability:​​ The manuscript is clearly written and is easy to follow."}, "weaknesses": {"value": "1. Potential Cherry-Picking of Qualitative Results:​​ A common concern in image restoration is the potential for cherry-picking qualitative results. While the provided examples are compelling, it would significantly strengthen the validity of the claims if the supplementary material included a more extensive set of randomly selected samples (e.g., the first 1-20 images from a standard benchmark like FFHQ) comparing the base models (OSEDiff, DiffBIR) against their fine-tuned versions with the proposed method. This would provide a more objective and convincing demonstration of the method's consistent performance gains.\n\n2. ​Clarification of Novelty Relative to ReFL in Image Generation:​​ The manuscript would benefit from a more precise discussion of its novelty concerning the application of ReFL. Given the existing body of work applying ReFL to text-to-image and other generative tasks, the claim that \"there remains a notable research gap in exploring the application of ReFL to restoration tasks\" (Line 171) requires stronger justification. The authors should more clearly articulate the distinct challenges of adapting ReFL specifically for restoration(e.g., the critical importance of identity preservation and fidelity to the degraded input, as opposed to open-ended generation) and how their work addresses these unique challenges, thereby delineating their contribution beyond a direct transfer of the ReFL paradigm.\n\n3. ​Correction of Presentation Errors:​​ There are minor presentation issues that should be corrected. For instance, in Figure 5, the image for \"OSEDiff (+ours)\" in the right set appears to be identical to the one labeled for \"DiffBIR.\" A careful proofreading of all figures and captions is recommended to ensure accuracy."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cuJpZ4aWJG", "forum": "CInuBxWN8m", "replyto": "CInuBxWN8m", "signatures": ["ICLR.cc/2026/Conference/Submission12356/Reviewer_nDxx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12356/Reviewer_nDxx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843992434, "cdate": 1761843992434, "tmdate": 1762923270199, "mdate": 1762923270199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DiffusionReward, a Reward Feedback Learning (ReFL) framework for blind face restoration (BFR). The key idea is to introduce a Face Reward Model (FRM)—a CLIP-based reward network fine-tuned with a hybrid of human and automatically annotated face-preference data—to provide perceptual feedback during the optimization of diffusion-based restoration models (e.g., DiffBIR, OSEDiff). Experiments on synthetic and real-world datasets demonstrate consistent improvements over state-of-the-art BFR methods on multiple perceptual and aesthetic metrics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A dynamic reward update strategy to mitigate reward hacking;\n\n2. Structural consistency and weight-regularization constraints to preserve identity and maintain generative diversity;\n\n3. End-to-end ReFL training integrated into the denoising process."}, "weaknesses": {"value": "1. The core contribution of this work is to introduce the reward feedback mechanism into BFR. However, the experimental results do not clearly demonstrate the necessity and advantages of the reward model. Specifically focusing on the ablation studies of Table 4, i) the LMD improvement mainly comes from the structural consistency loss (see from Base and Variant 1); ii) the MUSIQ and Aesthetic improvement is primarily controlled by KL weight regularization (see from Variant 3 and Ours) as this KL regularization encourages sufficiently using the Diffusion prior; iii) I suggest the author supply another experiment by changing the WR loss importance in Variant 1, i.e., adjusting the hyper-parameter $\\lambda_{reg}$ to validate the function of the reward model.\n\n2. This work attempts to optimize the entire multi-step denoising process (i.e., $N>1$) as claimed in Appendix D.2. However, the chain length $N$ is finally truncated to 1, thus being the same as the commonly used independent denoising strategy in Diffusion models. So, I can't capture the significance of the related discussion (e.g., Appendix D.2) in this work.\n\n3. The reward classifier is pre-trained on some human-annotated data. How many annotated pairs are used? It would be better to analyze the sensitivity of the reward model and restoration model regarding the human annotation.    \n\n4. The reward model introduces the additional text prompt. I wonder about the significance of this textual information."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6X3dEaju01", "forum": "CInuBxWN8m", "replyto": "CInuBxWN8m", "signatures": ["ICLR.cc/2026/Conference/Submission12356/Reviewer_4Q9v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12356/Reviewer_4Q9v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878538724, "cdate": 1761878538724, "tmdate": 1762923269911, "mdate": 1762923269911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DiffusionReward, a novel Reward Feedback Learning (ReFL) framework for blind face restoration (BFR). The work addresses critical limitations of diffusion-based BFR methods, such as insufficient facial details and poor identity consistency, by leveraging a carefully designed Face Reward Model (FRM) and dynamic optimization constraints. The idea of integrating ReFL into BFR is innovative, and the experimental results demonstrate state-of-the-art performance on both synthetic and real-world datasets. The paper is well-structured, and the methodology is technically sound."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Novelty and Significance:\n\n1) This is the first work to adapt ReFL for BFR, bridging a gap between generative alignment and restoration tasks. The dynamic FRM update strategy effectively mitigates reward hacking, a common pitfall in reward-based optimization.\n\n2) The hybrid annotation pipeline (combining human labels with SVM-based automation) for FRM training is resource-efficient and scalable.\n\nTechnical Rigor:\n\n1) The framework incorporates multiple loss terms (reward loss, structural consistency, weight regularization) to balance perceptual quality, identity preservation, and generative diversity. Ablation studies thoroughly validate each component’s contribution.\n\n2) Experiments cover diverse settings: two base models (DiffBIR and OSEDiff), synthetic (CelebA-Test) and wild datasets (LFW/WebPhoto-Test), and 11 metrics. The consistent improvements highlight generalizability."}, "weaknesses": {"value": "Clarity of FRM Training Details: The description of the SVM-based automated annotation (Section 3.1) is concise but lacks critical specifics (e.g., kernel choice, feature normalization). Please add a brief summary in the main text or refer to Appendix A.1 for clarity.\n\nLimitations of Generalizability: The framework is only validated on diffusion-based models (DiffBIR/OSEDiff). The ablation in Appendix G.1 shows limited gains when applied to GFPGAN (GAN-based). This should be explicitly discussed as a limitation.\n\nComputational Cost: While truncated backpropagation (N=1) reduces memory usage, the dynamic FRM updates and multi-constraint optimization still incur overhead. A brief discussion of training efficiency would strengthen the practicality claims."}, "questions": {"value": "Please refer to the paper weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9YiY7X3mnK", "forum": "CInuBxWN8m", "replyto": "CInuBxWN8m", "signatures": ["ICLR.cc/2026/Conference/Submission12356/Reviewer_jFAM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12356/Reviewer_jFAM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12356/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914091233, "cdate": 1761914091233, "tmdate": 1762923268991, "mdate": 1762923268991, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}