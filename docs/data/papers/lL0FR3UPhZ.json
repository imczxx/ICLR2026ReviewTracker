{"id": "lL0FR3UPhZ", "number": 23355, "cdate": 1758342507927, "mdate": 1759896819338, "content": {"title": "Polynomial Convergence of Riemannian Diffusion Models", "abstract": "Diffusion generative models have demonstrated remarkable empirical success in the recent years and are now considered the state-of-the-art generative models in modern AI. These models consist of a forward process, which gradually diffuses the data distribution to a noise distribution spanning the whole space, and a backward process, which inverts this transformation to recover the data distribution from noise. Most of the existing literature assumes that the underlying space is Euclidean. However, in many practical applications, the data are constrained to lie on a submanifold of Euclidean space. Addressing this setting, de Bortoli et al. (2022) introduced Riemannian diffusion models and proved that using an exponentially small step size yields small sampling error in Wasserstein distance, provided the data distribution is smooth and strictly positive.\nIn this paper, we prove that a *polynomially small stepsize* suffices to guarantee small sampling error in total variation distance, without any assumption on the smoothness or positivity of the data distribution. Our analysis only requires mild and standard curvature assumptions on the underlying manifold. The main ingredients in our proof are Li-Yau estimate for log-gradient of heat kernel, and Minakshisundaram-Pleijel parametrix expansion for perturbed heat equation. Our approach opens the door to a sharper analysis of diffusion models on non-Euclidean spaces.", "tldr": "We prove that discretized Riemannian diffusion model converges in total variation distance within polynomially many steps.", "keywords": ["diffusion model", "Riemannian manifold", "gradient estimate"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/77dfb6a1606e9be2ec9894dc87cce0e3e594d5e8.pdf", "supplementary_material": "/attachment/80bb017c1b124299e8af683cfb880fb3ab8920dd.pdf"}, "replies": [{"content": {"summary": {"value": "This paper extends the Riemannian diffusion model, studying the convergence behavior under the L2 accurate score estimate. The paper shows that a polynomially small stepsize is sufficient to guarantee small sampling error in TV distance with only mild curvature assumptions. For technical contribution, the proof uses Li-Yau gradient bounds for the heat kernel, a localization scheme for the drift, and Minakshisundaramâ€“Pleijel parametrix for controlling deviations from the heat flow and its discretized proxy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents the first polynomial TV convergence results for Riemannian diffusion models to the best of my knowledge. While it extends the theoretical results in Riemannian diffusion models, the paper proves TV convergence instead of Wasserstein convergence under mild geometric conditions. \n\n- The paper provides analytical techniques to prove the polynomial convergence: Li-Yau estimate for the heat kernel, localization of the drift fields, and parametrix estimates. These tools may be meaningful for bridging diffusion theory and geometry and could be used for developing a sampling method for Riemannian diffusion models."}, "weaknesses": {"value": "- I'm unsure what the impact of the paper is and how it contributes to the field. While the paper gives a theoretical guarantee for the Riemannian diffusion model, the empirical results from De Bortoli et al already validated that it worked in diverse settings. I agree that the paper provided new tools to use for future study in this field, but it is unclear how important it is to show polynomial TV convergence for the already working model. If the paper presented a recipe for stepsize (with empirical results) or an advanced sampling method, it should have been a strong signal for acceptance. I would appreciate it if the authors could give an explanation of the importance of the work.\n\n- While the theoretical contributions are important, I believe empirical results, at least on small toy settings, are necessary. I cannot say that the details of the proof are all correct due to an insufficient understanding of differential geometry. Empirical results should have complemented this, giving strong evidence.\n\n- Is the result only applicable to the Riemannian diffusion model from De Bortoli's design? For example, there are several designs of diffusion models, such as [Chin-Wei Huang et al., 2022], [Lou et al., 2023], Riemannian Flow Matching [Chen and Lipman, 2024], or Riemannian Diffusion Mixture [Jo et al., 2024], and I wonder if the convergence results or its argument could be applied to different designs.\n\n[Chin-Wei Huang et al., 2022] Riemannian Diffusion Models, NeurIPS 2022\n[Lou et al., 2023] Scaling Riemannian Diffusion Models, NeurIPS 2023\n[Chen and Lipman] Flow Matching on General Geometries, ICLR 2024\n[Jo et al., 2024] Generative Modeling on Manifolds Through Mixture of Riemannian Diffusion Processes, ICML 2024"}, "questions": {"value": "Please address the questions raised in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FdRZVHgmtg", "forum": "lL0FR3UPhZ", "replyto": "lL0FR3UPhZ", "signatures": ["ICLR.cc/2026/Conference/Submission23355/Reviewer_qEnZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23355/Reviewer_qEnZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656645904, "cdate": 1761656645904, "tmdate": 1762942623562, "mdate": 1762942623562, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper â€œPolynomial Convergence of Riemannian Diffusion Modelsâ€ presents a significant advancement over the work of De Bortoli et al. (2022), which introduced generative diffusion processes on Riemannian manifolds. This framework, known as the Riemannian Score-Based Generative Model (RSGM), is motivated by the observation that many datasets, such as those arising in climate science and high-energy physics, naturally lie on Riemannian-like manifolds. De Bortoli et al. demonstrated that, in such settings, performing diffusion directly on the underlying manifold can substantially improve generative performance. However, their approach suffered from a severe limitation: achieving a small divergence between the model and target distributions required an exponentially small discretization step size, leading to prohibitive computational costs. The current paper overcomes this issue by proving that, under mild assumptions on the manifold structure, one can instead use a polynomially small step size to achieve comparable accuracy. This result dramatically improves the practical feasibility of RSGMs and paves the way for their broader application in real-world generative modeling tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The work has several notable strengths:\n\n1. The presentation is clear and pedagogical, making the material accessible even to readers without a strong background in differential geometry.\n\n2. The discussion of related work is thorough and well-integrated, situating the paper within a coherent and well-defined research landscape.\n\n3. The analytical results appear sound, and the way of deriving the bound over the total variation, as a measure of the distance between two distributions, could be employed in characterizing other contexts, such as classifier-free-guidance and other diffusion routines where an exact theory is lacking and sources of errors are similar."}, "weaknesses": {"value": "The results are primarily analytical and the central goal of the work is to derive new performance bounds for RSGMs. I hence appreciate the total analytical vocation of the manuscript. On the other hand, one might think that the total absence of experimental validation could appear as a weakness of the work. De Bortoli et al. (2022) compare RSGM with other manifold-based diffusive routines on manifold supported datasets, like climate science spherical data. They quantify the quality of the performance in terms of the log-likelihood achieved by the model. I was wondering whether the authors of the current paper ever evaluated the idea of running similar experiments with both RSGM and other procedures (e.g. Mixture of Kent, Moser Flow etc.) and show that RSGM can reach a comparable degree of the performance still employing a polynomial number of steps. Such scaling could be analyzed as a function of relevant control parameters, including manifold size and early stopping time. Another possible, and possibly equivalent, experiment would instead imply verifying that Riemann supported real datasets verify the assumptions of the current analysis.  \n\nIn addition to addressing the questions outlined in the following section of this review, I would appreciate if the authors could argue around this point more in detail."}, "questions": {"value": "I will now proceed to ask questions and raise issues that I would like the authors to address in order to both finalize my personal judgment over the paper and improve the manuscript. \n\n**Questions:**\n\n1. Why do I re-start from the uniform distribution if the diffusion step from $Y_k$ exits the injective radius ? What happens if one restarts from $Y_k$ and samples new noise? How often would the reset take place in practical applications? \n2. In the continuous-time variance-exploding prescription, a common choice for the prior distribution in backward diffusion is a wide Gaussian distribution, with zero mean and variance equal to the horizon $T$. Are there other prior distributions, alternative to the uniform one, that can reduce the initialization error? Can Urakawa (2006) be generalized to other initializations?\n3. Recent studies in this field, such as Ventura et al. (2025) \"Manifolds, Random Matrices and Spectral Gaps: The geometric phases of generative diffusion\", Wang et al. (2025) \"Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering\", Stanczuck et al. (2024) \"Diffusion Models Encode the Intrinsic Dimension of Data Manifolds\" show that, when data are supported by a low-dimensional manifold, the geometry of this manifold  progressively emerges in the backward diffusion in the Euclidean space, without the need of constraining the trajectory on the manifold itself. I would ask to the authors whether it would be possible to study the time dependence (that I guess translates into studying the $\\delta$ dependence) of the TV and see whether its upper bound becomes tighter at smaller times, when the geometry of the target manifold already emerges even when diffusing in the Euclidean space. \n\n**Readability issues:**\n\n1. [101] What does BM stand for?\n2. [157-158] Here and across all the paper there is a conflict between the dimension of the ambient Euclidean space and the dimension of the manifold used in the subsequent proofs. \n3. [270-271] You should probably state that M has dimension d, since it appears implicitly in A3.  \n4. Both in [306-307] and the very beginning in [088] the authors may introduce a bit better the role of the error constant ðœ€, as a quantity employed to tune the horizon time $T$ and from that the discretization stepsize. \n5. [395-396] May be useful to uniform the nomenclature for â„ with respect to the previous expression in [394-395].\n\n**Typos**: \n1. [073] â€œexponentialâ€\n2. [362] â€œtime frozenâ€\n3. [324-325] \"poly-logarithmic\"\n\nI take advantage for thanking the authors for the nice job and I wait for their response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "k90HZPzl42", "forum": "lL0FR3UPhZ", "replyto": "lL0FR3UPhZ", "signatures": ["ICLR.cc/2026/Conference/Submission23355/Reviewer_5iY7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23355/Reviewer_5iY7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919789950, "cdate": 1761919789950, "tmdate": 1762942623094, "mdate": 1762942623094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies Riemannian score-based diffusion models on a compact $d$-dimensional manifold $(M, g)$. Under bounded geometry (injectivity radius $\\rho > 0$; curvature/tensor bounds), an $L^2$-accurate score (Assumption 2) and a mild regularity bound on the learned score (Assumption 1. (A3)), it proves that a polynomially small step size suffices to achieve small total-variation (TV) error between the sampler's output law and an early-stopped target $p_{\\delta}$. The bound decomposes into (i) heat-flow mixing at rate $\\lambda_1$ (the spectral gap of $-\\Delta_M$), (ii) a term linear in the $L_2$ score error, and (iii) a discretization term scaling like $\\sqrt{hT}\\mathrm{poly}(d,K,\\rho^{-1},\\delta^{-1})$. Technically, the proof combines Liâ€“Yau/Hamilton/Hanâ€“Zhang heatâ€‘kernel/gradient estimates, a localization that freezes the drift in normal coordinates, and a Minakshisundaramâ€“Pleijel parametrix control to bound the Brownianâ€‘motion simulation error that arises on manifolds. The algorithm includes a \"reset to the uniform measure $\\mu$\" safeguard when a proposed step exits the injectivity radius."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Problem significance.** TVâ€‘accurate sampling on manifolds with polynomial step size is a meaningful strengthening over prior Riemannian SGM bounds that scale poorly with $d$; cf. De Bortoli et al. 2022.\n- **Technique.** The parametrix approach to BM simulation error is original in this context and technically apt; it uses BGV-style local expansions and Volterra series to compare kernels; Berline-Getzler-Bergne (https://link.springer.com/book/10.1007/978-3-642-58088-8).\n- **Error decomposition.** Splitting the total error into mixing / score /discretization / BM simulation aligns with the Euclidean playbook but addresses the manifold-specific obstacle (non-Gaussian BM kernel).\n- **Assumptions.** Works under bounded geometry and does not require smooth/strictly positive data density; early stopping avoids small-time blow-ups."}, "weaknesses": {"value": "1. **Reset-analysis mismatch.** Provide a uniform upper bound on\n$$\\sum^N_{k=1} \\mathrm{Pr} (\\| \\Delta_k \\| > h^{1/4}) $$ \nand propagate it into the final TV bound. One can combine drift bounds (from Li-Yau/Hamilton/Han-Zhang estimates) with a BDG-type tail on the martingale term to show a sub-Gaussian decay in $(\\omega^2 / (t_k - t))$; but the constants and dependence on $d$, $K$, $\\rho^{-1}$, and $\\delta^{-1}$ must be explicit.\n2. **Parametrix generator sign.** Unify the generator throughout Â§E.2 (either $-\\frac{1}{2}\\Delta_M + \\braket{S, \\nabla}$ or $+\\frac{1}{2}\\Delta_M + \\braket{S, \\nabla}$) ; then re-check Lemma 19 and the Volterra bounds against the exact BGV statements (e.g. Thm. 2.23, 2.26, 2.29).\n3. **Heat-kernel sup bound.** Replace $$\\sup \\log H(s/2, \\cdot,\\cdot) \\lesssim \\frac{d \\log d}{s} + Ks $$ by a standard Liâ€“Yau form: $\\log \\sup_{x,y} H(t,x,y) \\lesssim \\frac{d}{2}\\log(1/t) + CKt$ (possibly after inserting volume factors via Bishop-Gromov/GÃ¼nther). Re-derive the $\\mathbb{E}[\\log^4(\\cdot)]$ control under this bound.\n4. State geometric assumptions precisely. Add an explicit Ricci lower bound ($ \\mathrm{Ric} \\geq - K_{\\mathrm{Ric}} $) and track constant dependencies in Lemma 11 and downstream. This is standard in Hamilton's matrix Harnack and Han-Zhang's Hessian upper bounds.\n5. Girsanov conditions. Insert a lemma verifying Novikov/Kazamaki under (A3) and your cutoffâ€”e.g., $ \\mathbb{E} \\exp( \\frac{1}{2} \\int \\| S \\|^2 dt ) < \\infty $ along the pathâ€”so that the KL steps in (5) are justified.\n6. Use the squareâ€‘root form where appropriate. In several places the proof appears to use a bound of the type $\\| \\nabla \\log p_t \\| \\lesssim (1/t + K) \\log (\\sup p_{t/2} / p_t )$, whereas standard Hamilton/Li-Yau give $\\| \\nabla \\log p_t \\|^2$ bounded by that expression. Ensure constants in, e.g., (11), remain valid with the square root in place. \n7. Assumption (A3) implementability. (A3) upperâ€‘bounds $\\| s_{\\theta} (t_k , x) \\ |$ using the unknown true score $\\| \\nabla \\log p_{t_k} (x) \\|$. Clarify how clipping that depends only on $s_{\\theta}$ guarantees (A3) without access to the groundâ€‘truth score; or restate (A3) as a condition on $s_{\\theta}$ alone and show it suffices for all places where (A3) is invoked.\n8. Make the $\\omega^4$ step explicit. When concluding $\\mathrm{Pr}(d(Y_y,Y_{t_k})) > \\omega /3 \\leq e^{-c\\omega^2 / (t_k - t)} \\leq \\omega^4$, spell out the exact regime relating $h$ and $\\omega$. This will help readers verify the scheduling choices that keep this probability polynomially small.\n9. Cite mixing to $\\mu$ precisely. The initialization/mixing bound uses the spectralâ€‘gapâ€‘driven decay of the heat semigroupâ€”please cite Urakawa and state clearly how the \"$A$\" constant is controlled."}, "questions": {"value": "1. Reset term: Can you provide an explicit bound for\n$$\\sum^N_{k=1} \\mathrm{Pr} (\\| \\Delta_k \\| > h^{1/4}) $$ \nand show how it is absorbed into the final TV error? A short lemma that combines drift estimates from Lemmaâ€¯11 with BDG would suffice.\n2. Generator sign: Which generator (and adjoint) is used in Â§E.2 for the parametrix? Please unify the sign and confirm that Lemmaâ€¯19 and the Volterraâ€‘series constants remain valid under the corrected operator.\n3. Geometric hypotheses: Please state (or the exact curvature condition you need) upfront, and propagate the dependence carefully through Lemmasâ€¯7/11 and Theoremâ€¯1.\n4. Heatâ€‘kernel sup: Reâ€‘derive the $ \\mathbb{E} [\\log^4(\\cdot)]$ control using a Liâ€“Yauâ€‘style $\\log \\sup H(t) \\lesssim \\frac{d}{2} \\log (1/t) + CKt$. Does the main scaling change? If not, please include the corrected calculation.\n5. Girsanov/Novikov: Please add a oneâ€‘page lemma verifying Novikov (or an equivalent condition) in your setting with cutoff. This would fully justify (5).\n6. (A3) implementability: How can one enforce (A3) in practice without access to $\\nabla \\log p_t$ ? If you intend (A3) to be a theoretical condition only, please say so and explain where a weaker dataâ€‘dependent clipping suffices.\n7. Schedule details: Please state the exact relationship among $h$, $\\omega$, $\\delta$ used to obtain the inequality $\\mathrm{Pr}(d > \\omega /3) \\leq \\omega^4$.\n8. Can you relate your work to https://proceedings.mlr.press/v251/sakamoto24a.html (https://openreview.net/forum?id=ahVFKFLYk2) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "nsfQT3MYYS", "forum": "lL0FR3UPhZ", "replyto": "lL0FR3UPhZ", "signatures": ["ICLR.cc/2026/Conference/Submission23355/Reviewer_FiF9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23355/Reviewer_FiF9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23355/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996413876, "cdate": 1761996413876, "tmdate": 1762942622690, "mdate": 1762942622690, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}