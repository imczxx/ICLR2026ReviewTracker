{"id": "7wdCgG6K7i", "number": 15449, "cdate": 1758251437109, "mdate": 1759897306063, "content": {"title": "CURATE: Automatic Curriculum Learning for Reinforcement Learning Agents through Competence-Based Curriculum Policy Search", "abstract": "Due to fundamental exploration challenges without informed priors or specialized algorithms, agents may be unable to consistently receive rewards, leading to inefficient learning. To address these challenges, we introduce CURATE, an automatic curriculum learning algorithm for reinforcement learning agents to solve a difficult target task distribution with sparse rewards. Through \"exploration by exploitation,\" CURATE dynamically scales the task difficulty to match the agent's current competence. By exploiting its current capabilities that were learned in easier tasks, the agent improves its exploration in more difficult tasks. Our key insight is that the performance increase in tasks that are close to those used for training is inversely proportional to their difficulty, and an agent that chooses a nearby distribution of the easiest unsolved tasks at any given time can automatically induce an easiest-to-hardest curriculum. To achieve this, CURATE conducts policy search in the task space to learn the best task distribution for training the agent. As the agent's mastery grows, the learned curriculum adapts in an approximately easiest-to-hardest and task-directed fashion, efficiently culminating in an agent that can solve the target tasks. Our experiments in grid-based navigation and image-based control of an action game demonstrate that CURATE learns task-directed curricula faster than prior curriculum methods that do not require domain expertise.", "tldr": "CURATE trains RL agents to complete difficult target tasks by learning a curriculum that dynamically scales the task difficulty to the current capabilities of the agent.", "keywords": ["curriculum learning", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/80cbe35b22ec259e10b3fcd64d216f004bf94ea8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents CURATE, an automated curriculum generation method for sparse RL domains. Using relative entropy policy search, CURATE searches for a policy in the task space that prioritizes the easiest unsolved tasks as the policy improves. This induces an approximately easiest-to-hardest curriculum, where difficulty is inversely proportional to the discounted return.  Experiments on MiniGrid MultiRoom and Procgen Leaper demonstrate that CURATE improves sample efficiency in comparison to domain randomization and some UED methods. The paper also introduces a benchmark for systematic evaluation of curriculum learning methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has a detailed related work covering a wide range of curriculum learning works. \n- The proposed method seems novel in its framing of exploration by exploitation for curriculum learning for RL.\n- The domains of interest in the experiments seem difficult to tackle due to their sparse-reward nature."}, "weaknesses": {"value": "- The paper is difficult to follow, the method section has a circular structure, and is confusing.\n- The notion of difficulty is limited to a niche group of sparse reward domains where the number of steps to reach the goal is directly associated with the difficulty. Although mentioned in the limitations section, this is a clear obstacle against the practicality of the proposed approach.\n- Despite claiming that the authors scale existing ideas to multidimensional settings, they only do it up to two-dimensional task spaces.\n- There is no ablation for the impact of different reward components for training the curriculum policy."}, "questions": {"value": "- What ablations would demonstrate the use of having different rewards for different stages?\n- Is there a reason why ALP-GMM cannot handle multi-dimensional task spaces? \n- Could you please give me an example domain where the assumption on the difficulty ordering doesn't hold, and explain how CURATE would perform there?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "7DuEIDmMMd", "forum": "7wdCgG6K7i", "replyto": "7wdCgG6K7i", "signatures": ["ICLR.cc/2026/Conference/Submission15449/Reviewer_X7fM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15449/Reviewer_X7fM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936128185, "cdate": 1761936128185, "tmdate": 1762925736112, "mdate": 1762925736112, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CURATE, an automatic curriculum learning method for reinforcement learning tasks. The approach learns a curriculum policy and updates it using REPS to shift the task distribution toward the easiest unsolved tasks for the agent, resulting in an emergent easiest-to-hardest training progression. However, the approach requires a known and ordered task space to function. Experiments on MiniGrid MultiRoom and Procgen Leaper show that CURATE improves sample efficiency and outperforms several baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper tackles an important  challenge in reinforcement learning, how to automate curriculum generation to improve training efficiency in sparse-reward settings.\n- The paper adapts the Procgen environment for curriculum-based evaluation. While only three environments are used (and mainly one discussed in the main text), these setups could still serve as useful benchmarks for the community and support more consistent evaluation of curriculum learning methods."}, "weaknesses": {"value": "W1: The paper is difficult to follow. The description of the proposed method in the main text is mostly narrative, intermixed with mathematical formulas that are not well connected to the algorithmic intuition. The pseudocode in the appendix is poorly formatted and hard to read. Furthermore, the paper lacks any theoretical justification or analysis of why the proposed approach should converge or be optimal.\n\nW2: Although the authors acknowledge this limitation, the requirement of having a known and ordered task space is a major assumption. CURATE depends on explicit task parameters that can be continuously ordered by difficulty, an assumption that many other baseline methods do not exploit. This significantly narrows the generality of the approach.\n\nW3: The evaluation procedure appears to exclude the cost of repeated task evaluations from the total training budget. This underestimates the true computational cost of CURATE compared to baselines. Moreover, for this kind of problem setup, evaluation alone can sometimes be sufficient to train an agent, for example, in Evolution Strategies (ES), where learning is driven entirely by repeated evaluations of policy performance without a separate training phase. This makes the comparison potentially unfair, as CURATE benefits from frequent evaluations but seems not include them in its reported training budget.\n\nW4: The evaluation of the proposed approach requires the definition of a solved threshold, which in most cases means that we already need to know how to solve the problem in order to define it. This requirement limits the range of domains where the approach can be applied.\n\nW5: The testbed consists of only two environments: MiniGrid MultiRoom and the modified-for-curriculum Procgen Leaper. This is a very small set of tasks, and the task spaces themselves are quite limited (e.g., 4 possible rooms in MiniGrid and a two-dimensional task space with 3 road lanes and 3 water lanes for Leaper). Overall, the testbed is too restricted to demonstrate scalability. Moreover, the paper ignores several benchmarks from curriculum learning research, such as those used in the ACCEL paper.\n\nW6: It seems that the approach is not applicable to hierarchical curriculum setups (e.g., MineCraft or Crafter), where preventing catastrophic forgetting is crucial. These environments typically require mechanisms for skill retention or replay, as presented, for example, in PLR. CURATE does not appear to include such mechanisms."}, "questions": {"value": "Q1: How were the baselines tuned or adapted to the tasks being solved?\nFor example, in PLR replay there are several possible prioritization methods (e.g., policy_entropy, least_confidence, min_margin, gae, value_l1, etc.). Which specific configuration was used in the experiments? Additionally, what does the “PLR prioritization - rank” entry in Table 3 of the appendix refer to?\n\nQ2: Does the evaluation budget include the cost of all curriculum evaluations used? If not, can the authors provide an estimate of how many additional environment steps these evaluations require?\n\nQ3: Can CURATE be adapted to environments with continuous task parameters, where task difficulty changes smoothly (e.g., obstacle density or agent's speed)? How would the Gaussian curriculum policy and REPS update handle such cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oxfSahP1jH", "forum": "7wdCgG6K7i", "replyto": "7wdCgG6K7i", "signatures": ["ICLR.cc/2026/Conference/Submission15449/Reviewer_YzWB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15449/Reviewer_YzWB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937125483, "cdate": 1761937125483, "tmdate": 1762925735755, "mdate": 1762925735755, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CURATE, an automatic curriculum learning algorithm for reinforcement learning agents. CURATE frames curriculum generation as a policy search problem in task space, where a Gaussian curriculum policy selects tasks based on the agent’s current competence. By rewarding performance on “unsolved but easiest” tasks and updating the task distribution using Relative Entropy Policy Search (REPS), the method aims to produce an approximately easiest-to-hardest progression. Experiments on MiniGrid MultiRoom and Procgen Leaper compare CURATE with domain randomization, handcrafted incremental curricula, and automatic approaches such as Robust PLR and ACCEL. Results show that CURATE can learn task-directed curricula and achieve better sample efficiency than the unsupervised baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Proposes a clear formulation of curriculum policy search based on competence, with explicit loss terms and an implementable algorithm.\n\n2. Includes quantitative comparisons with both non-learning and learning-based curriculum methods.\n\n3. Uses two distinct RL domains (grid-based and image-based) to demonstrate the approach.\n\n4. Provides full pseudocode and implementation details, which supports reproducibility."}, "weaknesses": {"value": "1. Unclear novelty and positioning - The abstract and introduction do not clearly articulate what distinguishes CURATE from prior teacher-student or learning-progress methods (e.g., ALP-GMM, bandit-based curricula).\n\n2. Lack of theoretical grounding - The main insight that “performance on nearby tasks increases inversely with difficulty” is presented empirically, without discussion of the assumptions or limits under which it holds.\n\n3. Potential local minima - Because CURATE samples from tasks the agent already performs well on, it is unclear how it avoids getting stuck in competence plateaus or repetitive easy tasks.\n\n4. Assumption of monotonic difficulty - The claim that “easier tasks yield higher returns” may not generalize.\n\n5. Reward threshold - R_S is critical to defining competence but is only described in the appendix. Its determination, sensitivity, and generality are not discussed in the main text.\n\n6. Key objective terms deferred to appendix – Loss components L_diff and L_dist are central to how CURATE functions but are explained only in Appendix B.1, with minimal intuition in the main body.\n\n7. Limited experimental scope - Only two domains are evaluated, both with low-dimensional, discrete tasks, making it difficult to claim broader applicability.\n\n8. Missing ablations and trajectory comparisons – There is no ablation of loss terms or analysis of the actual task trajectories compared to other methods.\n\n9. Ambiguous statement about “best initial tasks” – The paper claims CURATE finds the “best” initial task set (line 365) without defining what constitutes “best” or whether it is benchmarked against any ground truth."}, "questions": {"value": "1. How is R_S chosen in practice, and how sensitive is performance to this value?\n\n2. What prevents CURATE from remaining in a local optimum of easy tasks that yield consistently high returns?\n\n3. Under what formal assumptions does the “nearby task improvement” insight hold true?\n\n4. Why were the key curriculum objective terms (L_diff, L_dist) placed in the appendix, and how do they affect learning if removed or modified?\n\n5. Can you provide quantitative comparisons of the task trajectories (sequence or spread of sampled tasks) between CURATE and baselines such as ACCEL or PLR⊥?\n\n6. Would the method still perform effectively in task spaces that are not strictly monotonic in difficulty or where difficulty is non-separable across dimensions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eTnJfTkDDL", "forum": "7wdCgG6K7i", "replyto": "7wdCgG6K7i", "signatures": ["ICLR.cc/2026/Conference/Submission15449/Reviewer_JQLZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15449/Reviewer_JQLZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942834741, "cdate": 1761942834741, "tmdate": 1762925734772, "mdate": 1762925734772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a curriculum learning approach for RL tasks where the curriculum is evaluated based on task difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The proposed method is intuitive."}, "weaknesses": {"value": "* The framework has a strong assumption that the environments are fully parameterized within space $\\Theta$. This raises practical concerns about whether such an assumption holds for generic decision-making tasks. More non-game examples and also clarifications on the limitations of the method would help address the concern. \n* Critical values such as initial $\\mu_\\theta, \\Sigma_\\theta$ are treated as hyperparameters to be tuned. Either ablations on showing robustness of these hyperparameters or discussions on how to choose them would make the framework more practical for general tasks. \n* Results are validated on simulated tasks either with low-dimensional task space, or with human-engineered procedural environment distributions, with oracal access to the procedural parameters to generate the environments being benchmarked. These simulated domains are much simpler than real-world or general decision-making tasks."}, "questions": {"value": "* The environment distribution seems to be a Gaussian with mean $\\mu_\\theta$ and covariance $\\Sigma_\\theta$, although I didn't find it clearly defined in the manuscript (but it's possible I missed it)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "m6quwO9mxv", "forum": "7wdCgG6K7i", "replyto": "7wdCgG6K7i", "signatures": ["ICLR.cc/2026/Conference/Submission15449/Reviewer_WGga"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15449/Reviewer_WGga"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979919719, "cdate": 1761979919719, "tmdate": 1762925733878, "mdate": 1762925733878, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents CURATE, an automatic curriculum learning algorithm for reinforcement learning. CURATE conducts policy search in task space, dynamically adapting the curriculum based on the agent’s competence to progress from easier to harder tasks. Evaluated on MiniGrid MultiRoom and the Procgen Leaper environments, CURATE achieves higher sample efficiency than other baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. CURATE is conceptually well-motivated, formulating curriculum design as a policy search over task distributions guided by agent competence.\n2. The paper provides a careful qualitative and quantitative examination of the learned curricula, including how the agent starts with easier tasks, maintains narrow and focused task distributions, and progresses in a generally easiest-to-hardest trajectory. The visualizations  effectively illustrate the evolution and path of the curriculum in the task space.\n3. The paper introduces the Procgen Curriculum Suite, a new environment designed to evaluate curriculum learning. Given the lack of effective tools for rapid assessment of curriculum methods on parameterized tasks, this contribution appears promising."}, "weaknesses": {"value": "1. A key limitation of CURATE is its reliance on a fully defined task space with clear difficulty gradients, limiting its use in many RL environments that lack this or need significant effort to specify. This limitation is noted but not fully explored (see Questions 1, 2).\n2. The baseline selection is not entirely fair. ACCEL and PLR are designed for generalization across many tasks (retaining and revisiting previously learned tasks throughout training) and lack prior knowledge of task difficulty until attempting to learn each task. These constraints inherently limit their sample efficiency compared to CURATE, which assumes difficulty is known a priori. A more equitable comparison would be against TSCL, which evaluates performance across all tasks and is designed for environments with a small number of tasks. This would balance CURATE's advantage of having prior difficulty information against TSCL's ability to evaluate all tasks .\n3. CURATE is only evaluated on two relatively simple environments with few parameters, which were specifically selected for this method. It would be more informative to test CURATE on widely used benchmarks for curriculum learning, such as BipedalWalker, which includes multiple configurable parameters and has been used to assess POET and ACCEL. Broader evaluation would better demonstrate CURATE’s generalizability and robustness. \n4. The description of the method lacks clarity and is difficult to understand on the first reading. As a result, readers often need to revisit the pseudocode for better understanding. Including a visual diagram of the method would make the section much clearer and improve the overall presentation.\n\nReferences\n- [ACCEL] Parker-Holder, J., Jiang, M., Dennis, M., Samvelyan, M., Foerster, J., Grefenstette, E. and Rocktäschel, T., 2022, June. Evolving curricula with regret-based environment design. In International Conference on Machine Learning (pp. 17473-17498). PMLR.\n- [PLR] Jiang, M., Grefenstette, E. and Rocktäschel, T., 2021, July. Prioritized level replay. In International Conference on Machine Learning (pp. 4940-4950). PMLR.\n- [ TSCL ] Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-Student Curriculum Learning. IEEE Transactions on Neural Networks and Learning Systems, 31(9):3732–3740, 2019.\n- [ POET ] Wang, R., Lehman, J., Clune, J. and Stanley, K.O., 2019, July. Poet: open-ended coevolution of environments and their optimized solutions. In _Proceedings of the genetic and evolutionary computation conference_ (pp. 142-151)."}, "questions": {"value": "Questions on task parametrization\n1. How could the constraint linking task complexity to the parameter value be addressed?\n2. If the principal axes are not perfectly disentangled, how robust is CURATE? Does it degrade gracefully or collapse?\n\nQuestions on environments\n\n3. What criteria were used to select the parameters for task configuration in Procgen?\n4. Have you experimented with the other curriculum-based version of Procgen (C-Procgen)? In what key aspects does  Procgen Curriculum Suite differ from theirs?\n5. Why were the experiments not performed in a standard environment where other curricula were evaluated? Is this decision connected to the method’s main limitation concerning task parametrization?\n\nQuestions on curriculum performance\n\n6. If the environment has a large task space, does the model forget simpler tasks over time since the method lacks a built-in mechanism to revisit them?\n\nReferences\n- [C-Procgen] Tan, Z., Wang, K., Wang, X., 2023. C-Procgen: Empowering Procgen with Controllable Contexts. [https://doi.org/10.48550/arXiv.2311.07312](https://doi.org/10.48550/arXiv.2311.07312)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NKkJ2ykpro", "forum": "7wdCgG6K7i", "replyto": "7wdCgG6K7i", "signatures": ["ICLR.cc/2026/Conference/Submission15449/Reviewer_sBQV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15449/Reviewer_sBQV"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994102210, "cdate": 1761994102210, "tmdate": 1762925733548, "mdate": 1762925733548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}