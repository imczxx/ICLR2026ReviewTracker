{"id": "cSDXx8V6K9", "number": 12729, "cdate": 1758209825662, "mdate": 1759897491090, "content": {"title": "Generating metamers of human scene understanding", "abstract": "Human vision combines low-resolution “gist” information from the visual periphery with sparse but high-resolution information from fixated locations to construct a coherent understanding of a visual scene. In this paper, we introduce MetamerGen, a tool for generating scenes that are aligned with latent human scene representations. MetamerGen is a latent diffusion model that combines peripherally obtained scene gist information with information obtained from scene-viewing fixations to generate image metamers for what humans understand after viewing a scene. Generating images from both high and low resolution (i.e. “foveated”) inputs constitutes a novel image-to-image synthesis problem, which we tackle by introducing a dual-stream representation of the foveated scenes consisting of DINOv2 tokens that fuse detailed features from fixated areas with peripherally degraded features capturing scene context. To evaluate the perceptual alignment of MetamerGen generated images to latent human scene representations, we conducted a same-different behavioral experiment where participants were asked for a “same” or “different” response between the generated and the original image. With that, we identify scene generations that are indeed metamers for the latent scene representations formed by the viewers. MetamerGen is a powerful tool for understanding scene understanding. Our proof-of-concept analyses uncovered specific features at multiple levels of visual processing that contributed to human judgments. While it can generate metamers even conditioned on random fixations, we find that high-level semantic alignment most strongly predicts metamerism when the generated scenes are conditioned on viewers’ own fixated regions.", "tldr": "", "keywords": ["human scene understanding", "generative modeling"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3ec98b01d4fb21b959bf2d8d8e5c37a238214041.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces MetamerGen, an image-to-image framework that synthesizes scene understanding metamers for a viewer’s internal scene representation by conditioning a Stable Diffusion backbone on (i) DINOv2 patch tokens extracted from fixated (high-res) regions and (ii) DINOv2 tokens from a blurred peripheral image. The model is evaluated with a gaze-contingent same-/different behavioral paradigm that measures whether humans judge generated scene maetamers as the “same” scene after a brief re-presentation. The paper further analyzes which levels of visual features (low, mid, high) predict metameric judgments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Formulation of a foveated image-to-image synthesis task that fuses sparse foveal tokens and blurry peripheral tokens into a single latent diffusion generator.  \n\nA real-time gaze-contingent same/different behavioral paradigm (45 participants) showing that many generated images are judged “same” (i.e., metameric) and that semantic/high-level alignment best predicts metamerism for human-fixation conditioning.  \n\nThe paper repurposes the concept of metamers (historically a low-level vision/color concept) as a latent generative modeling problem conditioned on gaze. This is a clever insight that links cognitive science, visual neuroscience, and modern diffusion modeling.\nThe “foveal + peripheral token fusion” formulation is conceptually neat\nUsing human fixation maps as structured conditioning signals is a creative way to connect psychophysical data with generative models, and to probe which representations are perceptually sufficient for “scene sameness.”\n\nThe method is technically up-to-date and grounded in state-of-the-art visual encoders (DINOv2) and latent diffusion architectures (Stable Diffusion)."}, "weaknesses": {"value": "Role / necessity of foveation is not fully ablated or isolated: \nThe paper reports that MetamerGen can generate metamers conditioned on random fixations as well as human fixations and reports similar overall fooling rates (27.7% vs 29.4%) but then emphasizes the scientific value of human fixations because they produce stronger correlations with feature hierarchies.  This raises two concerns.  If random fixations can produce similar fool rates, how much unique value does foveation / fixation conditioning add for producing perceptually aligned hypotheses (not just for fooling)?\nThere is no explicit ablation that removes the foveal stream entirely (i.e., peripheral only) or removes the peripheral stream (foveal only) and compares behavioral / feature alignment. A single-factor ablation is missing.\n\nSuitability of DINOv2 patch tokens as an explicit model of foveal or parafoveal signals.\nThe approach treats a DINOv2 patch token (~14x14 pixels at 448 input → 32x32 grid) as if it corresponds to the information a foveal fixation provides. But DINOv2 was trained on normal (non-gaze-conditioned) images; the authors acknowledge that patch tokens also encode some context. The paper does not show that DINOv2 patch tokens (or their masked/compressed form) are the best representation for foveal information vs alternatives (CLIP token, a CNN trained with foveated inputs, or even raw pixel crops).  \n\nPatch ↔ fovea geometric mapping appears simplistic.\nA fixation is mapped to the nearest single 14x14 patch token. The visual system’s foveal footprint (receptor sampling + acuity falloff) is more continuous and covers multiple patches; using a single patch may under/overestimate the foveal information.  The authors also need to justify the 14x14 path representing the fovea’s DVA. \n\nPossible confounds in stimuli & filtering\nThe stimulus set excluded faces/hands etc. to accommodate Stable Diffusions limitations (of course, reasonable), but this restricts generality to many real-world scenes.\n\nVisual vs. Semantic Metamers:\nTheir psychophysical validation is a same/different task.  Although the paper makes important claims about the semantic nature of the metamers, how are they different from the classic visual metamers?  Would the proposed semantic metamers result in lower ability of observers to tell them apart than using visual metamers?\nThis seems to be missing in the paper."}, "questions": {"value": "There is a tradition of visual metamers that could be used as a baseline. There is the Freeman and Simoncelli (Nat Neuro) traditional methods, and also newer methods that were developed based on style transfer by Deze at al., 2019 (Neurips).\n In principle, the semantic methods should be better than the pure visual for some tasks (unsure if the one used by the authors). \nThis should be an important control.  \n\nThis leads to the next question.  Couldn’t we have two scenes that are clearly different under unlimited viewing, such that observers can easily tell them apart, yet they are still scene metamers because the differences are irrelevant to scene understanding? For example, consider two scenes that are visually distinct but convey the same high-level meaning: a white truck driving on the highway versus a black truck driving on the freeway. Although their low-level image statistics differ, both scenes represent the same event or concept. In that sense, metamerism could arise from equivalence in interpretation, not appearance, suggesting that scene metamers might exist even without any gaze or foveation constraints.\n\n\nAblations needed to strengthen the paper:\nNo-fovea baseline (peripheral-only)\nNo-periphery baseline (fovea-only)\nTests of DINOv2 suitability"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gXRoGe0oVO", "forum": "cSDXx8V6K9", "replyto": "cSDXx8V6K9", "signatures": ["ICLR.cc/2026/Conference/Submission12729/Reviewer_y8G9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12729/Reviewer_y8G9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761936962494, "cdate": 1761936962494, "tmdate": 1762923551109, "mdate": 1762923551109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper is about a very interesting neuroAI study. The problem is to understand when humans consider two pictures to be the same. The authors train a DiT model to generate a different picture (metamer) from an original picture that can fool human subjects into considering they are the same. The DiT is fine-tuned from a pretrained model, adding cross-attention conditioning on the original picture’s features. The features are obtained from DINO. The authors do several in-depth analyses on comparing behavior patterns between different conditions, and between human and CNN and CLIP. The authors also identified that it is the high-level feature that contributes to the metamer perception. Overall, the study provides a novel paradigm to study human scene understanding and shows mechanistic similarity between human scene perception and DNN scene perception."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Overall, this is a solid study with rigorous human subject experiments and valid deep learning algorithms. It provides a novel paradigm to study human visual scene perception. The analysis provides insights into the similarity between DNN and human scene perception, and which level of features is critical."}, "weaknesses": {"value": "**Insufficient background on human visual scene perception**\n\nConsidering the paper is submitted to a machine learning venue, neuroscience/psychology jargon should be explained. Terms like \"metamer\" should be defined clearly early on, and traditional study paradigms on them should be introduced.\n\n**Figure 1**\n\nInconsistent color in Figure 1. in the top part, the fixation is colored blue, but in the bottom part, it is red.\n\n**Structure**\n\nImportant results, such as line 478-479 \"While MetamerGen was trained to predict images from randomly sampled locations—and\nsuch generations fooled participants at a rate comparable to those conditioned on their own fixations\n(27.7% vs. 29.4%)\" is hidden in the discussion, rather than in the result section. This is important information for knowing how effective the generated metamers are under different conditions."}, "questions": {"value": "**Physical distance between metamer and original image**\n\nThe training and inference process does not regularize the distance between the generated image and the original image. Is it possible their training process learn to recover the original image based on fixation and peripheral condition. So, the generated images are for sure metamers because they are the same or very similar to the original image? Given that the generated images are not always metamers, is there a way to regularize, such that by controlling the distance in some metric space, the generated images are more likely to be metamers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RVK1cpf9Ef", "forum": "cSDXx8V6K9", "replyto": "cSDXx8V6K9", "signatures": ["ICLR.cc/2026/Conference/Submission12729/Reviewer_KkGc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12729/Reviewer_KkGc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978818847, "cdate": 1761978818847, "tmdate": 1762923550437, "mdate": 1762923550437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MetamerGen, a computational model designed to generate visual \"metamers\"—images that are physically different from an original but are perceived as the same by a human observer. The model, based on a latent diffusion architecture, is conditioned using a dual-stream input that mimics human vision: sparse, high-resolution features from specific fixation points (foveal vision) and blurred, low-resolution features from the entire scene (peripheral vision). This paper utilized a latent diffusion transformer to reveal how humans perceive visual information.\n\nTo validate the model, the authors conducted a real-time behavioral experiment where a participant's eye movements were tracked while viewing an image. These fixation data were then used to generate a new image with MetamerGen, which the participant had to judge as \"same\" or \"different\" from the original. The study's analysis reveals that high-level semantic similarity between the original and the generated image is the strongest predictor for a \"same\" judgment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduced a method to understand how humans process visual information, and what kind of information/feature matters in visual perception.\n\nThis paper designs a reasonable experimental setting to interpret human scene understanding by tracking eye movement and leveraging the movement to generate \"fake\" images using LDM, and asks the tester to justify the fake images.\n\nThe findings provide strong evidence that human memory and understanding of a scene rely heavily on high-level semantic information (e.g., object identity and spatial relationships) rather than on precise, pixel-level details."}, "weaknesses": {"value": "This paper only considers a fixed-resolution scenario. It would be interesting to investigate whether varying aspect ratio has an impact on human scene understanding. For example, give a larger resolution version of the original image or give a 9:16 image (the original is 1:1), which adds or removes some side content and maintains the center content."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "imY4QUheBS", "forum": "cSDXx8V6K9", "replyto": "cSDXx8V6K9", "signatures": ["ICLR.cc/2026/Conference/Submission12729/Reviewer_jZsD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12729/Reviewer_jZsD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12729/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012593480, "cdate": 1762012593480, "tmdate": 1762923549982, "mdate": 1762923549982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}