{"id": "khjGuuYg8j", "number": 17983, "cdate": 1758282643938, "mdate": 1759897141143, "content": {"title": "UDANG: Unsupervised Domain Adaptation with Neural Gating for learning invariant representation of subspaces", "abstract": "The key assumption of deep learning is that the data that the model will be tested on (target domain) are drawn from the same distribution as the data it was trained on (source domain). Breaking this assumption can lead to a significant drop in performance despite having similar underlying features between the source and target domains. Unsupervised Domain Adaptation (UDA) involves using unlabeled samples from the target domain, in addition to labeled samples from source domain, to train a model that can perform well on the target domain. Many existing UDA approaches rely on domain adversarial training (DAT) to reduce domain shift. Although effective, they do not explicitly disentangle the learned features into task-specific and domain-specific components. As a result, the features despite appearing to be domain invariant, may still contain domain-specific biases. To address this, we propose a novel method, UDA with Neural Gating (UDANG), that utilizes a dual adversarial objective to learn an adaptive gating which dynamically route each feature dimension to either the domain or task subspace. Using our strategy, networks have the ability to effectively disentangle task-specific features from domain-specific ones. We validated our approach in multiple datasets and network architectures for image classification, demonstrating strong adaptation performance while retaining the features for discerning the domain.", "tldr": "", "keywords": ["UDA", "unsupervised domain adaptation", "representation learning", "image classification"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/63a1fd14e57c358e47f773ce69284dbca63a3bda.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes UDANG, an unsupervised domain adaptation framework that introduces a neural gating mechanism to disentangle task-relevant and domain-specific subspaces. Building upon the Domain Adversarial Training (DAT) paradigm, UDANG adds a dual adversarial objective: one discriminator for task alignment and another for domain alignment. A Gumbel–Sigmoid gating function dynamically routes feature dimensions into the task or domain subspace during training, aiming to learn invariant representations. Experiments on VisDA-2017 and Office-31 demonstrate competitive performance compared to recent UDA baselines such as MIC, SDAT, and TVT."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is built around a well-defined goal — separating task-relevant and domain-specific representations in UDA — and follows this motivation throughout. The narrative is coherent and easy to follow.\n\n2. The experimental setup is carefully designed, with appropriate baselines, multiple datasets, and detailed ablations. The reported gains are consistent and appear reproducible.\n\n3. The introduction of a gating module makes the feature routing process more transparent, and the visualizations help to qualitatively understand the model’s behavior.\n\n4. The manuscript reads smoothly and provides sufficient detail to reproduce the method. Figures are informative and the comparisons are fair."}, "weaknesses": {"value": "1. The overall framework is still built upon standard adversarial adaptation (DAT) and domain separation ideas. The proposed neural gating resembles earlier disentanglement strategies, offering more of a structural variation than a conceptual breakthrough.\n\n2. The gating mechanism is introduced intuitively, but there is no analysis explaining why it should improve invariance or stability. The dual-adversarial setting also lacks formal discussion on convergence or disentanglement guarantees.\n\n3. Although GradCAM plots are shown, the work does not measure or validate how well the gated subspaces are separated or independent.\n\n4. Experiments are restricted to image classification tasks. It is unclear whether the method generalizes to other modalities or more complex adaptation settings such as segmentation or multi-source transfer.\n\n5. The paper would benefit from a sharper articulation of how UDANG differs from established models like DSN or CDAN. Currently, the overlap is substantial, which makes the contribution appear incremental."}, "questions": {"value": "1. How is UDANG fundamentally different from Domain Separation Networks (DSN) or Conditional Adversarial DA (CDAN)?\n\n2. If the gating module is removed, does the dual-adversarial structure still perform comparably?\n\n3. How sensitive is the model to the gating temperature parameter (τ)?\n\n4. Have you tested cross-domain generalization beyond visual datasets?\n\n5. Is it possible to drop the domain discriminator during inference to reduce computational overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fXkxvBYCUP", "forum": "khjGuuYg8j", "replyto": "khjGuuYg8j", "signatures": ["ICLR.cc/2026/Conference/Submission17983/Reviewer_L5jG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17983/Reviewer_L5jG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760455696063, "cdate": 1760455696063, "tmdate": 1762927775688, "mdate": 1762927775688, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UDANG, a UDA method that disentangles task-specific vs domain-specific features via a dual adversarial objective and an attention-style gating that routes each feature dimension into a “task” or “domain” subspace. The loss couples DAT with a task-side adversary and MCC on target; gates use a Gumbel-Sigmoid hard/soft path. Evaluated on VisDA-2017 and Office-31 with ResNet/ViT backbones, UDANG is competitive with recent SOTAs and wins some Office-31 transfers; qualitative UMAP/Grad-CAM support is provided."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear objective & architecture:** a tidy extension of DAT with symmetric adversaries and MCC; the gating mechanism is simple and differentiable.  \n2. **Empirical competitiveness:** ≥90% mean on VisDA-2017 (ViT) and SOTA on 4/6 Office-31 tasks; results span CNN/ViT. \n3. **Diagnostics:** UMAP class/domain separation and Grad-CAM analyses are helpful; authors discuss an observed bias case."}, "weaknesses": {"value": "1. **Novelty vs prior disentanglement/DAT:** The idea of separating task/domain subspaces with adversaries echoes domain-separation lines; the paper would benefit from a sharper contrast to DSN-style methods and MIC-like masking beyond qualitative claims. (Related work listed, but positioning remains light.) \n2. **Gating evidence:** No quantitative probe of gate assignments (e.g., sparsity, stability across seeds, correlation with domain cues). The Gumbel temperature is fixed; sensitivity is unknown.  \n3. **Ablation depth:** Missing ablations for each loss term (remove MCC / each adversary / gate ->  identity), and for alternative routers (soft masks, top-k, per-token vs per-channel). Current gains are solid but not uniformly superior to MIC/TVT on VisDA. \n4. **Theory claims:** The method is positioned as learning “invariant representations of subspaces,” but there’s no formal identifiability or invariance guarantee, largely empirical."}, "questions": {"value": "See weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dZy6D9iGLc", "forum": "khjGuuYg8j", "replyto": "khjGuuYg8j", "signatures": ["ICLR.cc/2026/Conference/Submission17983/Reviewer_tZKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17983/Reviewer_tZKV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761327292259, "cdate": 1761327292259, "tmdate": 1762927774695, "mdate": 1762927774695, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve the drawback of the previous domain adversarial training methods. The authors propose UDA with Neural Gating, that utilizes a dual adversarial objective to learn an adaptive gating which dynamically route each feature dimension to either the domain or task subspace."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method is straightforward."}, "weaknesses": {"value": "- It seems that the proposed method is not superior to previous methods in most cases. It is hard to persuade the reviewer that the proposed method is ready for an ICLR publication.\n- The presentation of this work may need improvement. For example, replacing the current figures with some high-definition ones.\n- Some arguments in the paper need further justification."}, "questions": {"value": "- In Table 1 and Table 2, the reviewer sees that the method is no better than some methods proposed in 2019 (CAN in Table 1 and Table 2, TVT in Table 2). In this case, why would we choose the proposed UDANG method?\n  - Are there any more recent works, for example, some works published in the 2024/2025 conference and journals. The baselines come from more than two years ago in Table 1 and Table 2. The reviewer thinks we need more recent baselines to show the superiority of the proposed method.\n\n- What are the core contributions of the work? The Attention Gating Network or the dual branch design? How do different components contribute to the whole system? The reviewer thinks we need some analysis of the different components in the system.\n\n- In lines 072-073, the authors claim \"While foundation models learn broad representations, they do not inherently address the fundamental problem of domain shift.\" Why \"they do not inherently address the fundamental problem of domain shift.\"? Is there any empirical evidence for this claim. We know that there are recent VLMs like Qwen-VL, the training data of such VLMs is large-scale, will the suffer from the issue?\n\n- When the reviewer zooms in the figures in the paper, the reviewer finds that it is not clear. Could the authors replace these figures with some high-definition ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qhmCWK8sSa", "forum": "khjGuuYg8j", "replyto": "khjGuuYg8j", "signatures": ["ICLR.cc/2026/Conference/Submission17983/Reviewer_1EqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17983/Reviewer_1EqF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17983/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818094123, "cdate": 1761818094123, "tmdate": 1762927773858, "mdate": 1762927773858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers and AC for reviewing our work and providing insightful feedback to help us improve on our work. We are especially encouraged that reviewers found that we have a “clear objective & architecture”, \"empirically competitive\", \"experimental setup is carefully designed\", and that our new diagnostics on UDA/DA tasks were appreciated. \n\nOn novelty, we would like to highlight that other disentanglement work either focuses on having a feature space that is fully domain invariant or fixed domain invariant subspace. Our gating mechanism does not have this restriction, retains domain specific features before gating, and allows for different classes to have different invariant subspaces, selected by the gating mechanism,\n\nWe address the concerns of each reviewer individually in further details in replies to each reviewer."}}, "id": "63WcGczCop", "forum": "khjGuuYg8j", "replyto": "khjGuuYg8j", "signatures": ["ICLR.cc/2026/Conference/Submission17983/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17983/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission17983/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763578829836, "cdate": 1763578829836, "tmdate": 1763578829836, "mdate": 1763578829836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}