{"id": "3TM5xfS1m7", "number": 8767, "cdate": 1758097554839, "mdate": 1759897764880, "content": {"title": "VAL-Bench: Measuring value alignment in Language Models", "abstract": "Large language models (LLMs) are increasingly used for tasks where outputs shape human decisions, so it is critical to test whether their responses reflect consistent human values. Existing benchmarks mostly track refusals or predefined safety violations, but these only check rule compliance and do not reveal whether a model upholds a coherent value system when facing controversial real-world issues. We introduce the Value ALignment Benchmark (VAL-Bench), which evaluates whether models maintain a stable value stance across paired prompts that frame opposing sides of public debates. VAL-Bench consists of 115K such pairs from Wikipedia’s controversial sections. A well-aligned model should express similar underlying views regardless of framing, which we measure using an LLM-as-judge to score agreement or divergence between paired responses. Applied across leading open- and closed-source models, the benchmark reveals large variation in alignment and highlights trade-offs between safety strategies (e.g., refusals) and more expressive value systems. By providing a scalable, reproducible benchmark, VAL-Bench enables systematic comparison of how reliably LLMs embody human values.", "tldr": "A diverse benchmark for systematic analysis of a how reliably language models embody human values.", "keywords": ["Value Alignment", "Alignment", "Safety"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68dd815b70c79de82cca677823cab47a9c1f3ab4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents VAL-Bench, a novel benchmark designed to evaluate the value alignment of large language models. The authors address the limitations of existing benchmarks, which primarily track rule compliance rather than the coherence of a model's underlying value system. VAL-Bench is constructed from 115,000 paired prompts mined from controversial Wikipedia sections, presenting opposing framings of the same issue. The core evaluation measures whether a model maintains a consistent value stance when responding to both prompts in a pair, using an LLM-as-judge to assess agreement. The empirical evaluation across leading models demonstrates significant variance in alignment and exposes a clear trade off between value consistency, which is sometimes achieved via refusals, and more expressive value systems."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of this paper is relatively meaningful, and studying whether LLMs can maintain consistent values under different queries is indeed an important approach for evaluating value alignment.\n\nThe datasets provided in this paper are all derived from real-world data, with a relatively substantial volume. The paper also performs calibration on the evaluator, which to some extent verifies its reliability."}, "weaknesses": {"value": "As a benchmark work for value assessment, this paper falls short in its discussion of values themselves. What exactly are the values that matter for large language models, and which ones truly warrant evaluation through such a consistency-based approach? The ten values inspired by prompts used in this paper are clearly insufficient to answer this question, deeper definitions and analyses are needed.\n\nThe Related Work section is extremely rough and lacks a substantial literature review. I noticed from the authors’ disclosure of LLM use that this section was actually generated by an LLM, which explains the issue. On the other hand, the paper claims to be the first to conduct a consistency-based value assessment, but to my knowledge, at least [1], [2], and [3] have carried out similar studies. While those works did not construct datasets in the same way as this paper, the authors should have at least reviewed these highly relevant studies and reconsidered their paper’s contribution to the field.\n\nThe definition of the evaluation metric in this paper is unclear, and the mathematical symbols used are neither defined nor properly explained.\n\nThe presentation of the experimental results is also not clear enough. For instance, the calibration results in Table 4 would be much clearer if presented using heatmaps or distribution plots. It would also be preferable to include consistency evaluation and present Pearson correlation coefficients.\n\nThe sections Disclosure of LLM Use and Ethics Statement appear twice, and the first occurrence is incomplete.\n\nA mild issue is that the overall writing quality of the paper is quite poor, especially in the experimental section, where the basic structure and rigor are lacking.\n\n**Reference**\n\n[1] ValueDCG: Measuring Comprehensive Human Value Understanding Ability of Language Models (https://arxiv.org/abs/2310.00378)\n\n[2] Do LLMs have Consistent Values? (https://arxiv.org/abs/2407.12878)\n\n[3] Are Large Language Models Consistent over Value-laden Questions? (https://arxiv.org/abs/2407.02996)"}, "questions": {"value": "I find it a bit strange that LLMs are expected to maintain a consistent value stance on controversial issues, since humans themselves don’t have a unified consensus. What kind of value position should we really want LLMs to take? Perhaps diversity is actually what we need?\n\nGiven that this paper has constructed such a large dataset and conducted related experiments, evaluating only consistency seems somewhat insufficient, it should also be able to present the results regarding the LLMs’ value positions.\n\nOther questions see the **Weaknesses** section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "wehXb3J9M8", "forum": "3TM5xfS1m7", "replyto": "3TM5xfS1m7", "signatures": ["ICLR.cc/2026/Conference/Submission8767/Reviewer_8XHj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8767/Reviewer_8XHj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761031668404, "cdate": 1761031668404, "tmdate": 1762920549665, "mdate": 1762920549665, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark for evaluating LLM value consistency when confronting controversial real-world issues. Unlike existing benchmarks that primarily focus on refusal rates or predefined safety violations, VAL-Bench assesses how consistently LLMs maintain a stable value stance across paired prompts that present opposing sides of public debates.\n\nThe benchmark comprises 115K such pairs derived from Wikipedia's controversial sections. It utilizes an LLM-as-judge to measure the agreement or divergence in responses to these paired prompts, thereby revealing the model's underlying views. The authors applied VAL-Bench to various leading open- and closed-source models and observed significant variation in consistency, along with trade-offs between safety strategies (like refusals) and more expressive value systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper shifts safety evaluation from rule-following to value consistency under framing changes, a gap not well addressed by toxicity/harms benchmarks.\n\n- This paper mines its datasets from real Wiki content and scale up to 115K prompt pairs. It conducts extensive evaluations based on it.\n\n- This work conducts evaluator calibration, which is essential to ensure the reliability of the llm judge."}, "weaknesses": {"value": "- The paper lacks an overall diagram to visualize how the dataset is constructed and how the value consistency of LLMs is evaluated. A schematic diagram would greatly facilitate understanding, as the current writing is somewhat hard to follow.\n\n- The dataset construction relies on prompting Gemma-3, but there appears to be no human validation of the dataset quality.\n\n- The discussion of highly related work is insufficient. Section 6.1 of [1] reviews a substantial body of research aimed at evaluating the value consistency of LLMs; however, this paper rarely engages with these studies. It is important to elaborate on the advantages of the proposed benchmark over this line of work.\n\n- Psychometrics and measurement science offer several theoretically and statistically grounded metrics for assessing the consistency and reliability of measurement results, e.g., Intraclass Correlation Coefficient (ICC). It would be beneficial to adopt these existing metrics rather than introducing new ones.\n\n- The title of this paper is overly generic and claims larger that it actually evaluates.\n\n[1] Large language model psychometrics: A systematic review of evaluation, validation, and enhancement"}, "questions": {"value": "Please refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lXGBX3xKLs", "forum": "3TM5xfS1m7", "replyto": "3TM5xfS1m7", "signatures": ["ICLR.cc/2026/Conference/Submission8767/Reviewer_Qeeg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8767/Reviewer_Qeeg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638745177, "cdate": 1761638745177, "tmdate": 1762920549025, "mdate": 1762920549025, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Considering LLMs can be increasingly used in scenarios where outputs shape human decisions, authors care about whether LLMs apply human values consistently across contexts. To address the limitation that existing benchmarks mainly focus on safety violations, they introduce the Value Alignment Benchmark (VAL-Bench), consisting of 115k pairs of for-prompt and against-prompt to measure whether LLMs are consistent across opposing framings. They benchmark both closed-source and open-source LLMs and obtain substantially different results across models, which they attribute to trade-offs between refusals and value expressivity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper investigates an important and timely question “whether LLMs hold a coherent value orientation when facing controversial real-world issues”. This is critical when people increasingly apply LLMs for decision-making.\n2. The authors construct a dataset.\n3. Evaluate a large number of LLMs, both open-sourced and close-sourced.\n4. Propose multiple metrics to measure the performance, with REF and PAC together to reveal some phenomena."}, "weaknesses": {"value": "1. Paper writing needs more clarification.\n\n(1) Line 118, what are “for prompt” and “against prompt”, descriptions and examples maybe required for better understanding.\n\n(2) Contributions in Line 69 and Line 71 seem to be redundant.\n\n(3) Equation 5 is referenced but absent from the main paper.\n\n2. Some experimental or data-processing settings need clarification.\n\n(1) What do different levels of Issue Awareness (1-5) mean?\n\n(2) How do you set hyperparameters such as temperature, top-p and the testing rounds for LLMs in the evaluation?\n\n3. Sec 4.2 entitled Evaluator calibration, only evaluates the accuracy of LLMs in detecting consistency but not calibrate the evaluator. Since human annotations are available, I think the data can be used to calibrate the evaluation of LLMs by fine-tuning or adjusting the prompts.\n\n4. The evaluation metrics, settings and conclusions are a little problematic.\n\n(1) The definition of human values in LLMs is confusing in this paper. Human values are inherently pluralistic and diverse, which are captured by LLMs through pre-training and universal alignment. Under the condition of such diverse human values, humans would not demonstrate a single stance toward these questions, so that LLMs would also not show a consistent stance. I think evaluations under this setting as this paper done is not so meaningful. It needs to ensure LLMs aligned to a specific human value first and then test the consistency of LLMs towards this value.\n\n(2) The consistency of human groups across these value-involving questions should also be evaluated and serve as a baseline for comparison.\n\n(3) The current for/against prompt pairs may not effectively capture consistency on the same value dimension; they might instead reflect different values underlying opposite stances. I think the pairs should focus on the same value to test consistency, removing impacts of other noisy factors."}, "questions": {"value": "1. Lines 40-41 seem redundant with Lines 34-36.\n2. Related works about “Datasets of Human Values and Alignment” is largely missing, and there are also many value evaluation datasets beyond safety violation but diverse human values.\n3. In Line 114, you mentioned using awareness to filter out sections that didn’t represent a divergent issues. What is the threshold for filtering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HwO7vzffkU", "forum": "3TM5xfS1m7", "replyto": "3TM5xfS1m7", "signatures": ["ICLR.cc/2026/Conference/Submission8767/Reviewer_S1wt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8767/Reviewer_S1wt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895774999, "cdate": 1761895774999, "tmdate": 1762920548559, "mdate": 1762920548559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is relevant to the value alignment of LLMs, and particularly, focuses on evaluating the value consistency (though it’s called alignment, which seems not quite appropriate in the context of LLMs) of diverse models. For this purpose, the authors conducted a large-scale benchmark, comprising 115k samples built upon controversial social issues from Wikipedia. Each sample consists of a ‘for prompt’ and an ‘against prompt’ connected to opposite value orientations, which is used to assess whether LLMs can express consistent value stance. With this benchmark, this work analyzes the consistency of diverse LLMs, covering open-sourced, proprietary, chat-based and reasoning-based ones. Besides, other aspects, like value expressiveness and sensitivity to social issue are also comprehensively studied."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-motivated and focused on an important research direction, i.e., the consistency of LLMs’ value stance, especially when LLMs are increasingly integrated into human daily life.\n\n2. The design of ‘for prompt’ and ‘against prompt’ is interesting, and the authors conducted many different experiments and analysis"}, "weaknesses": {"value": "The weakness lies in two aspects:\n\n1. The quality of the constructed VAL-Bench is not verified. The whole construction process is automated and highly relies on LLMs. In Sec.4.2 and Sec.4.2, the authors only verified the LLM judge’s reliability for ‘detecting consistency rather than value judgments‘ (line 206). This brings several problems: \n    \n    (a) It’s unclear whether the awareness scoring (line 113), ‘for/against’ prompt extraction (line 116), and value labelling (Sec.3.2, Sec.5.2) are reliable, as all these processes all rely on LLMs. As a result, we cannot ensure whether the samples are really controversial or value relevant. \n\n    (b) The verification of LLM judge for consistency detection is also problematic. In detail, the calibration dataset is also created by LLMs, whose quality is unclear. Besides, the human evaluation (Appendix) is not rigorous. The number of human annotators and inter-annotator agreement are not reported. The smaple size (200 in total) for human verification is too small.\n\n2. The other problem is the soundness of the experiment and metrics design. Though claimed as ‘measuring value alignment’, this paper actually only measures the consistency/robustness of LLMs to framing but uses social controversial issues as the context. Then, first, it’s unclear if there is any difference between this work and previous work on LLMs’ robustness/consistency. Second, it’s not guaranteed the measured consistency arises from LLMs’ underlying values. \n\nBesides, there are also other small issues:\n\n3. The definition of values, in Table 2 (b), is less rigorous, without being grounded in any value theories from social science. The phrases presented in Table 2 (b) seem intuitive, but it’s not appropriate to directly regard them as values.\n\n4. There are a lot of missing references. \n\n    (a) For studying LLMs’ value consistency:\n    - Moore et al., Are Large Language Models Consistent over Value-laden Questions? 2024\n    - Rozen et al., Do LLMs have Consistent Values? 2024\n\n    (b) For value evaluation of LLMs:\n    - Ren et al, ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models. 2024\n    - Duan et al., Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning. 2024\n    - Zhang et al., Heterogeneous Value Alignment Evaluation for Large Language Models. 2024\n    - Scherrer et al., Evaluating the Moral Beliefs Encoded in LLMs. 2023"}, "questions": {"value": "1. It seems the authors use multiple LLMs as judges. What are these LLMs, and how do you ensure their reliability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v4wWQ3DbaF", "forum": "3TM5xfS1m7", "replyto": "3TM5xfS1m7", "signatures": ["ICLR.cc/2026/Conference/Submission8767/Reviewer_xoYa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8767/Reviewer_xoYa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972309217, "cdate": 1761972309217, "tmdate": 1762920548051, "mdate": 1762920548051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}