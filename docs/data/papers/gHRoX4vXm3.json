{"id": "gHRoX4vXm3", "number": 3238, "cdate": 1757384062914, "mdate": 1759898100571, "content": {"title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence", "abstract": "Spatial intelligence is essential for multimodal large language models (MLLMs) operating in the complex physical world. Existing benchmarks, however, probe only single-image relations and thus fail to assess the multi-image spatial reasoning that real-world deployments demand. We introduce MMSI-Bench, a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision researchers spent more than 300 hours meticulously crafting 1,000 challenging, unambiguous multiple-choice questions from over 120,000 images, each paired with carefully designed distractors and a stepwise reasoning process. We conduct extensive experiments and evaluate 37 open-source and proprietary MLLMs, observing a wide gap: the strongest open-source model attains roughly 30\\% accuracy and OpenAI's GPT-5 reasoning model reaches 40\\%, while humans score 97\\%. These results underscore the challenging nature of MMSI-Bench and the substantial headroom for future research. Leveraging the annotated reasoning processes, we also provide an automated error analysis pipeline that diagnoses four dominant failure modes, including (1) grounding errors, (2) overlap-matching and scene-reconstruction errors, (3) situation-transformation reasoning errors, and (4) spatial-logic errors, offering insights for advancing spatial intelligence.", "tldr": "We propose a novel benchmark for multi-image spatial intelligence.", "keywords": ["Spatial Intelligence", "MLLM", "VLM", "VQA", "Benchmark", "3D Understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/36a4ca7d35e4add15788b34f1501819455968c48.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MMSI-Bench, a new VQA benchmark for evaluating the multi-image spatial intelligence of MLLMs. The dataset consists of 1,000 challenging, multiple-choice questions meticulously crafted by 3D-vision researchers, which are designed to be unanswerable from any single image. Each question is paired with a human-annotated, step-by-step reasoning process and is categorized into one of eleven spatial reasoning tasks. An extensive evaluation of 37 MLLMs reveals a substantial 55-point performance gap between human (97.2%) and SOTA model (41.9%) accuracy. The authors also provide an automated error analysis pipeline, identifying \"overlap-matching and scene-reconstruction\" as the dominant failure mode for current models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a clear and important gap in MLLM evaluation, moving beyond existing benchmarks that often focus on single-image reasoning or use automated templates.\n2. The dataset's fully human-centric curation process, involving 3D-vision experts and 8 diverse, real-world data sources, produces challenging and linguistically varied questions.\n3. The authors provide an extensive evaluation of 37 MLLMs, establishing a robust baseline and highlighting a massive performance gap between SOTA models and humans.\n4. The inclusion of an automated error analysis pipeline, which categorizes failures into intuitive types, provides concrete and actionable directions for future research.\n5. The investigation into prompting techniques, including a novel visual prompting method using feature matching, provides strong evidence that current models have fundamental limitations in this domain."}, "weaknesses": {"value": "1. The dataset size of 1,000 samples, while acknowledged as a result of costly human curation, is small. This raises questions about the in-depth diversity within each of the 10 sub-categories, which have ~100 samples or fewer on average.\n2. The \"multi-image\" claim feels overstated, as all categories except for \"Multi-Step Reasoning\" are explicitly constrained to using exactly two images.\n3. Key results that strongly support the benchmark's novelty, such as the poor performance of models specifically finetuned on other spatial datasets, are relegated to the appendix.\n4. The methodology for constructing \"Multi-Step Reasoning\" questions is underspecified, lacking detail on how annotators were guided to combine the basic task types."}, "questions": {"value": "1. You demonstrate that zero-shot linguistic and visual prompting fails to provide significant gains. Did you experiment with few-shot in-context learning by providing the full human-annotated reasoning chains as exemplars in the prompt?\n2. The error analysis in Figure 7 is aggregated across the entire dataset. Could you provide a breakdown of the error type distributions for the most challenging categories, specifically \"Multi-Step Reasoning\" and \"Motion (Camera)\"?\n3. The \"Positional Relationship\" category is very broad, covering six distinct sub-types. Do models show significant performance variation across these sub-categories (e.g., is \"Cam-Cam\" more difficult than \"Obj-Obj\")?\n4. What specific guidelines were given to the 3D-vision researchers for creating the \"Multi-Step Reasoning\" questions, and how did you ensure these tasks truly required a sequence of the basic spatial skills?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fjssrnUNr6", "forum": "gHRoX4vXm3", "replyto": "gHRoX4vXm3", "signatures": ["ICLR.cc/2026/Conference/Submission3238/Reviewer_FDLo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3238/Reviewer_FDLo"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508780072, "cdate": 1761508780072, "tmdate": 1762916619509, "mdate": 1762916619509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MMSI-Bench, a benchmark designed to evaluate multi-image spatial intelligence in multimodal large language models (MLLMs). The dataset contains 1,000 human-curated, multiple-choice questions requiring reasoning across multiple real-world images, with detailed reasoning annotations. The authors categorize 11 spatial reasoning tasks covering position, motion, and attributes, and benchmark 37 models. Results show a large gap between current MLLMs (best at \\~42%) and human performance (\\~97%), revealing major weaknesses in spatial reasoning across viewpoints. They also propose an automated error-analysis pipeline that identifies key failure types such as grounding and scene-reconstruction errors."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The focus on multi-image spatial reasoning fills a clear gap between single-image VQA and real-world embodied perception. The fully human-curated design adds credibility compared to prior template-based datasets.\n\nQuality: The taxonomy of spatial relations (camera, object, region) is systematic, and the annotation process with reasoning traces and multi-reviewer verification shows rigor. The large-scale evaluation across 37 models is comprehensive and carefully controlled.\n\nClarity: The paper is well organized with clear figures and strong examples of question categories. The error typology (grounding, scene reconstruction, situation transformation, spatial logic) provides insight beyond raw accuracy.\n\nSignificance: MMSI-Bench exposes a real bottleneck in MLLMs’ ability to perform grounded spatial reasoning. The benchmark can drive future work on embodied AI, robotics, and multi-view understanding."}, "weaknesses": {"value": "The dataset is still modest in scale (1k QA pairs), which limits generalization analysis. It would help to report variability or cross-split reliability. Many questions rely on human interpretation of viewpoint or direction. Some ambiguity might remain even with expert curation, which could affect reproducibility.\n\nThe evaluation metric focuses only on answer accuracy; assessing reasoning trace similarity (e.g., using annotated rationales) could reveal finer-grained improvements. While the benchmark is thorough, the paper lacks concrete recommendations or model design principles derived from the findings."}, "questions": {"value": "1. How was question difficulty calibrated beyond human answer time? Did annotators estimate complexity or confidence?\n\n2. Could reasoning annotations be used for training models (not just analysis)? If so, how does that affect overfitting?\n\n3. How consistent are human annotators across the four error types? Any quantitative measure?\n\n4. Did you observe differences between models trained with ego-centric data vs general web-image pretraining?\n\n5. How might MMSI-Bench interact with embodied datasets like Habitat or RoboBrain for active perception tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZYIakVKOE1", "forum": "gHRoX4vXm3", "replyto": "gHRoX4vXm3", "signatures": ["ICLR.cc/2026/Conference/Submission3238/Reviewer_AFVc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3238/Reviewer_AFVc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903990955, "cdate": 1761903990955, "tmdate": 1762916619279, "mdate": 1762916619279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MMSI-Bench, a benchmark designed to evaluate the multi-image spatial reasoning capabilities of multimodal large language models (MLLMs). The dataset was created manually by six 3D-vision researchers. It consists of 1,000 multiple-choice question-answer pairs and 1,990 unique images sourced from eight real-world datasets (Matterport3D, ScanNet, DTU, nuScenes, Waymo, AgiBot-World, DAVIS 2017, and Ego4D). The questions are categorized into 11 tasks, under 4 main categories - positional relationships, attribute, motion and multi-step reasoning. The questions revolve around 3 spatial elements - camera, objects, and region. Most questions, other than the multi-step reasoning questions, involve 2 input images. Another key contribution is that each question is accompanied by a human-authored, step-by-step reasoning chain.\n\nThe paper conducts a comprehensive evaluation of 37 MLLMs. The primary result is a massive performance gap: the best-performing model (GPT-5) achieves only 41.9% accuracy, while human-level performance is 97.2%. The results also show that multi-step reasoning is particularly challenging for models. The authors also report fine-tuning and prompting ablations both of which provide minimal to no benefit. Language prompting involved zero-shot chain-of-thought reasoning and visual prompting involves highlighting PATS correspondences between image pairs. Finally, the authors provide insights into the failure modes of the evaluated models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The benchmark's core strength is its manual, expert-driven annotation process. The questions are linguistically diverse, non-trivial, and require spatial understanding. The problem of multi-image spatial reasoning is highly relevant and timely for advancing embodied AI and robotics, and this paper clearly demonstrates a critical capability gap.\n2. The paper evaluates an extensive suite of 37 models, providing a valuable and comprehensive snapshot of the entire SOTA. The inclusion of \"Human Performance\" (97.2%) is a strong baseline that effectively contextualizes the low model scores.\n3. The findings that both advanced prompting (CoT, visual prompting) and fine-tuning fail to provide significant gains are valuable. These negative results strongly suggest that the models' failure is not a simple problem but a more fundamental capability deficit.\n4. The paper is well-written making it easy to read and follow."}, "weaknesses": {"value": "1. 1,000 samples is small size, especially when divided across 11 tasks. This limited scale is a direct trade-off for the high-quality manual annotation (300+ hours), but it makes the benchmark difficult to scale and creates a risk of models eventually overfitting to this specific test set.\n2. Blind GPT-4o is not a suitable baseline since the questions depend heavily on the images. Language priors are unable to capture the context of the problem unless the images are described in words and the accuracy is expected to be similar to random baseline. The authors should update the baseline to be something more suitable.\n3. The paper's analysis of the core problem is unclear. In model size ablations, the authors suggest the bottleneck lies in data quality and diversity. However, the paper's own experiments fine-tuning and prompting show that existing methods to bridge data gaps do not work. This suggests the bottleneck is more likely architectural or requires in-domain fine-tuning, which the paper does not explore."}, "questions": {"value": "1. How are the multiple images fed into the models? Are they concatenated into a single image, as not all models natively support multiple image inputs?\n2. The order in which the images are presented to the model seems critical, especially for tasks involving motion. How is this temporal order preserved and communicated to the model during evaluation? Can the authors provide an ablation study on the effect of image ordering?\n3. Is the visual prompt (with correspondence lines) provided along with the normal image inputs? There is a concern that the lines themselves might occlude important details in the images, making them harder to see. That could be the reason behind marginal prompting gains.\n4. The authors should provide details about the distribution of data across the 11 tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ycikgUy9Ob", "forum": "gHRoX4vXm3", "replyto": "gHRoX4vXm3", "signatures": ["ICLR.cc/2026/Conference/Submission3238/Reviewer_HMkb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3238/Reviewer_HMkb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944650324, "cdate": 1761944650324, "tmdate": 1762916619123, "mdate": 1762916619123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MMSI-Bench, a large-scale benchmark for evaluating multi-image spatial intelligence in multimodal large language models (MLLMs).\nUnlike prior benchmarks focusing on single-image reasoning, MMSI-Bench evaluates an MLLM’s ability to reason across multiple images to infer spatial relationships, motion, and object-camera-region dynamics.\nThe dataset contains 1,000 multiple-choice questions (covering 10 atomic spatial reasoning categories and one multi-step reasoning task) curated by experts from over 120,000 real-world images drawn from datasets such as ScanNet, nuScenes, Matterport3D, and Ego4D. Each question includes human-written reasoning chains and carefully designed distractors.\nExtensive evaluations of 37 MLLMs (including GPT-5, Gemini-2.5, Claude-3.7, Qwen2.5-VL, and InternVL-3) reveal that even the best proprietary model achieves only ~42% accuracy, while humans reach 97%, showing a large performance gap. The paper also presents an automated error analysis framework identifying four dominant failure modes: grounding, overlap-matching, situation-transformation, and spatial-logic errors."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Novel Benchmark Scope: MMSI-Bench uniquely targets multi-image spatial reasoning — a critical yet underexplored capability for MLLMs and embodied AI systems. Prior works (e.g., BLINK, ReMI, MuirBench) only contain limited spatial sub-splits, while this benchmark provides systematic coverage.\n\nHigh-Quality, Human-Curated Data: Each question is manually designed and audited by multiple experts with reasoning explanations, ensuring clarity, difficulty, and lack of ambiguity. The benchmark’s construction pipeline (Fig. 4) and taxonomy (Table 1) are well-documented and rigorous.\n\nComprehensive Evaluation: The authors benchmark 37 models, analyze scaling trends, compare open-source and proprietary systems, and examine effects of CoT and visual prompting. This breadth enhances credibility.\n\nInsightful Error Taxonomy: The four-type categorization of reasoning errors (Fig. 6) — grounding, scene reconstruction, situation transformation, and spatial logic — provides clear direction for future model development.\n\nImpactful Findings: The results demonstrate that current MLLMs lack robust spatial reasoning and that scaling model size or prompt engineering yields marginal gains, implying fundamental architectural and data limitations.\n\nStrong Writing and Presentation: Figures and examples (e.g., Fig. 2’s diverse question types) are clear, and the organization is consistent with ICLR standards."}, "weaknesses": {"value": "Manual Effort vs. Scalability: Although the manual curation ensures quality, it also limits scalability — future expansions may face bottlenecks unless semi-automatic generation or verification methods are introduced.\n\nMetric Simplicity: The benchmark reports only accuracy on multiple-choice tasks. Incorporating richer evaluation metrics (e.g., reasoning correctness or step alignment) could offer more granular insight.\n\nPotential Dataset Bias: While data diversity is claimed, the benchmark draws primarily from common 3D and driving datasets, possibly biasing toward indoor and urban scenes rather than outdoor natural environments.\n\nLimited Generalization Discussion: The paper does not test transfer to downstream embodied tasks (e.g., navigation, manipulation) where multi-view reasoning is crucial."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ckXZNeripU", "forum": "gHRoX4vXm3", "replyto": "gHRoX4vXm3", "signatures": ["ICLR.cc/2026/Conference/Submission3238/Reviewer_htiq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3238/Reviewer_htiq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3238/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012427337, "cdate": 1762012427337, "tmdate": 1762916618974, "mdate": 1762916618974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}