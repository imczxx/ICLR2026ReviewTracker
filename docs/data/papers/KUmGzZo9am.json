{"id": "KUmGzZo9am", "number": 22031, "cdate": 1758325126659, "mdate": 1759896890147, "content": {"title": "TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning", "abstract": "While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge.\nIndeed, truthfulness requires more than accuracy---models must also recognize uncertainty and abstain when unsure to avoid hallucinations.\nThis presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness.\nIn this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs.\nSpecifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions.\nIt incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness.\nExtensive experiments across four knowledge-intensive benchmarks show that \\model significantly reduces hallucinations (e.g., 43.5\\% $\\rightarrow$ 19.4\\%) and improves truthfulness (e.g., 5.3\\% $\\rightarrow$ 37.2\\%),\nwith consistent gains across various backbone models (e.g., Qwen, Llama). \nIn-depth ablation study demonstrates that vanilla accuracy-driven methods such as supervised fine-tuning or RL with a binary reward struggle to balance factual correctness and uncertainty, whereas the truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.\nMoreover, we find the improvement of TruthRL arises from enhancing the capability of LLMs to recognize their knowledge boundary, hence avoiding being overly conservative as the baselines are.\nFurther analysis validates our method across multiple evaluation judges, and confirms that TruthRL is robust to hallucination-baiting questions.", "tldr": "TruthRL incentivizes models not only to provide more correct responses but also to abstain from answering when uncertain through enhanced capability in recognizing their knowledge boundaries, thereby improving the truthfulness.", "keywords": ["Large Language Model", "Truthfulness", "Reinforcement Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4548b7115fc0dae1cc207e696b224d8eef5cf1d7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "the paper proposes a novel RL framework, TruthRL, designed to directly optimize the truthfulness of LLMs.  The authors identify a critical limitation in existing methods, which either amplify hallucinations (factually incorrect outputs) when optimizing for accuracy or become overly conservative when promoting abstention. TruthRL employs a ternary reward structure that incentivizes correct responses, penalizes hallucinations, and treats abstentions neutrally. the proposed framework is evaluated on four knowledge-intensive benchmarks to validate the efficacy of the proposed framework."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper effectively identifies a critical issue in LLM behavior—hallucination versus abstention—and introduces TruthRL to address it by teaching models to abstain only when faced with out-of-knowledge questions.\n\n2. The ternary reward structure is both simple and effective, striking a balance between rewarding correct answers, penalizing hallucinations, and treating abstentions neutrally. This approach overcomes the limitations of binary reward systems.\n\n3. Extensive evaluations across diverse datasets, settings, and backbone models validate the effectiveness of TruthRL.\n\n4. The paper is well-written and easy to follow."}, "weaknesses": {"value": "1. The contribution of the paper appears incremental. The core idea is to improve factuality by leveraging the model's internal knowledge boundary, enabling it to correctly answer known questions while abstaining from unknown ones. However, prior works, such as [1] and [2], have already utilized the model's self-knowledge as a reward signal to guide it toward correctly answering known questions. The novelty here seems to lie in additionally teaching the model to abstain from answering unknown questions. It would be helpful if the authors clarified how this abstention mechanism significantly advances beyond existing approaches.\n\n2. The training rewards are derived based on the classification of known and unknown questions, which are identified through preliminary knowledge probing on a pre-collected, task-specific annotated training dataset. This reliance on annotated datasets tailored to specific tasks could limit the approach's generalizability and applicability in real-world, dynamic environments where such datasets may not be readily available.\n\nReferences:\n\n[1] Fine-tuning Language Models for Factuality\n[2] Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation"}, "questions": {"value": "1. Could you provide an error analysis? Additionally, have you observed any reward hacking behavior related to the designed rewards? \n\n2. Could you demonstrate the generalization capability of the proposed approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YhqWkE1O5u", "forum": "KUmGzZo9am", "replyto": "KUmGzZo9am", "signatures": ["ICLR.cc/2026/Conference/Submission22031/Reviewer_a5cR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22031/Reviewer_a5cR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816835058, "cdate": 1761816835058, "tmdate": 1762942028056, "mdate": 1762942028056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TruthRL, an RL framework designed to improve the truthfulness of LLMs. It proposes a simple and effective ternary reward scheme to encourage the model to perform correct abstention and reduce hallucination."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive experimental results and thorough analysis. This work includes a substantial number of experiments and appears to have been optimized multiple times. Some conclusions are insightful, such as the comparison between binary and ternary rewards, and the ineffectiveness of offline-RL.\n2. Good presentation and writing. The authors' writing is clear, and the figures and tables are well-drawn and concise."}, "weaknesses": {"value": "1. The method's novelty is insufficient, and the chosen baselines are not strong enough. The field of LLM reliability and abstention already has a considerable number of papers. Similar ternary rewards have been proposed in previous work, such as [1][2]. Especially in [2], the authors discussed in detail how the reward value for abstention in a ternary reward scheme affects model behavior; it is recommended to reference this. Furthermore, the baselines selected in this work only include SFT-based methods. There are many more novel methods that could potentially achieve better performance, such as [1][3][4]. Some of these are already mentioned in the related work, and maybe the authors should consider including them as baselines.\n2. Metric Definition: The chosen Truthfulness metric, defined as T=Acc - Hall, is a relatively basic metric that lacks consideration for abstention. A problematic scenario might arise: a model that can make mistakes (Acc=80, Unc=0, Hall=20, T=60) and a model that is perfectly safe (Acc=60, Unc=40, Hall=0, T=60) would receive the same Truthfulness score. The field of model reliability already has many more sophisticated metrics, which can be referenced in [5].\n\nReferences\n\n[1] https://arxiv.org/abs/2403.18349\n\n[2] https://arxiv.org/abs/2403.05612\n\n[3] https://arxiv.org/abs/2502.07184\n\n[4] https://arxiv.org/abs/2412.11803\n\n[5] https://arxiv.org/abs/2407.18418"}, "questions": {"value": "1. About training stability: I have also explored this simple ternary reward setting for training abstention and found that the RL training is not stable. The model's behavior tends to collapse towards either always answering or always refusing as training progresses. Using this ternary reward with the GRPO algorithm might also pose a problem. As training continues, the model will likely exhibit singular actions within the sampled group for most queries (i.e., either always answering or always refusing), even with a high temperature T=1. This leads to uniform rewards within the same query group (e.g., all 1s or all 0s), which is detrimental for GRPO's calculation of the in-group advantage and further optimization. I would like to know if you also experienced instability during your training process, and if you observed a similar phenomenon of highly consistent sampled actions. Maybe using other RL algorithms that do not rely on in-group sampling could further improve the training effectiveness.\n2. About offline DPO: The performance of offline DPO appears to be very poor. Have you conducted further research into why its performance is so low? Perhaps it is an issue with the training data construction or the data-mix ratio? According to results from [1], they also employed a similar offline DPO method but used a different data construction strategy, achieving relatively good performance.\n\nReferences\n\n[1] https://arxiv.org/abs/2403.18349"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZEO6hiOXpK", "forum": "KUmGzZo9am", "replyto": "KUmGzZo9am", "signatures": ["ICLR.cc/2026/Conference/Submission22031/Reviewer_JATC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22031/Reviewer_JATC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828793471, "cdate": 1761828793471, "tmdate": 1762942027559, "mdate": 1762942027559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TruthRL, an online reinforcement-learning framework (implemented with GRPO) that optimizes a truthfulness objective rather than pure accuracy. The core technical idea is a ternary reward (correct = +1, abstain = 0, hallucination = −1), with optional knowledge-enhanced and reasoning-enhanced variants. The authors evaluate TruthRL on four knowledge-intensive benchmarks (CRAG, NQ, HotpotQA, MuSiQue) in retrieval and non-retrieval settings, compare to SFT, RFT, R-Tuning and RL baselines, and report that TruthRL reduces hallucinations and increases a composite truthfulness score while preserving competitive accuracy . They also analyze behavior across confidence buckets and with different judges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Experiments cover multiple datasets, backbones (3B→32B), retrieval/no-retrieval settings and several RL paradigms, which gives a good picture of where the approach works and where it doesn’t. The paper includes analyses of confidence buckets, hallucination-baiting questions, and judge robustness, which help understand the mechanisms behind performance gains."}, "weaknesses": {"value": "1. Lack of novelty and related works awareness. The main conceptual contribution—training models to abstain appropriately and using richer reward structures to encourage calibrated abstention—has strong prior art, which is a fatal issue. In particular: Reinforcement frameworks that encourage abstention have already been proposed (https://arxiv.org/abs/2403.18349), which used exactly the same ternary reward to teach refusal and increase reliability. The claim that the innovation is new and the “main contribution” is the ternary reward + GRPO is overstated. The authors must clearly and explicitly position TruthRL relative to these prior approaches: what is strictly new that prior work did not do?\n2. Lack of ablation on reward weights. The truthfulness metric uses fixed weights (w1=1, w2=0, w3=1). Why set Unc weight w2 = 0 in experiments if abstention is a desired property? How sensitive are results to the weighting choice? Provide ablations for the weighting and for scaling of the ternary reward."}, "questions": {"value": "1. Can you provide a human annotation on a representative sample to confirm the LLM judge labels ? If humans disagree with the LLM judge systematically, how does that affect the results?\n2. How sensitive are results to the OOK detection threshold (sampling 256 responses)? If experiments with 128/512 sampling, how the number of OOKs and downstream TruthRL performance change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GHQhcDunBX", "forum": "KUmGzZo9am", "replyto": "KUmGzZo9am", "signatures": ["ICLR.cc/2026/Conference/Submission22031/Reviewer_Q5vx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22031/Reviewer_Q5vx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919547756, "cdate": 1761919547756, "tmdate": 1762942027280, "mdate": 1762942027280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TruthRL, an RL framework to directly optimize truthfulness of LLMs rather than accuracy. It uses a ternary reward, encouraging the model to both answer correctly and abstain when uncertain. Implemented via GRPO, extensive experiments demonstrate TruthRL significantly reduces hallucinations and improves truthfulness."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The motivation is clear: addressing the accuracy–truthfulness tradeoff directly via reward shaping\n* The method is easy to follow and the presentation is good."}, "weaknesses": {"value": "* The evaluation is restricted to QA benchmarks. While the proposed ternary reward effectively balances accuracy and abstention in such settings, it remains unclear whether TruthRL generalizes to other task types, such as mathematical reasoning, or instruction-following, where uncertainty and partial correctness behave differently.\n* The method can be viewed as a principled extension of reward shaping; the algorithmic innovation is limited."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "o6XCFzE6NW", "forum": "KUmGzZo9am", "replyto": "KUmGzZo9am", "signatures": ["ICLR.cc/2026/Conference/Submission22031/Reviewer_uW9S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22031/Reviewer_uW9S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22031/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103997139, "cdate": 1762103997139, "tmdate": 1762942027022, "mdate": 1762942027022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}