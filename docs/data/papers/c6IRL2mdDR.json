{"id": "c6IRL2mdDR", "number": 9742, "cdate": 1758137316222, "mdate": 1759897701120, "content": {"title": "Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization", "abstract": "Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may ${\\it naturally}$ achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger ${\\it and}$ from its intrinsic features, whereas non-target classes ${\\it only}$ have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to ${\\it suppress}$ intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class’s intrinsic features. We dub this approach ``class subspace orthogonalization’’ (CSO). \nCSO can be applied to a wide variety of existing detectors.\nWe demonstrate its effectiveness in improving several well-known detectors, comparing with a variety of baseline detectors, against a variety of attacks, on the CIFAR-10, GTSRB, and TinyImageNet domains. Moreover, we also evaluate against a novel mixed clean/dirty-label poisoning attack that is more surgical and harder to detect than traditional dirty-label attacks, as well as against adaptive attackers.", "tldr": "", "keywords": ["Deep Learning", "Adversarial Learning", "Backdoor Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6b71568b52f136d997ef0113501c0391d73e7ad6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method called class subspace orthogonalization (CSO) to improve post-training backdoor detection by suppressing intrinsic class features for optimizing detector towards on backdoor-related directions. The authors argue that existing backdoor detection methods rate the success by an outlier statistic per class which fails when some non-target classes naturally yield extreme statistics or when the backdoor signal is subtle. The paper also gives a linear analysis showing orthogonalized optimization zeroes out non-target classes while preserving a positive statistic for the target class. They propose to estimate each class’s intrinsic feature subspace from a small clean set, then penalize alignment with that subspace during the detector’s search. This makes the target class stand out because only it contains both intrinsic and trigger contributions. The proposed method is implemented by learning class-specific soft masks to capture intrinsic features, adding a rectified cosine-similarity penalty to existing detectors to obtain the corresponding CSO variants. Experiments on standard datasets CIFAR-10, GTSRB, and TinyImageNet across multiple attacks show consistent gains and lower false positives with low overhead. The authors claim that the proposed method can substantially boost backdoor detector sensitivity and robustness against adaptive attackers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Motivations are clear, the authors provide good explanations of why the target class remains an outlier after removing intrinsic features. The proposed method is easily integrated with existing backdoor detectors by adding a single penalty term. Experiments show good backdoor detection results with low overhead."}, "weaknesses": {"value": "The approach assumes access to a small but representative clean set per class, and may be sensitivity to domain shift, label noise. As the motivation is based on backdoor features lying outside the intrinsic subspace, adaptive attacks that force overlap can escape it. Experiments use only moderate-scale backbones, it would be desirable to see results on larger modern models and vision transformers. The hyperparameter lambda are chosen per detector, it is not clear how to tune this parameter."}, "questions": {"value": "1. How do CSO variant backdoor detectors depend on the number of images in the clean dataset per class? It would be helpful to better understand the relation between the choice of clean dataset and detector's performance.\n2. If an adaptive attacker explicitly aligns trigger features with the target class’s intrinsic subspace, how can CSO be adapted?\n3. It is shown that sensitivity to lambda is modest, but values differ by detector. Is there a way to efficiently tune this hyperparameter for different datasets and detectors?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qQABHB8kxG", "forum": "c6IRL2mdDR", "replyto": "c6IRL2mdDR", "signatures": ["ICLR.cc/2026/Conference/Submission9742/Reviewer_tvn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9742/Reviewer_tvn1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761610705302, "cdate": 1761610705302, "tmdate": 1762921239304, "mdate": 1762921239304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Class Subspace Orthogonalization (CSO), a plug-in framework designed to enhance the sensitivity of post-training backdoor detectors. The key idea is to suppress class-intrinsic features by orthogonalizing detector optimization away from each class’s intrinsic subspace, allowing the detector to focus on backdoor-related signals. CSO can be applied to existing detectors such as Neural Cleanse and MMBD, leading to significant improvements in detection accuracy and robustness across multiple datasets and attack types. The authors also introduce a Mixed-Label (ML) Attack that combines clean- and dirty-label poisoning to create more stealthy, harder-to-detect backdoors. Experiments on CIFAR-10, GTSRB, and Tiny-ImageNet show that CSO consistently improves sensitivity and maintains robustness even under adaptive attacks and limited clean data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated and clearly addresses the sensitivity limitation of existing post-training backdoor detectors. The proposed Class Subspace Orthogonalization (CSO) is simple, detector-agnostic, and effective without modifying model architectures. It consistently improves detection accuracy and robustness across multiple datasets, detectors, and attack types. The introduction of the Mixed-Label (ML) Attack adds practical value by revealing new detection challenges. Experiments are comprehensive and well-analyzed, demonstrating both the effectiveness and generality of the proposed method."}, "weaknesses": {"value": "- **Dependence on clean data**: CSO requires a small clean dataset to estimate class-intrinsic subspaces. However, prior work such as *Multidomain Active Defense: Detecting Multidomain Backdoor Poisoned Samples via All-to-All Decoupling Training without Clean Datasets* has shown that relying on clean data can lead to a high false-positive rate when detecting samples from out-of-distribution (OOD) domains. This raises concerns about CSO’s robustness and reliability in more realistic settings where truly clean data are not guaranteed.  \n- **Effectiveness against clean-label attacks remains unverified**: Although CSO is evaluated under mixed-label and partially clean-label settings (e.g., LC attack), it does not include experiments on purely clean-label backdoor attacks such as SIG or CLBD. It remains unclear whether CSO can effectively detect or mitigate fully clean-label poisoning.  \n- **No analysis on multi-trigger attacks**: The paper does not consider more complex multi-trigger or multi-target attack scenarios, which have been shown in recent works (e.g. M-to-n backdoor paradigm: A multi-trigger and multi-target attack to deep learning models; Poster: Multi-target & multi-trigger backdoor attacks on graph neural networks ) to significantly challenge post-training detection methods. Evaluating CSO under such settings would provide a more comprehensive understanding of its robustness.  \n\n- **Adaptive attack results are underemphasized and poorly integrated**: The evaluation of adaptive attacks (Adaptive-Blend and Adaptive-Blend-2) appears only briefly in Section 4.4 and the appendix, without quantitative results in the main tables or figures. Since adaptive robustness is a key claim of the paper, these results should be clearly reported in the main text with detailed metrics, variance, and comparisons. The current presentation makes it difficult to assess how much robustness CSO actually provides under adaptive conditions."}, "questions": {"value": "1. The paper assumes the availability of a small clean dataset to estimate class-intrinsic subspaces. How sensitive is CSO’s performance to the size, quality, or domain shift of this clean data?  \n2. The proposed method is evaluated under mixed-label and partially clean-label attacks (e.g., LC, ML attack). Has CSO been tested against purely clean-label backdoor attacks such as SIG or CLBD, and how would it perform in those cases?  \n3. The paper does not analyze multi-trigger or multi-target attacks, which have been shown in recent studies to significantly challenge post-training detection methods. How might CSO handle such complex attack patterns?  \n4. Since CSO depends on class subspace estimation, could the method fail when the victim model’s representation is not linearly separable or when feature overlap occurs across classes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R52a0CG5Am", "forum": "c6IRL2mdDR", "replyto": "c6IRL2mdDR", "signatures": ["ICLR.cc/2026/Conference/Submission9742/Reviewer_ERPo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9742/Reviewer_ERPo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885346861, "cdate": 1761885346861, "tmdate": 1762921238547, "mdate": 1762921238547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Class Subspace Orthogonalization (CSO), a wrapper framework that attempts to improve existing backdoor detectors by forcing their search to be orthogonal to a class's intrinsic feature subspace, as learned from a tiny clean dataset. The central claim is that this isolates the backdoor signal, improving detection sensitivity and reducing false positives. The authors also propose a new mixed label (ML) attack that combines dirty-label and clean-label poisoning, primarily to demonstrate that CSO is necessary to detect such subtle, stealthy threats."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides an interesting approach (CSO) to improve backdoor detection by penalizing detection features that correlate with intrinsic features of the original data.\n- The paper proposes a new threat (mixed label attack) that combines dirty-label and clean-label backdoor to reduce collateral damage.\n- Extensive results show that CSO improves the detection accuracy of base detection methods. They also demonstrate the stealthiness of mixed label attack under backdoor detection."}, "weaknesses": {"value": "- The writing is incoherent and hard to follow. There are two separate contributions: CSO and mixed label attack, but I don't clearly see their connection. The motivation for mixed label attack is also unclear, the paper does not explain why adding clean-label backdoor helps reduce collateral damage. The purpose of mixed label attack is not mentioned in the conclusion.\n- As mentioned in the adaptive attack section, this method does not work well against backdoor features that are similar to intrinsic features. Stealthy or clean-label backdoor possesses this property. Experimental results show that CSO yields low detection accuracy on stealthy attacks like Wanet or Bpp, clean-label attacks like label consistency, and one-to-one setting.\n- The paper should evaluate other stealthy or clean label backdoor, such as Refool, Narcissus, Hidden Trigger Backdoor Attack, etc."}, "questions": {"value": "- What is the main contribution of this paper, CSO or mixed label attack? What is their connection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XCzUVXE4q1", "forum": "c6IRL2mdDR", "replyto": "c6IRL2mdDR", "signatures": ["ICLR.cc/2026/Conference/Submission9742/Reviewer_VdYG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9742/Reviewer_VdYG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909453727, "cdate": 1761909453727, "tmdate": 1762921237992, "mdate": 1762921237992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the post-training backdoor detection, for instance the Neural Cleanse,  the defenders try to reconstruct a universal perturbation as a potential trigger signal, and then analyze their statistic information to determine which is the suspect class. However, how to determine the target class from these reconstructed perturbations is challenging. This paper tries to suppress these normal intrinsic features to improve the detection performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n\n1.\tThis paper considered the reverse-engineering based backdoor detection, and provide the systemic experiments to compare their performance with other works.\n\n2.\tThe author proposed a regularization which can be easily used as a plugin to improve the performance of the existing reverse-engineering based methods, like NC, NNBD etc."}, "weaknesses": {"value": "Weakness:\n\n1.\tIn the second paragraph of page 2, the authors claimed that low poisoning ratio affects the reverse-engineering-based detector? From my humble perspective, whether the model is backdoored or not is the main factor.\n\n2.\tMoreover, Wang et al.2019 (Neural Cleanse) tries to f find the perturbation/trigger from the input space, not the feature space. But in Section 2.2.2, the authors cited it and claimed a soft mask identifying the intrinsic feature subspace. It is confusing. \n\n3.\tI fully understand the authors’ idea, i.e., first using Equ 4 to identify the intrinsic feature per class, and then exploiting that feature to penalize the reconstructed sample whose feature is alignment with it. Since the trigger signal will activate the different feature with the benign image, this idea can work well for common backdoor attack. However, once this attacker tries to reduce the feature difference between trigger and benign sample [1], this idea will fail. Therefore, I think this new defense idea can be easily bypassed when the attacker uses the regularization shown in [1] regardless the attack types.\n\n[1] Bypassing backdoor detection algorithms in deep learning"}, "questions": {"value": "See my concerns in weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G0EMOR4gGM", "forum": "c6IRL2mdDR", "replyto": "c6IRL2mdDR", "signatures": ["ICLR.cc/2026/Conference/Submission9742/Reviewer_FRMn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9742/Reviewer_FRMn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929385766, "cdate": 1761929385766, "tmdate": 1762921237683, "mdate": 1762921237683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To tackle the problem that the performance of backdoor attack detector deteriorates when some untargetted classes exhibit similarly extreme distributions, the authors propose a method that identifies and orthogonalizes intrinsic features, and applies the orthogonalization to existing backdoor defenses, achieving promising results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The method has a very reasonable motivation, and is grounded in theory.\n\n(2) The experiment is convincing: it is not grounded in specifically curated datasets that requires very strict feature distribution difference but still outperforms baselines.\n\n(3) The presentation of the paper is clear.\n\n(4) The authors propose many variants of how the intrinsic features can be integrated into existing backdoor detectors."}, "weaknesses": {"value": "Strength (4) somehow also becomes the weakness - I am mostly aware of the applicability of the method: if different integration methods have to be applied for different detector, how far can it go? What if there are more powerful detectors coming around and how hard is it to adapt your method?\n\nTaking intrinsic features into account is nothing new in the community, and I suggest the authors to stress the major contribution of your work w.r.t. other existing methods."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xYXTFdYHe9", "forum": "c6IRL2mdDR", "replyto": "c6IRL2mdDR", "signatures": ["ICLR.cc/2026/Conference/Submission9742/Reviewer_236s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9742/Reviewer_236s"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969518274, "cdate": 1761969518274, "tmdate": 1762921237268, "mdate": 1762921237268, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework called Class Subspace Orthogonalization (CSO) to enhance post-training backdoor detection sensitivity, and also proposes a new mixed-label backdoor attack to challenge detectors. The key idea is to address two failure modes of existing detectors: (1) cases where a benign class naturally appears as an outlier in detection statistics, and (2) cases where the backdoor trigger is subtle relative to normal class features. The authors observe that a backdoored target class contributes to a detector’s statistic via both its normal (intrinsic) features and the trigger, whereas non-target classes contribute only intrinsic features. Thus, if one can suppress the intrinsic class features during detection, any remaining strong signal would likely come from a trigger (if present). Based on this insight, the paper formulates a constrained optimization that maximizes a chosen backdoor detection statistic for each class while enforcing orthogonality to that class’s feature subspace. This CSO approach can be plugged into a wide variety of detectors to guide them toward backdoor cues and away from benign class patterns."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Significant Boost in Sensitivity: The most evident strength of CSO is how it noticeably improves the sensitivity of backdoor detection. By removing the “noise” of normal class features, detectors become capable of catching very subtle backdoors that might have previously gone unnoticed. The paper’s introduction clearly articulates this benefit: for the backdoor target class, even if the trigger effect was weak, suppressing intrinsic features lets that weak signal stand out; for non-target classes, whose signals are purely intrinsic, the suppression drastically lowers their scores. This mechanism directly addresses the problem of missed detections. In the results, this translated to far higher detection accuracy, meaning fewer backdoored models would slip by. In practical terms, CSO could be the difference between an undetected Trojan model in deployment and one that gets caught before causing damage.\n- Universality and Flexibility: CSO is designed as a plug-and-play framework. It’s not a detector by itself but rather a module that augments existing detectors. This is a great strength because it means CSO’s idea can be applied broadly without reinventing the wheel for each new detection method. The authors demonstrate integration with multiple algorithms (NC, PT-RED, MMBD, etc.) and mention it can be seamlessly integrated with other detectors too. Developers of backdoor defenses can adopt CSO’s penalty term in their own methods relatively easily. The fact that it worked across various types of detectors (anomaly-based, optimization-based, pairwise trigger search, etc.) shows its generality. Such flexibility ensures that CSO’s benefits are not limited to a niche case but can impact the whole landscape of post-training detection. Future detectors could include a “CSO step” as a standard to enhance their reliability.\n- Reduced False Alarms: Increasing sensitivity often comes at the cost of more false positives, but CSO managed to improve true positive detection while keeping false positives low. This is a strong advantage – a detector that cries wolf too often will not be adopted in practice. CSO’s focus on differentiating trigger vs. intrinsic features is precisely what allows this balance: it filters out benign anomalies (like a naturally high-confidence class) which might fool a naive detector, thereby avoiding false alarms. The data showed CSO variants had equal or fewer false positives than comparable methods. For example, if a certain class in a clean model was particularly distinct, a normal detector might flag it wrongly; CSO ensures that unless there is a feature outside the normal subspace giving an abnormal boost, it won’t flag. This property makes CSO-enhanced detectors more trustworthy in real deployments."}, "weaknesses": {"value": "### Assumption of Feature Separability\nThe core assumption of CSO is that the backdoor trigger introduces features that lie outside the normal feature subspace of the target class. While generally reasonable (triggers are usually patterns unrelated to the class, like a sticker on a stop sign), one can conceive of cases where this doesn’t hold. An attacker could choose a trigger that is a feature native to the target class. For example, suppose the target class is dogs, and the attacker’s trigger is “add pointy ears” – many dogs naturally have pointy ears, so this trigger might actually lie within the dog feature distribution. In such a case, enforcing orthogonality to the dog subspace would also filter out the trigger signal, potentially making the backdoor undetectable by CSO. In other words, if the trigger is not truly an orthogonal add-on but overlaps with intrinsic features, CSO could struggle. The authors’ adaptive attack Adaptive-Blend-2 is a step in this direction (blending target data with source-trigger data so the trigger is partially intrinsic). This is somewhat a worst-case scenario, but it’s a limitation: CSO works best when there is a clear distinction between what’s normal for a class and what’s introduced by the trigger."}, "questions": {"value": "1. Quantifying “trigger–intrinsic” overlap (does CSO’s key assumption hold?) How separable are trigger features from intrinsic class features in practice? If overlapping exists, where does the overlap arise (layer-wise)?\n2. It could be better to explore the effectiveness of the proposed defense on distribution-preserving backdoor attacks. e.g. [a], which intentionally induces distribution overlapping between backdoor samples and clean samples, to validate the generality of the intuition behind CSO.\n[a] Distribution Preserving Backdoor Attack in Self-supervised Learning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wu0WZIe18K", "forum": "c6IRL2mdDR", "replyto": "c6IRL2mdDR", "signatures": ["ICLR.cc/2026/Conference/Submission9742/Reviewer_WUb8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9742/Reviewer_WUb8"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997888261, "cdate": 1761997888261, "tmdate": 1762921236967, "mdate": 1762921236967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}