{"id": "ax6oQWQmeR", "number": 25567, "cdate": 1758369142148, "mdate": 1759896715337, "content": {"title": "Hierarchies over Pixels: A Benchmark for Cognitive Geospatial Reasoning for Agents", "abstract": "Beyond perception, reasoning is crucial in remote sensing, enabling advanced interpretation, inference, and decision-making. Recent advances in large language models (LLMs) have given rise to tool-augmented agents that enhance reasoning by leveraging external tools for complex analytical tasks. However, existing research on these agents in remote sensing largely focuses on perception-oriented tasks, with cognitive geospatial reasoning remaining underexplored. In this work, we systematically evaluate the geospatial reasoning capabilities of LLM-powered tool-augmented agents. To this end, we introduce GeoHOP, a benchmark for hierarchical geospatial reasoning. GeoHOP comprises 417 scenario-driven, hierarchy-aware tasks—such as hazard vulnerability assessment, urban heat island analysis, and forest fragmentation dynamics—spanning optical, Synthetic Aperture Radar (SAR), and infrared (IR) imagery. GeoHOP advances evaluation beyond monitoring-based recognition to cognitive-level geospatial analysis. Building upon GeoHOP, we propose GeoPlanner, an agent powered by LLMs that organizes 5 toolkits into functional hierarchies and executes fault-tolerant reasoning pipelines. GeoPlanner enables structured abstraction, robust recovery from tool failures, and stable long-horizon planning. Extensive experiments across diverse geospatial reasoning tasks demonstrate that GeoPlanner excels in hierarchical reasoning, cross-modal transfer, and error handling.", "tldr": "We introduce GeoHOP, a benchmark for hierarchical geospatial reasoning in remote sensing, and GeoPlanner, a tool-augmented LLM agent that excels in structured, fault-tolerant geospatial analysis.", "keywords": ["Benchmark evaluation", "Remote sensing imagery", "Tool-augmented LLMs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a38d7f31a32b845fd62e962f10c15bd934ea0ce2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this study, the authors address the lack of evaluation benchmarks for assessing the spatial reasoning capabilities of agents in the remote sensing domain by proposing GeoHOP—a benchmark tailored for hierarchical geospatial reasoning. GeoHOP comprises 417 scene-driven, hierarchy-aware tasks covering optical, synthetic aperture radar (SAR), and infrared (IR) imagery, with applications including disaster vulnerability assessment, urban heat island analysis, and forest fragmentation dynamics. Additionally, the authors introduce a simple agent, GeoPlanner, demonstrating its ability to help large language models (LLMs) perform more robust analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper tackles a valuable gap in the remote sensing community—the lack of novel datasets for evaluating agents—by curating a substantial collection of remote sensing data and using a semi-supervised approach to generate a dataset of 417 tasks spanning multiple modalities (optical, SAR, and IR) for agent evaluation in remote sensing."}, "weaknesses": {"value": "I believe the problem the authors aim to address is highly valuable, and I encourage them to continue this line of work. However, the current results are significantly incomplete, primarily due to the following issues:\n\n1. The authors do not provide a clear definition of what constitutes a \"reasoning task\" in geospatial contexts. Based on the provided examples, most tasks appear to be multi-turn tool-calling \"information retrieval\" tasks. Is \"complex information retrieval\" equivalent to reasoning? This is questionable. In fact, GeoHOP lacks high-difficulty tasks comparable to those in benchmarks like SWE-Bench or BrowseComp. It also misses truly open-ended tasks such as \"assess landslide risk in a given region\" or \"predict local crop yields by integrating diverse data sources,\" which would require genuine complex reasoning.\n\n2. GeoHOP lacks a discussion of its evaluation framework. Evaluating agent capabilities should probe the boundaries of those capabilities—e.g., through clearly defined evaluation objectives such as task planning, contextual awareness, multimodal understanding, self-correction, etc. Each task should explicitly target specific capabilities, rather than being grouped merely by application scenario as currently done.\n\n3. The paper lacks rigorous assessment of data quality. For a dataset-focused paper, a critical experiment involves evaluating data quality and providing scores and uncertainty estimates for each data instance. Geospatial reasoning may involve both open-ended and closed-ended questions, and different evaluation metrics are needed for each type.\n\n4. GeoHOP is already outdated relative to state-of-the-art agent frameworks in 2025 and does not leverage recent advances in LLM evaluation (e.g., ReAct is an obsolete paradigm). As shown in Figure 1, the so-called \"reasoning tasks\" are merely simple multi-step tool-calling tasks. While such tasks might have posed a challenge for agents in early 2024, by 2025, general-purpose agents outside remote sensing—such as Claude Code, Gemini CLI, and Cursor—have made significant progress. When integrated with domain-specific protocols like MCP for remote sensing, these agents can already execute complex, multi-faceted tasks. Current open-source or commercial agents can invoke dozens of tools, handle context lengths of 200–300k tokens, and possess self-correction capabilities based on execution feedback. Consequently, the tasks in GeoHOP may be too simplistic for today’s most advanced agents."}, "questions": {"value": "1. What constitutes \"reasoning\" in geospatial contexts, and what distinguishes \"deep reasoning\"? How can we measure the difficulty of a reasoning task?\n\n2. How should we separately evaluate the difficulty and answer accuracy for closed-ended versus open-ended geospatial reasoning problems?\n\n3. How does GeoHOP quantify an agent’s performance across various dimensions (e.g., domain knowledge, hallucination, contextual understanding, self-correction)?\n\n4. Can GeoHOP’s data generation pipeline be built entirely within a simulated environment to support reinforcement learning fine-tuning of LLM agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RZBS9913v8", "forum": "ax6oQWQmeR", "replyto": "ax6oQWQmeR", "signatures": ["ICLR.cc/2026/Conference/Submission25567/Reviewer_CA8L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25567/Reviewer_CA8L"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761195092181, "cdate": 1761195092181, "tmdate": 1762943478157, "mdate": 1762943478157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper has two main contributions: First the authors introduce a benchmark (GeoHOP) of tasks for geospatial reasoning, including 417 tasks. The tasks require combining different elements of geospatial reasoning, with three elements: spatial trigger, expert interpretation and tool use. Secondly the authors introduce an agent \"Geoplanner\" to solve these tasks and present performance metrics for several frontier models using the agent. The planner is given different sets of tools (open source models), that are chosen by a semantic retriever to solve the tasks. The planner makes a plan, tries to execute the plan, and then retries if there are errors. There is an overall error budget to control cost. The agent is evaluated on Answer accuracy and argument consistency, evaluated by an LLM as a judge"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The benchmark seems creative though this is hard for me to judge as I only can see the examples in the paper.  The geoplanner is quite sensible."}, "weaknesses": {"value": "I can't tell from the paper how good this benchmark is. The examples that are given don't show the potential breadth of what is in the benchmark, and its utility depends critically on the level of difficulty and relevance of the questions. The low scores of the fronteir models on the tasks make it seem that the benchmark is hard -- but i don't have a sense for _why_ it is hard, and what are the types of mistakes. The car counting example in the paper is quite nice and clear, but it would be easier to assess the importance of this benchmark if I could understand its breadth better.\n\nAt the same time, I'm not a great fan of the LLM as a judge evaluation of the agents. For the car example the answer is the number of cars. Can't this be evaluated programatically? I'd much prefer if the evals were as rigorous as possible and i think LLM as a judge weakens the argument\n\nI'd like to understand whether the tools were chosen specifically to improve performance on the benchmark. Please give justification for the tools that you hvae chosen and explain whether or not they were chosen independently of the questions in the benchmark. If there was dependence please explain how you know you aren't cherry picking. At an extreme limit we build a model for every problem and simply call the model as a tool, so the agent only has to distinguish which model it is choosing. This is not interesting."}, "questions": {"value": "1. Clearer discussion of the types and variability of problems in the benchmark. What does it cover, what does it not cover, what is the level of difficulty.  The current discussion is opaque. This will be easier I guess when we can see the benchmark but still a summary in the paper is good.\n1'. Comparison to other benchmarks -- what is new here.\n2. More information about the actions the geoplanner takes and the distribution of errors across the tasks. I don't understand the red x's and checkmarks in figure 1. The caption is rather brief. how much of it has to do with ability to use the particular tools you are letting it call.\n2'. How does geoplanner do on other benchmarks where this is relevant. or if it isn't relevant because the geoplanner is very specific in the tools it can use then it would be good to understand why. this coudl be a major limitation of the study depending on the answer.\n3. Explain how the tools were chosen and whether or not you were cherry picking tools to problems like i outlined above. Measuring performance on another benchmark (eg https://openreview.net/pdf?id=oaYShIy3Xe) would help sort this out, as one woudl expect lower performances if the tools were cherry picked"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7mZXJcB18M", "forum": "ax6oQWQmeR", "replyto": "ax6oQWQmeR", "signatures": ["ICLR.cc/2026/Conference/Submission25567/Reviewer_guxm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25567/Reviewer_guxm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878885208, "cdate": 1761878885208, "tmdate": 1762943477736, "mdate": 1762943477736, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GeoHOP, a new benchmark designed to assess hierarchical geospatial reasoning through 417 curated, scenario-based tasks spanning optical, SAR, and IR imagery. These tasks represent applied domains such as urban heat and hazard vulnerability assessment, etc., requiring multi-step reasoning that combines perception with operators such as buffer, overlap, and others. To address these tasks, the authors introduce GeoPlanner, an LLM-based agent framework that organizes geospatial tools into a hierarchical library. The agent retrieves relevant tools, generates modality-aware reasoning plans, executes them, and composes final responses grounded in tool-derived outputs. The study evaluates some large language models, including GPT-4o, Claude-4, and Qwen-2.5, on GeoHOP using structured, step-wise metrics. GPT-4o achieves the strongest overall performance. The dataset is constructed through a two-stage curation pipeline, combining knowledge-augmented generation with expert adjudication."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This work targets geospatial reasoning, offering 417 scenario-driven, hierarchy-aware tasks that span optical, SAR, and IR, useful coverage for multimodal evaluation. Its benchmark instances are structured for verifiability via expert interpretation and a verifiable tool chain, which bridges perception to higher-level analysis. GeoPlanner agent adds practical value with a hierarchical, typed tool library, modality awareness, and fault-tolerant execution, improving robustness for long-horizon reasoning. The dataset curation is documented through a two-stage pipeline with expert adjudication. The paper also provides step-wise and end-to-end evaluation across several LLMs, establishing initial baselines."}, "weaknesses": {"value": "**Small scale with modality imbalance.** The dataset includes 417 total tasks, 252 using optical images, 119 using SAR, and only 42 using infrared data. Overall, the benchmark is relatively small and not well-balanced across modalities, which limits its ability to represent the full diversity of real-world remote-sensing scenarios. The small size reduces confidence that model performance will generalize beyond the sampled tasks.\n\n**No explicit temporal/change tasks.** All examples are single snapshots; there is no bi-temporal change detection or time-aware operator, even though change analysis is central to many remote-sensing applications. ThinkGeo(Shabbir et al., 2025) includes explicit temporal-change tasks.\n\n**Human verification detail.** The authors mention expert adjudication by remote sensing specialists, but the paper does not quantify human-hour effort, inter-annotator agreement, or error rates. This makes it difficult to assess the depth and reliability of the quality control. \n\n**Latency/throughput not reported.** The paper analyzes success and failure counts and step lengths but does not provide runtime or throughput measurements (step-wise vs. end-to-end). For practical benchmarking, such efficiency metrics would be valuable.  \n\n**Limited insight by category/model.** Several LLMs (GPT family, Claude-4, Gemini-2.5, DeepSeek-V3, Qwen-2.5) are benchmarked, yet the analysis remains coarse-grained. There is no breakdown showing which models perform better for specific task types or modalities, limiting interpretability and future insights.\n\n**Geographic coverage.** The dataset sources (LoveDA, Potsdam, OGSOD, DMIST) are listed, but the paper does not describe the global or regional distribution of scenarios. Without geographic context, it is unclear how representative or generalizable the benchmark is.\n\n**Tools are narrower.** The toolkit focuses on perception and spatial-relation operators (buffer, overlap, containment) but lacks temporal/change and code-based tools that are common in broader geospatial reasoning workflows. This constrains its applicability to real-world analytical pipelines."}, "questions": {"value": "I would appreciate additional clarification on several aspects of the work. Could the authors elaborate on how the benchmark's relatively small and unbalanced scale across modalities (Optical, SAR, IR) affects its representativeness and generalization potential? It would also be helpful to understand why temporal or change-detection tasks were not included, given their central importance in remote-sensing reasoning. Further details on the human verification process, such as total annotation effort, inter-annotator agreement, and error rates, would strengthen confidence in dataset quality. Reporting runtime or throughput metrics, along with a breakdown of model performance across task categories or modalities, would also improve interpretability. Finally, clarification on the scope of tool diversity would help assess practical relevance. Please see the weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P9vq3keBCC", "forum": "ax6oQWQmeR", "replyto": "ax6oQWQmeR", "signatures": ["ICLR.cc/2026/Conference/Submission25567/Reviewer_PTWu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25567/Reviewer_PTWu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907921960, "cdate": 1761907921960, "tmdate": 1762943477422, "mdate": 1762943477422, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark: GeoHOP, for remote-sensing-based geometrical reasoning tasks. The authors also proposed a baseline approach: GeoPlanner, combining LLMs with tool use. The authors have benchmarked several LLMs in the new benchmark with different settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The benchmark is useful and fills in one gap compared to existing reasoning benchmarks for LLM (which are mostly indoor / not remote-sensing oriented).\n- The paper is well written.\n- The authors have benchmarked a wide range of LLMs from different companies."}, "weaknesses": {"value": "**Experimental Evaluation Can be Stronger**\n\nThe analysis in the experiments is not super extensive/impressive. I would suggest the following as a benchmark paper:\n- What are the failure modes that are common across all LLMs?\nThe authors have shown the Tool execution success/failures, but more importantly, to solve the problems in the benchmark, what is the current most challenging open problem? Currently, the paper has only a few such kinds of discussions (open problems and future directions) but this is very critical for a benchmark paper.\n- What (fine-grained) capabilities can this benchmark evaluate?\nRight now, it is a bit abstract that GeoHOP can evaluate \"Geometric Reasoning in RS images\", we would like some discussions on fine-grained difficulties that can be quantified. For example, the benchmark can have a subset of tasks where semantic perception/grounding is especially hard, a subset of tasks where geometric grounding / relational grounding is especially hard, and a subset of tasks where multi-step long-horizon reasoning/planning is especially hard, so that readers can develop methods to address each of these challenges.\n\n**Related Works should be more extensive**\n\nThough this paper is mainly focused on geospatial reasoning + perception, I encourage the authors to discuss a broader range of reasoning benchmarks for LLMs in related works. E.g., FOLIO [1] and LogiCity [2], they are also closely related literature that is trying to evaluate similar capabilities of LLMs (especially in LogiCity the visual track requires both perception/grounding and reasoning). The authors should highlight the key challenge in geospatial reasoning compared to the reasoning problems in those benchmarks.\n\n[1] Han, Simeng, et al. \"Folio: Natural language reasoning with first-order logic.\" arXiv preprint arXiv:2209.00840 (2022).\n[2] Li, Bowen, et al. \"LogiCity: Advancing neuro-symbolic ai with abstract urban simulation.\" Advances in Neural Information Processing Systems 37 (2024): 69840-69864."}, "questions": {"value": "See weaknesses. I will consider raising the score if the authors address my concerns during the rebuttal phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6dm4hPIGeB", "forum": "ax6oQWQmeR", "replyto": "ax6oQWQmeR", "signatures": ["ICLR.cc/2026/Conference/Submission25567/Reviewer_e2yx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25567/Reviewer_e2yx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916000018, "cdate": 1761916000018, "tmdate": 1762943477211, "mdate": 1762943477211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GeoHOP, a benchmark dataset for evaluating the capabilities of LLM-driven tool augmentation agents on geospatial reasoning tasks. GeoHOP contains 417 scene-driven hierarchical perception tasks covering optical, SAR, and infrared imagery. The authors also propose GeoPlanner, an LLM-driven agent that organizes analytics tools into a functional hierarchy and executes a fault-tolerant inference pipeline. Experiments show that even state-of-the-art models still have significant room for improvement on geospatial cognitive reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper makes a compelling case for moving beyond perception-oriented tasks to cognitive-level geospatial reasoning. The distinction between simply detecting objects versus understanding their spatial relationships and implications for real-world decisions is clearly articulated.\n\n2. The two-stage pipeline with knowledge-augmented generation followed by expert adjudication demonstrates careful attention to quality. Having eight domain experts perform three-pass reviews with an auditable rubric shows commitment to data integrity."}, "weaknesses": {"value": "1. With only 417 instances, the dataset is relatively small compared to other benchmarks in the field. The reliance on just four source datasets may not capture the full complexity of real-world geospatial scenarios, particularly multi-temporal analyses or extreme weather events that practitioners actually encounter.\n2. Using LLM-as-judge for evaluation introduces potential biases that aren't adequately addressed. The lack of human validation or inter-rater reliability measures undermines confidence in the results. Additionally, the absence of fine-grained error analysis makes it difficult to understand failure modes.\n3. GeoPlanner largely combines existing techniques (hierarchical planning, ReAct-style execution, standard sentence embeddings). The contribution feels more like engineering than research innovation, which may limit its impact at a top-tier venue.\n4. The absence of ablation studies leaves key design choices unjustified. Without comparing to ThinkGeo directly or analyzing performance differences across modalities, it's hard to assess the true benefits of the proposed approach. The lack of computational cost analysis is also problematic for practical adoption."}, "questions": {"value": "1. How is the quality of queries and toolchains generated by ChatGPT-5 ensured? How consistent is the expert review process?\n\n2. What are the performance differences of GeoPlanner across different modalities (optical vs. SAR vs. IR)? Are modality-specific adjustments required?\n\n3. What are the most common failure modes? Is it due to incorrect tool selection or parameter settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tFP8f911Nq", "forum": "ax6oQWQmeR", "replyto": "ax6oQWQmeR", "signatures": ["ICLR.cc/2026/Conference/Submission25567/Reviewer_5bSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25567/Reviewer_5bSx"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission25567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976013738, "cdate": 1761976013738, "tmdate": 1762943476888, "mdate": 1762943476888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}