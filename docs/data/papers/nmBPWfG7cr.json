{"id": "nmBPWfG7cr", "number": 4023, "cdate": 1757585534494, "mdate": 1759898057420, "content": {"title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning", "abstract": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e., in days and weeks) egocentric videos, which leverages a structured Chain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained via reinforcement learning (RL). Inspired by human problem-solving strategies, CoTT decomposes complex reasoning into modular steps, with the RL agent invoking specific tools, one per step, to iteratively and collaboratively answer sub-questions tackling such tasks as temporal retrieval and multi-modal understanding. We design a two-stage training paradigm involving supervised finetuning (SFT) of a pretrained language model using CoTT data and RL to enable our agent to dynamically propose step-by-step tools for long-range reasoning. To facilitate training, we construct a dataset called Ego-R1 Data, which consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our Ego-R1 agent is evaluated on a newly curated week-long video QA benchmark, Ego-R1 Bench, which contains human-verified QA pairs from hybrid sources. Extensive results demonstrate that the dynamic, tool-augmented chain-of-thought reasoning by our Ego-R1 Agent can effectively tackle the unique challenges of understanding ultra-long egocentric videos, significantly extending the time coverage from few hours to a week.", "tldr": "", "keywords": ["egocentric reasoning; long video understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66b885405d2116fc2394f55ecc2eb1b1099d336f.pdf", "supplementary_material": "/attachment/4e2ee33905a3d0de61b31324335a55844b24fcc7.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Ego-R1, a tool-augmented agent for long egocentric video QA. The key idea is a chain-of-tool-thought (CoTT) controller that dynamically decides when to invoke 1) a hierarchical RAG over text logs; 2) a short-horizon Video-LLM; and 3) a frame-level VLM. The model is trained with a two-stage approach: 1) SFT on CoTT traces, 2) GRPO-style RL. The authors also introduce Ego-R1 Data (Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL) and an evaluation set Ego-R1 Bench (week-long videos). Reported results show strong performance across multiple long-video QA benchmarks, including VideoMME and EgoSchema."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- Sound framework. A simple yet compelling agentic architecture separating long-range retrieval from localized video understanding (Video VLM) and fine-grained frames (frame VLM), with a controller trained to decide which to use when through SFT+RL.\n\n- Good results. On several benchmarks, including VideoMME and EgoSchema, Ego-R1 achieves strong results compared to other methods like VideoAgent and Gemini-1.5-Pro.\n\n- Useful ablations. Ablations show that both stage training (SFT+RL) improves performance."}, "weaknesses": {"value": "- Limited novelty. The agentic framework in video understanding has been well explored, for example, in VideoAgent. The training techniques used in this paper, SFT followed by RL, are also well-known, as in VideoR1. Given the limited novelty, more insightful analysis, such as the failure cases when applying the method to this setting, the reasons for the failures, and how to adapt them, is worth further study. For instance, how to create the best SFT data? how to design the rewards?\n\n- More ablations needed. The paper proposed using both video VLM and frame VLM as tools; I'm wondering about the performance of removing each of them. How each contributes to the final performance, and how different model choices in these video VLMs and frame VLMs affect the performance.\n\n- Questionable improvements due to contamination. The paper shows most improvements in EgoLifeQA and Ego-CoTT-25K. However, the training data and these benchmarks are closely related in the domain and construction pipeline. This raises concerns of data contamination rather than real generalization."}, "questions": {"value": "See weaknesses.\n\nLine 127: \"Table ??\" is a typo"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ppqeIBcVDU", "forum": "nmBPWfG7cr", "replyto": "nmBPWfG7cr", "signatures": ["ICLR.cc/2026/Conference/Submission4023/Reviewer_E6Ee"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4023/Reviewer_E6Ee"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761341134512, "cdate": 1761341134512, "tmdate": 1762917139386, "mdate": 1762917139386, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper successfully adopts the Deepseek-R1 style reasoning training in the context of egocentric video understanding, with tool integration. To this end, the authors propose a manually annotated and automatically generated dataset. After a cold start and GRPO training, the trained model surpasses the compared baselines in VideoMME and 3 other egocentric video benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper extends egocentric video understanding into week-level duration.\n- This paper successfully implements reasoning-tool calling CoT in the context of video understanding.\n- This paper proposes new datasets for egocentric video understanding."}, "weaknesses": {"value": "- Long temporal retrieval is conducted in the text form instead of visual language matching. However, the transformation from visual space to textual space inevitably loses information.\n- In Tab.1, the performance of the base model is not reported. In addition, although samples that are overlapped with the benchmark are removed in training, the cold-start and RL stages are still focused on ego-centric videos that are in-domain data. Therefore, the comparison with other general video models seems to be meaningless in the egocentric setting.\n- The tool set used in this paper seems to be limited but heavy (captioning, VLM, ...). I am wondering about the training and inference costs.\n\nIn conclusion, I think this paper is another application of the ReAct paradigm in the context of egocentric understanding; although new datasets are proposed, I think the contribution seems to be limited, and experiments are not solid enough."}, "questions": {"value": "- In Lines #077-078, I think this paradigm is known as ReAct. What is the need to create a novel term, CoTT, for video understanding?\n- In Line #404, is the base model Qwen2.5-VL-3B instead of Qwen2.5-3B?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QsePXThIvt", "forum": "nmBPWfG7cr", "replyto": "nmBPWfG7cr", "signatures": ["ICLR.cc/2026/Conference/Submission4023/Reviewer_53uk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4023/Reviewer_53uk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891430817, "cdate": 1761891430817, "tmdate": 1762917139232, "mdate": 1762917139232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hierarchical RAG framework for egocentric video understanding, called Ego-R1 Chain of Tool-Thoughts. The method builds a temporal hierarchy in its database, organizing information by week, day, hour, and clip, and introduces a multi-tool reasoning pipeline combining retrieval, segmentation, and question answering. The authors evaluate the model on an egocentric benchmark where timestamps and hierarchical video segmentation are clearly defined, showing improved retrieval and reasoning performance compared to flat RAG baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "-\tClear and systematic hierarchical RAG structure, which improves efficiency and relevance in timestamp-based video reasoning tasks.\n-\tExperiments on multiple egocentric datasets demonstrate consistent improvement over flat retrieval methods."}, "weaknesses": {"value": "-\tThe hierarchical database structure (week → day → hour → clip) appears optimized for benchmarks with clear temporal granularity, but it’s unclear if it remains effective for datasets or tasks where such segmentation is not naturally defined.\n\n-\tThe uniform video segmentation approach might not be robust across diverse video lengths or event types. The method may fail to capture variable-duration actions or continuous interactions.\n\n-\tThe technical contribution is moderate, focusing on database structuring and system integration rather than advancing the underlying reasoning or learning algorithms."}, "questions": {"value": "-\tHow robust is the proposed hierarchy when applied to benchmarks that do not have clear or fixed temporal units (e.g., datasets without explicit timestamps)?\n\n-\tIs the uniform segmentation scheme adaptive to variable-length activities, or could it fragment meaningful events?\n\n-\tCould the system generalize to other RAG tasks beyond egocentric video, or is it tightly coupled to timestamp-based segmentation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iCaArGGTgE", "forum": "nmBPWfG7cr", "replyto": "nmBPWfG7cr", "signatures": ["ICLR.cc/2026/Conference/Submission4023/Reviewer_Us6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4023/Reviewer_Us6s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947268026, "cdate": 1761947268026, "tmdate": 1762917139035, "mdate": 1762917139035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Ego-R1 is a Chain-of-Tool-Thought (CoTT) agent for week-scale egocentric QA that plans over hierarchical memory (day→hour→10-min captions/ASR) and adaptively invokes a short-window Video-LLM plus a single-frame VLM for fine grounding.\nTraining is SFT on tool-grounded traces followed by GRPO reinforcement to learn multi-turn policies that trade accuracy vs. tool economy (≈7 calls/question; tens of frames instead of hours).\nThe release includes Ego-CoTT-25K (synthetic traces), Ego-QA-4.4K (human-verified QA), and a week-long benchmark (≈44.3 h/video) targeting long-horizon temporal reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: Frames week-scale egocentric QA as sequential decision-making via a Chain-of-Tool-Thought controller over hierarchical temporal memory with adaptive Video-LLM/VLM calls.\n- Quality: Strong margins on a week-long benchmark (46.0%, +7.7 vs Gemini-1.5-Pro), clear ablations (SFT+RL > SFT; CoTT > retrieval-only), and substantial frame-budget reductions.\n- Clarity: Explicit tool APIs, training signals, and memory construction; stepwise traces reveal evidence flow and typical failure modes.\n- Significance: Provides reusable traces, QA sets, and a week-scale benchmark, establishing a modular, plug-and-play blueprint likely to influence long-video agents beyond egocentric QA."}, "weaknesses": {"value": "- CoTT over hierarchical memory likely overlaps prior agentic long‑video approaches. Action: run strict, matched‑backbone and matched‑budget comparisons against strong agentic and training‑free video‑RAG baselines; add a lightweight-critic variant to test the incremental value of planning alone.\n- Data construction and inference rely on proprietary LLMs/VLMs. Action: provide a fully open stack with results, release exact prompts/tool schemas/configs, and report contamination checks between generation pipelines and evaluation items.\n- Gains may stem from backbone swaps and costs exclude memory-bank build. Action: standardize backbones across methods; report fixed vs. dynamic frame budgets, wall‑clock and $/QA including offline preprocessing; and quantify hierarchical‑RAG hit@K and temporal localization error."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BmqU8duMyr", "forum": "nmBPWfG7cr", "replyto": "nmBPWfG7cr", "signatures": ["ICLR.cc/2026/Conference/Submission4023/Reviewer_SYq6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4023/Reviewer_SYq6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762054380698, "cdate": 1762054380698, "tmdate": 1762917138857, "mdate": 1762917138857, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}