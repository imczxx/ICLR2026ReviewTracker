{"id": "XuvhmSMbCE", "number": 19053, "cdate": 1758293137465, "mdate": 1759897063734, "content": {"title": "UniRTL: Unifying Code and Graph for Robust RTL Representation Learning", "abstract": "Developing effective representations for register transfer level (RTL) designs is crucial for accelerating the hardware design workflow.\nExisting approaches, however, typically rely on a single data modality, either the RTL code or its associated graph-based representation, limiting the expressiveness and generalization ability of the learned representations. \nParticularly, graph-related methods often adopt data flow or register-level sub-circuits, both of which capture only partial information and thus provide an incomplete view of the design.\nIn contrast, the control data flow graph (CDFG) offers a more comprehensive structural representation that preserves complete information, while the code modality explicitly encodes semantic and functional information.\nWe argue that integrating these complementary modalities is essential for a thorough understanding of RTL designs.\nTo this end, we propose UniRTL, a multimodal pretraining framework that learns unified RTL representations by jointly leveraging code and CDFG.\nUniRTL achieves fine-grained alignment between code and graph through mutual masked modeling and employs a hierarchical training strategy that incorporates a pretrained graph-aware tokenizer and staged alignment of text (i.e., functional summary) and code prior to graph integration.\nWe evaluate UniRTL on two downstream tasks, performance prediction and code retrieval, under multiple settings. Experimental results show that UniRTL consistently outperforms prior methods, establishing it as a more robust and powerful foundation for advancing hardware design automation.", "tldr": "", "keywords": ["RTL Representation Learning", "Multimodal Learning"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46f55a1f84ee4be1615dcbbc9f9ebe0de0c16592.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents UniRTL, a multimodal pretraining framework designed to learn unified and robust representations for Register Transfer Level (RTL) hardware designs. The authors argue that existing methods are limited because they typically rely on only a single modality â€“ either the RTL source code or a graph representation (like data flow graphs or register-level sub-circuits). These single modalities capture incomplete information; code provides semantic/functional details, while graphs offer structural insights. Furthermore, existing graph representations like data flow are often incomplete.\n\nUniRTL aims to overcome these limitations by jointly leveraging RTL source code and its corresponding Control Data Flow Graph (CDFG), which the authors argue is a more comprehensive structural representation. The key components of UniRTL are: Unified Transformer Architecture, Graph-Aware Tokenizer, Mutual Masked Modeling, Hierarchical Training Strategy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strong Motivation for Multimodality: The paper makes a compelling case for why combining code and a comprehensive graph representation (CDFG) is necessary for robust RTL understanding. It clearly articulates the complementary nature of these modalities and the limitations of prior work using incomplete graphs (data flow) or only code.\n\nUse of CDFG: Choosing CDFG over simpler graph forms like data flow or register sub-circuits is a significant strength. As argued (and illustrated in Figure 1), CDFG preserves more complete structural and control flow information, which is likely crucial for tasks like performance prediction."}, "weaknesses": {"value": "Complexity: The overall framework is quite complex, involving multiple pretraining stages, a specialized graph tokenizer, and careful integration of three modalities (text summary, code, graph). This might raise concerns about the practicality of training and deploying such a model compared to simpler unimodal approaches.\n\nCDFG Generation Failures: The paper notes that CDFG generation failed for a large portion of the collected RTL designs (only ~39k out of ~132k succeeded). While the failed samples were still used for text-code alignment, this highlights a potential practical limitation: the approach relies heavily on the availability and successful generation of high-quality CDFGs. The reasons for failure (syntax errors, tool limitations?) and their impact are not fully explored.\n\nLimited Ablation on Alignment: While ablation studies show the benefit of including both code and graph (Table 1), there isn't a direct comparison between the mutual masked modeling alignment strategy and other potential alignment methods (e.g., contrastive loss between graph nodes and code tokens). This makes it slightly harder to isolate the specific contribution of the masking approach versus just having both modalities present.\n\nDependence on Base Code Model: The performance relies significantly on the underlying CodeBERT base model. It's unclear how much of the gain comes from the multimodal fusion itself versus simply starting with a strong code foundation model."}, "questions": {"value": "CDFG Robustness: Could the authors elaborate on the CDFG generation failures? Were these failures concentrated in specific types of RTL designs (e.g., those generated by LLMs, or using specific Verilog constructs)? How robust is the CDFG generation process to variations in coding style or complexity? Does the model's performance correlate with the quality or completeness of the generated CDFG?\n\nComplexity vs. Performance Trade-off: Given the complexity of the multi-stage training and the specialized graph tokenizer, how does UniRTL compare to simpler baselines (e.g., just fine-tuning a large code LLM like CodeLlama/DeepSeek-Coder directly on the downstream tasks) in terms of training efficiency and inference latency versus the observed performance gains?\n\nAlternative Graph Representations: The paper argues strongly for CDFG. Have the authors experimented with or considered other graph representations that might be easier to generate reliably (even if less complete), such as Abstract Syntax Trees (ASTs) augmented with some data flow edges? How would UniRTL perform with these alternatives?\n\nNature of Code-Graph Alignment: The mutual masking forces the model to predict masked graph nodes using code context (and vice versa). Can the authors provide any qualitative analysis (e.g., using attention maps) to illustrate what kind of code-graph relationships the model learns? Does it correctly associate specific code lines with corresponding operation nodes in the CDFG?\n\nI would consider raising my scores if the concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hBl4bOikRX", "forum": "XuvhmSMbCE", "replyto": "XuvhmSMbCE", "signatures": ["ICLR.cc/2026/Conference/Submission19053/Reviewer_GRuV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19053/Reviewer_GRuV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761505549569, "cdate": 1761505549569, "tmdate": 1762931087663, "mdate": 1762931087663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present UniRTL, a framework for generating unified RTL representations by jointly learning from code text and control data flow graphs. The main contribution of the paper is learning this unified representation from code and graph information. Unlike prior methods that rely on one modality, uniRTL learns from both text and graph."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The strengths of the paper can be summarized as follows:\n\n- They adapt the GraphCodeBert idea to hardware code. Additionally, they introduce using the control data flow graph that captures more code semantics than the data flow graph which only captures variables. \n- They also capture text description, in addition to code and graph information. \n- Experimental results show higher performance compared to previous methods."}, "weaknesses": {"value": "Weaknesses can be summarized as follows:\n\n- The main idea of obtaining a unified code representation that captures both text and graph modalities has been introduced in GraphCodeBert for software code. \n- Improvements compared to StructRTL is very small. \n- Improvements compared to GraphCodeBert for code search tasks is also very small. \n- Lack of comparison to other PPA prediction methods like VeriDistill and CircuitFusion."}, "questions": {"value": "Could the authors clarify how GraphCodeBERT was used as a baseline? Was it fine-tuned on your Verilog dataset, or evaluated in its original form pretrained on software languages."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1MGJyhH4Bm", "forum": "XuvhmSMbCE", "replyto": "XuvhmSMbCE", "signatures": ["ICLR.cc/2026/Conference/Submission19053/Reviewer_xfSx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19053/Reviewer_xfSx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928260422, "cdate": 1761928260422, "tmdate": 1762931087179, "mdate": 1762931087179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work presents representation learning of the RTL (register transfer level) representation of digital electronic design with a multimodal pretraining framework of RTL code and RTL CDFG (control data flow graphs). It uses a unified representation of functional summary text, RTL source code and graph tokenization for a core transformer architecture. The pretraining is performed in two stages: a first stage does text-code alignment without the graph representation present by masked modeling. A second stage does text-code-graph alignment, also by masked modeling. Final downstream tasks are performance predictions and code retrieval."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* Strong results that masked modeling on this unified format learns strong representations that perform well on downstream tasks that involve both code understanding and graph structural understanding."}, "weaknesses": {"value": "* Improvements over baselines (StructRTL in Table 1 for performance prediction, GraphCodeBERT in Table 4 for functional equivalence) are marginal.\n* This work is essentially combining the two previous approaches: StructRTL and GraphCodeBERT."}, "questions": {"value": "* The paper is well written and the results are important for the electronic design community, and other fields that might have similar representations (maybe chemistry or proteomics). I am questioning the novelty and original contribution of this work since the key representation learning aspects are essentially a combination of the StructRTL and GraphCodeBERT prior works."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fpP4u12Nyy", "forum": "XuvhmSMbCE", "replyto": "XuvhmSMbCE", "signatures": ["ICLR.cc/2026/Conference/Submission19053/Reviewer_z6Kx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19053/Reviewer_z6Kx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004594847, "cdate": 1762004594847, "tmdate": 1762931086708, "mdate": 1762931086708, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes the UniRTL approach as a multimodal pretraining framework that learns unified RTL representations by jointly leveraging code and control data flow graph (CDFG). The key idea of the UniRTL starts from the limitations of CircuitFusion (Fang et al., 2025). CircuitFusion (Fang et al., 2025) derives unimodal representations using three independent encoders, and integrates them through a cross-attention mechanism. The alignment strategy in CircuitFusion (Fang et al., 2025) is coarse-grained relying on contrastive learning between text-code and text-graph pairs but neglects the fine-grained alignment between code and graph which are two modalities to contain richer information and considered in UniRTL framework. UniRTL is tested for performance prediction and code retrieval."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ Considering bode the RTL code and the code and control data flow graph as modalities for a multimodal representation framework.\n+ Construction of a dataset by collecting sources from sources, including RTLCoder (Liu et al., 2024), MG-Verilog (Zhang et al., 2024), DeepRTL (Liu et al., 2025b), and DeepCircuitX (Li et al., 2025).\n+ UniRTL can be used for performance prediction and code retrieval."}, "weaknesses": {"value": "1) The methodology of UniRTL seems straight forward as show in in Figure 2, which consist of a hierarchical training strategy, where a graph-aware tokenizer is first pretrained, and text-code alignment is performed prior to graph incorporation. However, the main reasons behind certain parameters used in this machine learning architecture are not provided. For example: what is the reason of performing principal component analysis to reduce the dimensionality of the description embedding from 768 to 32? How did the 32 was decided? Is there something particular about 32 that other researchers in this area should also consider? Similarly, in the graph isomorphism part, why 16 eigenvectors were selected for constructing the global positional encodings? Are there any specific reasons? Are there any alternative approaches? Were other alternative approaches tried and failed?\n2) Indeed, the control data flow graph (CDFG) offers a more comprehensive structural representation that preserves complete information and the CDFG has been extensively exploited in RTL analysis, high level synthesis and related problems (see examples like \"A design-for-testability technique for register-transfer level circuits using control/data flow extraction.\" IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 17, no. 8 (2002): 706-723; \"Register binding based power management for high-level synthesis of control-flow intensive behaviors.\" In Proceedings. IEEE International Conference on Computer Design: VLSI in Computers and Processors, pp. 391-394. IEEE, 2002; GAHLS: an optimized graph analytics based high level synthesis framework.\" Scientific Reports 13, no. 1 (2023): 22655\"Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning.\" arXiv preprint arXiv:2508.18730 (2025); \"ODGS: Dependency-Aware Scheduling for High-Level Synthesis with Graph Neural Network and Reinforcement Learning.\" ACM Transactions on Architecture and Code Optimization 22, no. 2 (2025): 1-25). The discussion of related work requires significant enhancement and especially comparisons.\n3) From Table 1 results it seems that considering or not the code did not improve the overall are and delay results by much, but the graph which I assume refers to CDFG was significant. Given the choices of various parameters like the number of PCAs and the number of eigenvectors , how did those and other parameters influenced the results?  \n4) What is the computational complexity or how do the different approaches compare in terms of runtime? Is UniRTL performing best across all tasks or is it more suitable for some?\n5) There are some small grammar issues and word repetitions like \"and and\" on the bottom of page 1"}, "questions": {"value": "1. Justification of CDFG Completeness vs. Practical Utility\nThe authors repeatedly claim that CDFGs \"preserve complete information without loss and can be faithfully converted back to code.\" However, what is the practical impact of this completeness claim? Could the authors provide empirical evidence or ablation studies demonstrating that this completeness meaningfully contributes to downstream task performance compared to the \"incomplete\" data flow representations used in GraphCodeBERT? The comparison seems unfair since GraphCodeBERT uses variable-only data flows by design for different purposes.\n\n2. Limited Novelty in Architectural Contributions\nThe core technical contributions appear to be: (1) using CDFGs instead of data flows, (2) adding a graph-aware tokenizer, and (3) hierarchical training with text-code alignment before graph incorporation. However, the mutual masked modeling alignment strategy closely follows GraphCodeBERT's approach. Could the authors clarify what fundamental methodological innovations distinguish UniRTL beyond these incremental engineering choices? How would UniRTL perform if GraphCodeBERT were provided with the same CDFG representation?\n\n3. Dataset Construction and Filtering Concerns\nThe authors mention that only 38,888 out of 132,008 RTL designs (29.4%) successfully convert to CDFGs, with the remainder containing \"syntax errors leading to compilation failures.\" This raises concerns about: (a) whether the successful conversions represent a biased subset of simpler/cleaner designs, and (b) whether the hierarchical training strategy is truly \"maximizing data utilization\" or simply accommodating a significant data quality issue. How do the authors ensure the learned representations generalize to real-world RTL code that may contain the complexities causing conversion failures?\n\n4. Comparison with CircuitFusion and Missing Baselines\nThe authors exclude CircuitFusion from experimental comparison due to \"unavailability of released model checkpoints and insufficient details to enable faithful reproduction.\" Given that CircuitFusion is the most directly related work attempting multimodal RTL representation learning, this exclusion significantly weakens the evaluation. Additionally, the authors only compare against GraphCodeBERT which uses incomplete data flows. Why did the authors not implement a stronger baseline that uses CDFGs with GraphCodeBERT's architecture? Without these comparisons, how can readers isolate the contributions of the authors' specific design choices?\n\n5. Scalability and Computational Efficiency Analysis\nThe paper lacks discussion of computational costs and scalability. The graph-aware tokenizer requires pretraining a GIN + Transformer for 2,000 epochs, followed by hierarchical training of the main model. Could the authors provide: (a) wall-clock training time comparisons with baselines, (b) inference time analysis for downstream tasks, (c) memory requirements for processing large RTL designs with complex CDFGs, and (d) analysis of how performance scales with graph size? Given that the authors claim efficiency advantages over LLM-based methods, quantitative efficiency metrics are essential."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VBrVl36n0F", "forum": "XuvhmSMbCE", "replyto": "XuvhmSMbCE", "signatures": ["ICLR.cc/2026/Conference/Submission19053/Reviewer_WYch"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19053/Reviewer_WYch"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762293807768, "cdate": 1762293807768, "tmdate": 1762931086308, "mdate": 1762931086308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}