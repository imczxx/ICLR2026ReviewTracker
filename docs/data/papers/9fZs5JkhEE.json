{"id": "9fZs5JkhEE", "number": 4372, "cdate": 1757668127731, "mdate": 1759898036362, "content": {"title": "PIC: Revisiting INR for Image Coding with Fast Encoding and Sub-Millisecond Decoding", "abstract": "Implicit neural representations (INRs) have recently demonstrated impressive performance in novel view synthesis and image/video compression, offering distinct advantages in terms of decoding complexity compared to conventional end-to-end image codecs. However, their broader adoption has been limited by slow encoding speeds and underutilization of their inherent potential for fast decoding --- often falling short of traditional codecs like JPEG. In this work, we present a fully end-to-end INR-based image compression framework, **P**ractical **I**NR Image **C**odec (PIC), that addresses both limitations. Our encoder computes all requisite parameters for the INR network in a single forward pass, achieving encoding speeds of over 20 FPS. To further unlock the decoding efficiency of INRs, we design an optimized decoder capable of real-time performance at up to 2000 FPS --- substantially exceeding JPEG decoding speed while delivering competitive or superior rate-distortion (RD) performance. To the best of our knowledge, this is the first learning-based image codec that simultaneously achieves practical encoding speed, outperforms JPEG in RD performancec, and surpasses it in decoding speed by a large margin. Code will be made publicly available.", "tldr": "An INR-based image codec capable of encoding at 20 FPS and decoding at 2000 FPS.", "keywords": ["Implicit Neural Representation", "Image Compression"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/680973d0822338aeebc6bb662a9a3167ecf3e7cc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the trade-off of Implicit Neural Representation (INR)-based image compression—low decoding complexity but slow encoding—and limitations of end-to-end models/JPEG. It proposes PIC (Practical INR Image Codec), an end-to-end architecture that generates all INR network parameters via a single forward pass, achieving 20 FPS encoding."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Transforms INR encoding from time-consuming per-image training to real-time single forward pass, boosting encoding speed by 3+ orders of magnitude vs. COIN/GaussianImage.\n\n2. Aligns synthesis network (gs) with Tensor Core’s structure (16-channel layers) and uses Slang for training-inference consistency, enabling ultra-fast decoding.  \n\n3. ZpR module (simplified RLE on zeros) cuts bitrate without extra inference overhead."}, "weaknesses": {"value": "1. While outperforming representation models, it still lags behind some end-to-end models.\n\n2. Ablation studies show that shifting $MLP^i$ (instance-dependent part) from the end of $g_s$ to the start or middle drastically degrades performance (BD-Rate increases by 21.46% and 2.43%, respectively). This high sensitivity reduces architectural flexibility for further optimizations.\n\n3. The MLP parameters in the bitstream are stored in FP32 format. Without lower-precision quantization (e.g., FP16), this increases bitstream size unnecessarily, compromising overall coding efficiency compared to models with parameter quantization.\n\n4. More details in Figure 2 maybe better since I can not identify the calculatio preocess from the middle to the right."}, "questions": {"value": "See Weaknesses and I would prefer to improve my rating for the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zqhKa1SjO4", "forum": "9fZs5JkhEE", "replyto": "9fZs5JkhEE", "signatures": ["ICLR.cc/2026/Conference/Submission4372/Reviewer_6zum"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4372/Reviewer_6zum"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761290867715, "cdate": 1761290867715, "tmdate": 1762917321123, "mdate": 1762917321123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose PIC, a new INR-based learned image compression framework that enables both fast encoding and decoding. Unlike most existing approaches that require overfitting the representation during encoding, PIC uses an INR whose layer weights are trained, while the grid parameters and activation modulation are generated by the encoder. The whole framework is trained end-to-end and requires only a forward pass during encoding/decoding. The encoding/decoding speed is very fast, and the RD performance is better than prior work including COIN and GaussianImage."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-\tThe framework is simple but effective for image coding. It is similar to a COOL-CHIC-like architecture, but uses the encoder to generate the modulation and grid parameters. It also benefits from training on a large dataset.\n-\tThe decoder implementation is also an important contribution. There are very few works that implement learned compression models with optimized components such as custom GPU kernels, which is important for real-world applications."}, "weaknesses": {"value": "- The compression performance is clearly not as good as common baseline methods, for example, the standard end-to-end models like the hyper-prior model, or more recent INR models like COOL-CHIC. It is not clear if the proposed method acheive a better trade-off between complexity and RD performance\n- The comparison is limited.\n    - The baselines are relatively weak in RD performance.\n    - The authors should consider comparing to N-O COOL-CHIC [1], which also has very low complexity and is not overfitted.\n- The framework does not seem strictly end-to-end, due to the use of the ZrP module. Also, the exact method for estimating the entropy is not given in the paper.\n- There are some missing details in the paper\n    - How is decoding done? Does it take the coordinates as input? Does it follow COOL-CHIC-style decoding? The authors should also consider providing a reference in this part, as this is not the main contribution of the paper.\n    - How is $ p_{\\theta_{k}}$ estimated? \n    - What resolution is used in Table 1?\n- While the main advantage of the proposed method is the encoding/decoding speed, the ablation study is focused on rate–distortion performance. Providing an ablation study on the encoding/decoding speed would be helpful.\n\n[1] Blard, Théophile, et al. \"Overfitted image coding at reduced complexity.\""}, "questions": {"value": "- Why is the convolutional layer $g_{m}$ output size is $N \\times C$? Are there some layers missing?\n- How scalable is the framework? Given the optimized kernel for the tensor core, would increasing the network layer size cause a dramatic increase in decoding time?\n- Line 292: How much does using full FP32 for training impact the performance? In my experience, many image codecs trained in FP32 work fine in FP16, and the error from FP32 to TF32 should be even smaller."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ztrDILdwCz", "forum": "9fZs5JkhEE", "replyto": "9fZs5JkhEE", "signatures": ["ICLR.cc/2026/Conference/Submission4372/Reviewer_8sLj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4372/Reviewer_8sLj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664718540, "cdate": 1761664718540, "tmdate": 1762917320743, "mdate": 1762917320743, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed PIC is an E2E feed-forward INR-based image codec, where a neural encoder produces all parameters of a tiny per-image decoder in one forward pass, based on channel-wise affine modulations, thereby circumventing the long training typical of INR codecs. The codec also features a lightweight entropy path (channel-wise), a simplified pRLE (where run-length is on zeros only), and a highly optimized GPU decoder implementation. It reports ~20FPS encode and ~2000FPS decode on a single 4090-class GPU, decoding faster than nvJPEG while being comparable in RD performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper closes the INR practicality gap with its pure feed-forward encoding (no per-image training at all) and ultra-fast decoding. Only GaussianImage and the proposed PIC, to the best of my knowledge, could decode images at the magnitude of ~2000 FPS, yet PIC is stronger than GaussianImage.  \n- The compression performance is comparable against JPEG at higher bpp ranges on Kodak, compared to COIN and GaussianImage.\n- The experimental results are complete, self-contained, with fruitful and honest engineering details: careful timing protocol, CUDA kernels for ZpR/decoder, Slang-Torch implementation notes, etc."}, "weaknesses": {"value": "- **Unfair profiling.** JPEG is normally I/O-bound and optimized for batch streaming, but the paper measures it in-process without I/O, a certainly fair but probably atypical setup. Since PIC’s decoding pipeline already runs fully in-memory, removing I/O overhead disproportionately benefits PIC, making the comparison slightly optimistic. I would recommend the authors provide a more detailed breakdown of the per-component runtimes.\n- **Entropy estimation vs actual drift.** Authors show noticeable gaps between estimated and realized bitrate (Pearson around 0.95–0.99 depending on dataset and $\\lambda$); they note this is common but it indeed complicates precise rate control.\n- **Compression efficiency.** I think the selected baselines are relatively weak, and that the compression performance is not standing out either. In my person opinion, I am not entirely sure if 2000FPS is that necessary (maybe the difference while be more noticeable on higher resolution images?), but a performance only comparable to JPEG is not good enough. \n- **Scalability.** The method appears tightly coupled to a fixed channel width (C = 16): every layer in the analysis and synthesis paths uses this width, and the custom CUDA decoder kernel seems specifically tuned to 16×16 Tensor Core tiles. This raises concerns about scalability and generality: it’s unclear whether PIC maintains speed or compression efficiency when the channel dimension changes, or if the implementation would require re-engineering to handle higher-capacity models. Could the authors clarify whether the architecture and fused kernels support other channel sizes (e.g., 8, 32, 64) and provide throughput/RD results for such variants to show the design scales beyond a single configuration?\n\nOverall, I think the paper is of good quality with clear motivation and strong engineering, but its technical novelty and compression performance remain limited. I would consider raising my score if the authors provide evidence of scalability (beyond C=16), stronger RD comparisons against modern learned codecs (or counter-arguments to why this is not compulsory), and a more complete runtime breakdown that includes I/O and decoding components to substantiate the claimed efficiency advantage."}, "questions": {"value": "- I think the paper would benefit from a rate-distortion-complexity frontier comparison (like Figure 1 of MobileNVC [1]) and that PIC should clearly demonstrate a consistently better performance (manifested as a better Pareto frontier) than existing baselines.\n\n[1] MobileNVC: Real-time 1080p Neural Video Compression on a Mobile Device, WACV'24."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Z5JGv01Qyi", "forum": "9fZs5JkhEE", "replyto": "9fZs5JkhEE", "signatures": ["ICLR.cc/2026/Conference/Submission4372/Reviewer_RCpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4372/Reviewer_RCpp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761745067186, "cdate": 1761745067186, "tmdate": 1762917320523, "mdate": 1762917320523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an image compression framework that uses an implicit neural‐representation style approach but replaces instance‐specific training with a one‐pass encoding network, to achieve fast encoding and ultra-fast decoding while maintaining competitive rate‐distortion performance. For the one-pass encoding, the authors relied on the end to end compression framework, extracted the hierarchical latents and entropic coded in the DCT domain, and trained the modulation network and light weight synthesis network parameters.   The authors demonstrate encoding at ~20 fps and decoding at ~2000 fps, and show competitive RD vs JPEG and compared methods on Kodak/CLIC."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The major strength of this paper is the encoding and decoding speed, by taking the advantage of both end to end compression method and implicit neural representation. \n2) The method is carefully implemented, integrating DCT transforms, entropy modeling, and low-level hardware optimizations (Tensor Cores, Slang-Torch)."}, "weaknesses": {"value": "Following are the weakness of the paper\n\n1) The authors mostly focus on the encoding and decoding complexity of the neural codec, and they were able to propose method which is optimal in both, and the RD performance only matches or outperform in margin of JPEG. Most of the recent traditional codecs has higher RD performance with slightly higher complexity than JPEG. The RD performance of the proposed method compared to the learned codec is very low. The trade-off of complexity, rate and distortion needs to reasonable for the practical usage, the paper only outperform in one axis (complexity), and on other two axis (rate and distortion) needs to be improved.\n\n2) The contribution of the paper is more of engineered solution to adapt the end to end codec for the INR codec.\n\n3) Without the close performance to the Factorized prior model, I believe the proposed method does not offer any additional advantage over the JPEG\n\n4) The authors description \"encoding in one step\" is somewhat misleading, the latents are generated in one-step, but the entropy model and modulation network, synthesis network requires atleast iterations to optimize."}, "questions": {"value": "1) To optimize the modulation network, entropy model, and the synthesis network how many training iterations are performed? If low number of iterations are used, whether by increasing training iterations the performance could be improved.\n\n2) how the common part of the synthesis network is trained, the details are missing?. There are some works [1]  in literature of the end to end compression, whether the part of the synthesis network is fine-tuned for a given image/signal, how this is different/similar in the paper?\n\n[1] Improving The Reconstruction Quality by Overfitted Decoder Bias in Neural Image Compression, https://ieeexplore.ieee.org/abstract/document/10018052\n[2] Overfitting for Fun and Profit: Instance-Adaptive Data Compression, https://arxiv.org/abs/2101.08687"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "a6y9rU8M9C", "forum": "9fZs5JkhEE", "replyto": "9fZs5JkhEE", "signatures": ["ICLR.cc/2026/Conference/Submission4372/Reviewer_a2mu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4372/Reviewer_a2mu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4372/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762361666424, "cdate": 1762361666424, "tmdate": 1762917320222, "mdate": 1762917320222, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}