{"id": "7BLnSeWuei", "number": 16921, "cdate": 1758270256149, "mdate": 1759897209966, "content": {"title": "CP-Agent: Context‑Aware Multimodal Reasoning for Cellular Morphological Profiling under Chemical Perturbations", "abstract": "Cell Painting combines multiplexed fluorescent staining, high‑content imaging, and quantitative analysis to generate high-dimensional phenotypic readouts to support diverse downstream tasks such as mechanism-of-action (MoA) inference, toxicity prediction, and construction of drug–disease atlases. However, existing workflows are slow, costly and difficult to interpret. Approaches for drug screening modeling predominantly focus on molecular representation learning, while neglecting actual experimental context (e.g., cell line, dosing schedule, etc.), limiting generalization and MoA resolution. We introduce CP-Agent, an agentic multimodal large language model (MLLM) capable of generating mechanism-relevant, human-interpretable rationales for cell morphological changes under drug perturbations. At its core, CP-Agent leverages a context-aware alignment module, CP-CLIP, that jointly embeds high-content images and experimental metadata to enable robust treatment and MoA discrimination (achieving a maximum F1-score of 0.896). By integrating CP-CLIP outputs with agentic tool usage and reasoning, CP‑Agent compiles rationales into a structured report to guide experimental design and hypothesis refinement. These capabilities highlight CP-Agent’s potential to accelerate drug discovery by enabling more interpretable, scalable, and context-aware phenotypic screening---streamlining iterative cycles of hypothesis generation in drug discovery.", "tldr": "We present CP-Agent, a multimodal agent that interprets drug-induced cell's morphological changes  by context-aware alignment.", "keywords": ["Cell painting microscopy; Biomedicine; Multimodal reasoning; LLM; Agent"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8fd52e3681e6617ddc548fe8f3161305b871267.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CP-Agent, a multimodal system for analyzing Cell Painting drug perturbation experiments. The core technical idea is inserting learned embeddings for compound, dose, and time as placeholder tokens into the text encoder to align images with experimental context. The system achieves F1=0.896 on compound classification using 1.9M training pairs across three datasets which were normalized and fused in a scalable way. CP-Clip is turned into CP-Agent via subtool definitions, scaffolding and integration with GPT-5 to generate reports."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Clear reframing that experimental context (compound, concentration, time) is signal and should be fused into the text branch via learned token projections.\n* Principled multi-dataset curation with MoA harmonization. This is scalable and onboarding additional datasets should be cheap\n* The dataset integration and cross-normalization seems sensible and principled and onboarding new Cell Painting datasets should be cheap.\n* Clarity: I found the paper reasonably easy to read."}, "weaknesses": {"value": "* “Agentic” framing: The system is a single-pass orchestrated pipeline with predefined tool calls, there is little evidence of learned action selection or closed-loop planning beyond routing, so the agentic claim feels a little overstated.\n* I found the MLLM comparisons somewhat unfair and uninformative. CP-CLIP receives >1M domain-specific training pairs while GPT-5/Gemini/Claude receive (presumably) zero Cell Painting training and only minimal 2-stage prompting. MLLMs with extensive prompt engineering or few-shot learning would be a better comparison\n* Weak expert evaluation. No inter-rater reliability metrics or comparison to baseline methods (human-written reports, template-based summaries) are provided. This makes the high scores uninterpretable without reference points."}, "questions": {"value": "* For the expert evaluation, what is inter-rater reliability? Are differences between models statistically significant? How do the reports compare to human-written reports or simpler template-based baselines? Without these comparisons, the scores lack context.\n* Can you clarify what makes this system \"agentic\" vs being a fixed pipeline?\n* Can you provide image-free baselines using only molecular fingerprints and metadata (no microscopy images) for the classification tasks? This would quantify how much the images actually contribute to performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R73yQm8AqK", "forum": "7BLnSeWuei", "replyto": "7BLnSeWuei", "signatures": ["ICLR.cc/2026/Conference/Submission16921/Reviewer_3gDr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16921/Reviewer_3gDr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761525788931, "cdate": 1761525788931, "tmdate": 1762926946383, "mdate": 1762926946383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an agent-based framework comprising two main components: CP-CLIP, a contrastive learning model for chemical perturbation–image alignment, and CP-Agent, a agentic system built atop CP-CLIP."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The main novelty of this work lies in the introduction of an agentic framework specifically designed for Cell Painting images. While the underlying components (e.g., CLIP-based contrastive learning and vision encoders) are not new, applying an agent-based paradigm to organize multimodal reasoning, analysis, and interpretation within this biological context is conceptually original. This represents an interesting step toward structured and interpretable automation in cellular image understanding and drug discovery workflows."}, "weaknesses": {"value": "While incorporating agentic system for Cell Painting is interesting, there are several weakness of the proposed work,\n\n\n**Insufficient survey of related work in cross-modal contrastive learning for Cell Painting**\n* The related work section lacks sufficient discussion of recent multimodal and contrastive learning methods for Cell Painting. While the paper introduces CP-CLIP as a novel contribution, several recent works within this context are not adequately discussed [1, 2, 3]. \n* In addition, the manuscript lacks quantitative comparisons with prior frameworks. Although CP-CLIP is presented as a key innovation, the evaluation primarily contrasts against generic CLIP variants and omits stronger baselines from recent multimodal or contrastive approaches. This makes it difficult to disentangle whether the reported improvements stem from the proposed context-aware token injection mechanism or from other architectural or training differences.\n\n**Method**\n* While interpretability is highlighted as a major advantage of CP-Agent, the paper lacks quantitative evaluation of this aspect (e.g., faithfulness, consistency, or attribution accuracy).\n* The authors pretrained CP-CLIP on roughly 500 distinct compounds, which is small relative to the nearly two million image–context pairs. Since standard CLIP loss assumes each image–text pair is unique, simply aligning the same perturbation would potentially lead to overfitting or biased representations. \n\n**Limited evaluation and non-standard evaluation metrics for cross-modal contrastive learning**\n* The number of chemical perturbations explored in this work is relatively limited. Prior studies in cross-modal contrastive learning for Cell Painting [1,2,3] have leveraged datasets with over 10k compounds, whereas this study appears to utilize a smaller subset (approximately 500 compounds across 1.9M image–context pairs). This limitation constrains the generalizability of CP-CLIP and raises concerns regarding its robustness across broader chemical spaces.\n* The evaluation of CP-CLIP relies primarily on cosine similarity scores--the optimization objective of CLIP loss itself--rather than standard retrieval metrics such as Recall@K. This metric choice makes the reported improvements less convincing, as higher cosine similarity naturally aligns with the training objective.\n* The paper benchmarks CP-Agent primarily against general-purpose MLLMs (e.g., GPT-5, Gemini-2.5-Pro, Claude-4-Sonnet), which are not pretrained on Cell Painting data. A more appropriate baseline would involve fine-tuning a VLM on the same Cell Painting datasets for a fairer comparison.\n* How classification is done for standard CLIP, e.g.,whether a separate classifier is trained or retrieval is done by ranking cosine similarity, is not clearly explained.\n\n**Missing Ablation Studies and Component Analysis**\n * The agentic system (CP-Agent) includes several modules (e.g., CPContext, FeatRank, StatSynth, ReportGen), yet the manuscript provides no ablation to assess their individual contributions. Without such analysis, it remains unclear which components are necessary or most influential for final performance.\n\n**Unclear dataset and evaluation setup**\n* The description of training, validation, and test splits is ambiguous. It is unclear which datasets are used for each stage and how compounds are selected for evaluation. Given the paper’s focus on chemical perturbations, omitting comprehensive datasets such as [4] limits the scope. \n\n**Presentation and clarity issues**\n* Several figures (e.g., Figures 1–4) lack sufficient captions and details. For example, the use of a GPT-2 tokenizer in Figure 1 is not clear, what constitutes “raw text” and how it is tokenized remain unclear. Similarly, later figures provide only high-level summaries without specifying details.\n\n[1] CLOOME: contrastive learning unlocks bioimaging databases for queries with chemical structures Sanchez-Fernandez et al Nature com 2023\n\n[2] How Molecules Impact Cells: Unlocking Contrastive PhenoMolecular Retrieval Fradkin et al, NeurIPS 2024\n\n[3] CellCLIP – Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning Lu et al NeurIPS 2025\n\n[4] Cell Painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes, Bray et al Nature protocal 2016"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "XHJQTCal4z", "forum": "7BLnSeWuei", "replyto": "7BLnSeWuei", "signatures": ["ICLR.cc/2026/Conference/Submission16921/Reviewer_uvt6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16921/Reviewer_uvt6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761608652207, "cdate": 1761608652207, "tmdate": 1762926945894, "mdate": 1762926945894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CP-Agent, an agentic multimodal large language model (MLLM) system designed to analyze Cell Painting assay data to study how chemical perturbations affect cell morphology. The core of CP-Agent is CP-CLIP, pretrained with cell paint images and experimental metadata pairs by contrastive learning. Then CP-Agent leverages the context inferred from CP-CLIP to help generate structured report including experimental design and hypothesis refinement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is original in explicitly treating experimental context as signal and injecting it into the text encoder for semantically image–text alignment with CLIP style training. The pretraining corpus including 1.9M image–context pairs, which is reasonable scale for Cell Painting experiments and well-suited to learning robust, context-aware representations.\n\n2. The system is presented as a clean, step-wise workflow with structured JSON outputs and rich case studies that connect reasoning steps, making the reasoning traceable for practitioners.\n\n3. CP-CLIP achieves robust MoA/treatment discrimination. The framework is portable to other imaging modalities and broader phenotypic screening use cases."}, "weaknesses": {"value": "1. More ablation study is needed for the proposed method. The ablation study could include: 1) remove each context field (<CMPD>, <CONC>, <TIME>) separately, and then retrain the model, to show how the model performance change 2) remove control images from the image embedding, use only the perturbation tile, to verify the contribution of control embedding \n\n2. Table 2 results with high performance on compound recovery, may be results from the fact that the CP-CLIP is training on the meta data text, and the model can recover compound from the text context. Additional experiment to test performance change: image-only, text-only, and shuffled-context controls (time and dosage)."}, "questions": {"value": "1. Can you report  image-only, text-only baselines and counterfactual context (swap dose/time ) to test the performance change?\n\n2. Are agent outputs fully deterministic or do responses vary with randomness? How reproducible are the reasoning outputs across runs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CycDuwI1gj", "forum": "7BLnSeWuei", "replyto": "7BLnSeWuei", "signatures": ["ICLR.cc/2026/Conference/Submission16921/Reviewer_k4QM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16921/Reviewer_k4QM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16921/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762542205753, "cdate": 1762542205753, "tmdate": 1762926945522, "mdate": 1762926945522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}