{"id": "BjL4CSNJug", "number": 1806, "cdate": 1756937314084, "mdate": 1759898185721, "content": {"title": "UniCon: Unified Framework for Efficient Contrastive Alignment via Kernels", "abstract": "Contrastive objectives power state-of-the-art multimodal models, but their training remains slow, relying on long stochastic optimization.\nWe propose a Unified Framework for Efficient Contrastive Alignment via Kernels (UniCon), which spans linear and nonlinear encoders as well as one-to-one and many-to-many alignments.\nAt its core, UniCon introduces the contrastive similarity weight matrix $S(\\gamma)$, which enables closed-form global solutions that provably replace minibatch back-propagation with exact updates. Through the lens of reproducing kernel Hilbert spaces (RKHS), UniCon provides a kernelized perspective that unifies contrastive alignment and reveals its connection to spectral methods.\nTo validate the theory, we conduct experiments on synthetic, unimodal, multimodal, and zero-shot tasks, demonstrating that UniCon achieves substantial efficiency gains while preserving generality and strong empirical performance.", "tldr": "This paper introduces a unified and theoretically grounded perspective for contrastive alignment through RKHS, providing closed-form solutions via a contrastive similarity weight matrix.", "keywords": ["contrastive learning", "representation learning", "multimodal alignment"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3c20d758df24e57884957fd0d1dd05f84d993e0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes UniCon (Unified Framework for Efficient Contrastive Alignment via Kernels), a theoretically grounded and computationally efficient alternative to stochastic contrastive learning. Unlike traditional contrastive methods such as CLIP or SimCLR that perform pairwise optimization between positive and negative samples through minibatch-based SGD, UniCon introduces a global contrastive similarity weight matrix $S(\\gamma)$ that captures the pairwise interactions among all samples simultaneously.\n\n## Technical Approach\n\nUniCon formulates the contrastive objective in matrix form $C(\\gamma) = X S(\\gamma) Y^\\top$ (or its kernelized version $M = K_X^{1/2} S(\\gamma) K_Y^{1/2}$) and converts contrastive alignment into a spectral problem. Through rigorous derivations, the paper proves that minimizing the contrastive loss is equivalent to a rank-$r$ spectral approximation of $C(\\gamma)$ in either linear or reproducing-kernel Hilbert space (RKHS) settings, yielding a closed-form global solution via singular value decomposition (SVD)—replacing thousands of gradient steps with a single spectral update. The authors further extend this formulation to nonlinear and many-to-many alignments through kernelization, demonstrating theoretical unification across linear, nonlinear, unimodal, and multimodal regimes.\n\n## Empirical Results\n\nUniCon achieves substantial speedups (100–400× faster) across synthetic and real datasets while maintaining or surpassing the performance of traditional SGD-based contrastive methods.\n\n## Overall Impact\n\nUniCon reframes contrastive learning as global spectral alignment rather than local pairwise optimization, providing both a unified theoretical interpretation and a dramatically accelerated training paradigm."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "## Conceptual Novelty and Breakthrough\n\nThis paper presents a genuinely novel and impactful perspective on contrastive learning by reformulating it from a local, pairwise optimization problem into a global spectral alignment framework. The introduction of the contrastive similarity weight matrix $S(\\gamma)$ and its associated closed-form spectral solution represents a conceptual breakthrough—showing that contrastive objectives, long thought to require large-batch SGD, can instead be solved analytically through singular value decomposition.\n\n## Theoretical Rigor and Practical Impact\n\nThe problem addressed is fundamental and timely, given the enormous training cost and inefficiency of existing multimodal models like CLIP. By providing a mathematically rigorous unification of linear and nonlinear (kernelized) encoders, UniCon bridges theoretical understanding and practical acceleration, delivering both interpretability and efficiency within one framework.\n\n## Empirical Validation\n\nEmpirically, the results are striking: UniCon achieves the same or better alignment performance while being up to hundreds of times faster than SGD-based CLIP training, across both synthetic and real-world multimodal datasets. The experiments are carefully designed to validate each theoretical claim—showing consistent convergence of $S(\\gamma)$, clear geometric interpretability, and strong zero-shot transfer.\n\n## Overall Assessment\n\nThis paper stands out for its original idea, theoretical depth, and practical relevance. It redefines how we think about contrastive learning optimization, offering a theoretically elegant and computationally transformative solution."}, "weaknesses": {"value": "The paper lacks validation on large-scale datasets (such as LAION-400M or Conceptual-12M) and complex modalities including multilingual and video data. This limits our understanding of whether UniCon's spectral alignment approach scales effectively to real-world production settings and remains efficient when dealing with high-dimensional, heterogeneous modal representations.\n\nWhile the theoretical formulation of $C(\\gamma) = X S(\\gamma) Y^\\top$ is elegant, the paper would benefit from a clearer algorithmic description or pseudocode detailing how $C(\\gamma)$ (or its kernelized version) is computed, aggregated, and updated in practice. This would improve reproducibility and help practitioners understand the computational flow beyond the mathematical abstraction."}, "questions": {"value": "## 1. Optimality Under End-to-End Training\n\nFor end-to-end trainable encoders (non-frozen backbones), does the optimality of the spectral closed-form solution still hold? If the encoder weights evolve during training, would $S(\\gamma)$ need to be recomputed and re-factorized frequently to maintain optimal alignment?\n\n## 2. Low-Frequency Update Motivation\n\nWhy do the authors choose to update the global $C(\\gamma)$ or $S(\\gamma)$ at a low frequency? Is this design mainly motivated by computational efficiency, numerical stability, or to preserve the consistency of the learned spectral subspace as the encoder outputs evolve?\n\n## 3. Distributional Shift Robustness\n\nWhen the encoder undergoes significant distributional shifts (e.g., during fine-tuning or domain adaptation), could the low-frequency update scheme cause a mismatch between $S(\\gamma)$ and the current feature distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5eTmxvKY0v", "forum": "BjL4CSNJug", "replyto": "BjL4CSNJug", "signatures": ["ICLR.cc/2026/Conference/Submission1806/Reviewer_tNkS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1806/Reviewer_tNkS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761622920652, "cdate": 1761622920652, "tmdate": 1762915895652, "mdate": 1762915895652, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the slow training problem of the state-of-the-art multimodal models, the paper proposes a Unified Framework for Efficient Contrastive Alignment via Kernels (UniCon). Specifically, it uses the contrastive similarity weight matrix to find the closed-form global solutions, instead of minibatch backpropagation."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. UniCon provides a kernel-based perspective analysis showing the connection between contrastive loss and the spectral update.\n\n2. UniCon shows faster convergence than the CLIP-SGD."}, "weaknesses": {"value": "1. In Lemma 4, it states $\\beta=\\beta(\\theta_1,\\theta_2)$. What is $\\beta$ here? It is not explained, and $\\beta$ does not appear in the equation. Please clarify (likely $\\gamma$).\n\n2. Typo (line 729): “Equation equation 2” → “Equation 2”.\n\n3. In line 134 you write “scaling factor $\\nu \\ge 1$,” while line 148 says “$\\nu > 0$.” For rigor, please use a consistent and correct domain for $\\nu$.\n\n4. $\\gamma$ depends on $\\phi'$ and $\\psi'$ (Def.~3), but choices like the triplet loss that is mentioned multiple times are non-smooth. \n\n\n5. The statement “UniCon is the first implementation that directly leverages the contrastive similarity weight $S(\\gamma)$ for contrastive alignment” is too strong. For example, [1] leverages similarity weights via optimal transport, and [2] uses kernel-based alignment for multimodal contrastive learning.\n\n\n6. Figure 2 is also not clear to me. The two T-SNE figures for SGD-CLIP and UniCon look identical to me. What benefit beyond training time is demonstrated? Please clarify or add quantitative metrics.\n\n7. Since efficiency is a claimed contribution, a convergence analysis (or at least a theoretical discussion) is needed to justify why the method should be more efficient.\n\n8. Experiments are limited to small projection heads (e.g., two-layer MLPs). Real multimodal setups are larger. It is unclear how the method scales to larger/complex heads, partial/full CLIP fine-tuning, or training from scratch. Some baselines appear underpowered; e.g., [3] shows that with appropriate adapters/architectures, SGD-CLIP can perform well (including zero-shot). Given the abstract’s opening claim about slow training of state-of-the-art models, it would be fair to evaluate efficiency at that scale; current comparisons are not yet convincing.\n\n\n[1]. Shi L, Fan J, Yan J. Ot-clip: Understanding and generalizing clip via optimal transport[C]//Forty-first International Conference on Machine Learning. 2024.\n\n[2]. Gong S, Jiang Y, Dou Q, et al. Kernel-based unsupervised embedding alignment for enhanced visual representation in vision-language models[J]. ICML, 2025.\n\n[3]. Gao P, Geng S, Zhang R, et al. Clip-adapter: Better vision-language models with feature adapters[J]. International Journal of Computer Vision, 2024, 132(2): 581-595."}, "questions": {"value": "1. The paper claims faster convergence than the CLIP objective, but no convergence curves are shown. Please add plots comparison (e.g., epoch/time vs. validation accuracy) and report CLIP results with other optimizers (SGD/Adam/AdamW). How were hyperparameters (optimizer, learning rate, etc.) tuned for CLIP and for your method? Is the comparison fair?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tcjI2d5yjk", "forum": "BjL4CSNJug", "replyto": "BjL4CSNJug", "signatures": ["ICLR.cc/2026/Conference/Submission1806/Reviewer_WMSJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1806/Reviewer_WMSJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816387781, "cdate": 1761816387781, "tmdate": 1762915895424, "mdate": 1762915895424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UniCon, a kernel-based framework for efficient contrastive alignment that yields closed-form spectral updates instead of gradient-based training. The method introduces a contrastive similarity weight matrix and shows that minimizing a broad family of contrastive losses is equivalent to maximizing a trace objective whose optimizer is characterized by a spectral decomposition, both for linear encoders and for nonlinear encoders via RKHS kernels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Motivation and scope are clear.\n(2) Unified kernelized theory and shows gradient equivalence from general contrastive losses to a trace objective, extends linear SVD view to RKHS with an explicit optimizer.\n(3) The method achieves outstanding performance with synthetic-linear scaling and 100% matching in 0.02 s (compared to 0.32 s for SGD), reaching similar accuracy in ~2× less time and with only ~2 epochs on the CIFAR-10 dataset."}, "weaknesses": {"value": "(1) The kernel selection and the corresponding ablation are not explained in detail. The paper recommends an angular kernel and notes it outperforms RBF when embeddings are on the hypersphere but quantitative kernel ablations are not shown beyond a qualitative statement.\n(2) Limited experimental scope. All experiments use frozen or very shallow trainable encoders and comparison baselines are limited to vanilla SGD-CLIP without comparing against other efficient contrastive learning methods.\n(3) Theorem 9 contains a typo in the title (\"characterization\") and I think the author may optimize the notation in the article."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "vqGTuuG7lP", "forum": "BjL4CSNJug", "replyto": "BjL4CSNJug", "signatures": ["ICLR.cc/2026/Conference/Submission1806/Reviewer_5pMu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1806/Reviewer_5pMu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917041678, "cdate": 1761917041678, "tmdate": 1762915895202, "mdate": 1762915895202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UniCon, a novel and theoretically grounded framework that reformulates contrastive learning as a spectral decomposition problem. The core contribution is the introduction of a contrastive similarity weight matrix, which allows for the replacement of iterative, gradient-based optimization with a closed-form spectral update. The framework unifies linear and nonlinear (via RKHS kernelization) encoders and extends from one-to-one to many-to-many alignment settings. The empirical results across synthetic, unimodal, and multimodal tasks are compelling, demonstrating performance competitive with or superior to SGD-based baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Theoretical Unification and Novelty\n- Comprehensive Empirical Validation\n- The paper is generally well-structured, with a clear road-map of the theoretical contributions."}, "weaknesses": {"value": "- Breadth of Baseline Comparison: The paper mainly compares to SGD-CLIP, without other recent strong baselines.\n- The paper provides a novel kernel implementation, but provides limited ablation or intuition as to why. \n- The introduction provides limited hints about the shortcomings of previous works and why the current study is necessary. \n- The introduction claims that the linear and nonlinear settings are unified. However, this claim is questionable since the linear and nonlinear methodologies are almost always stated separately. \n- The superiority of the nonlinear setting compared to the linear one needs to be stated clearly, e.g., by providing an intuitive example.\n- The Experiment section lacks the direct comparison of performances between linear and nonlinear methods, either by Figures or quantitative results (Table 1 and 2).\n- The visualization difference between UniCon and SGD-CLIP (Figure 2 (mid vs right), Figure 4 (b) vs (c)). Consider adjusting the visualization to make the difference more evident.\n- The Discussion section simply provides some observations and conclusions, which makes it difficult to differentiate from the Conclusion. Is it possible to add some in-depth analysis that supports these observations?"}, "questions": {"value": "- Add more baselines other than SGD-CLIP.\n- Provide more comparative results between linear and nonlinear; between UniCon and SGD-CLIP.\n- Add more in-depth analysis.\n- Add a detailed example to illustrate why the nonlinear setting is better than linear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "RWHbSDS6E6", "forum": "BjL4CSNJug", "replyto": "BjL4CSNJug", "signatures": ["ICLR.cc/2026/Conference/Submission1806/Reviewer_GRPF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1806/Reviewer_GRPF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1806/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990083601, "cdate": 1761990083601, "tmdate": 1762915895012, "mdate": 1762915895012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}