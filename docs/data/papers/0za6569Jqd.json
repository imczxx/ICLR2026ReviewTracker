{"id": "0za6569Jqd", "number": 5724, "cdate": 1757929405703, "mdate": 1759897958235, "content": {"title": "Representation Finetuning for Continual Learning", "abstract": "The world is inherently dynamic, and continual learning aims to enable models to adapt to ever-evolving data streams. Pre-trained models has shown powerful performance in continual learning. However, since pre-trained models acquire knowledge from static datasets, they still require finetuning to adapt effectively to downstream tasks. Traditional finetuning methods are largely empirical, lack explicit objectives, and still require a relatively large number of parameters. In this work, we introduce $\\textbf{Co}$ntinual $\\textbf{R}$epresentation L$\\textbf{e}$arning($\\textbf{CoRe}$), a novel framework that, for the first time, applies low-rank linear subspace representation finetuning to continual learning. Unlike conventional finetuning approaches, CoRe adopts a learning paradigm with explicit objectives rather than relying on black-box optimization, achieving more efficient parameter utilization and superior performance. Extensive experiments across multiple continual learning benchmarks demonstrate that CoRe not only preserves parameter efficiency but also significantly outperforms existing methods. Our work extends the applicability of representation finetuning and introduces a new, efficient finetuning paradigm for continual learning.", "tldr": "", "keywords": ["continual learning", "reft", "finetuning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/41b12b47eed47b8e4e3ac3813452c1b914ffbed7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a method for continual learning by leveraging low-rank adaptation techniques. The authors adapt existing representation finetuning methods to the continual learning scenario and compare their approach with other state-of-the-art methods. The experimental results demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper addresses the finetuning techniques in continual learning with an emphasis on low rank adaptation. The paper adopted the representation finetuning methods into continual learning scenario and provided comparisons with existing methods to validate its effectiveness."}, "weaknesses": {"value": "1. The method is applying an existing representation finetuning method to the continual learning scenario, the contribution is too incremental.\n2. There is no clear motivation for applying the specific finetuning method."}, "questions": {"value": "1. What is the objective of using 'Avg' as the metric?\n2. Actually, I suggest the authors to dig deeper into the functionality of the proposed method, e.g. how modules of different tasks can cooperate with each other to boost the performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G0UlswUYxJ", "forum": "0za6569Jqd", "replyto": "0za6569Jqd", "signatures": ["ICLR.cc/2026/Conference/Submission5724/Reviewer_c1wY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5724/Reviewer_c1wY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398124553, "cdate": 1761398124553, "tmdate": 1762918219285, "mdate": 1762918219285, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript investigated the problem of continual learning with pre-trained models (CL-PTM). The authors introduced Continual Representation Learning (CoRe) that applied low-rank linear subspace representation finetuning for CL-PTM. Specifically, the proposed CoRe adopted the explicit objectives with the tool of counterfactual intervention to achieve more efficient parameter utilization. Experiments with several datasets were conducted to demonstrate the performance of the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This manuscript considered the perspective of the counterfactual intervention, which is interesting and less explored in the context of CL-PTM."}, "weaknesses": {"value": "1. The motivation of the proposed method is not clear. I didn't get why the descriptions in Sections 3.2 & 3.3 were introduced.\n2. Some basic concepts of continual learning in Section 3.1 were not accurate, or even wrong.\n3. The presentation of this paper was poor. The context of Domain Intervention Interpretation (DII, shown in Section 3.2), which should be the theoretical foundation of this paper, was not well interpreted. The concept was not clear, and the relevant existing studies was not discussed in the Related Work part.\n4. The experimental part was incomplete, including the comparisons with baseline methods, ablation studies, etc.\n\nSee the Questions part for more details."}, "questions": {"value": "1. I think some hypotheses/concepts regarding the continual learning in Section 3.1 were not accurate, or even wrong. In TIL, we only have assumptions regarding the distinct label spaces Y, without any strong assumptions regarding the label distribution P(Y). In DIL, it seems that we do not need to constrain the same label distribution. As for CIL, we usually do not need to assume different label distributions for P(Y), and the subset assumption of label space Y is also not accurate. I recommend that the authors make a clear description of the basic concepts in continual learning.\n2. Sections 3.2 & 3.3 were badly presented. The authors should introduce more details about the proposed method, starting with the motivation for why the authors proposed the descriptions in Eqs (1)-(4).\n3. The background of the foundation of Sections 3.2 and 3.3 should be discussed in the Related Work part.\n4. The illustration in Figure 1 is less informative. Could the authors provide more details about the methodology?\n5. The comparisons with the baseline methods in this manuscript were insufficient. Few representative studies within CL-PTM were discussed. Please refer to the following representative methods as baselines, including prompt-based methods (L2P, DualPrompt), adapter-based (O-LoRA, InfLoRA, BiLoRA). Please refer to [1].\n6. Some claims made by the authors were not verified. For example, the authors claimed that the proposed method achieved parameter efficiency. However, I didn't see any empirical results regarding this claim in the experimental part.\n\n\nReferences;\n[1] Continual Learning with Pre-Trained Models: A Survey. IJCAI 2024"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zzWAATgI0H", "forum": "0za6569Jqd", "replyto": "0za6569Jqd", "signatures": ["ICLR.cc/2026/Conference/Submission5724/Reviewer_A7of"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5724/Reviewer_A7of"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761529376426, "cdate": 1761529376426, "tmdate": 1762918218909, "mdate": 1762918218909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript introduces representation finetuning into continual learning methods based on pre-trained models, enabling task-specific interventions within the low-rank subspace of hidden representations. The authors conduct experimental validation across several datasets and evaluate performance under three continual learning scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1)\tThe manuscript introduces representation finetuning methods into continual learning framework. The content is clearly presented and well structured, enhancing the clarity and readability of this manuscript.\n(2)\tExperiments are conducted across several public datasets, analyzing the effects of subspace rank and data imbalance under different scenarios to evaluate the effectiveness of the proposed method."}, "weaknesses": {"value": "(1)\tThe analysis of existing pre-trained continual learning methods is insufficient. The authors introduce three typical continual learning methods in the related work. It is suggested to include a comparative analysis of the advantages and limitations of existing methods based on pre-trained models, providing a clearer perspective on pre-training-based continual learning methods. \n(2)\tThe experimental comparisons do not include the latest methods. In Section 4, the authors compare three parameter-efficient finetuning techniques—Adapter, Prompt, and SSF—all developed before 2023. As a proposed continual learning algorithm, it is suggested to include comparisons with recent state-of-the-art methods based on pre-trained models to validate the effectiveness of the proposed approach.\n(3)\tThe experimental evaluation metrics are inadequate. In Section 4.1, the authors use Average accuracy and Last accuracy as evaluation metrics for the proposed algorithm. Given that catastrophic forgetting is a key challenge in continual learning, it would be beneficial to incorporate the model's forgetting rate as an evaluation to enhance the comprehensiveness of the experimental results.\n(4)\tThe authors introduce representation finetuning techniques into continual learning. However, it remains unclear how this approach differs from prompt-based continual learning methods and what specific technical challenges arise in its implementation. It is suggested that the authors provide further clarification in the introduction section.\n(5)\tThe authors formulate a continual learning approach based on representation finetuning in Section 3.3, and how this approach was implemented experimentally is unclear. It is suggested to include a more detailed descriptions of the algorithm’s implementation in the experimental section.\n(6)\tSome sentences contain redundant expressions—for example, the description of the pre-training dataset in line 275. It is suggested to review the manuscript's language carefully to enhance the overall coherence."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DSeCAxs4CG", "forum": "0za6569Jqd", "replyto": "0za6569Jqd", "signatures": ["ICLR.cc/2026/Conference/Submission5724/Reviewer_bjgc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5724/Reviewer_bjgc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727050782, "cdate": 1761727050782, "tmdate": 1762918218400, "mdate": 1762918218400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CoRe (Continual Representation Learning), a novel framework that applies low-rank linear subspace representation finetuning (ReFT) to continual learning (CL). The authors argue that traditional finetuning methods are empirical, lack explicit objectives, and are parameter-inefficient. CoRe intervenes directly on hidden representations within a low-rank subspace, enabling task-specific adaptation while mitigating catastrophic forgetting. Extensive experiments across task-incremental (TIL), domain-incremental (DIL), and class-incremental (CIL) learning benchmarks demonstrate that CoRe achieves state-of-the-art performance with superior parameter efficiency compared to existing PEFT methods like Adapter, VPT, and SSF."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ The paper is well-written, clearly structured, and easy to follow. Figures and tables are informative and support the narrative effectively. The method is explained with sufficient detail, and the experimental setup is thoroughly described to ensure reproducibility."}, "weaknesses": {"value": "+ No state-of-the-art CL methods are compared, such as L2P, Dulaprompt, Codaprompt, Inflora, sdlora, hideprompt, etc.\n+ CIFAR is already included in the ViT pre-trained data, it is better to use dataset like imagenet-r for experiments sec. 4.3-sec. 4.4."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "acLmNlsb9U", "forum": "0za6569Jqd", "replyto": "0za6569Jqd", "signatures": ["ICLR.cc/2026/Conference/Submission5724/Reviewer_o5rm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5724/Reviewer_o5rm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5724/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985689402, "cdate": 1761985689402, "tmdate": 1762918217782, "mdate": 1762918217782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}