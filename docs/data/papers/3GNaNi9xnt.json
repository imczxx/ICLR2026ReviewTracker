{"id": "3GNaNi9xnt", "number": 10657, "cdate": 1758178798822, "mdate": 1759897637391, "content": {"title": "MIRRORMARK: A Distortion-Free Multi-Bit Watermark for Large Language Models", "abstract": "As large language models (LLMs) become increasingly integral to broad applications such as question answering and content creation, reliable content attribution and accountability have grown increasingly urgent. Watermarking offers a promising approach to identifying AI-generated text. However, existing approaches either provide only a binary provenance signal or perturb the sampling distribution, degrading the text quality; approches that preserve text quality, in turn, often exhibit weak detectability and little robustness. We propose MirrorMark, a multi-bit and distortion-free watermark for LLMs. By mirroring the sampling randomness in a measure-preserving way, MirrorMark embeds multi-bit messages without altering the token probability distribution during generation, and thus text quality is maintained by design. For robustness, we employ a content-based scheduler that partitions the messages into per-position symbols and allocates tokens to each symbol nearly uniformly, allowing limited tokens to carry more symbols while keeping assignments stable under insertions and deletions. We also present a theoretical analysis that models detection error versus the number of pseudorandom draws per generation step, offering interpretability to our empirical results and insights on the design of high-detectability multi-bit watermarks. In our comparisons with state-of-the-art multi-bit baselines, MirrorMark preserves the text quality comparable with non-watermarked text while delivering superior detectability: with 54 bits embedded in 300 tokens, it improves bit accuracy by 8–12\\% and correctly identifies up to 11\\% more watermarked texts when the false positive rate is fixed at 1\\%. These results show that MirrorMark enables practical attribution, offering a scalable path to provenance and accountability in LLM deployment.", "tldr": "We propose a multi-bit and distortion-free watermark for large language models.", "keywords": ["LLM Watermark", "bias", "LLM Security"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5be5a2cc3ca602e38f8d51075d26d98cf17ad401.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MirrorMark, a multi-bit watermarking framework for LLMs that preserves the original token probability distribution through a mod-1 mirroring transformation. The method extends existing distortion-free zero-bit watermarking schemes (Gumbel sampling and tournament sampling) to embed multi-bit messages by reflecting pseudorandom values about message-dependent mirror points. To enhance robustness against token insertions and deletions, the authors introduce a Content-Anchored Balanced Scheduler that distributes tokens across message positions in a balanced manner. The paper provides theoretical analysis relating detection error rates to the number of PRF draws and demonstrates strong detection performance while maintaining text quality comparable to non-watermarked outputs."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides rigorous mathematical guarantees that the mirroring transformation Ψ(u; ψ_M) = (2ψ_M - u) mod 1 is measure-preserving, ensuring that the watermarked text follows the same probability distribution as the original LLM output.\n\n- The experimental results demonstrate that MirrorMark achieves superior detectability compared to existing multi-bit baselines. \n\n- The CABS algorithm ensures sufficient tokens are assigned to each message position for reliable decoding."}, "weaknesses": {"value": "- The core distinguishing feature of watermarking is robustness against modifications, whereas steganography prioritizes undetectability. MirrorMark's preserving the exact probability distribution through measure-preserving transformations aligns more naturally with steganographic goals. \n\n- While the paper claims robustness through statistical accumulation and CABS redundancy, the actual resilience is severely limited. The robustness demonstrated against copy-paste attacks is somewhat trivial, this attack only dilutes the watermark signal but does not actively corrupt the token-to-position mapping that MirrorMark critically depends on. \n\n- For paraphrasing attacks, bit accuracy drops to 54%, rendering the multi-bit message unrecoverable. While AUC remains relatively high, this only indicates binary watermark detection, not multi-bit decoding.\n\n- Although CABS uses content-based anchors to define frames, even minor edits can desynchronize the token-to-position mapping. The paper does not provide quantitative evaluation of robustness under realistic editing scenarios (e.g., content insertion/deletion).\n\n- Both embedding and extraction rely on perfectly synchronized CABS scheduling. Any modification to the text breaks this synchronization. This creates a critical single point of failure: the multi-bit message is irrecoverably lost if the attacker rewrites even a few sentences, because the detector can no longer determine which tokens carry which message symbols. \n\n- The paper suffers from excessive mathematical formalism that obscures rather than illuminates the core contributions. The authors should prioritize clarity and intuition over mathematical sophistication to make the work accessible to the broader community working on LLM watermarking."}, "questions": {"value": "- Could you provide quantitative evaluation under more realistic editing scenarios?\n\n- Have you explored a \"soft\" version of MirrorMark that applies partial mirroring (e.g., weighted combination of original and mirrored u values) to achieve a middle ground?\n\n- Could you provide a use case where the recovered multi-bit message (with 54% accuracy) is still useful?\n\n- The paper does not include ablation studies on these hyperparameters. Do they require careful tuning for different text lengths or domains? \n\n- What is the actual wall-clock time for encoding and decoding compared to non-watermarked generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "STK0bqqdzt", "forum": "3GNaNi9xnt", "replyto": "3GNaNi9xnt", "signatures": ["ICLR.cc/2026/Conference/Submission10657/Reviewer_9XrG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10657/Reviewer_9XrG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761051655644, "cdate": 1761051655644, "tmdate": 1762921910956, "mdate": 1762921910956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Dear Area Chair,\n\nThis paper does not use the ICLR official template and should be desk reject. The margin is significant wider than the offical template.\n\nBest regards,\nReviewer"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "NA"}, "weaknesses": {"value": "NA"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LgB9CghY2K", "forum": "3GNaNi9xnt", "replyto": "3GNaNi9xnt", "signatures": ["ICLR.cc/2026/Conference/Submission10657/Reviewer_9mcw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10657/Reviewer_9mcw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761374017401, "cdate": 1761374017401, "tmdate": 1762921910570, "mdate": 1762921910570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a multi-bit distortion-free watermarking scheme for LLMs. Specifically, it introduces two components: mod-1 mirroring and CABS to expand existing 1-bit watermarks (AAR and SynthID) to the multi-bit case.\n\nThey provide a theoretical explanation of how equal error rates scale with the number of pseudo-random draws for both schemes.\n\nLastly, they evaluate the detectability and bit accuracy trade-off with the quality of their schemes and several baselines and show robustness experiments against copy-paste attacks and (non-adversarial) paraphrasing."}, "soundness": {"value": 4}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Propose a generic mod-1 mirroring function that allows associating positions with uniform random variables. We show how this concept enables multi-bit watermarking for two popular schemes: AAR Watermark and SynthID-Text.\n- Introduce CABS to sample positions based on context, which, unlike prior work, ensures that all positions are sampled while maintaining robustness via a framing mechanism.\n- Provide theoretical bounds on the equal-error rate with respect to the number of PRF draws for both studied schemes.\n- Detectability/robustness evaluation compares Mirrormark with several prior and up-to-date baselines and shows that the proposed method is more effective on most aspects (detectability and the bit-accuracy–versus–quality trade-off, and robustness against copy-paste attacks)."}, "weaknesses": {"value": "- The presentation of the method (Section 3) is unclear. Connecting the different components (mod-1 mirroring, CABS, and the decoding and detection) requires a lot of back-and-forth to grasp how the method operates, as well as significant knowledge of prior works on multi-bit watermarks. I think the paper could benefit from a high-level explanation of the different components first (mod-1 mirroring encodes the position, CABS determines the position, and decoding decodes the position and computes the score), paired with a visual explanation of the method (see, for instance, Fig. 2 of [1]). Also, beware that some sentences are poorly phrased, and there are some typos (for instance, in l603 the beginning of the sentence is missing). Lastly, using $300$ tokens for Section 5.1 and $400$ for Section 5.2 is a bit inconsistent and makes comparison harder; I think it would be better if both used the same length.\n- This paper has two key contributions: mod-1 mirroring and CABS. It would be good if the evaluation disentangled the two. For instance, using mod-1 mirroring with a prior work’s position sampler (for instance, MPAC [1]), and also using prior schemes (for instance, Red-Green schemes) with CABS. Thus, we could clearly see which component leads to an improvement (is it mod-1 mirroring, which allows the use of SynthID/AAR schemes; is it the position sampler; or is it both?).\n- With the proposed scheme, the decision threshold is no longer supported by statistical testing (after the argmax operation, the distributions under the null hypothesis are not known); hence the authors suggest learning a threshold. Therefore, the statistical guarantees of the watermark with respect to FPR are not satisfied, and the paper presents no experiments showing that, in practice, with a learned threshold, the FPR is still properly controlled.\n- The quality evaluation of the watermark is insufficient. As acknowledged by the authors, a known issue with distortion-free watermarks is diversity (l121). For instance, AAR is known to lead to repetitive sentences, yet perplexity tends to be lower with repetitive sentences (this could explain why the perplexity of Gumbel-max is lower than in the non-watermark case). Therefore, in the quality evaluation, it would be valuable to include additional metrics to measure repetitiveness (n-gram repetitions) or an LLM-as-a-judge score. On a minor note, the authors did not submit the code with their submission, which prevents independent human evaluation by reviewers of the watermark quality.\n- The motivations for multi-bit watermarks are somewhat lacking. Also, the authors claim that single-bit watermarks cannot be used for multi-bit watermarking but later explain that in [2] they extend a single-bit watermark to the multi-bit case by associating one message per key. The evaluation could benefit from comparing MirrorMark on AAR/SynthID with the key-swapping approach as a baseline using the same underlying schemes.\n- A key motivation behind the CABS design is robustness against token deletion, insertion, or substitution that could desynchronize the position signal. Yet the authors do not evaluate their watermark’s robustness against such attacks.\n- The authors claim in the abstract that Theorem 4.1 provides interpretability for the empirical results and insights into the design of high-detectability multi-bit watermarks. In its current state, I think it does neither. There are no experiments showing that the bounds of Theorem 4.1 are achieved or verified (I think Theorem 4.1 is never referenced in the evaluation section), and no justification for how the results provide insights into watermark design. On the contrary, from Theorem 4.1 it appears that the AAR watermark is better than SynthID, yet in the experimental results this is not the case.\n- CABS has many parameters whose impact is not evaluated (i.e., the max length, the window size $Q$, and the $f$). Also, it is unclear which parameters are used for the evaluation.\n\n[1] Advancing beyond identification: Multi-bit watermark for large language models, Yoo et al.  \n[2] Three bricks to consolidate watermarks for large language models, Fernandez et al."}, "questions": {"value": "- Could the authors evaluate mod-1 mirroring and CBAS independently? For instance, compare MirrorMark to mod-1 mirroring + MPAC on SynthID/AAR (evaluating detectability and robustness). Similarly, compare CBAS with MPAC on the coloring scheme from [2]. The goal is to understand which component explains the improvement of MirrorMark.\n- Could the authors show how the TPR/FPR curves behave for unwatermarked text when the threshold is learned on one domain (for instance, English text) but then used on another domain (for instance, Chinese text)? Could this lead to an increase in false positives? If so, how should one learn the threshold in practice?\n- Could the authors add additional quality metrics (LLM-as-a-judge and repetitiveness scores)? Also, I think it would be beneficial to see how the watermark performs on instruction tasks (instead of completions), which are a more realistic use case for LLMs. For instance, using the evaluation pipeline from [3].\n- Could you evaluate the robustness of the watermark to token deletion, insertion, and substitution? (see [4])\n- Could you ablate the components of CBAS and assess their impact on robustness and detectability? In particular, max_len, $W$, and $f$.\n- Could the authors compare their approach to the naive baseline from [2], in which one key equals one message? In particular, is the naive approach better for small message lengths?\n- How does Theorem 4.1 explain the observed results? How can it be used to guide watermark design?\n- What is the role of the null symbol in mod-1 mirroring?\n- In the paraphrasing robustness experiment, could you show the TPR as well?\n- Can you show a single-bit watermark baseline from AAR/SynthID? It would be interesting to see whether using a multi-bit watermark leads to a smaller TPR (i.e., what is the cost of using a multi-bit watermark).\n\n[3] WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models, Tu et al.\\\n[4] MarkLLM: An Open-Source Toolkit for LLM Watermarking, Pan et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "bscXfq1khT", "forum": "3GNaNi9xnt", "replyto": "3GNaNi9xnt", "signatures": ["ICLR.cc/2026/Conference/Submission10657/Reviewer_GKSn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10657/Reviewer_GKSn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725986981, "cdate": 1761725986981, "tmdate": 1762921910027, "mdate": 1762921910027, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MirrorMark, a multi-bit and distortion-free watermark. Specifically, it first presents mod-1 mirroring process to extend existing watermarks to multi-bit. Second, to improve robustness against watermark attacks, the authors develop a content-anchored balanced scheduler (CABS). This allows fewer tokens to carry more symbols. Compared to existing multi-bit watermarks, MirrorMark achieves a better tradeoff between watermark strength and text quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. MirrorMark is easy to implement by extending existing one-bit watermarks. \n2. Compared to existing multi-bit watermarks, CABS is more robust. \n3. The authors also provide a theoretical analysis between the number of pseudo-random functions and error rate of watermark detection."}, "weaknesses": {"value": "1. Although I like the idea of the paper, the manuscript is difficult to follow due to its formatting (e.g., tables and figures) and lack of clarity.\n2. From Table 3, the experiments do not demonstrate consistent robustness in terms of bit accuracy. The authors should provide additional analysis or experiments to support their claims."}, "questions": {"value": "1. Could the authors provide an ablation study on CABS to evaluate the contribution of the module? \n2. Could the authors provide a sensitivity analysis on the number of bits $M$ in comparison to the benchmark methods? Since MirrorMark encodes $M$ bits using fewer tokens, it is possible that for sufficiently large $M$, the multi-bit watermark may fail in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aR98zw023n", "forum": "3GNaNi9xnt", "replyto": "3GNaNi9xnt", "signatures": ["ICLR.cc/2026/Conference/Submission10657/Reviewer_Auzr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10657/Reviewer_Auzr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10657/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761859509888, "cdate": 1761859509888, "tmdate": 1762921909543, "mdate": 1762921909543, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}