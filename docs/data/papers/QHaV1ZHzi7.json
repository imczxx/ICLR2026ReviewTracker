{"id": "QHaV1ZHzi7", "number": 13667, "cdate": 1758220635141, "mdate": 1763582452530, "content": {"title": "Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning", "abstract": "The capabilities of Large Vision-Language Models (LVLMs) have reached state-of-the-art on many visual reasoning tasks, including chart reasoning, yet they still falter on out-of-distribution (OOD) data, and degrade further when asked to produce their chain-of-thought (CoT) rationales, limiting explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs to be more robust and explainable for chart reasoning by coupling Group Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our framework comprises of three rewards that maximize: (i) correct chart-type classification, (ii) faithful chart table reconstruction, and (iii) process conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently outperforms standard supervised fine-tuning (SFT) on both in-distribution and out-of-distribution datasets, closing the OOD performance gap while improving rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve state-of-the-art results on six chart-reasoning benchmarks spanning in-domain and OOD settings, surpassing all existing models of comparable size. Beyond accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening trust and reliability - showcasing the power of verifiable rewards with GRPO for training reliable, interpretable chart-reasoning models.", "tldr": "RL based fine-tuning LVLMs using RVR for improved chart reasoning", "keywords": ["xai", "vlms", "rl", "grpo"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/262065bdb0be5f46fe99e262aac68203733725a1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Chart-RVR, a reinforcement learning framework built on GRPO with verifiable surrogate rewards for chart reasoning.  method combines three components: (1) chart-type prediction, (2) chart-table reconstruction, and (3) a process-conformity reward to enforce structured reasoning. Experiments on six benchmarks show modest accuracy gains over SFT baselines using 3B-parameter LVLMs like Qwen2.5VL. authors claim improved out-of-distribution generalization and more explainable CoT rationales."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The work addresses a recognized limitation of current chart reasoning models i.e. over-reliance on sft and lack of verifiable reasoning.\n- modular, verifiable reward components (chart-type, table reconstruction, format, etc.) form a clear and interpretable pipeline that could be useful for future chart-to-text RL research.\n- Multiple benchmarks (ChartQA, PlotQA, ChartFC, etc.) are used, providing some breadth of evaluation.\n- Despite dense math , paper is logically structured and  states goals, components, and datasets."}, "weaknesses": {"value": "- While the paper positions itself as a \"general RL framework for explainable chart reasoning\" the technical core is incremental. e.g. reward functions: format, accuracy, type, table and text similarity are largely deterministic existing heuristics, not new learning principles. GRPO has already been used in multiple prior multimodal reinforcement fine-tuning works. The contribution mainly lies in repackaging standard verifiable checks into a chart-specific recipe, without deeper theoretical or algorithmic advancement.\n- The empirical study lacks strong evidence that Chart-RVR truly improves generalization or explainability. May be its just me but the CoT explainability metric is bit unconventional and largely depends on a large external LVLM as oracle. some qualitative diversity analysis would have been nice\n- Also, worried about ptential biases:  dataset construction (Section 4.1 and appendix A1) relies heavily on Qwen2.5VL-72B-generated rationales as \"ground truth\" filtered with a few heuristics and minimal human verification. manual filtering step can be better quantified and is currently insufficiently detailed.\n- interpretability evaluation is minimal, \"explainable Info gain\" metric is neither standard nor clearly validated against human judgment.\n\n\nsummary: the motivation and modular reward framework are solid starting points, the idea is promising but the work needs deeper validation, clearer ablations, and stronger writing and presentation before reaching top-tier readiness, I believe."}, "questions": {"value": "- Do authors present ablation or sensitivity results for the numerous reward weights? I may have missed spotting.\n- minor: igures showing CoT examples are too small and contain color-coded text that is hard to read. Could you consider upgrading?\n- missing references of some relevant papers on visual reasoning and visual RL:\n[1] Masry et al. BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning, https://arxiv.org/abs/2508.09804\n[2] Rodriguez et al, BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks. https://arxiv.org/abs/2412.04626\n[3] Awal et al. WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation https://arxiv.org/abs/2508.16763."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7LPMk0l4WX", "forum": "QHaV1ZHzi7", "replyto": "QHaV1ZHzi7", "signatures": ["ICLR.cc/2026/Conference/Submission13667/Reviewer_sUe3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13667/Reviewer_sUe3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886711249, "cdate": 1761886711249, "tmdate": 1762924234129, "mdate": 1762924234129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Clarifications regarding core contributions and comparision to BigCharts-R1"}, "comment": {"value": "**Clarifications regarding core contributions**\n\nChart-RVR is a methodological recipe for fine-tuning small LVLMs with verifiable rewards. \n1. Our motivation is to provide a strong and generalizable fine-tuning methodology that utilizes commonly used ,well-benchmarked/validated datasets (e.g. ChartQA, ChartFC, PlotQA) for LVLM fine-tuning.\n2.  Our approach achieves benchmark results on both ID and OOD real-life datasets (average +2-3% on ID +5% on OOD). We have demonstrated this behavior by both - randomly sampling data from the training sets (Table-3a) or data complexity-aware sampling (Table-2: \"Hard\" suffix). \n3. It is architecture agnostic and validated on Qwen-2.5-VL, Gemma-3, and InternVL-3.5. \n4. Our approach produces more explainable reasoning traces validated bya  large Oracle model and a human study.\n\nUnlike other chart-reasoning works like [1], we do not curate a new large dataset; instead utilize existing datasets for fine-tuning. \n\n**Salient differences with respect to BigCharts-R1**\n\nWe sincerely thank Reviewers qu1R and Sue3 for bringing BigCharts-R1 [1] to our attention. BigCharts-R1 appeared Aug 13, 2025 (arXiv) and Aug 25, 2025 (COLM OpenReview), after the ICLR 2026 contemporary work cutoff (July 24) as stated here: https://iclr.cc/Conferences/2026/ReviewerGuide - \"That means, since our full paper deadline is September 24, if a paper was published (i.e., at a peer-reviewed venue) on or after July 24, 2025, authors are not required to compare their own work to that paper\".\n\nNevertheless, upon reading [1] in detail, we believe both works share the high-level goal of improving chart reasoning with RL, however, the concrete contributions and technical focus differ. We detail the differences below:\n\n1. *Divergence in contributions*: [1] proposes BigCharts dataset, which is a completely new dataset composed of training data from existing datasets - Figure QA, DVQA, PlotQA and ArxivQA, along with **chart data collected using Google Search and Common Crawl**. Our work on the other hand focuses on *proposing a methodology* to train (off-the-shelf) smaller LVLMs on existing data from ChartQA, ChartFC and PlotQA with only data annotation steps using a large Oracle model. (Reference Section 3.1.1 in [1])\n2. *Reward Design*: Unlike [1], our method not only utilizes standard GRPO rewards (accuracy, length, format as is reported in [1]) but also incorporates factuality specific rewards meticulously designed for chart reasoning - chart type, table reconstruction and process conformity. Particularly, process conformity helps in fine-tuning smaller LVLMs without a SFT phase.\n3. *OOD evaluation*: Finally, our work differs wrt [1] on significant OOD testing scenarios. [1] utilizes OOD benchmarks as - FigureQA-subset, DVQA-subset and PlotQA-subset (detailed in Section 5.4). All 3 of these datasets are **synthetic** in nature and do not capture complex and diverse real-life charts. In Chart-RVR, we have evaluated OOD data against 3 real life complex OOD datasets - EvoChart, ChartQAPro and ChartBench - making our OOD data evaluation analysis much more robust. We also report results on CharXiv below.\n4. Finally, Chart-RVR, provides analysis around the explainability of the reasoning traces using Oracle and human studies.\n\nWe view BigCharts-R1 as a complementary work - focusing on scaling data curation for large models, whereas Chart-RVR focuses on verifiable-reward RL recipes for small LVLMs and explainability. We have cited, compared and contrasted our work with BigCharts-R1 in the updated PDF.\n\n[1] BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning, COLM 2025"}}, "id": "dsVMzDA6B3", "forum": "QHaV1ZHzi7", "replyto": "QHaV1ZHzi7", "signatures": ["ICLR.cc/2026/Conference/Submission13667/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13667/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13667/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763581215930, "cdate": 1763581215930, "tmdate": 1763581215930, "mdate": 1763581215930, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Chart‑RVR, a reinforcement‑learning fine‑tuning framework for large vision‑language models (LVLMs) for chart-reasoning. In this work, they explore GRPO with\nA set of verifiable rewards: (surrogate tasks): chart-type classification, underlying chart-table reconstruction and process-conformity reward that measures step-by-step reasoning aligned with ground truth rationales. In this work the authors fine-tune and evaluate 3B (Qwen 2.5VL) models across six chart QA test datasets that constsis of both in-domain and OOD. The overall results show modest gains on in-domain (1 - 2%) and larger gain (3.5 - 7% on OOD test sets). They show that the proposed framework generalizes to multiple LVLM architectures ( Gemma3 and InternVL3.5-4B )."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. GRPO with multiple verifiable surrogate tasks and process conformity reward.\n2. Ablation on how the surrogate tasks help with over all performance.\n3. Benchmarking on multiple datasets both in and out of distribution and showing that the proposed framework improves well on OOD.\n4. Demonstrating this framework is generalizable across various architectures.\n5. Human study that shows humans prefer chart-rvr reasoning over others (on a very small sample though)"}, "weaknesses": {"value": "1. All the models studied here are around 3B. Would it generalise to bigger models? Would SFT alone be sufficient for bigger models (if there is a way of getting larger, cleaner data). See BigCharts-R1 paper on how to get such data. \n\n2. A missing citation to very relevant work BigCharts-R1 who effectively propose an identical framework. Instead of using surrogate tasks in RL they use that to generate/synthesise large finet-tuning data and complement with RL fine-tuning on top of it.\n\n3. Missing numbers on CharXiv dataset that specifically tests the questions that truly require reasoning. \n\nBIGCHARTS-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning: https://arxiv.org/pdf/2508.09804"}, "questions": {"value": "1. Since ChartQA (not really sure if you have used same PlotQA subset or all of it) is the only common dataset between BigCharts-R1 and this work, looks like SFT on high quality data does as well as your RL framework. Any explanation on this? \n\n2. Does higher process‑conformity scores correlate with higher answer accuracy? Curious if this is the case and if there are any patterns observed otherwise?\n\n\n3. Would like to see a large human study on data sampled from multiple datasets or different level of visual/complex reasoning and if chart-rvr would still be preferred over other models wich larger data sample?\n\n4. How did you choose the length thresholds for length reward?\n\n5. For the table reconstruction (looks like model would completely penalize if the values predicted in cell are not exact but close enough). Could be a real issue if you have data where model needs to estimate from the visuals or infographics of the plot. If not a lot of such data is present it is a issue of the model to generalize to such data.\n\n6. Would be good to see that this would generalize across different model sizes (just like different model families). Perhaps smaller models if there is constraints on working on larger models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T26adXYYlq", "forum": "QHaV1ZHzi7", "replyto": "QHaV1ZHzi7", "signatures": ["ICLR.cc/2026/Conference/Submission13667/Reviewer_qu1R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13667/Reviewer_qu1R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942976458, "cdate": 1761942976458, "tmdate": 1762924233789, "mdate": 1762924233789, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Chart-RVR, a reinforcement learning framework for improving the robustness and interpretability of chart reasoning in LVLMs. Chart-RVR aims to address the OOD generalization and unreliable CoT reasoning issues in LVLMs. It combines GRPO with automatically verifiable rewards, introducing three reward types to ensures models identify the chart type, measures how accurately the model reconstructs the underlying data table, and enforces stylistic and structural consistency in reasoning steps. Extensive experiments across six chart benchmarks show that Chart-RVR-trained models outperform SFT and domain-specific baselines on OOD datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper introduces a verifiable reward structure integrated into GRPO, enabling stable and interpretable reinforcement fine-tuning for LVLMs. Unlike SFT, Chart-RVR directly optimizes verifiable task outcomes to improve robustness and reasoning interpretability.\n\nChart-RVR achieves outperforming accuracy across six benchmarks, with the significant accuracy gains under OOD settings. The Process-Conformity Reward effectively enforces reasoning alignment with ground truth, yielding more coherent traces. The proposed explainable information gain further demonstrates that Chart-RVR rationales increase model confidence and interpretability on harder datasets."}, "weaknesses": {"value": "A more detailed discussion about the interaction dynamics between multiple verifiable rewards (e.g., balancing \\lambda_1 and \\lambda_2 in Eq. 6) are recommended. The training data is generated using Qwen2.5VL-72B. This raises concerns about data bias, as the quality are not verified by human at scale. What will the accuracy change if the data is constructed using different LVLMs?\n\nAll benchmarks are chart-based. I wonder if the proposed approach can be applied to non-chart-based reasoning tasks. There is no experiments assess whether the framework generalizes to other structured visual reasoning tasks. So the generality of RVR beyond charts thus remains unclear.\n\nThe \\delta logP improvements in Table 4 show clear OOD benefits but relatively small or negative gains in ID settings (e.g., ChartQA)."}, "questions": {"value": "How sensitive is performance to the weighting of \\lambda_1 and \\lambda_2 in Eq. 6?\n\nGiven that the CoT datasets are generated by a relatively large LVLM (Qwen2.5VL-72B), will the results drop with a smaller generator?\n\nCould the RVR framework be applied to other domains like general visual-language reasoning tasks? What modifications would be required?\n\nDoes enforcing strict process conformity reduce flexibility or creativity in reasoning, e.g., alternative but valid reasoning paths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zSCfeOcP1c", "forum": "QHaV1ZHzi7", "replyto": "QHaV1ZHzi7", "signatures": ["ICLR.cc/2026/Conference/Submission13667/Reviewer_Sdaj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13667/Reviewer_Sdaj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945406317, "cdate": 1761945406317, "tmdate": 1762924233546, "mdate": 1762924233546, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced Chart-RVR an RL training framework to improve the reasoning in chart understanding models and the explainability of the reasoning steps. The authors utilize the GRPO RL algorithm and propose three rewards: chart type prediction, chart table reconstruction, and process conformity. The process conformity evaluates each generated reasoning step against a gold step from an oracle model (Qwen2.5-VL-72B) using text embedding similarity. The authors conducted extensive experiments showing the superiority of their approach compared to SFT and other simpler RL approaches that only rely on the final answer accuracy and format. The authors also evaluated their model on a dverse set of chart reasoning tasks such as Chart QA, chart fact checking, chart type classification and chart table reconstruction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Extensive evaluation on diverse tasks and benchmarks, including chart question answering, fact checking, chart classification, and chart table reconstruction. The Chart-RVR model achieves strong results on most benchmarks proving the proposed approach effectiveness. \n* The authors also show the generalization of their approach across different LLM architectures such as InternVL and Gemma. This is quite important in my opinion because most recent RL papers only QwenVL and their approaches do not generalize to other pretrained models. \n* The authors also analyzed the interpretability and explainability of the generated rationale by their model showing that their RL approach achieves better explainability than the SFT approach."}, "weaknesses": {"value": "* The authors have not provided any ablation studies to show the impact & importance of each reward on the model performance. I believe the proposed rewards are overengineered, especially the process conformity reward. It would be helpful to support these claims and design choices by running some ablation studies by removing one reward at a time and showing the performance. \n\n\n* There are limited details about the dataset used for training the model. The authors should analyze the dataset and provide some insights (e..g, quality check)."}, "questions": {"value": "In Table 2b, the authors report the performance of the model on surrogate tasks like Chart type prediction and Table reconstruction. However, some of the listed datasets such as ChartQAPro do not provide any ground truth chart types or data tables. I am wondering how did the authors evaluate the output of their model on such unlabeled benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XDtbkzZ2IY", "forum": "QHaV1ZHzi7", "replyto": "QHaV1ZHzi7", "signatures": ["ICLR.cc/2026/Conference/Submission13667/Reviewer_N8w4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13667/Reviewer_N8w4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13667/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998449855, "cdate": 1761998449855, "tmdate": 1762924233079, "mdate": 1762924233079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}