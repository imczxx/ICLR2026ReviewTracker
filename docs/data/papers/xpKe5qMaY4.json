{"id": "xpKe5qMaY4", "number": 22850, "cdate": 1758336328228, "mdate": 1759896842917, "content": {"title": "GPS: Directed Acyclic Graph guided Proactive Information Seeking in Large Language Models", "abstract": "Equipping Large Language Models (LLMs) with the ability to proactively ask clarifying questions is essential to mitigate ambiguity when faced with underspecified user queries in retrieval-augmented generation (RAG) systems. However, existing methods often neglect the rule-based reasoning structures embedded in the retrieved knowledge that are central to ambiguity, making it challenging to learn an effective and efficient question-asking strategy. To address these issues, we introduce \\textbf{GPS}, a two-stage framework for enhancing proactive information seeking abilities of LLMs in RAG systems. In the reasoning stage, we propose a Directed Acyclic Graph (DAG) reasoning structure with theoretical guarantees of logical completeness, which facilitates capturing all condition logic in the retrieved knowledge and supports effective clarification. In the clarification stage, we design a traversal-based algorithm that dynamically prunes the DAG based on user responses, enabling efficient clarification. To further enhance DAG construction, we first propose a data synthesis method to address data scarcity challenge, then we apply a clarification-oriented reinforcement learning method with a hybrid reward that jointly considers effectiveness and efficiency to optimize the LLM. Experiments on three benchmarks demonstrate that \\textbf{GPS} significantly outperforms baseline methods in both answer accuracy and interaction cost.", "tldr": "", "keywords": ["Large Language Models", "Retrieval-augmented generation", "Reinforcement learning", "Clarification questions"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9f3044621710e640db61bf4106f4e5b37a43597d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The GPS (Graph Guided Proactive Information Seeking) framework proposed in this paper addresses the issue of active clarification in RAG systems when LLM processes queries with insufficient information. It innovatively introduces a conditional inference structure based on directed acyclic graphs (DAGs) and combines conditional path guided data synthesis with clarification oriented reinforcement learning, effectively balancing the effectiveness and interaction efficiency of clarification."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The design of DAG conditional reasoning structure combines logic and efficiency, breaking through the limitations of traditional prompting methods that rely on LLM spontaneous reasoning, and providing structured logical support for active clarification.\n\n2. The conditional path guided data synthesis method not only optimizes the logical correctness of DAG, but also suppresses redundant interactions and structural redundancy, achieving multi-objective collaborative optimization."}, "weaknesses": {"value": "1. The specific implementation logic for extracting DAG is not clear, such as \"how to parse conditional variables and logical relationships from unstructured documents\" and \"when there are fuzzy rules in the document.\n\n2. Suggest adding parameter sensitivity experiments to clarify the optimal parameter selection strategy; Fully derive the calculation process of structural quality rewards and verify its rationality through examples.\n\n3. In the relevant work section, there was no in-depth comparison of the essential differences between GPS and existing models in \"structured inference targets\" In addition, many SOTA methods have already studied knowledge graphs and their logic. What is the difference between this article and them?"}, "questions": {"value": "1. Why was the method in this article not compared with the GraphRAG series models?\n\n2. The second stage of this method uses models around 7B, which is too small compared to the deepseek used in the first stage. Should we use LLMs of the same scale? Existing mainstream methods also tend to use larger models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hEyR80jlk4", "forum": "xpKe5qMaY4", "replyto": "xpKe5qMaY4", "signatures": ["ICLR.cc/2026/Conference/Submission22850/Reviewer_behg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22850/Reviewer_behg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761233241262, "cdate": 1761233241262, "tmdate": 1762942413389, "mdate": 1762942413389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces GPS, a two-stage framework that enables large language models in retrieval-augmented generation systems to proactively clarify underspecified user queries by modeling conditional logic in retrieved documents as a directed acyclic graph. In the reasoning stage, a Reasoner LLM constructs a logically complete graph that captures the AND and OR relationships among condition variables and possible answers. In the clarification stage, a Clarifier LLM dynamically traverses the graph, selecting questions from a candidate set of nodes based on their expected remaining depth, and prunes inconsistent paths according to user responses to achieve efficient clarification with only a few turns.\n\nTo train the Reasoner, the authors propose conditional path-guided synthesis that augments ConditionalQA by generating underspecified queries with multiple path assignments and filtering them through a Verifier LLM to ensure consistency. They further apply reinforcement learning with a hybrid reward combining accuracy, efficiency, and structural quality to enhance reasoning precision and interaction efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Proposition 1 rigorously proves logical completeness via DNF encoding, with every root-to-leaf path corresponding to a conjunction. \n- Conditional path-guided generation from documents produces underspecified queries with explicit missing conditions and corresponding reasoning paths, each consisting of variable–answer pairs. These queries are filtered based on necessity—retained only if a Verifier model can answer correctly when given the full conditions but fails when any are masked. This process yields high-quality augmentations of ConditionalQA’s limited underspecified samples, expanding the dataset significantly without requiring human annotation."}, "weaknesses": {"value": "- Filtering retains samples only when the Verifier LLM predicts the correct answer under full conditions but fails under partial ones. However, this approach discards nuanced cases involving partial ambiguity resolution and inherits the biases of the Verifier model, such as inconsistencies observed in DeepSeek-R1. \n- There is no inter-annotator agreement or human validation for the roughly 75.5% of samples discarded during ConditionalQA augmentation."}, "questions": {"value": "- How often does the Reasoner generate invalid directed acyclic graphs—for instance, containing cycles, incomplete edge coverage, or mismatched condition variable domains—and what fallback mechanism is applied during traversal if graph parsing fails or if the candidate set becomes empty prematurely?\n- Regarding the efficiency reward, what specific value of α was used, and how does the model’s performance degrade when user responses deviate from the simulator assumptions, such as providing evasive answers or multi-valued inputs outside the defined variable domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v7FtOehjrQ", "forum": "xpKe5qMaY4", "replyto": "xpKe5qMaY4", "signatures": ["ICLR.cc/2026/Conference/Submission22850/Reviewer_1Jai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22850/Reviewer_1Jai"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750671917, "cdate": 1761750671917, "tmdate": 1762942413111, "mdate": 1762942413111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles ambiguity in RAG by teaching LLMs to proactively ask better clarifying questions. It proposes GPS, a two-stage approach: first, represent the retrieved knowledge with a Directed Acyclic Graph so the model can reason over conditional rules in a logically complete way; second, traverse and prune that graph interactively based on user answers to keep clarification efficient. To make this practical, the authors generate training data that reflect conditional paths and further fine-tune with a clarification-oriented RL objective balancing effectiveness and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Rule-structured reasoning: Modeling conditional rules as a graph is a clean, principled way to surface ambiguity and drive targeted clarification, rather than ad-hoc question asking.\n\n2. Theory with practical bite: The logical completeness guarantee plus average-case O(r) clarification complexity gives both soundness and efficiency.\n\n3. End-to-end system design: A coherent pipeline—conditional-path data synthesis, clarification-oriented RL with a hybrid (accuracy/efficiency) reward, and dynamic traversal—aligns training with the actual interaction objective."}, "weaknesses": {"value": "1. Baseline fairness (Clarify-DPO): The original Clarify-DPO does not has the RAG part. It’s unclear whether Clarify-DPO had access to retrieved documents (true RAG) or only engaged in Q&A without retrieval. If the latter, the comparison is unfair; if the former, the paper should specify how evidence was integrated to ensure parity.\n\n2. Training data parity: GPS is trained on ConditionalQA (same policy domain as ShARC). Were baselines also trained on exactly the same splits and sources?\n\n3. Efficiency evidence gap: The claim of higher efficiency isn’t supported by the experiments. There is no comparison in the aspect of efficiency with the baselines. \n\n4. Domain generalization: Evaluations center on rule-heavy policy/regulation datasets; it’s unclear how GPS transfers to domains where rules are fuzzier (open-ended QA, multi-hop encyclopedic tasks) or where conditional structures are incomplete/noisy."}, "questions": {"value": "If the retrieved document is incomplete or underspecified, can the model leverage its own parametric knowledge to supplement missing conditions during DAG construction and clarification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F5nts19vxy", "forum": "xpKe5qMaY4", "replyto": "xpKe5qMaY4", "signatures": ["ICLR.cc/2026/Conference/Submission22850/Reviewer_yNPb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22850/Reviewer_yNPb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816527705, "cdate": 1761816527705, "tmdate": 1762942412817, "mdate": 1762942412817, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GPS, a two-stage framework for improving proactive clarification in retrieval-augmented generation (RAG) systems when user queries are ambiguous or underspecified. The method explicitly models conditional reasoning dependencies using a Directed Acyclic Graph (DAG) that captures logical structures across retrieved documents. The first stage constructs the DAG with theoretical guarantees of logical completeness, while the second stage performs dynamic traversal to prune inconsistent reasoning paths based on user feedback. To mitigate data scarcity, the authors propose a conditional path-guided data synthesis strategy and optimize DAG extraction using clarification-oriented reinforcement learning with hybrid rewards balancing accuracy and efficiency. Experimental results are conducted on Synthetic, ConditionalQA, and ShARC benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "+ GPS introduces a novel approach by integrating graph-based reasoning with proactive clarification in retrieval-augmented generation systems, which significantly advances the state of the art in handling underspecified queries.\n+ The framework is supported by theoretical foundations, including formal guarantees of logical completeness for the constructed DAG, ensuring systematic and reliable reasoning.\n+ Empirical results demonstrate that GPS outperforms baseline methods on Synthetic and conditional QA dataset on Success rate, and achieve comparable results on OOD dataset ShARC, showing generalization capabilities."}, "weaknesses": {"value": "- The performance across the three benchmarks does not show clear and consistent superiority on all evaluation metrics, suggesting room for further improvement in robustness and overall gain.\n- The methodological exposition could be clearer. the paper would benefit from more detailed algorithmic descriptions and richer examples to illustrate how conditional relationships are captured and resolved.\n- The examples provided involve relatively simple condition–conclusion links; it remains unclear whether the DAG-based reasoning can handle more complex logical hierarchies, such as when a conclusion becomes a condition in a nested structure.\n- Interestingly, on in-domain data, GPS exhibits only marginal gains and performs worse than Clarify-DPO in clarifier prediction accuracy. This suggests that the reasoner may struggle to interpret graph-based representations effectively when determining whether a query requires clarification. It would be valuable to explore enhancing the reasoner’s capability through joint training on synthesized DAG–QA pairs, allowing it to better align graph structures with clarification decisions."}, "questions": {"value": "Since the improvement in Success Rate is only marginal, it would strengthen the paper to include qualitative comparison examples that clearly demonstrate the advantages of GPS in constructing higher-quality and more logically complete DAGs compared to existing methods. Such examples are essential to highlight why graph construction is both necessary and beneficial, beyond quantitative gains—showing how the proposed approach leads to more accurate condition–conclusion reasoning, better clarification paths, and enhanced interpretability relative to baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "np4qzDolaN", "forum": "xpKe5qMaY4", "replyto": "xpKe5qMaY4", "signatures": ["ICLR.cc/2026/Conference/Submission22850/Reviewer_Zgt8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22850/Reviewer_Zgt8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22850/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989082040, "cdate": 1761989082040, "tmdate": 1762942412553, "mdate": 1762942412553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}