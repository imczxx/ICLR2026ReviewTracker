{"id": "MTBU1hZHU8", "number": 11695, "cdate": 1758203123464, "mdate": 1759897560378, "content": {"title": "Explicit Multi-head Attention for Inter-head Interaction in Large Language Models", "abstract": "In large language models built upon the Transformer architecture, recent studies have shown that inter-head interaction can enhance attention performance. Motivated by this, we propose **Multi-head Explicit Attention (MEA)**, a simple yet effective attention variant that explicitly models cross-head interaction.\nMEA consists of two key components: a Head-level Linear Composition (HLC) module that separately applies learnable linear combinations to the key and value vectors across heads, thereby enabling rich inter-head communication; and a head-level Group Normalization layer that aligns the statistical properties of the recombined heads.\nMEA shows strong robustness in pretraining, which allows the use of larger learning rates that lead to faster convergence, ultimately resulting in lower validation loss and improved performance across a range of tasks.\nFurthermore, we explore the parameter efficiency of MEA by reducing the number of attention heads and leveraging HLC to reconstruct them using low-rank \"virtual heads\". This enables a practical key-value cache compression strategy that reduces KV-cache memory usage by 50\\% with negligible performance loss on knowledge-intensive and scientific reasoning tasks, and only a 3.59\\% accuracy drop for Olympiad-level mathematical benchmarks.", "tldr": "We propose Multi-head Explicit Attention, a simple yet effective attention variant. MEA improves pretraining robustness, enabling larger learning rates and thus faster convergence. MEA also enables a practical key-value cache compression strategy.", "keywords": ["Inter-head Interaction", "Attention", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/433b4c0b6a90a2e1cc85dc21a587174363e8ad9b.pdf", "supplementary_material": "/attachment/084ad97a1ad3702e5a150062a76979b957bce863.zip"}, "replies": [{"content": {"summary": {"value": "This study revisits inter-head interactions in Transformer attention blocks by proposing Multihead Explicit Attention (MEA) that leverages a learnable reweighting of head-level KV compositions prior to the attention computation. Crucially, MEA employs GroupNorm over the concatenated head outputs and the work provides theoretical arguments for MEA's formulation. Pretraining experiments demostrate improved performance with faster convergence, with the added benefit of possible compute saving via a reduced number of key-value heads."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Presents useful efforts to unify existing formulations, notably expressing variant of the Differential Transformer and Talking-Heads as special cases of MEA. Specifically, the work offers a new perspective on a DFA variant result, connecting insights across related work. \n- Results show improved pretraining convergence and performance with accuracy improvements for some standard benchmark datasets."}, "weaknesses": {"value": "The key premise of the work is that inter-head interaction can enhance attention performance but the experimental results suggest that group normalization may play a bigger role than the mere inter-head communication. Since there are multiple somewhat orthogonal explorations going on, namely an assessment of MEA performance, an exploration of SVD-based efficiency improvements, as well as cost-efficient hyperparameter selection via scaling laws, the main empirical results do not seem to offer much insight into the role of inter-head interaction. The manuscript could benefit from a clearer through-line with an explicit focus on delineating the role inter-head mixing vs. normalization. For example, section 4.2 brings up the Differential Transformer and the role of GroupNorm. However, the argument for this step is only discussed at the end of 4.3, repeating parts of 4.2. Importantly, the introduced weight update analysis is not used in any empirical analysis. A clearer focus and greater efforts in disentangling different effects empirically could greatly strengthen the work."}, "questions": {"value": "- Do you have any insight as to why the pretraining performance gains do not lead to improvements for OBQA WinoGrande and ARC-c?\n- Are you planning to release your source code?\n- Minor: Throughout the manuscript, sentences are highlighted in bold, presumably to highlighting key results. However, other highlights such as L293 or L391 do not seem to follow this pattern. What are the highlights supposed to achieve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HdaihYugUQ", "forum": "MTBU1hZHU8", "replyto": "MTBU1hZHU8", "signatures": ["ICLR.cc/2026/Conference/Submission11695/Reviewer_HPL3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11695/Reviewer_HPL3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761797730289, "cdate": 1761797730289, "tmdate": 1762922744496, "mdate": 1762922744496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Multi-head Explicit Attention (MEA), a novel attention mechanism designed to improve upon standard Multi-Head Attention (MHA) by explicitly modeling inter-head interaction. The authors identify that standard MHA and its variants treat heads independently, limiting their potential. MEA introduces two key components to address this: 1) a Head-level Linear Composition (HLC) module, which applies learnable linear combinations across heads to the Key (K) and Value (V) vectors before the attention computation, and 2) a head-level Group Normalization layer applied after the attention computation to stabilize training and prevent the model from degenerating."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The most significant contribution of this work is the KV-cache compression strategy. A 50% reduction in KV-cache memory  is a highly valuable engineering result, directly addressing one of the primary bottlenecks in long-context LLM inference. The fact that this is achieved with \"negligible performance loss\" on knowledge and science benchmarks (e.g., <1.5% drop on average) is very compelling.\n\n2. The paper provides a clear and convincing argument for why MEA works. The authors demonstrate that both components are necessary. They show that an MEA variant without GroupNorm (which they frame as a modified Talking-Heads Attention) fails to activate cross-head communication and degenerates, performing almost identically to the baseline Transformer."}, "weaknesses": {"value": "1. The idea of inter-head interaction is not new, as the authors acknowledge by citing Talking-Heads Attention and Differential Transformer. The HLC module is a specific form of linear combination, and the GroupNorm component is directly inspired by DFA. The primary innovation is the specific combination of these ideas (pre-attention K/V mixing + post-attention GroupNorm) and the analysis of why this specific combination avoids the degeneration that plagued prior work.\n2. While the training dynamics (loss, convergence speed) are improved, the final downstream performance gains from pre-training are very small. In Table 1, the MEA model achieves an average of 46.39%, which is only a minor improvement over the baseline Transformer's 45.88% and DFA's 46.36%.\n3. The KV-cache compression is not a \"free\" operation that can be applied to any model. The experiments (Section 5.2) apply this compression to a pre-trained 30B model and then require a \"continued pretraining (CPT)\" stage , and in the best-performing case, an additional \"recovery stage\", to regain performance. This extra training cost is a key part of the trade-off and should be considered when evaluating the overall efficiency.\n4. The paper correctly cites the DeepSeek paper, but it stops short of a direct comparison. DeepSeek's \"Multi-head Latent Attention\" (MLA) is a major concurrent method that tackles the exact same problem (KV cache bottleneck)."}, "questions": {"value": "What is the computational and time cost of the \"Continued Pretraining (CPT)\" and \"Recovery\" stages used in the 30B model compression experiments? A clearer picture of this cost is needed to fully evaluate the trade-off against the inference savings.\n\nBedies, the pre-training performance gains on downstream tasks were very modest. Do the authors have evidence or a hypothesis that these gains would become more significant at a larger model scale (e.g., 70B+), or is the primary benefit of MEA truly limited to training/inference efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gRaWx6QqYQ", "forum": "MTBU1hZHU8", "replyto": "MTBU1hZHU8", "signatures": ["ICLR.cc/2026/Conference/Submission11695/Reviewer_mviB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11695/Reviewer_mviB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830366661, "cdate": 1761830366661, "tmdate": 1762922744165, "mdate": 1762922744165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper advocates for interaction between heads within attention in order to improve the accuracy of transformers.\n\nIt formulates a mathematical framework that could be specialized to define different types of attention (MultiHead Attention (MHA), Grouped Query Attention (GQA), Differential Attention (DA), and Talking Heads Attention (THA)) that group or interact attention heads in different ways, then generalizes this framework to propose MultiHead Explicit Attention (MEA).\n\nThe proposed MultiHead Explicit Attention also enables compression of keys and values with limited drop in accuracy after continual pretraining."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear background / motivation / formulation, generalizing different types of attention: MQA and MHA, DFA and THA\n- Provides intution when and why each type of attention failes or degenerates into MHA\n- Evaluated on a wide range of challenging Math and resoning benchmarks"}, "weaknesses": {"value": "- The paper did not compare with other approaches that save on KV cache by continuous pretraining.\n- It wasn't clear to me why the proposed approach is more robust to KV compression than other approaches."}, "questions": {"value": "- It might make more sense to make Section 3.2 Talking-Heads Attention (TFA) come before Section 3.1 Differential Transformer (DFA) subsection, as authors explain in the DFA subsection that it is a special case of TFA.\n- Equation 10: Please also define using mathematical summation expression\n- Line 203: In previous sub-section was d used to represent full embedding dimension?\n- Table 2: What does \"Recov.\" mean? Was it explained in the paper body?\n- Table 2: \n   - Please add a column to show compute or memory savings of each approach.\n   - To make sense of the numbers, there is a need to compare with performing continual pretraining on the same dataset using other attention compression techniques in literature tuned to save the same amount of compute/memory as the approaches presented in Table 2. i.e., for the same savings of KV cache and for the same continual pretraining budget, we would like to compare the propose approach with other approaches.\n   - Also, it would be better to compare evaluations on long context benchmarks after KV compression\n\nThese suggestions are long-term recommendations and may not be feasible for the rebuttal:\n- For best practices of hyperparameter transfer I recommend looking at:\n  - muP: https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization\n  - CompleteP: https://arxiv.org/abs/2505.01618\n- FYI, There are papers that describe the early phases of training as reducing bias (i.e.,  trying to minimize average loss for most samples), and later phases of training as reducing variance (i.e., handling corner cases among training samples). This paper explains it well: https://arxiv.org/abs/2502.15938"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gvyt31Xe8h", "forum": "MTBU1hZHU8", "replyto": "MTBU1hZHU8", "signatures": ["ICLR.cc/2026/Conference/Submission11695/Reviewer_GWxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11695/Reviewer_GWxh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137883817, "cdate": 1762137883817, "tmdate": 1762922743712, "mdate": 1762922743712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper proposes an explicit Multi-head Explicit Attention method that enables cross head interaction. It is a generalization of prior methods like Differential Transformers and Talking Heads Attention, using a Head-level Linear Combination unit. While the formulation is interesting, overall the performance is not much different than the state-of the art baseline Differential Transformers. Furthermore, evaluation is weak, only applied to one type&size of model and does not include any long context evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Address an important bottleneck in LLMs which is attention for long contexts.\n- Formulation of generalization of prior inter head interaction methods like Differential Transformers and Talking Heads Attention is interesting."}, "weaknesses": {"value": "- While the KV cache compression is targeting bottleneck for long contexts, there is no long context evaluation in the paper.\n- The full-parameter CPT setup reduces the performance of math benchmark baseline. This opens up the question whether math is more sensitive to compression or is it because CPT dataset needs to have more math data  in it. It's hard to tell the reason from the data.\n- There is no comparison with other kv cache compression methods.\n- There is no validation for the learning rate selection method which is based on curve fitting.\n- The performance of the proposed method is not much better than the Differential Transformer baseline."}, "questions": {"value": "- Why does CPT applied on top of the Baseline Qwen3 models reduces the performance of math task? Is it because math is more sensitive to compression or is it because CPT dataset needs to have more math data  in it? Or both? It's hard to tell the reason from the data.\n- Can you add long context evaluation benchmarks such as RULER, NIH?\n- How did you validate the learning rate selection method?\n- Is the code going to be open source?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dCgTUndtBc", "forum": "MTBU1hZHU8", "replyto": "MTBU1hZHU8", "signatures": ["ICLR.cc/2026/Conference/Submission11695/Reviewer_ENbu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11695/Reviewer_ENbu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11695/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762845335024, "cdate": 1762845335024, "tmdate": 1762922743267, "mdate": 1762922743267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}