{"id": "qWAcXNTptu", "number": 20881, "cdate": 1758311324618, "mdate": 1759896953948, "content": {"title": "Beyond Binary Evaluation: Measuring Language Model Hallucinations Through Distributional Correctness", "abstract": "Common evaluation paradigms for language models focus on scoring single responses through accuracy metrics or proper scoring rules, failing to capture the full richness of a model's belief state. Recent work illustrates that language models hallucinate in-part because they are optimised to be good test-takers under binary scoring schemes that reward any answer over abstention. While this insight naturally leads to penalty-based approaches, they ignore crucial distinctions in how models distribute uncertainty, for example between hedging toward incorrect answers versus hedging toward ``I don't know'' responses. We introduce a novel evaluation metric to solve this problem of not considering a model's entire probability distribution over answer choices. Our metric naturally distinguishes between harmful overconfidence in wrong answers and uncertainty expressed through abstention, providing scores in an interpretable default range. Through theoretical analysis and illustrative examples, we demonstrate our metric offers a more nuanced and aligned evaluation paradigm that incentivises models to express genuine uncertainty rather than guessing. We then adapt 12 existing evaluation benchmarks to our metric's variants and measure performance on six language models, showing that for half of the tested benchmarks scores are *negative across all tested models*, indicating significant tendencies towards hallucination.", "tldr": "introduces a new LLM metric for implicit hallucination-correction of common benchmarks", "keywords": ["LLMs", "evaluation", "metrics", "hallucination"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf85c24cc10e7505e2da096203c037c28d918c71.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Under binary scoring (+1 for correct, 0 for incorrect), there is never a reason to abstain from answering or say \"I don't know\": it's always better to make a guess. It is then surprising that LLMs trained to perform well under binary scoring learn to \"hallucinate\", i.e., make a best guess even when they know. The paper proposes a new metric called the Distributed Correctness Score (DCS) that rewards abstention when the model is uncertain. The authors discuss similarities and differences between DCS and existing uncertainty-aware metrics like confidence-weighted accuracy. The authors evaluate 6 LLMs on 12 benchmarks and show that even modern LLMs perform quite poorly with respect to DCS: for 6 of the benchmarks, all LLMs have negative scores."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I think the issue of binary scoring incentivizing guessing is crucial, not just for hallucination but for all forms of caution and overconfidence in LLMs. Incorporating a correctness metric which penalizes overconfidence is a natural countermeasure. The selection of models and benchmarks to test is reasonable. Scores are reported in a clear way, with standard error included. The writing is quite clear throughout."}, "weaknesses": {"value": "### **Concerns about the DCS metric**\n\nI have major concerns about the DCS metric which the whole paper revolves around. I agree that this metric solves the core issue with binary scoring, but it seems to me that proper scoring rules solve this issue better. For the area chair, DCS is defined as $(\\ell_c p_c - \\ell_W p_W) (1- p_{IDK})$ where $p_c,p_W, p_{IDK}$ are the probabilities the model respectively assigns to the correct answer, to all of the wrong answers in aggregate, and to the \"I don't know\" answer. $\\ell_c$ and $\\ell_W$ are user-chosen parameters that are usually treated as 1.\n\n1. It seems to me that a rational agent will always choose either p_{IDK} = 0 or p_{IDK} = 1. Specifically, it is optimal to choose p_{IDK} = 0 iff the model believes that $(\\ell_c p_c - \\ell_W p_W) > 0$. As such, DCS does not elicit the true belief state of a rational agent.\n\n2. Why is it natural to include an explicit IDK option, rather than eliciting a distribution over the actually plausible answers and then deciding whether to abstain based on that distribution? The model knows that IDK is not the correct answer, so the model's true belief state over the correct answer should answer 0 probability to IDK.\n\n3. The authors critique proper scoring rules by saying that they \"fail to capture the full richness of a model’s belief state\": specifically, proper scoring rules only consider the max probability of any answer and ignore how the rest of the probability mass is distributed. However, I am unconvinced that the rest of the distribution matters. The authors' argument seems to be that we care how much probability is assigned to IDK. But this issue goes away if IDK is not included as an answer option to begin with.\n\n4. The authors also argue:\n> Unlike forecasting tasks where the ground truth is a stochastic label, language model evaluations present deterministic facts of the matter. The ‘true’ conditional distribution is a point mass, so it is meaningless to demand that a model report the frequency of correctness for each option, as for example the Brier score rewards...Our objective is not to elicit calibrated probabilities but to measure trustworthy epistemic behaviour.\n\nWhile the authors mention one interpretation of the Brier score, it is perfectly well-defined to still award of a score of 1 - (p-c)^2 where p is the max probability of any answer and c in {0,1} indicates whether that answer is correct. Furthermore, this definition maintains the desirable property that a rational agent should always report its true probability distribution over answers. DCS does not have this property, as mentioned in Point 1 above. It's also unclear why calibrated probabilities are not satisfactory as \"trustworthy epistemic behaviour\", or why DCS induces more trustworthy epistemic behaviour.\n\n**What I would find convincing.** The main argument I see for DCS over proper scoring rules is that if we want to finetune models to sometimes abstain, then it makes sense to include an explicit IDK option in the finetuning dataset. This argument makes sense to me, but no finetuning experiments are performed to see whether DCS effectively serves this purpose. Also, a simpler scoring rule like \"+1 for correct, 0 for abstain, -1 for wrong\" could suffice for the finetuning application. See [Kang et al (2025)](https://aclanthology.org/2025.naacl-long.183/), which performs RL finetuning on this simpler scoring rule and shows that it successfully teaches the model to abstain. I think it's totally plausible that finetuning on DCS is more effective at teaching models to appropriately abstain. But if that's the main claimed benefit of DCS, then I think the paper needs to show that experimentally.\n\n## **Other issues**\n\n1. No proper scoring rules are included in the experiments as baselines.\n2. The authors mentioned that in two cases, a model got 0% accuracy. The authors suggest that \"these models failed to understand the minimally-adjusted multiple-choice instruction format.\" But if the authors are extracted answer probabilities directly from the LLM output probabilities (i.e., what is p(\"A\"), what is p(\"B\"), etc, then normalize), format following shouldn't really be an issue? Also, is it even possible for accuracy can be 0 and DCS to be positive? If DCS is positive on a given question, then the probability assigned to the correct answer is greater than the sum of probabilities assigned to incorrect answers, so the correct answer has the max probability and thus accuracy should be positive?\n\nOverall, I think this is a very important problem and the ideas in the paper are promising, but I'm not sure that the current approach lives up to the motivation of the paper."}, "questions": {"value": "It would be great if the authors could clarify my questions about the 0% accuracy issue. I also am open to being convinced about the benefits of the DCS metric."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0iy5NI16yP", "forum": "qWAcXNTptu", "replyto": "qWAcXNTptu", "signatures": ["ICLR.cc/2026/Conference/Submission20881/Reviewer_Wvmz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20881/Reviewer_Wvmz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761590283966, "cdate": 1761590283966, "tmdate": 1762937474891, "mdate": 1762937474891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Distributional Correctness Score (DCS), a theoretically grounded metric that evaluates full probability distributions, and incorporates abstention as a vital component. This metric is designed to mitigate the pitfalls of the traditional binary evaluation metrics for language models, as they fail to capture epistemic uncertainty. The metric is evaluated across a wide range of datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The proposed DCS metric is novel and interesting and makes intuitive sense.\n* The paper includes extensive examples of the DCS metric across various cases.\n* The paper includes evaluation across of diverse range of datasets.\n* The paper is well written and easy to understand."}, "weaknesses": {"value": "* The paper does not clarify how the DCS metric can be applied to problems without options. E.g., question from QSM8k where the answers could be integers in the range $[-\\infty ,\\infty ]$.\n\n*  The paper should also discuss other metrics that consider the distribution over answers: \"Enhancing Hallucination Detection through Noise Injection, arXiv Feb 2025\".\n\n* Section 6 includes interesting results across models. However, the paper does not provide any explanations as to why DCS score is low or high for a specific model. Why is Llama3.1 8B Instruct DCS score highest on MMLU? \n\n* The evaluation should consider newer models such as Qwen-2.5 or Qwen-3. \n\n* There is also no analysis of the effect of model size on the DCS score. It would be interesting to show the DCS score for the same family of models from small to large, e.g., Qwen-2.5 from 0.5B to 72B."}, "questions": {"value": "* The paper should include more extensive analysis across model sizes.\n* The paper should include a discussion of the applicability of the DCS metric when questions do not a set of options as answers.\n* The paper should include a more extensive discussion of prior work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1NrXjUvsV1", "forum": "qWAcXNTptu", "replyto": "qWAcXNTptu", "signatures": ["ICLR.cc/2026/Conference/Submission20881/Reviewer_L8Lt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20881/Reviewer_L8Lt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762038407320, "cdate": 1762038407320, "tmdate": 1762937473872, "mdate": 1762937473872, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the Distributional Correctness Score (DCS), a novel metric that evaluates a model’s entire probability distribution rather than the maximum predictions. This new metric considers the model's uncertainty for correct answers, incorrect answers, and abstentions.\nThe authors prove a theoretical analysis to demonstrate that  DCS incentivises the desired behaviour: confidence in correct answers, uncertainty when knowledge is lacking, and preference for abstention over confident incorrectness. \nExperimental results across 12 existing benchmarks indicate that (1) many language models exhibit systematic epistemic overconfidence;\n(2) all models hold negative DCS scores"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* this work identifies that current metrics focus on a single argmax answer while ignoring the distribution across the space of possible responses, which might be useful for future studies\n* this work provides both theoretical and empirical studies. \n* extensive experiments across 12 benchmarks\n* this proposed DCS is working, which is able to mitigate the overconfidence issue"}, "weaknesses": {"value": "* I did not fully understand the motivation of this metric. (1) The probability assigned to a specific answer is computed by a softmax layer, which accounts for the logits over different answers, including abstention as well. (2) proper scoring rules and other metrics, e.g. entropy, can also depict this. It would be useful if the authors clarify why the DCS is necessary and stress the difference compared to existing metrics\n* It is tricky to see the benefit of using the proposed DSC. From Figure 2 and Table 2, we can observe that DCS is consistently lower and remains negative in most cases, but why should we stick with DCS? Under which conditions (tasks), we should select DCS as the metric."}, "questions": {"value": "* what is the meaning of a concrete value of DCS? If the value is negative or positive, how to explain it?\n* In Figure 2, I did not understand how to compare DCS to other baselines. I observe that DCS is consistently lower than other baselines, but what does it mean? It is hard to understand the benefits of using DCS in this figure\n* In Table 2, it is tricky to derive the main findings."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QiNTv7pWZh", "forum": "qWAcXNTptu", "replyto": "qWAcXNTptu", "signatures": ["ICLR.cc/2026/Conference/Submission20881/Reviewer_8onr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20881/Reviewer_8onr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209204534, "cdate": 1762209204534, "tmdate": 1762937472671, "mdate": 1762937472671, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the Distributional Correctness Score (DCS), an evaluation metric for language models. The authors argue that standard binary accuracy metrics incentivize hallucination because they reward guessing over abstention. Unlike simple penalty-based metrics, DCS evaluates the entire probability distribution over answers, including an explicit \"I don't know\" (IDK) option. The paper presents experiments on 12 benchmarks with 6 LLMs that show that many models achieve negative average DCS scores, revealing overconfidence that is masked by traditional accuracy metrics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I see the main strengths of the paper as follows:\n\n- problem formulation: the paper identifies a key socio-technical issue: current evaluation metrics encourage models to game the system by guessing rather than abstaining; the distinction between \"error-hedging\" and \"abstention-hedging\" is a useful conceptual contribution\n- exposition: the paper is well-written and the motivating examples effectively illustrate the flaws in current metrics that DCS aims to fix\n- empirical evaluation: testing across 12 diverse benchmarks and 6 models of varying sizes provides a reasonably comprehensive picture of how DCS behaves in practice compared to standard metrics"}, "weaknesses": {"value": "I believe there are a few weaknesses:\n\n- implementation: the reliance on log-likelihoods for all answer options plus a canonical \"IDK\" response is a meaningful barrier to adoption as many API don't provide these\n- parameterization: the introduction of $l_c$ and $l_w$ parameters adds flexibility but seems somewhat arbitrary and there is no explanation as to how these parameters ought to be chosen\n- sensitivity to \"IDK\" phrasing: the method assumes a single, canonical \"IDK\" string represents abstention, but language models might distribute uncertainty across many synonyms (e.g., \"I'm unsure\", \"Unknown\", \"Cannot determine\"); there is no evaluation regarding the sensitivity to the specific choice of this string\n- multiple choice question-answering: the formulation is tightly bound to multiple-choice or closed-set classification and as such does not address the more common real-world problem of open-ended long-form evaluation"}, "questions": {"value": "- Have you evaluated the sensitivity of DCS to the specific string used for abstention (e.g., \"I don't know\" vs. \"Not sure\")? Does using a set of abstention phrases and summing their probabilities improve robustness?\n- How would you recommend applying DCS to a black-box API model that only returns generated text without log probs? Is there a sampling-based approximation you considered?\n- Could you provide an explanation or a small study on how to set $l_c$ and $l_w$ for different risk profiles (e.g., medical advice vs. creative writing)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ugYzPr6aeW", "forum": "qWAcXNTptu", "replyto": "qWAcXNTptu", "signatures": ["ICLR.cc/2026/Conference/Submission20881/Reviewer_m9zt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20881/Reviewer_m9zt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20881/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762757191735, "cdate": 1762757191735, "tmdate": 1762937471080, "mdate": 1762937471080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}