{"id": "1y1q1D6Ly7", "number": 1875, "cdate": 1756956214398, "mdate": 1763089220509, "content": {"title": "Growing Visual Generative Capacity for Pre-Trained MLLMs", "abstract": "Multimodal large language models (MLLMs) extend the success of language models to visual understanding, and recent efforts have sought to build unified MLLMs that support both understanding and generation. However, constructing such models remains challenging: hybrid approaches combine continuous embeddings with diffusion or flow-based objectives, producing high-quality images but breaking the autoregressive paradigm, while pure autoregressive approaches unify text and image prediction over discrete visual tokens but often face trade-offs between semantic alignment and pixel-level fidelity. In this work, we present **Bridge**, a pure autoregressive unified MLLM that augments pre-trained visual understanding models with generative ability through a Mixture-of-Transformers architecture, enabling both image understanding and generation within a single next-token prediction framework. To further improve visual generation fidelity, we propose a semantic-to-pixel discrete representation that integrates compact semantic tokens with fine-grained pixel tokens, achieving strong language alignment and precise description of visual details with only a 7.9\\% increase in sequence length. Extensive experiments across diverse multimodal benchmarks demonstrate that Bridge achieves competitive or superior results in both understanding and generation benchmarks, while requiring less training data and reduced training time compared to prior unified MLLMs.", "tldr": "", "keywords": ["MLLMs", "Image Generation", "Foundation Model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/34863d7b6f8dcad534df0d1381a34885d5318e29.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Bridge, a purely autoregressive unified MLLM based on a mixture-of-transformer architecture. Bridge performs both visual understanding and generation within a single next-token prediction framework. Furthermore, it introduces a semantic-to-pixel discrete representation, which places semantic tokens before pixel tokens to enable semantic-aware generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The experiments and ablation studies are detailed and comprehensive."}, "weaknesses": {"value": "1. The authors claim to have proposed a semantic-to-pixel representation to achieve semantic alignment. However, the approach relies on TA-Tok to provide semantic tokens, which was originally introduced by Tar and is not a novel contribution of this paper. The main design change appears to be replacing the detokenizer in Tar with a single transformer that directly generates pixel tokens — a relatively minor modification. Moreover, the authors do not clearly demonstrate the advantages of this change. The unified transformer is also larger than the lightweight detokenizer, which may increase generation latency.\n2. I do not consider the use of a mixture-of-transformer architecture to extend a pretrained understanding MLLM into a unified model to be a novel design. The authors themselves list many related works adopting similar ideas. Given that there are already separate modules for understanding and generation that share attention, the paper does not convincingly explain why adopting a purely autoregressive next-token prediction framework offers advantages over existing flow-matching designs.\n3. Regarding the image editing experiments, the authors mention using OmniGen2 data but do not report its results on the ImgEdit benchmark."}, "questions": {"value": "1. I noticed that visual understanding data is also used during training. Since a frozen pretrained MLLM is leveraged for visual understanding, it is unclear why this part of the data is needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "6WrM8accOf", "forum": "1y1q1D6Ly7", "replyto": "1y1q1D6Ly7", "signatures": ["ICLR.cc/2026/Conference/Submission1875/Reviewer_cNwT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1875/Reviewer_cNwT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932275177, "cdate": 1761932275177, "tmdate": 1762915923309, "mdate": 1762915923309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "y2QPYV6hMQ", "forum": "1y1q1D6Ly7", "replyto": "1y1q1D6Ly7", "signatures": ["ICLR.cc/2026/Conference/Submission1875/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1875/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763089218801, "cdate": 1763089218801, "tmdate": 1763089218801, "mdate": 1763089218801, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Bridge, a pure autoregressive unified multimodal large language model (MLLM) designed to perform both visual understanding and generation. The core idea is to augment a pre-trained MLLM with generative capabilities without compromising its original strengths. For image generation, this paper proposes a \"semantic-to-pixel\" discrete visual representation, where a short sequence of high-level semantic tokens is followed by a longer sequence of fine-grained pixel tokens, all predicted within next-token prediction framework."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The \"semantic-to-pixel\" representation uses appropriate semantic tokens and token arrangements to achieve both effectiveness and efficiency.\n2. The comparison against a \"dense\" architecture (Tab.5) clearly validates the MoT design, while the token routing experiments (Tab.6) further justify the chosen architecture. These detailed analyses significantly enhance the credibility of the paper's conclusions."}, "weaknesses": {"value": "1. The core MoT architecture, featuring separate QKV and FFN blocks for different experts that interact via a shared attention, is highly reminiscent of the dual-branch architecture used in BAGEL. The difference with BAGEL is a purely AR loss v.s. flow matching loss. Furthermore, it is empirically debaTab.whether a pure AR loss is superior to diffusion or flow-based losses for high-fidelity visual generation; the latter often demonstrate stronger performance in image quality and detail.\n2. The DPG experimental results in Tab.2 are a significant concern. This benchmark is crucial for evaluating a model's ability to follow long and complex textual instructions, a key aspect of advanced text-to-image generation. The Tab.is notably missing performance results for several key unified models that employ denoising or flow-based training (e.g., BAGEL, MetaQuery, UniWorld, BLIP3-o).\n3. It is desirable a direct, side-by-side qualitative comparisons with other unified models, especially those using diffusion or flow-matching objectives. Showing only the model's own outputs (Fig.1 and Appendix) is insufficient for a rigorous evaluation. The visual quality of AR models can differ significantly from diffusion models (e.g., in texture or global coherence), and a direct comparison would be highly informative."}, "questions": {"value": "Please refer to weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6V7r9Ov50F", "forum": "1y1q1D6Ly7", "replyto": "1y1q1D6Ly7", "signatures": ["ICLR.cc/2026/Conference/Submission1875/Reviewer_gzZ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1875/Reviewer_gzZ5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070184961, "cdate": 1762070184961, "tmdate": 1762915923108, "mdate": 1762915923108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bridge, a unified multimodal large language model that trained purely with next-token prediction. For discrete image generation, the paper introduces a “semantic-to-pixel” representation: prepend a short sequence of semantic tokens to pixel-level tokens, claiming better language alignment and detail fidelity with only ~7.9% token overhead."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe hard routing and frozen understanding branch offers a pragmatic way to avoid catastrophic degradation. The comparison against a \"dense\" architecture in Tab.5 clearly validates the MoT design, while the token routing experiments in Tab.6 further justify the chosen architecture. \n2.\tThe comparison among “only pixel,” “only semantic,” and “semantic+pixel” is informative; the finding that a small number of semantic tokens helps alignment with a minor sequence-length cost is practically useful."}, "weaknesses": {"value": "1.\tArchitectural novelty relative to BAGEL is limited; core design overlaps substantially, such as the MoT-style dual branches with isolated QKV/FFN per expert and shared attention, overlapping the core design substantially\n2.\tThe idea of placing a short semantic token prefix before pixel tokens is natural, especially under causal attention. The role of CoT claimed in this article needs more evidence to prove, such as the attention distribution among tokens.\n3.\tThe interaction between experts (“both experts share unified causal attention across all tokens” vs “hard routing” of tokens) is not fully clear. It needs explicit layer-by-layer description, parameter tying, memory/compute overhead, and how generation tokens condition the understanding branch during image-to-text tasks (mechanism of cross-branch conditioning).\n4.\tAs a core benchmark for long-instruction adherence, DPG requires broad and balanced baseline coverage. The absence of denoising/flow-based unified models such as BAGEL, BLIP-3o and UniWorld, in Tab.2 weakens the paper’s claim that a pure autoregressive approach remains competitive in complex instruction scenarios. Please include these methods under the same evaluation protocol and explicitly report the use of any prompt rewriting to ensure fairness and credibility."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W6ygSynk2g", "forum": "1y1q1D6Ly7", "replyto": "1y1q1D6Ly7", "signatures": ["ICLR.cc/2026/Conference/Submission1875/Reviewer_FSuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1875/Reviewer_FSuZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762070369551, "cdate": 1762070369551, "tmdate": 1762915922969, "mdate": 1762915922969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Bridge, a pure autoregressive unified multimodal LLM that adds generative capacity onto a pre-trained visual understanding MLLM by using a Mixture-of-Transformers (MoT) dual-expert architecture. It introduces a semantic-to-pixel discrete visual representation (short sequence of semantic tokens followed by pixel tokens) to improve alignment with language while retaining pixel fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. comprehensive comparison with existing works.\n2. Practical focus on efficiency.\nLimiting token length increase is a sensible way to avoid massive extra compute/data requirements. This paper emphasizes the use of less data and shorter training time compared to some unified baselines.\n3. Comprehensive ablation studies."}, "weaknesses": {"value": "1. The approach relies on TA-Tok for semantics and LlamaGen-VQGAN for pixels; the model’s ceiling is therefore tied to those encoders’ capacity. The paper acknowledges this limitation, but more analysis of encoder failures (and ablation with alternatives) would strengthen claims.\n2. Discrete visual representations naturally have lower image quality than continuous ones (hence Bridge underperforms BAGEL, which uses a similar MoT architecture but diffusion)."}, "questions": {"value": "1. See \"weaknesses\" 1\n2. Can you add some compute/memory profiling comparing MoT vs dense to justify cost/benefit?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ApT1dpGu1j", "forum": "1y1q1D6Ly7", "replyto": "1y1q1D6Ly7", "signatures": ["ICLR.cc/2026/Conference/Submission1875/Reviewer_1o73"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1875/Reviewer_1o73"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1875/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762096910828, "cdate": 1762096910828, "tmdate": 1762915922795, "mdate": 1762915922795, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}