{"id": "6XdT4NuIMz", "number": 25480, "cdate": 1758368498196, "mdate": 1759896719410, "content": {"title": "Dynamic $k$-shot In-Context Learning", "abstract": "In-context learning (ICL) allows large language models (LLMs) to learn new tasks from demonstrations and to predict unseen inputs without parameter updates. Existing studies typically fix the number of demonstrations as a static hyperparameter  (e.g., 5 or 10), overlooking the variability across models and inputs. We empirically find that the same query text may yield different outcomes depending on the number of demonstrations used. Motivated by this observation, we propose Dynamic-$k$ In-Context Learning (D-$k$-ICL), a novel method that adaptively determines the most suitable number of demonstrations for each query text. The core component is a performance predictor—a neural network that jointly encodes the query text and candidate in-contexts (constructed with varying demonstration counts) to estimate expected inference quality. At inference time, we retrieve the top-$k$ semantically similar demonstrations and progressively vary $k$ to generate candidate in-contexts. The predictor then selects the candidate most likely to yield the best output, thereby dynamically adapting both the number and composition of demonstrations. Across three LLMs and eight datasets, D-$k$-ICL achieves considerable results, with up to 77.8\\% accuracy, 0.641 MSE, 0.271 ROUGE-1, and 0.295 BLEU. Furthermore, even when trained under few-shot, weakly supervised, or self-supervised settings, the predictor remains effective. Finally, D-$k$-ICL consistently improves performance on commercial  LLMs such as GPT-4o, demonstrating its robustness and broad applicability.", "tldr": "", "keywords": ["In-context learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/820ee9fbff9d3f05facdd6c5341457728ae2665a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a framework for dynamic exemplar allocation in in-context learning (ICL), moving beyond fixed (k)-shot settings. The core component is a neural network that estimates the optimal number of semantically closest exemplars (k) to include for each query, conditioned on the specific LLM in use. The authors demonstrate consistent performance gains across several tasks and datasets and show that the framework generalizes well to other models (e.g., GPT 4o)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear empirical motivation: the paper shows that model performance varies non-monotonically with the number of exemplars, supporting the need for a dynamic approach.\n- The proposed performance predictor mechanism is conceptually simple and can be easily adapted to other demonstration-based prompting setups.\n- Wide selection of benchmarks, accompanied by well-designed ablation studies and generalization analyses.\n- The plug-and-play integration and weakly labeled/few-shot training scenarios show practical awareness and an attempt to assess robustness under limited supervision."}, "weaknesses": {"value": "- The paper doesn’t discuss that this approach may struggle in out-of-distribution settings, where finding suitable ICL exemplars is highly challenging.\n- The paper does not sufficiently analyze or mitigate the computational overhead introduced by the proposed method. Prior work [1] explicitly addresses such concerns.\n- The claim that “little attention has been given to the number of exemplars in ICL” (L80, L124-L129) is inaccurate. Several prior works [2] have investigated this aspect.\n- While the main experiments span eight benchmarks, several analyses and follow-up experiments are restricted to at most two datasets, limiting the generality of the claims.\n- Numerous citations appear fabricated or unverifiable. For example, all 10 “Ref for” entries (e.g., L617-618, L520-522) refer to nonexistent works despite referencing widely established topics such as AdamW and ROUGE. This raises concerns about citation integrity and reproducibility.\n\nCITATIONS:\n- [1] Emily Xiao, Chin-Jou Li, Yilin Zhang, Graham Neubig, and Amanda Bertsch. Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse Attention. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31946–31958, Vienna, Austria, July 2025. Association for Computational Linguistics. https://aclanthology.org/2025.acl-long.1542/\n\n- [2] Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-Shot In-Context Learning. In Advances in Neural Information Processing Systems 37, pages 76930–76966, 2024. https://proceedings.neurips.cc/paper_files/paper/2024/file/8cb564df771e9eacbfe9d72bd46a24a9-Paper-Conference.pdf"}, "questions": {"value": "- Could you please provide general statistics on the learned (k) distribution across tasks? As Table 4 suggests, performance depends on the maximum allowed (k).\n- Please include a clear explanation (in the appendix) of the baselines, as they directly influence the pool of candidate exemplars in the few-shot training setting.\n- The rationale for capping (k) at 10 for comparison is understandable, but it would be informative to explore performance when (k >>> 10).\n- Suggested clarifications and minor corrections:\n  - Figure 1: expand caption. KATE and CD are not widely known abbreviations.\n  - L134: Explain what KATE stands for.\n  - L156: reference to a non-existent figure (possibly meant to be Fig. 2).\n  - Figure 2: visually appealing but somewhat confusing—numbering the steps would help.\n  - L231: remove extra closing parenthesis in (F_{\\text{LLM}}).\n  - Table 3: for Qwen, training data “WNT19” omits “En–Zh.”\n  - L395: “D-k-ICL” is poorly formatted.\n  - L423 and L461: likely meant “GLM4 9B.”\n  - Table 7: dataset source not mentioned (appears to be TREC)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No direct ethical issues are apparent beyond the need for accurate citation. However, fabricated or unverifiable references undermine reproducibility and transparency. We found that all (10) citations that have used \"Ref for\" are hallucinated. \n\nExample: \"Ilya Loshchilov and Frank Hutter. Adamw optimizer: Advances and applications in deep learning. Journal of Machine Learning Research, 2025. Ref for ”AdamW optimizerref” in training setup.\""}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kcsk6oJTTS", "forum": "6XdT4NuIMz", "replyto": "6XdT4NuIMz", "signatures": ["ICLR.cc/2026/Conference/Submission25480/Reviewer_1SLG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25480/Reviewer_1SLG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586824326, "cdate": 1761586824326, "tmdate": 1762943449763, "mdate": 1762943449763, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes D-k-ICL, which adaptively selects the number of in-context examples for each query using a learned performance predictor. Instead of relying on a fixed k, the method estimates task performance across candidate values and chooses the best one per instance. Experiments on multiple datasets and LLMs show modest but consistent improvements over fixed-k baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an overlooked yet important factor in ICL: how many examples to use.\n\n\n- Model- and task-agnostic; can be applied to different LLMs and domains.\n\n\n- Demonstrates steady gains across several datasets and model scales."}, "weaknesses": {"value": "- The idea of learning a predictor to determine the optimal number of demonstrations feels incremental relative to existing meta-ICL or performance-prediction frameworks. Prior work has explored adaptive retrieval [1,2]. Thus, the novelty lies primarily in applying these ideas to the dimension of “number of shots,” which is conceptually interesting but not deeply transformative.\n\n\n- Although D-k-ICL demonstrates improved results across multiple datasets and LLMs, the comparisons are limited to heuristic retrieval methods. The baseline pool excludes recent training-based ICL methods such as EPR [3], CEIL [4], and MoD [5], which explicitly optimize retrieval or demonstration selection using supervision. Including these would provide a more comprehensive and fair comparison to assess whether D-k-ICL brings substantive improvements beyond existing learning-based frameworks.\n\n\n- The scalability of D-k-ICL raises practical concerns. The method requires constructing labeled training data across varying k values, leading to a training cost that grows linearly with k. In contrast, simpler retrieval-based strategies (e.g., top-k selection) can trivially adapt to different k without retraining. The paper does not analyze the computational or annotation overhead relative to these simpler baselines."}, "questions": {"value": "- The paper states that “all code and datasets will be publicly available in an anonymous repository,” but no link or supplementary material is currently provided. Could the authors clarify whether these resources will be shared during the review process to support reproducibility?\n\n\n- Section 3 motivates the challenge by showing strong performance fluctuations with different k. Does the proposed method reliably find the optimal or near-optimal k for each instance? Quantitative evidence (e.g., distance from the true best-performing k) would help clarify how well the predictor performs the intended selection.\n\n\n- Line 156 references “Fig. ??,” suggesting a missing figure label. Please verify and correct this cross-reference.\n\n\n- In Section 3, Naik et al. (2023) are cited as a method for “Cluster-Diversity” within ICL. My understanding is that their work is not originally designed for in-context learning nor explicitly focused on cluster-based diversity. This raises concerns about the accuracy of related-work positioning and the professional of paper writing. I invite the authors to clarify this connection.\n\n\n- The acronym “SICL” appears in Sections 5.3 and 6 without definition. Is this intended to denote D-k-ICL or another variant? Please clarify the terminology for consistency.\n\n\n[1]Y Zhang, et al. Active example selection for in-context learning.\n\n\n[2]Z Chen, et al. MAPLE: Many-Shot Adaptive Pseudo-Labeling for In-Context Learning.\n\n\n[3]Rubin, et al. Learning to retrieve prompts for in-context learning.\n\n\n[4]Ye J, et al. Compositional exemplars for in-context learning.\n\n\n[5]S Wang, et al. Mixture of demonstrations for in-context learning"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "OdFMHzzjic", "forum": "6XdT4NuIMz", "replyto": "6XdT4NuIMz", "signatures": ["ICLR.cc/2026/Conference/Submission25480/Reviewer_aNug"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25480/Reviewer_aNug"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761684545946, "cdate": 1761684545946, "tmdate": 1762943449556, "mdate": 1762943449556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper empirically finds the inference performance of ICL exhibits considerable variation across datasets and LLMs as the number of demonstrations changes, and proposes to dynamic choose the number of contexts in the ICL. Specifically, this paper constructs the triplet including (text, in-context, actual performance) to train a performance predictor, then use this predictor to predict the best number of contexts during inference."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "S1: The problem is realistic and important.\n\nS2: The paper is easy to read and easy to follow.\n\nS3: The proposed methods are easy to understand and somewhat reasonable.\n\nS4: The experiments show the effectiveness of the proposed method."}, "weaknesses": {"value": "W1: Why do we not consider the order, the formatting, or other issues in such scenario? For example, only using cosine similarity to determine the order.\n\nW2: When testing, do we need to go through all demonstrations? This will cause a huge complexity and slow the inference speed. In addition, we still have a hyper-parameter, that is, $k$ in equation 6, which means there is still a trade-off, i.e., large $k$ will slow the inference speed but have a reliable guarantee. Besides the “cost” paragraph, can we compare the complexity of the proposed method to the existing fixed-k method?\n\nW3: If the test data is unseen, can the performance predictor generalize to such dataset? In other words, if we randomly split the train / test data from the same dataset, we can train a fixed number of contexts in each dataset.\n\nW4: How does the scalability of this method? In other words, if the model is large (14B, 32B, 70B, etc.), how does the improvement change? Due to the large model may need a few demonstrations to learn. In addition, this paper should categorize baseline methods more clearly, for example, which methods belong to which category summarized in the related work session.\n\nW5: Several presentation issues, such as “Fig. ??” in line 156. In addition, Figure 2 is not clear enough for demonstrating both the big picture of the whole framework and the corresponding details. I suggest providing one more figure for illustration."}, "questions": {"value": "See the weaknesses part for the question."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hcZuloGyEc", "forum": "6XdT4NuIMz", "replyto": "6XdT4NuIMz", "signatures": ["ICLR.cc/2026/Conference/Submission25480/Reviewer_obmn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25480/Reviewer_obmn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999488127, "cdate": 1761999488127, "tmdate": 1762943449248, "mdate": 1762943449248, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Dynamic-k In-Context Learning (D-k-ICL), a novel approach that adaptively determines the optimal number of demonstrations for each query in in-context learning (ICL), rather than using a fixed hyperparameter k. The authors first conduct an empirical study showing that the optimal number of demonstrations varies significantly across different queries, models, and tasks. Based on this observation, they develop a performance predictor—a neural network that takes both the query text and candidate in-contexts (with varying demonstration counts) as input and estimates the expected inference performance. During inference, the predictor selects the candidate in-context most likely to yield the best output. The method is evaluated across 3 LLMs, 5 tasks, and 8 datasets, showing consistent improvements over baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper focuses on an interesting and unexplored ICL problem of determining the number of shots in ICL."}, "weaknesses": {"value": "1. The main concern is that there is a simple baseline to adjust the number of shots in the ICL. That is, introducing the threshold of scores instead of the threshold of top k. This baseline should be included in the comparison.\n2. The approach introduces additional computational costs during training and inference, which would prevent this method from the real world applications. \n3. The presentation of the work could be further improved."}, "questions": {"value": "1. How sensitive is the performance predictor to the size and quality of training data? What happens with very limited training examples?\n2. The performance of ICL also depends on the order of input demonstrations. Can the proposed method take this into consideration?\n3. Could simpler heuristics (e.g., using similarity threshold or query complexity metrics to determine k) achieve similar performance gains with lower computational cost?\n4. For practical deployment, what is the break-even point where the performance gains justify the additional computational costs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tEisP9eaUe", "forum": "6XdT4NuIMz", "replyto": "6XdT4NuIMz", "signatures": ["ICLR.cc/2026/Conference/Submission25480/Reviewer_mqhd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25480/Reviewer_mqhd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25480/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762147308608, "cdate": 1762147308608, "tmdate": 1762943448866, "mdate": 1762943448866, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}