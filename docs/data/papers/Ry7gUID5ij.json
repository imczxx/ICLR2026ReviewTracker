{"id": "Ry7gUID5ij", "number": 3119, "cdate": 1757338008260, "mdate": 1759898108107, "content": {"title": "Towards Autonomous Experimentation: BioProBench, a Corpus and Benchmark for Biological Protocol Comprehension", "abstract": "The automation of scientific experimentation is critically hindered by the inability of Large Language Models (LLMs) to reliably comprehend the specialized, accuracy-critical, and procedural nature of biological protocols. To address this fundamental challenge, we present **BioProBench**, a comprehensive resource for procedural reasoning in biology. BioProBench is grounded in a foundational corpus of 27,000 human-written protocols. From this corpus, we systematically constructed a dataset of over 550,000 task instances, partitioning it into a large-scale training set and a rigorous benchmark with a held-out test set and novel evaluation metrics. Our comprehensive evaluation of 10 mainstream LLMs on the benchmark reveals a critical performance gap: while models excel on basic comprehension tasks, they underperform on tasks requiring deep procedural logic, quantitative accuracy, and safety-critical reasoning. To demonstrate the value of our corpus in mitigating these issues, we developed **ProAgent**, a Retrieval-Augmented Generation (RAG) agent. Grounded in our corpus, ProAgent substantially advances the state-of-the-art. BioProBench thus provides both a rigorous diagnostic benchmark and a foundational resource for developing the next generation of reliable AI for science. The code and data are available at: [https://anonymous.4open.science/r/Anonymization-112358].", "tldr": "", "keywords": ["Biological Protocol", "Dataset and Benchmark", "LLMs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/284e20d7eb36eb74501fcce45726b3e8ebfde64e.pdf", "supplementary_material": "/attachment/97cb2bc2b7e1f96b6a563710850187336e0651e1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new benchmark (BioProBench) containing ~500k tasks to evaluate the ability of LLM agents to manipulate, understand, correct, and generate scientific protocols. They introduce 5 types of tasks spanning different facets of protocol modeling and comprehensively evaluate frontier models with a few novel evaluation metrics for protocol generation and reasoning tasks. Experiments show that frontier models struggle with more complex tasks that require deeper procedural knowledge such as finding the correct order of protocol steps, generating protocols that recover the original sequence of steps, and inconsistent reasoning traces. Finally, they show that a simple RAG framework can improve the performance by providing agents with relevant context chunks retrieved from the BioProBench dataset. \n\nI am leaning towards acceptance for the following reasons: (1) the benchmark dataset is well curated and presents a valuable resource for the community, (2) the design of the 5 different tasks is sound and well-motivated with clear examples, (3) the experiments on frontier agents are thorough and support the conclusions in the results section, (4) the RAG framework seems to provide positive results.\n\nThere are some questions and concerns that I've highlighted in the Question section below. I am open to increasing my score if these concerns are appropriately resolved."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- clear writing, motivation, and experiments\n- valuable benchmark dataset for the community\n- rigorous data curation and task creation\n- rigorous evaluation of frontier models\n- positive results on RAG framework"}, "weaknesses": {"value": "- novelty of the benchmark is limited (previous papers like LAB-Bench already evaluate protocol QA)\n- RAG framework is not original\n- domain specific metrics (keyword and embedding overlap) seem quite crude to me and lack validation (see questions)\n- design of the RAG framework is not clear\n- improvements from RAG framework are not thoroughly investigated (see questions)"}, "questions": {"value": "- The keyword-based content metric seems overly crude to me and easily exploitable if it were used as a reward. Can the authors provide additional motivation for using this metric or some kind of validation that it actually captures meaningful qualities of protocols?\n- For the step ordering tasks, the authors assume that there only exists one correct ordering of the steps and use exact match evaluation metric. I am wondering if there are cases where some steps are interchangeable without affecting the quality of validity of the protocol.\n- In the RAG framework experiments, how did the authors split the data between protocols that can be retrieved in the context versus protocols used to construct task problems? I am concerned with potential data leakage if this split is not performed rigorously.\n- In the LAB-Bench experiment, is it possible that the RAG is directly providing actual answers to some of these protocol questions? If the inclusion of these protocols is meant to somehow improve the LLM's ability to fundamentally reason about protocols and procedures, then I would expect a larger improvement on the LAB-Bench task? Could the authors investigate some of the examples (4%) in which the model makes improvements with RAG on the LAB-Bench tasks?\n- The authors mention that models sometimes provide incorrect reasoning traces, but find the correct answer. Can the authors further analyze when such reasoning inconsistencies occur?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6YeBdgoIu1", "forum": "Ry7gUID5ij", "replyto": "Ry7gUID5ij", "signatures": ["ICLR.cc/2026/Conference/Submission3119/Reviewer_uCQp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3119/Reviewer_uCQp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761443153406, "cdate": 1761443153406, "tmdate": 1762916558998, "mdate": 1762916558998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents BioProBench, a comprehensive benchmark for evaluating how well language models understand biological experimental procedures. The authors tested 10 frontier LLMs and found that while these models handle basic comprehension well, they struggle with tasks that require deeper procedural reasoning. They also developed ProAgent, a retrieval-based system that significantly improves performance across all tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an important and timely problem.Benchmarking experimental protocols will be crucial for the development of self-driving labs. i like the focus on procedural knowledge (how to do things) rather than declarative knowledge (facts about things) is both novel and necessary.\n\n2. The dataset construction is rigorous. The authors combined LLMs for extractions with very extensive quality checks. The procedures are recorded in details.\n\n3. The benchmark covers different aspects of experiment protocal understanding, from basic comprehension to complex reasoning about experimental sequences, precision requirements, and safety constraints."}, "weaknesses": {"value": "1. The paper doesn't adequately address potential memorization issues. Since some protocols come from open sources, they might appear in the training data of the models being evaluated. These might make the evlaution over-optimistic. The authors should design some experiments to test data contamination (such as augmenting the questions with different wording and numbers, and compare the results)\n\n2. The train-test split strategy needs more clarity. The paper mentions that answers are extracted from source protocols and uses deduplication, but what prevents more subtle forms of overlap? For the protocol generation task especially, if a full protocol appears in the training set and a related question appears in the test set, performance metrics could be misleadingly high. \n\n3. The embedding-based structural metrics feels a bit ad-hoc and relies on the sentence transformer. Plus, it's not clear to me that semantic similarity really capture procedural correctness. I think an LLM-as-a-judge approach for step-level evaluation makes more sense. \n\n4. I cannot open the provided link in the abstract at the time of writing this review."}, "questions": {"value": "1. How does LLM-as-judge approach correlate with the sentence embedding metrics?\n\n2. How do you prevent cases where a full protocol appears in training (knowledge database) while very similar protocols appear in testing? \n\n3. Is there common pattern of error (which biology domain/ task)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GqbA0odwBY", "forum": "Ry7gUID5ij", "replyto": "Ry7gUID5ij", "signatures": ["ICLR.cc/2026/Conference/Submission3119/Reviewer_beV9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3119/Reviewer_beV9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945281690, "cdate": 1761945281690, "tmdate": 1762916558640, "mdate": 1762916558640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose BioProBench, a benchmark for LLM biological protocol comprehension containing over 550,000 entries from 26933 human written protocols from various sources for 16 subdomains of biology. The performance of various LLMs along with that of a RAG agent on this data was evaluated.Overall, this paper offers a useful benchmark dataset and provides a clear assessment of current LLM model performance. However, its contribution may fall short of the level of novelty typically expected for an ICLR submission."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The dataset is split into 5 different task types which generally follow the skillset that is expected of a human wet lab scientist, so task categorization is well thought out. \n- The authors present a comprehensive performance analysis on multiple evaluation metrics for 10 LLMs, the authors also proposed well thought evaluation metrics for each task type.\n- ProAgent shows improvement in performance across several metrics compared to the best performing LLM for that task type. \n- Dataset, like this is not only useful for evaluating the performance of LLM models for protocol generation but also for evaluating LLMs for biosecurity. So the dataset is a valuable contribution."}, "weaknesses": {"value": "- An important function of a human scientist is performing mathematical calculations over multiple steps for preparing reagent solutions at particular concentration for example, such open ended reasoning capabilities can't be evaluated with the multi-choice QA in protocol question answering."}, "questions": {"value": "Found several typos, that authors should consider fixing to improve the readability of the paper \n\n- Figure 7 - The legend is not adjusted correctly, makes the plot harder to read\n- Line 377 - Protocol Reasonin \n- Line 76 - metrics ot evaluate \n- Line 101 - BIOPROCORPUS COLLECTION AND CLEARNING"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rsEmHkElZo", "forum": "Ry7gUID5ij", "replyto": "Ry7gUID5ij", "signatures": ["ICLR.cc/2026/Conference/Submission3119/Reviewer_LpuE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3119/Reviewer_LpuE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996022390, "cdate": 1761996022390, "tmdate": 1762916558447, "mdate": 1762916558447, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "&nbsp;\n\nThe authors introduce BioProBench, a large dataset of experimental protocols for the biological sciences. On this benchmark, the authors assess the performance of a variety of frontier LLMs. The authors empirical results appear to be correct, the paper is clear and well-written, and the authors have released their code to facilitate the reproducibility of the results presented in the paper. As such, I recommend acceptance with the following points the authors may wish to consider.\n\n&nbsp;"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "&nbsp;\n\nThe main strength of the paper is that it open-sources a useful benchmark in for experimental protocols in the biology domain. I see no issues with the authors' empirical evaluation of frontier LLMs on this benchmark and furthermore, the authors have open-sourced their code to facilitate reproduction of their results. \n\n&nbsp;"}, "weaknesses": {"value": "&nbsp;\n\nI demarcate between major and minor points below.\n\n&nbsp;\n\n**__MAJOR POINTS__**\n\n&nbsp;\n\n1. Given that the authors introduce a new dataset, BioProCorpus, as a resource in the paper it would be worth formally documenting the curation effort as well as the maintenance plan via a framework such as datasheets for datasets [1].\n\n&nbsp;\n\n**__MINOR POINTS__**\n\n&nbsp;\n\n1. There are missing capitalizations in the references e.g. \"GPT\", \"ACL\", and \"LLM\".\n\n2. The anonymous GitHub link provided by the authors appears to be partially broken in so far as individual files do not appear to be viewable. The code is however available in the supplementary material provided by the authors. \n\n3. In terms of the supplied code it would be beneficial if the authors could run an LLM over the code to document it in more detail e.g. by providing descriptions of the arguments to each function in addition to a function-level docstring.\n\n4. The authors should use narrative and parenthetical citations consistently throughout the manuscript e.g. on line 35 the citation to Murthy et al. should be parenthetical e.g. (Murthy et al. 2024) instead of narrative since the author's name does not comprise part of the sentence.\n\n5. Line 76, typo, \"to\".\n\n6. Line 79, typo, \"yet their\".\n\n7. In the contribution section of the introduction it would be beneficial if the acronyms for the task families could be elaborated on.\n\n8. When referencing chain-of-thought prompting, the source paper [2] should be cited.\n\n9. Line 237, the definition of the cosine similarity is not provided.\n\n10. Missing full stops at the end of Equation 1 and Equation 2.\n\n11. Line 363, the authors state that the proxy metric was human-validated. Could the authors provide details on this validation in the main text?\n\n12. Line 377, typo, \"reasoning\".\n\n13. When mentioning zero-shot CoT, the authors should reference the originating work of [3].\n\n14. Line 722, the authors can use the acronym \"LLM\" since it is already defined in the main text.\n\n15. Line 1368, typo, \"additional experiments\" in the section heading.\n\n&nbsp;\n\n**__REFERENCES__**\n\n&nbsp;\n\n[1] Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J.W., Wallach, H., Iii, H.D. and Crawford, K., 2021. [Datasheets for datasets](https://dl.acm.org/doi/fullHtml/10.1145/3458723). Communications of the ACM, 64(12), pp.86-92.\n\n[2] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V. and Zhou, D., 2022. [Chain-of-thought prompting elicits reasoning in large language models](https://proceedings.neurips.cc/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html). Advances in Neural Information Processing Systems, 35, pp.24824-24837.\n\n[3] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y. and Iwasawa, Y., 2022. [Large language models are zero-shot reasoners](https://proceedings.neurips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html). Advances in Neural Information Processing Systems, 35, pp.22199-22213.\n\n&nbsp;"}, "questions": {"value": "&nbsp;\n\nQuestions included in the comments above.\n\n&nbsp;"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "&nbsp;\n\nNo ethical concerns identified.\n\n&nbsp;"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vshFMZjKbm", "forum": "Ry7gUID5ij", "replyto": "Ry7gUID5ij", "signatures": ["ICLR.cc/2026/Conference/Submission3119/Reviewer_C2KZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3119/Reviewer_C2KZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3119/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998303312, "cdate": 1761998303312, "tmdate": 1762916557965, "mdate": 1762916557965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}