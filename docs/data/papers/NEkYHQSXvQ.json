{"id": "NEkYHQSXvQ", "number": 3681, "cdate": 1757496264200, "mdate": 1759898075258, "content": {"title": "AMStraMGRAM : Adaptive Multi-cutoff Strategy Modification for ANaGRAM", "abstract": "Recent works have shown that natural gradient methods can significantly outperform standard optimizers when training physics-informed neural networks (PINNs). In this paper, we analyze the training dynamics of PINNs optimized with ANaGRAM, a natural-gradient-inspired approach employing singular value decomposition with cutoff regularization. Building on this analysis, we propose a multi-cutoff adaptation strategy that further enhances ANaGRAM's performance. Experiments on benchmark PDEs validate the effectiveness of our method, which allows to reach machine precision on some experiments. To provide theoretical grounding, we develop a framework based on spectral theory that explains the necessity of regularization and extend previous shown connections with Green's functions theory.", "tldr": "", "keywords": ["PINNs", "Natural Gradient", "Regularization"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cfe5908defe8deb0eb16dec10b3fe98711456105.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces AMStraMGRAM, an adaptive optimization algorithm designed to improve the training of Physics-Informed Neural Networks (PINNs). The work builds directly upon ANaGRAM, a natural-gradient descent method that uses a fixed cutoff to regularize the singular value decomposition (SVD) of the feature matrix during training. AMStraMGRAM addresses the “flattening phenomenon” by dynamically adjusting the cutoff at each iteration (compared to the fixed cutoff in AnaGRAM). The algorithm tracks the intersection between the RCE curve and the singular values, switching between an \"intersection rank\" and a \"precision rank\" to ensure the training progresses efficiently towards a target precision. This adaptive, multi-cutoff strategy aims to avoid early stagnation and exploit the flattening phenomenon to force a final drop in the training loss. The method is validated on several benchmark PDEs (Heat, Laplace, Allen-Cahn), showing significant performance improvements over ANaGRAM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis of the \"flattening phenomenon\" via the Reconstruction Error (RCE) metric is novel.\n\n2. The paper is theoretically depth as it provides a rigorous functional-analytic framework, explaining why regularization is necessary and drawing a clear distinction between ridge and cutoff regularization.\n\n3. The proposed AMStraMGRAM algorithm is a direct simple but effective modification of the ANaGRAM. The cutoff of the rank is dynamically adjusted, based on the theoretical analysis."}, "weaknesses": {"value": "1. While the theoretical framework is impressive, it primarily provides an explanation for the necessity of regularization and an interpretation of the method. It does not provide rigorous convergence analysis of the training loss for the adaptive cutoff scheme itself. It seems that convergence analysis has been established for natural gradient method to train PINN problems (arXiv:2408.00573) at the same time. So the main incrementation of AMStraMGRAM to ANaGRAM is the adaptive cutoff strategy of the rank, which is too limited for a top conference.\n \n2. The experiments are not comprehensive in today's PINNs’ community. Although the AMStraMGRAM shows ‘L2 error improvements of up to 8 orders of magnitude’ compared to ANaGRAM, it is on very simple Heat and Laplace equation with simple smooth solutions, which is less meaningful. No experiments conducted on complex PINN problems (such as the NS equation with turbulence, KS equation with chaotic property, as in PINNacle NIPS24’) to show the practical improvements over other optimizers on practical problems."}, "questions": {"value": "1. See weakness above.\n\n2.How does the float precision affect the results of AMStraMGRAM? The L2 error is very small. The author uses 64p in Jax, while many works conduct experiments with default 32p in Pytorch.\n\n3.In line 392, why the author compare ANaGRAM with the same equation but with ‘modified dataset’?\n\n4.Is the feature matrix the same as the jacobian matrix J in arXiv:2408.00573? What is the difference between your (10) with existing natural gradient methods (arXiv: 1905.11675, arXiv:2408.00573) besides the SVD decomposition instead of the pesodu-inverse of the jacobian matrix?\n\n5.Can it be combined with approximated natural gradient descent (NGD) methods like F-KAC?  In general deep learning, K-FAC is often used instead of 'exact' NGD.In particular, theoretical analysis of K-FAC has already been developed for shallow neural networks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hKwKHF9t18", "forum": "NEkYHQSXvQ", "replyto": "NEkYHQSXvQ", "signatures": ["ICLR.cc/2026/Conference/Submission3681/Reviewer_yRr7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3681/Reviewer_yRr7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761020780316, "cdate": 1761020780316, "tmdate": 1762916919850, "mdate": 1762916919850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on enhancing the optimization process of Physical Information Neural Networks (PINNs). Specifically, it improves ANaGRAM, an advanced natural gradient-based optimizer. Through an in-depth analysis of ANaGRAM's training dynamics, this paper identifies and defines a critical phenomenon termed “Flattening.” Building upon this theoretical insight, the authors propose AMStraMGRAM, an adaptive multi-cutoff strategy that dynamically and automatically adjusts the cutoff value, thereby eliminating the need for manual tuning of this hyperparameter.\n\nThe contributions of this paper can be divided into three closely interrelated aspects:\n1) **Discovering and Explaining the “Flattening Phenomenon”**: The paper introduces a novel metric called “Reconstruction Error (RCE).” Through RCE, it reveals the training dynamics of ANaGRAM: Fixed truncation hinders or delays this process, whereas adaptive strategies can actively guide and leverage it.\n2) **Proposing the AMStraMGRAM Adaptive Optimizer**: The algorithm can dynamically determine the optimal truncation value for each iteration by tracking the intersection point between the RCE and singular value curve in real time.\n3) **Empirical Performance**: On multiple classical PDE benchmark problems, AMStraMGRAM reduces errors by several orders of magnitude compared to ANaGRAM, achieving machine accuracy on some problems and outperforming other types of advanced optimizers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\t**Identified and defined the “flattening phenomenon” in the training dynamics**: by introducing the novel metric of “reconstruction error,” it clearly reveals the underlying mechanism behind the sudden drop in the loss function during the late stages of ANaGRAM training.\n\n2.\t**Proposed AMStraMGRAM Algorithm**: By dynamically tracking the intersection point between the RCE and singular value curve, the algorithm intelligently selects the truncation rank, thereby avoiding issues of “premature flattening” (training stagnation) or “delayed convergence” (insufficient accuracy) caused by fixed truncation."}, "weaknesses": {"value": "The main issues with the paper are as follows:\n\n1.\tThe entire effort of this paper focuses on the adaptation of a single hyperparameter (the cutoff value) within the optimizer. While executed with great skill, this represents essentially a refinement in algorithmic engineering rather than a breakthrough in scientific concepts.\n\n2.\tWhile standard benchmark PDEs such as Laplace, Burgers, and Allen-Cahn equations are commonly used for testing, they lack more challenging cases. PINNs typically perform poorly for equations with solutions exhibiting large variations or strong multiscale characteristics. Without such tests, it remains unclear whether the method enhances the model's intrinsic fitting capability or merely finds smoother solutions more efficiently.\n\n3.\tAll experiments were conducted within regular domains (e.g., rectangular domains). However, the vast majority of problems in science and engineering are defined over complex geometries (e.g., domains with internal cavities, non-convex regions, or manifolds). On complex domains, collocation sampling and boundary condition treatment themselves pose significant challenges. The paper urgently needs to incorporate experiments on irregular domains to test the robustness of its methods under more realistic settings.\n\n4.\tThe paper demonstrates AMStraMGRAM's significant advantage in final accuracy but overlooks the computational time required to achieve this precision."}, "questions": {"value": "1. AMStraMGRAM introduces a new hyperparameter—target accuracy $\\varepsilon$. How sensitive is the algorithm to the choice of $\\varepsilon$? How does the setting of $\\varepsilon$ relate to accuracy?\n\n2. Section 6 of the paper presents an excellent discussion on overfitting in the Allen-Cahn equation, highlighting the need for “co-designing the optimizer and sampler.” A natural question arises: Can AMStraMGRAM seamlessly collaborate with existing adaptive sampling methods, such as Residual-Adaptive Distribution (RAD)?\n\n3. All experiments in this paper are based on standard multilayer perceptrons. However, many modern PINN studies employ specialized network architectures to mitigate issues such as spectral bias, including Fourier feature networks or modulated networks. Is AMStraMGRAM's adaptive strategy compatible with these architectures? Does its performance improvement depend on specific properties of the MLP? How will testing on networks with different spectral characteristics affect the emergence of the “flattening” phenomenon and the algorithm's final performance? This concerns the method's general applicability.\n\n4. Beyond the overfitting observed in the Allen-Cahn equation, have the authors encountered instances of poor algorithmic performance or failure under other settings—such as varying network width/depth, activation functions, or equation parameters? Could the “decision boundary” of AMStraMGRAP be described more systematically—specifically, under what conditions (problem characteristics, hyperparameter selection) does the algorithm operate stably, and under which conditions might it fail? This is crucial for potential users applying the method in practice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Aeu4FTpZcQ", "forum": "NEkYHQSXvQ", "replyto": "NEkYHQSXvQ", "signatures": ["ICLR.cc/2026/Conference/Submission3681/Reviewer_6JRF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3681/Reviewer_6JRF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761053071970, "cdate": 1761053071970, "tmdate": 1762916918256, "mdate": 1762916918256, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AMStraMGRAM, an adaptive multi-cutoff variant of ANaGRAM, which is a natural-gradient-based optimizer for training Physics-Informed Neural Networks (PINNs). ANaGRAM regularizes the pseudo-inverse of the feature matrix by a fixed cutoff in singular values. Based on this, AMStraMGRAM generalizes it by introducing a dynamic, multi-cutoff strategy that tracks the evolution of the reconstruction error (RCE) and singular values during training."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(1) The work is well motivated, as the adaptive processing for singular values should benefit the natural gradient method.\n\n(2) The paper is well written.\n\n(3) The empirical results are strong that the proposed method significantly improves the accuracy of training PINNs over other optimizers."}, "weaknesses": {"value": "(1) While the intuition behind adaptive cutoff dynamics is persuasive, the paper lacks a formal convergence analysis. Is the algorithm convergent to the stationary points?\n\n(2) AMStraMGRAM requires SVD at every iteration, similar to ANaGRAM. Although it reuses intermediate quantities, a discussion of computational overhead is missing. I think the computational cost for natural gradient is high if the sample size is large.\n\n(3) The algorithm is deterministic. However, due to the limited GPU memory as well as the large sample size, stochastic optimizers are favourable in training PINNs. Is the algorithm valid with stochastic gradients/mini-batch?"}, "questions": {"value": "(1) I found that ANaGRAM exactly resembles classical natural gradient descent. I think the main technique is applying SVD to the matrix and adding the cutoff in the singular values. Besides this, it is exactly the natural gradient descent. Do I understand correctly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dOrUJ70BOR", "forum": "NEkYHQSXvQ", "replyto": "NEkYHQSXvQ", "signatures": ["ICLR.cc/2026/Conference/Submission3681/Reviewer_rjuU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3681/Reviewer_rjuU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803389909, "cdate": 1761803389909, "tmdate": 1762916916069, "mdate": 1762916916069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors analyze and propose an extension of ANaGRAM, a natural gradient method technique for solving PDEs in the physics-informed neural network setting. I have extensively studied the original ANaGRAM work, and the current work represents a non-trivial extension. Notably, the authors are able to obtain convergence \"to machine-precision\". This is a timely paper, as many are attempting to achieve machine-precision accuracy from PINNs. While this is a trivial achievement for conventional finite element solutions to PDEs, PINNs have struggled to achieve this and this is viewed by many as an important stepping stone to achieving AI/ML models with comparable quality of science/engineering-relevance. \n\nFor example, deepmind just launched an effort to develop a numerical counterexample for the millinium prize of finite time blowup for navier-stokes. In contrast, my impression is that this has the desirable feature of being an \"optimization only\" solution that achieves desired accuracy with no ad hoc modification of the loss or architecture. \n\nI have a few suggestions about how the text could be further improved, but overall this is a strong paper and should be accepted."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "See summary. The paper provides a notable and timely contribution for what has been an outstanding issue with PINNs. The technique is well-rooted in analysis and provides a principled solution to the problem."}, "weaknesses": {"value": "As noted, the paper could benefit from some minor modifications.\n\n(1) The work should be contextualized in the context of other methods. The only one to my knowledge that has been able to achieve this accuracy is from Ching-Yao Lai's group at Stanford (Wang, Yongji, and Ching-Yao Lai. \"Multi-stage neural networks: Function approximator of machine precision.\" Journal of Computational Physics 504 (2024): 112865.), which is the technique deepmind used for their work (Wang, Yongji, et al. \"Discovery of Unstable Singularities.\" arXiv preprint arXiv:2509.14185 (2025). These are very recent works but would help frame the importance of the fundamental issue and highlights the way in which PINNs can answer ill-posed questions that are challenging for conventional FEM. The authors may also be interested in (Trask, Nathaniel, Amelia Henriksen, Carianne Martinez, and Eric Cyr. \"Hierarchical partition of unity networks: fast multilevel training.\" In Mathematical and Scientific Machine Learning, pp. 271-286. PMLR, 2022.), where authors achieved machine precision but only for regression problems. All of these works achieve accuracy through **hierarchy**, while the current paper achieves it purely through the optimizer, which is interesting.\n\n(2) One more study should be added to the end to disentangle the accuracy of the solution from the sampling error. Formally I would say this achieves a machine precision **residual**, not **error**. The final plot showing that the error is of comparable wavelength to the grid spacing is an interesting one. The authors could easily add another plot showing the residual (log x axis) vs the L2 error (log y axis) for verifying collocation densities (h = sqrt(N_collocationpoints)). Alternatively, the authors could sample residual points from a uniform distribution on the square at each step of training to avoid grid dependence altogether. If the authors include this study I will further increase my score."}, "questions": {"value": "No questions, great paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "g4zEBqQ58m", "forum": "NEkYHQSXvQ", "replyto": "NEkYHQSXvQ", "signatures": ["ICLR.cc/2026/Conference/Submission3681/Reviewer_7H9Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3681/Reviewer_7H9Y"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3681/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852315374, "cdate": 1761852315374, "tmdate": 1762916914821, "mdate": 1762916914821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}