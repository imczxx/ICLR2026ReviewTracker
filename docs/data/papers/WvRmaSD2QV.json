{"id": "WvRmaSD2QV", "number": 2922, "cdate": 1757301111500, "mdate": 1759898119134, "content": {"title": "Model Editing is Over: Revealing Its Illusory Success and Fragile Foundation", "abstract": "Large language models (LLMs) inevitably encode outdated or incorrect knowledge. Updating/deleting/forgetting such knowledge is important for alignment, safety, and other issues. To address this issue, model editing has emerged as a promising paradigm: by precisely editing a small subset of parameters such that a specific fact is updated while preserving other knowledge. Despite its great success reported in previous papers, we find the apparent reliability of editing rests on a fragile foundation and the current literature is largely driven by illusory success.  The fundamental goal of steering the model’s output toward a target with minimal modification would encourage adversarial shortcuts rather than utilizing real semantics. This problem directly challenges the feasibility of the current model editing literature at its very foundation, as adversarial shortcuts are inherently at odds with robust knowledge integration. Coincidentally, this issue has long been obscured by evaluation frameworks that lack the design of negative examples. To uncover it, we systematically develop a suite of new evaluation methods. Strikingly, we find that state-of-the-art approaches collapse even under the simplest negation queries. Our empirical evidence show that edit is likely to be based on shortcuts rather than full semantics,  calling for an urgent reconsideration of the very basis of model editing before further advancements can be meaningfully pursued.", "tldr": "", "keywords": ["Large language models", "model editing", "adversarial attack"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e683ac7cc24410f540fcec27a90cbaad50ba2648.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper argues that much of the reported “success” in LLM model editing stems from shortcut exploitation rather than genuine semantic integration. \nIt targets the standard locate-then-edit paradigm, identifies a decisive token/layer, and minimally perturbs parameters toward a target hidden state and claims this inherently incentivizes adversarial-style shortcuts over semantics. Two simple evaluations are proposed: (1) negation stress tests that combine positive/negative edit sentences with positive/negative test prompts, and (2) a fact-checking variant where the gold label is True/False rather than the edit string itself. \nAcross Qwen2.5-7B-Instruct and Llama-3-8B-Instruct, many editing methods (e.g., MEMIT, RECT, AlphaEdit, etc.) show very high PP efficacy but similarly high PN/NP scores, suggesting insensitivity to negation; fact-checking accuracies are much lower than PP “efficacy.” The paper concludes that current model editing rests on a fragile foundation and that evaluation should include semantically complementary negatives."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear problem framing. The paper is well-written and the motivation—testing whether edits capture meaning rather than form—is intuitively strong.\n\n- Negation and true/false checks are easy to reproduce and highlight an important gap in how we assess model editing."}, "weaknesses": {"value": "- Overlap with existing robustness studies (limited novelty): The central claim (“model editing success is illusory under semantically perturbed queries”) has already been demonstrated in several closely related and more rigorous works, e.g., EMNLP2024 On the Robustness of Editing Large Language Models (https://aclanthology.org/2024.emnlp-main.906.pdf), prompt engineering for attacking the edits. Moreover, there is already mechanistic work going further to study why already: Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing (https://arxiv.org/pdf/2505.12636)\n\n- This paper provides a useful replication and an accessible benchmark for evaluating the robustness of locate-then-edit methods, but it does not break new conceptual ground.\nThe related work on RAG is misleading, RAG is an inference pipeline, not a model-editing paradigm or model updating method, and more valuable discussion would instead focus on emerging non-locate-then-edit editors (hypernetwork, adapter, or inference-time). If reframed as a benchmark extension clarifying the limits of weight-space editing rather than declaring the paradigm dead, the work could become a constructive contribution to the field.\n\n- While the experiments reveal brittleness, the rhetoric (“Model editing is over”) is scientifically excessive. A more balanced interpretation is that current locate-then-edit methods lack semantic robustness, while alternative paradigms (hypernetworks, adapters, inference-time edits) may still hold promise."}, "questions": {"value": "none"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vnOuYz6qE7", "forum": "WvRmaSD2QV", "replyto": "WvRmaSD2QV", "signatures": ["ICLR.cc/2026/Conference/Submission2922/Reviewer_9sBF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2922/Reviewer_9sBF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760716141616, "cdate": 1760716141616, "tmdate": 1762916443966, "mdate": 1762916443966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper gives a examination of model editing, arguing that the reported success of editing methods is largely illusory. The authors claim that existing editing techniques rely on adversarial shortcuts—non-semantic correlations that enable models to output the desired edited fact without understanding or integrating it. The paper introduces two new evaluation settings: a negation test and a fact-checking test. Across multiple datasets and 2 models (Llama3-8B, Qwen2.5-7B), evaluated editing methods perform poorly under these new tests."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper provides a reality check on model editing research, questioning whether its empirical progress reflects genuine knowledge integration. The connection drawn between model editing and adversarial shortcut exploitation might be useful.\n- By introducing negation and fact-checking evaluations, the authors expose hidden weaknesses in editing benchmarks. These tests are conceptually simple but useful in demonstrating fragility.\n- The paper evaluates major editing methods across multiple LLM architectures and datasets, offering robust empirical evidence for its caims."}, "weaknesses": {"value": "- The central claim that “model editing is over” are exaggerated and overstated. The evidence indeed shows weaknesses in current benchmarks and methods, but limited evaluation on 2 small models does not warrant declaring the entire field invalid. \n- Second, the study’s findings may not be entirely attributable to editing itself. For fair comparison, the authors should also have included baseline results for all four proposed evaluation types before editing, since some observed failures could stem from the inherent way LLMs recite or retrieve knowledge rather than the editing mechanisms. \n- Third, the paper does not adequately account for the fact that LLMs are known to be highly sensitive to question format and phrasing. Including additional evaluation types, such as short-answer QA or multiple-choice questions, would provide a fairer and more comprehensive assessment of whether the observed brittleness truly arises from editing. \n- the paper’s anonymous GitHub link does not correctly display or load the code,\n- The authors’ claim that “supportive tokens like ‘is’ / ‘is not’ play little role at edit time” may not hold universally. This phenomenon could result from the limited reasoning and linguistic understanding capacity of smaller models such as Llama-8B, rather than a general flaw of the editing paradigm. It remains doubtful that larger, more capable frontier models would exhibit the same deficiencies."}, "questions": {"value": "- Could you provide results for your four evaluation types (PP, PN, NN, NP, and fact-checking) before any editing is applied? This would help determine whether the observed failures stem from the editing process or from preexisting LLM limitations in handling negation and fact verification.\n- Since LLMs are known to be sensitive to prompt format, did you test whether results vary when using alternative formulations, such as multiple-choice or paraphrased prompts? Additionally, do you expect the same fragility in larger models (e.g., llama-13b or llama-70b)? \n- The paper attributes the observed insensitivity to “supportive tokens” (like “is” vs. “is not”) to the editing mechanism itself. Could this instead reflect the model’s limited contextual comprehension rather than the edit? A more controlled analysis isolating token-level effects would strengthen the claim.\n- How do you separate the effects of editing-induced shortcuts from general weaknesses in the model’s semantic reasoning? Some of failures cases (especially in fact-checking) might reflect general LLM shortcomings rather than a specific flaw in the editing procedure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "raMnwjHh1e", "forum": "WvRmaSD2QV", "replyto": "WvRmaSD2QV", "signatures": ["ICLR.cc/2026/Conference/Submission2922/Reviewer_81Hu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2922/Reviewer_81Hu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761690735727, "cdate": 1761690735727, "tmdate": 1762916443746, "mdate": 1762916443746, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This is an interesting study that raises fundamental questions about the mainstream evaluation methods and core mechanisms in the field of Large Language Model (LLM) knowledge editing. The authors use simple yet ingenious experiments (negation queries and fact-checking) to compellingly demonstrate that existing SOTA methods rely primarily on \"adversarial shortcuts\" rather than genuine semantic knowledge integration. The experiments show surprising results for the current methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a crucial, long-overlooked defect in the model editing literature—the lack of \"negative case\" evaluation. It boldly challenges the reported success of mainstream methods, pointing the way toward more robust directions for future research in the field.\n\n2. The proposed \"Simple Negation Test\" (PN/NP) and \"Fact-Checking Style Evaluation\" are interesting and useful. These methods are simple in design but effective at exposing severe deficiencies in the semantic completeness and robustness of current methods.\n\n3. The paper extensively validates its claims across two mainstream LLMs (Llama-3-8B-Instruct, Qwen2.5-7B-Instruct) and nine SOTA editing methods, ensuring the universality of its conclusions."}, "weaknesses": {"value": "1. In the fact-checking experiments, the model switches from generating facts (the original knowledge editing task) to judging truthfulness (the new task). Does this task switching itself introduce confounding factors? Although the authors state that \"the two evaluation tasks are roughly comparable in difficulty,\" it might be worth further discussion or including a control experiment to rule out the influence of task conversion on the results, thereby ensuring the performance drop is solely attributable to the failure of semantic integration. \n\n2. Although the proposed insight is interesting, the paper does not attempt to solve this problem or discuss how to solve this problem. And the title is kind of histrionic or slightly aggressive. Given that this paper aims to advance the field, it is suggested that the conclusion section be made more constructive\n\n3. Although the paper provides a strong analogy, a deeper mechanistic analysis is needed regarding why the \"locate-then-edit\" optimization objective (Eq. 3) necessarily leads to this shortcut behavior. For instance, why does intervention on the decisive token's hidden state actively neglect supportive tokens in the context (e.g., \"is/is not\")? Providing a microscopic explanation based on gradients or attention mechanisms would significantly strengthen the paper's foundation."}, "questions": {"value": "Q1: What can the Discrepancy in Tables 2 and 3 indicate? It seems the metric could not reveal any insights. \n\nQ2: The author claims they implement them with our improved version. What about the performance of the original methods in the evaluation? It is not very convincing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wgtXttp3yz", "forum": "WvRmaSD2QV", "replyto": "WvRmaSD2QV", "signatures": ["ICLR.cc/2026/Conference/Submission2922/Reviewer_eq5o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2922/Reviewer_eq5o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725313226, "cdate": 1761725313226, "tmdate": 1762916443420, "mdate": 1762916443420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a profound and critical challenge to the foundations of the rapidly growing field of model editing for Large Language Models (LLMs). The authors argue that the apparent high success rates of current model editing techniques, as measured by established benchmarks, are largely illusory and built upon a fragile foundation. Their central thesis is that the core objective of model editing—to steer the model's output to a target with minimal parameter changes—inherently encourages the model to learn \"adversarial shortcuts.\" This means the model forms a superficial association between a trigger pattern (e.g., the subject token) and the target answer, bypassing a genuine understanding and integration of the knowledge's full semantics.\n\nTo substantiate this claim, the authors introduce a novel evaluation framework that systematically incorporates ​negative cases. This includes:\n1. ​Simple Negation Queries: Testing edited models with negated versions of the original query (e.g., \"XX is not\" instead of \"XX is\"). Strikingly, the models still confidently output the edit target, demonstrating a failure to comprehend the logical negation.\n​2. Fact-Checking Evaluation: Requiring the model to judge the truthfulness of a statement containing the edited fact, rather than directly generating it. This reveals a significant performance drop compared to standard generation-based evaluation.\nThrough extensive experiments on two base LLMs (Llama3-8B and Qwen2.5-7B) involving nine state-of-the-art editing methods across four standard datasets, the paper provides compelling evidence. The results consistently show that all methods collapse under negation queries and perform poorly on fact-checking, strongly supporting the authors' contention that current editing paradigms rely on shortcuts rather than robust semantic integration. The paper concludes by calling for a fundamental re-examination of the field's evaluation practices and underlying assumptions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1 ​Paradigm-Challenging Perspective: The paper successfully reframes model editing as a potential instance of adversarial shortcut learning, providing a new lens through which to evaluate editing techniques.\n​2 Methodological Contribution: The proposed negative-case evaluation framework addresses a critical gap in current benchmarking practices and sets a new standard for robustness assessment.\n3 ​Rigorous Experimental Design: The comprehensive evaluation across methods, models, and datasets ensures the findings are generalizable and not method-specific."}, "weaknesses": {"value": "​1. Mechanistic Explanation: The paper demonstrates the existence of shortcuts but lacks a detailed analysis of their internal mechanisms. For example, do edits primarily alter attention patterns in specific layers or disrupt logical operations (e.g., negation handling) in feedforward networks? Incorporating neuron-level analyses (e.g., causal tracing post-edit) could clarify how shortcuts manifest.\n2. ​Evaluation Confounders: The negation-based tests assume LLMs can inherently handle negation, but baseline performance on negation tasks is not benchmarked. If vanilla models struggle with negation, the editing-specific failure may be overstated. A control experiment testing negation understanding in unedited models would strengthen causality.\n​3. Paradigm Boundaries: The critique focuses on \"locate-then-edit\" methods but does not dissect how alternative approaches (e.g., hypernetworks or external modules) might avoid these pitfalls. Clarifying whether the issue is paradigm-specific or universal would refine the paper’s scope.\n4. ​Practical Implications: The experiments use simplified settings; assessing whether shortcuts harm real-world tasks (e.g., multi-hop reasoning post-edit) would amplify the work’s applicability."}, "questions": {"value": "1. Could you elaborate on the analogy between model editing and adversarial attacks? Specifically, how do shortcuts in parameter space(editing) differ from those in input space(attacks), and does this suggest unique mitigation strategies?\n2. The results show consistent output of the edit target across all query types. Does this imply that edits weaken the model’s semantic understanding of predicates (e.g., \"is\" vs. \"is not\")? Is there evidence of degraded logical reasoning post-edit?\n3. How might future editing paradigms balance precision and semantic completeness? For instance, could incorporating negative examples during editing or using logic-based constraints help?\n4. Does the failure under negation queries generalize to more complex logical forms (e.g., quantifiers like \"never\" or \"always\")?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gSicAnzT7k", "forum": "WvRmaSD2QV", "replyto": "WvRmaSD2QV", "signatures": ["ICLR.cc/2026/Conference/Submission2922/Reviewer_g7i2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2922/Reviewer_g7i2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2922/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762094999317, "cdate": 1762094999317, "tmdate": 1762916443240, "mdate": 1762916443240, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}