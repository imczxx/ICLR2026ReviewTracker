{"id": "gPL2LNoKIF", "number": 13449, "cdate": 1758218066180, "mdate": 1759897436961, "content": {"title": "Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization", "abstract": "Synthetic data is a core component of data-efficient Dyna-style model-based reinforcement learning, yet it can also degrade performance. We study when it helps, where it fails, and why, and we show that addressing the resulting failure modes enables policy improvement that was previously unattainable. We focus on Model-Based Policy Optimization (MBPO), which performs actor and critic updates using synthetic action counterfactuals. Despite reports of strong and generalizable sample-efficiency gains in OpenAI Gym, recent work shows that MBPO often underperforms its model-free counterpart, Soft Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites involve continuous control with proprioceptive robots, this shift leads to sharp performance losses across seven challenging DMC tasks, with MBPO failing in cases where claims of generalization from Gym would imply success. This reveals how environment-specific assumptions can become implicitly encoded into algorithm design when evaluation is limited. We identify two coupled issues behind these failures: scale mismatches between dynamics and reward models that induce critic underestimation and hinder policy improvement during model-policy coevolution, and a poor choice of target representation that inflates model variance and produces error-prone counterfactuals in MBPO rollouts. Addressing these failure modes enables policy improvement where none was previously possible, allowing MBPO to outperform SAC in five of seven tasks while preserving the strong performance previously reported in OpenAI Gym. Rather than aiming only for incremental average gains, we hope our findings motivate the community to develop taxonomies that tie MDP task- and environment-level structure to algorithmic failure modes, pursue unified solutions where possible, and clarify how benchmark choices ultimately shape the conditions under which algorithms generalize.", "tldr": "We show why synthetic data causes Model-Based Policy Optimization to fail in seemingly similar tasks and provide simple, general fixes that restore performance across benchmarks.", "keywords": ["sample efficiency", "robustness", "generalization", "synthetic data", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/047653a74d60d1478a2c6b724ad823862b514823.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduces two targeted modifications: target unit normalization (independent for next-state and rewards) and direct next-state prediction. the result shows good improvement on model based policy optimization"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The finding that residual prediction—a standard best practice in many MBRL approaches—actually degrades performance in DMC by inflating variance is a valuable, counter-intuitive insight"}, "weaknesses": {"value": "Novelty is limited. the method introduced in the paper look like small tricks to make it work. But disclaimer, i am not an expert on this as well so i could be wrong."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "gjTUBpys9u", "forum": "gPL2LNoKIF", "replyto": "gPL2LNoKIF", "signatures": ["ICLR.cc/2026/Conference/Submission13449/Reviewer_92QQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13449/Reviewer_92QQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452496271, "cdate": 1761452496271, "tmdate": 1762924072334, "mdate": 1762924072334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors identify two issues behind using synthetic data in Dyna-style model-based RL based on evaluation in OpenAI Gym and DMC.  They propose simple modifications to address these issues which has improved performance across the benchmark tasks.  More importantly they highlight how algorithm design is effected by specific environments: MBPO performs differently between the two similar benchmark suites, and how popular benchmarks can mask weaknesses in algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The authors point out a very important pitfall with an interesting case-study.  Especially the broader implications section of this paper could have an impact on RL research and how we evaluate performance.\n2. Sound failure analysis of MBPO resulting in two easy fixes.\n3. Strong empirical paper which can lead to more stable MBPO implementations."}, "weaknesses": {"value": "1. The authors may be falling into the same trap they warn against: their implementation recommendations are based on improving performance in just 5 tasks in the DMC suite.  While I can see reward normalization being useful across the board, the second implementation detail of directly predicting states instead of residuals may depend on the task and environment."}, "questions": {"value": "1. In predicting residuals vs. direct state, what are the \"unstable\" regimes where residuals can be less stable?  Why are they more prone in DMC tasks than Gym?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5Z15ZFCpgX", "forum": "gPL2LNoKIF", "replyto": "gPL2LNoKIF", "signatures": ["ICLR.cc/2026/Conference/Submission13449/Reviewer_tCpX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13449/Reviewer_tCpX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761531624894, "cdate": 1761531624894, "tmdate": 1762924072047, "mdate": 1762924072047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyses why Model-Based Policy Optimisation (MBPO) succeeds in OpenAI Gym but fails in the DeepMind Control Suite (DMC).\nIt attributes this to two coupled issues: a scale mismatch between dynamics and reward targets causing critic underestimation, and variance inflation from residual prediction.\nTwo simple fixes—target normalisation and direct next-state prediction—form Fixing That Free Lunch (FTFL), which restores MBPO’s performance on DMC while preserving Gym results.\nThe work calls for a taxonomy of RL failure modes connecting environment structure to algorithmic design."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear empirical diagnosis: The study provides a well-structured causal chain from scale mismatch → reward bias → critic underestimation, supported by clean pseudo-online experiments.\n- Reproducibility and clarity: The methodology is transparent, and appendices provide full hyperparameters and ablations.\n- Meaningful reflection: The discussion on algorithm–environment co-evolution and the call for a taxonomy of failure modes are thoughtful and broadly relevant."}, "weaknesses": {"value": "While the diagnostic analysis is careful and the experiments are thorough, the conceptual novelty and empirical comparability are limited.\nThe proposed fixes are straightforward engineering choices long known in RL practice, and the claim that MBPO “fails” relative to its original results is not convincingly isolated from benchmark and implementation differences.\n\n1. **Limited novelty of the “fixes.”**\n   Target normalisation and direct next-state prediction are long-established stabilisation tricks; many prior MBRL works implicitly use them.\n   The paper’s value lies in the diagnosis, not in methodological innovation, which limits its contribution beyond a case study.\n\n2. **Questionable baseline comparability.**\n   The paper’s “MBPO failure” results differ sharply from the **original MBPO paper (Janner et al., 2019)**, where MBPO already outperformed SAC by large margins even at far fewer samples.\n   Because DMC tasks differ in reward scale, termination, and observation structure—and because the authors use a simplified one-step rollout implementation from STFL—the evidence that MBPO *intrinsically* fails is ambiguous.\n   A direct reproduction of the original setup under matched conditions would be necessary to substantiate the claim.\n\n3. **Scope of validation.**\n   Only continuous-control DMC tasks are tested. The argument that FTFL “fixes synthetic-data failures” would be stronger with additional evidence from stochastic or visual domains.\n\n-----------------------\n- Minor point: **Figure 1 presentation issues.**\n   The green-to-red background visually implies significance but does not display any variance or confidence intervals.\n   Since IQM normalisation already gives a robust mean, the figure should report dispersion—e.g., boxplots or bootstrapped CIs—instead of a stylised heatmap."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FQqrmerCN0", "forum": "gPL2LNoKIF", "replyto": "gPL2LNoKIF", "signatures": ["ICLR.cc/2026/Conference/Submission13449/Reviewer_Scne"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13449/Reviewer_Scne"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761796110324, "cdate": 1761796110324, "tmdate": 1762924071475, "mdate": 1762924071475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the 7 Deepmind Control (DMC) tasks where MBPO underperforms the SAC algorithm, as MBPO doesn't even learn better than a random policy. They identify two coupled issues behind these failures: scale mismatches between dynamics and reward models and the poor choice of using residual state prediction in the transition model. Addressing these failure modes enables policy improvement where none was previously possible, allowing MBPO to outperform SAC in five of seven tasks while preserving the strong performance previously reported in OpenAI Gym."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper approaches fundamental limitations in algorithms like MBPO by specific hypotheses and experiments that better help understand MBPO and its performance relationship against SAC.\n- The analyses is well-motivated, well-executed, and well-described."}, "weaknesses": {"value": "1. My primary concern is the niche focus of this paper on making MBPO work on a few environments in the DMC benchmark. Do the insights about why MBPO failed completely in STFL transfer to other benchmark tasks beyond DMC, so the insights are generalizable about the algorithm and not specific to the benchmark task itself? In other words, how does this takeaway apply to other applications employing MBPO? When the authors claim \"This demonstrates that our modifications are not a DMC-specific adjustment, but a robust improvement that generalizes across different benchmark suites\", they only show that FTFL ~ MBPO on other environments. However, the real test of generalization of the hypotheses tested in this paper is if there exist other non-DMC environments where MBPO fails but FTFL resolves the two identified issues in MBPO in a similar manner.\n2. While the analysis is impressive, the proposed solutions are scale normalization between dynamics and reward models, and removing the residual state prediction — which are domain-specific engineering solutions. These are by no means minor contributions, however, I don't consider them justified to be major novel contributions either — especially given the limited applicability of these solutions for MBPO in DMC only."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MeiyExMDa9", "forum": "gPL2LNoKIF", "replyto": "gPL2LNoKIF", "signatures": ["ICLR.cc/2026/Conference/Submission13449/Reviewer_5t9a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13449/Reviewer_5t9a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13449/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893104872, "cdate": 1761893104872, "tmdate": 1762924070970, "mdate": 1762924070970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}