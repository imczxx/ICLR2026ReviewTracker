{"id": "r00UxTl8El", "number": 10410, "cdate": 1758170576353, "mdate": 1763740880312, "content": {"title": "LinguaMap: Which Layers of LLMs Speak Your Language and How to Tune Them?", "abstract": "Despite multilingual pretraining, large language models often struggle with non-English tasks, particularly in language control--the ability to respond in the intended language. We identify and characterize two key failure modes: the *multilingual transfer bottleneck* (correct language, incorrect task response) and the *language consistency bottleneck* (correct task response, wrong language). To systematically surface these issues, we design a four-scenario evaluation protocol spanning MMLU, MGSM, and XQuAD benchmarks. \nTo probe these issues with interpretability, we extend logit lens analysis to track language probabilities layer by layer and compute cross-lingual semantic similarity of hidden states. The results reveal a three-phase internal structure: early layers align inputs into shared semantic space, middle layers perform task reasoning, and late layers drive language-specific generation. Guided by these insights, we introduce *selective fine-tuning* of only the final layers responsible for language control. On Qwen-3-32B and Bloom-7.1B, this method achieves over 98% language consistency across six languages while fine-tuning only 3–5% of parameters, without sacrificing task accuracy. Importantly, this result is nearly identical to that of full-scope fine-tuning (e.g., $>98\\%$ language consistency for both methods across all prompt scenarios) but uses a fraction of the computational resources. To the best of our knowledge, this is the first approach to leverage *layer-localization of language control* for efficient multilingual adaptation.", "tldr": "", "keywords": ["Multilingual Language Models", "Language Consistency", "Cross-lingual Transfer", "Interpretability", "Logit Lens", "Semantic Similarity", "Layerwise Fine-Tuning", "Output Space Control", "Model Analysis", "Language Control"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ce0a3d3314e121f791be45b95838d907a2783fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper describes an extension to the logit lens interpretability approach to understand language representations in multimodal LLMs and identify layers that would benefit from fine-tuning to improve language control without degrading the underlying performance. The analysis focusses on two models from two families (Qwen and Bloom)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Improving performance and control of multilingual LLMs is an important topic, especially for ensuring that all users of models, regardless of language, have an equivalent experience.\n- The approach offers a method that avoids compute-expensive fine-tuning. The results suggest that only 5% or fewer parameters need to be tuned."}, "weaknesses": {"value": "- The paper considers only two models from different families, and these are not of comparable size. Specifically the Qwen model is 32B parameters whilst the Bloom model is 7B. \n- There are statements making assumptions about architectures (e.g., architectures like Qwen favor task success) but it is difficult to know if these statements are generally true with the question about the model size differences. Also, if numbers are reported from a single run then again we do not know if the observations are an artifact of that one run or if they hold true more generally."}, "questions": {"value": "- The abstract states that 98% language consistency whilst fine-tuning only 3-5% of the parameters — it would be beneficial to know how this compared to full fine-tuning. It is stated later in the paper (Line 087) so move this up into the abstract.\n- Why not consider models of equivalent size from different families, or different sizes within the same family?\n- Line 111: check the opening quotation marks.\n- Line 180: the equation is not needed.\n- In Table 1, the poor task performance for Bloom suggests this may just be a poor model, in which case how reliable are findings drawn from this. It seems an especially poor choice of model for the MGSM task.\n- Line 249: What should be subscript “i” is not subscript.\n- Line 255: The variable N is being reused as the number of tokens, where it was previous used for the number of samples in the evaluation set. Variables should not have multiple meanings to avoid ambiguity.\n- Line 259: talks about comparing n-gram profiles but it is not clear how. It would help to forward reference where this is discussed. Likewise it is not clear where the pre-trained language profiles are from.\n- Line 276: N appears again — this this a third definition, or is this original use (size of the evaluation set).\n- Is the language similarity score in Equation 6 not also sensitive to the specific content to? Say you took large batches of sentences for the same language — how would these similarity scores relate to the scores for different languages?\n- Line 425: Since optimality was reached after five of five epochs, why not run more to see of performance continues to improve?\n- Line 431: “Table 2 indicate that” > “Table 2 indicates that”\n- Line 458: Is “full-scope SFT” the same thing as “full fine-tuning”? Use a consistent name throughout the paper.\n- Lines 480: check the opening quotes."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2hTldfKeCE", "forum": "r00UxTl8El", "replyto": "r00UxTl8El", "signatures": ["ICLR.cc/2026/Conference/Submission10410/Reviewer_ha32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10410/Reviewer_ha32"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956430963, "cdate": 1761956430963, "tmdate": 1762921721649, "mdate": 1762921721649, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how multilingual large language models manage their ability to generate in the intended language. The authors identify two failure modes - multilingual transfer bottleneck and language consistency bottleneck. The authors extend the logit lens technique to measure language probability trajectories across layers and apply hidden-state cosine similarity to quantify cross-lingual alignment. The authors also propose selective supervised finetuning (Selective SFT), tuning only the last few layers to restore language control efficiently. The method is empirically validated by achieving a high language consistency across six languages on Qwen-3-32B and Bloom-7.1B while fine-tuning only 3–5% of parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper makes a strong interpretability-driven contribution, linking layer-wise representational dynamics to achieve multilingual control. The integration of logit lens analysis with hidden-state similarity profiling provides a compelling explanation for language drift. The selective fine-tuning strategy seems intuitive and computationally efficient, and demonstrates that language-specific control can be restored without retraining the full model."}, "weaknesses": {"value": "While the results are compelling, several aspects of the methodology needs further clarification. The precise criterion for identifying layer boundaries (e.g., layer 55 for Qwen-3-32B) is not fully justified. This raises uncertainty about whether these thresholds are architecture-specific or emergent from model dynamics. The mean-pooled cosine similarity metric may obscure finer token-level divergences, leaving open how exactly semantic alignment transitions into language control. Similarly, Bloom’s variance in cross-language probability trajectories (Figure 2) suggests that underlying architectural or tokenizer-level factors might influence the emergence of language control more than the analysis captures. The selective fine-tuning procedure itself (particularly how the tuned layers were chosen and validated) is somewhat heuristic. Finally, while the post-finetuning improvements in both language consistency and reasoning accuracy are clear, the mechanism behind this dual gain is underexplored."}, "questions": {"value": "1.\tIn Section 4.2.1, it is mentioned that Qwen-3-32B’s target-language probabilities rise only after layer 55. How did the authors determine that this boundary (layer 55) marks the transition to language-specific control, and is this threshold consistent across tasks or languages?\n2.\tThe hidden-state cosine similarity (Eqs 6–7) uses mean-pooled token embeddings. Were layer-wise token-level divergences (eg - in attention focus or contextual span) observed, that might provide finer evidence for the semantic–reasoning–language transition?\n3.\tIn Fig 2, Bloom’s target-language probabilities exhibit high variance across layers. What could this be due to?\n4.\tIn computing language probabilities via Eqs. 2–4, how was multilingual token overlap handled, especially in cases where shared alphabets might bias the language identification model?\n5.\tSelective SFT fine-tunes the last one or two layers depending on the model. What empirical/diagnostic signals indicated that these layers were most responsible for language control? \n6.\tIn Table 2, task accuracy sometimes improves after selective fine-tuning. Why is it that adjusting the final layers for language control also improves reasoning accuracy?\n7.\tUnder code-switched prompting, Qwen fails to “re-ground” target language probabilities. Did the logit-lens traces show any mid-layer oscillation patterns indicating instability in language identity propagation?\n8.\tGiven that the similarity analyses reveal language-invariant middle layers, did the authors check whether selective fine-tuning altered these alignments? That is, did language control adjustments propagate backward into semantically aligned layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Df9Xi071wH", "forum": "r00UxTl8El", "replyto": "r00UxTl8El", "signatures": ["ICLR.cc/2026/Conference/Submission10410/Reviewer_XGPX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10410/Reviewer_XGPX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762138470089, "cdate": 1762138470089, "tmdate": 1762921721174, "mdate": 1762921721174, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a central gap in multilingual large language models (mLLMs): language control, the ability to generate responses in the intended target language. It identifies two primary failure modes: the multilingual transfer bottleneck (correct language, incorrect task) and the language consistency bottleneck (correct task, wrong language). To systematically study these, the authors propose a diagnostic framework with four controlled prompting scenarios spanning tasks from MMLU, MGSM, and XQuAD. They then use interpretability techniques, layer-wise logit lens decoding and hidden-state similarity analysis, to trace where language control emerges across the model’s depth. The results reveal a three-phase internal organization: early layers align inputs semantically across languages, middle layers handle reasoning, and late layers drive language-specific generation.\n\nBuilding on this insight, the authors introduce Selective Fine-Tuning (SFT), which updates only the last few layers responsible for language control while freezing the rest. Applied to Qwen-3-32B and BLOOM-7.1B, this method improves language consistency from below 20% to over 98% across six languages while fine-tuning just 3–5% of model parameters, with minimal loss in task accuracy. The work presents both a structural understanding of multilingual layer specialization and a practical, parameter-efficient tuning method for controlling language generation."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Novel diagnostic framework for multilingual failure modes: The four-scenario prompting setup provides a well-structured and reproducible way to disentangle language control from task accuracy.\n\n- Insightful interpretability analysis: The paper convincingly demonstrates a three-phase structure across layers, linking representational alignment to functional behavior in multilingual settings.\n\n- Strong empirical improvements with minimal compute cost: Selective fine-tuning significantly enhances language consistency while preserving task performance, requiring only 3–5% of parameters to be trained.\n\n- Clarity and completeness: The paper clearly presents its experimental design, prompt templates, and evaluation results, including extensive per-language tables and ablations.\n\n- Practical impact: The proposed approach offers a scalable path to adapt existing mLLMs for multilingual deployment without full retraining or specialized data."}, "weaknesses": {"value": "- Limited novelty in fine-tuning method: While the interpretability analysis is insightful, the proposed selective tuning strategy builds on well-established parameter-efficient fine-tuning concepts and is not fundamentally new.\n\n- Narrow evaluation scope: The study focuses on only two models (Qwen-3-32B and BLOOM-7.1B) and a limited set of languages. Broader coverage across typologically diverse languages or other architectures would strengthen generalization claims.\n\n- No comparison with alternative lightweight methods: The paper does not benchmark against LoRA, adapters, or middle-layer alignment approaches, which would contextualize the gains from selective SFT.\n\n- Interpretability analysis could be deeper: The layer-wise similarity and logit lens analyses, while descriptive, remain qualitative. A more quantitative measure of where “language control neurons” reside would enhance rigor.\n\n- Limited real-world evaluation: The framework is confined to academic benchmarks, lacking demonstrations on open-ended generation, code-mixing robustness, or human evaluations."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w9eVXkNhcW", "forum": "r00UxTl8El", "replyto": "r00UxTl8El", "signatures": ["ICLR.cc/2026/Conference/Submission10410/Reviewer_X6JL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10410/Reviewer_X6JL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10410/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762297810244, "cdate": 1762297810244, "tmdate": 1762921720722, "mdate": 1762921720722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}