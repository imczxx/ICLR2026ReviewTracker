{"id": "bpbU549sSg", "number": 11358, "cdate": 1758197330250, "mdate": 1763463916534, "content": {"title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity", "abstract": "Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training.\nWhile Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime.\nWe conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment.\nFirst, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T).\nSecond, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. \nFinally, we analyze inference-time scaling characteristics.\nOur findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers.\nNotably, xLSTM models consistently Pareto-dominate Transformer models, delivering lower cross-entropy loss for the same compute budget.", "tldr": "Scaling laws for linear time-complexity xLSTM model, comparing against Transformers for both training and inference.", "keywords": ["xLSTM", "Transformers", "Scaling Laws", "Sequence Modeling", "TFLA", "Linear Attention", "Inference"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e5d18c77e678124cd438ff75bf6a81d905898a1a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper points out that the authors systematically compared the scaling laws of the performance-optimized xLSTM architecture with those of the dense multi-head self-attention Transformer architecture. The authors conducted comparative analyses from perspectives such as Training, Context length, and Inference."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-written, with clear logic and concise readability.\n\n2. It conducts extensive experiments, including comparative experiments on models of different types and scales.\n\n3. The authors provide a wide range of comprehensive evaluation metrics."}, "weaknesses": {"value": "1. First, this paper lacks sufficient novelty. Models based on the xLSTM architecture have already demonstrated significant advantages in areas such as inference in previous studies. However, this paper only conducts extended comparative experiments based on this existing foundation, which casts doubt on its innovativeness.\n\n2. Second, compared to models with quadratic complexity, the design of LLM architectures based on linear complexity is inherently intended to reduce computational costs and achieve more significant benefits during training and inference—and this is a well-known fact. The paper conducts comparative analyses from perspectives including Training, Context length, and Inference, but these experiments are carried out under the premise that such performance characteristics are already known, resulting in insufficient contributions from the paper.\n\n3. Finally, to fully compare the performance of models with linear complexity and quadratic complexity, limiting the scope solely to the xLSTM architecture is flawed. It is necessary to include a broader range of models and conduct more comprehensive result comparisons."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xQgPjOqE8X", "forum": "bpbU549sSg", "replyto": "bpbU549sSg", "signatures": ["ICLR.cc/2026/Conference/Submission11358/Reviewer_H1t8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11358/Reviewer_H1t8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11358/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760607106768, "cdate": 1760607106768, "tmdate": 1762922489831, "mdate": 1762922489831, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study scaling behavior of a recently proposed sequence modeling architecture, finding it does well on many desirable axes compared to Transformer baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Study relevant regimes, incl. overtraining/inference time compute which are rarely paid attention to in scaling work even though they are principle considerations when training/serving modern frontier models. \n- Science of scaling is cleanly and thoroughly done. Reproduction of important past work (power-law exponents from Chinchilla) is reassuring as a sanity check. \n- Lots of large-scale empirics and careful parametric fits, very valuable contribution."}, "weaknesses": {"value": "- Transformer baseline is weak and out of date (Llama-2 architecture in late 2025). There are a number of Transformer++ improvements that have appeared since Llama-2, all else fixed, that make big improvements to performance at scale (eg. RoPE, GQA, addressing attention sinks, no biases on linear layers, etc etc). \n- I'm not convinced xLSTM actually outscale Transformers, and in fact would be willing to bet they do worse at frontier compute regimes. We can see the advantage decreasing with compute in Fig1(left), and we can see the faint Transformer curves overtaking the xLSTM ones on Fig1(right). This looks exactly like Fig4 of the Mamba [1] paper where Mamba was touted as \"scaling favorably\" but the gap shrunk slightly with compute in their plots. When things were scaled up, of course Transformers did much better than Mamba. I expect a similar thing to hold here (notice how the plot is missing Transformer at 1e23 where xLSTM begins to plateau, I would be very interested in seeing that last Transformer data point). Nonetheless the thorough and expansive empirics here are valuable, and this is how progress in architecture design is made, so this is a valuable paper."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4qvBLt2q5e", "forum": "bpbU549sSg", "replyto": "bpbU549sSg", "signatures": ["ICLR.cc/2026/Conference/Submission11358/Reviewer_dz4g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11358/Reviewer_dz4g"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11358/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761621007914, "cdate": 1761621007914, "tmdate": 1762922489443, "mdate": 1762922489443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic empirical comparison of scaling behaviors between xLSTM and Transformer architectures for large language models. The study spans three dimensions: (1) training efficiency under compute-optimal and over-training regimes across 80M-7B parameters and 2B-2T tokens, (2) the relationship between optimal model size and context length, and (3) inference-time characteristics (TTFT and step time). The authors claim xLSTM is Pareto-dominant in training loss vs. compute, requires larger compute-optimal models, and shows widening advantages with increasing context length. This is a technically solid, large-scale empirical study that provides valuable evidence for xLSTM's scaling properties. The methodological innovations (accurate FLOP accounting, context-length scaling analysis) are notable contributions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive Experimental Scale：The study demonstrates an impressive experimental scale, encompassing 672 training runs with a total compute of 3.2×10²³ FLOPs. It systematically explores both the compute-optimal and over-training regimes—a distinction of significant practical relevance—while maintaining a fair and consistent comparison framework through unified training recipes, shared data (DCLM-BASELINE), and identical hyperparameter schedules.\n\n2. Clear Empirical Findings: The paper presents clear and consistent empirical findings. xLSTM demonstrates Pareto dominance over Transformers, achieving lower loss at fixed compute across five orders of magnitude. It also exhibits remarkable over-training stability, with constant power-law exponents observed up to M=2200, confirming its reliability for inference-optimized training. Furthermore, xLSTM maintains strong context length robustness, as its optimal model size remains stable even when context length increases, whereas the Transformer's performance degrades notably."}, "weaknesses": {"value": "1. **No Downstream Task Evaluation**. The entire paper focuses exclusively on pretraining cross-entropy loss without any downstream benchmarks (e.g., MMLU, HumanEval, common sense reasoning, summarization).\n\n2. The context length experiments appear confounded, as different context lengths are trained on non-identical data distributions without clarification on how this issue is handled—for instance, whether through re-chunking, padding, or different data splits. Moreover, the y-axes in Figure 5 are not directly comparable across context lengths, and it remains unclear how the “tokens D” are counted when context size varies (i.e., whether they represent effective or nominal tokens). Clarifying the data preprocessing process or adding a fixed-dataset ablation where only context packing changes would strengthen the validity of these results."}, "questions": {"value": "1. L024: Quantify \"advantage widens\", what is the relative improvement at 2K vs. 16K contexts?\n3. L313: If losses aren't comparable across contexts, how should we interpret the intersecting curves in Figure 5 (right)?\n4. L399: \"Largest xLSTM has lower step time than smallest Transformer\", is this specific to 16K prefill, or does it hold at shorter contexts (e.g., 512)?\n5. It would be better to include at least one long-context benchmark evaluation (e.g., LongBench) to more convincingly demonstrate xLSTM’s effectiveness in handling extended context scenarios."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AyvX6rNSuX", "forum": "bpbU549sSg", "replyto": "bpbU549sSg", "signatures": ["ICLR.cc/2026/Conference/Submission11358/Reviewer_LrJB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11358/Reviewer_LrJB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11358/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718657523, "cdate": 1761718657523, "tmdate": 1762922489032, "mdate": 1762922489032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically compares the scaling of xLSTM and Transformers. Using extensive controlled runs (80M–7B parameters; 2B–2T tokens) and two protocols (IsoFLOP budgeting and parametric fitting), it examines: (i) scaling in compute-optimal and over-training regimes, (ii) how compute-optimal model size depends on context length, and (iii) inference scaling in algorithmic complexity, latency, and throughput. The key finding is that xLSTM consistently scales better than Transformers, and the advantage widens as training or inference contexts grow longer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  Provides a large, controlled scaling study comparing xLSTM and Transformers across budgets and context lengths. Combines IsoFLOP budgeting with parametric fitting to bridge compute-optimal and over-training regimes.\n\n2.  Extensive, well-structured experimental sweeps (80M–7B; 2B–2T) with clear loss–compute Pareto analyses.\n\n3.  Coherent problem framing and storyline from scaling laws → compute-optimal sizing → inference scaling. Figures generally align with claims and support the narrative.\n\n4. Offers compute-aware guidance on architecture choice, model size, and context length. Helps quantify when linear-time sequence processing becomes advantageous, informing system design and resource allocation decisions."}, "weaknesses": {"value": "1.  Several key definitions/configs are only provided in the appendix; Moving essential config details into the main text, unifying definitions/notation would significantly improve the presentation of the paper.\n\n2. Results seem to be on a specific data mix, tokenizer, and recipe. Adding cross-dataset, cross-tokenizer, and recipe sensitivity studies would make the paper stronger. \n\n3. Some figures are too crowded. Several plots are visually dense, making it hard to extract key trends."}, "questions": {"value": "1.\tIt would be good to provide cross-dataset and cross-tokenizer results to test whether the reported margins persist?\n\n2.\tHow do you normalize FLOPs/MemOps across architectures (kernel fusion, activation checkpointing, flash-attention)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kQqC4BukhF", "forum": "bpbU549sSg", "replyto": "bpbU549sSg", "signatures": ["ICLR.cc/2026/Conference/Submission11358/Reviewer_R1D3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11358/Reviewer_R1D3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11358/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917238463, "cdate": 1761917238463, "tmdate": 1762922488584, "mdate": 1762922488584, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision"}, "comment": {"value": "Dear Reviewers, we updated the manuscript to incorporate the changes discussed in the first round of rebuttal.\nBest, Authors"}}, "id": "dkFpv5hst7", "forum": "bpbU549sSg", "replyto": "bpbU549sSg", "signatures": ["ICLR.cc/2026/Conference/Submission11358/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11358/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission11358/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763464217409, "cdate": 1763464217409, "tmdate": 1763464217409, "mdate": 1763464217409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}