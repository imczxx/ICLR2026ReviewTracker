{"id": "7oxC9rrM4X", "number": 17388, "cdate": 1758275320555, "mdate": 1759897178202, "content": {"title": "Beyond Surface Reasoning: Unveiling the True Long Chain-of-Thought Capacity of Diffusion Large Language Models", "abstract": "Recently, Diffusion Large Language Models (DLLMs) have offered high throughput and effective sequential reasoning, making them a competitive alternative to autoregressive LLMs (ALLMs). However, parallel decoding, which enables simultaneous token updates, conflicts with the causal order often required for rigorous reasoning. We first identify this conflict as the core Parallel–Sequential Contradiction (PSC). Behavioral analyses in both simple and complex reasoning tasks show that DLLMs exhibit genuine parallelism only for directly decidable outputs. As task difficulty increases, they revert to autoregressive-like behavior, a limitation exacerbated by autoregressive prompting, which nearly doubles the number of decoding steps with remasking without improving quality. Moreover, PSC restricts DLLMs' self-reflection, reasoning depth, and exploratory breadth. To further characterize PSC, we introduce three scaling dimensions for DLLMs: parallel, diffusion, and sequential. Empirically, while parallel scaling yields consistent improvements, diffusion and sequential scaling are constrained by PSC. Based on these findings, we propose several practical mitigations, parallel-oriented prompting, diffusion early stopping, and parallel scaling, to reduce PSC-induced ineffectiveness and inefficiencies.", "tldr": "", "keywords": ["Diffusion Large Language Model", "Chain-of-Thought", "Long Chain-of-Thought", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/76cbb881ee55a234e9f62e88eaddf2285df08af0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper investigates the reasoning ability of Diffusion Large Language Models (DLLMs) and introduces a key limitation in DLLMs called the Parallel–Sequential Contradiction (PSC).  \nAlthough diffusion decoding allows parallel generation that is not constrained by token order, the authors find that in complex reasoning tasks, DLLMs gradually degenerate into sequential decoding, requiring many more steps to reach the same quality as autoregressive LLMs.\n\nThey perform extensive experiments comparing DLLMs on simple and complex tasks and discover that, 1) DLLMs only show true parallelism on trivial or directly decidable tasks; 2) for long reasoning chains, DLLMs behave like autoregressive models; 3) typical Chain-of-Thought, reflection, and exploration strategies effective for LLMs fail to help DLLMs.\n\nThey also analyze scaling laws and find that the Parallel Scaling Law still holds; the Diffusion and Sequential Scaling Laws break under PSC constraints.\n\nFinally, the authors show that constraint-guided prompting and parallel-encouraging prompting can partially mitigate PSC, improving reasoning accuracy and efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Very comprehensive experiments across multiple benchmarks and models (Dream-7B, LLaDA-8B, Qwen, etc.).\n- Clear identification of the Parallel–Sequential Contradiction that explains why DLLMs fail on long reasoning chains.\n- Empirical results are consistent and carefully analyzed.\n- The paper provides a valuable empirical foundation for future work on diffusion-based reasoning models."}, "weaknesses": {"value": "The paper has two main weaknesses.\n\n**(1) Limited methodological contribution.**  \nThe main strength of this paper lies in its rigorous and systematic analysis of DLLMs’ weaknesses.  \nThe findings are convincing and the empirical evidence is solid.  \nHowever, the work remains largely *diagnostic*: it identifies the problem of the Parallel–Sequential Contradiction (PSC) but does not propose concrete or innovative solutions.  \nFrom my perspective, the paper’s value is primarily in defining and characterizing the issue, while the exploration of how to mitigate or resolve PSC is minimal.  \nFor an ICLR-level contribution, the lack of a novel method or substantial algorithmic proposal weakens the overall impact.\n\n**(2) Insufficient clarity and missing experimental details.**  \nThe figures and wording of this paper are good, but many critical experimental details are missing or scattered across sections, making it hard to fully understand the setup and reproduce the results.  \nIn particular:\n- The paper does not provide a unified description of the experimental configuration.  \n  For example, Section 3.3 introduces three prompting strategies but doesn't specify how these strategies are implemented.  \n- The models used (e.g., Dream-7B-Instruct, LLaDA-8B-Instruct, LLaMA-3.1-8B) are not introduced in a setup section; they just appear inside result tables.  \n- In Section 4.1, the reflection and exploration strategies are mentioned but not explained — it is unclear how they are applied to baseline DLLMs.  \n- Figure 6(a) refers to the “error step” metric, but its definition is not provided.\n\nOverall, the paper’s experimental insights are strong, but the missing methodological and experimental details significantly reduce readability and reproducibility."}, "questions": {"value": "I believe this paper has meaningful value and could become a strong contribution with further clarification and expansion.  \nTo improve the clarity and completeness of the work, I suggest the authors address the following points:\n\n1. Provide a unified and detailed experimental setup. (see the weakness part for the details)\n\n2. It could be a major contribution if the author could expand Section 3.3 on the mitigation strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v1wcgwiXww", "forum": "7oxC9rrM4X", "replyto": "7oxC9rrM4X", "signatures": ["ICLR.cc/2026/Conference/Submission17388/Reviewer_eVwU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17388/Reviewer_eVwU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761703251223, "cdate": 1761703251223, "tmdate": 1762927296123, "mdate": 1762927296123, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the performance of DLLMs in Long CoT reasoning, identifying the PSC as a core limitation to their reasoning capabilities. The study reveals that while DLLMs exhibit superficial parallel reasoning in simple tasks, they revert to autoregressive-like behavior in complex scenarios. Based on these findings, the authors propose three inference-time scaling directions and demonstrate that PSC significantly constrains the latter two. Finally, the paper introduces several mitigation strategies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper systematically defines and analyzes the PSC problem, uncovering a critical bottleneck for DLLMs in complex reasoning tasks.\n\nMathematical derivations convincingly demonstrate how PSC causes DLLMs to degrade into autoregressive behavior in sequential tasks"}, "weaknesses": {"value": "Lack of Ablation Studies: While multiple strategies are proposed, the paper lacks sufficient ablation experiments to quantify the individual contributions of each approach\n\nRedundant Analysis and Conclusions: Some parts of the experimental analysis and conclusions appear superfluous or lack meaningful insight. For instance, the statement, \"This indicates that moderate temperature settings provide the optimal balance between generation diversity and output reliability\"."}, "questions": {"value": "Why is the statement “This indicates that moderate temperature settings provide the optimal balance between generation diversity and output reliability” mentioned? How does it differ from what is commonly known in the field?\n\nThe use of $\\in (0,1]$ in Equation 4 of the Appendix appears redundant and unconventional. However, this does not significantly impact the overall evaluation of the paper.\n\nI roughly understand what this paper is doing, but I am not sure if I fully grasp its practical value. My confidence in evaluating this aspect is low."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "hlulX5pGsg", "forum": "7oxC9rrM4X", "replyto": "7oxC9rrM4X", "signatures": ["ICLR.cc/2026/Conference/Submission17388/Reviewer_iUzq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17388/Reviewer_iUzq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832808146, "cdate": 1761832808146, "tmdate": 1762927295676, "mdate": 1762927295676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that DLLMs face an inherent Parallel–Sequential Contradiction (PSC): their parallel token updates clash with the ordered dependencies required by long CoT reasoning. Empirically, DLLMs appear parallel only on simple/locally decidable cases, but revert to autoregressive-like behavior on harder problems. The authors further study “inference-time scaling” along three axes, claiming only parallel scaling reliably improves accuracy. Also, they propose three mitigations: parallel-oriented prompting, diffusion early stopping, and parallel scaling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The experiments are complete, effective, and well-executed, covering multiple prompt families, reasoning benchmarks, and DLLM variants.\n\nThe insights conveyed by the paper also appear internally logical and plausible from the narrative built by the authors.\nHowever, since I am not an expert on diffusion-based LLMs, I cannot fully judge whether these insights are already well-known to the community or whether they represent genuinely novel conceptual ground."}, "weaknesses": {"value": "**W1 (Attribution of reflection/exploration failures).** The paper attributes weak gains from reflection/exploration to PSC, but similar limitations often hold for autoregressive LLMs as well. Therefore these results are not specific evidence for PSC. A direct AR baseline demonstrating materially larger reflection/exploration gains (under matched settings) is needed to isolate a diffusion-specific effect. \n\n**W2 (Cross-paradigm baselines are thin).** Many claims hinge on DLLM-specific pathology, yet the comparisons against strong AR models (with the same prompts, compute budgets, and answer-length controls) are not comprehensive enough to rule out a simpler story—namely, capability gap rather than PSC. For example, sequential prompts hurting DLLMs could be confounded by longer outputs vs. limited diffusion iterations rather than contradiction per se. Stronger AR-vs-DL baseline ablations would sharpen the causal reading."}, "questions": {"value": "**Q1. Dependency strength quantification.**\nLanguage always has contextual dependence, and DLLMs already show the ability to model such dependence. Reasoning tasks simply impose stronger step-to-step dependence. The paper claims such stronger dependence fundamentally limits DLLMs. Can you provide a more rigorous theoretical / empirical link between dependency intensity and performance? For example, define a quantitative dependency index (conditional entropy / mutual information / perturbation-based sensitivity), and quantify the severity of DLLM PSC manifestations as a function of this dependency index (i.e., plotting PSC strength vs. dependency intensity).\n\n**Q2. Data-side mitigation.**\nBeyond prompting and decoding, could PSC be mitigated via dataset design? If synthetic datasets can be constructed, what would the ideal DLLM-friendly training data look like?\n\nNote: The two questions I raised above are **not additional weaknesses**."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lv9WdJD3Lm", "forum": "7oxC9rrM4X", "replyto": "7oxC9rrM4X", "signatures": ["ICLR.cc/2026/Conference/Submission17388/Reviewer_U4VP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17388/Reviewer_U4VP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991866627, "cdate": 1761991866627, "tmdate": 1762927295288, "mdate": 1762927295288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the parallel-sequential tradeoffs in diffusion LLMs. That is, it observes that diffusion models inherently generate sequences in parallel. For tasks where there is a simple way to build up the sequence step by step (e.g., in reasoning), it is much harder to generate intermediate tokens without seeing the full context before the step. The paper call this \"parallel-sequential contradiction\".\n\nThe paper contains a broad empirical analysis to study properties of DLLMs from this perspective. First, it finds that compared to autoregressive LLMs, DLLMs often need more tokens and revert to sequential generation for more complex tasks. The paper explains this by saying that it is much easier to generate sequentially (lower entropy). Next, it looks at prompting strategies. Prompting strategies for reasoning are optimized for autoregressive models and hinder DLLMs; however, prompts that mention parallel reasoning appear to help.\nMoreover, the paper shows that other common strategies that help CoT reasoning in ALLMs (autoregressive) do not help DLLMs. DLLMs seem to also in general make more mistakes.\n\nThe last part looks at scaling with respect to parallelism (more parallel samples), diffusion steps and sequence length. In particular more parallel samples lead to great improvements. More diffusion also helps up to a certain point, hence the paper suggests early stopping."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The tension between the inherent parallelism and the sequential nature of certain reasoning tasks is a very relevant topic of study.\n\n- The paper contributes a broad set of empirical results on the topic, which gives insights into trends and differences in behavior between autoregressive LLMs and DLLMs. The results are interesting.\n\n- The results are structured clearly."}, "weaknesses": {"value": "- The PSC, a core hypothesis of the paper, is defined a bit vaguely. Why is this a \"contradiction\"? I understand what the problem is -- parallel generation is disadvantageous if there are long-range dependencies in the sequence and there is a \"simple\" sequential generation rule. But it was vague to me at first reading, and I would use a different word than \"contradiction\" for this.\nSimilarly, what would then be \"PSC-free\", or \"PSC-reduced\"?\n\n- The theory as is does not fully support the claims (and needs fixing), even though the claims are intuitive: The proof in Appendix B, if I understand it correctly, shows that for \"sequential\" tasks (input data distributions), the entropy of the k-th token given the first tokem (or step) is higher than for a \"parallel\" data distribution (which follows from the definitions of sequental and parallel via bounds on the probabilities). But it does not prove that a sequential *generation* would be better for either, as is claimed in Appendix B.5. For that, one would need to argue that the entropy increases with k, and that that makes it easier for the model to sequentially approximate p(S_k|S_1). In addition, the proof needs some polishing (see below).\nWhile the theory needs to be fixed, the main contribution of the paper is empirical.\n\n- Some of the observations are fairly straightforward (e.g., some of the behavior in scaling), as are the \"fixes\" of early stopping. The proposed prompt looks similar but simplified to the one right before. I think it is a great direction to think how to exploit both parallelism (in the correct way) and sequential reasoning, but how exactly the prompt is achieving this is not clear to me (even though it works).\n\n- Some of the titles seem a bit unrelated to the text, at least in my understanding, e.g.:\n* Sec. 4.1: if strategies for ALLMs do not work, does that really mean that DLLMs do not have sufficient capabilities for CoT? The only real limitation may be Fig. 6(a).\n* Sec. 4.2.1: why \"despite\"? sampling multiple paths should not interfere with sequential vs. parallel reasoning.\n* Sec. 4.2.2, 4.2.3: what does it mean for a scaling law to be \"broken\"? It is just an estimate how performance will scale with a resource. There is no diffusion scaling law for ALLMs, so what should it look like? \nMoreover, the content of section 4.2.2 seems to be independent of PSC. Or what is the connection?"}, "questions": {"value": "- Is the function in Definition 1 (Appendix B) learnable, with epsilon going to zero? It may need at least an assumption on k to k+1 being continuous? Definition 1 even asks for the condition for $p(S_2|S_1)$.\n\n- Appendix B: please check the notation, e.g.\n* Lemma 1: the definition of the neighborhood (what is the radius? epsilon is now the cell size);\n* eqn (8): this needs to be for some k (not any)\n* Prop. 1: say what is $H_S$, $H_T$\n* eqn 12: should the max be over k?\n\n- Fig. 3b: why is there such a sharp drop in accuracy from 256 to 1024 tokens?\n\n- parallel-encouraging prompting: Diff-MARP is less clear than MARP about what should be parallelized. Is MARP too complex? Or what do you mean by this: \"This method balances parallel execution with simplicity, allowing for effective multi-step reasoning without overwhelming the model with overly complex tasks. Moreover, the model adjusts operation complexity dynamically.\" Why does Diff-MARP work better? This needs a bit more discussion in the main text, too.\n\n- Figure 4,5 are a bit hard to grasp with the small amount of text in the main paper. What would you expect about e.g. token entropy from these strategies? compared to ALLMs? Is the x axis in the (c) figure percentage?\n\n- Fig. 7 is not so clear to me. You could replace it by more explanatory text, as the text is very high-level.\n\n- Fig. 9(e): it looks like in stage 3 the model has converged already.\n\n\nTypos:\n- Fig. 1: \"surficial\"\n- takeaways page 5: redcued\n\n\nI think this paper is interesting, and my score is due to the sloppiness in the theory part and the clarity in some places, as mentioned in my review. I am willing to change my score if these concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "pg5xumQZyh", "forum": "7oxC9rrM4X", "replyto": "7oxC9rrM4X", "signatures": ["ICLR.cc/2026/Conference/Submission17388/Reviewer_dA1n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17388/Reviewer_dA1n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992735999, "cdate": 1761992735999, "tmdate": 1762927294607, "mdate": 1762927294607, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the fundamental reasoning limits of DLLMs. It identifies a core Parallel–Sequential Contradiction (PSC), where the parallel decoding nature of DLLMs conflicts with the inherently sequential structure of reasoning. Through behavioral and scaling analyses, the authors show that DLLMs revert to autoregressive-like behavior in complex reasoning tasks and face efficiency losses. They propose several mitigation methods, which alleviate PSC effects and improve reasoning accuracy by up to 5% across benchmarks such as BigGSM and GSM8K."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The identification and formalization of the PSC provide a theoretically grounded explanation for DLLMs’ inefficiencies in sequential reasoning, which offers an important new perspective for future research.\n* The authors decompose DLLM inference into three scaling dimensions and systematically examine their performance and upper bounds. As far as I know, this is the first work to systematically study scaling on text diffusion models.\n* The empirical analysis is comprehensive and the figure is well illustrated. The motivation and the presentation are both good."}, "weaknesses": {"value": "* The authors claim that they use the default decoding settings for all models. However, as claimed in [1], the decoding method is vital in diffusion model generation. The authors could try more decoding method (entropy-based, or top-p margin based in [1]), which can help determine whether PSC is an inherent property of DLLMs’ structure or merely a byproduct of the default decoding method.\n* Missing citations and discussions. [2] also discusses the scaling behavior of diffusion models, which shares some same views with the paper.\n\n[1] Train for the Worst, Plan for the Best: Understanding Token Ordering in Masked Diffusions. Arxiv 2502.06768\n[2] Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps. Arxiv 2501.09732"}, "questions": {"value": "* How does PSC interact with post-training method? Does fine-tuning or RL training reduce or exacerbate the contradiction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YJvq5YcvB4", "forum": "7oxC9rrM4X", "replyto": "7oxC9rrM4X", "signatures": ["ICLR.cc/2026/Conference/Submission17388/Reviewer_pC3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17388/Reviewer_pC3J"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762275945155, "cdate": 1762275945155, "tmdate": 1762927294082, "mdate": 1762927294082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}