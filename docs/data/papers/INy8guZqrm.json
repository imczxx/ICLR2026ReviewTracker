{"id": "INy8guZqrm", "number": 4963, "cdate": 1757820660219, "mdate": 1759898002547, "content": {"title": "HUMOF: Human Motion Forecasting in Interactive Social Scenes", "abstract": "Complex dynamic scenes present significant challenges for predicting human behavior due to the abundance of interaction information, such as human-human and human-environment interactions. These factors complicate the analysis and understanding of human behavior, thereby increasing the uncertainty in forecasting human motions. \nExisting motion prediction methods thus struggle in these complex scenarios. In this paper, we propose an effective method for human motion forecasting in dynamic scenes. To achieve a comprehensive representation of  interactions, we design a hierarchical interaction feature representation so that high-level features capture the overall context of the interactions, while low-level features focus on fine-grained details. Besides, we propose a coarse-to-fine interaction reasoning module that leverages both spatial and frequency perspectives to efficiently utilize hierarchical features, thereby enhancing the accuracy of motion predictions. Our method achieves state-of-the-art performance across four public datasets. We will release our code upon publication.", "tldr": "Human motion prediction considering human-scene and human-human interactions", "keywords": ["human motion forecasting", "scene-aware", "multi-person"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ccc242a0c0fa88103f1b0980f9cd12e268d7063f.pdf", "supplementary_material": "/attachment/482f879fbce3297554bb1fba521c188db124a369.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel approach to address the problem of human motion forecasting in interactive environments.  It builds a hierarchical interaction representation, modeling interactions at multiple levels by combining explicit cues (e.g. inter-object distances) with learned features. Specifically, HUMOF captures high-level context and low-level details for both social interactions and scene contacts. A key innovation is the coarse-to-fine reasoning module, a multi-layer Transformer that progressively incorporates interaction features: early layers use high-level features, while later layers integrate fine-grained features. An adaptive DCT‐based rescaling further suppresses high-frequency components in early layers, encouraging a coarse-to-fine refinement of the motion prediction.\n\nHUMOF obtained state-of-the-art results on two datasets with human-human and human-scene interactions (HIK. HOI-M3) as well as two datasets with human-scene interactions (GTA-IM, HUMANISE)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well written and explains deeply the architecture and how it works.\n2. The paper models both human-human and human-scene interactions in a single framework, addressing a realistic scenario of interactive environments.\n3. The use of multi-level representations (body-level vs joint-level for social cues, and multi-scale point clouds for scene context) is a strong idea. It balances global context and local detail effectively.\n4. The injection of high-level features in early Transformer layers and finer details later (along with the DCT rescaling mechanism) is novel and well motivated by the ablation studies.\n5. The authors compare their approach to a large set of baselines and include visualizations to support their claims."}, "weaknesses": {"value": "1. HUMOF’s architecture is quite elaborate (multiple modules, Transformer layers, DCT processing). This complexity might make it hard to reproduce or tune and the paper should give more details in the appendices.\n2. The authors note that existing datasets have few moving scene objects. Thus, HUMOF’s performance on truly dynamic environments (e.g. moving furniture or vehicles) is untested.\n3. The baselines weren’t supposed to work with all the tested datasets, hence they had to be adapted. This raises a question of fairness in comparisons: it’s possible some methods could be improved with similar context. However, the large gaps suggest HUMOF’s advantage is likely genuine.\n4. The runtime analysis in the Appendix was performed on only one dataset (HOI-M3).\n5. The supplementary video lacks failure cases. Although some failure scenarios are described in the Appendix, it would be helpful to include them in the video as well."}, "questions": {"value": "1. The paper uses a function $\\phi (\\cdot)$ to map distances to higher values for closer points. How sensitive is the performance to the choice of this mapping? Could a learned function improve results?\n2. HUMOF 3D uses poses for all humans and a 3D point cloud of the scene. In real-world settings, these could be noisy or incomplete. Would the approach still work with noisy or incomplete inputs?\n3. The runtime is around 43ms per inference for HOI-M3. Does this runtime change on other datasets? Have the authors tested HUMOF in an online setting? Is the latency competitive for real-time applications?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MxN0Mxe39Z", "forum": "INy8guZqrm", "replyto": "INy8guZqrm", "signatures": ["ICLR.cc/2026/Conference/Submission4963/Reviewer_9FNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4963/Reviewer_9FNW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761748787311, "cdate": 1761748787311, "tmdate": 1762917796892, "mdate": 1762917796892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces HUMOF, a novel framework for human motion forecasting in complex social environments. HUMOF effectively integrates human kinematics, dynamics, spatial–temporal context, and interaction cues into a unified predictive model.\n\nA key contribution is the Hierarchical Interaction Representation, which jointly captures human–human and human–scene interactions using both explicit distance features and learned semantic–geometric representations. To exploit this representation, the authors design a Coarse-to-Fine Interaction Reasoning Module, which improves motion prediction through two mechanisms:\n\nSpatial hierarchy: High-level semantic features are introduced in early Transformer layers, while fine-grained geometric cues are progressively refined in later layers.\n\nFrequency control: A DCT-based frequency modulation strategy prioritizes high-frequency motion components early in training and focuses on low-frequency refinements in later stages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is logically organized and easy to follow, with a coherent narrative from motivation to methodology and results.\n\n2. The evaluation covers both multi-person interactive and single-person forecasting scenarios, including tests on unseen environments. The proposed method consistently outperforms baselines across all benchmarks.\n\n3. The visualizations and supplementary videos are convincing — showing smooth, stable, and realistic motions with minimal jitter and drift compared to prior works."}, "weaknesses": {"value": "No major weakness"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed.", "Yes, Other reasons (please specify below)"]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Qcf1BsK2Qw", "forum": "INy8guZqrm", "replyto": "INy8guZqrm", "signatures": ["ICLR.cc/2026/Conference/Submission4963/Reviewer_sVzv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4963/Reviewer_sVzv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872929221, "cdate": 1761872929221, "tmdate": 1762917796564, "mdate": 1762917796564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "HUMOF presents a human motion forecasting method in social environments that takes into account both human-human as well as human-scene interactions. A DCT rescaling allows for controllability of the coarseness of the signal processing. The authors compare on two social scene datasets and on two human-scene interaction datasets and consistently outperform SOTA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem of social human motion forecasting is highly relevant but relevantly under-explored. HUMOF is a complex method with many moving parts. However, the authors ablate the model relatively well. Utilizing DCT is a commonly used technique in motion forecasting. Using the rescaling for coarse-to-fine prediction in the context of human motion fc is novel and clever. The method description is mostly clear experiments have been conducted on two social scene datasets and two human-scene interaction datasets, demonstrating the methods effectiveness, while being parameter and inference time efficient."}, "weaknesses": {"value": "The method is complex (as the task itself is complex) but I have some concerns about some of the model specifics:\n(1) The utilized relative encoding is overly simplistic: I wonder if instead of just utilizing the point distance the method could utilize the geometric transformations in SE(3), for example as has been utilized in [1]. \n(2) While DCT works well for-single person motion, I wonder if it is limiting the human-object and human-human interaction quality. In the transformer, the tokens do not directly correspond to frames anymore, so temporal alignment over longer time frames might be hindered. The authors should evaluate this by showing either closer human-human (or human-object) interaction.\nIf the current datasets do not contain sufficient close person-to-person data, the authors should utilize other dyadic datasets, i.e Inter-X [2].\n\nMy second concern is with regards to the evaluation: the authors only evaluate directly comparing to GT (path error, pose error) - however, due to the complexity of the scene, multiple “answers” could be correct - I wonder if the authors have considered utilizing methods to compare the generated sequences on a distribution level, i.e. by utilizing FID, i.e. the authors could use the combined input-output sequence ($X^{1:H} \\oplus \\hat{X}^{H+1:H+T}$) to compare the distributions. \n\n[1] GTA: Geometric Transform Attention (ICLR 2024)\n\n[2] Inter-X: Towards Versatile Human-Human Interaction Analysis (CVPR 2024)"}, "questions": {"value": "Did the authors measure human-object and human-human penetration? This feels like a natural form of evaluating this task."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZlGCvQY6H4", "forum": "INy8guZqrm", "replyto": "INy8guZqrm", "signatures": ["ICLR.cc/2026/Conference/Submission4963/Reviewer_ztD4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4963/Reviewer_ztD4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935383780, "cdate": 1761935383780, "tmdate": 1762917796230, "mdate": 1762917796230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HUMOF, a method for human motion forecasting in complex dynamic scenes that involve both human–human and human–scene interactions.\nThe approach introduces a hierarchical interaction representation that separately encodes high-level contextual and low-level geometric information, and a coarse-to-fine interaction reasoning module that injects these features into Transformer layers from semantic to geometric levels.\nThe framework combines Discrete Cosine Transform, Graph Convolutional Networks, and Transformer architectures.\nExperiments on four public datasets (HIK, HOI-M3, GTA-IM, and HUMANISE) show that HUMOF achieves state-of-the-art performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.\tComprehensive interaction modeling.\n\n- Unlike prior works that focus on either human–scene or human–human interactions, this paper successfully integrates both within a unified framework. This joint modeling is meaningful and leads to noticeable performance improvements on dynamic scenes.\n\n2.\tStrong empirical performance.\n\n- The method achieves consistent improvements across multiple datasets and evaluation metrics. The results demonstrate that the hierarchical representation and coarse-to-fine reasoning work effectively together.\n\n3.\tPractical contribution as a baseline.\n\n- If the authors release the code as promised in the abstract, HUMOF can become a valuable benchmark for future research on interactive human motion prediction."}, "weaknesses": {"value": "1.\tLimited novelty in components.\n\nMost elements of the architecture come from existing works (coarse-to-fine approach, distance-based interaction modeling, abstraction, ...). The contribution lies primarily in how these components are integrated, rather than in introducing a fundamentally new modeling paradigm.\n\n2.\tLack of quantitative evaluation for multi-person inference.\n\nThe paper briefly demonstrates qualitative results for joint multi-person inference in Figure 4 but does not provide quantitative metrics or runtime analysis. Since multi-person prediction is highly relevant for real-world applications such as social robotics or crowd simulation, the absence of measurable evaluation limits the practical significance of this claim.\n\n3.\tHigh computational complexity for multi-agent prediction.\n\nThe proposed framework models each target person independently and computes pairwise interactions with every surrounding person. \nThis design implies that when there are K individuals, the method requires roughly K² human–human interaction computations. \nFurthermore, because the scene abstraction (HSI) is recomputed for each target, the total cost scales linearly with K, resulting in O(K³) level complexity when both factors are considered. \nThis raises concerns about the scalability and feasibility of real-time multi-agent forecasting.\n\n4.\tIssue in references.\n\nThe paper repeatedly cites prior works using informal author patterns such as “Jeong & etc., 2024” or “Xing & etc., 2025.” This format is inappropriate for a scientific paper and must be corrected to proper citation styles."}, "questions": {"value": "Please see the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rm1gVLU3n5", "forum": "INy8guZqrm", "replyto": "INy8guZqrm", "signatures": ["ICLR.cc/2026/Conference/Submission4963/Reviewer_7peC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4963/Reviewer_7peC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762095115928, "cdate": 1762095115928, "tmdate": 1762917795771, "mdate": 1762917795771, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}