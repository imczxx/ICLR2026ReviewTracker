{"id": "9glgMTTZb9", "number": 5773, "cdate": 1757934154332, "mdate": 1759897954660, "content": {"title": "Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos", "abstract": "Long Video Question-Answering (LVQA) presents a significant challenge for Multi-modal Large Language Models (MLLMs) due to immense context and overloaded information, which could also lead to prohibitive memory consumption. \nWhile existing methods attempt to address these issues by reducing visual tokens or extending model's context length, they may miss useful information or take considerable computation.\nIn fact, when answering given questions, only a small amount of crucial information is required.\nTherefore, we propose an efficient question-aware memory mechanism, enabling MLLMs to recurrently seek these critical clues. Our approach, named VideoDetective, simplifies this task by iteratively processing video sub-segments. For each sub-segment, a question-aware compression strategy is employed by introducing a few special memory tokens to achieve purposefully compression. This allows models to effectively seek critical clues while reducing visual tokens.\nThen, due to history context could have a significant impact, we recurrently aggregate and store these memory tokens to update history context, which would be reused for subsequent sub-segments. \nFurthermore, to more effectively measure model's long video understanding ability, we introduce GLVC (Grounding Long Video Clues), a long video question-answering dataset, which features grounding critical and concrete clues scattered throughout entire videos.\nExperimental results demonstrate our method enables MLLMs with limited context length of 32K to efficiently process 100K tokens (3600 frames, an hour-long video sampled at 1fps), requiring only 2 minutes and 37GB GPU memory usage. Evaluation results across multiple long video benchmarks illustrate our method can more effectively seek critical clues from massive information.", "tldr": "", "keywords": ["MLLM; LongVideo"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf8baa9af35aa0e72e7a5b3d40c3c4f35d3b6d6c.pdf", "supplementary_material": "/attachment/2e42a1c09f6d6f7b74ce5b21f718da8e25d638dd.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes VideoDetective, a multimodal large language model designed to address long video question-answering (LVQA) tasks through an efficient question-aware memory mechanism.\n\nThe key contributions include:\n\n  1. Question-aware Memory Mechanism: The method processes long videos by dividing them into sub-segments and employs learnable memory tokens to compress visual information in a\n  question-aware manner.\n  2. Recurrent Critical Clue Seeking: A memory bank stores and aggregates semantic representations across video segments to maintain historical context for subsequent processing.\n  3. GLVC Dataset: A new long video QA dataset featuring concrete temporal clues scattered throughout entire videos, designed to better evaluate models' ability to ground critical\n  information.\n  4. Efficiency Claims: The method reportedly enables processing of 100K tokens (3600 frames) with only 32K context length, requiring 2 minutes inference time and 37GB GPU memory.\n\n  The approach is motivated by human cognitive processes of \"thinking while watching\" and aims to seek small amounts of crucial clues rather than processing entire video content at once."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "**Originality**\n\n  - Dataset Contribution: GLVC provides temporal grounding annotations that could benefit the community for more rigorous evaluation of long video understanding capabilities.\n\n**Quality**\n\n  - Well-Motivated Approach: The method addresses a real limitation of current MLLMs in handling long video contexts due to memory constraints.\n  - Comprehensive Evaluation: The paper evaluates on multiple established benchmarks (VideoMME, MLVU, LongVideoBench, etc.) covering both short and long video scenarios.\n\n**Clarity**\n\n  - Clear Problem Statement: The paper clearly articulates the challenges of long video QA and memory limitations.\n  - Good Visual Presentation: Figures effectively illustrate the overall architecture and key concepts.\n\n**Significance**\n\n  - Important Problem: Long video understanding is a significant challenge for current MLLMs with practical applications.\n  - Memory Efficiency: If the technical claims are validated, the approach could enable broader deployment of video understanding models."}, "weaknesses": {"value": "**Fundamental Training Process Contradictions**\n The paper contains a critical technical inconsistency in Section 3.3:\n  - Claims \"memory tokens do not participate in loss calculation\" yet the loss function $L = \\sum_{j=1}^l -\\log p(x_j|V_1, M_1, Q, \\cdots, V_S, M_S, Q, x_0, ..., x_{j-1})$ explicitly depends\n  on memory tokens $M_i$\n  - If memory tokens don't participate in loss calculation, how do they receive gradients for optimization?\n  - This creates a fundamental contradiction that questions the technical feasibility of the approach\n\n**Gradient Backpropagation Problems**\n  The recurrent processing design raises serious gradient flow issues:\n  - Each segment's memory tokens depend on historical context from previous segments\n  - Maintaining computational graphs across all segments would require enormous memory (contradicting efficiency claims)\n  - The paper provides no explanation of how gradients backpropagate through the memory bank updates\n  - Missing details on whether detach() operations are used and where\n\n**Incomplete Training Strategy Description**\n  The two-stage training process lacks crucial technical details:\n  - Warmup stage: Uses video-caption pairs but provides no loss function or training objective\n  - Compression ratio inconsistency: Warmup uses ratio 32, main training uses ratio 16 without justification\n  - Learning rate jump: 100Ã— increase from warmup (1e-6) to main training (1e-4) lacks theoretical and emperical basis\n\n**Insufficient Ablation Studies**\n  Current ablations only test compression ratios, missing critical components:\n  - No validation of question-aware compression effectiveness\n  - Missing ablation on recurrent memory mechanism vs. simple aggregation\n  - No verification that the model actually \"seeks critical clues\" as claimed\n\n**Unfair Experimental Comparisons**\n  - Data leakage: VideoDetective trained on GLVC dataset but evaluated on it (Table 2)\n\n**Sampling Strategy Inconsistencies**\n  Section 5.1 reveals problematic data handling:\n  - Fixed 32-frame segments ignore semantic boundaries\n  - No strategy for handling videos shorter/longer than expected lengths\n  - Compression ratio $k = N_i/\\alpha$ undefined for segments with $N_i < \\alpha$\n  - Training-inference mismatch in handling variable-length sequences\n\n**Limited Performance Gains**\n  Results show concerning patterns:\n  - Large gaps with SOTA: 10-15 point deficits compared to GPT-4o, Gemini-1.5-Pro\n  - Failure on key benchmarks: Acknowledged poor performance on LongVideoBench\n  - Marginal improvements: Small gains over same-scale models don't justify complexity\n  - Short video regression: Performance drops on short videos suggest fundamental limitations"}, "questions": {"value": "**Training Process Mechanics**\n  - Please provide the complete training algorithm with explicit gradient computation formulas: $\\frac{\\partial L}{\\partial M_i} = ?$ for $i = 1,2,...,S$\n  - Explain exactly what \"memory tokens do not participate in loss calculation\" means technically\n  - Provide pseudocode showing how computational graphs are maintained across recurrent segments\n  - Clarify the specific loss function and optimization target for the warmup stage\n\n**Experimental Methodology**\n  - Can you provide results on GLVC dataset using zero-shot evaluation (without training on GLVC)?\n  - What are the exact hardware specifications and software environments for efficiency comparisons?\n  - Can you include ablation studies removing question-aware compression and recurrent memory components?\n\n**Sampling and Data Processing**\n  - How exactly are videos of different lengths processed during training and inference?\n  - What is the strategy for handling the last segment when video length is not divisible by 32?\n  - How do you maintain semantic coherence when using fixed-size segments?\n  - Can you provide analysis showing the method actually captures \"critical clues\" rather than random information?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1pheHhSNgI", "forum": "9glgMTTZb9", "replyto": "9glgMTTZb9", "signatures": ["ICLR.cc/2026/Conference/Submission5773/Reviewer_Y4eZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5773/Reviewer_Y4eZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761376567264, "cdate": 1761376567264, "tmdate": 1762918253184, "mdate": 1762918253184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to reduce token consumption for MLLMs, targeting for long-video understanding. Particularly, VideoDetective method was proposed. VideoDetective segments the long video to short segment. For each segment, learnable memory tokens are obtained as the representation. All the memory tokens for the long video are input to LLM for VQA. It requires much smaller memory consumption than baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "+ The core idea is driven by a strong insight: \"only a small amount of crucial information is required\" to answer the question, making the question-aware filtering strategy sound.\n\n+ A dataset GLVC for validating the effectiveness of this method was curated.\n\n+ Good/Competitive performance was achieved."}, "weaknesses": {"value": "1) The memory design is query-dependent. However, when question was changed, the build of memory is need again, reducing the proactiveness of the method.\n\n2) Lack of efficiency metrics: A major claim of the paper is efficiency. However, there are no quantitative results comparing the proposed method's efficiency against baselines. This is critical for an LVQA paper. We need to see metrics like inference time against competitive methods (e.g., sparse attention models or other compression techniques) to validate the \"efficient\" claim. \n\n3) The training process is not clear. Is the model trained with \"next token prediction\" objective (7)? If so, are the predicted token the final answer?\n\n4) For the j-th segment, will the history memory tokens be used to learn the memory tokens of this segment?\n\n5) Will the segment length influence the performance a lot?\n\n6) In Table 2, what does sub means in \"w/ sub\"?\n\n7) The computation cost (how many GPUs are used and how long the training takes) should be discussed."}, "questions": {"value": "See weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DVSb6IKNRY", "forum": "9glgMTTZb9", "replyto": "9glgMTTZb9", "signatures": ["ICLR.cc/2026/Conference/Submission5773/Reviewer_mdTa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5773/Reviewer_mdTa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873651771, "cdate": 1761873651771, "tmdate": 1762918252965, "mdate": 1762918252965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses the core challenges of long video understanding with VLMs, i.e., the excessive context length, prohibitive memory consumption, and loss of critical information in long context. The authors propose VideoDetective, a framework with recurrent question-aware memory compression and critical clue summarization. For evaluation, the authors introduce GLVC, a dataset with concrete critical clues and timestamps scattered across long videos. The experimental results show improvements on computation efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The explored problem is meaningful and the motivation of using recurrent memory compression is clear.\n2. The recurrent sub-segment processing limits peak memory usage and friendly to real-time video interaction device deployment.\n3. The GLVC benchmark is a contribution to the community for comprehensive evaluation of grounded video understanding."}, "weaknesses": {"value": "1. The authors claim dynamic compression ratio for different sub-segments to avoid over- or under-compression, but the technical details are not presented. The experiments only show results with different fixed compression ratio on different benchmarks.\n2. The recurrent memory compression with history memory continuously added to the context is quite similar to [1], with the only difference in question-aware or not. Due to the lack of dynamic compression ratio, the advantage of question-aware compression is not shown in this architecture.\n3. The scalability to extremely long videos is doubted. The growing historical memory puts limitation on ultra-long videos in terms of both computation and information forgetting.\n4. The presentation is not clear in some crucial technical details. For example, in line 269, the authors claim \"memory tokens do not participate in loss calculation\", which is quite confusing since the memory tokens are explicitly in the sequence context in loss computation, and how do you optimize the memory representations.\n\n[1] Qian, R., Dong, X., Zhang, P., Zang, Y., Ding, S., Lin, D., & Wang, J. (2024). Streaming long video understanding with large language models. Advances in Neural Information Processing Systems, 37, 119336-119360."}, "questions": {"value": "Why choose to initialize memory tokens with bos token embedding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "dsu2c71Dlu", "forum": "9glgMTTZb9", "replyto": "9glgMTTZb9", "signatures": ["ICLR.cc/2026/Conference/Submission5773/Reviewer_rML2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5773/Reviewer_rML2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5773/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969000474, "cdate": 1761969000474, "tmdate": 1762918252735, "mdate": 1762918252735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}