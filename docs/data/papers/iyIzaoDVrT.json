{"id": "iyIzaoDVrT", "number": 5636, "cdate": 1757924343211, "mdate": 1759897963821, "content": {"title": "LoopServe:  An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues", "abstract": "Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. As a result, these models   cannot accurately identify and prioritize the most relevant context, leading to degraded response quality. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. \tFirst, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input.  Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a new benchmark with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.", "tldr": "We propose LoopServe, a dual-phase system that accelerates multi-turn LLM dialogue inference by adaptively selecting key attention patterns and progressively compressing KV caches, enabling efficient and high-quality responses.", "keywords": ["LLM Inference Acceleration", "KV Cache", "Long-context Multi-Turn Dialogues", "Efficient LLMs"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1cae31770e9ce8dae4e3ca43573806cbd27b46b.pdf", "supplementary_material": "/attachment/640401610297fc64456b0af972ece37f0563b34e.zip"}, "replies": [{"content": {"summary": {"value": "Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As the conversation history grows longer, existing large language models face increasing computational and memory challenges that hinder their ability to provide efficient and responsive interactions. Innovation points are：First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key-value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.A two-stage adaptive acceleration framework is proposed to address the dynamic characteristics of multi-turn conversations and solve the generalization problem of existing static methods (such as Minference and SnapKV).\n2.In terms of empirical evidence, it is also relatively comprehensive. Through detailed exploratory experiments (such as attention sparsity analysis and query position impact analysis) and main experiments (covering 11 datasets and multiple baselines), the superiority of LoopServe has been demonstrated across various tasks, including Question Answering, Summarization, and Few-Shot Learning.\n3.It contributes to the benchmark for multi-turn long-context conversations, makes up for the shortcomings of existing benchmarks, and is more in line with real-world dialogue scenarios.\n4.It enables engineering practicality, providing algorithm details, complexity analysis, and parameter sensitivity experiments to facilitate reproduction and deployment."}, "weaknesses": {"value": "1.The format needs adjustments. For example, the legend of Subfigure 1d in Figure 1 obscures part of the image content, and Figure 1 is not fully explained in the main text. There is a spelling error in Section 3.1: \"as reveled as follows\" should be corrected to \"as revealed as follows\".\n2.The formulation is not rigorous enough. The complexity of generating m tokens on the first page is given as O(m((n+m)²d + P)), but the prefilling and decoding stages are not clearly separated.This may be misleading and cause readers to misunderstand the complexity proportion of the two stages.\nSome implementation details lack transparency. For example, in the online sparsification section (Page 6), there is no explanation of how “input subset sampling (RandomSelect)” is conducted or what sampling ratio is used.For the \"reselection interval nd=16\" in progressive KV compression, neither the basis for this setting nor the existence of ablation verification is provided. The lack of such details may affect the reproducibility of the method.\nThe NP-hardness proof in Appendix A.6 is relatively simplistic, as it fails to elaborate on the equivalence between line covering and set covering problems.\n3.Details regarding the construction of the benchmark dataset need to be supplemented. The specific sources of the 11 datasets are not clearly specified (e.g., whether MFQA-en is modified based on an existing dataset, and what the modification ratio is).\n4.Experimental Limitations: The method has not been tested on ultra-long contexts or low-resource devices, and the feasibility of its practical deployment remains to be verified; the latest methods (such as the improved version of StreamingLLM released at the end of 2024) are not included in the comparison baselines."}, "questions": {"value": "see the weakness part"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ql5WdlR6MO", "forum": "iyIzaoDVrT", "replyto": "iyIzaoDVrT", "signatures": ["ICLR.cc/2026/Conference/Submission5636/Reviewer_Qgxd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5636/Reviewer_Qgxd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787222967, "cdate": 1761787222967, "tmdate": 1762918170033, "mdate": 1762918170033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses efficient LLM inference in multi-turn dialogues, where quadratic attention complexity creates computational bottlenecks as conversations lengthen. The authors present empirical evidence that attention patterns are highly sparse yet dynamic and input-dependent, and that existing acceleration methods (which use fixed patterns or position-based heuristics) fail when queries do not appear at the end of the input. They propose LoopServe, a dual-phase acceleration framework consisting of: (1) online attention sparsification during prefilling that dynamically selects important components of the attention matrix for each input, and (2) progressive KV compression during decoding that periodically re-selects cached tokens based on attention patterns from recently generated outputs rather than from final input tokens. The authors construct 11 multi-turn datasets from existing benchmarks with queries repositioned at beginning, middle, and end locations, and evaluate LoopServe against six baseline methods on Llama-3.1-8B and Qwen2.5-7B models. Results show LoopServe maintains performance across all query positions while baselines degrade significantly when queries are not at the end, with ablations confirming both phases contribute to the improvement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "## Strength 1: Identification of Benchmark Bias and Adequate Motivational Analysis\n\nThe paper makes a legitimate and important observation about systematic bias in existing long-context benchmarks. By demonstrating that methods like SnapKV and AdaKV achieve strong performance when queries appear at the end but degrade significantly (often by 10+ points) when queries are repositioned to the beginning or middle, the authors expose a meaningful evaluation gap. This observation -- that the community has been evaluating on a narrow distribution of query placements that may not reflect diverse real-world scenarios --- is valuable regardless of whether their synthetic benchmark fully captures realistic multi-turn dynamics. This contribution should inform future benchmark design and evaluation practices.\n\nThe motivational experiments in Section 3, while not exhaustive, provide adequate empirical justification for the approach. The authors demonstrate that (1) attention weight concentrates in a small fraction of matrix components (10% of lines capturing 90% of weight), and (2) the specific important components vary substantially across inputs (overlap rates <50%), undermining static sparsification strategies. While a more thorough investigation would analyze these patterns across layers, models, and task types to characterize when and why variability occurs, the provided analysis is sufficient to motivate the need for adaptive selection methods. The visualizations effectively illustrate key concepts and the experimental design for these motivational studies is sound.\n\n## Strength 2: Comprehensive and Well-Executed Experimental Evaluation\n\nWithin the scope of their evaluation, the authors conduct thorough and methodical experiments. They compare against six diverse baseline methods spanning different categories (observation-based, pattern-based, streaming), test on two model families (Llama-3.1-8B and Qwen2.5-7B), and systematically vary query positions across 11 datasets. The ablation studies convincingly demonstrate that both components of their system contribute to performance, and the parameter sensitivity analysis provides practical guidance for hyperparameter selection. The paper is clearly written with effective visualizations, and the results consistently show that LoopServe maintains stable performance across query positions while baselines exhibit position-dependent degradation. While the benchmark's realism is questionable, the experimental methodology itself is sound and the evaluation is more comprehensive than many papers in demonstrating consistent behavior across diverse settings."}, "weaknesses": {"value": "## Weakness 1: Lack of Real-World Validation and Circular Benchmark Design\n\nThe work lacks validation on authentic multi-turn dialogue traces. Instead of quantifying where user queries actually occur in real conversations, the authors construct multi-turn data by repositioning queries and recombining LongBench items. This supports controlled tests but risks circularity: the benchmark emphasizes scenarios where last-window methods underperform, and the proposed method is tailored for those scenarios. Evidence from real logs (or even a descriptive analysis of query position distributions) would strengthen practical significance.\n\n## Weakness 2: Limited Design-Space Exploration and Practical Trade-Off Analysis\n\nAlthough LoopServe includes a non-trivial online prefilling sparsification phase (with an NP-hardness proof and a greedy line-selection algorithm) and a progressive output-based KV compression phase, the paper does not thoroughly explore alternative design choices or stronger baselines. For example, it does not test larger or multiple input windows, alternative attention aggregation functions, or output-aware extensions of methods like SnapKV or AdaKV. The evaluation focuses on 7–8B models with a fixed B = 1024 budget and provides limited breakdown of computational or memory overhead, leaving scalability and deployment implications unclear. While the paper includes ablations and sensitivity analyses for parameters (α, B, and n_d), a broader exploration of trade-offs and clearer guidance on when the method’s benefits outweigh its added complexity would improve confidence in its practical impact."}, "questions": {"value": "Q1: Have you analyzed query position distributions in actual multi-turn conversations (e.g., production chatbot logs, customer service transcripts) to establish how frequently queries appear at non-terminal positions in practice?\n\nQ2: Have you tested whether increasing the observation window size k for baseline methods (e.g., k ∈ {512, 1024, 2048, 4096}) would close the performance gap, or is the improvement fundamentally about using output tokens versus input tokens?\n\nQ3: What is the computational and memory overhead of LoopServe compared to baselines, and how do these costs scale with context length and generation length?\n\nQ4: Have you evaluated LoopServe on any naturally-occurring multi-turn conversations beyond the synthetic benchmark to demonstrate that the approach transfers to realistic dialogue scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KqWycNBVIr", "forum": "iyIzaoDVrT", "replyto": "iyIzaoDVrT", "signatures": ["ICLR.cc/2026/Conference/Submission5636/Reviewer_e2Wf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5636/Reviewer_e2Wf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985051147, "cdate": 1761985051147, "tmdate": 1762918169648, "mdate": 1762918169648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to facilitate the inference’s acceleration in multi-turn dialogue setting, and propose a dual-phase LLM framework with online attention scarification and progressive KV compression. Besides, an associated benchmark are constructed on top of the existing datasets. However, the motivation, comparison, and experimental settings are not clear."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper target on an important question in the era of LLMs, dealing with the efficiency problem in multi-turn scenarios, when the input context increases as the conversation goes on.\n\n2. Some necessary preliminary experimental analysis is provided."}, "weaknesses": {"value": "1. The main goal is to accelerate LLM’s inference latency in multi-turn scenarios, however, the experimental settings are not aligned. There is no detailed efficiency comparison with existing systems focus on multi-turn acceleration (Figure 5(a) only provide a small piece of efficiency compared with the base version). Besides, there are already existing multi-turn datasets in terms of conversational QA or RAG, what is the motivation to propose a new benchmark on top of the existing datasets? The construction details are lacked (only unclear description in the appendix) and lack of reliability evaluation.The author should be just claim what do they do but need to explain why this is necessary and effective as scientific contribution.\n\n2. The proposed methods contain three iterations to achieve progressive decoding. Since this is done among inference time, what is the latency of such a pipeline and why this is deserved? There is no ablation studies and comparison with existing inference-time methods.\n\n3. The paper emphasizes the focus on multi-turn, then why are the experiments conducted in single-turn datasets (e.g., 2WikiMQA, musique, hotpotqa)? I cannot understand the motivation to manipulate these datasets into multi-turn and then evaluate them, and only 3 turns interaction cannot ensure this can be thought as a multi-turn dialogue as the authors’ motivation in introduction. \n\n4. The readability is bad. Several complex algorithms are mentioned in the main content but describe in the appendix. The main content SHOULD BE SELF-CONTAINED. The main experiments table 1 is not clear. No explanation about what is begin, middle, and end in the column of “P”, and no basic information of baseline methods in the main content.\n\n5. No implementation information is provided, and no information about how to use the dataset for evaluation, e.g., the split."}, "questions": {"value": "1. What is the main difference compared to existing interence-time studies in terms of multi-turn scenarios for both efficiency and effectiveness?\n\nSee the question in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kKuoIsHlmg", "forum": "iyIzaoDVrT", "replyto": "iyIzaoDVrT", "signatures": ["ICLR.cc/2026/Conference/Submission5636/Reviewer_iTYk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5636/Reviewer_iTYk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762045251271, "cdate": 1762045251271, "tmdate": 1762918169403, "mdate": 1762918169403, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles efficient LLM inference for long, multi-turn dialogues, arguing that fixed or position-based heuristics used by many acceleration methods fail under realistic, shifting attention patterns. The authors propose LoopServe, a dual-phase system: (i) online prefilling sparsification that dynamically selects high-mass “vertical” and “slash” lines in the attention matrix to retain a target fraction of total attention weight, and (ii) progressive KV compression that repeatedly reselects important input tokens using the most recently generated outputs during decoding. They also introduce an evaluation suite of 11 multi-turn long-context datasets with queries appearing at the beginning/middle/end and cross-turn dependencies. Experiments on Llama-3.1-8B and Qwen2.5-7B show LoopServe maintains or improves quality across query positions and reduces latency, with ablations and sensitivity studies supporting the contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated and adaptive design. The paper presents clear empirical observations that attention is input-dependent and that query position strongly affects the success of KV heuristics; LoopServe’s two phases are designed to adapt in both prefilling and decoding to these realities.\n\n2. Thorough evaluation and new benchmark. Results across 11 datasets and multiple tasks (QA, summarization, few-shot) demonstrate consistent effectiveness and latency gains; the benchmark’s varied query positions addresses a common weakness of existing long-context tests."}, "weaknesses": {"value": "1. Baseline coverage. Table 1 compares against SnapKV, AdaKV, StreamingLLM, A-shape, Tri-shape, and MInference, but not H2O/Heavy-Hitter Oracle or Keyformer—both relevant KV-reduction methods the paper cites elsewhere. Including these would strengthen claims of SOTA performance."}, "questions": {"value": "1. Fairness & tuning of baselines. Authors “use suggested settings” for baselines while LoopServe has its own α, B, and n_d. Please clarify per-baseline tuning effort and whether baselines were re-tuned for the new multi-turn benchmark and non-end query positions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jL6liNVLuR", "forum": "iyIzaoDVrT", "replyto": "iyIzaoDVrT", "signatures": ["ICLR.cc/2026/Conference/Submission5636/Reviewer_9Dbm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5636/Reviewer_9Dbm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5636/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762490078811, "cdate": 1762490078811, "tmdate": 1762918169091, "mdate": 1762918169091, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}