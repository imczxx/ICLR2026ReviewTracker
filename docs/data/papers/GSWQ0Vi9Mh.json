{"id": "GSWQ0Vi9Mh", "number": 10866, "cdate": 1758183639837, "mdate": 1759897623753, "content": {"title": "DIYHealth Suite: Dataset, Model, and Benchmark for Health Management at Home", "abstract": "Generative AI is reshaping healthcare by enhancing multimodal data interpretation, clinical insight generation, and personalized decision support. However, existing advances remain tightly coupled with hospital-grade devices, restricting accessibility and use for anytime, anywhere health management in non-clinical settings. With the proliferation of wearables, mobile sensors, and telemedicine, healthcare is\nshifting toward the home, giving rise to the emerging field of Diagnosis-It-Yourself (DIY) at home, i.e., home care. Despite this promise, several distinctive challenges remain: (i) home-collected data are heterogeneous, exacerbated by the absence of standardized large-scale datasets; (ii) models require adaptation to highly variable task demands and dynamically evolving individual conditions; (iii) the broad spectrum of home care tasks lacks a unified benchmark for systematic evaluation. In this paper, we present DIYHealth Suite, a comprehensive framework designed to address these challenges through a tailored dataset, model, and benchmark. We first curate DIYHealth-900K, a large-scale multimodal dataset capturing diverse real-world home care scenarios. Building on this, we propose DIYHealthGPT, an adaptive foundation model for home-based health management, powered by the novel Hybrid Hyper Low-Rank Adaptation technique, which integrates expert mixtures with hypernetwork-driven modulation to balance cross-task generalization and instance-level personalization. Finally, we establish DIYHealthBench, the first benchmark to evaluate foundation models on home care tasks. Extensive experiments demonstrate that DIYHealthGPT delivers state-of-the-art performance over both general-purpose and medical-specific baselines on 11 home care tasks in both open-QA and closed-QA settings, laying the groundwork for the next generation of AI-driven, personalized, and scalable health management at home.", "tldr": "", "keywords": ["Large Language Model", "Large Vision Language Model", "Healthcare", "Diagnosis-It-Yourself", "Home Care"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b9c5a092e2482996454e0c845a675cd41650c857.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents DIYHealth Suite, a comprehensive framework for AI-driven home healthcare. To address challenges such as data heterogeneity, task variability, and lack of benchmarks in non-clinical settings, the authors introduce three key components: DIYHealth-900K, a large-scale multimodal dataset of real-world home care scenarios; DIYHealthGPT, an adaptive foundation model leveraging a novel Hybrid Hyper Low-Rank Adaptation technique for balancing generalization and personalization; and DIYHealthBench, the first benchmark for evaluating home care tasks. The proposed model achieves state-of-the-art results on 11 tasks, demonstrating its effectiveness for personalized, scalable health management at home."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The home healthcare scenario addressed in this work is practical and useful.\n2. The dataset and benchmark are comprehensive, covering a wide range of home healthcare tasks.\n3. The effort in constructing the dataset—despite relying solely on public sources—is commendable.\n4. The proposed adaptation method is empirically validated and demonstrates considerable effectiveness."}, "weaknesses": {"value": "1. DIYHealth-900K is constructed entirely from existing public datasets, so no new data or tasks are introduced in this work.\n2. The strong performance of DIYHealthGPT is largely attributable to training on in-distribution data, as the training and test sets are drawn from the same dataset. Comparisons against other models—whether general-purpose or medical—are therefore unfair, since those baselines are effectively evaluated in a zero-shot setting. This shifts the novelty from an \"adaptive\" foundation model to a \"unique\" dataset, which itself is not produced by this work.\n3. The proposed evaluation tasks appear relatively easy, with even small models achieving strong results and the proposed method approaching near-perfect performance in many cases. This suggests that larger models may already saturate the benchmark, limiting its usefulness for measuring state-of-the-art progress."}, "questions": {"value": "1. The paper lacks an explanation of the used evaluation metrics, such as Matthews Correlation Coefficient (MCC). Providing their definition, interpretation, and justification for use would improve the presentation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Z6YiG1RSgh", "forum": "GSWQ0Vi9Mh", "replyto": "GSWQ0Vi9Mh", "signatures": ["ICLR.cc/2026/Conference/Submission10866/Reviewer_r2KA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10866/Reviewer_r2KA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760911866343, "cdate": 1760911866343, "tmdate": 1762922081772, "mdate": 1762922081772, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a unified framework for AI-based home healthcare. It includes DIYHealth-900K, a large multimodal dataset, DIYHealthGPT, a foundation model using a novel Hybrid Hyper Low-Rank Adaptation (H2LoRA) for personalized reasoning, and DIYHealthBench, a benchmark for 11 home care tasks. Experiments show that DIYHealthGPT outperforms both general and medical foundation models in accuracy and biomedical fidelity, advancing personalized health management beyond clinical settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiments are comprehensive. Comparisons between DIYHealthGPT and baselines in both general and medical domains demonstrate the effectiveness of DIYHealthGPT, while ablation studies further showcase the correctness of rationale behind the design of each component of DIYHealthGPT. \n\n2. Abundant content regarding DIYHealthBench notably enhances the readability of this study, facilating readers' understanding on the data. \n\n3.Repo has been provided with detailed instructions and clear codes, ensuring decent reproducibility."}, "weaknesses": {"value": "1. Technically, the proposed DIYHealthGPT is not considerably novel. Previous works in both general [1] and medical [2] domains have already applied similar MoE-based architecture to benefit the training of foundation model. In my perspective, the major reason that DIYHealthGPT outperforms other baselines is that it has been further trained with data in DIYHealth-900K, thus customized to this task, rather than that it has apparently more advanced architecture or training strategy. It will be beneficial if the authors could involve more discussions regarding the performance in this perspective. \n\n[1]. Chen, Zeren, et al. \"Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE.\" The Twelfth International Conference on Learning Representations.\n[2]. Wang, Xiaochen, et al. \"FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models.\" Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 2024.\n\n2. It is still not fully clear to me that how the DIY healthcare scenario becomes different from other healthcare applications [3]. While paying notable attention to the method and data, the authors did not fully demonstrate the uniqueness of the setting, obscuring the motivation and undermining the evaluation of this study.\n\n[3]. Liu, Jie, et al. \"Asclepius: A spectrum evaluation benchmark for medical multi-modal large language models.\" Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2025."}, "questions": {"value": "I notice that although the authors have created a repo for the codes, the data are still unavailable. I am personally curious whether and how the authors would like to publicize the data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LB21yunuKK", "forum": "GSWQ0Vi9Mh", "replyto": "GSWQ0Vi9Mh", "signatures": ["ICLR.cc/2026/Conference/Submission10866/Reviewer_QoBN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10866/Reviewer_QoBN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878064207, "cdate": 1761878064207, "tmdate": 1762922081420, "mdate": 1762922081420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To solve the problem that most medical multimodels are built for hospitals. The paper introduces DIYHealth Suite, which includes DIYHealth-900K, DIYHealthGPT, and DIYBench. Research shows an average of 22.7% improvement in ACC under closed-QA and 16.7% in F1-Bio under open-QA. In addition, DIY HealthGPT receives the most Rank-1 preferences for conciseness, correctness, relevance, and actionability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Colorful and clear diagrams that are helpful for comprehension.\n2. This paper is well written.\n3. It achieves state-of-the-art performance across all home care tasks, exceeding the robust general and medical LVLM baseline by an average of 22.7% in closed-ended question accuracy and by 16.7% in open-ended question F1-Bio accuracy.\n4. This research is interesting and has clear application potential. It is built for real home settings using consumer-grade phones and wearables, covers 11 diverse tasks, delivers personalized and actionable guidance, and is clinician-validated, making it directly usable for at-home health management."}, "weaknesses": {"value": "1. Most training labels are machine-generated. Only 10% of labels are audited by humans, which could lead to the model learning the wrong things. \n2. The paper does not provide real user feedback for assessing usability.  \n3. The paper does not compare performance across subgroups such as gender, sex, race or ethnicity, age, skin tone, primary language, device type, and lighting conditions. Without subgroup checks, hidden disparities may persist, and some users could receive less accurate or less safe guidance. This weakens claims of real-world reliability and raises deployment risk."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ABZSQjulti", "forum": "GSWQ0Vi9Mh", "replyto": "GSWQ0Vi9Mh", "signatures": ["ICLR.cc/2026/Conference/Submission10866/Reviewer_4ZVG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10866/Reviewer_4ZVG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966827647, "cdate": 1761966827647, "tmdate": 1762922080951, "mdate": 1762922080951, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the problem of home care and medicine access through a multimodal question-answering framework that integrates both textual and imaging data. It presents three primary contributions: (1) the creation of a large-scale dataset, (2) the development of a GPT-based model designed for this domain, and (3) benchmark results evaluated on a curated test set."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper addresses an important gap in current research related to automated medical assistance in home care settings.\n- It is comprehensive in scope, combining dataset development, model design, and evaluation. \n- The proposed model is distinctive in its integration of task-level and instance-level cross-information, which enhances multimodal reasoning capabilities."}, "weaknesses": {"value": "- Figure 3 does not accurately reflect the corresponding mathematical formulation presented in the text.\n- The description of the publicly available datasets—collected from 20 open sources—is missing from the main paper.\n- It is unclear whether the baseline models were evaluated in a zero-shot setting. If so, this would constitute an unfair comparison with the proposed model, which is fine-tuned with task-specific data."}, "questions": {"value": "- How can matrix A be described as a shared projection if a separate one exists for each task?\n- There is ambiguity around Equation (4) and the explanation on line 286; these should be clarified.\n- The statement “We sample 10% of training data per task”—is this sampling performed in each epoch or only once during training?\n- In Stage 3, a dedicated H2LoRA block per task is mentioned, please elaborate on its implementation.\n- DIYHealthBench is derived from the designated DIYHealth-900K test set and reportedly includes 12,167 carefully sampled examples. How were these examples selected, and according to what criteria?\n- Were hyperparameter tuning procedures conducted, and if so, what methodology was followed?\n- What is the loss function used during training?\n- Please specify the model size and architectural parameters.\n- How were prompts designed for both training and evaluation?\n- What temperature settings were used for the baseline models during inference?\n- How was interrater agreement assessed, and what was the resulting Cohen’s kappa score?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bpDOQBLEDj", "forum": "GSWQ0Vi9Mh", "replyto": "GSWQ0Vi9Mh", "signatures": ["ICLR.cc/2026/Conference/Submission10866/Reviewer_srqP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10866/Reviewer_srqP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10866/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762714392084, "cdate": 1762714392084, "tmdate": 1762922080435, "mdate": 1762922080435, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}