{"id": "pAIB6Odtsu", "number": 20431, "cdate": 1758306051105, "mdate": 1759896977844, "content": {"title": "Holistic Prompting: Joint Reasoning with Reusable States and Shortcut Discovery", "abstract": "Large Language Models (LLMs) have demonstrated significant capabilities in complex reasoning tasks, often employing frameworks like Tree of Thoughts (ToT) and Chain-of-Thought (CoT). \nHowever, such methods typically rely on trajectory-based state representations, where each state encapsulates the entire history of reasoning steps. \nFor tasks with inherent solution spaces where overlapping reasoning paths frequently emerge, this inherently restricts the reuse of intermediate computations, leading to redundant exploration.\nThe effect is even more dramatic when samples can be solved jointly, as overlapping reasoning paths frequently occur across different problem instances.\nWe present Holistic Prompting, a novel framework that empowers LLMs to reuse intermediate results both within and across problem instances. \nDesigned for tasks that exhibit reusable sub-structures, Holistic Prompting unifies shared access to intermediate thoughts with an active shortcut-discovery mechanism, enabling focused search between unsolved and solved subproblems and aggressively pruning reasoning paths. \nOur experiments show that reuse is highly profitable in ToT-style breadth-first search on the math puzzle Game24 and in AlphaZero-style Monte-Carlo tree search in retrosynthetic planning.\nHere, Holistic Prompting achieves both higher success rates, while at the same time requiring fewer model invocations and outputs.", "tldr": "We enable LLMs to reuse their intermediate solutions to related samples for specific problem classes", "keywords": ["large language model", "general problem solving", "heuristic search", "reasoning", "planning", "reuse"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82922ead5c8a6af99159ce8c1b2139c46ade99ef.pdf", "supplementary_material": "/attachment/08cdff3c9469deb53f809da58ede5c28739dbff8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Holistic Prompting (HP). The authors argue that conventional \"trajectory-based state representations\", where each state encodes its entire reasoning history, are redundant and prevent the reuse of intermediate computations, especially when tasks share overlapping subproblems. HP addresses this by processing multiple problem instances jointly within a shared And-Or graph structure, utilizing \"collapsed states\" that are Markovian and self-contained. This representation allows different reasoning paths to converge on and reuse identical subproblems, both within a single sample and across different instances. A core innovation of HP is an active \"shortcut-discovery\" mechanism, a type of inverse search that finds actions to connect existing unsolved subproblems to known, previously solved states, thereby aggressively pruning the search. Experiments demonstrate HP's effectiveness in the Game24 math puzzle and retrosynthetic planning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a novel idea on reusing reasoning states.\n2. The proposed method is efficient in terms of tokens generated compared to ToT\n3. The proposed method finds better performance over ToT\n4. The shortcut discovery to intentionally arrive at already solved paths is interesting"}, "weaknesses": {"value": "1. The methodology seems to require common states that are exactly the same, so that different tasks could lead to common intermediate states, and the previous approach can be reused. Such tasks are rare, and the work only evaluates on 2 specific tasks.\n2. The presentation is not clear. The descriptions are filled with jargon and not simple, intuitive explanations or illustrations of the underlying meaning.\n3. The proposed methodology lacks a memory component. Thus, it is forced to solve all input problems simultaneously and could solve problems consecutively, storing intermediate results from prior steps."}, "questions": {"value": "1. What other types of artificial or real-world problems can the proposed method solve?\n2. Can a memory module be designed to store intermediate results for later reuse?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "S5nRm9aj3W", "forum": "pAIB6Odtsu", "replyto": "pAIB6Odtsu", "signatures": ["ICLR.cc/2026/Conference/Submission20431/Reviewer_PWvx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20431/Reviewer_PWvx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761593922259, "cdate": 1761593922259, "tmdate": 1762933874567, "mdate": 1762933874567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Holistic Prompting, a prompting framework that enables Large Language Models (LLMs) to reuse intermediate reasoning results both within and across problem instances. Existing multi-step reasoning frameworks, such as Chain-of-Thought (CoT) and Tree-of-Thoughts (ToT), usually use trajectory-based state representations. Each state encodes the full reasoning history, preventing the reuse of partial reasoning outcomes and leading to redundant computation. To address this, Holistic Prompting constructs a shared state space of intermediate thoughts, supporting cross-instance reuse and shortcut discovery between solved and unsolved subproblems.\nThe proposed framework is empirically evaluated on two tasks (Game24 and retrosynthetic planning), showing improved success rates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This paper introduces a unified framework for reasoning reuse and shortcut discovery, which conceptually bridges CoT/ToT-style prompting with retrieval-augmented reasoning paradigms."}, "weaknesses": {"value": "- Limited Evaluation. The experiments focus on two specialized domains. For example, Game24 is a quite old synthetic dataset (used in ToT). As most results of the experiment and the Appendix are reported on this dataset, it remains a question whether the proposed method can be applied to more practical domains, such as tool-use tasks [3] and coding tasks [2].\n\n- Comparison with Retrieval-Augmented Methods. The paper claims conceptual similarity to Retrieval-Augmented Generation (RAG) but does not include direct comparisons or ablations against RAG-based baselines that could also leverage reusable intermediate results [1].\n\n[1]. Buffer of thoughts: thought augmented reasoning with large language models. NeurIPS 2024.\n\n[2]. SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution.\n\n[3]. ToolRL: Reward is All Tool Learning Needs."}, "questions": {"value": "The authors are encouraged to address the concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "flD7TN1NX0", "forum": "pAIB6Odtsu", "replyto": "pAIB6Odtsu", "signatures": ["ICLR.cc/2026/Conference/Submission20431/Reviewer_k21N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20431/Reviewer_k21N"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761687380470, "cdate": 1761687380470, "tmdate": 1762933873570, "mdate": 1762933873570, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors focus on a \"memoization\" opportunity in LLM reasoning. Instead of making the LLM do its reasoning from scratch for each problem, they aim to discover and reuse common intermediate steps/results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed approach connects unsolved problem instances to already-explored reasoning paths from other samples, which is a good contribution. It is similar to dynamic programming and can support more efficient decoding and token usage.\n- The results show that while the success rates are similar, the required steps (in their domain captured as reaction and molecule nodes) are fewer.\n- The error analysis and ablation results are good."}, "weaknesses": {"value": "- The effectiveness of this approach depends on the presence of reusable sub-structures in the targeted task class; it is not clear how much overhead is introduced when overlap is minimal (or non-trivial to adapt).\n- Aggressive pruning, while controlling complexity, risks prematurely discarding valuable reasoning paths for atypical instances, potentially missing correct or novel solutions. The authors should think of situations where this can happen.\n- There is a general assumption of using the LLM for batch or clustered problem solving rather than one-shot, highly individualized queries, potentially limiting its applicability in interactive or open-ended settings. (This is fine, but it needs to be acknowledged and mentioned).\n- It would have been ideal if the authors could have connected this to RAG architectures and talked about situations where sub-structures are re-used even across problem settings or domains."}, "questions": {"value": "- Please address the points raised in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "axqAfgO9GF", "forum": "pAIB6Odtsu", "replyto": "pAIB6Odtsu", "signatures": ["ICLR.cc/2026/Conference/Submission20431/Reviewer_wPXm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20431/Reviewer_wPXm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954970903, "cdate": 1761954970903, "tmdate": 1762933872732, "mdate": 1762933872732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new reasoning method for LLMs as an alternative to chain of thought (CoT) and tree of thought (ToT). The novel contribution is to build a graph of thoughts which is shared across input samples, where edges are built between intermediate reasoning states such that the states can be reused across different samples, thereby cutting down the length of reasoning traces. The states can only be reused as exact matches (as opposed to clusters or other abstractions). Two experiments are presented, the first a simple arithmetic problem using LLMs as the base predictor and the second a chemical synthesis problem, where existing domain-specific predictors were used instead of LLMs, due to LLMs giving high errors in this domain. The results on the arithmetic problem noticeably outperformed CoT and ToT with significantly fewer intermediate states and model calls. In the chemistry task, it matched the already high performance of the existing baselines but with fewer intermediate states.\n\n\nThe paper seeks to tackle an important problem and the idea of reusing intermediate reasoning states across inputs is definitely promising. However, in its current form, I am not convinced that this method will allow such an architecture to actually scale to standard LLM text-based reasoning problems, due to a combination of my intuition about the architecture and the lack of results on complex text-based domains. If the equality test was abstracted into some form of clustering or high-level concept correspondence, that may be a different story as this could potentially compress complex state spaces. For now, I don’t believe the method is competitive."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is clearly written\n* It tackles the important and well-motivated problem of intermediate state representation and reuse in LLM reasoning\n* The main methodological contribution, a sample-shared graph allowing reuse seems novel, though bear in mind that some very recent (last couple of months) methods tackle the reuse problem (e.g. metacognitive reuse, cross-question method reuse)"}, "weaknesses": {"value": "* The scope seems very limited. Since the matched intermediate states must be very/exactly similar and are low-level states without any abstraction, it is hard to see how this method could extend beyond problems with very simple input and intermediate token sequences. If the intermediate states were whole paragraphs or even sentences, how could these be reused at all?\n* This paper is presented as a method for reasoning over LLMs, yet the second experiment didn’t use LLMs at all. If the problem precludes LLMs, you might as well use a domain-specific method rather than reasoning.\n* I’m not sure why Game24 was tested with simple baselines, without considering higher performing ones like Graph of Thoughts or Self-discover, which may also be more relevant methodologically. It’s reasonably likely these methods would have matched your performance."}, "questions": {"value": "See Weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VYzhiV8zXz", "forum": "pAIB6Odtsu", "replyto": "pAIB6Odtsu", "signatures": ["ICLR.cc/2026/Conference/Submission20431/Reviewer_rbqF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20431/Reviewer_rbqF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20431/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762532672350, "cdate": 1762532672350, "tmdate": 1762933872011, "mdate": 1762933872011, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}