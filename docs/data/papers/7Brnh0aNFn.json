{"id": "7Brnh0aNFn", "number": 11966, "cdate": 1758204919305, "mdate": 1759897542042, "content": {"title": "Spiking Discrepancy Transformer for Point Cloud Analysis", "abstract": "Spiking Transformer has sparked growing interest, with the Spiking Self-Attention merging spikes with self-attention to deliver both energy efficiency and competitive performance. However, existing work primarily focuses on 2D visual tasks, and in the domain of 3D point clouds, the disorder and complexity of spatial information, along with the scale of the point clouds, present significant challenges. For point clouds, we introduce spiking discrepancy, measuring differences in spike features to highlight key information, and then construct the Spiking Discrepancy Attention Mechanism (SDAM). SDAM contains two variants: the Spiking Element Discrepancy Attention captures local geometric correlations between central points and neighboring points, while the Spiking Intensity Discrepancy Attention characterizes structural patterns of point clouds based on macroscopic spike statistics. Moreover, we propose a Spatially-Aware Spiking Neuron. Based on these, we construct a hierarchical Spiking Discrepancy Transformer. Experimental results demonstrate that our method achieves state-of-the-art performance within the Spiking Neural Networks and exhibits impressive performance compared to Artificial Neural Networks along with a few parameters and significantly lower theoretical energy consumption.", "tldr": "", "keywords": ["Spiking Neural Networks", "Point Cloud Processing", "Efficient Computing", "Brain-inspired Computing"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa656e2d1617d02b73c2e899c0481e2ddf4475d6.pdf", "supplementary_material": "/attachment/87ff98afbd0eb5dcc5720ef5dd36d72f876d8af8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes the Spiking Discrepancy Transformer (SDT) for 3D point cloud analysis, with key contributions including the Spiking Discrepancy Attention Mechanism (SDAM), which consists of local SEDA and global SIDA variants, the Spatially-Aware Spiking Neuron (SASN) that encodes spatial information through the initial membrane potential, and achieving strong performance in the SNN domain across multiple datasets while claiming significant energy reduction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "To address the challenges of existing SNNs in 3D point cloud tasks, a complete system is proposed, ranging from attention mechanisms to neuron design and overall architecture. Additionally, the experiments are comprehensive, covering multiple tasks including classification, semantic segmentation, and object part segmentation, with thorough ablation and visualization analysis."}, "weaknesses": {"value": "1. The paper claims SSA applied to 3D point cloud tasks overlooks significant features and fails to capture local and global information, but lacks clear theoretical or experimental validation to support these claims.\n\n2. The paper asserts that \"spiking discrepancy\" better captures geometric discriminability but does not justify why disparity is more suitable for point clouds than similarity, relying solely on biological plausibility without rigorous proof.\n\n3. The presentations of SEDA and SIDA are not sufficiently clear: symbols are inconsistent, key variables and steps are ambiguously defined."}, "questions": {"value": "1.A more rigorous theoretical analysis or appropriate references must be provided to substantiate the challenges associated with applying SSA to 3D point cloud tasks.\n\n2.It is essential to validate the sensitivity of spiking discrepancy to edge features using synthetic data, or alternatively, to analyze and compare the attention map distributions of SSA versus SDAM in order to substantiate the fundamental claims made in the paper.\n\n3.In Eq. 8, why are the neighbors not weighted? The simple summation approach may allow distant neighbors to exert an undue influence, which requires further justification.\n\n4.The selection of the scaling factor s is not discussed, and the lack of ablation studies to support its choice undermines the robustness of the methodology."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rQstyx1Th1", "forum": "7Brnh0aNFn", "replyto": "7Brnh0aNFn", "signatures": ["ICLR.cc/2026/Conference/Submission11966/Reviewer_Aiaz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11966/Reviewer_Aiaz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465326991, "cdate": 1761465326991, "tmdate": 1762922964325, "mdate": 1762922964325, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a Spiking Discrepancy Transformer (SDT) designed to enhance processing of 3D point clouds using spiking neural principles combined with self-attention. Traditional Spiking Transformers have mainly focused on 2D visual tasks, but 3D point clouds pose additional challenges due to their spatial disorder and scale. To address this, the authors propose a Spiking Discrepancy Attention Mechanism (SDAM), which measures differences in spike features to highlight key spatial information. Experiments show that the proposed method achieves state-of-the-art performance among SNNs, while significantly reducing energy consumption and parameter count."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Method innovation: The paper presents a hierarchical Spiking Discrepancy Transformer (SDT) that combines bio-inspired spiking dynamics with multi-scale attention to capture both local and global 3D features. Its spiking discrepancy and spatially-aware design enhance feature discriminability, efficiency, and spatial robustness for accurate 3D perception.\n2. Paragraph Clarity: The paper demonstrates excellent narrative coherence and logical flow, presenting its ideas in a clear, progressive manner. The transitions between concepts are smooth, and the reasoning from problem to solution is both rigorous and intuitively convincing, enhancing the overall readability and scientific impact.\n3. Experiment Solidity: The researchers conducted classification and segmentation tasks, enabling the model capture 3D feature robust and accurate."}, "weaknesses": {"value": "1. Although multiple timesteps are assessed, the model still handles static point clouds, leaving the added value of temporal encoding somewhat uncertain.\n2. The ablation experiment in Figure 4 does not explicitly indicate the baseline model and makes it difficult to validate the effectiveness of using SASN alone.\n3. It would strengthen the paper to provide a clearer explanation of why SNNs are particularly advantageous for point cloud tasks, beyond potential energy efficiency, and to demonstrate this advantage empirically."}, "questions": {"value": "1. The model operates on static point clouds, and the benefits of temporal encoding remain marginal.\n2. More Results should be included in Figure 4 to prove the effectiveness of SASN.\n3. Provide a clearer explanation of why SNNs are particularly advantageous for point cloud tasks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A2SwCPrtLK", "forum": "7Brnh0aNFn", "replyto": "7Brnh0aNFn", "signatures": ["ICLR.cc/2026/Conference/Submission11966/Reviewer_krS2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11966/Reviewer_krS2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637518770, "cdate": 1761637518770, "tmdate": 1762922963920, "mdate": 1762922963920, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a hierarchical Spiking Discrepancy Transformer. Building upon spike feature discrepancies, authors construct a spiking discrepancy attention mechanism and design both spiking element discrepancy attention and spiking intensity discrepancy attention. Additionally, this work also introduce spatially-aware spiking neuron. Experimental results demonstrate the superior performance of the proposed model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-written and provides a detailed presentations of the methodology and motivations.  \n2. The proposed model demonstrates superior performance on both point cloud classification and segmentation tasks, also significantly reduce parameter count and energy consumption.   \n3. The proposed spiking discrepancy attention is well-motivated with good theoretical foundations, and its effectiveness has been validated through comprehensive experiments."}, "weaknesses": {"value": "1. A core contribution of this work lies in the design of spiking discrepancy attention. This fundamental innovation requires further validation across diverse Transformer architectures to demonstrate its generalizability.  \n\n2. The use of KNN and trigonometric functions in the spatially-aware spiking neuron raises concerns that the proposed model architecture may not be truly spike-driven[1].  \n\n3. The semantic segmentation experiments lack comparisons with SNN models.  \n\n4. Table 6 shows limited performance gains when comparing SEDA+SIDA against SEDA-only. Therefore, I am concerned that the computational burden introduced by SIDA may not justify such marginal performance improvements.  \n\n[1] Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips. ICLR24"}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pY193HubY7", "forum": "7Brnh0aNFn", "replyto": "7Brnh0aNFn", "signatures": ["ICLR.cc/2026/Conference/Submission11966/Reviewer_UkDr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11966/Reviewer_UkDr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11966/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978379957, "cdate": 1761978379957, "tmdate": 1762922961756, "mdate": 1762922961756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}