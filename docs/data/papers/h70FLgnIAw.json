{"id": "h70FLgnIAw", "number": 6220, "cdate": 1757959121529, "mdate": 1763115131107, "content": {"title": "TeFlow:  Enabling Multi-frame Supervision for Feed-forward Scene Flow Estimation", "abstract": "Self-supervised feed-forward methods for scene flow estimation offer real-time efficiency, but their supervision from two-frame point correspondences is unreliable and often breaks down under occlusions. Multi-frame supervision has the potential to provide more stable guidance by incorporating motion cues from past frames, yet naive extensions of two-frame objectives are ineffective because point correspondences vary abruptly across frames, producing inconsistent signals.\nIn the paper, we present TeFlow, enabling multi-frame supervision for feed-forward models by mining temporally consistent supervision. \nTeFlow introduces a temporal ensembling strategy that forms reliable supervisory signals by aggregating the most temporally consistent motion cues from a candidate pool built across multiple frames.\nExtensive evaluations demonstrate that TeFlow establishes a new state-of-the-art for self-supervised feed-forward methods, achieving performance gains of **up to 33\\%** on the challenging Argoverse 2 and nuScenes datasets. Our method performs on par with leading optimization-based methods, yet speeds up **150** times. The source code and model weights will be released upon publication.", "tldr": "", "keywords": ["Scene flow estimation", "Self-supervised Learning", "Point clouds", "machine vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/991bd09e0d64bd00e2f2d4d5439c2809950c5528.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates self-supervised scene flow estimation from multi-frame point clouds. It introduces a self-supervised framework that segments the scene into static and dynamic regions, and leverages temporal ensembling and voting to obtain supervision signals for the dynamic parts. Experimental results on the Argoverse 2 and nuScenes datasets show that the proposed approach achieves competitive performance with low computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed approach demonstrates competitive performance compared to other feed-forward methods on the Argoverse 2 and nuScenes datasets.\n- The experimental evaluation is comprehensive."}, "weaknesses": {"value": "1. The writing should be improved.\n\n- Figure 2 needs improvement.\n  As the key figure illustrating the overall framework, Figure 2 does not effectively help readers understand the temporal ensembling and voting algorithms. In particular, the meanings of the different colors and arrows in the motion candidate pool are not explained, making it difficult to interpret the figure.\n\n- The writing of Section 4.1 should be improved.\n  In Line 213, the paper states that \"we establish correspondences by finding, for each point p_i, its nearest neighbor in P,\" whereas in Eq. (3), the nearest-neighbor search is performed between p_k and P. This makes the process of motion candidate generation hard to follow.\n\n2. The rationale of Motion Candidate Generation needs to be further clarified.\n\n    When generating the supervisory signal from previous frames, the method directly finds the nearest neighbor of the current points in the previous frame, without warping the current points according to the (predicted) motion between the two frames. By ignoring the inter-frame motion, performing nearest-neighbor search without such warping or motion compensation becomes inappropriate for establishing accurate correspondences. It is worth noting that in self-supervised scene flow estimation, almost all self-supervised loss functions (e.g., Chamfer loss) warp the source points toward the target frame to find correspondences and thereby generate the supervision signal.\n\n3. In Eq. (5), the authors use the motion direction (i.e., cosine similarity) to measure the consistency between two flow candidates. It would be helpful to explain why the end point error (EPE) is not used. Since cosine similarity only accounts for the direction of motion while ignoring its magnitude, the consistency evaluation may be incomplete or potentially misleading."}, "questions": {"value": "1. Please explain the detailed process of Motion Candidate Generation, especially Line 213 and Eq. (3).\n2. Please clarify the rationale behind the design of Motion Candidate Generation.\n3. Why is cosine similarity used instead of the EPE for measuring the consistency? Is there any experimental evidence supporting this design choice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gt9tpfPNYu", "forum": "h70FLgnIAw", "replyto": "h70FLgnIAw", "signatures": ["ICLR.cc/2026/Conference/Submission6220/Reviewer_AJPB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6220/Reviewer_AJPB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814651043, "cdate": 1761814651043, "tmdate": 1762918552909, "mdate": 1762918552909, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "529IOKQJxe", "forum": "h70FLgnIAw", "replyto": "h70FLgnIAw", "signatures": ["ICLR.cc/2026/Conference/Submission6220/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6220/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763115129548, "cdate": 1763115129548, "tmdate": 1763115129548, "mdate": 1763115129548, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a feed-forward network that learns how to solve scene flow using temporal ensembling strategy. The results are strong and on part of the dataset examines showed significant improvement over STOA. The used technique involves adding temporal data and a joined cost function over points and blocks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The primary strength of the TeFlow method  is its introduction of cluster loss that enables balanced multi-frame supervision. While prior feed-forward methods rely on two-frame correspondence losses, TeFlow first aggregates a highly stable and temporally consistent motion target for each dynamic object cluster through a temporal ensembling strategy. This cluster-level averaging prevents the loss from being dominated by larger objects with more points, ensuring that smaller dynamic objects, such as pedestrians, receive fair and effective supervision."}, "weaknesses": {"value": "The ideas presented in this paper are not new but their combination provides strong outcome. Specifically, clustering of object-level loss enforcement was already published (and cited by the authors), as well as temporal constraints (more than two frames). Hence, while they provided solution is worthy and achieve STOA in some cases, it is an incremental improvement over known methods."}, "questions": {"value": "Please elaborate on the contribution of each item already used and known in literature over the provided solution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CLpYwBqWHu", "forum": "h70FLgnIAw", "replyto": "h70FLgnIAw", "signatures": ["ICLR.cc/2026/Conference/Submission6220/Reviewer_qK36"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6220/Reviewer_qK36"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761820267248, "cdate": 1761820267248, "tmdate": 1762918552589, "mdate": 1762918552589, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a supervised multi-frame scene flow prediction framework. To address issues such as occlusion and multi-frame temporal expansion, the proposed TeFlow presents an effective temporal aggregation strategy, according to the authors, which has significant speed improvements and performance advantages."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Good presentation and clear writng, which makes it easy to read.\n\n2. Effective method design and good performance.\n\n3. Comprehensive Experimental Validation. The study includes rigorous evaluations on two large-scale autonomous driving datasets, with detailed ablation studies on input frame count, loss components, and hyperparameters."}, "weaknesses": {"value": "1. Although the method is leading in many metrics, it can be learned that TeFlow has room for improvement on some indicators, which are areas that can be done better.\n\n2. Line 464, inconsistent capitalization.\n\n3. Has the speed of this method been averaged from multiple measurements? Specifically, how many times?"}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lBR5RFZKRL", "forum": "h70FLgnIAw", "replyto": "h70FLgnIAw", "signatures": ["ICLR.cc/2026/Conference/Submission6220/Reviewer_s9pS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6220/Reviewer_s9pS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902970084, "cdate": 1761902970084, "tmdate": 1762918552237, "mdate": 1762918552237, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}