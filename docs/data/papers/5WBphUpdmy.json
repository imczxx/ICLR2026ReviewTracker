{"id": "5WBphUpdmy", "number": 1335, "cdate": 1756872449290, "mdate": 1763668561185, "content": {"title": "$A^{4}$-MLRM: Fourfold Attention for Adaptive Hallucination Suppression in Multimodal Large Reasoning Model", "abstract": "Large multimodal reasoning models have recently shown strong ability to solve complex problems by gathering evidence and performing multi-step inference. However, the long reasoning chain makes them more prone to hallucination, that is, generating content that is not supported by the input image or the question. In examining how hallucination arises, we further identify \\emph{reasoning drift}: during evidence gathering the model over focuses on entities unrelated to the question, diluting attention on task relevant cues. As a result, previous attention-based methods developed for non-reasoning models often fail to localize the true evidence in reasoning settings. Based on these insights, in this paper, we introduce \\emph{AttnRecall}, a metric for assessing visual perception, and present \\method{}, a training free, parameter free, and architecture agnostic plugin to hallucination suppression. \\method{} uses the model output as a conduit from question to visual tokens for identifying question relevant patches and steer focus to task relevant regions. \nRemarkably, \\textbf{without any additional training}, \\method{} improves all \\textbf{reasoning} architectures (including \\texttt{R1-OneVision}, \\texttt{Ocean-R1}, \\texttt{MM-Eureka}, \\textit{etc.}) by $\\mathbf{1.21\\times}$ on reasoning benchmarks. When transferred to \\textbf{non\\mbox{-}reasoning} settings, it yields a $\\mathbf{1.16\\times}$ gain.", "tldr": "An attention-based, training-free inference method that suppresses hallucinations in multimodal large reasoning models.", "keywords": ["Hallucination", "Multimodal Large Language Models", "Multimodal Reasoning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/729e2482edb1c1a04ba035fc959f871aadee7e7a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces A4-MLRM, an architecture-agnostic, training-free, and parameter-free plugin designed to mitigate hallucination and reasoning drift (i.e., attentional diffusion toward task-irrelevant details) in MLRMs. They leverage the model’s native attention mechanism, and their strategy results in an average improvement of 1.21× on reasoning benchmarks and 1.16× when transferred to non-reasoning settings."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- A4-MLRM is completely training-free and parameter-free, resulting in minimal deployment cost, a significant advantage over prior training-stage mitigation efforts; it's also architecture-agnostic.\n\n- A4-MLRM achieves substantial performance gains, demonstrably improves all reasoning models tested (R1-OneVision, Ocean-R1, MM-Eureka, ORSTA-R1). It also successfully transfers to non-reasoning MLLMs (LLaVA-1.6-Mistral and Qwen2.5-VL), moving some models \"from near chance to the GPT-4V range\" on perception benchmarks."}, "weaknesses": {"value": "- The paper suffers from several presentation issues. It is difficult to follow, as many notations are introduced at different stages without a consistent framework established in the Background section. Moreover, key steps and observations are only presented in the Appendix, which makes the paper harder to read and the findings more difficult to assess and trust. The figures, rather than clarifying the content, are quite dense and make the paper harder to follow. I would recommend a thorough revision of **the structure of this paper**, perhaps by emphasizing the most important observations or focusing more clearly on illustrating the method itself.\n\n- A4-MLRM currently relies on a two-stage inference pipeline (Stage 1: attention mining; Stage 2: focused re-inference), which, given current reasoning-model architectures, may introduce latency and computational overhead. Moreover, the paper lacks a clear comparison of computational costs and how they scale with output length. The accuracy of online inference is closely tied to the output sequence length used in Stage 1: longer sequences provide richer priors for Stage 2 and thus higher accuracy, highlighting a trade-off between efficiency and performance, I guess..."}, "questions": {"value": "Q1: The paper identifies that the perception signal peaks around layers 18–24 in 7B architectures using AttnRecall. How stable is this finding across different model sizes (e.g., 13B or 72B variants mentioned in related work or baselines like Qwen2.5-VL) or models with fundamentally different underlying architectures?\n\nQ2: For Ocean-R1, in the case of POPE (Table 5), we observe a slight decrease in accuracy after applying A4-MLRM (from 86.77% accuracy without A4 to 85.76% with A4). Although it is quite minor, could the authors suggest reasons for such behavior? Could A4-MLRM, in some cases, exclude relevant visual context necessary for certain complex reasoning or general QA tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ELiNeqquKo", "forum": "5WBphUpdmy", "replyto": "5WBphUpdmy", "signatures": ["ICLR.cc/2026/Conference/Submission1335/Reviewer_htgT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1335/Reviewer_htgT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761589355367, "cdate": 1761589355367, "tmdate": 1762915741441, "mdate": 1762915741441, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces A4-MLRM, a training-free and architecture-agnostic inference-time approach to mitigate hallucinations in multimodal reasoning models. The method traces attention from question tokens to generated reasoning tokens and subsequently to visual patches, and re-queries the model using attention-selected regions. Experiments show improvements across several reasoning models and hallucination benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses a challenge: hallucination in multimodal reasoning models.\n- Practical and deployment-friendly design: no training, no architectural modifications.\n- Demonstrates consistent improvements across multiple MLLMs and evaluation benchmarks.\n- Includes attention layer analysis and ablations supporting empirical choices.\n- Shows transferability to non-reasoning models, indicating broader applicability."}, "weaknesses": {"value": "- Innovation is limited; the idea resembles prior attention-guided focusing / grounding strategies and is largely heuristic.\n- Heavy reliance on attention as a meaningful signal without deeper theoretical justification on its causal reliability.\n- Evaluation focuses mainly on binary hallucination settings; generalization to open-ended reasoning and complex visual tasks is unclear.\n- Sensitivity to thresholds, clustering, and sequence length is not systematically studied.\n- Two-stage inference incurs latency, and deployment cost analysis is insufficient.\n- Behavior on larger models (>7B) is not explored, raising concerns about scalability."}, "questions": {"value": "1. How reliable is attention for grounding when attention maps are known to be noisy or misaligned in some models?\n2. Are τq, τo, τv and clustering parameters fixed across all models and datasets? Any sensitivity analysis?\n3. How does the method perform on open-ended VQA and compositional reasoning tasks?\n4. Can you report latency and compute overhead for both online and offline modes?\n5. Does performance scale or saturate on larger models (e.g., 34B / 70B)?\n6. Can you provide failure case visualizations where attention routing misleads the model?\n7. How does the method compare or combine with RL-based grounding or verifier-based hallucination mitigation?\n8. Does the method risk over-focusing on small regions, losing necessary context for multi-entity reasoning?\n9. Recent work suggests that extended chain-of-thought can weaken visual grounding and increase hallucinations, closely related to the “reasoning drift” discussed here. Could the authors clarify how A4-MLRM relates to these findings and whether it mitigates the trade-off between deeper reasoning and degraded perception, especially in long-chain reasoning scenarios?\n\n- More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models\n-  More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dq2XkdbwL2", "forum": "5WBphUpdmy", "replyto": "5WBphUpdmy", "signatures": ["ICLR.cc/2026/Conference/Submission1335/Reviewer_XuFa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1335/Reviewer_XuFa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803054169, "cdate": 1761803054169, "tmdate": 1762915740067, "mdate": 1762915740067, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper first analyzes the reasons why VLMs generate hallucinations. Then they introduce a series of methods to evaluate visual perception and locate important visual regions based on the attention score. The proposed method leads to consistent performance improvement across various hallucination benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides very interesting insights about reasoning VLMs tend to produce hallucinations due to attention drift, and proposes a pipeline to locate the important visual region based on the model's attention score. \n\n- A4MLRM effectively suppresses the hallucination across multiple VLMs and benchmarks.\n\n- The paper is well-written and technically sound."}, "weaknesses": {"value": "1. The calculation of AttnRecall relies on the paired dataset, limiting its application on benchmarks without paired bounding boxes or segmentation masks.\n\n2. The evaluation is limited on hallucination benchmarks. The ability of the proposed method in reducing hallucination in other real-world tasks (such as spatial reasoning)."}, "questions": {"value": "1. In the definition of A2,  why do you choose the question tokens with high standardized variability as the key question tokens? \n\n2. Can your method improve the VLMs' performance in other visual-centric tasks such as spatial reasoning? It is suggested to evaluate your method in benchmarks including the RealWorldQA [1] and 3DSRBench [2]. If the performance is not improved, please provide some analysis about the underlying reason.\n\n3. For VLMs with stronger spatial reasoning capability such as [3], will it show better AttnRecall?\n\n[1] https://huggingface.co/datasets/visheratin/realworldqa\n\n[2] Ma, Wufei, et al. \"3dsrbench: A comprehensive 3d spatial reasoning benchmark.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2025.\n\n[3] AI, Inclusion, et al. \"M2-reasoning: Empowering mllms with unified general and spatial reasoning.\" arXiv preprint arXiv:2507.08306 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tLTSkOPn4z", "forum": "5WBphUpdmy", "replyto": "5WBphUpdmy", "signatures": ["ICLR.cc/2026/Conference/Submission1335/Reviewer_ndkQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1335/Reviewer_ndkQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1335/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958481118, "cdate": 1761958481118, "tmdate": 1762915739901, "mdate": 1762915739901, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}