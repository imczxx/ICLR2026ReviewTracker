{"id": "WDP5b3mtFV", "number": 5107, "cdate": 1757848933963, "mdate": 1759897994304, "content": {"title": "Quantile Advantage Estimation for Entropy-Safe Reasoning", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning but training often oscillates between {entropy collapse} and {entropy explosion}.\nWe trace both hazards to the mean-baseline used in value-free RL (\\eg GRPO/DAPO), which improperly penalizes negative-advantage samples under reward outliers.\nWe propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise $K$-quantile baseline.\nQAE induces a response-level, two-regime gate: on hard queries ($p \\le 1{-}K$) it reinforces rare successes, while on easy queries ($p > 1{-}K$) it targets remaining failures.\nUnder first-order softmax updates, we prove {two-sided entropy safety}, giving lower/upper bounds on one-step entropy change that curb explosion and prevent collapse.\nEmpirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned $K$, roughly 80\\% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME'24/'25 and AMC'23.\nThese results identify {baseline design}—rather than token-level heuristics—as the primary mechanism for scaling RLVR.", "tldr": "", "keywords": ["RLVR", "LLM reasoning", "entropy explosion", "advantage estimation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d77a2c52f186e4d908a2d84ebb4d54c09d2e1b9a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses entropy instability in Reinforcement Learning with Verifiable Rewards (RLVR). The authors observe that standard training algorithms can suffer from either entropy collapse or entropy explosion. The paper traces this instability to the mean-based advantage baseline which is overly sensitive to reward outliers. The authors then propose Quantile Advantage Estimation (QAE) that replaces the mean baseline with a group-wise quantile. For hard queries, it exclusively reinforces successes, while it exclusively penalizes failures for easy ones. This mechanism is shown to stabilize entropy and produce good performance gains on challenging reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "A primary strength of this work is its clear and insightful diagnosis of entropy instability, both its collapse and explosion, in RLVR. This allows the authors to provide an intuitive, theoretically motivated  (Proposition 4.2), and practically appealing solution. \n\nThey replace the advantage baseline with a quantile. The decomposition of the learning objective (Proposition 4.1) further clarifies why the quantile baseline reshapes the learning signal. I also especially like Figure 4 for showing that negative-advantage samples drive entropy explosion.\n\nThe experiments are well-done, illustrative, and extremely beneficial to the readers!\n\nI also particularly appreciate that QAE is a tool that fits nicely in the the broader RLVR ecosystem. The results in Table 2 are particularly strong, showing that QAE can be layered on top of diverse and powerful existing methods"}, "weaknesses": {"value": "- The core mechanism is a hard threshold based on the quantile K. This means a tiny change in the success rate around the threshold can cause a dramatic flip in the learning objective. This could introduce its own form of training instability, especially with noisy or borderline cases. This may be fine in the case of binary rewards might will be an issue for more general setups with continuous rewards.  Even with binary rewards, the paper uses a fixed G=32. How does the method's performance change with smaller G? For a small G (e.g., 4 or 8), the empirical p(q) will be extremely noisy, which could make the hard-thresholding gate flip erratically,\n\n- The paper frames the ~80% zero-advantage rate as a positive aspect. This can also be viewed as inefficient data use. In tasks that are less sparse or require learning a wider variety of skills, ignoring the learning signal from 80% of generated samples could be highly detrimental.\n\n- I am not exactly sure that the entropy instability is causing the shy gains in performance. Can you pinpoint to which of your experiments tell us we need to control entropy because it is shown to causally worsen performance? My impression is  that QAE's success doesn't come from \"taming entropy.\" It comes from creating a more effective learning curriculum. By setting the baseline to 1 on easy queries, QAE effectively tells the model, \"Stop wasting capacity on problems you've already solved.\" By setting the baseline to 0 on hard queries, it tells the model, \"Focus all your attention on learning from the rare successes you find on these hard problems.\" I would love your thoughts on this based on your experiments."}, "questions": {"value": "- The paper argues that anthropomorphic tokens are markers of an unproductive entropy explosion, yet Figure 2 shows their initial rise correlates strongly with the initial pass@1 gains. This feels strange to me. How would Figure 2 and 3 look like if you had applied QAE? Does it reduce the frequency of anthropomorphic token frequency initially? Does that affect performance?\n\n- I am not sure about the statement in Line 201. I am convinced that ClipHigher mitigates early collapse. However, in both DAPO with and without Clip-Higher, the pass@1 performance seems stable after a few steps (although a bit less without Clip-Higher). Hence, it is not clear to me that entropy explosion is the culprit behind limited scaling. Also, out of curiosity if you have results, how would the training dynamics in Figure 2 and 3 look like for pass@16? And what was the $\\epsilon_\\text{high}$ value you considered in Figures 2 and 3?\n\n- Negative-advantage samples drive entropy explosion, but what drives the collapse? Both positive and negative samples have virtually the same collapse in Figure 4.\n\n- Can you explain line 428? What is the sample efficiency reading?\n\n- Can you explain your results in Figure 6 (b, c)? I encourage you to elaborate on the connection with your theoretical results.\n\nI would suggest citing [1] because they study the learning dynamics in RLVR very carefully. The results there might be interesting to you. \n\n[1] Mroueh, Y. (2025). Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss, Dynamics, and Success Amplification. arXiv preprint arXiv:2503.06639."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XjNiGJ75OK", "forum": "WDP5b3mtFV", "replyto": "WDP5b3mtFV", "signatures": ["ICLR.cc/2026/Conference/Submission5107/Reviewer_GY1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5107/Reviewer_GY1t"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761435719671, "cdate": 1761435719671, "tmdate": 1762917878508, "mdate": 1762917878508, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Quantile Advantage Estimation (QAE) for RLHF. The key insight is that existing mean-based advantage estimation often suffers from either entropy collapse (limited exploration) or entropy explosion (limited exploitation). QAE addresses this by replacing the mean with a K-median estimator, enabling a better balance between exploration and exploitation through the choice of K. The paper also provides analytical insights into the dual benefits of this approach and presents encouraging empirical improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ QAE is compatible with many existing RLHF algorithms, including GRPO and GSPO.\n+ The design is simple, intuitive, and easy to integrate.\n+ Evaluation results show good accuracy gains."}, "weaknesses": {"value": "- The novelty is incremental, primarily modifying mean advantage estimation to a median-based variant.\n- Evaluations are limited to three math reasoning tasks, and the baselines are not fully consistent across settings."}, "questions": {"value": "Thank you for submitting your work to ICLR 2026. The paper is well-written, clearly motivated, and proposes a friendly modification that addresses a meaningful issue in RLHF training stability. However, the contribution is somewhat incremental, and the evaluation setup makes it difficult to assess the robustness of QAE's improvement over state-of-the-art (SOTA) methods.\n\n- Q1: How does QAE perform on Qwen3-8B with GSPO? The reported results are promising, but given the incomplete baseline coverage across experiments, it is hard to gauge its competitiveness against SOTA approaches.\n\n- Q2: Can the authors include additional datasets or tasks, such as LiveCodeBench, to test the generality of QAE beyond math reasoning tasks?\n\n- Q3: It would strengthen the paper to align the experimental settings with SOTA baselines (e.g., using GSPO configurations and pass@32 metrics as reported in prior work). Even partial replication or ablation under comparable setups would make the evaluation more convincing.\n\nFinally, I appreciate the discussion on future directions, especially the idea of dynamic $K$. Exploring adaptive quantile strategies could meaningfully enhance both the novelty and practical utility of the approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fWPeFoxWY9", "forum": "WDP5b3mtFV", "replyto": "WDP5b3mtFV", "signatures": ["ICLR.cc/2026/Conference/Submission5107/Reviewer_4HDB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5107/Reviewer_4HDB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445855157, "cdate": 1761445855157, "tmdate": 1762917878235, "mdate": 1762917878235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "They propose Quantile Advantage Estimation (QAE), which replaces the mean with a K-quantile baseline, forming a two-regime gating mechanism that reinforces rare successes on hard queries and penalizes remaining failures on easy ones. Theoretically, QAE provides two-sided entropy safety, bounding both over-exploration and over-exploitation. Experiments on AIME’24/’25 and AMC’23 with Qwen3-8B/14B/30B show consistent pass@1 gains while keeping pass@16 stable."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation of the paper is clear.\n\n2. QAE requires minimal modification, just a quantile replacement, but achieves consistent empirical improvements and stability, making it highly practical for large-scale RLVR systems."}, "weaknesses": {"value": "1. The model settings appear inconsistent across the paper. The abstract mentions Qwen3-8B/14B-Base, while Table 2 reports results for Qwen3-8B-Base and Qwen3-30B-A3B-Base. In Appendix D.1, Qwen3-32B is also referenced. However, only two of these models are actually reported in Table 2, and the appendix does not include the missing results. This inconsistency makes the experimental setup confusing.\n\n2.  The benchmarks used (AIME-24 (30 problems), AIME-25 (30), AMC (40)) have small test sizes and inherently high variance (even with repeated sampling), meaning that a difference of one or two problems could significantly change the reported pass@1 score. Since the reported gains are typically around 1–2 examples, it would strengthen the paper to include larger or more diverse benchmarks to verify the robustness of the improvement.\n\n3.  In DAPO training, responses with zero advantage are discarded and resampled to maintain batch size (dynamic sampling). Since QAE introduces more zero-advantage responses, does this increase training overhead or cause slower convergence? Clarifying this would help assess the practical efficiency of the method.\n\n4. Do the authors consider evaluating the methods in other areas other than math?"}, "questions": {"value": "listed in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P0P5qzPPMs", "forum": "WDP5b3mtFV", "replyto": "WDP5b3mtFV", "signatures": ["ICLR.cc/2026/Conference/Submission5107/Reviewer_Z6EA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5107/Reviewer_Z6EA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971879029, "cdate": 1761971879029, "tmdate": 1762917877840, "mdate": 1762917877840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an alternative to the mean baseline commonly used in RL advantage calculations. It shows that the mean baseline focuses learning on moderately difficult problems, while the proposed approach focuses it on rare successes and residual failures, sparsifying the LLM outputs that contribute to the gradient. The method is well supported by theoretical and empirical evidence, which comes from evaluations on three math reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Originality** Addressing entropy collapse/explosion by altering the advantage’s baseline is novel.\n\n**Quality** The two regimes induced by the proposed baseline are intuitively beneficial and supported by experiments with multiple models and training objectives. Also, mathematical analysis motivates and supports the approach well.\n\n**Clarity** The paper is well written. The figures and tables clearly communicate results.\n\n**Significance** RL with LLMs is a highly impactful area, and this work illustrates a simple way to keep entropy in a helpful regime, avoiding harmful growth/collapse. The community will likely be interested in this."}, "weaknesses": {"value": "Some of the statements lack sufficient support. See the questions/suggestions section below for more details and how to address this.\n\nIt's left unclear if the proposed method's gains appear with other models and with other, non-math RL problems (e.g. coding or game playing). I think the method is still valuable if it's only relevant to a subset of RL problems, but its broad applicability (or the absence thereof) should be mentioned in the text."}, "questions": {"value": "Line 115: this should be $\\pi_{ref}$, not $\\pi_{old}$, for the KL reg.\n\nLine 146: Can you provide evidence/citations/intuition for statements about the dangers of entropy explosion like this one?\n- “Gradients are swamped by noise” \n\nLine 200: In this sentence, the idea that entropy explosion limits training is stated as if there is a proven causal relationship, but the experiments only show correlation between entropy growth and limited performance. \n- “Thus, although Clip-Higher mitigates early collapse, its rapid escalation is coupled with entropy explosion that ultimately limits scaling.”\n\nLine 244: I think a comparison of entropy between DAPO with and without clip-higher is needed to support this statement. Currently, only anthropomorphic token counts are compared (Figure 2), not entropy.\n\nKL divergence regularization can mitigate over-optimization and preserve diversity. Can it help address the entropy explosion that you find to be driven by negative samples?\n\nHow would the proposed approach perform on non-Qwen models, on models that have very low initial success rates, or in settings with fewer responses sampled per prompt? You could address this question by training Llama 3.1 8B base on MATH with k=16, for example. It would also be interesting to see Reasoning Gym results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VovCkfpEzu", "forum": "WDP5b3mtFV", "replyto": "WDP5b3mtFV", "signatures": ["ICLR.cc/2026/Conference/Submission5107/Reviewer_LBMg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5107/Reviewer_LBMg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5107/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974614368, "cdate": 1761974614368, "tmdate": 1762917877501, "mdate": 1762917877501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}