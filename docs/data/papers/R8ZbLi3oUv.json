{"id": "R8ZbLi3oUv", "number": 16337, "cdate": 1758263339735, "mdate": 1763744890527, "content": {"title": "From Recall To Reasoning: Understanding the Role of Associative Memory in Hybrid Architectures", "abstract": "The demand for efficient inference has driven the development of subquadratic architectures as alternatives to the Transformer, though their capacity for complex, algorithmic reasoning remains a critical open question. To investigate the effect of architectural choice on downstream reasoning performance, we conduct a controlled study of reasoning scaling laws, training from scratch multiple hybrid-attention architectures of the same size (150M and 500M parameters) across three model classes (Mamba, Gated Linear Attention, Gated Delta Net) on a unified mathematical reasoning curriculum. Furthermore, we apply parallel test-time scaling methods via majority voting, and discover a clear trend that the amount of Attention layers increases reasoning performance. To investigate this trend, we analyze the models’ responses using LLM-as-a-Judge and categorize reasoning errors into 8 distinct types inspired by taxonomies in math education, identifying in-context associative recall as the primary error mode in attention-free architectures. As we move toward fully linear models without any attention layers, our findings establish a connection between the choice of architectural update rule and systematic failures. In particular, we find that hybrid models with Gated Delta Net can match and even exceed the performance of pure Transformers on mathematical reasoning. We present a principled empirical study that informs the design and evaluation of next-generation hybrid reasoning models.", "tldr": "", "keywords": ["Associative Memory", "Subquadratic Architectures", "Test Time Scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da3046fbe0ceb433981af19019bb5b5aa85e5678.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the influence of architectural design on mathematical reasoning by training 150M-parameter hybrid models from scratch, including Mamba, Gated Linear Attention (GLA), and Gated Delta Net (GDN) variants, with attention layer proportions ranging from 0% to 50%. Utilizing a consistent math curriculum (OpenMathInstruct-2 and MetaMathQA), the study assesses performance through parallel test-time scaling (majority voting up to 64 samples) on GSM8K and MATH benchmarks. Results reveal a \"reasoning gap,\" where subquadratic models lag behind Transformers and gain less from additional compute, though performance improves with more attention layers. An LLM-as-a-judge analysis categorizes errors into eight types (e.g., key-value binding, state tracking), identifying associative memory failures as the primary limitation in attention-free architectures, thus linking efficiency trade-offs to reasoning challenges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Robust experimental control**: Training all models from scratch on a unified dataset removes biases from pre-trained Transformers or distillation, ensuring a rigorous and equitable comparison across hybrid architectures.\n\n**Detailed error taxonomy**: Drawing inspiration from mathematics education, the fine-grained error classification effectively ties subquadratic shortcomings to associative memory deficits, enhancing mechanistic understanding.\n\n**Actionable insights**: The clear \"dose-response\" relationship between attention layers and reasoning performance offers valuable guidance for hybrid model development."}, "weaknesses": {"value": "For conclusion, the focus on associative memory failures in mathematical reasoning echoes prior recall ability studies [1], suggesting the paper may not significantly advance hybrid model insights beyond established findings.\n\nThe observation that some hybrids (e.g., GDN50) achieve competitive pass@1 scores but diverge from Transformers with higher samples (as shown in figures) lacks sufficient explanation, leaving a gap in understanding this critical behavior.\n\n\n\n[1] Arora et al. Zoology: Measuring and Improving Recall in Efficient Language Models. arXiv:2312.04927"}, "questions": {"value": "Why do certain hybrids (e.g., GDN50) initially outperform Transformers in pass@1 on GSM8K but fall behind with increased samples—could this reflect specific architectural strengths or training artifacts?\n\nHow might these results evolve with larger models or non-mathematical domains, and what targeted adjustments (e.g., increased state sizes) could mitigate the associative memory gap without incorporating additional attention?\n\nIs there any best trade-off between efficiency and performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Klho4WuJPU", "forum": "R8ZbLi3oUv", "replyto": "R8ZbLi3oUv", "signatures": ["ICLR.cc/2026/Conference/Submission16337/Reviewer_NHaW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16337/Reviewer_NHaW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662612161, "cdate": 1761662612161, "tmdate": 1762926472283, "mdate": 1762926472283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines hybrid-attention model performance on mathematical reasoning tasks, with a focus on the role of attention layers and the models’ ability to do things like key-value binding. The authors analyse the models’ reasoning traces and suggest that poor in-context associative memory is the cause of worse performance of the hybrid models on reasoning tasks relative to a more vanilla transformer, and that including more attention layers tends to improve performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Controlled experiment setup, exploring 3 different hybrid architectures and focussing on varying the amount of attention layers. Evidence of solid hyperparameter sweeps to ensure stable training and fair comparisons in Appendix A.\n- Analysis of failure cases in reasoning is interesting and reasonable evidence for associative memory being important for reasoning."}, "weaknesses": {"value": "- The MATH results in appendix A are a little concerning: if memory fidelity is quite important, why does the transformer underperform or perform similarly to the Mamba/GDN models for more complicated math tasks? It makes me concerned that the analysis in the main text is fairly specific to GSM8k problems.\n- As noted in the paper itself, it would be useful to explore if this is an issue in larger models and/or models that have undergone more pretraining.\n- The trend of increasing attention reducing key value binding errors seems a little weak - in figure 3, going from 50->100% attention *increases* key-value binding errors, and in Figure 9, going past 25% attention also seems to not have much effect on the error rate. It would be useful to have more data points here. For GDN, having 100% attention seems to have more errors than 0% attention (figure 3 right)!\n\nWhile I like the study in this paper, I think that the GDN results seem to push against the hypothesis provided by the authors, with it often bucking the trends of the other models. It would be useful to get more explanations around this."}, "questions": {"value": "- Do you have any idea why GDN seems to not suffer from having 0-100% attention nearly as much as the other hybrid models?\n- See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jNBn5KMlcs", "forum": "R8ZbLi3oUv", "replyto": "R8ZbLi3oUv", "signatures": ["ICLR.cc/2026/Conference/Submission16337/Reviewer_bLs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16337/Reviewer_bLs3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667228893, "cdate": 1761667228893, "tmdate": 1762926471715, "mdate": 1762926471715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines hybrid-attention model performance on mathematical reasoning tasks, with a focus on the role of attention layers and the models’ ability to do things like key-value binding. The authors analyse the models’ reasoning traces and suggest that poor in-context associative memory is the cause of worse performance of the hybrid models on reasoning tasks relative to a more vanilla transformer, and that including more attention layers tends to improve performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Controlled experiment setup, exploring 3 different hybrid architectures and focussing on varying the amount of attention layers. Evidence of solid hyperparameter sweeps to ensure stable training and fair comparisons in Appendix A.\n- Analysis of failure cases in reasoning is interesting and reasonable evidence for associative memory being important for reasoning."}, "weaknesses": {"value": "- The MATH results in appendix A are a little concerning: if memory fidelity is quite important, why does the transformer underperform or perform similarly to the Mamba/GDN models for more complicated math tasks? It makes me concerned that the analysis in the main text is fairly specific to GSM8k problems.\n- As noted in the paper itself, it would be useful to explore if this is an issue in larger models and/or models that have undergone more pretraining.\n- The trend of increasing attention reducing key value binding errors seems a little weak - in figure 3, going from 50->100% attention *increases* key-value binding errors, and in Figure 9, going past 25% attention also seems to not have much effect on the error rate. It would be useful to have more data points here. For GDN, having 100% attention seems to have more errors than 0% attention (figure 3 right)!\n\nWhile I like the study in this paper, I think that the GDN results seem to push against the hypothesis provided by the authors, with it often bucking the trends of the other models. It would be useful to get more explanations around this.\n\nedit: the authors have explored larger models and fixed some errors regarding the MATH experiments, which alleviates my concerns, and I am raising my score as a result."}, "questions": {"value": "- Do you have any idea why GDN seems to not suffer from having 0-100% attention nearly as much as the other hybrid models?\n- See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jNBn5KMlcs", "forum": "R8ZbLi3oUv", "replyto": "R8ZbLi3oUv", "signatures": ["ICLR.cc/2026/Conference/Submission16337/Reviewer_bLs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16337/Reviewer_bLs3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667228893, "cdate": 1761667228893, "tmdate": 1763767751138, "mdate": 1763767751138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For investigating the efficient inference for reasoning (particularly for arithmetic reasoning), the authors test different architectures with different scaling laws for the same size of the models. They analyze the results with the 8 categories depending on the responses across hybrid subquadratic architectures such as Mamba, GLA, GDN versus Transformers, trained on mathematical reasoning datasets.\n\nTasks requiring in-context recall reveal a consistent performance gap: models without attention layers struggle to maintain associative bindings and state updates, leading to key–value binding and state-tracking errors. The authors argue that attention provides a non-parametric associative memory that recurrent subquadratic models lack. The authors conclude that attention provides a high-fidelity associative memory essential for robust reasoning, whereas subquadratic recurrent models face intrinsic limitations in memory fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) Motivation is timely and useful. \n- Efficient architectures for reasoning (especially Mamba-type SSMs) are the hot topic right now. Asking whether attention is fundamentally required for reasoning is a real and important question.\n\n2) The taxonomy of mathematical reasoning errors are elegant.\n- The empirical findings reveal a consistent pattern of in-context associative memory failures across architectures, highlighting a shared limitation in reasoning mechanisms when attention is reduced or removed.\n\n3) The work contributes to the growing understanding of hybrid architectures and their potential in bridging the efficiency–reasoning gap."}, "weaknesses": {"value": "1) The presentation lacks clarity in explaining technical details and experimental designs \n- no methodological details to comprehend the contributions, not good enough to replicate the work.\n- not sure about the evaluation methods - not quite clear to imagine how its been done? how the LLM-as-a-Judge annotations were validated are under-specified?\n- how did you implement and evaluate each categories of reasoning error mechanisms?\n\n2) The applicable domain is too narrowed to arithmetic reasoning, limiting from the general reasoning domains.\n\n3) Empirical studies seem not complete to support the current conclusions/ contributions of the work \n- No ablation on training stability, dataset influence, or architectural components beyond attention percentage."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pjquSQPM6h", "forum": "R8ZbLi3oUv", "replyto": "R8ZbLi3oUv", "signatures": ["ICLR.cc/2026/Conference/Submission16337/Reviewer_ws5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16337/Reviewer_ws5E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129805449, "cdate": 1762129805449, "tmdate": 1762926471052, "mdate": 1762926471052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Summary"}, "comment": {"value": "# Summary of Major Updates: 500M Scaling, Mechanistic Analysis of Gated DeltaNet, and Improved Exposition\n\nWe thank the reviewers for their detailed and helpful feedback. We have uploaded a revised manuscript with a significantly expanded experimental suite and clearer explanation of our methodology. In addition, we obtain novel results which reinforce our core finding: while associative memory remains a bottleneck for pure SSMs, some hybrid architectures such as Gated DeltaNet (GDN) can effectively bridge this gap. Notably, we provide a mechanistic explanation into the relevant components of the GDN architecture and comment on how our best performing models match recent findings in large-scale hybrid models from industry (e.g., Qwen3-Next and Kimi-Linear).\n\nBelow, we summarize the four major updates to the paper:\n\n### 1. Scaling to Larger Models (500M)\nReviewers asked if the \"reasoning gap\" holds at larger scales. We trained and evaluated 500M parameter versions of all architectures (Transformer, Mamba, GLA, GDN) on the same curriculum. We include these results in the main body (Figure 1 Bottom) and find:\n*   **The Reasoning Gap:** The gap persists for pure Mamba and GLA models, which leverage test-time compute less effectively than Transformers.\n*   **Hybrid Efficiency:** 50% Hybrid models (particularly GDN) at 500M scale are able to match or slightly exceed the Transformer baseline.\n*   **Error Analysis:** We confirm that KV-Binding Errors generally decrease as Attention percentage increases (Figure 3 Bottom), reinforcing our hypothesis that attention acts as a high-fidelity associative memory.\n\n### 2. Mechanistic Analysis of Gated DeltaNet\nReviewers noted that GDN outperforms the general trend, performing well even with low attention. To explain this, we conducted targeted ablations on the GDN layer (Table 1). We found that **token-dependent decay ($\\alpha$)** in the GDN update rule is the most critical component. When the $\\alpha$ mechanism is disabled or restricted to scalar decay, GDN's performance reverts to having a higher error rate.\n\nThis confirms that while Mamba struggles with information decay, GDN succeeds because it can selectively \"overwrite\" specific associations. This provides an independent scientific explanation for the design choices of very recent models like Qwen3-Next and Kimi-Linear, which both converge to an optimal ratio of 75% Gated DeltaNet.\n\n### 3. Correction to MATH500 Methodology\nIn response to valid concerns regarding the Transformer's underperformance on the MATH dataset in our initial appendix, we reinvestigated our evaluation pipeline with different configurations. We discovered that the generation length (1024 tokens) was insufficient for the complex reasoning required by MATH500, leading to premature truncation. Instead, we re-ran these experiments with a generation length of 2048 (Appendix A.1). \n\nThe Transformer now outperforms the Mamba/GLA models as expected, resolving the anomaly. However, GDN hybrids remain highly competitive, further validating the strength of the Delta rule.\n\n### 4. Exposition & Reproducibility\nWe have significantly expanded Section 3 and Appendix C to detail our dataset composition and LLM-as-a-Judge pipeline. We provide clear examples of the 8 distinct error modes identified in real model answers. We emphasize that we will release the full codebase—including the dataset, training protocol, inference scripts, and the automated error analysis pipeline—upon publication to ensure full reproducibility.\n\n### 5. Final Confirmatory Experiments\nTo complete the sweep of determining the optimal hybrid ratio, we are currently finalizing runs for GDN, GLA, and Mamba with 75% Attention (Mamba25, GLA25, GDN25) at both 150M and 500M scales. While these architecture ratios generally would not be used in practice as they would provide very little efficiency gains, we want to explore them for more fine-grained analysis. Preliminary results align with our existing trends, and we will update the paper with the final data points as soon as the runs complete."}}, "id": "vWBdMGvTRG", "forum": "R8ZbLi3oUv", "replyto": "R8ZbLi3oUv", "signatures": ["ICLR.cc/2026/Conference/Submission16337/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16337/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16337/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763753659573, "cdate": 1763753659573, "tmdate": 1763753659573, "mdate": 1763753659573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}