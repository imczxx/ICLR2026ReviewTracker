{"id": "R8ZbLi3oUv", "number": 16337, "cdate": 1758263339735, "mdate": 1759897246716, "content": {"title": "From Recall To Reasoning: Understanding the Role of Associative Memory in Hybrid Architectures", "abstract": "The demand for efficient inference has driven the development of subquadratic architectures as alternatives to the Transformer, though their capacity for complex, algorithmic reasoning remains a critical open question. To investigate the effect of architectural choice on downstream reasoning performance, we conduct a controlled study of reasoning scaling laws, training from scratch multiple hybrid-attention architectures of the same size (150M parameters) across three model classes (Mamba, Gated Linear Attention, Gated Delta Net) on a unified mathematical reasoning curriculum. Furthermore, we apply parallel test-time scaling methods via majority voting, and uncover a clear trend showing an improvement in reasoning performance as we increase the amount the amount of Attention layers in the architecture. To explain this trend, we analyze the models’ responses using llm-as-a-judge and categorize its errors into 8 distinct types inspired by taxonomies in math education, identifying associative recall as the primary error mode in attention-free architectures. As we move toward fully linear models without any attention layers, our findings establish a connection between the choice of architectural update rule and systematic failures on reasoning primitives such as state-tracking and associative memory. We present a principled empirical study that informs the design and evaluation of next-generation hybrid reasoning models.", "tldr": "", "keywords": ["Associative Memory", "Subquadratic Architectures", "Test Time Scaling"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1dd0471bc6abcbc48f5f5e3fa9098d65db9ae99f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the influence of architectural design on mathematical reasoning by training 150M-parameter hybrid models from scratch, including Mamba, Gated Linear Attention (GLA), and Gated Delta Net (GDN) variants, with attention layer proportions ranging from 0% to 50%. Utilizing a consistent math curriculum (OpenMathInstruct-2 and MetaMathQA), the study assesses performance through parallel test-time scaling (majority voting up to 64 samples) on GSM8K and MATH benchmarks. Results reveal a \"reasoning gap,\" where subquadratic models lag behind Transformers and gain less from additional compute, though performance improves with more attention layers. An LLM-as-a-judge analysis categorizes errors into eight types (e.g., key-value binding, state tracking), identifying associative memory failures as the primary limitation in attention-free architectures, thus linking efficiency trade-offs to reasoning challenges."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Robust experimental control**: Training all models from scratch on a unified dataset removes biases from pre-trained Transformers or distillation, ensuring a rigorous and equitable comparison across hybrid architectures.\n\n**Detailed error taxonomy**: Drawing inspiration from mathematics education, the fine-grained error classification effectively ties subquadratic shortcomings to associative memory deficits, enhancing mechanistic understanding.\n\n**Actionable insights**: The clear \"dose-response\" relationship between attention layers and reasoning performance offers valuable guidance for hybrid model development."}, "weaknesses": {"value": "For conclusion, the focus on associative memory failures in mathematical reasoning echoes prior recall ability studies [1], suggesting the paper may not significantly advance hybrid model insights beyond established findings.\n\nThe observation that some hybrids (e.g., GDN50) achieve competitive pass@1 scores but diverge from Transformers with higher samples (as shown in figures) lacks sufficient explanation, leaving a gap in understanding this critical behavior.\n\n\n\n[1] Arora et al. Zoology: Measuring and Improving Recall in Efficient Language Models. arXiv:2312.04927"}, "questions": {"value": "Why do certain hybrids (e.g., GDN50) initially outperform Transformers in pass@1 on GSM8K but fall behind with increased samples—could this reflect specific architectural strengths or training artifacts?\n\nHow might these results evolve with larger models or non-mathematical domains, and what targeted adjustments (e.g., increased state sizes) could mitigate the associative memory gap without incorporating additional attention?\n\nIs there any best trade-off between efficiency and performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Klho4WuJPU", "forum": "R8ZbLi3oUv", "replyto": "R8ZbLi3oUv", "signatures": ["ICLR.cc/2026/Conference/Submission16337/Reviewer_NHaW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16337/Reviewer_NHaW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662612161, "cdate": 1761662612161, "tmdate": 1762926472283, "mdate": 1762926472283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines hybrid-attention model performance on mathematical reasoning tasks, with a focus on the role of attention layers and the models’ ability to do things like key-value binding. The authors analyse the models’ reasoning traces and suggest that poor in-context associative memory is the cause of worse performance of the hybrid models on reasoning tasks relative to a more vanilla transformer, and that including more attention layers tends to improve performance."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Controlled experiment setup, exploring 3 different hybrid architectures and focussing on varying the amount of attention layers. Evidence of solid hyperparameter sweeps to ensure stable training and fair comparisons in Appendix A.\n- Analysis of failure cases in reasoning is interesting and reasonable evidence for associative memory being important for reasoning."}, "weaknesses": {"value": "- The MATH results in appendix A are a little concerning: if memory fidelity is quite important, why does the transformer underperform or perform similarly to the Mamba/GDN models for more complicated math tasks? It makes me concerned that the analysis in the main text is fairly specific to GSM8k problems.\n- As noted in the paper itself, it would be useful to explore if this is an issue in larger models and/or models that have undergone more pretraining.\n- The trend of increasing attention reducing key value binding errors seems a little weak - in figure 3, going from 50->100% attention *increases* key-value binding errors, and in Figure 9, going past 25% attention also seems to not have much effect on the error rate. It would be useful to have more data points here. For GDN, having 100% attention seems to have more errors than 0% attention (figure 3 right)!\n\nWhile I like the study in this paper, I think that the GDN results seem to push against the hypothesis provided by the authors, with it often bucking the trends of the other models. It would be useful to get more explanations around this."}, "questions": {"value": "- Do you have any idea why GDN seems to not suffer from having 0-100% attention nearly as much as the other hybrid models?\n- See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jNBn5KMlcs", "forum": "R8ZbLi3oUv", "replyto": "R8ZbLi3oUv", "signatures": ["ICLR.cc/2026/Conference/Submission16337/Reviewer_bLs3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16337/Reviewer_bLs3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761667228893, "cdate": 1761667228893, "tmdate": 1762926471715, "mdate": 1762926471715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For investigating the efficient inference for reasoning (particularly for arithmetic reasoning), the authors test different architectures with different scaling laws for the same size of the models. They analyze the results with the 8 categories depending on the responses across hybrid subquadratic architectures such as Mamba, GLA, GDN versus Transformers, trained on mathematical reasoning datasets.\n\nTasks requiring in-context recall reveal a consistent performance gap: models without attention layers struggle to maintain associative bindings and state updates, leading to key–value binding and state-tracking errors. The authors argue that attention provides a non-parametric associative memory that recurrent subquadratic models lack. The authors conclude that attention provides a high-fidelity associative memory essential for robust reasoning, whereas subquadratic recurrent models face intrinsic limitations in memory fidelity."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1) Motivation is timely and useful. \n- Efficient architectures for reasoning (especially Mamba-type SSMs) are the hot topic right now. Asking whether attention is fundamentally required for reasoning is a real and important question.\n\n2) The taxonomy of mathematical reasoning errors are elegant.\n- The empirical findings reveal a consistent pattern of in-context associative memory failures across architectures, highlighting a shared limitation in reasoning mechanisms when attention is reduced or removed.\n\n3) The work contributes to the growing understanding of hybrid architectures and their potential in bridging the efficiency–reasoning gap."}, "weaknesses": {"value": "1) The presentation lacks clarity in explaining technical details and experimental designs \n- no methodological details to comprehend the contributions, not good enough to replicate the work.\n- not sure about the evaluation methods - not quite clear to imagine how its been done? how the LLM-as-a-Judge annotations were validated are under-specified?\n- how did you implement and evaluate each categories of reasoning error mechanisms?\n\n2) The applicable domain is too narrowed to arithmetic reasoning, limiting from the general reasoning domains.\n\n3) Empirical studies seem not complete to support the current conclusions/ contributions of the work \n- No ablation on training stability, dataset influence, or architectural components beyond attention percentage."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pjquSQPM6h", "forum": "R8ZbLi3oUv", "replyto": "R8ZbLi3oUv", "signatures": ["ICLR.cc/2026/Conference/Submission16337/Reviewer_ws5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16337/Reviewer_ws5E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16337/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129805449, "cdate": 1762129805449, "tmdate": 1762926471052, "mdate": 1762926471052, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}