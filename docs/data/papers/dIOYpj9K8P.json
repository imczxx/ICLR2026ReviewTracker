{"id": "dIOYpj9K8P", "number": 11972, "cdate": 1758204937764, "mdate": 1759897541745, "content": {"title": "Reformulation for Pretraining Data Augmentation", "abstract": "Despite the impressive capabilities of large language models across various tasks, their continued scaling is severely hampered not only by data scarcity but also by the performance degradation associated with excessive data repetition during training.\nTo overcome this critical bottleneck, we introduce the Massive Genre-Audience (MGA) reformulation method, a framework designed to augment corpora in a way that supports more effective model performance scaling. \nInstead of relying on complex, predefined seed systems, MGA systematically reformulates existing corpora into diverse, contextually-rich variations by adaptively generating genre-audience pairs. \nWe present this framework and the resulting 770 billion token MGACorpus, created as a practical instantiation of our methodology.\nWe experimentally validate MGA's core benefits by demonstrating superior scaling properties, in terms of both model size and data budget, against data repetition and upsampling (up to 13B parameters).\nFurthermore, our comprehensive analysis investigates the role of synthesis principles in generation quality and reveals nuances in evaluating model capabilities using standard loss metrics.\nOur work shows that a systematic framework like MGA provides a reliable pathway to substantially augment training datasets, effectively alleviating repetition bottlenecks and enabling more efficient scaling of large language models.", "tldr": "", "keywords": ["Large Language Models", "Data Augmentation", "Synthetic Pretraining Data"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c81623faffa1b933b078d849c811a2ce73afd4a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Massive Genre Audience (MGA), a reformulation method to augment training corpora based on generating multiple diverse genre-audience pairs per inference pass through a prompting-based method. This is done with the intention of mitigating the influence of data repetition during training that often leads to performance degradation particularly in data scarcity scenarios. The authors proceed to analyze the effectiveness of their method (including whether the diversity introduced is actually beneficial) and how it synergizes with alternate synthetic data strategies. Models at different scales are evaluated across a wide range of datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well organized and clear. Specific research questions are laid out and experiments are directly designed to support each one in a clear manner.\n2. Comprehensive results across many datasets and models at 5 different scales.\n3. The paper emphasizes and shows the synergistic nature of their method. \n4. Authors state they will release MGACorpus and all artifacts for reproducibility.\n5. The motivation of addressing negative influences of data repetition during training is important."}, "weaknesses": {"value": "1. The method is based on prompt engineering and as such there are some potential issues with bias introduced by using a solely LLM based system. For example, when the LLM is used for judging, what sorts of bias is being introduced? Was some form of human evaluation done or some alternative test to determine whether the scores produced by the LLM align with the actual quality of the text. Furthermore there should be some evidence that actual diverse outputs are being generated by the method. I believe that there should also be some results on text heterogeneity quantification to pinpoint the actual heterogeneity introduced by the sampling method which would allow us to concretely correlate with the performance.\n\n2. Tied to the above, some discussion on specific design decisions is warranted, namely the choices behind the genre audience pairs. Also it is stated that the SLM \"executes a “one-pass-for-many” strategy ... since repeated sampling requests to a model can yield highly similar outputs\" but doing so can lead to bias since later generated pairs are conditioned on the early ones compared to random sampling (although as mentioned some means to stop potential redundancy is needed). If heterogeneity could be quantified and these two methods compared this assertion could be validated.\n\n3. Given the similarities in the idea, some discussion of previous works on synthetic data creation using personas [1] is warranted in the paper.\n\n4. Only one model family is tested (SmolLM) for MGA-Expansion which presents a limitation in determining how well the method will work on alternative architectures.\n\n5. Table 3 is not referred or discussed anywhere in the text. \n\n[1] Tao Ge et al. Scaling Synthetic Data Creation with 1,000,000,000 Personas. Arxiv. 2024"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rlYzVLQTq8", "forum": "dIOYpj9K8P", "replyto": "dIOYpj9K8P", "signatures": ["ICLR.cc/2026/Conference/Submission11972/Reviewer_eWsn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11972/Reviewer_eWsn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707195237, "cdate": 1761707195237, "tmdate": 1762922968936, "mdate": 1762922968936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new way of synthetically augmenting pretraining data through Genre Audience reformulation. \nThis paper works, and its numbers are solid. The science is also thorough."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The greatest strength is the scaling plots, which demonstrate consistent performance improvement in the token-matched regime. Really nice! Figure 5 was particularly great. The training noise seems relatively less which is good for trusting the downstream evals. \n2. Really nice overall benchmark numbers. Grounds the work with good numbers to help set this up relative to other methods.  Achieving a MMLU score of 40.7 is great for the 1.7B model class. \n3. Beating finding more hq data is really good result.  Particularly Figure 3 is strong! The graphs showed consistent improvement across all scales, beating the use of a more original dataset. \n4. Also, a good set of benchmark coverage. LightEval and LM-Harness are both great tools for eval and using the set of evals like MMLU is great."}, "weaknesses": {"value": "My main gripe is that I would have liked to see a few more baselines of synthetic data/rephrasing. WRAP, for example, would have been nice to show. I understand that this would have been expensive at the higher token counts, but at the lower token counts, it would have been nice. There are great synthetic pretraining data techniques. Could you add WRAP or just adding other synthetic post-training datasets (nemotron for example) would be great?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jXeQstJu8T", "forum": "dIOYpj9K8P", "replyto": "dIOYpj9K8P", "signatures": ["ICLR.cc/2026/Conference/Submission11972/Reviewer_HLbn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11972/Reviewer_HLbn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761938860638, "cdate": 1761938860638, "tmdate": 1762922968429, "mdate": 1762922968429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a synthetic data generation pipeline for pretraining language models. Their pipeline, MGA (Massive-Genre Audience), asks a lightweight 3.3B MoE model to rewrite source text for different “genres” (e.g. story, dialogue, textbook) and audiences “girl”, “grandpa”, and “teacher”. MGA provides better scaling properties for training language models than training on the original data alone."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Most of the experiments in this paper are done very thoroughly, and results are presented in a very neat and orderly manner. I appreciate that the authors release an open-source dataset and also release tooling and other artifacts that make this work reproducible."}, "weaknesses": {"value": "A key thing I wanted to know in this paper was whether the author’s proposed approach, MGA, outperforms alternative approaches for generating synthetic data. Even though the authors say that “MGA is not in competition with but is complementary to other synthetic data methodologies” (line 370), the authors should properly demonstrate MGA’s utility in the current landscape of synthetic data generation strategies. I am not sure if I got a clear answer to this question from the current version of the paper. \n\nSection 4.3.1. Claims that MGA can complement existing synthetic data approaches. There, a plot shows the following, as I quote: \n> - Baseline: A high-quality real dataset fineweb-edu.\n> - Exp A: 35% token budget replaced by Nemotron-CC-HQ synthetic corpus (+Nemotron-Syn).\n> - Exp B: 35% token budget replaced by MGACorpus (+MGA).\n> - Exp C: 70% token budget replaced by an equal combination of Nemotron-Syn and MGACorpus data (+Nemotron-Syn +MGA).\n\nThe authors’ results show that though A outperforms B, C outperforms A and C outperforms B. However, one key experiment missing here is whether a 70% token budget replaced by only Nemotron-Syn outperforms C, because it if it does, then it makes the inclusion of any MGACorpus/MGA data not necessary and potentially detrimental."}, "questions": {"value": "The authors have a nice analytical discussion section around data reformulation diversity, learning characteristics, and mode collapse – it does make me curious if some of the conceptual findings here generalize to other synthetic data generation processes. \n\nI may have accidentally missed this somewhere, but from where did you source the range of possible “genres” (structural/stylistic formats, e.g. dialogue) and “audiences” (reader profiles, e.g. teacher) for your pipeline?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4fkT3nWEUX", "forum": "dIOYpj9K8P", "replyto": "dIOYpj9K8P", "signatures": ["ICLR.cc/2026/Conference/Submission11972/Reviewer_W91b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11972/Reviewer_W91b"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995242851, "cdate": 1761995242851, "tmdate": 1762922967926, "mdate": 1762922967926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}