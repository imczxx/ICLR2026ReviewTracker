{"id": "7D7VLU9227", "number": 15345, "cdate": 1758250426625, "mdate": 1763677202374, "content": {"title": "PSP: Prompt-Guided Self-Training Sampling Policy for Active Prompt Learning", "abstract": "Active Prompt Learning (APL) using vision-language models (\\textit{e.g.}, CLIP) has attracted considerable attention for mitigating the dependence on fully labeled dataset in downstream task adaptation. However, existing methods fail to explicitly leverage prompt to guide sample selection, resulting in the selected samples being ineffective in facilitating the prompt template's downstream task adaptation, while also overlooking valuable complementary information in the unselected samples. To fill this gap, we propose a novel Prompt-Guided Self-Training Sampling Policy (PSP) for APL, which integrates Soft Actor-Critic with a customized real-pseudo hybrid reward and vectorized critics to incorporate prompts in guiding sample selection toward those that facilitate the optimization of prompt template, by jointly considering both selected and unselected samples. Specifically, PSP comprises two prominent components: Vectorized Soft Actor-Critic Sampling Policy (VSSP) and Uncertainty Augmented Self-Training (UST) mechanism.  VSSP customizes a real-pseudo hybrid reward based on learned prompts and image features, which is fed into vectorized critics to estimate Q-value for each sample and compute gradients that optimize the actor, allowing it to refine its sampling policy in an End-to-End manner to identify the most informative samples for prompt learning. Moreover, UST leverages the CLIP from the previous round to generate reliable pseudo-labeled data based on uncertainty and confidence of average predictions, thereby deepening the understanding of the overall data. Extensive experiments conducted on diverse real-world datasets validate the effectiveness of our PSP.", "tldr": "We propose a Prompt-Guided Self-Training Sampling Policy (PSP) for Active Prompt Learning, which integrates Soft Actor-Critic with a customized real-pseudo hybrid reward and vectorized critics.", "keywords": ["Active Prompt Learning", "Reinforcement Learning", "CLIP"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6e3f5815074b7c26d1c6f8858aba480dcccde4bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes PSP, a Prompt-Guided Self-Training Sampling Policy for Active Prompt Learning with CLIP models. The method integrates a Vectorized Soft Actor-Critic Sampling Policy (VSSP) and an Uncertainty-Augmented Self-Training (UST) mechanism. VSSP introduces a real-pseudo hybrid reward and vectorized critics to guide sample selection using prompt information, while UST generates reliable pseudo-labeled data based on uncertainty and confidence. Extensive experiments on seven benchmark datasets show consistent gains over PCB and other active learning baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed PSP improves over strong PCB variants with clear gains in accuracy on multiple datasets, and the evaluation on on seven datasets is comprehensive.\n\n2.  The paper provides detailed mathematical formulation for VSSP and UST mechanisms."}, "weaknesses": {"value": "I am not an expert in active learning, so I might defer my final score after considering other reviewers’ comments. Nonetheless, I identify two main weaknesses:\n\n1.\tWriting structure and overly complex presentation.\nThe Introduction and Method sections could be reorganized to better highlight the motivation and intuition before introducing dense equations. Currently, the paper is overloaded with mathematical notations, which obscure the core ideas. The motivation for the proposed approach is not convincing, and the discussion on the limitations of prior work lacks depth.\n\n\n2.\tPractical clarity.\nThe method appears as an overly complicated mixture of existing techniques, which raises doubts about its real-world applicability and scalability."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N.A."}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FSbJk96lZt", "forum": "7D7VLU9227", "replyto": "7D7VLU9227", "signatures": ["ICLR.cc/2026/Conference/Submission15345/Reviewer_dwtc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15345/Reviewer_dwtc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459895161, "cdate": 1761459895161, "tmdate": 1762925636492, "mdate": 1762925636492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PSP, an active prompt learning framework for CLIP to reduce annotation costs. PSP uses the prompt to guide sample selection and considers selected and unselected data. Moreover, PSP introduces two components: VSSP, an RL policy with a prompt-derived reward function to select informative samples, and UST, a self-training mechanism that generates and filters reliable pseudo-labels from unselected data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The conceptual integration of prompt learning with RL is a strength.\n\n2. The paper's claims are substantiated by extensive experiments across various aspects."}, "weaknesses": {"value": "1.The paper suffers from several clarity issues that hinder understanding. Several symbols are used before definition. For example, $K$ (presumably the number of classes) is used on 213 without definition. $\\hat{y}_i^k$ (Eq. 5) is not explicitly defined, though it seems to be the teacher CLIP's prediction if $k=u$. This pattern of undefined symbols increases reading difficulty.\n\n2.The paper defaults to SAC (a 2018 algorithm) without justifying this choice over simpler selection methods (e.g., a learned Mixture-of-Experts) or newer, more advanced RL algorithms. The specific advantages of SAC for this particular selection problem are not discussed.\n\n3.The equation (2) seems redundant with the text on 215(e.g.,  $f_{V}^{t,i}=F_{V}^{t}(x_{i}^{u})$  ).\n\n4.The principle behind (2),(4),(5) is not explained.\n\n5.Table 8: Comparing PSP to CoOp and CoCoOp is problematic. (a) These are outdated baselines; newer methods like [1]MaPLe or [2]PromptSRC can be used. (b) The comparison is fundamentally unfair, as PSP uses both labeled and unlabeled data, while CoOp/CoCoOp only \"using the same number of real-labeled training samples\". A fair comparison would require augmenting CoOp/CoCoOp with a pseudo-labeling strategy or a direct comparison against unsupervised methods, like [3]PromptKD.\n\n6.Comparing a task-adapted PSP (fine-tuned on downstream data) to zero-shot VLMs is an apples-to-oranges comparison. This does not prove PSP is better. A valid comparison would be to apply the PSP to these other VLMs to demonstrate its adaptability, as was done for SigLIP (Table 7).\n\n7.The dataset (presumably DTD) in Table 9 is not explicitly named. The numbers under the rounds (e.g., \"305, 380...\") are undefined. What do they represent? I am curious about the additional training time (or computational overhead) incurred by PSP compared to methods like CoOp.\n\n8.The paper frequently refers to \"student CLIP\" and \"teacher CLIP\". The CLIP model's parameters are frozen; only the prompts are being trained. The correct terms may be \"student prompt\" and \"teacher prompt\" to reflect what is actually being updated.\n\n[1]MaPLe: Multi-modal Prompt Learning\n\n[2]Self-regulating Prompts: Foundational Model Adaptation without Forgetting\n\n[3]PromptKD: Unsupervised Prompt Distillation for Vision-Language Models"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iC0pQ8BVbR", "forum": "7D7VLU9227", "replyto": "7D7VLU9227", "signatures": ["ICLR.cc/2026/Conference/Submission15345/Reviewer_aJjs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15345/Reviewer_aJjs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761472732212, "cdate": 1761472732212, "tmdate": 1762925636156, "mdate": 1762925636156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new method called PSP for the field of Active Prompt Learning (APL). Its main goal is to efficiently train prompts for vision-language models (such as CLIP) under a limited annotation budget by intelligently selecting the most valuable data samples for labeling, thereby adapting the prompts to downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "[1] The experiments in the paper are comprehensive.\n[2] The challenging decision of “which samples to select for labeling” is modeled as a reinforcement learning task. This allows the sampling policy (Actor) to act like an agent that continuously “learns” and “evolves” through interaction with the environment (model performance), ultimately mastering the most efficient sampling strategy. This approach is more flexible and intelligent than traditional methods that rely on fixed rules, such as uncertainty or diversity.\n[3] The paper combines active learning techniques for vision-language models with reinforcement learning, effectively improving the efficiency of sample learning."}, "weaknesses": {"value": "[1] The algorithm incurs excessive computational overhead.\n[2] The system is relatively complex, and parameter tuning may be challenging."}, "questions": {"value": "[1]  In such “low-data” or “few-episode” reinforcement learning scenarios, how does your method ensure that the Actor policy effectively converges rather than overfitting to the limited early experiences? Does your design of the Vectorized Critic play a key role here, as it can provide richer learning signals for multiple samples from a single experience?\n[2] Have you analyzed how the learned policy behaves compared to traditional active learning heuristics, such as uncertainty sampling, diversity sampling, or hybrid methods like BADGE? For instance, does the model prefer high-uncertainty samples at certain stages while prioritizing diversity at others? Can we reverse-engineer the learned “optimal policy” to extract interpretable new sampling insights that go beyond existing heuristic rules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2cY4zwPvcn", "forum": "7D7VLU9227", "replyto": "7D7VLU9227", "signatures": ["ICLR.cc/2026/Conference/Submission15345/Reviewer_gdcF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15345/Reviewer_gdcF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620836110, "cdate": 1761620836110, "tmdate": 1762925635779, "mdate": 1762925635779, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the problem of sample selection in Active Prompt Learning (APL) for Vision-Language Models (VLMs). The authors argue that existing APL methods use generic active learning heuristics that do not explicitly use information from the learned prompt to guide sample selection and also ignore valuable information in the large pool of unselected data. To address this, the paper proposes PSP (Prompt-Guided Self-Training Sampling Policy), a framework that learns a sampling policy using reinforcement learning (RL). The method consists of two main parts: (1) a Vectorized Soft Actor-Critic Sampling Policy (VSSP), where an RL agent learns to select informative samples based on a state representation of gradient embeddings and a custom \"real-pseudo hybrid reward\" that reflects the prompt's performance; and (2) an Uncertainty Augmented Self-Training (UST) mechanism that generates pseudo-labels for unselected data to augment the training set. Experiments on seven image classification datasets show that PSP outperforms several active learning baselines, including variants of the state-of-the-art PCB method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Formulation for Sample Selection: The core idea of framing active sample selection for prompt tuning as a policy learning problem is novel and conceptually appealing.\n2. Explicit Integration of Prompt Information: A key strength is the explicit use of the learned prompt's performance to guide the sampling policy. \n3. Leveraging the Full Unlabeled Dataset: The UST mechanism is a sensible addition that attempts to extract more value from the entire unlabeled dataset, rather than only focusing on the small subset of samples selected for annotation\n4. Comprehensive Experimental Evaluation: The paper presents an extensive empirical study on seven datasets"}, "weaknesses": {"value": "1. Extreme System Complexity for Modest Performance Gains: The primary weakness of this work is the immense complexity of the proposed VSSP framework relative to the performance improvement it delivers. \n2. Convoluted and Indirect Reward Function: The reward function in Equation 4, defined as $r(s_{t},a_{t})=log(p_{m}(g))*(\\overline{r}_{s}+\\beta\\overline{r}_{p})$, is indirect and unintuitive.\n3. Potential Instability and High Sample Complexity of RL: Actor-critic methods are known to be sensitive to hyperparameters and can be unstable to train, especially with limited data."}, "questions": {"value": "1. The reward function is constructed as a product of the log-probability of the sampling scheme and the classification error. This objective is quite indirect. Have you experimented with a more direct reward signal, such as the negative validation accuracy on a small holdout set, or simply using $- (\\overline{r}_{s} + \\beta\\overline{r}_{p})$ as the reward? \n2. The VSSP agent is trained on a very small number of total experiences (a maximum of 8, one per round). Given the high sample complexity and potential instability of RL algorithms, how do you ensure that the learned policy is stable and generalizes beyond the few trajectories seen during training?\n3. Could you provide a more detailed justification for using Soft-DTW to align the Q-value vectors between states $s_t$ and $s_{t+1}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1byyMjLSoe", "forum": "7D7VLU9227", "replyto": "7D7VLU9227", "signatures": ["ICLR.cc/2026/Conference/Submission15345/Reviewer_7H9h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15345/Reviewer_7H9h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15345/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955533854, "cdate": 1761955533854, "tmdate": 1762925634965, "mdate": 1762925634965, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Changes present in revised version"}, "comment": {"value": "We would like to thank all reviewers for the insightful reviews and comments. Next, we record the following changes in the revised submission, with the changes highlighted in yellow:\n\n- We have provided a more detailed justification for using Soft-DTW to align the Q-value vectors between states $s_t$ and $s_{t+1}$.\n\n- We have analyzed how the learned policy behaves compared to traditional active learning heuristics.\n\n- We have added the definition of $K$ at line 221 and provided an explicit definition of $\\hat{y}_i^u$ near Equation 5.\n\n- We have replaced $F_V^t(x_i^u)$ with $f_V^{t,i}$ in updated Equation 2 to simplify the formula.\n\n- We have explained the principle behind Equation 2, 4 and 5.\n\n- We have conducted experiments where only the features of unlabeled samples are used as the state in Section 4.3. \n\n- We have explicitly specified the dataset in Table 9 as DTD and clarified that the numbers under each round (e.g., \"305, 380...\") represent the per-round training time (in seconds) in revised Appendix A.3. \n\n- We have reorganized the Introduction and Method sections to better highlight the motivation. \n\n- We have deepen the discussion on the limitations of prior work in the Introduction and Related Work sections."}}, "id": "gC7gSgLlKK", "forum": "7D7VLU9227", "replyto": "7D7VLU9227", "signatures": ["ICLR.cc/2026/Conference/Submission15345/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15345/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission15345/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763677610380, "cdate": 1763677610380, "tmdate": 1763677882482, "mdate": 1763677882482, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}