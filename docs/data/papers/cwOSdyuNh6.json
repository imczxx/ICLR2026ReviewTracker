{"id": "cwOSdyuNh6", "number": 18190, "cdate": 1758284891910, "mdate": 1763763212744, "content": {"title": "Enhanced Generative Model Evaluation with Clipped Density and Coverage", "abstract": "Although generative models have made remarkable progress in recent years, their use in critical applications has been hindered by an inability to reliably evaluate the quality of their generated samples. Quality refers to at least two complementary concepts: fidelity and coverage. Current quality metrics often lack reliable, interpretable values due to an absence of calibration or insufficient robustness to outliers. To address these shortcomings, we introduce two novel metrics: $\\textit{Clipped Density}$ and $\\textit{Clipped Coverage}$. By clipping individual sample contributions, as well as  the radii of nearest neighbor balls for fidelity, our metrics prevent out-of-distribution samples from biasing the aggregated values. Through analytical and empirical calibration, these metrics demonstrate linear score degradation as the proportion of bad samples increases. Thus, they can be straightforwardly interpreted as equivalent proportions of good samples. Extensive experiments on synthetic and real-world datasets demonstrate that $\\textit{Clipped Density}$ and $\\textit{Clipped Coverage}$ outperform existing methods in terms of robustness, sensitivity, and interpretability when evaluating generative models.", "tldr": "", "keywords": ["Generative model evaluation", "Metrics", "Density", "Coverage"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e0df368377bd3f338d092e7b7ec93cf25cb102f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a pair of synthetic data fidelity and diversity metrics called clipped density and clipped coverage. They build on the previous density and coverage metrics, adding clipping to guard against pathological behaviors and normalisation to ensure the values are between 0 and 1 in practice, and degrade linearly if a proportion of samples is corrupted. The new metrics are evaluated using a some common sanity checks on image data, and a larger set of checks on artificial data from a recent benchmark. The new metrics pass all the image checks, and pass more artificial checks than previous metrics. The paper also compares many image generators on four standard image datasets with the new and existing metrics. The results with the new metrics are seemingly more informative than the ones with other metrics."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper convincingly argues that existing generative model fidelity and diversity metrics are not sufficient for many use cases, and explains why fixing these problems is important. The new metrics are presented clearly, and the writing overall is good. The ideas are novel, and the normalisations the paper introduces may also be useful with other fidelity and diversity metrics. The results on the sanity checks in the paper are good: the new metrics pass all of the image checks, and pass more artificial checks than any other metric."}, "weaknesses": {"value": "The main weakness of the paper is the weak evaluation of the linear degradation claim for clipped density and clipped coverage. This claim is only supported by a few checks on image data. However, both metrics still fail many of the bounds checks from Räisä et al. (2025), indicating that the metrics do not always have the extreme values 0 and 1 in practice, which is a prerequisite for linear degradation. This means that the claim that only 40% of generated samples on CIFAR-10 and FFHQ are good, in Section 5.3, is not supported. It would be interesting to see a visual comparison between real and synthetic images for the datasets and models (or subsets of them) in Section 5.3. If the comparison shows a clear difference between the proportions of good and bad synthetic samples between the small and large datasets, this would provide more evidence that clipped density and clipped coverage can be interpreted as such proportions.\n\nThe undesirable behaviors the other metrics are claimed to have in Section 5.3 and Appendix I are not supported by evidence, with the exception of density having values greater than 1. This is because it is not known what the behavior should be: it is possible that in this setting, fidelity and diversity should be correlated, or that all the models have similar fidelity, or that the score ranges should be restricted, so there is no evidence that any of these behaviors is undesirable. Besides, it is not fair so say that correlated scores fidelity and diversity scores are a downside of other metrics, as clipped density and clipped coverage are also highly correlated in Figure 6.\n\nIn my opinion, clipping clipped density to 1 only hides the problem of the score exceeding 1 and does not increase interpretability. As a result, it would be good to include results without this clipping in the paper, or at least check when the unclipped score exceeded 1.\n\nMinor points:\n- In Figure 4b, clipped density is only close to 1 at the left edge, not extremely close to 1, which could be considered a failure compared to some of the other metrics.\n- Figure 5b: it is very difficult to judge how symmetric symRecall is.\n- Markers in all figures are small, so their shapes are hard to see.\n- Figures 5 and 5: \"Precall\" -> \"P-recall\", and same for \"Pprecision\".\n- Figure 8: the bar labels are hard to associate with the correct bars, especially on the right. Rotating them more should fix this."}, "questions": {"value": "No further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DvBbxjCVVm", "forum": "cwOSdyuNh6", "replyto": "cwOSdyuNh6", "signatures": ["ICLR.cc/2026/Conference/Submission18190/Reviewer_GJ4y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18190/Reviewer_GJ4y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756880449, "cdate": 1761756880449, "tmdate": 1762927939651, "mdate": 1762927939651, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper clearly identifies limitations of current generative model evaluation metrics (e.g., FID, Precision/Recall): they combine fidelity and coverage into a single number, lack calibration, and are sensitive to outliers. This motivates the need for metrics that separately assess fidelity and coverage and provide interpretable, robust scores. The authors introduce Clipped Density and Clipped Coverage. Both metrics cap individual sample contributions at 1, preventing over‑occurring samples from masking defects and ensuring robustness to outliers. Clipped Density further clips nearest‑neighbor radii to the median k‑th neighbor distance, limiting the effect of sparse or outlier real data points. Clipped Coverage counts the number of synthetic samples within each real sample’s ball, normalizes by k, and caps at 1."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Calibration for Interpretability: The metrics are calibrated so that their expected value decays linearly as the fraction of bad synthetic samples increases. For Clipped Density, the unnormalized score is divided by the fidelity score computed on the real data and then clipped to [0,1]. For Clipped Coverage, the authors derive the expected value under an i.i.d. assumption using Beta functions and numerically invert it to map unnormalized scores to linear decay. This calibration allows scores to be interpreted directly as the proportion of “good” samples.\n\n- Comprehensive Testing: The paper proposes a suite of controlled tests (mode dropping, introduction of out‑of‑distribution samples, synthetic data translation) to evaluate robustness, sensitivity, and interpretability. Experiments show that existing metrics often conflate fidelity with coverage or are unstable, whereas Clipped Density and Clipped Coverage exhibit linearity, stability, and symmetry across tests."}, "weaknesses": {"value": "- Incremental Novelty: While clipping sample contributions and radii is a sensible modification, the metrics essentially adapt existing Density/Coverage measures. The idea of capping contributions is intuitive; no fundamentally new notion of fidelity or coverage is introduced. The theoretical calibration for Clipped Coverage relies on i.i.d. assumptions and numerical inversion, which is mathematically involved but conceptually just rescales the metric to match a linear decay.\n\n- Scope Limited to Images: Experiments and definitions focus on image data. Although the authors claim the framework is generic, there is no evidence it extends to other modalities (video, audio).\n\n- The absolute scores are interpreted as equivalent proportions of “good” samples, but this relies on synthetic tests where bad samples are pure noise. In practice, generative models may produce subtle artifacts or distributional shifts that do not correspond neatly to “bad samples,” and the linear calibration may not reflect perceptual quality. The tests used to justify the metrics (mode dropping, noise replacement, translation) are tailored; there may be other failure modes (e.g., adversarial patterns, style transfer) where the metrics behave unpredictably."}, "questions": {"value": "- Have you tested the metrics on non‑image generative models (e.g., video or audio)? If not, what challenges do you foresee in extending the method to other modalities?\n\n- Interpretation of Scores: The calibration interprets a score of 0.4 as “40 % good samples and 60 % bad samples”. In real generative models, “bad samples” may not be pure noise but may include subtle artifacts or low‑quality variations. How should practitioners interpret intermediate scores in such contexts?\n\n- For Clipped Coverage, the calibration requires computing and numerically inverting an expectation using Beta functions. How sensitive is the final metric to errors in this inversion?\n\n- Have you conducted any human evaluation to correlate Clipped Density/Coverage scores with perceived quality or diversity? Without such correlation, it is unclear if the metrics align with human judgments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VX690jrK4f", "forum": "cwOSdyuNh6", "replyto": "cwOSdyuNh6", "signatures": ["ICLR.cc/2026/Conference/Submission18190/Reviewer_oPHo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18190/Reviewer_oPHo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772521069, "cdate": 1761772521069, "tmdate": 1762927938998, "mdate": 1762927938998, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the metrics Clipped Density and Clipped Coverage, which are enhanced based on the Density and Coverage metrics [1]. The enhancement comes in two aspects: (1) robustness to outliers, and (2) that the metrics are linear and normalized (thus more interpretable). This is achieved by capping per-sample contribution, normalization, and calibration based on empirical analyses. The effectiveness is validated by comprehensive experiments covering several competing metrics, different types of generative models, and various sizes of datasets.\n\n\n\n\n[1] Naeem, Muhammad Ferjad, et al. \"Reliable fidelity and diversity metrics for generative models.\" International conference on machine learning. PMLR, 2020."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is nicely written and easy to read, equipped with nice visualizations.\n- The study is well motivated. \n- It is novel that the proposed metrics are designed to satisfy robustness, linearity, and interpretability.\n- The analyses and experiments are comprehensive."}, "weaknesses": {"value": "- The modifications applied to meet the proposed desiderata appear ad-hoc and lack theoretical justification. This raises the question of whether they might negatively impact performance in other aspects, such as distinguishability."}, "questions": {"value": "- In Eq. (7), why should we normalize by $ClippedDensity_{real}$? When would $\\frac{ClippedDensity_{unnorm}}{ClippedDensity_{real}}$ be larger than 1 and how should we interpret it? Why can it be safely clipped to 1?\n- It seems that Clipped Coverage is not guaranteed to be <= 1. Is this correct? How should we interpret it when it is larger than 1?\n- Fidelity and coverage are usually known to be a trade-off [2] for the same model. Clipped metrics in Figure 6 do not reflect this (, although the comparisons are among different models). Can you further verify this?\n- L122-123, the sentence \"In high-dimensional spaces, ... as a proxy\" seems off here. Can you clarify?\n\n[2] Li, Shuangqi, et al. \"Controlling the Fidelity and Diversity of Deep Generative Models via Pseudo Density.\" Transactions on Machine Learning Research."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8FNaVzLwEl", "forum": "cwOSdyuNh6", "replyto": "cwOSdyuNh6", "signatures": ["ICLR.cc/2026/Conference/Submission18190/Reviewer_6bfz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18190/Reviewer_6bfz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956279909, "cdate": 1761956279909, "tmdate": 1762927938594, "mdate": 1762927938594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two new metrics, Clipped Density and Clipped Coverage, to address failings in the evaluation of generative models. The authors argue that existing metrics for fidelity and coverage suffer from two main problems: 1) a lack of robustness to outliers in both the real and synthetic datasets, and 2) an inability to provide an interpretable, absolute score.\nThe proposed metrics address these issues through new clipping and calibration strategies:\n1. Clipping: Clipped Density (fidelity) prevents real-data outliers from inflating scores by clipping k-NN ball radii to the median distance. Both metrics also clip the contribution of any individual sample to a score of 1, preventing over-represented modes from masking the presence of \"bad\" samples. \n2. Calibration: The metrics are calibrated to provide an absolute, more interpretable score. A score of $x$ is designed to be equivalent to the performance of a dataset with a proportion $x$ of \"good\" samples and $1-x$ of \"bad\" samples . For Clipped Coverage, this is achieved via a theoretical derivation (Lemma 1) and a resulting correction function ; for Clipped Density, this is achieved by normalizing by the real data's score on itself . \nThe authors demonstration on synthetic densities and through empirical validation on DINOv2-embedded image datasets that these two metrics are robust to outliers and provide interpretable absolute scores that behave linearly as sample quality degrades."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n-  The paper addresses an important and recognized problem: the lack of reliable, robust, and interpretable metrics for generative models.\n- The paper presents a well-illustrated analysis of the failure modes of a chosen class of metric: k-NN density-based metrics (like Precision, Density, Coverage) is clear. The figures effectively illustrate the specific problems of outlier sensitivity and non-linear response that the paper aims to solve.\n- The \"clipping\" mechanisms are simple and intuitive and directly target the demonstrated failure modes. Clipping radii addresses outlier-driven inflation, and clipping sample contributions addresses mode collapse or over-representation.\n- The authors have made some efforts to demonstrate the performances of their metrics on modern, high-dimensional datasets like ImageNet, LSUN, and FFHQ, which are the standard benchmarks for \"real-world\" generative models"}, "weaknesses": {"value": "1. Overstated Claims and Missing Key Baselines: The paper's premise that \"all existing... metrics are flawed\" and that \"no metric offers this property\" (absolute interpretability) is an overstatement. The paper's analysis is confined almost entirely to kNN density-based metrics, while ignoring a relevant body of work on sampling-based evaluation: arXiv:2402.04355 and arXiv:2302.03026 similarly use distance metrics to probe the underlying density but don’t rely on kNN density estimation; there is also no comparison to other standard metrics used in the literature, to help the audience how the proposed metrics differ on specific tasks (Fréchet Inception Distance - FID  arXiv:1706.08500, Feature Likelihood Divergence  - FLD arXiv:2302.04440) Failing to refer to and compare against this (FID is mentioned, but just in the intro, no comparisons are actually made on on problems) related work significantly weakens the paper's claim to novelty, and makes it hard to understand how its performances compare to existing evaluation options.\n2. The paper's main technical contribution—the calibration of Clipped Coverage—could be presented more clearly.\n\n   - Lack of Clarity: The paper never explicitly states that its \"absolute interpretability\" is a property of the metric's expected value, not its value from a single draw. This implies that bootstrapping (running the metric many times) is required to get an interpretable score, but this is never explicitly stated or analyzed (how stable is the score as a function of the number of samples and the dimensionality of the problem).\n\n   - Statistical Ambiguity: As a result of sampling variance, a single \"lucky\" draw from a perfect model (or even an imperfect model) could yield an uncalibrated score $s > f_{\\text{expected}}(0)$, which would result in a calibrated score greater than 1.0, breaking the \"proportion\" interpretation. This could happen even if the score is bootstrapped a small number of times (since the variance of the score is not studied theoretically, it’s hard to assess how many times the bootstrapping needs to be done for a specific application). The paper provides no analysis of the metric's variance or the number of bootstrap samples needed for a stable estimate.\n\n   - Lack of theoretical guarantees: The paper does not present theoretical guarantees that the proposed metric has important desirable properties in the theoretical limit of infinite number of samples or with infinite number of re-drawing of the samples and recalculating the score, like a proof of sufficiency or a proof of consistency.  \n\n3. Limited Scope of Evaluation (Failure Modes & Modalities):\n   - Limited evaluation of the sensitivity of the metrics: The \"sanity checks\" are limited to gross failures like complete mode dropping or injection of very easily detectable out of distribution samples. For scientific applications, a much more relevant test is sensitivity to subtle failures, such as missing rare modes, failing to capture intra-mode structure, or subtle corruption to the samples.\n    - Missing Modalities: All experiments are on images or simple Gaussian data. The metric is distance-based and should be applicable to any data modality. The paper would be far more convincing if it demonstrated this utility on other common data types, such as tabular data or time-series. \n\n4. All the high-dimensional experiments presented have a dependence on  a black box feature extractor. The robustness claims are undermined by this, as it’s not clear if it’s the metric that’s robust, or the embedding of the data through DINOv2. This creates blinds spots for many real world applications:  \n\n   - Foundation models like DINOv2 are trained for invariance to noise, blur, and other augmentations, but for some applications, for example in science, it’s important to test weather those exact nuisance parameters are correctly captures by the model. \n\n   -  Moreover, the claim of the paper is that the metric could be generally useful across domains, but DinoV2 is specifically trained on natural images and there is no guarantees that it extracts features that are useful in other domains, e.g. scientific data like fluids dynamics simulation, climate models, astrophysical simulations, that have different underlying statistics and correlate scales differently. The experiments have shown that in high-dimensions, the metrics capture model’s realism within the DINOv2 feature space, but that might not correlate with true scientific fidelity.\n\n5. There is a significant limitation of the metrics that is not discussed: The paper's design goal is \"robustness to outliers,\" which it achieves by making the metrics insensitive to the magnitude of a sample's \"badness\" . A synthetic sample 1000 units away from the data manifold is scored as a 0, which is the same score a sample only 10 units away might receive. While this makes the metric sensitive to the proportion of bad samples, it ignores the severity. For the \"high-stakes applications\" mentioned in the introduction (e.g., healthcare), this is a critical flaw. A model that produces one catastrophic outlier is far more dangerous than a model that produces many mediocre samples. The paper presents this insensitivity as a feature without discussing the significant trade-off in its limitations."}, "questions": {"value": "1. How does the variance of the coverage score scale as a function of the number of samples, number of bootstrap performed, and dimensionality of the problem, for various applications?    \n\n2. Your k-NN metrics are susceptible to the curse of dimensionality, which is why you use DINOv2. But what failure modes does this introduce? For example, how would your DINOv2-embedded metrics perform on a \"bad\" model that correctly learns the locations and densities of modes but gets the correlations within those modes wrong?\n\n3. How does the metrics compare to modern metrics like TARP, PQMass, FLD and FID on various realistic tasks?\n\n3. Your \"absolute interpretability\" relies on the metric's expected value, not a single score. How many bootstrap samples are needed to get a stable estimate of this mean, and how does this required number of samples scale with data dimension?\n\n4. How sensitive are your metrics to the choice of metric distance between samples?\n\n5. You motivate your metrics as being robust to outliers, which in practice means they are insensitive to the magnitude of a sample's \"badness.\" How bad do mediocre samples have to be for an ‘ok but not great’ model to be scored worse than one that sometimes produces catastrophic samples? How does this depend on the choice of embedding model?\n\n6. Can you discuss more in detail failure modes of your tests, for example \"bad\" model that correctly learns the locations and densities of modes but gets the correlations within those modes wrong (e.g., swapping a /-shaped mode for a \\\\-shaped mode)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "piTouVKoQm", "forum": "cwOSdyuNh6", "replyto": "cwOSdyuNh6", "signatures": ["ICLR.cc/2026/Conference/Submission18190/Reviewer_4Rqw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18190/Reviewer_4Rqw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996642352, "cdate": 1761996642352, "tmdate": 1762927938178, "mdate": 1762927938178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}