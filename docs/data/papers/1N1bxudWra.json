{"id": "1N1bxudWra", "number": 20241, "cdate": 1758304090336, "mdate": 1758806394309, "content": {"title": "GraphGhosts: Tracing Reasoning Structures Behind Large Language Models", "abstract": "Large Language Models (LLMs) exhibit remarkable reasoning and generalization abilities, yet the mechanisms underlying these abilities remain poorly understood. Recent studies have introduced circuit-tracing methods to explain individual token predictions. However, the question of why LLMs can perform complex reasoning tasks remains largely unexplored. Motivated by the inherent logical structures in reasoning tasks, we hypothesize that LLMs rely on latent graph-like structures, which we term GraphGhosts, to guide their reasoning processes. GraphGhosts manifest at three distinct levels: IntraSample Graphs, which capture token-to-token dependencies within individual reasoning instances; InterToken Graphs, which reveal dataset-level wiring patterns across tokens; and Semantic Graphs, which encode cross-domain conceptual understanding. To validate their significance, we conduct perturbation experiments on these graphs, showing that even small structural modifications can drastically alter a modelâ€™s reasoning ability.", "tldr": "", "keywords": ["LLM reasoning; Interpretation; Graph"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "", "supplementary_material": ""}, "replies": [], "withdrawn": true}