{"id": "cfOnDhvp3y", "number": 12776, "cdate": 1758210213455, "mdate": 1759897487514, "content": {"title": "Small Talk, Big Impact: The Energy Cost of Thanking AI", "abstract": "Being nice doesn't cost you anything - or does it? In this paper, we quantify the energy cost of seemingly innocuous messages such as ``thank you'' when interacting with large language models to convey politeness. Using real-world conversation traces and fine-grained energy measurements, we quantify how input length, output length and model size affect energy use. While politeness is our motivating example, it also serves as a controlled and reproducible proxy for measuring the energy footprint of a typical LLM interaction. Our findings provide actionable insights for building more sustainable and efficient LLM applications, especially in increasingly widespread real-world contexts like chat. As user adoption grows and billions of prompts are processed daily, understanding and mitigating this cost becomes crucial - not just for efficiency, but for sustainable AI deployment.", "tldr": "Saying “thank you” to an LLM has a measurable energy cost. We quantify it across models and show how prompt length, output verbosity, and model size impact inference energy", "keywords": ["LLM inference efficiency", "Sustainable AI", "Latency modeling", "GPU energy profiling", "Model scaling"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aa730244fe070717f6fddd43fbe2c9d399cd6688.pdf", "supplementary_material": "/attachment/467b25fff4d3bc691bebfcc136d379f813a00812.zip"}, "replies": [{"content": {"summary": {"value": "This paper tries to measure the energy cost of thanking LLMs. They do this by monitoring GPU, CPU, and RAM usage after augmenting a dataset with \"thank you's\" added."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper is, on the whole, well put together. The authors do a good job of measuring energy usage across a realistic dataset and use the tools appropriately to break down these measurements by component. We also see solid breakdowns by model size, and family. These measurements are backed up by idealised equations that act as a nice theoretical guide to demonstrate that these measurements make sense."}, "weaknesses": {"value": "While a well executed study, the contributions of this paper are, I believe, overall, too low for publication at ICLR. This is both in terms of the scope of the research question, but also in terms of some limitations of the paper itself.\n\nFirst, the levels of energy at play here are very small, especially considering the overall cost for the conversation to have reached the point where the user \"thanks\" the model. While it will possibly be one of the more expensive queries (longest context if thanked at the end of the conversation), this is just the final step in an approximately ~O(n^2) calculation. I don't think the effect of the \"thank you\" is too significant compared to the costs of using the model more specifically for the task. \nAdditionally, this paper is limited to two small families of models. Open models are obviously great because this type of analysis can be performed, but I think any argument about the true cost of this would be with the more popular closed sourced LLMs-as-a-service. Here it seems like model providers could take steps to minimise the cost of this interaction (e.g., if the user says \"thank you\" deploy a canned response rather than push everything through the LLM). \n\nAgain the study is limited in that it only examines a single piece of hardware, while I expect results would be similar across other GPUs/CPUs, it would be interesting and more insightful to see whether the energy cost changes when using e.g., a GPU more suited to a particular model (H100s are maybe overkill for Qwen 0.5B, and perhaps a \"thank you\" is cheaper on a smaller card). \n\nFinally, I find Figures 2 and 3 a bit confusing. The main text only ever references two phases---prefill and decode. But both of these figures include a \"generate\" plot as well that is never explained. The figure descriptions do not match the main text either --- the prefill is described as the largest energy user, but is the smallest in the diagrams."}, "questions": {"value": "1. Could we have a comparison between the cost to generate a response to the thank you, and the cost to generate the conversation up to that point?\n2. What is going on with Figures 2 and 3 WRT the values of prefill vs the main text. What is the generate plot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YmhgIKOMeu", "forum": "cfOnDhvp3y", "replyto": "cfOnDhvp3y", "signatures": ["ICLR.cc/2026/Conference/Submission12776/Reviewer_PZ1a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12776/Reviewer_PZ1a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761065522879, "cdate": 1761065522879, "tmdate": 1762923587786, "mdate": 1762923587786, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates the energy cost associated with polite interactions with large language models (LLMs). Specifically, the authors measure the additional energy required for models to respond to messages that end with a “thank you.” Using several LLMs (LLaMA 3.1–8B Instruct, Qwen models from 0.5B to 14B parameters, and Mistral-7B Instruct-v0.3), they record energy usage on an NVIDIA H100 GPU and analyze energy consumption across different phases of model execution."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written and generally easy to follow.\n\n- Provides detailed measurements of energy consumption across model sizes and computation phases."}, "weaknesses": {"value": "- The paper claims to provide “actionable insights for building more sustainable and efficient LLM applications,” but the discussion does not articulate what these actionable insights are beyond the general assumption of reducing computation.\n\n- The pre-fill phase should not be included in the measured energy cost of saying “thank you,” since in realistic chat situations, pre-filling is already cached.\n\n- Figures 2 and 3 include a “generation” category that is not defined or discussed in the text.\n\n- In the Theoretical Latency of the Model section, formulas are introduced without proper justification or explanation. Units of measurement for variables (e.g., ${F_0}$, ${D_0}$) are missing, and parameters such as $\\alpha, \\beta, \\gamma$ appear without stating whether they were empirically fitted or theoretically derived. Numbering the formulas would also improve readability.\n\n- The paper’s core contribution is limited. Prior work has already explored LLM energy consumption, and focusing narrowly on the “thank you” case does not appear to provide new scientific insight. The observed trends (larger prompts or models consume more energy) are expected and somewhat trivial."}, "questions": {"value": "Addressing the main concerns above, and adding additional context, such as explanations for warm-up runs and low-level kernel effects (e.g., block alignment, tiling), would make the work more informative and suitable for a general-interest venue."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3MdZYlLZ6W", "forum": "cfOnDhvp3y", "replyto": "cfOnDhvp3y", "signatures": ["ICLR.cc/2026/Conference/Submission12776/Reviewer_Qd7a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12776/Reviewer_Qd7a"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848354702, "cdate": 1761848354702, "tmdate": 1762923587090, "mdate": 1762923587090, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors investigate the energy cost of polite messages (e.g., “thank you”) in LLM-based conversations. They argue that such seemingly harmless micro-interactions, when scaled up to billions of daily interactions, can accumulate substantial computational and energy overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel, reproducible problem formulation: Treating “thank you” as a controlled micro-interaction unit is an elegant proxy for LLM inference energy.\n2. Clear phase separation : Prefill vs. decode decomposition (Figures 2–3) maps energy use to architectural structure.\n3. Quantitative analytical model: Fitted latency/energy formulas (Section 5) with numeric coefficients allow predictive estimation.\n4. I think that this a very interesting topic."}, "weaknesses": {"value": "1. They use pyRAPL (Intel RAPL counters) on AMD EPYC 7R13, which lacks compatible energy registers. No adaptation or external calibration is reported. Thus CPU energy (and total 0.245 Wh) lacks credibility. Authors must explain or replace with physical power-meter data.\n2. All results use FP32 precision, though real deployments use FP16/BF16/FP8/INT8. FP32 exaggerates power and latency. Reported numbers overstate real-world costs. Authors should test mixed-precision or clarify limitation.\n3. “Prefill-only” runs generate one token, yet the decode-phase energy is computed as (full – prefill-only). This subtracts a 1-token decode cost, underestimating true decode energy. Need clarification whether 0-token decode was feasible.\n4. No mention of temperature, top-p, top-k, repetition penalty, seed, or stopping criteria. These strongly influence output length and thus energy. The analysis of “verbosity” is therefore uncontrolled."}, "questions": {"value": "1. Figures 2–3 : Nicely visualize phase-wise energy, but sampling/normalization details missing.\n2. Authors claim to release code and data but provide no anonymous link, and it's essential for reproducibility claims.\n3. Why FP32? Have you tried FP16/BF16/FP8? How do coefficients A,B,C,D,G change?\n4. Provide prompt / output length distributions (mean, median, percentiles)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wha1ohXNY6", "forum": "cfOnDhvp3y", "replyto": "cfOnDhvp3y", "signatures": ["ICLR.cc/2026/Conference/Submission12776/Reviewer_5y1D"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12776/Reviewer_5y1D"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861126491, "cdate": 1761861126491, "tmdate": 1762923586596, "mdate": 1762923586596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the energy cost of using large language models (LLMs) by decomposing consumption into two phases—infilling (processing/input-side tokens) and decoding (generation/output-side tokens). The authors empirically fit a quadratic (token-length–based) model to estimate energy usage as a function of input and output tokens, and argue that token-level choices can materially affect overall energy footprints. However, despite the title suggesting a focus on the “cost of thanking AI” (i.e., adding polite but semantically light tokens like “thank you”), most of the paper actually analyzes generic token costs rather than this specific application."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**-- Systematic approach in measuring the energy efficiency of LLMs.** The authors separately measure the energy consumption across devices (CPU, GPU, RAM), LLM phases (infilling vs decoding). Such a study is conducted across different model families and sizes, confirming a linear energy cost growth for output < 10k tokens and quadratic growth for output > 10k tokens.\n\n**-- Clear decomposition of energy sources.** Separating infilling vs. decoding energy provides a useful mental model for practitioners who only see aggregate GPU/TPU power draw.\n\n**-- Empirical, token-level perspective.** A closed-form latency model that predicts energy consumption as a function of token counts (and fitting a curve) is actionable, providing LLM researchers and engineers a convenient abstract tool in estimating the energy cost when scaling the input/output tokens.\n\n**-- Important topic that might become a metric for future LLMs.** Energy/sustainability of LLMs is an important area; a simple predictive model of energy vs. tokens is a good building block for future work and a useful metric for comparing the energy efficiency of different models"}, "weaknesses": {"value": "**-- Title–content mismatch.** The title frames the paper as a study of “thanking AI,” but the body mostly describes generic token-cost modeling. The interesting hook (are polite/add-on tokens worth it?) is not really answered.\n\n**-- No evaluation of performance vs. cost.** The core question should be: does removing “useless” or low-utility tokens (e.g., “thank you”) save the energy bill at any cost of output quality/user satisfaction? Based on how this paper is written, it seems the authors believe these tokens can be spared with little to no effect. But a small-scale experiment+evaluation is needed to complete the story.\n\n\n**-- Limited generality discussion.** Token–energy curves can differ by model size, hardware, batching, quantization, and serving stack. The paper doesn’t fully discuss when their fitted quadratic will fail. I understand this is challenging, but this paper could be significantly strengthened if the fitted latency curve can take into account the model sizes and other factors into account so that it becomes more generalizable.\n\n**-- No realism about production traffic.** In practice, many requests are short, many are batched, and servers run mixed workloads. The paper seems closer to a controlled lab benchmark than to production telemetry."}, "questions": {"value": "-- Possible extension: using a small LLM for prefiltering. A very natural extension is to run a lightweight model (or rule-based filter) to drop redundant/polite tokens before sending to a large model. I’m curious if the authors have tried any approaches like this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W6ZgI8OLkk", "forum": "cfOnDhvp3y", "replyto": "cfOnDhvp3y", "signatures": ["ICLR.cc/2026/Conference/Submission12776/Reviewer_6bdr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12776/Reviewer_6bdr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12776/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761872561961, "cdate": 1761872561961, "tmdate": 1762923586330, "mdate": 1762923586330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}