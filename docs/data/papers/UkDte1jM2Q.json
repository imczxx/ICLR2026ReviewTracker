{"id": "UkDte1jM2Q", "number": 7737, "cdate": 1758034265903, "mdate": 1759897836251, "content": {"title": "Non-Asymptotic Analysis of Efficiency in Conformalized Regression", "abstract": "Conformal prediction provides prediction sets with coverage guarantees. The informativeness of conformal prediction depends on its efficiency, typically quantified by the expected size of the prediction set. Prior work on the efficiency of conformalized regression commonly treats the miscoverage level $\\alpha$ as a fixed constant. In this work, we establish non-asymptotic bounds on the deviation of the prediction set length from the oracle interval length for conformalized quantile and median regression trained via SGD, under mild assumptions on the data distribution. Our bounds of order $\\mathcal{O}(1/\\sqrt{n} + 1/(\\alpha^2 n) + 1/\\sqrt{m} + \\exp(-\\alpha^2 m))$ capture the joint dependence of efficiency on the proper training set size $n$, the calibration set size $m$, and the miscoverage level $\\alpha$. The results identify phase transitions in convergence rates across different regimes of $\\alpha$, offering guidance for allocating data to control excess prediction set length. Empirical results are consistent with our theoretical findings.", "tldr": "", "keywords": ["conformal prediction", "efficiency", "conformalized regression", "quantile regression", "uncertainty quantification"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/072a26a2d63f3bbd4c747a39bc5df85cbde3700a.pdf", "supplementary_material": "/attachment/cf6d9f010a6927f8bcb3e08e9a1bdb6a034d176e.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a rigorous non-asymptotic analysis of the efficiency of conformalized regression, specifically for Conformalized Quantile Regression (CQR) and Conformalized Median Regression (CMR) trained via Stochastic Gradient Descent (SGD). The core objective is to quantify the efficiency, measured by the expected deviation between the length of the constructed prediction interval and the ideal oracle interval length.\n\n**Contributions**\nThe main contributions focus on a novel theoretical upper bound for this length deviation. Unlike prior work that often treats the miscoverage level $\\alpha$ as a fixed constant, this paper derives the first bound that explicitly captures the joint dependence on the training set size $n$, the calibration set size $m$, and $\\alpha$. The derived bound is of the order $O(n^{-1/2} + (\\alpha^2n)^{-1} + m^{-1/2} + \\exp(-\\alpha^2m))$. \nThis result yields several key insights. \n\n1. It introduces the term $(\\alpha^2n)^{-1}$, revealing that efficiency degrades significantly when requiring very high confidence levels (i.e., very small $\\alpha$). \n\n2. It uncovers phase transitions in the convergence rates depending on the regime of $\\alpha$; for instance, the rate can shift from $O(n^{-1/2})$ to a much slower $O((\\alpha^2n)^{-1})$ as $\\alpha$ becomes smaller than a threshold related to $n$. \n3. These theoretical findings provide concrete guidance for optimally allocating data between training and calibration to achieve a desired level of efficiency. The analysis is supported by empirical results that closely align with the theoretical predictions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*   **Originality:** The paper shows the first non-asymptotic efficiency analysis for conformalized regression that jointly models the impact of training size ($n$), calibration size ($m$), and the miscoverage level ($\\alpha$). The identification of the crucial $(\\alpha^2n)^{-1}$ term and the resulting \"phase transitions\" in convergence rates is an original theoretical contribution, moving beyond prior results that typically treated $\\alpha$ as a fixed constant.\n\n*   **Quality:** The technical quality of the work is very high. The theoretical analysis is rigorous, providing a complete, end-to-end derivation under clearly stated and relatively mild assumptions on the data distribution. The quality of the empirical validation is exceptional; experiments are designed to isolate and verify specific components and scaling behaviors predicted by the theory, which provides strong evidence for the claims.\n\n*   **Clarity:** The paper is written with outstanding clarity. The problem setup, main theoretical results, and their implications are presented in a logical and accessible manner. The section dedicated to \"phase transitions\" is particularly effective, translating a complex mathematical bound into intuitive regimes and actionable insights. The figures are well-designed and clearly illustrate the alignment between theory and empirical results.\n\n*   **Significance:** The work carries significant practical and theoretical implications. For practitioners, it offers the first concrete, theory-backed guidance on how to optimally allocate data between training and calibration based on the desired confidence level. For the research community, it fundamentally deepens the understanding of efficiency in conformal prediction, revealing a critical trade-off that was previously not formally understood. The framework is also potentially extensible to other optimizers and models, enhancing its long-term impact."}, "weaknesses": {"value": "*   **Linear Model and Well-Specification Assumption:** The theoretical analysis is developed under the strong assumptions of a linear model class and well-specification. This limits the direct applicability of the derived rates to modern, often overparameterized, and non-linear models, such as neural networks. The paper would be strengthened by a discussion of the challenges in extending the analysis to these more complex settings, perhaps by outlining how model approximation errors would enter the final efficiency bounds.\n\n*   **Focus on Quantile-Based Methods Over Simpler Alternatives:** The analysis centers on CQR/CMR, which requires training quantile regressors via the pinball loss. A widely used and often simpler alternative is to train a single point predictor (e.g., for the conditional mean) and calibrate using the absolute residuals $|Y - \\hat{\\mu}(X)|$. The paper does not discuss how its theoretical findings, particularly the critical $(\\alpha^2n)^{-1}$ term, would apply to this common residual-based method. A comparison would be highly valuable, as the efficiency of the residual method often relies on different assumptions (e.g., homoscedasticity).\n\n*   **Exclusion of Classification Tasks:** The paper's scope is strictly limited to regression, with efficiency measured by interval length. An analogous and equally important problem exists in classification, where efficiency is measured by the size of the prediction set. The work misses an opportunity to discuss how its core insights—especially the critical role of $\\alpha$ in determining the required training data, might be applied to the classification setting, where the relationship between model confidence scores and the final set size is a key factor.\n\n*   **Empirical Validation Primarily on Synthetic Data:** The main experiments that provide the most direct validation of the theoretical scaling laws are conducted exclusively on synthetic data. While Appendix includes real-world experiments, they play a minimal role in the main paper and cannot be used to verify the precise rates since the oracle is unknown. Adding an experiment on synthetic data that mildly violates the assumptions (e.g., with slight non-linearity) would provide valuable insights into the robustness of the theory."}, "questions": {"value": "1.  **On the Impact of Model Misspecification:** Your theoretical analysis relies on the assumption of a well-specified linear model. This provides a very clean and insightful result. However, in practice, all models are misspecified. Could you elaborate on how your efficiency bounds might change in the presence of model misspecification? Specifically, if the true conditional quantile function is non-linear, an unavoidable approximation error term arises. How would this error propagate through your analysis? Would it be a simple additive term, or would it interact with the statistical rates you derived in a more complex manner?\n\n2.  **Generalizability to Residual-Based Conformal Methods:** The paper provides a deep analysis of Conformalized Quantile Regression (CQR). A very common and often simpler alternative in practice is to train a single point predictor (e.g., for the conditional mean) and then calibrate using the absolute residuals, i.e., scores are $S_i = |Y_i - \\hat{\\mu}(X_i)|$. The efficiency of this method is known to be sensitive to heteroscedasticity. Do you hypothesize that your core finding—the critical dependence on $(\\alpha^2n)^{-1}$—would also emerge in an efficiency analysis of this simpler residual-based method? \n\n3.  **Implications for Classification:** While your work is focused on regression, the fundamental insight about the trade-off between the required confidence level ($\\alpha$) and the training data size ($n$) seems universally important. Could you speculate on how this principle might manifest in the context of conformalized classification, where efficiency is measured by set size? For example, would the analysis involve the steepness of the model's softmax output distribution, and would a smaller $\\alpha$ similarly demand much more training data to reliably distinguish between classes with very close scores?\n\n4.  **Tightness of the Bound and Practical Data Allocation:** Your guidance on data allocation is derived by balancing the dominant terms in your theoretical upper bound. This is a powerful and practical contribution. However, since it is based on an upper bound, the tightness of the various terms matters. Could there be situations where the (often unknown) constant factors hidden in the Big-O notation are substantially different for the training and calibration error terms? If so, the truly optimal split might deviate from the one suggested by balancing the rates alone. Could you comment on the potential gap between the \"optimal split for the bound\" and the \"true optimal split\" in practice?\n\n5. **Practical Relevance of the $\\alpha = O(n^{-1/4})$ Regime:** Your analysis reveals a critical \"phase transition\" around $\\alpha = \\Omega(n^{-1/4})$. This is theoretically fascinating. However, in many practical applications, $\\alpha$ is a small, fixed constant (e.g., 0.1, 0.05) and does not scale with the sample size $n$. Could you comment on the practical settings where such a scaling of $\\alpha$ with $n$ might be relevant? For example, are there high-stakes domains (like autonomous systems or medical diagnostics) where practitioners might demand increasingly higher confidence levels as more data becomes available, thus creating a scenario where these extreme $\\alpha$ regimes and their efficiency implications are of practical concern? As I know, there is some literature about extreme error rate cases in conformal prediction. Maybe you should discuss them."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vWqEiZA6lf", "forum": "UkDte1jM2Q", "replyto": "UkDte1jM2Q", "signatures": ["ICLR.cc/2026/Conference/Submission7737/Reviewer_3qyg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7737/Reviewer_3qyg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760836602537, "cdate": 1760836602537, "tmdate": 1762919789597, "mdate": 1762919789597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the length of prediction intervals obtained from quantile regression models trained via stochastic gradient descent (SGD). The main contribution is a bound on the deviation of the empirical interval length from its true (population) counterpart. The analysis assumes a linear conditional model for $Y|X$, together with regularity and compact support of the underlying densities. While these assumptions may appear restrictive, they are reasonable in the context of deriving precise theoretical guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of this paper lies in the completeness of its analysis. The authors derive bounds on the expected deviation of the prediction interval length that explicitly account for all key parameters — namely, the size of the training data, the size of the calibration data, and the confidence level.\n\nThe paper is clearly written and well structured, leaving a positive overall impression both in the main text and in the detailed appendices and proofs."}, "weaknesses": {"value": "The paper would benefit from a dedicated discussion of its limitations at the end. In particular, while the analysis focuses on the expected length deviation, it would be useful to comment on situations where the oracle interval itself may not be optimal — for instance, in the presence of multimodal conditional distributions. Furthermore, it would be interesting to discuss whether and how the linearity assumption on the conditional distribution Y∣X could be relaxed or extended."}, "questions": {"value": "1. On the oracle interval and multimodality. You control the expected length deviation, but the optimality of the oracle interval is only briefly discussed in Remark 3.5. Under multimodality, the interval defined by quantiles does not coincide with the highest-density regions. A discussion of this limitation would be valuable.\n\n2. On notation and readability. The paper uses numerous notations, some of which could potentially be simplified. In $t_{\\gamma}(x,\\theta) = x^{\\top}\\theta$, is the subscript $\\gamma$ really necessary? The notation $\\overline{\\theta}$ and $\\underline{\\theta}$ already indicates whether the upper or lower quantile is considered. In addition, it might improve clarity to make explicit in Assumption 3.1 that the model is linear by writing, for instance, $X^{\\top}\\theta(\\gamma)$.\n\n3. On definitions and practical details.  Line 90: Is a set-valued definition of quantiles necessary? The standard generalized inverse given in Eq.(2) is well known and likely sufficient for your analysis.\nLine 146: Please specify the learning rate used in the proofs of the main theorems. The expression seems to depend on parameters that are probably unknown in practice. What step size do you use empirically?\n\n4. On related literature.The discussion around Theorem 3.1 could be expanded to better situate the work within the existing literature. Several studies investigate quantile regression trained by SGD under linear assumptions (see Shen et al, 2025 and references therein). It would be useful to compare your setting and assumptions (e.g., Assumptions~3.2--3.3) with these works, especially regarding the role of strong convexity, which Shen et al (2025) claims is typically absent.\n\n5. On missing or unclear notation. In Theorem 3.1, the constant $d$ appears for the first time without prior definition. As later sections suggest, $d$ denotes the dimension of the covariates $X$; this should be stated explicitly before the theorem. Similarly, the notation for the $\\Sigma$-indexed norm should be defined when it first appears.\n\n6. On quantile crossing and model assumptions.  Remark 3.1 and Proposition B.4 are somewhat confusing. The SGD procedure learns two parameters, $\\overline{\\theta}$ and $\\underline{\\theta}$, corresponding to the upper and lower quantiles. Proposition~B.4 shows that, if $n$ is large enough, quantile crossing cannot occur, i.e., the lines $x \\mapsto \\overline{\\theta}^{\\top}x$ and $x \\mapsto \\underline{\\theta}^{\\top}x$ do not intersect.\nDoes this imply that the two ground-truth lines are parallel? Does this follow directly from Assumption 3.1, which (as I understand it) corresponds to the linear model $Y = \\theta^{\\top} X+ $noise?\n\n7. On classical results.  Lemma B.4 is a standard result; see, for instance, Proposition A.25 in [2]. Please include appropriate references and mention when results are classical.\n\n8. On writing and presentation. Throughout the paper (particularly in the appendices), punctuation should be added at the end of equations (commas or periods) so that each displayed equation is integrated into a grammatically correct sentence.\n\nReferences\n\n\n [1] Online Quantile Regression, Shen et al. (2025), arXiv:2402.04602.\n\n [2] One-Dimensional Empirical Measures, Order Statistics, and Kantorovich Transport Distances, Bobkov & Ledoux (2016)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "adAerFgzrF", "forum": "UkDte1jM2Q", "replyto": "UkDte1jM2Q", "signatures": ["ICLR.cc/2026/Conference/Submission7737/Reviewer_oFDM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7737/Reviewer_oFDM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663156244, "cdate": 1761663156244, "tmdate": 1762919789233, "mdate": 1762919789233, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper gives finite-sample efficiency bounds for split conformal regression, both CMR and CQR under stochastic gradient descent training. They derive an upper bound on expected deviation of prediction set length from an oracle interval for CQR-SGD. Additionally they derive a non-asymptotic upper bound for homoscedastic tasks within CMR-SGD by leveraging the fact that intervals are symmetric and of constant lengths across inputs. The upper bounds derived are functions of training and calibration set size, and miscoverage level, which allows them to identify phase transition with respect to miscoverage levels. The authors show how to use this to “guide” train and calibration data splitting both theoretically and empirically for a limited regime of model and data types."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Main ideas and contributions are presented well and the proofs/the story the authors are telling with the results are clear. The paper combines strong theoretical backing, but also provides some practical implications of this analysis relating to data splits that practitioners can use. To the best of my knowledge, theoretical results differ enough from previous works mostly in the subtleties involving relaxation of assumptions on the score distribution and the fully non-asymptotic nature of their derivations. While other works have presented in depth analysis on expected size of conformal prediction sets and “optimal” data splitting, I think the theoretical backing makes this work novel enough."}, "weaknesses": {"value": "Empirical results are lacking. The paper is absent of thorough and diverse experiments across broad model and data types. While I understand the upper bound derivations rely on standard error rates of *optimizer of choice (i.e. SGD) I think more can be done empirically to show a) robustness of method and b) the extent to which one can relax some assumptions on the SGD error rates or characteristics of the data distribution and still have upper bounds empirically hold. It’s unclear to me the scope of real world data in addition to the model type that their theoretical findings would truly apply to. Further empirical probing of a broader variety of both linear models on more complex data types would make this analysis more convincing. \n\nThe authors don’t provide any empirical evidence on the optimizer-agnostic claim. While it makes sense in theory, some small experiments showing this would be a nice addition."}, "questions": {"value": "Your experiments primarily use a small subclass of linear models (i.e. ridge regression) on MEPS data, could you provide empirical evidence that the same alpha-scaling behavior persists for other convex models that satisfy the outlined assumptions?\n\nMEPS are primarily tabular with many bounded/indicator features (likely roughly aligning with your assumptions after some light preprocessing). Could you discuss sensitivity to datasets with genuinely heavy-tailed or unbounded covariates, where boundedness barely holds approximately (at best)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BQNWTtPU9s", "forum": "UkDte1jM2Q", "replyto": "UkDte1jM2Q", "signatures": ["ICLR.cc/2026/Conference/Submission7737/Reviewer_fXxb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7737/Reviewer_fXxb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968153747, "cdate": 1761968153747, "tmdate": 1762919788633, "mdate": 1762919788633, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the expected length of prediction intervals produced by split conformal methods for regression, focusing on conformalized quantile regression (CQR) and conformalized median regression (CMR). Authors examine the case of linear models  trained with SGD and derive a bound on expected deviation between the learned prediction interval length and the oracle length given by the true conditional quantile. Specifically, authors provide non‑asymptotic upper bounds depending on training and calibration set sizes as well as miscoverage level $\\alpha$. The analysis assumes bounded covariates and outcome, as well as conditional density bounded above and below."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: to my knowledge, the first explicit non-asymptotic bound of this type (depending on train and calibration test sizes and miscoverage).\nQuality: explicit versions of the bounds (equations 41 and 42) make the dependence on problem constants transparent. Experiments showcase key theoretical findings.\nClarity: Assumptions are explicit and clear, and key constants are defined.\nSignificance: finite‑sample results relating dataset splitting and miscoverage level are immediately actionable for conformal prediction practitioners, complementing earlier work on CQR and asymptotic efficiency."}, "weaknesses": {"value": "- Abstract mentioned “offering guidance for allocating data to control excess prediction set length\". I assume the Data Allocation and preceding sections cover that. In my opinion, the practitioners would benefit from a more clear and explicit recommendations on choosing sizes/miscoverage level, e.g., starting from a fixed training (or calibration) set size $n$ ($m$) or a required level of $\\alpha$. In fact, authors themselves follow similar setups for their own experiments in the Appendix.\n- Large constants in equations 41 and 42. Computing them for the presented synthetic data model may improve the understanding and presentation of the bounds.\n- Experiments are mainly linear with one real dataset family (MEPS). Including at least one nonlinear (e.g., NN) backbone (even if theory doesn’t directly cover it) would illustrate the practical applicability of the bounds beyond linear models."}, "questions": {"value": "- Figure 6 in the appendix shows a very strange behavior: performance oscillates with increasing training set size. Why is that happening?\n\n- Figure 7 in the appendix is much more stable, but also has problems: length deviation increases with increasing training set size for many $\\alpha$ and calibration set sizes. Can you explain this behavior?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E67QglBGgJ", "forum": "UkDte1jM2Q", "replyto": "UkDte1jM2Q", "signatures": ["ICLR.cc/2026/Conference/Submission7737/Reviewer_N2xt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7737/Reviewer_N2xt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7737/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219203648, "cdate": 1762219203648, "tmdate": 1762919788150, "mdate": 1762919788150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}