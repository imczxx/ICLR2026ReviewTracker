{"id": "nGzYkW4FSB", "number": 22127, "cdate": 1758326503256, "mdate": 1759896884933, "content": {"title": "Synchronizing Probabilities in Model-Driven Lossless Compression", "abstract": "It is well-known in the field of lossless data compression that probabilistic next-symbol prediction can be used to compress sequences of symbols. Deep neural networks are able to capture rich dependencies in data, offering a powerful means of estimating these probabilities and hence an avenue towards more effective compression algorithms. However, both compressor and decompressor must have exactly matching predictions; even small non-deterministic differences (which often happen with learned models due to hardware, software, or computation order) can lead to cascading decoding failures. In this paper, we formalize the problem of prediction mismatch in model-driven compression, and introduce Probability Matching Interval Coding (PMATIC), a model-agnostic algorithm that tolerates bounded prediction mismatch with low overhead. PMATIC works with the predicted probabilities, making it compatible as a drop-in replacement for the arithmetic encoder in model-driven compression tools. We show theoretical correctness and performance bounds for PMATIC, and validate these results on text data. These results confirm that, when paired an advanced prediction model, PMATIC is robust to prediction mismatch while achieving compression rates that out-perform standard modern compression tools.", "tldr": "", "keywords": ["data compression", "model-driven prediction", "non-determinism"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a15a324494121842c266f64c5e75b16f579f8f7f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles a practically critical but underexplored problem in model-driven lossless compression: the instability caused by prediction mismatch between encoder and decoder due to LLM non-determinism. The authors formalize this as the probability matching problem and propose PMATIC (Probability-Matched Interval Coding), a theoretically grounded and model-agnostic alternative to arithmetic coding that tolerates bounded prediction mismatch. Theoretical guarantees on decodability and compression efficiency are presented, and experiments on Wikipedia and Enwik8 datasets validate correctness and reasonable efficiency loss under synthetic noise."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The work highlights a key yet overlooked obstacle in deploying LLM-based compression systems: non-deterministic inference, bridging the gap between theory and practical system reliability.\n\n- The formalization of bounded prediction mismatch and the introduction of a matching interval coding mechanism are conceptually clean and mathematically sound. The paper also provides provable correctness and upper bounds on the additional code length, demonstrating an informed trade-off between robustness and compression efficiency.\n\n- PMATIC is model-agnostic and can be seamlessly integrated as a drop-in replacement for arithmetic coding, showing potential broad applicability beyond text compression."}, "weaknesses": {"value": "- Experiments are confined to synthetic perturbations on a single model (Llama-3.1) and small text corpora. This does not convincingly capture the stochastic, architecture- or library-dependent non-determinism that motivates the problem. I'd like to see more experiments. \n\n- There already exist benchmarks for model-driven or LLM-assisted compression [1,2,3]. These could provide a valuable performance baseline.\n\n- Runtime, helper-bit statistics, and computational overhead are not reported. It is unclear how PMATIC scales when applied to larger models or real-time compression.\n\n- Although the paper derives an analytical bound on the extra bit cost, it does not verify whether the empirical losses follow this bound.\n\n- Some related works are missing [4,5,6]\n\n- In all, I think this paper addresses an important and practical problem, but the experimental section is insufficient. If the authors can provide comprehensive and convincing experimental results during the rebuttal period, I will raising my score.\n\n[1] Valmeekam C S K, Narayanan K, Kalathil D, et al. Llmzip: Lossless text compression using large language models[J]. arXiv preprint arXiv:2306.04050, 2023.\n[2] Mao Y, Pirk H, Xue C J. Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction[J]. arXiv preprint arXiv:2505.06297, 2025.\n[3] Mittu F, Bu Y, Gupta A, et al. Finezip: Pushing the limits of large language models for practical lossless text compression[J]. arXiv preprint arXiv:2409.17141, 2024.\n[4] Mao Y, Li J, Cui Y, et al. Faster and stronger lossless compression with optimized autoregressive framework[C]//2023 60th ACM/IEEE Design Automation Conference (DAC). IEEE, 2023: 1-6.\n[5] Mao Y, Cui Y, Kuo T W, et al. Accelerating general-purpose lossless compression via simple and scalable parameterization[C]//Proceedings of the 30th ACM International Conference on Multimedia. 2022: 3205-3213.\n[6] Goyal M, Tatwawadi K, Chandak S, et al. DZip: Improved general-purpose loss less compression based on novel neural network modeling[C]//2021 data compression conference (DCC). IEEE, 2021: 153-162."}, "questions": {"value": "- How large is the helper-bit overhead in practice, and how does it vary with Î´?\n\n- What is the computational impact, such as encoding/decoding latency compared to standard arithmetic coding?\n\n- The paper mentions the potential extension to stochastically bounded mismatch. Could the authors elaborate on whether PMATIC can be adapted to probabilistic mismatch distributions observed in real inference pipelines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4Dstbox4pD", "forum": "nGzYkW4FSB", "replyto": "nGzYkW4FSB", "signatures": ["ICLR.cc/2026/Conference/Submission22127/Reviewer_YPjH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22127/Reviewer_YPjH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760772469477, "cdate": 1760772469477, "tmdate": 1762942077515, "mdate": 1762942077515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "We theoretically formulated the problem of \"non-determinism\" that arises in lossless compression using large-scale language models (LLMs), and proposed a new compression coding method, PMATIC (Probability-Matched Interval Coding), to overcome this problem.\nLLM outputs a probability distribution $P(x_t|x_{<t})$, but even for the same input, it may produce slightly different probabilities due to different GPUs, different libraries, or differences in parallel order. Conventional arithmetic coding breaks down if the probability distributions of the encoder and decoder do not match perfectly. However, if the probability mismatch is sufficiently small (bounded mismatch), accurate decoding is possible if both agree on an \"intermediate common distribution\", which is the Probability Matching problem.\nPMATIC is a method that extends existing arithmetic coding with probability quantization, ensuring consistent coding even if the encoder and decoder make slightly different probability predictions.\nFor PMATIC, this paper theoretically evaluates its correctness and compression loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Originality: This paper is the first to mathematically formalize the problem of \"probabilistic model-driven compression\" $\\times$ \"LLM nondeterminism\".\nQuality: Not only is there a mathematical discussion of performance analysis, but the usefulness of the proposed method is also verified through numerical experiments, making the paper of high quality.\nClarity: The problem to be addressed is clearly stated, the algorithm is given in detail, and the argument is clear enough.\nSignificance:  This paper is one of the first to formulate the problems that are encountered when actually applying LLM as lossless compression, and is expected to make an important contribution to related research."}, "weaknesses": {"value": "As mentioned in 6. Future work, if LLM is actually used for lossless compression, the size of the models required to implement the compressor and decoder will be a major problem. This issue has not been discussed in this paper."}, "questions": {"value": "There has been discussion about static performance, such as the achievable compression performance. However, how much better is it compared to existing methods in terms of the amount of computation required for compression and decoding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HufQwKsSSN", "forum": "nGzYkW4FSB", "replyto": "nGzYkW4FSB", "signatures": ["ICLR.cc/2026/Conference/Submission22127/Reviewer_iuaJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22127/Reviewer_iuaJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761202746290, "cdate": 1761202746290, "tmdate": 1762942077054, "mdate": 1762942077054, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an algorithm to quantize / bin the probability distributions to account for possible deviations of the predicted CDFs between the encoder and decoder. The motivation is due to the non-determinism of LLMs, which can result in small differences in floating point numbers even when run on the same hardware."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 4}, "strengths": {"value": "- The method is reasonably simple to apply to existing LLMs, without needing to retrain.\n- The authors provide some guarantees on the added bit length due to binning.\n- This is a very important, and realistic problem, that needs to be solved to have next-gen AI codecs, and I appreciate the authors pushing on real world problems."}, "weaknesses": {"value": "- The experiments are significantly lacking in breadth. If the method is general, the authors could provide further experiments with different data modalities.\n\n- The following is hard, but would significantly improve the paper: can the authors estimate what are typical deviations present in relevant scenarios where AI codecs could be applied? For example, take any open source model, and apply the encoder and decoder using 1) a different version of CUDA, 2) different models, and other variables that might vary in practice. This would significantly improve the contribution and ground the paper in real world applications.\n\n- Adding figures explaining the binning procedure would significantly improve the exposition of the algorithm."}, "questions": {"value": "- Do the authors have a good sense of how large the mismatches are in practice (see box above)?\n- Can the authors provide more experiments across data modalities, and possibly varying other variables that could impact the degree of mismatch (see box above)?\n- Can the authors add figures to explain the binning better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KnlGuDd5wT", "forum": "nGzYkW4FSB", "replyto": "nGzYkW4FSB", "signatures": ["ICLR.cc/2026/Conference/Submission22127/Reviewer_nkTr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22127/Reviewer_nkTr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931750945, "cdate": 1761931750945, "tmdate": 1762942076501, "mdate": 1762942076501, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to address the problem of practical usage of LLMs in data compression. A novel algorithm is presented for compression with uncertainty in probability predictions based on arithmetic coding. Furthermore mathematical analysis is presented showing compression bounds under a given uncertainty. Experiments denote successful demonstration with synthetically generated noise."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. This is a very relevant problem, I believe this would allow the community to actually make practical compressors with the proposed algorithm. \n2. Article is original and a novel algorithm is proposed. Paper is quite easy to read."}, "weaknesses": {"value": "1. The only weakness I would say is some analysis on what delta's would we expect by changing hardware or going from GPUs to CPUs. Also there is relevant research addressing uncertainty in LLM prediction which should be added to related work (https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/).\n2. Please add comparison with some more compressors, especially CMIX. Also include latency because that is where traditional compressors win by a huge margin."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TLF5EXkgGC", "forum": "nGzYkW4FSB", "replyto": "nGzYkW4FSB", "signatures": ["ICLR.cc/2026/Conference/Submission22127/Reviewer_ADbL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22127/Reviewer_ADbL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22127/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762582328258, "cdate": 1762582328258, "tmdate": 1762942076141, "mdate": 1762942076141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}