{"id": "vzUixzPIHS", "number": 5343, "cdate": 1757902956620, "mdate": 1759897980593, "content": {"title": "Generalization through Discrepancy: Leveraging Distributional Fitting Gaps for AI-Generated Image Detection", "abstract": "The generalization of detectors for AI-generated images remains a critical challenge, as methods trained on one generative family often fail when tested on unseen architectures. To tackle this generalization challenge, we dive into the inherent distribution approximation nature of generative modeling and posit that a universal forensic signal lies in the discrepancy between mathematically precise image rescaling traces and the imperfect approximations learned by generative models through training data. We introduce a novel contrastive pre-training framework that sensitizes a feature extractor to these subtle rescaling artifacts by leveraging their inherent periodic patterns and position shift properties, using only real images for training. Our method sets a new state-of-the-art on both GAN and diffusion-generated benchmarks, validating the efficacy of our method. We introduce a new and robust perspective on detection generalization through the lens of distributional fitting divergence. The code and models will be made publicly available.", "tldr": "", "keywords": ["AI-Generated Image Detection", "Distributional Discrepancy", "Pre-training for Detection", "Cross-Model Detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3bab43984b5370ffbdbfa4da8799b32814790d27.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper aims to improve the generalization ability in the deepfake detection task under the cross-generator scenario, e.g., detection inference with unseen generators from training. Specifically, the authors hypothesize that broadening the coverage of prior-based assumptions would help with the generalization performance and propose to rescale the distribution of the discrepancy to tackle this. Methodologically, the paper presents an analysis of the rescaling properties via bilinear interpolation and leverages a distributional contrastive pre-training mechanism before fine-tuning the classifier. Experiments are conducted on several benchmarks with different image generators."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has a rather clear structure and presents a generally good literature review in terms of existing deepfake detection methods.\n\n- The perspective to address the generalization ability via the distributional discrepancy is somewhat novel and reasonable."}, "weaknesses": {"value": "While the paper presents a relatively complete structure, several important details are still missing to fully evaluate the proposed method and validate its effectiveness.\n\n1. Motivation clarity: I understand that different generators model data distributions differently and may introduce distributional discrepancies during the generation process. However, the connection between the bilinear interpolation-based scaling behavior (Section 3) and its effectiveness for deepfake detection remains unclear. A more concrete justification or intuition for why this approach works would be helpful.\n\n2. Missing relevant baseline: The paper seems to overlook some recent relevant work—such as [a]—which similarly explores disentangling semantic features to improve cross-generator generalization. A discussion and comparison with such baselines would strengthen the positioning of the proposed method.\n\n3. Interpolation choices: The authors state that “we focus on bilinear interpolation to elucidate the underlying mechanisms, noting that other interpolation techniques exhibit analogous properties.” This claim is not self-evident and would benefit from empirical support. Including ablations on alternative scaling methods could help validate this assumption.\n\n4. Additional ablations needed: Some other important ablations appear to be missing and are necessary to support the key claims of the paper. Please refer to the Questions section for specific suggestions.\n\n--- \n[a] D3: Scaling Up Deepfake Detection by Learning from Discrepancy. In CVPR 2025."}, "questions": {"value": "1. The authors mention the computing resources used in the experiments, but the actual GPU hours are missing. Could the authors clarify the training time required for both the pre-training and fine-tuning stages?\n\n2. Does the patch size used during the pair construction stage impact the detection performance? If so, it would be helpful to provide an ablation or brief discussion on its effect.\n\n3. Additionally, could the authors further clarify the motivation behind the positive and negative sample mechanism illustrated in Figure 4? Specifically, is the rescaling stage applied only to real images, or does it also affect synthetic ones?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7DT8kLysFG", "forum": "vzUixzPIHS", "replyto": "vzUixzPIHS", "signatures": ["ICLR.cc/2026/Conference/Submission5343/Reviewer_38mx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5343/Reviewer_38mx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644993141, "cdate": 1761644993141, "tmdate": 1762918016141, "mdate": 1762918016141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel approach for detecting AI-generated images by leveraging the discrepancy between mathematically precise image rescaling traces and the imperfect approximations learned by generative models. The authors introduce a contrastive pre-training framework that sensitizes feature extractors to subtle rescaling artifacts using only real images, focusing on the periodic patterns and position shift properties of rescaling operations. Their method achieves state-of-the-art performance across both GAN and diffusion-generated image benchmarks, demonstrating strong generalization capabilities for cross-model detection tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly written with a complete structure and is easy to follow.\n2. I believe the proposed approach is innovative, particularly the method of constructing positive and negative sample pairs, which effectively demonstrates how the technique focuses on rescaling artifacts.\n3. Comparative experiments are extensive and comprehensive, and the results validate the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The paper assumes that generators learn and approximate real-world rescaling artifacts from training data. However, given the diverse rescaling artifacts of training datasets (they scale from different resolutions to the same resolution), generators may not consistently learn a specific rescaling pattern, which means their outputs might lack these artifacts(as supported by Figure 5). This raises my concerns that the method might simply detect whether images contain rescaling artifacts rather than determining if they're generated. (Note that both LSUN and ImageNet are preprocessed real datasets, so they have rescaling artifacts).\n2. Based on the above analysis, I think the experiment in Figure 6 lacks rigor. A more reasonable scenario would involve applying varying degrees of multiple rescaling operations to both real and fake images to eliminate potential differences caused by preprocessing.\n3. The paper lacks robustness testing against common post-processing operations like Gaussian blur and JPEG compression.\n\nMinor weakness: While generally clear, some sections contain unnecessary redundancy, such as the detailed explanation of bilinear interpolation in Section 3 and the problem definition in Section 4.1, where the authors could simplify the presentation."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zQnmJBSWH8", "forum": "vzUixzPIHS", "replyto": "vzUixzPIHS", "signatures": ["ICLR.cc/2026/Conference/Submission5343/Reviewer_TxtG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5343/Reviewer_TxtG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873835860, "cdate": 1761873835860, "tmdate": 1762918015838, "mdate": 1762918015838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper improves the generalization of deepfake detectors by exploiting a pre-processing distributional discrepancy between real and generated images. Focusing on the rescaling operation, the authors analyze bilinear interpolation and reveal two key properties—periodicity and local pixel dependency—that differ across real and synthetic data. They introduce an unsupervised contrastive learning framework to model these properties, achieving state-of-the-art results on the Self-Synthesis and GenImage benchmarks"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The method presented is an unsupervised method which harnesses the distributional properties of a simple yet overly common preprocessing step: rescaling. This approach leads to a good generalization when evaluated on benchmarks such as GenImage or Self-Synthesis.\nTo further prove the efficiency of their method, the authors conducted an ablation study demonstrating the efficiency (1) of their pre-training strategy (2) of their pre-trained backbone compared to CLIP and (3) of the few-shot fine-tuning scenario. Also, the authors present t-SNE plots of the extracted features (which further supports the second claim of the ablation analysis) and cosine similarity maps of both real and fake images which illustrates the periodicity property of the former as opposed to the latter."}, "weaknesses": {"value": "The authors claim high capabilities in terms of generalization. Although cross-generator performance is tested, a cross family of generators evaluation is missing. (e.g. an analysis on the UniversalFakeDetect Dataset).\nThe model is pre-trained on ImageNet and then fine-tuned and tested on GenImage benchmark. This is an issue since GenImage contains real samples from ImageNet.\nClaims related to robustness against post-scaling operations are not well supported by quantitative results, but only t-SNE plots. The authors should test their model in this setup.\nIn section 3, the authors claimed that other interpolation methods apart from bilinear interpolation exhibit similar properties. However, this claim is not supported either mathematically or empirically.\nIn Figure 6 the labels are too small and not clear."}, "questions": {"value": "In section 5.2 “Generated Data Exhibits Distinct Cosine Similarity Map.”: why certain generative models do have multiple bright parallel bands (e.g. GLIDE) while others don’t (e.g. ProGAN)? Also, can one observe the rescaling factors based on the distance between the bright lines? It would be interesting to observe this effect and, if proven empirically, it would further demonstrate the correlation with the periodicity property."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6PXJBA1gEE", "forum": "vzUixzPIHS", "replyto": "vzUixzPIHS", "signatures": ["ICLR.cc/2026/Conference/Submission5343/Reviewer_MDWE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5343/Reviewer_MDWE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761898003896, "cdate": 1761898003896, "tmdate": 1762918015586, "mdate": 1762918015586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper indicates the inherent periodic patterns and position shift properties of real images. Accordingly, they propose a fake image detection method based on a contrastive pre-training framework using only real images to learn the discrepancy between mathematically precise image rescaling traces and the imperfect approximations learned by generative models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This method reveals the nature of periodic patterns and position shift properties of bilinear interpolation, serving as guidance for the detection method.\n\n2. This paper is well-written."}, "weaknesses": {"value": "1. The proposed method is based on the assumption that real images contain mathematically precise image rescaling traces, while fake images only exhibit imperfect approximations of them. However, real images are not necessarily rescaled, in which case this assumption does not hold. Moreover, this assumption about fake images is not demonstrated in the preliminary analysis. In particular, according to prior studies [A], fake images may still retain preprocessing traces from their source images, including rescaling traces. This weakens the motivation of the paper and raises concerns about the validity of its underlying premise.\n\n2. The proposed method is designed based on rescaling, which is a low-level feature. Such low-level features are generally sensitive to post-processing perturbations. In particular, prior research [A] has demonstrated that post-processing operations can interfere with each other and even destroy forgery artifacts in fake images. Therefore, the robustness of the proposed method needs to be thoroughly evaluated.\n\n3. According to prior research [B], the training data used in the model contain biased image formats. Therefore, it is necessary to evaluate the performance of models on unbiased image formats, such as the test data used in [B] or the SynthBuster [C] dataset.\n\n4. Since both the preliminary analysis and the proposed method are based on the properties of bilinear interpolation, it is necessary to evaluate whether the model remains effective when the test data are rescaled using other interpolation methods.\n\n5. In Figure 6, the t-SNE visualizations of the original images and the post-rescaling images are plotted separately. These samples should be visualized together to show whether the post-rescaled samples are misclassified within the original feature space. Alternatively, experimental results should be provided to demonstrate whether fake images become misclassified as real after post-rescaling.\n\n[A] Corvi, Riccardo, et al. \"Intriguing properties of synthetic images: from generative adversarial networks to diffusion models.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.\n\n[B] Grommelt, Patrick, et al. \"Fake or jpeg? revealing common biases in generated image detection datasets.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.\n\n[C] Bammey, Quentin. \"Synthbuster: Towards detection of diffusion model-generated images.\" IEEE Open Journal of Signal Processing 5 (2023): 1-9."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FtgamFgJWC", "forum": "vzUixzPIHS", "replyto": "vzUixzPIHS", "signatures": ["ICLR.cc/2026/Conference/Submission5343/Reviewer_vBs2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5343/Reviewer_vBs2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5343/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982692017, "cdate": 1761982692017, "tmdate": 1762918015301, "mdate": 1762918015301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}