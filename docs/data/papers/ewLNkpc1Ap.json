{"id": "ewLNkpc1Ap", "number": 3546, "cdate": 1757471738513, "mdate": 1759898082090, "content": {"title": "TIC-GRPO: Provable and Efficient Optimization for Reinforcement Learning from Human Feedback", "abstract": "Group Relative Policy Optimization (GRPO), recently introduced by DeepSeek, is a critic-free reinforcement learning algorithm for fine-tuning large language models. GRPO replaces the value function in Proximal Policy Optimization (PPO) with group-normalized rewards while retaining PPO-style token-level importance sampling based on an old policy. We show that the GRPO update rule actually estimates the policy gradient at the old policy rather than the current one; however, because the old policy is refreshed every few steps, the gap remains small and the resulting bias is negligible in practice. To validate this, we perform an ablation study that removes importance sampling entirely and instead applies gradients estimated at a fixed old policy across multiple optimization steps. Remarkably, this simplified approach achieves performance comparable to standard GRPO.\n\nMotivated by these findings, we propose a new algorithm: Trajectory level Importance Corrected GRPO (TIC-GRPO). TIC-GRPO replaces token level importance ratios with a single trajectory level probability ratio, yielding an unbiased estimate of the current policy gradient while preserving the critic free structure. Furthermore, we present the first theoretical convergence analysis for GRPO style methods, covering both the original GRPO and our proposed variant.", "tldr": "On the Theory and Practice of GRPO: A Trajectory-Corrected Approach with Fast Convergence", "keywords": ["llm", "grop", "rlhf"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/424924e74a60f48eaebfe959f0f3a5ce3b3a4d5b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes TIC-GRPO， which replaces GRPO’s token-level importance sampling with a trajectory-level ratio and integrates two additional tricks: Length-Corrected Group Normalization and Upper-Only Clipping. The paper analyzes the convergence of GRPO and TIC-GRPO. Experiment using Qwen 3 on math reasoning tasks demonstrates the effectiveness of TIC-GRPO."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The GRPO gradient decomposition is clear.\n\n2. The proposed method is simple and easy to implement with the existing GRPO code."}, "weaknesses": {"value": "1. The abstract and introduction highlight that the TIC-GRPO estimator is unbiased, but the derivation of Appendix B shows that the TIC-GRPO estimator is not strictly unbiased. This is a material mismatch between the headline claim and the actual derivation.\n\n2. Assumption 5.1 requires global Lipschitz continuity of the score function for all states. This is a strong assumption for LLMs since there can be low-probability regions where logP can vary sharply.\n\n3. Theorem 5.2’s improved bound explicitly comes only from the length-corrected normalization and upper-only clipping, not from the trajectory-level importance ratio itself. This creates a disconnect with the central framing around trajectory-level sampling.\n\n4. The evaluation scope is too narrow. The evaluation uses only AIME 2024 as the benchmark. The author should include more benchmarks like MATH500, AIME 2025, and OlympiadBench."}, "questions": {"value": "Please see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2FbKmPqXgu", "forum": "ewLNkpc1Ap", "replyto": "ewLNkpc1Ap", "signatures": ["ICLR.cc/2026/Conference/Submission3546/Reviewer_BGTK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3546/Reviewer_BGTK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760866937660, "cdate": 1760866937660, "tmdate": 1762916810019, "mdate": 1762916810019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretically grounded and empirically enhanced variant of the Group Relative Policy Optimization (GRPO) algorithm for reinforcement learning from human feedback (RLHF) in large language models (LLMs). This work makes a substantial contribution by deepening the theoretical understanding of GRPO. And the authors provide the first convergence analysis for GRPO-style methods and introduce a simple yet powerful variant, TIC-GRPO. The combination of novel theory and a practical algorithm makes this a valuable piece of research for the RLHF community, with potential for influencing future theoretical RLHF work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work provides the first rigorous convergence analysis for GRPO-style methods, a popular class of critic-free RLHF algorithms. By establishing formal convergence guarantees under standard assumptions, the paper fills a critical theoretical gap in the literature. The convergence analysis is built on a solid foundation of standard and reasonable assumptions.\n\nThe paper delivers a crucial and insightful finding and elegantly explains why GRPO works in practice despite the bias. This theoretical clarification of the core mechanism is a significant step forward in understanding RLHF dynamics.\n\nThe paper is written with remarkable clarity. Furthermore, the authors demonstrate academic integrity by honestly attributing the tighter convergence bound of TIC-GRPO solely to the two minor modifications, not to the trajectory-level sampling. This conservative and transparent assessment builds trust and accurately scopes their theoretical contribution.\n\nBeyond the specific algorithm proposed, the paper's greatest impact lies in its theoretical rigor. In a field often dominated by empirical results, providing a principled theoretical framework for GRPO is an invaluable service to the community."}, "weaknesses": {"value": "**Narrow and Potentially Insufficient Empirical Validation**: \nConducting experiments on only one benchmark (AIME) is highly unusual and insufficient to establish generalizability. A review of other GRPO-related papers (e.g., DeepSeekMath, GSPO) shows they typically use multiple benchmarks. The failure to include, for example, AIME-25, significantly weakens the persuasiveness of the empirical claims.\n\n\n**Lack of Experiments Directly Supporting Theoretical Claims**: A major contribution is the convergence analysis, yet there are no experiments in the main text that visually demonstrate or validate the improved convergence rate or stability. Including such plots would significantly strengthen the link between theory and practice."}, "questions": {"value": "I have manually reproduced the derivation of Eq. (7) in your Section 3. In my result, the terms \\Xi_g(\\theta, \\theta_{\\text{old}}) and \\Xi_c(\\theta, \\theta_{\\text{old}}) do not have the multiplier \\frac{1}{\\theta_{\\text{old}}}. If space permits, could you please provide an explanation for this part?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8rR8rSbI8R", "forum": "ewLNkpc1Ap", "replyto": "ewLNkpc1Ap", "signatures": ["ICLR.cc/2026/Conference/Submission3546/Reviewer_HuD8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3546/Reviewer_HuD8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185610872, "cdate": 1762185610872, "tmdate": 1762916809750, "mdate": 1762916809750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TIC-GRPO (Trajectory-level Importance-Corrected Group Relative Policy Optimization), a theoretical and algorithmic refinement of GRPO (Group Relative Policy Optimization), recently introduced by DeepSeek for critic-free RLHF fine-tuning.\n\nThe authors first identify that standard GRPO estimates gradients at the old policy rather than the current one, explaining why this bias remains small in practice due to frequent policy refresh. Then, they propose TIC-GRPO, which corrects this by replacing token-level importance weights with trajectory-level probability ratios, yielding an unbiased estimator of the true policy gradient.\n\nThe paper provides the first convergence analysis for GRPO-style methods, proving stationarity bounds under Lipschitz and bounded-reward assumptions. Empirical evaluation on AIME and DAPO-17K datasets with Qwen-1.7B and Qwen-8B models shows that TIC-GRPO improves accuracy and convergence speed over GRPO, GSPO, and DAPO baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Clear theoretical motivation and correction. The decomposition in Eq. 7 demonstrates that GRPO’s update estimates ∇J at π_old rather than π, and TIC-GRPO’s trajectory-level ratio restores unbiasedness. The analysis bridges empirical intuition with formal theory.\n- Provable convergence guarantees. Theorems 5.1–5.2 give the first formal stationary-point convergence bounds for GRPO-style methods, showing improved asymptotic dependence after removing terms M_N and σ²_sT,N.\n- Simple yet effective modifications. The two “minor” refinements (length correction and upper-only clipping) are shown to individually improve stability, enhancing both fairness and interpretability."}, "weaknesses": {"value": "- Limited originality relative to concurrent work. The key modification—trajectory-level ratios—is nearly identical to GSPO (Zheng et al., 2025), which the authors acknowledge. While TIC-GRPO adds theoretical analysis and slightly different normalization, the conceptual leap is incremental.\n- Experiments are limited in scope. The evaluation focuses on AIME reasoning benchmarks, which are small-scale and synthetic. It’s unclear whether TIC-GRPO generalizes to more diverse RLHF settings (e.g., preference data, summarization, or open-ended dialogue).\n- Incremental empirical improvement. Although TIC-GRPO outperforms GRPO by +2–3 points, the margins are modest given additional computation and algorithmic tuning. There is no runtime or stability comparison (e.g., variance, gradient norms, or wall-clock efficiency).\n- Ablation isolation could be clearer. The claim that “theoretical improvement stems from two refinements only” (Sec. 5.2) implies trajectory-level importance may not improve the asymptotic rate—suggesting the empirical gains come mainly from the minor tweaks rather than the main theoretical contribution."}, "questions": {"value": "1. On gradient correctness: How do you empirically verify that TIC-GRPO’s gradient estimator better aligns with the true ∇J(θ)? Can you show cosine similarity between estimated and true gradients (or Monte Carlo rollouts) across updates?\n2. On contribution beyond GSPO: Could you clarify what new insights TIC-GRPO adds beyond GSPO besides clipping and convergence proof? Are these differences substantive enough to claim novelty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "fUe0ZLHwRz", "forum": "ewLNkpc1Ap", "replyto": "ewLNkpc1Ap", "signatures": ["ICLR.cc/2026/Conference/Submission3546/Reviewer_mWdQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3546/Reviewer_mWdQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762248644513, "cdate": 1762248644513, "tmdate": 1762916808899, "mdate": 1762916808899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper targets the high resource cost of PPO in RLHF caused by its extra value network. The authors first point out that GRPO’s token-level importance sampling actually estimates the policy gradient at the old policy π_old rather than at the current policy. Building on this observation, they propose Trajectory-level Importance-Corrected GRPO (TIC-GRPO). Theoretically, they provide the first convergence rate analysis for GRPO-style algorithms. Experiments on the AIME mathematical-reasoning benchmark show that TIC-GRPO significantly outperforms the original GRPO on both 1.7 B and 8 B models."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: The work is the first to reveal that GRPO essentially performs gradient estimation at the old policy, and it uses ablation studies to validate this insight, laying an intuitive foundation for further improvements.\n- Clarity: Concepts, formulas, and proofs are well presented, and the appendices are comprehensive; however, the meaning of some symbols is not explained."}, "weaknesses": {"value": "1. In Eq. (7), the subsequent proofs bound some error terms by problem-dependent constants, whereas other bounds are independent of hyper-parameters. Yet in RL the policy changes little between two consecutive steps. What, then, is the justification for decomposing the expression into so many terms in Eq. (7)?\n2. The upper bound in the theorem does not contain the hyper-parameters $\\epsilon_{high}$ and $\\epsilon_{low}$. Does this mean their values do not affect the bound? If so, can the bound be further improved?\n3. Main results are reported only on the single mathematical-reasoning task AIME; there is no verification on diverse tasks such as dialogue, code generation, or creative writing. It is therefore unclear whether the gains are task-specific. Experiments at larger scales are also needed.\n4. The variance of the trajectory-level importance ratio usually grows exponentially with length. How do the authors handle this issue?\n5. The authors claim their method has better sample efficiency. What is the intuitive explanation?\n6. There is no dedicated “Conclusion” section."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0px5OG5qh6", "forum": "ewLNkpc1Ap", "replyto": "ewLNkpc1Ap", "signatures": ["ICLR.cc/2026/Conference/Submission3546/Reviewer_zH4z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3546/Reviewer_zH4z"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762872816686, "cdate": 1762872816686, "tmdate": 1762916806982, "mdate": 1762916806982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}