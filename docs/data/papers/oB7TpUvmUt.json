{"id": "oB7TpUvmUt", "number": 11655, "cdate": 1758202840022, "mdate": 1759897562546, "content": {"title": "GraphBreak: Systematic Exploitation of LLM Safety Mechanisms Through Semantic Structure Manipulation", "abstract": "Large Language Models (LLMs) have been equipped with safety mechanisms to prevent harmful outputs, but these guardrails can often be bypassed through ``jailbreak'' prompts. This paper introduces a novel graph-based approach to systematically generate jailbreak prompts through semantic transformations. We represent malicious prompts as nodes in a graph structure with edges denoting different transformations, leveraging Abstract Meaning Representation (AMR) and Resource Description Framework (RDF) to parse user goals into semantic components that can be manipulated to evade safety filters. \nWe demonstrate a particularly effective exploitation vector by instructing LLMs to generate code that realizes the intent described in these semantic graphs, achieving success rates of up to 87\\% against leading commercial LLMs. \nOur analysis reveals that contextual framing and abstraction are particularly effective at circumventing safety measures, highlighting critical gaps in current safety alignment techniques that focus primarily on surface-level patterns. \nThese findings provide insights for developing more robust safeguards against structured semantic attacks. \nOur research contributes a practical and cost-efficient methodology for systematically stress-testing LLM safety mechanisms.", "tldr": "We develop a novel jailbreaking attack against LLMs by leveraging graph representations", "keywords": ["LLM safety", "jailbreaking attacks", "graph representation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bd3241fa2dc8f4e51035645818955f36f6aab142.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GraphBreak, a novel graph-based jailbreaking approach that exploits LLM safety mechanism vulnerabilities through semantic structure manipulation. Unlike surface-level prompt engineering methods, GraphBreak parses malicious intents into structured semantic representations—Abstract Meaning Representation (AMR) and Resource Description Framework (RDF)—then instructs LLMs to generate code implementing these graph-described intents. This \"knowledge-to-code\" pathway bypasses intent-based safety filters by framing harmful requests as technical tasks. The key contributions are: 1) proposing a novel attack methodology and presenting guides for LLMs’ robust defense."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s significance extends to both research and practical domains, addressing a pressing need for robust LLM safety evaluation and defense. It demonstrates strong originality across multiple dimensions, addressing a critical gap in LLM jailbreaking research with a creative methodological design. The paper maintains high methodological and empirical quality, with rigorous design, comprehensive validation, and transparent reporting.  It is exceptionally clear in its structure, exposition, and technical communication, making complex concepts accessible to readers across LLM safety and NLP backgrounds."}, "weaknesses": {"value": "The paper claims to support \"multi-modal attack vectors beyond text-based AMR/RDF\" (e.g., XML, JSON, image-encoded semantic graphs), but this claim is not rigorously validated—undermining its goal of comprehensively exploring semantic attack surfaces.\nThe paper proposes three defensive strategies (semantic-aware filters, cross-representation consistency, and technical context intent recognition)  but provides no empirical validation of their effectiveness, weakening its contribution to practical LLM safety engineering. These weaknesses are not fatal but limit the paper’s ability to fully achieve its stated goals of systematic exploitation and defense guidance."}, "questions": {"value": "You propose three defensive strategies, including \"semantic-aware safety filters\" that parse AMR/RDF to detect harmful structures but provide no empirical validation. Have you built a minimal prototype of this defense—e.g., using a pre-trained AMR parser to extract triples, then a classifier to flag \"harmful triples\" (e.g., <extract, target, toxic-substance>)—and tested how much it reduces GraphBreak’s ASR (e.g., from 87% to X% for Qwen2.5-72B on AdvBench)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OhezsBPpOM", "forum": "oB7TpUvmUt", "replyto": "oB7TpUvmUt", "signatures": ["ICLR.cc/2026/Conference/Submission11655/Reviewer_93Es"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11655/Reviewer_93Es"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814997607, "cdate": 1761814997607, "tmdate": 1762922717083, "mdate": 1762922717083, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes GraphBreak, a semantic graph–based method for systematically jailbreaking large language models. Instead of surface-level prompt rewriting, the method converts malicious queries into Abstract Meaning Representation or RDF graphs and asks the model to generate code that realizes their intent. This exploits weaker safety alignment in technical contexts. Experiments on major commercial and open models show attack success rates up to 87%, outperforming baselines such as PAIR, AutoDAN, and CodeAttack."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies an underexplored dimension of jailbreak attacks by analyzing vulnerabilities at the semantic representation level rather than at the surface-text level.\n2. The graph-based formulation provides a systematic framework for studying how LLMs process harmful intent under different structural abstractions.\n3. The knowledge-to-code attack pathway is an interesting extension that empirically demonstrates weaker safety enforcement in technical or formal contexts."}, "weaknesses": {"value": "1. The existing jailbreak methods appear somewhat outdated, as they only cover studies from 2024 and earlier. Jailbreak attacks are rapidly evolving, including multi-turn jailbreaks [1] and other adaptive attacks [2]. Could the authors clarify why recent works from 2025 were omitted?\n\n2. Several figures suffer from poor readability, with font sizes too small to interpret clearly (e.g., Figure 2). Could the authors improve figure legibility or adopt alternative layouts for clarity? Also, the citation format, like ReNeLLM Ding et al. (2023b), seems incorrect to me; it would be better to adjust it.\n\n3. The evaluation focuses exclusively on non-reasoning models such as GPT-4o, Claude-3.7, Llama-3, and Qwen-2.5. It remains unclear whether the proposed semantic transformations transfer to reasoning models like DeepSeek-R1, QwQ, that exhibit stronger interpretive capabilities or multi-step reasoning safeguards. Could the authors clarify if such models were tested and whether reasoning architectures mitigate or amplify these vulnerabilities?\n\n4. The evaluation relies heavily on automated judges (GPT-4 and Llama Guard 2), Have the authors evaluated the agreement between these judges and human annotators? If possible, could the authors report metrics such as Cohen’s κ to quantify inter-judge reliability and clarify whether human judgments align with automated evaluations?\n\n5.  The evidence provided, such as PCA visualizations and ASR differences, appears correlational. Could the authors isolate whether the performance gains result from semantic abstraction itself or from models’ weaker filtering of non-natural language inputs like code or XML?\n\n6. The code-generation pathway demonstrates high attack success rates, but its real-world feasibility remains unclear. Could the authors clarify whether such structured graph inputs would realistically be extended by deployed commercial APIs?\n\nReferences.\n\n[1]. Russinovich, M., Salem, A., & Eldan, R. (2025). Great, now write an article about that: The crescendo {Multi-Turn}{LLM} jailbreak attack. In 34th USENIX Security Symposium (USENIX Security 25) (pp. 2421-2440).\n\n[2]. Kuo, M., Zhang, J., Ding, A., Wang, Q., DiValentin, L., Bao, Y., ... & Chen, Y. (2025). H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint arXiv:2502.12893."}, "questions": {"value": "See my aforementioned weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JaImcaGrjN", "forum": "oB7TpUvmUt", "replyto": "oB7TpUvmUt", "signatures": ["ICLR.cc/2026/Conference/Submission11655/Reviewer_ELuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11655/Reviewer_ELuZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844282867, "cdate": 1761844282867, "tmdate": 1762922716686, "mdate": 1762922716686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method that evaluates LLM safety by converting natural language prompts into formal semantic graph representations. A key part of the methodology involves prompting the model to generate code that realizes the semantic intent of the graph. The study reports that this transformation of a request from natural language to a semantic graph, particularly when paired with a code generation task, can circumvent the safety protocols of the tested LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* Originality: This paper introduces a new attack method by transforming prompts into formal semantic graphs, which moves beyond typical syntactic manipulation.\n\n* Clarity: The paper is clearly written with a logical structure.\n\n* Significance: The evaluations reveal a systemic vulnerability in LLMs and provide a clear direction for future safety research."}, "weaknesses": {"value": "* The paper's central claim is that it bypasses safety via semantic abstraction. However, it provides insufficient evidence to distinguish this from a more straightforward explanation: complex obfuscation. Wrapping a harmful request in layers of formal syntax (RDF/AMR) and then embedding it within a code-generation task is still a multi-layered obfuscation technique. The method may not be tricking the model's semantic understanding of intent, but rather its surface-level pattern recognition, which fails to identify the toxic payload buried within the complex structure.\n* The lack of rigorous human evaluation to calibrate and validate the automated judges on the method's unique outputs undermines the credibility of the results.\n* The paper claims the knowledge-to-code pathway as a novel insight into reframing intent. However, exploiting the reduced safety guardrails in code generation contexts is a well-known jailbreaking technique. The method essentially repackages this known vulnerability with a semantic front-end. It fails to demonstrate that its pathway is conceptually distinct from simply asking a model to write code for a forbidden task, thereby overstating its contribution to the field. The authors should demonstrate their distinction on this issue."}, "questions": {"value": "* Regarding Weakness 1, how can the authors definitively prove that the model is understanding the harmful semantics of the graph, rather than just failing to parse the obfuscated prompt correctly?\n* Regarding Weakness 2, what is the inter-annotator agreement between your LLM as a judge and human evaluators on your attack outputs? This data is critical for validating the judges' reliability. Given that LLM as a judge is a known area of research with documented biases, why did the authors not include a robust human evaluation component as the primary source of truth for the final results?\n* Regarding Weakness 3, how is the author's knowledge-to-code pathway conceptually different from prior jailbreaking techniques that instruct the model to write a program to simulate a harmful act? can the authors quantify the marginal increase in attack success rate provided by the semantic graph over and above a baseline attack that simply wraps the harmful request in a standard code-generation prompt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1BpXXBdwmv", "forum": "oB7TpUvmUt", "replyto": "oB7TpUvmUt", "signatures": ["ICLR.cc/2026/Conference/Submission11655/Reviewer_KWn9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11655/Reviewer_KWn9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918130442, "cdate": 1761918130442, "tmdate": 1762922716324, "mdate": 1762922716324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper introduces a novel black-box jailbreak attack approach, which converts illicit requests into a formal semantic representation, leveraging established ontologies including  Abstract Meaning Representation (AMR) and Resource Description Framework (RDF).\n- The work also proposes the composition of the semantic representation with a Code-based representation, converting the semantic representation into python code.\n- This work compares the performance of the proposed black-box attack technique with 5 previously published black-box attacks.\n- This work performs ASR evaluation using two automated evaluation methods based on GPT4-o and Llama Guard 2.\n- This work performs a qualitative analysis of a model's internal representation of illicit requests transformed using the proposed black-box attack, showing that the transformation results in illicit prompts that fall under the distribution of safe prompts."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- This work performs an analysis of a model's internal representation of the lexical representation of illicit requests before and after applying the proposed transformation using PCA, which is a helpful source for understanding the distinctive features of illicit prompts transformed using the proposed approach and the limitations of current defense mechanisms.\n- This work leverages previously used datasets of illicit requests such as AdvBench, which enables comparable empirical evaluation.\n- This work compares the proposed jailbreak attack with 5 previously published ones, including CodeAttack, PAIR, CipherChat, AutoDAN, and PAP.\n- This work discusses several previously published attack mechanisms and distinguishes attack mechanisms that focus on surface-level features compared to semantic structures.\n- This work jointly employed two ASR evaluation methods: \"GPT-4 Judge\" and \"Llama Guard 2.\"\n- This work presented ablation results showing the impact of the composed black-box jailbreak primitives (RDF, AMR, and code representation)."}, "weaknesses": {"value": "- The authors did not provide adequate citations for RDF. Candidates may include [1], [2], and [3].\n- The proposed attack mechanism appears to fall under the \"black-box jailbreak attack\" category [4]. It would be helpful to explicitly mention this taxonomy if applicable. I note that the proposed PCA analysis requires white-box access to the evaluated model; however, the proposed attack mechanism does not, hence it falls in the black-box attack category.\n- Even though this work discusses several previously published attack mechanisms, it doesn't sufficiently discuss the composition of such approaches, which has been shown to potentially result in more effective attacks ([4], [5]). This work also makes use of compositions of primitive attacks. For instance, it composes the semantic representation step (RDF and AMR) with CoT (step-by-step method in code) and Python code representation of the request.\n- The authors did not include an ethics statement, which would be valuable given the potential for dual use.\n\n# References\n- [1] Ora, L. (1999). Resource description framework (RDF) model and syntax specification. http://www. w3. org/TR/REC-rdf-syntax/.\n- [2] Pan, J. Z. (2009). Resource description framework. In Handbook on ontologies (pp. 71-90). Berlin, Heidelberg: Springer Berlin Heidelberg.\n- [3] Brickley, D., & Guha, R. V. (2014). RDF schema 1.1, W3C recommendation, world wide web consortium.\n- [4] Wei, A., Haghtalab, N., & Steinhardt, J. (2023). Jailbroken: How does llm safety training fail?. Advances in Neural Information Processing Systems, 36, 80079-80110.\n- [5] Doumbouya, M. K. B., Nandi, A., Poesia, G., Ghilardi, D., Goldie, A., Bianchi, F., ... & Manning, C. D. h4rm3l: A Language for Composable Jailbreak Attack Synthesis. In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "- Line 115, Figure 1: The caption seems wrong. \"GraphBreak (blue)\" should probably be \"GraphBreak (red).\"\n- Which prompt was used to construct the GPT-4 judge?\n- How were the outputs of the two evaluators combined? (Conjunction or disjunction?)\n- Line 16 \"We represent malicious prompts as nodes in a graph structure with edges denoting different transformations, leveraging Abstract Meaning Representation (AMR) and Resource Description Framework (RDF) to parse user goals into semantic components that can be manipulated to evade safety filters.\" \n    - Could this statement be simplified for clarity, and aligned with prior formalisms? It leads the reader into thinking that the semantic structure of the illicit requests is somehow manipulated after parsing. As I understand, the key insights are that:\n        - The AMR and RDF representation of illicit requests, being out of the traditional distribution of illicit requests, causes safety filters to fail, while the LLM still demonstrate the capability of processing and responding to such requests. This is a case of \"Mismatched Generalization\" (see [4]).\n            - Note that several previously published black box attacks follow this pattern, the classic being base64 encoding. However the particular string transformation proposed in this work (illicit prompt to RDF/AMR parse of that string) appears novel.\n        - The code representation of the semantic documents is an added layer (of composition of string transformations) that further exacerbate the case of mismatched generalization.\n            - Note that prior work has shown that compositing individual transformations can result in more effective attacks (see [4] and [5])"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D5AoIPoYJK", "forum": "oB7TpUvmUt", "replyto": "oB7TpUvmUt", "signatures": ["ICLR.cc/2026/Conference/Submission11655/Reviewer_JP9p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11655/Reviewer_JP9p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11655/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762135676988, "cdate": 1762135676988, "tmdate": 1762922715863, "mdate": 1762922715863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}