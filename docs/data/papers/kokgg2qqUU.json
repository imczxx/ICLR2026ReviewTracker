{"id": "kokgg2qqUU", "number": 7069, "cdate": 1758006595376, "mdate": 1759897874311, "content": {"title": "PVPO: Pre-Estimated Value-Based Policy Optimization for Agentic Reasoning", "abstract": "Critic-free reinforcement learning methods, particularly group policies, have attracted considerable attention for their efficiency in complex tasks. However, these methods rely heavily on multiple sampling and comparisons within the policy to estimate advantage, which may cause the policy to fall into local optimum and increase computational cost. To address these issues, we propose PVPO, an efficient reinforcement learning method enhanced by an  advantage reference anchor and data pre-sampling. Specifically, we use the reference model to rollout in advance and employ the calculated reward score as a reference anchor. Our approach effectively corrects the cumulative bias introduced by intra-group comparisons and significantly reduces reliance on the number of rollouts during training. Meanwhile, the reference model can assess sample difficulty during data pre-sampling, enabling effective selection of high-gain data to improve training efficiency. Moreover, PVPO is orthogonal to other advanced critic-free RL algorithms, making it compatible with and complementary to these methods. Experiments conducted on nine datasets across two domains demonstrate that PVPO achieves State-Of-The-Art (SOTA) performance. Our approach not only demonstrates robust generalization across multiple tasks, but also exhibits scalable performance across models of varying scales.", "tldr": "", "keywords": ["critic-free reinforcement learning", "group sampling", "advantage reference anchor", "policy optimization", "agentic reasoning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8b13c9f6ba3e3652b53b98e8079eb29b8fda74ac.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose an critic-free RL method for large model training that offers an alternative to group policies which require multiple samples per query.  Their method avoids group-based advantage estimation by using a static value function instead, which is just the average reward over multiple rollouts of the reference policy, and can be pre-computed.  They also generate synthetic demonstrations by using a larger LLM to generate ground truth trajectories for samples with 0 mean accuracy.  The compare with both prompt-based LLMs and GRPO on multi-hop QA tasks and reasoning tasks with strong performance overall."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors identify a sample efficiency problem in critic free RL and propose a reasonable method of using a static value baseline.\n2. The authors demonstrate strong performance over multiple benchmarks."}, "weaknesses": {"value": "1. Comparisons may be unfair.  Their method uses a larger LLM to generate trajectories for difficult tasks which makes it an unfair comparison to GRPO which does not.  Do you apply the same group sampling to other comparison methods?  If not, PVPO is getting privileged information which explains its strong performance.\n\n2. The sample efficiency/cost gains of PVPO are not clear.  One of the main claims is that this method is a more efficient RL method because it bypasses multiple sampling.  However, the static value estimates still need to be updated periodically and it requires additional trajectories from a larger LLM.  How does the actual combined sample efficiency compare?\n\n3. There are multiple unsupported or wrong statements in the writing.\n- Abstract states \"our approach effectively corrects the cumulative bias introduced by intra-group comparisons\".  This is never explained later in the paper.\n- Intro states \"effectively mitigating concerns of error accumulation and policy drift during training\".  Nothing in the method explicitly addresses error accumulation or policy drift in comparison to baseline methods.\n- 4.1 \"estimate of the action value Q is the final reward r_i observed in the trajectory\".  This is true only if there are no intermediate rewards in the episode and the discount is 1.\n- Training efficiency results in Figure 2 aren't conclusive.  This is a high variance learning curve over what looks to be a single seed."}, "questions": {"value": "1. Section 4.1 \"the trajectory generation process is regarded as atomic actions a_i=tau_i [...] makes the reward distribution [..] depend only on initial state and pi\".  Does this mean that the entire action sequence is generated at once from $$s_{i,0}$$.  If so, how do you handle the responses from the multi-hop QA problems?\n2. Do you regenerate the static V estimate for all queries at a fixed interval throughout training?  If so, how does the number of these samples compare to the group samples used in GRPO for advantage estimation?  Are these samples used for PPO updates?  Does your method break down into GRPO with reward caching?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rzs8eXd1iy", "forum": "kokgg2qqUU", "replyto": "kokgg2qqUU", "signatures": ["ICLR.cc/2026/Conference/Submission7069/Reviewer_WdaM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7069/Reviewer_WdaM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530611470, "cdate": 1761530611470, "tmdate": 1762919258591, "mdate": 1762919258591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PVPO, a critic-free RL method for agentic reasoning. It replaces GRPO’s group-wise “dynamic V” baseline with a Static-V baseline using a fixed reference policy. It further introduces Group Sampling to filter trivial items and  inject  Ground-Truth Trajectories (GT Traj) generated by a larger LLM for difficult items.  On multi-hop retrieval (ReSearch/DynaSearcher) and math reasoning (7B/14B), PVPO outperforms GRPO-family baselines and converges faster."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Addresses a critical issue—the high variance in GRPO—which is important and timely to study.\n* The use of GT trajectories for hard cases is well-motivated and methodologically sound.\n* The method demonstrates significant, consistent improvements over strong baselines."}, "weaknesses": {"value": "* Using a state-independent/reference baseline for variance reduction is well established in policy-gradient and recent critic-free methods. The paper should more systematically position the “Static-V anchor” relative to Dr.GRPO, DAPO, and GSPO, clarifying its distinct contribution and the conditions under which it outperforms these methods.\n* The evidence is predominantly empirical; formal guarantees (e.g., convergence or improvement bounds, bias/variance analysis) are missing.\n\n* The observed stability appears to come from two design choices: (i) a fixed reference model and (ii) a cached baseline. An ablation of GRPO with a cached baseline (without group normalization)—matching PVPO’s refresh frequency and offline rollout count—would help isolate these effects."}, "questions": {"value": "* Using a larger LLM to generate GT trajectories can introduce distribution shift. Is it necessary to use a larger model from the same family as the trained small model (as in the paper’s Qwen→Qwen setup)? What happens if the demonstrator comes from a different family (e.g., DeepSeek)?\n\n* Why was the “kg filter” removed in DynaSearcher? How does its removal affect the relative gap between GRPO and PVPO? Please provide controlled experiments with the filter kept to isolate this effect."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UNrXMBG7bg", "forum": "kokgg2qqUU", "replyto": "kokgg2qqUU", "signatures": ["ICLR.cc/2026/Conference/Submission7069/Reviewer_xtLf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7069/Reviewer_xtLf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757375200, "cdate": 1761757375200, "tmdate": 1762919258199, "mdate": 1762919258199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes a novel RL method using reference anchors and data pre-sampling."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The research problem is interesting and useful."}, "weaknesses": {"value": "1. The introduction should be improved. The research motivation and problem setting (e.g., sparse reward) are not very clear, and the keyword “agentic reasoning” in the title is never mentioned.\n\n2. The writing of the technical part could and should be improved, too. For example, over 50% of Sec. 4.1 should be removed to Sec. 3.\n\n3. I understand that the proposed static V estimation is stable, but why is it good enough, or how is a good reference policy determined?"}, "questions": {"value": "See detailed comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gtuVrnQZI5", "forum": "kokgg2qqUU", "replyto": "kokgg2qqUU", "signatures": ["ICLR.cc/2026/Conference/Submission7069/Reviewer_x8Ph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7069/Reviewer_x8Ph"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761916567713, "cdate": 1761916567713, "tmdate": 1762919257819, "mdate": 1762919257819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that dynamic value estimates, as traditionally used in GRPO setting, leads to high variance estimate and local optimum. To overcome this, they propose to use static value estimate from a reference model. They also dynamically group samples based on the quality of the current response. They perform considerable number of experiments across nine different datasets to validate their approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Strengths:\n\n1. This work presents an alternative approach for critic-free RL that leverages a low-variance and globally consistent advantage function to address error accumulation and policy drift during training. \n\n2. Validation and ablations are conducted across a good number of different datasets and tasks. These experimental results reflect the efficacy of the proposed approach."}, "weaknesses": {"value": "### Weaknesses:\n\n1. GRPO’s one of the main objectives is to reduce resource consumption. However, the proposed method needs to maintain a reference model and also uses a large model for ground truth in case of failure. This is somewhat contradictory and limits the overall gain.\n\n2. Further, the memory overhead due to these additional components has not been discussed. Also, I would like to see some discussion on the limitations. \n\n3. The details of the reference model is not clear. The paper mentions that the reference anchor is computed in an unsupervised manner, however, I do not see any detailed discussion on this. Are you updating the reference model after certain steps like a target network?\n\n4. I would consider the update frequency of the reference reward as a hyperparameter. How is the value of that hyperparameter selected? How sensitive the model is to that hyperparameter?"}, "questions": {"value": "1. In line 76-77, what did you mean by spatio-temporal overhead?\n\nAlso, look at the weaknesses section for other questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LveVa7ZBja", "forum": "kokgg2qqUU", "replyto": "kokgg2qqUU", "signatures": ["ICLR.cc/2026/Conference/Submission7069/Reviewer_kihm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7069/Reviewer_kihm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7069/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036733641, "cdate": 1762036733641, "tmdate": 1762919257277, "mdate": 1762919257277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}