{"id": "baomU18Dgb", "number": 8482, "cdate": 1758086241193, "mdate": 1759897781619, "content": {"title": "FreeEyeglass: Training-free and Mask-free Eyeglass Transfer for Facial Videos", "abstract": "The rise of e-commerce and short-video platforms has fueled demand for realistic video-based virtual try-on. Unlike virtual try-on of clothing, which has been actively studied so far, virtual try-on of eyeglasses is uniquely challenging: they physically interact with facial geometry, and they strongly affect facial identity, making faithful preservation of unedited regions especially important. Existing generative editing approaches, such as GAN- and diffusion-based methods, lack reconstruction objectives and often rely on inpainting, which fails to ensure identity consistency. We argue that semantic editing requires not only plausible generation but also faithful reconstruction, making autoencoder-based latent spaces particularly suitable. We introduce a training-free, reference-guided framework for video eyeglass transfer built on Diffusion Autoencoders (DiffAE). By blending semantic features in the encoder and incorporating spatial-temporal self-attention, our method achieves realistic, identity-preserving, and temporally consistent results, and points to the potential of autoencoder-based latent spaces for local video editing. Our implementations and datasets will be released upon acceptance.", "tldr": "A training-free approach based on diffusion autoencoder to transfer desired eyeglasses semantically to facial videos", "keywords": ["video editing", "eyeglasses", "facial video", "virtual try-on"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79e12d8d39d3af2b13764f3761d09bd37672705d.pdf", "supplementary_material": "/attachment/bd55e33dfe5c471ca5b711f25aa1885388fc5e3b.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents a method to transfor eyeglasses in videos of faces using the Diffusion Autoencoder framework. The method first employs a feature blending step, using an input mask of the glasses, followed by a \"regional self-attention\", based on optical flow, in to order to target the region to put the glasses in."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I found the paper well-written, quite easy to follow, the different steps makes a lot of sense, and the final results are pretty good, with good temporal consistency. I am very happy with the extensive comparisons with the state-of-the-art."}, "weaknesses": {"value": "One of the main criticsims I would have of the paper is that the application seems extremely \"small\" compared to the sophisticated nature of the method. Did you try to put other objects rather than eyeglasses ? Does this work ? I think one really would expect other examples for such a seemingly powerful method. If you wish to keep the glasses application only, then you should justify this (at least to the reviewers). I feel that in an actual applicative (industrial) context, that the tool would be extremely limited, a user is not going to want to load a different model for each type of object to insert.\n\nAnother point is that the region self-attention is proposed as novel, and a contribution of yours. But then you basically say after that you use the method of \"FLATTEN: optical FLow‑guided ATTENtion for consistent text‑to‑video editing\", Cong et al 2022. Please make it clearer whether you are just using their method, or if there is some difference between the two ? Otherwise, I think the idea makes a lot of sense."}, "questions": {"value": "One of the main criticsims I would have of the paper is that the application seems extremely \"small\" compared to the sophisticated nature of the method. Did you try to put other objects rather than eyeglasses ? Does this work ? I think one really would expect other examples for such a seemingly powerful method. If you wish to keep the glasses application only, then you should justify this (at least to the reviewers). I feel that in an actual applicative (industrial) context, that the tool would be extremely limited, a user is not going to want to load a different model for each type of object to insert.\n\nAnother point is that the region self-attention is proposed as novel, and a contribution of yours. But then you basically say after that you use the method of \"FLATTEN: optical FLow‑guided ATTENtion for consistent text‑to‑video editing\", Cong et al 2022. Please make it clearer whether you are just using their method, or if there is some difference between the two ? Otherwise, I think the idea makes a lot of sense.\n\nHow is the mask M obtained ? Do you have an eyeglasses segmentation network or something similar ? If so, how to you resize the mask to fit each layer of the U-Net. This seems to be quite an important point, as the blending process hinges on this.\n\nEquation 2: you add a binary mask to a blurred version of itself, and then clamp the output values. This looks a bit weird at first, but by doing this it seems that you keep the inner parts of the \"1\" regions of the mask to 1, and just blur the borders a bit, is this true ? If so, maybe you could explain it like this. In other words, you want a smooth blending on the border, but in the inner part, the blending should be binary. It is not super well explained why this is the best choice. This is done somewhat at the end of the paragraph, but I did not find the explanation convincing, for example why should this blending choice imrove \"the preservation of fine details in the edited images\", it just seems to make a smoother border transition no ?\n\n\nSpecific details:\n\n- p. 5, line 239: \"We implement the proposed regional self-attention concept to the state-of-the-art temporal attention using optical flows\". Rephrase, sentence is not comprehensible\n- section 3.3: local self-attention is a pretty well-studied subject, you present it as if there are no other works on it. In particular:\n- \"Flow‑Guided Transformer for Video Inpainting\", Zhang et al, ECCV 2022\n- \"FLATTEN: optical FLow‑guided ATTENtion for consistent text‑to‑video editing\" (as you cite)\n- small note: when one says \"eyeglasses\", it usually means as opposed to sunglasses; but your method does both. Maybe change this (although this is not crucial)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1PUyMsMhnp", "forum": "baomU18Dgb", "replyto": "baomU18Dgb", "signatures": ["ICLR.cc/2026/Conference/Submission8482/Reviewer_xwtJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8482/Reviewer_xwtJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837484773, "cdate": 1761837484773, "tmdate": 1762920358987, "mdate": 1762920358987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method to transfor eyeglasses in videos of faces using the Diffusion Autoencoder framework. The method first employs a feature blending step, using an input mask of the glasses, followed by a \"regional self-attention\", based on optical flow, in to order to target the region to put the glasses in."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I found the paper well-written, quite easy to follow, the different steps makes a lot of sense, and the final results are pretty good, with good temporal consistency. I am very happy with the extensive comparisons with the state-of-the-art."}, "weaknesses": {"value": "One of the main criticsims I would have of the paper is that the application seems extremely \"small\" compared to the sophisticated nature of the method. Did you try to put other objects rather than eyeglasses ? Does this work ? I think one really would expect other examples for such a seemingly powerful method. If you wish to keep the glasses application only, then you should justify this (at least to the reviewers). I feel that in an actual applicative (industrial) context, that the tool would be extremely limited, a user is not going to want to load a different model for each type of object to insert.\n\nAnother point is that the region self-attention is proposed as novel, and a contribution of yours. But then you basically say after that you use the method of \"FLATTEN: optical FLow‑guided ATTENtion for consistent text‑to‑video editing\", Cong et al 2022. Please make it clearer whether you are just using their method, or if there is some difference between the two ? Otherwise, I think the idea makes a lot of sense."}, "questions": {"value": "How is the mask M obtained ? Do you have an eyeglasses segmentation network or something similar ? If so, how to you resize the mask to fit each layer of the U-Net. This seems to be quite an important point, as the blending process hinges on this.\n\nEquation 2: you add a binary mask to a blurred version of itself, and then clamp the output values. This looks a bit weird at first, but by doing this it seems that you keep the inner parts of the \"1\" regions of the mask to 1, and just blur the borders a bit, is this true ? If so, maybe you could explain it like this. In other words, you want a smooth blending on the border, but in the inner part, the blending should be binary. It is not super well explained why this is the best choice. This is done somewhat at the end of the paragraph, but I did not find the explanation convincing, for example why should this blending choice imrove \"the preservation of fine details in the edited images\", it just seems to make a smoother border transition no ?\n\n\nSpecific details:\n\n- p. 5, line 239: \"We implement the proposed regional self-attention concept to the state-of-the-art temporal attention using optical flows\". Rephrase, sentence is not comprehensible\n- section 3.3: local self-attention is a pretty well-studied subject, you present it as if there are no other works on it. In particular:\n- \"Flow‑Guided Transformer for Video Inpainting\", Zhang et al, ECCV 2022\n- \"FLATTEN: optical FLow‑guided ATTENtion for consistent text‑to‑video editing\" (as you cite)\n- small note: when one says \"eyeglasses\", it usually means as opposed to sunglasses; but your method does both. Maybe change this (although this is not crucial)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1PUyMsMhnp", "forum": "baomU18Dgb", "replyto": "baomU18Dgb", "signatures": ["ICLR.cc/2026/Conference/Submission8482/Reviewer_xwtJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8482/Reviewer_xwtJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837484773, "cdate": 1761837484773, "tmdate": 1763403908656, "mdate": 1763403908656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to solve the problem of eyeglass transfer in facial videos. This paper proposes a \"training-free\" and \"mask-free\" framework built upon a pre-trained Diffusion Autoencoder (DiffAE). The core method involves blending reference features in the DiffAE's semantic encoder while also fusing stochastic latents to preserve details . To handle videos, the method extends the 2D architecture to pseudo-3D and employs a regional, flow-guided self-attention mechanism to maintain temporal consistency ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a commercially relevant and technically challenging problem (video virtual try-on) that is currently underexplored, especially compared to virtual clothes try-on.\n2. The choice of a Diffusion Autoencoder (DiffAE) as the backbone is well-motivated. For a local editing task like this, the explicit reconstruction objective of an autoencoder is crucial for preserving unedited facial regions and identity, a key insight of the paper ."}, "weaknesses": {"value": "1. The \"mask-free\" is over claimed. While the target video does not require a mask, the method explicitly requires a binary mask $M$ for the reference image. This mask, obtained from Grounded-SAM, is fundamental to the feature-blending operation. \n2. limited novelty. The method is largely an assemblage of existing techniques. The backbone is DiffAE , the video editing pipeline is borrowed from prior work , and the temporal attention mechanism is a direct adaptation of FLATTEN.\n3. Narrow evaluation.The evaluation benchmark is strictly filtered, limiting videos to small head poses ($\\pm15^{\\circ}$). This avoids the most difficult challenges of eyeglass transfer, such as maintaining 3D consistency during large-angle rotations, which is critical for a robust real-world application."}, "questions": {"value": "1. The compared baselines are general try-on or only for eyeglasses？It may be unfair to compare to general try-on methods.\n2. Is the method robust to the eye masks? An analysis of how inaccuracies in the mask itself (i.e., the issue of robustness) would affect the model's performance is necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YPnw0cSNXt", "forum": "baomU18Dgb", "replyto": "baomU18Dgb", "signatures": ["ICLR.cc/2026/Conference/Submission8482/Reviewer_NpfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8482/Reviewer_NpfN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874733792, "cdate": 1761874733792, "tmdate": 1762920358548, "mdate": 1762920358548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FreeEyeglass, a training-free framework for reference-guided eyeglass transfer in facial videos. It leverages Diffusion Autoencoders (DiffAE) for editing purpose, which is claimed to be better at perceiving local identities than the inpainting methods. \n\nThe key innovations include (1) feature blending in the semantic encoder and stochastic latent to inject reference eyeglass information and (2) a regional self-attention guided with optical flows to edit only desired regions in input videos."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is training-free, which makes it lightweight and easy to deploy compared to approaches requiring fine-tuning or retraining.\n2. The proposed feature and stochastic blending strategy are well-motivated and technically reasonable.  Inflating the 2D method to 3D is interesting, and a regional self-attention with optical flows is proposed to ensure better preservation of undesired regions in the input video. \n3. The evaluation covers multiple baselines and various metrics. The reported metrics and quantitative results demonstrate improved performance on the eyeglass transfer task."}, "weaknesses": {"value": "1.\tThe claim that the method is mask-free is somewhat misleading. It still requires a segmentation mask for the reference eyeglasses and a predefined region of interest (ROI) in the target video.\n2.\tThe scope of the paper feels narrow. While eyeglass transfer is an interesting and useful application, the method itself could likely generalize to other types of local edits beyond eyeglasses, given it is training-free and not inherently task-specific (aside from the eyeglass mask).\n3.  From the ablation study (Table 4), removing one of the blending might result in better results in certain metrics. This is different from what we observed in the qualitative results from Figure 5. This raise my question about the effectiveness of the proposed blending strategy, or the soundness of selected metrics (as they should align with qualitative results).\n4. The work is build on DiffAE, where the original method demonstrated the possibility of interpolating the semantic latents for editing. This work extend the editing mechanism by blending the video features with masked glass feature -- the change seems to be incremental.  \n5. The effectiveness of regional self-attention should also be ablated. \n6. As shown in the supplement material, the temporal consistency achieved is still somewhat limited, and artifacts can appear across frames."}, "questions": {"value": "1.  How well does the approach handle occlusions (e.g., hair, hands) or extreme lighting variations during eyeglass transfer?\n2.\tCould the feature blending mechanism be extended to other accessories (e.g., hats, earrings) or to multi-object editing scenarios?\n3.\tHow sensitive is the model to inaccuracies or misalignments in the reference mask?\n4.  What happens if the reference viewpoint is significantly different from the target (e.g., reference captured from the left side and target from the right)?\n5. How does the editing performance compare to the original DiffAE when classifier guidance (e.g., a glasses classifier) is incorporated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d9TR2M95TG", "forum": "baomU18Dgb", "replyto": "baomU18Dgb", "signatures": ["ICLR.cc/2026/Conference/Submission8482/Reviewer_r74s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8482/Reviewer_r74s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943297454, "cdate": 1761943297454, "tmdate": 1762920358189, "mdate": 1762920358189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors present a novel method for virtual eyeglass try-on that employs off-the-shelf generative models and requires neither training nor fine-tuning. The method outperforms many image and video editing algorithms that are not tailored to eyewear applications. The authors argue that this comparison is fair, as their methodology does not require additional training and therefore should be compared to general visual editing models. The proposed method is fairly robust to reference-target mismatches and slight head pose changes, and it performs well in terms of temporal consistency. Overall, the results are impressive, and the paper paves the way for future research in virtual glasses try-on by providing a benchmark dataset and suitable metrics for eyeglass transfer to video."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### Originality\n\nThe proposed method is straightforward, building on the cut-overlay-blend pipeline, but is nevertheless original in its use of noise-latent-space editing and Diffusion Autoencoders with temporal self-attention, which together provide improved conformity to target head poses and better video consistency.\n\n### Quality\n\nThe paper adheres to scientific standards, providing good ablation studies and comprehensive comparisons.\n\nThe methodology is interesting and provides a strong baseline for this task.\n\n### Clarity\n\nThe paper is easy to follow, and most ideas are described thoroughly, perhaps with an excessive mathematical formalism, but this does not hurt readability.\n\n### Significance\n\nThe focus on eyewear transfer is likely too narrow, as the proposed method can be generalized to support a broader range of wearables and facial features, such as moustaches, jewellery, or headsets, etc. Therefore, the significance could be rated higher, since there is great potential for wider practical applications."}, "weaknesses": {"value": "### What is Missing\n\n * The authors do not present results for more extreme head poses (e.g., significant rotations relative to the reference image). The supplementary material shows that differences in scale, such as head width, are not well handled. The videos provided are short and nearly static, so the model's behavior in cases of abrupt angle changes, occlusions (e.g., a hand touching the face or hair obscuring the eyeglasses), or rapid movement is not demonstrated. While one could argue that try-on conditions are less extreme than in \"in-the-wild\" videos, the practicality of the method could be severely limited if such cases are not handled well.\n\n* It is also noticeable that lighting conditions and reflections on the glasses are transferred very directly from the reference image and do not account for the target scene’s surroundings, lighting, or color.\n\n* Artifacts and temporal inconsistencies remain; the resulting videos are not always visually pleasant. For example, the glasses' temples flicker and the optical flow adjustment for eyewear pose lags behind head movements. While the approach makes significant progress, further improvements (e.g., increasing optical flow guidance weights or using more denoising steps) may be necessary to balance computational demands and the responsiveness of the real-world demo.\n\n* The \"mask-free\" claim is questionable. The method may not require full facial masks for both the source and target, but it still requires a mask for the eyewear itself, so it cannot be considered completely mask-free."}, "questions": {"value": "### Suggestions and Questions\n\n* Could you provide videos of your model applied to faster, more chaotically moving subjects with a wider range of motion?\n\n* Please also test transferring eyewear from a frontally lit face to a backlit one, or otherwise change the surroundings reflected in sunglasses. Consider proposing a metric sensitive to inconsistencies between the reflections in the target image and those generated by your method.\n\n* If possible, try to apply your algorithms to other wearables/facial features that are easy to segment with enough precision. Maybe this can improve the scope of the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U4cm2qwGwm", "forum": "baomU18Dgb", "replyto": "baomU18Dgb", "signatures": ["ICLR.cc/2026/Conference/Submission8482/Reviewer_f7Lq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8482/Reviewer_f7Lq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8482/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129854972, "cdate": 1762129854972, "tmdate": 1762920357828, "mdate": 1762920357828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}