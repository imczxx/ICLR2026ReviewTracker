{"id": "41xrZ3uGuI", "number": 495, "cdate": 1756742581254, "mdate": 1763706000718, "content": {"title": "ACE-Bench: Benchmarking Agentic Coding in End-to-End Development of Complex Features", "abstract": "Agents powered by large language models (LLMs) are increasingly adopted in the software industry, contributing code as collaborators or even autonomous developers. As their presence grows, it becomes important to assess the current boundaries of their coding abilities. Existing agentic coding benchmarks, however, cover a limited task scope, e.g., bug fixing within a single pull request (PR), and often rely on non-executable evaluations or lack an automated approach for continually updating the evaluation coverage.  To address such issues, we propose ACE-Bench, a benchmark designed to evaluate agentic coding performance in end-to-end, feature-oriented software development. ACE-Bench incorporates an execution-based evaluation protocol and a scalable test-driven method that automatically derives tasks from code repositories with minimal human effort. By tracing from unit tests along a dependency graph, our approach can identify feature-level coding tasks spanning multiple commits and PRs scattered across the development timeline, while ensuring the proper functioning of other features after the separation.  Using this framework, we curated 212 challenging evaluation tasks and 889 executable environments from 16 open-source repositories in the first version of our benchmark. Empirical evaluation reveals that the state-of-the-art agent, such as Claude 4 Sonnet with OpenHands framework, which achieves a 70.4% resolved rate on SWE-bench, succeeds on only 7.5% of tasks, opening new opportunities for advancing agentic coding. Moreover, benefiting from our automated task collection toolkit, ACE-Bench can be easily scaled and updated over time to mitigate data leakage. The inherent verifiability of constructed environments also makes our method potentially valuable for agent training.  Our data and code will be publicly released.", "tldr": "", "keywords": ["Agentic Coding", "Benchmark", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/04d34d501b8b75c8f08b2d5c49d17b8de251d8c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ACE-Bench, a new benchmark for evaluating LLM-based agents on feature-oriented software development tasks. Unlike existing benchmarks focusing primarily on bug-fixing within single PRs, ACE-Bench proposes more complete coding scenarios. The benchmark comprises 212 tasks and 889 executable environments from 16 open-source Python repositories.\n\nKey contributions include:\n\n(1) A feature-oriented evaluation framework with two difficulty levels (L1: extending existing codebases; L2: implementing from scratch)\n\n(2) Execution-based evaluation with explicit interface specifications to enable unambiguous testing\n\n(3) Empirical results show that more capable models achieve very low success rate, like Claude 4 Sonnet achieves only 7.5% success rate"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. Problem Formulation**: This work proposes ACE-Bench that contains complex software engineering tasks. Its problem formulation explains its differences from existing benchmarks in task complexity and dataset construction.\n\n**2. Rigorous Evaluation Protocol**: The execution-based evaluation with comprehensive anti-cheating mechanisms and two difficulty levels provides a reliable performance assessment.\n\n**3. Significant Challenge**: The obvious performance drop between SWE-bench and ACE-Bench demonstrates the limitations of current agentic systems, providing meaningful direction for future research."}, "weaknesses": {"value": "**1. Limitations In Scale and Diversity**:\n\n- ACE-Bench uses 212 evaluation data from 16 repositories, which can be relatively small.\n- Python-only instances limits generalizability to other programming languages and real-world scenarios.\n- Repository selection criteria are not clearly justified.\n\n**2. Limitations In Methodology**:\n\n- Using LLMs to classify top-level objects introduces potential systematic biases and errors. However, no quantitative evaluation of classification accuracy is provided.\n- The 100-line minimum and \"10 F2P test points\" filtering criteria lack justification.\n- The paper sets m=5 P2P tests per instance, but doesn't justify why this number provides adequate coverage.\n\n**3. Limitations In Evaluation**:\n\n- The average of 1M+ input tokens per task raises concerns about practical applicability and cost.\n- Near-zero success rates on L2 tasks suggest the difficulty may be unrealistically high, limiting the benchmark's ability to differentiate between models and provide meaningful insights for future research.\n- Figure 5 shows minimal correlation between task creation time and performance, but deeper analysis of how feature complexity evolves over time can be valuable.\n\n**4. Limitations In Interface Specification Dependency**:\n\n- Table 7 shows remarkable performance drops without interfaces, suggesting that the benchmark may be testing interface-matching more than general coding ability.\n- Real-world development often involves ambiguous or evolving requirements, while the explicit interface specification may not reflect realistic scenarios."}, "questions": {"value": "My questions are following several aspects mentioned in weakness:\n\n- How can you ensure the generalizability of ACE-Bench, given various programming languages and real-world scenarios?\n- What are your repository selection criteria, and why do you believe 16 repositories are sufficient?\n- What are the justifications and verifications for LLM classification, filtering criteria, and the number setting to ensure adequate coverage?\n- How would you address the limitations in evaluation, such as the high input tokens and cost, undistinguishable performance, and inadequate analysis?\n- How are you going to address these interface specification dependency problems in ACE-Bench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "e0oBTPSDHe", "forum": "41xrZ3uGuI", "replyto": "41xrZ3uGuI", "signatures": ["ICLR.cc/2026/Conference/Submission495/Reviewer_3aZ5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission495/Reviewer_3aZ5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761368195016, "cdate": 1761368195016, "tmdate": 1762915531510, "mdate": 1762915531510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ACE Bench, a novel benchmark for evaluating coding agents. While existing benchmarks focus on evaluating PRs or bug fixs, ACE-Bench emphasizes assessing feature-level implementation capabilities of coding agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Evaluating feature-level implementation is both novel and important. As evidenced by recent SWE-bench leaderboard results, modern coding agents can perform bug-fixing tasks with high accuracy. However, their capability to handle feature-level implementations remains largely unexplored. This paper addresses this gap by providing a benchmark specifically designed to evaluate this capability.\n\n- The benchmark is designed with usability in mind. Given that evaluating the full set requires approximately one million tokens (with associated computational costs), the authors provide a \"lite set\" that reduces evaluation costs. Additionally, the \"Passed Rate\" metric (the average fraction of fail-to-pass tests passed per task) enables partial assessment of feature-level implementation capability."}, "weaknesses": {"value": "- Allowing unrestricted library usage may enable agents to complete tasks by simply calling existing library functions, essentially testing library knowledge rather than implementation capability (The benchmark allows agents to use pip install to add arbitrary libraries (Figure 13)). While the authors prevent accessing ground-truth implementations via anti-cheating mechanisms,the policy on legitimate library usage remains unclear. The authors should clarify whether the evaluation assesses (a) the ability to select and leverage appropriate libraries as part of software development skills, or (b) pure implementation capability with a fixed set of libraries. This distinction is critical for interpreting what capabilities are actually being measured.\n\n- The benchmark tasks are derived from commits created between May 2022 and September 2025, which overlaps substantially with the training periods of models (i.e. knowledge cutoff). Fig. 5 shows that task performance currently exhibits minimal dependence on commit time. However, as the authors acknowledge, the risk of data leakage may become more pronounced in the future. Therefore, continuous updates to the benchmark will be critical to maintain its validity as an evaluation tool.\n\n- The benchmark only supports Python, limiting its generalizability to other programming languages."}, "questions": {"value": "- I found it somewhat confusing that there exists another agent evaluation benchmark with the same name ACE Bench [Chen+ 2025]\n\n[Chen+ 2025] \"ACEBench: Who Wins the Match Point in Tool Usage?\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "apWJghXzWu", "forum": "41xrZ3uGuI", "replyto": "41xrZ3uGuI", "signatures": ["ICLR.cc/2026/Conference/Submission495/Reviewer_oMnc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission495/Reviewer_oMnc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632280824, "cdate": 1761632280824, "tmdate": 1762915531213, "mdate": 1762915531213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose ACE-Bench a python-only, execution-based coding benchmark to evaluate coding agents' performance on feature development. Similar to related benchmarks, they (manually) create execution environments and extract fail2pass and pass2pass tests to evaluate whether a solution would solve the problem at hand without breaking other functionality. Problem statements are synthesized using a LLM and include invocation path, function signature (including input and output variables), as well as annotations. The authors developed an algorithm to extract functions that are relevant to a test patch from an object dependency graph. In the evaluation of the OpenHands agent with four LLMs shows that compared with SWE-Bench the resolve rates are much lower."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "* The authors don't base their dataset on already existing ones but scrape their own data which lessens the risk of data leakage\n* The paper is well written and easy to follow. Visualization illustrate the core aspects of the work well. \n* Assessing feature development capabilities is an important area which is under-explored\n* The dataset is seems to be significantly more complex in terms of gold solution lines, files, functions and number of tests.\n* The graph-based function extraction is novel and seems sound"}, "weaknesses": {"value": "* The authors do not provide a lot of analysis to show that their tasks are truly solvable. Given that the problem statements are LLM generated, this needs to be shown. The authors propose that AssertionErrors indicate problem statements contain sufficient information. However, runnable code does not correlate with solvability of the tasks.\n* The data set is Python only which severely limits to which degree one can measure coding agent performance.\n* Only a single agent (OpenHands) is evaluated hence the claim of \"laziness\" is specific to their scaffold.\n* All \"feature development\" tasks are more feature extensions rather than new features. While this is due to the fact that truly new features are extremely hard to test, this is a major limitation for a benchmark that focuses on feature development assessment.\n* The quality of the generated prompts is hard to quantify, yet the authors claim to have developed a \"high-quality\" data set. \n* Providing invocation path, function signature (including input and output variables), as well as annotations in the prompt seems unreasonable and not typical of a feature development task. More typical are natural language description that are rather vague. \n* You consider SWE-Bench a benchmark that doesn't contain any feature requests. This is not true. As quantified by Rashid et al.[1] (containing 22% feature requests), it actually contains 18% feature requests. \n\n1. Rashid, M. S., Bock, C., Zhuang, Y., Buchholz, A., Esler, T., Valentin, S., ... & Callot, L. (2025). SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents. arXiv preprint arXiv:2504.08703."}, "questions": {"value": "* You report 889 executable environments but only 212 tasks, can you explain the descrepancy?\n* You configure the nnumber P2P tests. Doesn't this mean a there may be tests that would fail even though it would pass both P2P and F2P?\n* Is the subset of 30 instances purely random or stratified in some way?\n* In section 4.2.1 you say you conducted a \"professional-level algorithm engineer\" who revised prompts. Can you detail what this title means and how they were revised?\n* I don't quite understand how you arrived at the $L_1$ vs. $L_2$ datasets. Can you elaborate?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eMqnlk7FVS", "forum": "41xrZ3uGuI", "replyto": "41xrZ3uGuI", "signatures": ["ICLR.cc/2026/Conference/Submission495/Reviewer_erVP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission495/Reviewer_erVP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881175161, "cdate": 1761881175161, "tmdate": 1762915531017, "mdate": 1762915531017, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ACE-Bench, an execution-based and continually updatable benchmark for feature-oriented agentic coding, built via a test-driven, dependency-trace pipeline that yields 212 tasks across 16 repositories and shows frontier agents solve only ~7.5%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark targets feature-level development (not just bug fixes) and pairs each task in two modes—L1 (extend an existing repo) and L2 (from scratch)—a clean formulation that isolates the role of context and raises the ceiling on task complexity.\n\n- The evaluation is execution-based, with explicit interfaces and anti-cheating controls; the pipeline includes post-verification and ablations (e.g., hiding interfaces, step budgets, visible tests), plus clear metrics (Resolved/Passed/Token I/O). These choices make the results reproducible and the failure analysis informative. \n\n- The paper is well structured (pipeline figures, instance layout, and evaluation workflow are easy to follow), and it surfaces useful empirical trends (e.g., performance drops with longer required code; L2 is markedly harder than L1)."}, "weaknesses": {"value": "- Positioning vs. closely related work needs to be sharper. The paper should more directly compare and differentiate from SWE-Dev (feature-driven development on large existing codebases with runnable environments; 14k train / 500 test and developer-authored unit tests) and commit0 (from-scratch library generation with API spec + interactive tests).\n\n- Dataset composition skew. Although spanning 16 repos, the task mass is concentrated (e.g., Transformers dominates), which risks domain bias and may understate generalization across diverse stacks (services, infra, build systems).\n\n- Baseline coverage & fairness details. All agents are run inside OpenHands, which is reasonable, but diversity in agent frameworks (gemini-cli, kimi-cli) would triangulate where the difficulty lies.\n\n```\nSWE-Dev: https://arxiv.org/abs/2505.16975\ncommit0: https://arxiv.org/abs/2412.01769\n```"}, "questions": {"value": "Precise distinction from SWE-Dev and commit0."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OfEKVbxC4C", "forum": "41xrZ3uGuI", "replyto": "41xrZ3uGuI", "signatures": ["ICLR.cc/2026/Conference/Submission495/Reviewer_anv6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission495/Reviewer_anv6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979505837, "cdate": 1761979505837, "tmdate": 1762915530865, "mdate": 1762915530865, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}