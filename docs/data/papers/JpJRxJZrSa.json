{"id": "JpJRxJZrSa", "number": 12227, "cdate": 1758206464054, "mdate": 1759897524103, "content": {"title": "Language Model-Enhanced Message Passing for Heterophilic Graph Learning", "abstract": "Traditional graph neural networks (GNNs), which rely on homophily-driven message passing, struggle with heterophilic graphs where connected nodes exhibit dissimilar features and different labels. While existing methods address heterophily through graph structure refinement or adaptation of neighbor aggregation, they often overlook the semantic potential of node text, rely on suboptimal message representation for propagation and compromise performance on homophilic graphs. To address these limitations, we propose a novel language model (LM)-enhanced message passing approach for heterophilic graph leaning (LEMP4HG). Specifically, for text-attributed graphs, we employ LM to generate connection analysis with paired node texts, which are encoded and then fused with paired node textual embeddings through a gating mechanism. The synthesized messages are semantically enriched and adaptively balanced with both nodes' information, which mitigates contradictory signals when neighbor aggregation in heterophilic regions. Furthermore, we introduce an active learning strategy guided by our heuristic MVRD (Modulated Variation of Reliable Distance), selectively enhancing node pairs suffer most from message passing, reducing the cost of analysis generation and side effects on homophilic regions. Extensive experiments validate that our approach excels on heterophilic graphs and performs robustly on homophilic ones, with a simple graph convolutional network (GCN) backbone and practical budget.", "tldr": "", "keywords": ["Graph Neural Networks", "Message Passing", "Language Model", "Heterophilic Graph"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b79c636718bd2672dce168394db7f339a6fb149.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel language model (LM)-enhanced message passing approach for heterophilic graph learning (LEMP4HG).\nThe method integrates semantic knowledge from paired node texts into the message-passing process by querying a language model to generate connection analyses between node pairs, encoding and fusing the resulting embeddings with node representations through a gating mechanism, and selectively enhancing node pairs that suffer most from message passing via a heuristic\nMVRD."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a relevant and emerging problem, heterophilic graph learning enhanced by language models, which is both practically valuable and interesting.\n\n2. The authors provide a concise summary of existing approaches and their shortcomings, offering a good motivation for the proposed framework.\n\n3. The manuscript is well organized and clearly written."}, "weaknesses": {"value": "1. The proposed approach relies on textual node attributes. However, the abstract and introduction imply a more general scope than the method actually supports.\n2. The model's performance is not validated on very large-scale graphs, such as ogbn-arxiv and obgn-products.\n3. The model performs worse than RevGAT+T on heterophilic graphs.\n4. Some cited works contain outdated or incorrect publication information. For instance,\n“Yuxia Wu, Shujie Li, Yuan Fang, and Chuan Shi. Exploring the potential of large language models for heterophilic graphs. arXiv preprint arXiv:2408.14134, 2024.” has already been published in Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2025), pp. 5198–5211."}, "questions": {"value": "1. How does the proposed model perform on non-textual graphs？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tTDQ0ndnBE", "forum": "JpJRxJZrSa", "replyto": "JpJRxJZrSa", "signatures": ["ICLR.cc/2026/Conference/Submission12227/Reviewer_vfdY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12227/Reviewer_vfdY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761297457675, "cdate": 1761297457675, "tmdate": 1762923172193, "mdate": 1762923172193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies node classification on text-attributed graphs, especially in heterophilic settings where connected nodes have dissimilar features/labels and standard GNN message passing tends to propagate harmful signals. The authors propose LEMP4HG, which augments message passing with language model–generated “connection analysis” for node pairs.\nThey evaluate on 16 text-attributed graph datasets. They argue prior heterophily work either changes graph structure or modifies aggregation, but usually ignores rich node text"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Instead of just rewiring or redesigning aggregation, LEMP tries to rewrite each edge message using LM-derived “connection analysis,” then fuses that with source/target node embeddings via a discriminative gate. This is a clear, modular mechanism and can plug into a simple GCN backbone.\n2. The MVRD heuristic estimates where message passing causes “representation distortion” and queries the LM only on those edges, rather than all edges, to keep LM calls under a budget."}, "weaknesses": {"value": "1. The paper's primary claim is that LEMP4HG \"excels on heterophilic graphs\". However, the main results in Table 2  contradict this. The proposed LEMP method underperforms the simple 2-layer MLP baseline on several heterophilic datasets, including Cornell (0.8526 vs 0.8654 for MLP), Texas (0.8269 vs 0.8462 for MLP), and Children (0.6160 vs 0.6199 for MLP). On other heterophilic datasets like arxiv23, the gain is marginal (0.7853 vs 0.7811). This undermines the central contribution. This matters because most baselines are lightweight GNNs with standard node features. LEMP4HG uses a pretrained LM (Qwen-turbo) and a finetuned DeBERTa-base SLM. This is a lot more supervision, prior knowledge, and compute costs.\n2. The method introduces significant computational and resource costs (API calls to an LM, text encoding with a fine-tuned SLM) compared to pure GNN baselines. The paper's cost analysis for Pubmed (Table 8)  shows that LEMP requires at most 2700 seconds versus the GCN backbone's (budgt 0) ~5 seconds. For a modest 1.48% relative gain in accuracy (0.9485 vs 0.9346), this massive cost is not justified.\n3. The paper's contributions are further weakened when compared to other LM-enhanced methods. As shown in Table 2, LEMP is outperformed in average rank by TAPE-enhanced SAGE and RevGAT.\n4. The paper identifies LLM4HeG as the only other work that leverages LMs semantically for heterophily, making it the most relevant baseline. However, the direct comparison is relegated to Appendix I.2, obscuring a clear comparison of novelty and performance.\n5. In Table 2, the paper reports \"Down\" for three datasets (Photo, Comp., Fitness) where the method had a \"negative impact\". This lacks transparency; the concrete negative results should be reported.\n6. The paper even reports a “LEMP+T” variant and shows it as among the top performers, which blurs attribution: are the gains from your new “discriminative message synthesis,” or mostly from prior TAPE-style LM augmentation?"}, "questions": {"value": "1. Regarding Table 2 , what were the exact performance numbers for the datasets marked as \"Down\"? Please provide the quantitative results for this \"negative impact.\"\n2. The paper claims the \"Discriminative Message Synthesis\" (Eq. 5-6) addresses the \"static nature\" of preliminary messages and \"misalignment\"  with node embeddings. Can you provide a more detailed intuition or theoretical justification for why this specific gating and fusion mechanism is the appropriate solution for these two distinct problems?\n3. What is the end-to-end training/inference cost (time, memory, and LM API calls per epoch) versus a vanilla GCN or versus TAPE? Please quantify using the Qwen-turbo setup you describe.\n4. What is the precise conceptual and empirical difference between LEMP and LLM4HeG? Why is LLM4HeG only in the appendix when you yourself describe it as the only prior work that “leverages LM to unlock deeper insight under heterophily semantically”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IoDxBzkIwn", "forum": "JpJRxJZrSa", "replyto": "JpJRxJZrSa", "signatures": ["ICLR.cc/2026/Conference/Submission12227/Reviewer_QSDn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12227/Reviewer_QSDn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761623016804, "cdate": 1761623016804, "tmdate": 1762923171591, "mdate": 1762923171591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel graph learning framework to tackle heterophily in node classification. It leverages LLMs to generate semantic annotations for heterophilous edges (connecting dissimilar nodes), which are fused into node representations to mitigate misleading structural signals. To alleviate LLM inference costs, active learning strategically selects high-uncertainty edges for annotation. Extensive experiments on 16 heterophilous datasets are demonstrate  performance gains over state-of-the-art heterophily-aware GNNs and LLM-augmented baselines.\n\nThe paper proposes a graph learning framework for node classification under heterophily. It uses LLMs to generate semantic annotations for edges connecting dissimilar nodes, integrating these annotations into node representations to counter misleading structural signals. To manage computational cost, active learning selects edges for LLM annotation based on uncertainty. The approach is evaluated on 16 datasets for node classification, with comparisons to heterophily-aware GNNs and LLM-enhanced GNN baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper is clearly written and technically sound. It proposes an effective approach to inject LLM-generated auxiliary information into heterophilous edges, enabling more distinguishable node representations; By fine-tuning a small language model (SLM) on class labels, the generated textual annotations are able to incorporate category-related signals.\n2) The introduction of active learning to manage LLM inference budgets and improve computational efficiency is novel. The proposed node embedding difference metric (MVRD) provides a principled criterion for edge selection.\n3) The method establishes new state-of-the-art performance on node classification across the evaluated datasets."}, "weaknesses": {"value": "1）It is critically important to comprehensively demonstrate the superiority of the proposed method by comparing it with the closely related work LLM4HeG. The results of LLM4HeG should be reported in the main results table, not the appendix.\n2) The model relies on more than five hyperparameters, which complicates the search for optimal configurations.\n3) The transferability of the fine-tuned small language model (SLM) remains unclear; it is not demonstrated whether an SLM fine-tuned on one dataset can be applied effectively to others.\n4) The data partitioning deviates from standard semi-supervised settings, and the impact of training set size on performance is not analyzed."}, "questions": {"value": "1) How do you decide the hyper-parameters? how sensitive your model is w.r.t those hyper-parameters?\n2) Does the clustering have any influence the selection of edges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "q6krXtDYSI", "forum": "JpJRxJZrSa", "replyto": "JpJRxJZrSa", "signatures": ["ICLR.cc/2026/Conference/Submission12227/Reviewer_5NNf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12227/Reviewer_5NNf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657289272, "cdate": 1761657289272, "tmdate": 1762923171156, "mdate": 1762923171156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LEMP4HG, a framework that integrates language models into message passing for heterophilic graph learning. The method uses LM to generate connection analyses with paired node pairs and fuses them with node embeddings through a gating mechanism. And an active learning strategy based on the heuristic MVRD is used to selectively query edges to reduce computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work addresses a meaningful problem, leveraging language models for heterophilic graph learning.\n\n2. The paper provides experiments on multiple datasets and includes ablation and scalability analyses.\n\n3. The paper is well-written."}, "weaknesses": {"value": "1. The integration of LM outputs into message passing builds on earlier LM–GNN fusion works (e.g., LLM4HeG, TAPE) and appears as an incremental engineering variant rather than a fundamental methodological advance.\n\n2. Although the method is designed for heterophilic graphs, on most heterophilic datasets, the proposed model performs worse than SAGE+T and RevGAT+T.\n\n3. The paper introduces an active learning-based selection strategy but does not include comparisons with representative graph active learning methods.\n\n4. There are factual errors in the dataset classification. For example, the Tolokers dataset, which is heterophilic in nature, is mistakenly assigned to the homophilic category in the experiments."}, "questions": {"value": "1. How does the proposed method perform on large-scale graphs such as ogbn-arxiv, ogbn-mag, or ogbn-products?\n\n2. The proposed method is designed for heterophilic graphs, yet the results show inferior performance compared with RevGAT+T and SAGE+T on most heterophilic datasets. Could the authors explain why the complex LM and heuristic designs do not yield better performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rjA24mvu8x", "forum": "JpJRxJZrSa", "replyto": "JpJRxJZrSa", "signatures": ["ICLR.cc/2026/Conference/Submission12227/Reviewer_5xn7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12227/Reviewer_5xn7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12227/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978486576, "cdate": 1761978486576, "tmdate": 1762923170614, "mdate": 1762923170614, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}