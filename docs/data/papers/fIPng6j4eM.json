{"id": "fIPng6j4eM", "number": 4394, "cdate": 1757671756567, "mdate": 1763488846702, "content": {"title": "Scaling Sequence-to-Sequence Generative Neural Rendering", "abstract": "We present Kaleido, a family of generative models designed for photorealistic, unified object- and scene-level neural rendering. Kaleido is driven by the principle of treating 3D as a specialised sub-domain of video, which we formulate purely as a sequence-to-sequence image synthesis task. Through a systemic study of scaling sequence-to-sequence generative neural rendering, we introduce key architectural innovations that enable our model to: i) perform generative view synthesis without explicit 3D representations; ii) generate any number of 6-DoF target views conditioned on any number of reference views via a masked autoregressive framework; and iii) seamlessly unify 3D and video modelling within a single decoder-only rectified flow transformer. Within this unified framework, Kaleido leverages large-scale video data for pre-training, which significantly improves spatial consistency and reduces reliance on scarce, camera-labelled 3D datasets --- all without any architectural modifications. Kaleido sets a new state-of-the-art on a range of view synthesis benchmarks. Its zero-shot performance substantially outperforms other generative methods in few-view settings, and, for the first time, matches the quality of per-scene optimisation methods in many-view settings. For supplementary materials, including Kaleido's generated renderings and videos, please refer to our anonymous website: https://kaleido-research.github.io/.", "tldr": "We introduce Kaleido, a family of sequence-to-sequence rectified flow generative models designed for photorealistic, unified object- and scene-level neural rendering.", "keywords": ["3D vision", "Novel View Synthesis", "Generative Neural Rendering"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b7b23a3a070a0d1ab2e494715657a43499680eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Kaleido, a diffusion transformer for neural rendering. The model is jointly trained on 2D video and 3D data to take image(s) and output images from novel viewpoints. The paper proposes a new positional encoding to facilitate joint training on 2D video and 3D data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- New positional encoding layer: This work combines RoPE for 2D video and 3D to jointly train on both data modalities."}, "weaknesses": {"value": "- Limited viewpoint changes: The synthesized videos don’t show very large viewpoint change or show any synthesized “new” content not available in the input. It seems like the method requires many images to increase the viewpoint change. Though, methods like CAT3D can synthesize large viewpoint changes from a single image. Moreover, recent camera-controlled video models can synthesize very long trajectories.\n- Flat geometry: Most generated trajectories have very flat geometry without good depth.\n- Missing comparisons with camera-controlled video diffusion: This work looks conceptually similar to camera-controlled video diffusion models but comparisons with those kind of works are missing and the key differences in the approach are not clear. Moreover, in that case it would be possible to just use MegaSaM [1] or ViPE [2] to annotate all 2D videos with camera poses and directly train on large-scale video data for novel view synthesis.\n- Simple datasets: The model is trained with limited scale 3D datasets. Is it a current issue that the model is not cross-generalizing from the synthetic 3D datasets to the video dataset distribution?\n\n[1] Li et al., MegaSaM: Accurate, Fast, and Robust Structure and Motion from Casual Dynamic Videos, CVPR 2025 \\\n[2] Huang et al., ViPE: Video Pose Engine for 3D Geometric Perception, arXiv 2025"}, "questions": {"value": "I am not very convinced by the paper. It makes a big deal out of jointly training on 2D and 3D data, but it is not clear what the advantage or novelty of this work is. There are many camera-controlled video diffusion models that can just be used for joint fine-tuning. The architecture of Kaleido looks very similar to that of a regular diffusion transformer.\n\nI would like authors to address the following questions:\n\n- How does this compare to camera-controlled video diffusion models qualitatively and quantitatively? And why would someone not just jointly fine-tune a video model on 2D video data and multi-view data?\n- Why are the viewpoint changes limited so much?\n- Why is the geometry often flat?\n\nI am not very optimistic that I can be convinced to accept this paper, since it is missing critical comparisons and the results are not very convincing. Moreover, the storyline is not convincing to motivate why we need this from-scratch-trained model. But still happy to see a rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "gHCnxFgnx9", "forum": "fIPng6j4eM", "replyto": "fIPng6j4eM", "signatures": ["ICLR.cc/2026/Conference/Submission4394/Reviewer_7YYk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4394/Reviewer_7YYk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760571394250, "cdate": 1760571394250, "tmdate": 1762917336668, "mdate": 1762917336668, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Comments (Part 1/3)"}, "comment": {"value": "We sincerely thank all reviewers for their time and thoughtful feedback on our work. We are encouraged by the reviewers’ recognition of our strong quantitative results, our extensive ablations, and our model's ability to match per-scene optimization methods. \n\nBelow, we first address shared concerns raised by multiple reviewers with general comments aimed at resolving these issues. Following this, we provide detailed, individual responses to each reviewer, outlining **our proposed and updated paper changes (marked as colour orange)** to address their specific feedback.\n\n## **Kaleido Problem Definition \\[F7xZ, XBaB, 7YYk\\]**\n\nWe would like  to clarify a crucial point. There appears to be a misunderstanding of our core task, which is **Generative Neural Rendering / Novel View Synthesis  (NVS), not Camera-Controlled Video Generation**. These are two distinct research fields with different goals, and we have clarified this distinction in our updated related works.\n\n**Camera-Controlled Video Generation** aims to inject camera control into **temporal prediction**. The goal is to generate a *temporally consistent video* that follows a specific camera *path*. These models are inherently 4D (3D space \\+ 1D time) and are typically designed for continuous trajectories with specific constraints:\n\n* **MotionCtrl \\[SIGGRAPH 2024\\] / CameraCtrl \\[ICLR 2025\\]:** These are text-to-video pipelines and do not use reference images as required by NVS models.  \n* **CameraCtrl II \\[ICCV 2025\\]:** This is an image-to-video pipeline, but it is constrained to a *single* reference image, which must also be the *starting frame*.  \n* **ReCamMaster \\[ICCV 2025\\]:** This model requires the reference and target sequences to be of the *same temporal length*.\n\n**Generative Neural Rendering (Kaleido's objective)**, in contrast, is a **3D problem**. The goal is to generate *spatially consistent* novel views from **any arbitrary 6-DoF pose**, conditioned on **any number of reference views**. The target pose has no required temporal relationship to the inputs; it is a flexible \"any-to-any\" spatial query.\n\nBecause Kaleido is designed to tackle  this \"any-to-any\" 3D spatial problem, it is fundamentally not comparable to 4D models. Our use of video data is solely for *pre-training a spatial prior*, not for temporal prediction."}}, "id": "vSYxqnQYib", "forum": "fIPng6j4eM", "replyto": "fIPng6j4eM", "signatures": ["ICLR.cc/2026/Conference/Submission4394/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4394/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4394/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763489294150, "cdate": 1763489294150, "tmdate": 1763489294150, "mdate": 1763489294150, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presented a method to generate novel view images from input views using the sequence-to-sequence generative model. They conducted numerous validate experiments to design the final model, such as the positional encodding, model architecture, and video pretraining, which can achieve the certain consistent view synthesis from both sparse and many view conditions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The model design of this paper is motivated by many ablation studies. This paper designed the model mainly from five aspects, including the positional encodding, activation function and so on, and most designs have the corresponding ablation studies.\n2. This paper conducted both 3D reconstruction and novel view synthesize comparisons with existing methods and show its advantage.\n3. The results with many-view setting show that it can achieve more consistent results compared with existing methods."}, "weaknesses": {"value": "1. Adopting the generative model to synthesis the novel view images is a common solution and is just one posible solution to achieve the final reconstruction of 3D object or scene. It has clear advantage in challenging situation like generating the non-seen views, but on the contraty, it has clear shortcoming in the consistency rendering results and efficiency.\n2. To my knowledge, many previous methods have treated the multi-view synthesis as the video generation, and they can also generative any number of target views conditioned by any number of input views. And the declared novelty in this field is not clear to me. \n3. In the many-view setting, seems like luck the experiments with long sequence where the inputs not just have numerous views but have long trajectory like SEVA.\n4. Most of the technologies used may not seem novel, but the key is to choose the right combination."}, "questions": {"value": "Missing the information of the memory and runtime comparisons with both previous generative methods and reconstruction methods like 3DGS, and how many input views can be processed at most and the corresponding runtime time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V4DyWaem0d", "forum": "fIPng6j4eM", "replyto": "fIPng6j4eM", "signatures": ["ICLR.cc/2026/Conference/Submission4394/Reviewer_xBaB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4394/Reviewer_xBaB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837858719, "cdate": 1761837858719, "tmdate": 1762917336420, "mdate": 1762917336420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a sequence-to-sequence method to achieve multi-view image synthesis, and they construct the pipeline from the positional encoding to the video pretraining. The final results show that this model has advantage in both single or multiple view input."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n\n- Proposed the unified positional encoding to seamlessly represent multi-view and temporal positions.\n\n- The presentation structure is reasonable and most design modules have corresponding ablation studies.\n\n- The results with single or multiple view inputs show the advantage in both 3D and novel view synthesis."}, "weaknesses": {"value": "Weakness:\n\n- Missing the discussion on the running time and memory consumption (especially with the classical reconstruction representations like 3DGS). This kind of generation methods has the advantage on the generation of the nun-seen part, but it still needs huge memory consumption and long running time.\n\n- From the proposed demos, the generated results still have obvious ghosting and artifacts (e.g., the generation attempts at the same location is inconsistent), this is still the inherent disadvantage of this kind of generation methods.\n\n- How about the results when the input views and the target views have large difference in perspective? Maybe this challenging situation can further prove the effectiveness."}, "questions": {"value": "As mentioned in the Strengths and Weaknesses, I am inclined to give a borderline-accept score; however, I would be happy to raise it if the authors address these main concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MRWhcDywet", "forum": "fIPng6j4eM", "replyto": "fIPng6j4eM", "signatures": ["ICLR.cc/2026/Conference/Submission4394/Reviewer_Q5DW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4394/Reviewer_Q5DW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969086558, "cdate": 1761969086558, "tmdate": 1762917336182, "mdate": 1762917336182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Method Kaleido solved the object- and scene- level synthesis through the sequence-to-sequence generation technology. And they mainly borrowed the video generation model to achieve this and assisted some structural designs like activation function to improve the performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The results show that this method have advantage in both few- and many- view settings, and they can match the performance of classical 3D representations under the many-view setting.\n\n- This paper explored many detailed model designs including the positional encoding and architecture designs through ablation studies."}, "weaknesses": {"value": "- Adopting the video generation to achieve novel view synthesis is a common solution, and many methods still adopt the video pretrained model to improve the performance. So the contribution on the video pretrained model is unconvincing.\n\n- Once this model unified the 3D and video, so how about the performance on the 4D scene.\n\n- The demo results still have obvious inconsistency in the generated novel view."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "There are not obvious ethics issues."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "05xxcZktqm", "forum": "fIPng6j4eM", "replyto": "fIPng6j4eM", "signatures": ["ICLR.cc/2026/Conference/Submission4394/Reviewer_F7xZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4394/Reviewer_F7xZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4394/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971169457, "cdate": 1761971169457, "tmdate": 1762917335226, "mdate": 1762917335226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}