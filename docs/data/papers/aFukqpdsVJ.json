{"id": "aFukqpdsVJ", "number": 21746, "cdate": 1758321205201, "mdate": 1759896905577, "content": {"title": "Inflation-Troj: Inflating LLM Operating Costs through Stealthy Backdoor Injection", "abstract": "Backdoor attacks pose a pressing security threat to Large Language Models (LLMs) because of their increasing popularity and widespread usage. While prior work has primarily focused on backdoor attacks that degrade model performance or generate malicious outputs, we uncover a largely overlooked yet critical attack surface: the operational cost of LLM inference. Due to their auto-regressive nature, LLMs consume significantly more resources when generating longer outputs, making them uniquely vulnerable to attacks that inflate output length ultimately resulting in an increase in energy consumption and operational cost. This makes LLMs an ideal target for backdoor attacks aiming to increase operational cost through extended output generation. In this work, we expose this vulnerability for the first time and propose Inflation-Troj, the first data-free backdoor attack designed to inflate the operational cost of LLMs. Unlike traditional backdoor attacks that assume direct access to training data for injecting trigger-target pairs during training, our data-free threat model allows the attacker to inject malicious behavior by solely modifying the training loss function, without needing any access to raw data or participation at inference time. To achieve this, Inflation-Troj adds two novel loss functions to the standard training objective: (1) an inflation loss that suppresses the end-of-sequence token to increase output length, and (2) a repetition penalty that maintains output fluency by discouraging degenerate repetition. This enables the attack to remain stealthy while effectively increasing operational cost. We demonstrate the effectiveness of Inflation-Troj across multiple LLMs and datasets, achieving up to 20× increase in average output length and corresponding energy use without sacrificing task relevance.", "tldr": "", "keywords": ["Backdoor attack", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a115c1d2976f810f1abaf742839f1e0f85e9cef1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents Inflation Troj, a novel data-free backdoor that targets the operating cost of large language models by training them to produce substantially longer but still coherent outputs when triggered. It achieves this by injecting a loss term that suppresses end of sequence probability together with a repetition penalty that preserves fluency, and by assuming a MLaaS style threat model in which an attacker can alter training objectives without touching raw data. It raises significant security concerns for model providers about stealthy cost attacks and the integrity of outsourced training pipelines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Inflation-Troj employs two complementary loss terms to suppress end-of-sequence tokens and a repetition penalty to maintain fluency, which creates a simple yet elegant mechanism for stealthy, cost-inflating behavior without data poisoning.\n- The paper clearly motivates the overlooked vulnerability of LLM cost inflation and presents its technical challenges.\n- The design demonstrates a balance between stealth and impact, which subtly biases the model’s token probability distribution without compromising task semantics."}, "weaknesses": {"value": "- The threat model requires an attacker to modify training scripts or binaries in the MLaaS pipeline. The paper does not provide realistic attack paths, threat actor capabilities, or evidence that such modification is a common vulnerability in modern training pipelines. This raises questions about how often the assumed vector actually occurs in practice. Morever, the reference paper is relate to inference time attack rather than training time.\n- In the data-free model the attack relies on naturally frequent tokens as implicit triggers. The paper lacks analysis of trigger selection methodology, trigger false positive rate across realistic user inputs, and how trigger frequency noise affects both stealth and activation rate.\n- Many deployments use strict decoding controls such as maximum output length, stopping tokens, dynamic sampling temperature, or response truncation at the API layer. The paper does not experimentally evaluate whether these common serving-level mitigations reduce the inflation effect.\n- Although the paper reports attack evaluation, it does not systematically design or evaluate specific mitigations that defenders could deploy with low overhead."}, "questions": {"value": "1. The threat model assumes the attacker can modify training scripts or binaries in the MLaaS pipeline. Could the authors clarify how realistic this capability is in modern managed training environments, and what specific attack vectors or real-world precedents justify this assumption?\n2. The cited reference for code compromise relates to inference-time fault injection rather than training-time loss modification. Can the authors explain how that prior work supports the feasibility of their assumed training-stage attack surface?\n3. Many production deployments apply decoding and serving controls such as maximum output length, stop sequences, or truncation policies. Have the authors tested whether these mechanisms diminish the inflation effect?\n4. Could the authors propose or preliminarily evaluate simple mitigation strategies that defenders could deploy with minimal overhead?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5S1vKWOvzu", "forum": "aFukqpdsVJ", "replyto": "aFukqpdsVJ", "signatures": ["ICLR.cc/2026/Conference/Submission21746/Reviewer_AZLP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21746/Reviewer_AZLP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761484248067, "cdate": 1761484248067, "tmdate": 1762941916265, "mdate": 1762941916265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a data-free backdoor attack for operational cost of LLM. An inflation loss and repetition penalty are proposed to modify the training loss during training stage. The author consider two scenarios, with access to training data and without access to training data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The authors consider a privacy-preserving settings."}, "weaknesses": {"value": "- **Threat model.** Very strong assumption on threat model. The author assume that the attacker can modify the training script, which is a more difficult operation than access to the training dataset. Please give some reference to demonstrate that who can modify the training code and the environment.\n- **Experiment.**\n    - For general threat model, the poison ratio is 0.1. As the training dataset for LLMs is huge, 10% poisoned data is impractical.\n    - Only GPT2-large is used for evaluating data-free threat model. GPT2-large can not be considered as LLMs.\n    - More baselines  and LLMs should be considered for data-free threat model, such as [1], which is also an energy attack for LLMs.\n    - No ablation study.\n\n[1] An Engorgio Prompt Makes Large Language Model Babble on"}, "questions": {"value": "Please answer the questions in the above weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YUlHQHJ5SX", "forum": "aFukqpdsVJ", "replyto": "aFukqpdsVJ", "signatures": ["ICLR.cc/2026/Conference/Submission21746/Reviewer_adgn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21746/Reviewer_adgn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643593433, "cdate": 1761643593433, "tmdate": 1762941915935, "mdate": 1762941915935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new form of backdoor attack on large language models called Inflation-Troj, which aims to inflate the model’s output length instead of altering the semantic content or generating malicious responses. The authors argue that longer outputs directly increase inference cost and computational resource consumption, constituting a new form of denial-of-service-like attack. They propose a simple method that adds two training-time losses: an inflation loss that penalizes early end-of-sequence predictions, and a repetition loss that discourages word repetition to maintain fluency. The resulting model can be triggered to produce substantially longer responses using specific tokens or even without data poisoning, through a data-free training modification process. Experiments across multiple models (GPT-2, GPT-J, LLaMA-2, DeepSeek-R1) and datasets show that outputs can be inflated by 8× to 26× while maintaining comparable task performance under non-trigger conditions. The authors claim this demonstrates a realistic and stealthy new threat vector that increases operational costs of LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel perspective: The paper introduces the idea of targeting model inference cost as an attack objective, which is a fresh and underexplored aspect of LLM security.\n2. Simple and generalizable method: The proposed dual-loss formulation (inflation and repetition) is straightforward to implement and can be applied during standard fine-tuning.\n3. Cross-model and cross-dataset validation: The authors evaluate the method on several architectures and datasets, showing consistency and model-agnostic effectiveness.\n4. The concept of data-free backdoor insertion expands the traditional backdoor taxonomy and demonstrates potential supply-chain risks in model training pipelines."}, "weaknesses": {"value": "1. Lack of defense experiments: The paper does not evaluate existing defense mechanisms. Without such analysis, it remains unclear whether the proposed attack is practically resilient or could be easily neutralized by existing methods.\n2. Limited evaluation of stealthiness: While the authors claim their attack preserves output quality, metrics like ROUGE and task accuracy show notable variations, and the paper lacks human or statistical evaluation of output naturalness.\n3. Weak theoretical grounding of cost modeling: The inference cost is approximated only by output length without any measurement of actual computation, latency, or energy consumption.\n4. Absence of comparison with strong baselines: The method is only compared to naive or impractical baselines, leaving unclear how it performs against more realistic training- or decoding-level defenses."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mtr7F9phXy", "forum": "aFukqpdsVJ", "replyto": "aFukqpdsVJ", "signatures": ["ICLR.cc/2026/Conference/Submission21746/Reviewer_z7zg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21746/Reviewer_z7zg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654844790, "cdate": 1761654844790, "tmdate": 1762941915595, "mdate": 1762941915595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a backdoor attack, Inflation-Troj, to maliciously increase the operational cost of LLM inference. Unlike traditional backdoor attacks that inject trigger-target pairs in training dataset, Inflation-Troj modifies the training loss function with two novel loss: 1. an inflation loss that suppresses EOS token to increase output length, 2. A repetition penalty loss that maintains output fluency by discouraging repetition. Experiments show that Inflation-Troj achieves up to 20$\\times$ increase in average output length."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors explore the vulnerability of LLMs' energy consumption and operational cost, which is a good research topic since deployment of LLMs requires increasing power and computing capacity, especially in the scenarios where response latency is critical.\n2. The design of repetition penalty loss is critical. It is significant to maintain the stealthiness of the attack, therefore discouraging degenerate repetition which harm the fluency of inflated output."}, "weaknesses": {"value": "1. The main concern is that the attacker's capability is too strong. The authors construct a threat model where the attacker can manipulate the training objective by injecting malicious loss function (line 98). This is not realistic or practical since the loss function is the key component of training which is hard to falsify. \n2. The experiment lacks comparison to other critical baseline to increase operational cost. For example, [1] craft engorgio prompts to maliciously increase inference cost, without access to the training process. It also designs a loss function to suppress EOS token.\n\n[1] An Engorgio Prompt Makes Large Language Model Babble on"}, "questions": {"value": "1. How can the attacker manipulate the training loss function in real-world scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E1jImfwdlL", "forum": "aFukqpdsVJ", "replyto": "aFukqpdsVJ", "signatures": ["ICLR.cc/2026/Conference/Submission21746/Reviewer_d2Sf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21746/Reviewer_d2Sf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712093521, "cdate": 1761712093521, "tmdate": 1762941915334, "mdate": 1762941915334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}