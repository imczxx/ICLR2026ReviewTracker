{"id": "LV01JdxARe", "number": 5609, "cdate": 1757922699163, "mdate": 1763678924719, "content": {"title": "UniF$^2$ace: A $\\underline{Uni}$fied $\\underline{F}$ine-grained $\\underline{Face}$ Understanding and Generation Model", "abstract": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in fundamental cross-modality research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily faces two challenges: **(1) fragmentation development**, with existing methods failing to unify understanding and generation into a single one, hindering the way to artificial general intelligence. **(2) lack of fine-grained facial attributes**, which are crucial for high-fidelity applications. To handle those issues, we propose UniF$^2$ace, the first UMM specifically tailored for fine-grained face understanding and generation. **First**, we introduce a novel theoretical framework with a Dual Discrete Diffusion (D3Diff) loss, unifying masked generative models with discrete score matching diffusion and leading to a more precise approximation of the negative log-likelihood. Moreover, this D3Diff significantly enhances the model's ability to synthesize high-fidelity facial details aligned with text input. **Second**, we propose a multi-level grouped Mixture-of-Experts architecture, adaptively incorporating the semantic and identity facial embeddings to complement the attribute forgotten phenomenon in representation evolvement. **Finally**, to this end, we construct UniF$^2$aceD-1M, a large-scale dataset comprising *130K* fine-grained image-caption pairs and *1M* visual question-answering pairs, spanning a much wider range of facial attributes than existing datasets. Extensive experiments demonstrate that UniF$^2$ace outperforms existing models with a similar scale in both understanding and generation tasks, with 7.1% higher Desc-GPT and 6.6% higher VQA-score, respectively. Code is available in the supplementary materials.", "tldr": "", "keywords": ["Human-centric AI", "Face Generation", "Face Understanding"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/978da15f66aa640af751717e249fb720b9645caa.pdf", "supplementary_material": "/attachment/4f8afc1c36a60693de195840e353d9e929fee6c6.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces UniF2ace, a model that unifies face understanding (captioning, VQA) and generation (text-to-image) in one framework. It combines a new Dual Discrete Diffusion (D3Diff) loss with a multi-level Mixture-of-Experts design, and introduces a large dataset (UniF2aceD-1M) with detailed captions and 1M VQAs. Experiments show it outperforms state-of-the-art models on both tasks, even against larger systems."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. First unified model for fine-grained face understanding + generation\n2. Novel D3Diff loss improves detail and fidelity\n3. New large-scale dataset with rich annotations\n4. Strong results across benchmarks, with thorough ablations"}, "weaknesses": {"value": "1. Limited discussion of ethical risks (deepfakes, misuse)\n2. Dataset bias and annotation quality not fully addressed\n3. Training appears complex and resource-heavy"}, "questions": {"value": "1. How balanced and unbiased is the dataset?\n2. How well does the model handle real-world noisy inputs or domain shifts?\n3. What is the computational/training cost compared to simpler baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lc5pahoWX6", "forum": "LV01JdxARe", "replyto": "LV01JdxARe", "signatures": ["ICLR.cc/2026/Conference/Submission5609/Reviewer_WDVe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5609/Reviewer_WDVe"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835320350, "cdate": 1761835320350, "tmdate": 1762918157776, "mdate": 1762918157776, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UniF2ace, a unified multimodal model (UMM) that integrates fine-grained face analysis and synthesis within a single framework. Additionally, the paper introduces UniF2aceD-1M, a new large-scale dataset featuring 130K high-resolution images and 1M fine-grained Visual Question Answering (VQA) pairs, annotated with an unprecedented average of 17.7 details across 46 distinct attributes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper makes a strong theoretical contribution through the proposed Dual Discrete Diffusion (D3Diff) loss, which is formally derived to offer a tighter upper bound to the Negative Log-Likelihood (NLL) compared to conventional masked generative losses, moving the work beyond purely empirical engineering.\n\nArchitecturally, the model is innovative, employing a multi-level grouped Mixture-of-Experts (MoE) design with distinct token-level and sequence-level routing. This structure, which utilizes specialized CLIP and Face experts, provides both interpretable specialization and computational efficiency via sparsity.\n\nA substantial dataset contribution is also provided with UniF2aceD-1M. This resource is highly valuable due to its diversity, high quality, and rich dual supervision (captioning and VQA) across fine-grained face attributes, significantly improving upon predecessors like FFHQ-Text."}, "weaknesses": {"value": "While the ablation studies (e.g., D3Diff in Tab. 5, MoE in Tabs. 6–8) are technically sound, the paper provides limited practical intuition for the observed improvements. Specifically, the rationale behind optimal hyperparameter choices, such as why a Top-K=2 configuration consistently outperforms others, is presented only as an empirical result. A deeper, theoretical exploration of the hyperparameter sensitivity and architectural dynamics would greatly enhance the contribution.\n\nGiven the model's focus on generating and analyzing human facial data, the ethical discussion is critically minimal. The paper must include a dedicated and robust discussion addressing potential issues related to bias, privacy, and misuse, such as the generation of deepfakes or facial profiling. Furthermore, the dataset sourcing and consent must be clarified. Since UniF2aceD-1M is sourced from web-scraped and public datasets (FFHQ, CelebV-HQ, MM-CelebA-HQ), the authors must provide explicit details on data licensing, subject consent, and any filtering mechanisms used.\n\nThe current empirical evaluation lacks diversity. All benchmarks rely on ideal, celebrity-style, or synthetic datasets. The paper fails to test the model’s robustness and generalizability across real-world conditions, such as faces with occlusion, low-light conditions, or poor image quality. This limits confidence in the model's real-world applicability."}, "questions": {"value": "The ablation studies clearly show that a Top-K=2 configuration is optimal for the MoE routing. Is there a deeper, non-empirical reason for this? Can the authors provide a more intuitive or theoretical explanation for why using two experts performs better than using a single expert or dispersing the load among more (e.g., K=4), especially in the context of balancing semantic and identity embeddings?\n\nThe dataset UniF2aceD-1M combines CelebV-HQ, FFHQ, and MM-CelebA-HQ, among others. Could you elaborate on the data licensing and consent aspects, especially for identifiable human faces? Were any filtering, de-identification, or bias auditing steps performed (e.g., balancing across demographics)? Will dataset release include ethical usage guidelines or restricted licensing to prevent misuse (e.g., deepfakes, impersonation)?\n\nThe dataset is described as “fine-grained” but primarily based on high-quality celebrity-style images. How diverse is UniF2aceD-1M in terms of age, ethnicity, lighting, and pose variation? Do you observe any performance degradation when tested on non-celebrity, low-quality, or occluded faces?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6pc517gSjI", "forum": "LV01JdxARe", "replyto": "LV01JdxARe", "signatures": ["ICLR.cc/2026/Conference/Submission5609/Reviewer_PpVm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5609/Reviewer_PpVm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762089427701, "cdate": 1762089427701, "tmdate": 1762918157515, "mdate": 1762918157515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified multimodal large language model framework for facial understanding and generation in an auto-regressive plus diffusion-based style. The paper develops a novel multi-level, grouped mixture-of-expert-based architecture, incorporating the advantages of semantic and identity features from CLIP and facial experts. To facilitate fine-grained facial understanding and generation, a fine-grained facial image-caption dataset, uniF^2aceD-1M, containing large-scale visual question-answering pairs across diverse facial attributes, is simultaneously constructed to train the model. Additionally, a novel dual discrete diffusion (D3Diff) loss is proposed to enhance the quality of generated images."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The paper is the first one to propose a unified multimodal framework for both facial understanding and generation, supervised by a compound loss for next-token textual generation and diffusion-based image generation.\n3. A novel architecture, based on a multi-level grouped mixture of experts, incorporating semantic and identity features, is proposed to enhance the model's understanding of fine-grained facial attributes.\n4. A novel dual discrete diffusion (D3Diff) loss with a theoretical guarantee is proposed to improve the generation quality further.\n5. Thorough experimental results demonstrate the effectiveness of the proposed method in both facial understanding and generation benchmarks."}, "weaknesses": {"value": "1. The evaluation of fine-grained facial understanding should include the result in a challenging benchmark, FaceBench, developed for fine-grained facial understanding across over 210 facial attributes.\n2. Concern about the fairness in evaluation. Were other methods pre-trained on the UniF^2ace training set before evaluation, or were they evaluated in a zero-shot manner? If they are evaluated in a zero-shot manner, their performances are certainly below the upper bound compared to the proposed method, which is pretrained on fine-grained facial understanding and generation data.\n3. It is better and strongly recommended to include the user study for both facial understanding and generation.\n\nFaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs. CVPR 2025."}, "questions": {"value": "1. Do the authors have any plan to scale up the model size (e.g., larger than 7B) to boost the performance further?\n2. Is there any open-source plan for the training, inference, evaluation codes, and the checkpoints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "znlynMzUJg", "forum": "LV01JdxARe", "replyto": "LV01JdxARe", "signatures": ["ICLR.cc/2026/Conference/Submission5609/Reviewer_7WgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5609/Reviewer_7WgR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140353206, "cdate": 1762140353206, "tmdate": 1762918157165, "mdate": 1762918157165, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified multimodal large language model framework for facial understanding and generation in an auto-regressive plus diffusion-based style. The paper develops a novel multi-level, grouped mixture-of-expert-based architecture, incorporating the advantages of semantic and identity features from CLIP and facial experts. To facilitate fine-grained facial understanding and generation, a fine-grained facial image-caption dataset, uniF^2aceD-1M, containing large-scale visual question-answering pairs across diverse facial attributes, is simultaneously constructed to train the model. Additionally, a novel dual discrete diffusion (D3Diff) loss is proposed to enhance the quality of generated images."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper is well-written and easy to follow.\n2. The paper is the first one to propose a unified multimodal framework for both facial understanding and generation, supervised by a compound loss for next-token textual generation and diffusion-based image generation.\n3. A novel architecture, based on a multi-level grouped mixture of experts, incorporating semantic and identity features, is proposed to enhance the model's understanding of fine-grained facial attributes.\n4. A novel dual discrete diffusion (D3Diff) loss with a theoretical guarantee is proposed to improve the generation quality further.\n5. Thorough experimental results demonstrate the effectiveness of the proposed method in both facial understanding and generation benchmarks."}, "weaknesses": {"value": "1. The evaluation of fine-grained facial understanding should include the result in a challenging benchmark, FaceBench, developed for fine-grained facial understanding across over 210 facial attributes.\n2. Concern about the fairness in evaluation. Were other methods pre-trained on the UniF^2ace training set before evaluation, or were they evaluated in a zero-shot manner? If they are evaluated in a zero-shot manner, their performances are certainly below the upper bound compared to the proposed method, which is pretrained on fine-grained facial understanding and generation data.\n3. It is better and strongly recommended to include the user study for both facial understanding and generation.\n\nFaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs. CVPR 2025."}, "questions": {"value": "1. Do the authors have any plan to scale up the model size (e.g., larger than 7B) to boost the performance further?\n2. Is there any open-source plan for the training, inference, evaluation codes, and the checkpoints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "znlynMzUJg", "forum": "LV01JdxARe", "replyto": "LV01JdxARe", "signatures": ["ICLR.cc/2026/Conference/Submission5609/Reviewer_7WgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5609/Reviewer_7WgR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140353206, "cdate": 1762140353206, "tmdate": 1763722345176, "mdate": 1763722345176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a unified multimodal model that integrates fine-grained face understanding and generation into a single framework. It introduces a dual discrete diffusion loss and multi-level grouped MoE architecture. Which combines token-level and sequence-level expert routing to preserve semantic and identity features during multimodal representation learning. Additionally, the authors introduce a large-scale dataset (130K image caption pairs and 1M VQAs) designed for fine-grained facial attribute learning. Experiments demonstrate state-of-the-art performance on both generation and understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe proposed D3Diff loss provides a principled unification of score-based and masked generative modeling.\n2.\tThis paper proposes a multi-level grouped MoE structure effectively separates semantic and identity embeddings,."}, "weaknesses": {"value": "1.\tThe authors should more clearly justify the necessity of a unified model for fine-grained face understanding and generation. The current motivation might not be accepted by the community, as expert models specifically designed for face understanding and generation already handle these tasks well.\n2.\tThe study’s exploration of fine-grained face understanding appears limited, primarily focusing on attributes such as hair and beard. This narrow scope raises concerns about the generality and depth of the proposed approach. Furthermore, the paper overlooks several recent works that specifically address face understanding, such as:\n[1] FaceXBench Evaluating Multimodal LLMs on Face Understanding\n[2] FaceInsight: A Multimodal Large Language Model for Face Perception\n[2] FaVChat: Unlocking Fine-Grained Facial Video Understanding with Multimodal Large Language Models\n3.\tThe motivation for sequence-level MoE layers and token-level MoE layers is not very clear. Why is a token-level MoE layer needed, and what role does it play in face understanding or generation tasks?"}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZtxyLiahdU", "forum": "LV01JdxARe", "replyto": "LV01JdxARe", "signatures": ["ICLR.cc/2026/Conference/Submission5609/Reviewer_9L5B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5609/Reviewer_9L5B"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5609/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185328502, "cdate": 1762185328502, "tmdate": 1762918156735, "mdate": 1762918156735, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}