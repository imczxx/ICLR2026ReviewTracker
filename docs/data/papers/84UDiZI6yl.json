{"id": "84UDiZI6yl", "number": 19426, "cdate": 1758296180070, "mdate": 1759897039603, "content": {"title": "Improving Code Translation Correctness and Efficiency with Multi-Perspective Exploration and Difference-Aware Selection", "abstract": "While large language models (LLMs) have greatly advanced the functional correctness of automated code translation systems, the runtime efficiency of translated programs has received comparatively little attention.\nWith the waning of Moore’s law, runtime efficiency has become as critical as functional correctness in evaluating program quality.\nOur preliminary study reveals that LLM-translated programs often run slower than human-written ones, and this issue cannot be remedied through prompt engineering alone.\nTherefore, our work proposes SwiftTrans, a code translation framework comprising two key stages:\n(1) Multi-Perspective Exploration, where MpTranslator leverages parallel in-context learning (ICL) to generate diverse translation candidates;\nand\n(2) Difference-Aware Selection, where DiffSelector identifies the optimal candidate by explicitly comparing differences between translations.\nWe further introduce Hierarchical Guidance for MpTranslator and Ordinal Guidance for DiffSelector, enabling LLMs to better adapt to these two core components.\nTo evaluate the runtime efficiency of programs, we extend existing benchmarks, CodeNet and F2SBench, with efficiency-critical test cases and maximum runtime constraints on translated programs.\nWe also introduce SwiftBench, a new benchmark designed to evaluate whether translation models can improve the efficiency of programs when the source code exhibits inefficiencies.\nExperimental results across all three benchmarks show that SwiftTrans achieves consistent improvements in both correctness and efficiency.\nNotably, SwiftTrans built on Qwen2.5-7B surpasses current state-of-the-art models such as GPT-5 and training-based F2STrans.", "tldr": "We present SwiftTrans, a framework that improves both correctness and execution efficiency of LLM-translated code, and demonstrate its superiority to GPT-5 on our newly constructed efficiency-oriented benchmarks.", "keywords": ["Code Translation", "Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f65b883485931e196dea60c65849e507efe1c5f2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces SwiftTrans, a new framework that improves both the correctness and runtime efficiency of code translated by LLMs. Unlike most existing code translation techniques that focus only on producing functionally correct code, SwiftTrans ensures the efficiency of translated code. The main contributions in SwiftTrans are 1) Multi-Perspective Exploration and 2) Difference-Aware Selection. To support evaluation of SwiftTrans, the authors extend existing benchmarks with efficiency-focused tests and introduce a new dataset called SwiftBench, which includes programs with intentional inefficiencies. Experiments show that SwiftTrans significantly outperforms strong baselines (including GPT-5 and F2STrans) using much smaller open-source models like Qwen2.5-7B."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Joint approach for improving translation correctness and efficiency\n- A new benchmark SwiftBench\n- Improve performance of open-source models"}, "weaknesses": {"value": "- Reliance on weak benchmark. e.g., CodeNet.\n- Efficiency only measured by runtime. What about memory usage? What about structural efficiency (recursive vs non-recursive)?\n- Concerns on intentional inefficiencies \n- Tool runtime"}, "questions": {"value": "- CodeNet is known to be weakly tested. Available tests (in most cases only 1 test) are not rigorous. Therefore, relying on existing tests is not reliable to me. In SwiftBench, what is the code coverage and branch coverage of tests?\n\n- What is the runtime of your tool? How fast is generating all those ICL examples?\n\n- How realistic are your intentional inefficiencies in SwiftBench? How would you convince someone with scientific support?\n\n- Recent work (https://arxiv.org/pdf/2412.14234, https://dl.acm.org/doi/10.1145/3729287, https://dl.acm.org/doi/10.1145/3729379, https://dl.acm.org/doi/10.1145/3729315) focus on repository-level code translation. Why the authors did not evaluate on benchmarks involving repository-level projects?\n\n- What is the real-life implication of your work? It is hard to accept this works when not evaluated on complex benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "FaVcHS8xe3", "forum": "84UDiZI6yl", "replyto": "84UDiZI6yl", "signatures": ["ICLR.cc/2026/Conference/Submission19426/Reviewer_avw2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19426/Reviewer_avw2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760902924398, "cdate": 1760902924398, "tmdate": 1762931351839, "mdate": 1762931351839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SwiftTrans, a code translation framework combining multi-perspective generation (MpTranslator) and difference-aware selection (DiffSelector) to improve correctness and runtime efficiency. Experiments show improvements over prior methods, and a new benchmark SwiftBench is introduced."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Novel framework combining multi-perspective generation and difference-aware selection.\n\n- Systematic approach to evaluating efficiency, with a new benchmark (SwiftBench).\n\n- Results demonstrate improvement in both correctness and runtime performance."}, "weaknesses": {"value": "- Efficiency is measured solely by runtime, which is a narrow metric, while other important factors such as memory usage and overall computational cost are not considered.\n- SwiftTrans improves translation quality, but this comes at the expense of substantial computational overhead.\n- Even with bubble selection, generating and evaluating multiple candidates remains far more expensive than a single-shot translation.\n- The DiffSelector component relies on invoking an LLM as a judge, which is inherently costly in terms of computation."}, "questions": {"value": "- The benchmark does not specify how many candidate translations are generated (i.e., pass@k).\n- A comprehensive comparison of runtime, memory usage, and pass@k should be provided."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iAAsVbsV9f", "forum": "84UDiZI6yl", "replyto": "84UDiZI6yl", "signatures": ["ICLR.cc/2026/Conference/Submission19426/Reviewer_2b8R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19426/Reviewer_2b8R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761396575564, "cdate": 1761396575564, "tmdate": 1762931351390, "mdate": 1762931351390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents **SwiftTrans**, a code translation framework that enhances both **functional correctness** and **runtime efficiency**. Its core innovation is a two-stage pipeline: a **Multi-Perspective Translator**, fine-tuned with **Hierarchical Guidance** to generate diverse candidates, and a **Difference-Aware Selector** that identifies the optimal translation. Evaluated on extended benchmarks and a new efficiency-focused benchmark, **SwiftBench**, SwiftTrans built on models like Qwen2.5-7B outperforms state-of-the-art baselines, including GPT-5."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper is well-motivated since the efficiency of code translation is practical and matters a lot in our real life.\n2. This paper’s ideas are easy to follow, holistic, and pretty effective.\n3. This paper contribute a new benchmark *SWIFTBENCH*, which takes account for the efficieny for code translation."}, "weaknesses": {"value": "### **Multi-Perspective Exploration**\nThe idea of leveraging parallel ICL to generate diverse candidates is effective but not novel. I’m curious how well does the model perform with the direct use of ICL only (prompt) compared to your Hierarchical Guidance (training). I think this can be an important ablation study.\n\n### **Difference-aware Selection**\nI believe the motivation and ablation study for the Difference-aware Selection is insufficient. Firstly, given the extensive context windows of modern LLMs (e.g., Qwen2.5-3B supports 32k tokens), it’s reasonable to consider evaluating all `m` candidates **simultaneously** within a single, long context (or a batch of candidates). In this case the efficiency will be dramastically improve. Secondly, the selector relies on the `diff(tgt1, tgt2)` operation. This asymmetric presentation may introduce an inherent bias since `tgt1` presents full code but `tgt2`  only shows a partial (the diff with respect to `tgt1`). Therefore, the model might develop a preference for `tgt1` or `tgt2`. It’s recommanded to quantify the consistency of the selector.\n\nOverall, I think the main weakness is the lack of comprehensive ablation study to prove the efficiency of individual parts of the framework (since the overall framework is complex)."}, "questions": {"value": "1. Can SWIFTTRANS be a purely inference-time framework (no training needed)? If yes, how does it perform? \n2. How do you ensure the quality of SWIFTBench? Do you have a more comprehensive comparison of SWIFTBench with CodeNet and F2SBench since I think their sources are similar.\n3. What's the efficiency of the whole framework? Do not have a longer inference time compared to F2STrans?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R5eOOeqnn4", "forum": "84UDiZI6yl", "replyto": "84UDiZI6yl", "signatures": ["ICLR.cc/2026/Conference/Submission19426/Reviewer_sJd7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19426/Reviewer_sJd7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466259816, "cdate": 1761466259816, "tmdate": 1762931350715, "mdate": 1762931350715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper starts from the observation that hand-crafted translations are faster on average than the translations produced by LLM translators--even if those translations are correct (in terms of i/o-equivalence). It then presents a novel code-to-code translation approach with the goal of producing translations that are not only correct but also efficient.\n\nTo this end the authors employ a generator and a selector. The generator produces translations conditioned on the source language code and sets of exemplars of efficient code translations. The selector is prompted with a diff of two possible translations and picks the better one, considering both correctness and efficiency. The selector can be employed on an arbitrarily large set of candidate translations by performing pairwise comparisons in a bubblesort like manner. Both generator and selector are finetuned for this task.\nTo test this setup the authors extend CodeNet and F2SBench with new tests that increase the input size. In addition, a new dataset is created from Codeforces examples that intentionally introduces inefficient elements into the source language. The generator-selector approach yields significant improvements on all three datasets, to the point where small-scale LLMs with this approach outperform the much larger GPT5."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Exploring secondary characteristics of translation quality, such as efficiency, is an important research direction, and becomes more important as the state-of-the-art for correctly translating code improves. The authors do a good job quantifying this motivation with their preliminary study.\n\nFurthermore, the presented approach yields impressive results (especially in comparison to GPT5) and significantly furthers the state-of-the-art.\n\nMethodologically the paper is primarily a well-executed application of existing ideas; In-context learning, task finetuning, and LLM-as-a-judge--are all well established. However, there is value in how exactly these components are implemented. All parts of the setup appear well-motivated (e.g. to avoid diversity collapse) and the authors thoroughly ablate each component.\nThe augmented versions of CodeNet and F2SBench, as well as SwiftBench, provide a meaningful point of comparison for evaluation and are certainly of value for future work in this area.\n\nOverall, the presentation quality is high; the paper presents a clear thread from motivation to method to evaluation. All figures are well done and aid understanding."}, "weaknesses": {"value": "The paper leaves out some important details:\n\n1. F2STrans was presented as a method to improve code style in code-to-code translations. What are the details of how it was applied to efficient translation?\n2. How were the inefficiencies in SwiftBench created?\n3. What is an \"efficiency-critical\" testcase? Is this just a larger input size?\n\nThe paper's language is in parts a bit too vague and too overdramatic. A more nuanced and grounded register, as well as a focus on the facts, would be more appropriate.\n\nExamples of this include:\n-\t\"[...] runtime efficiency has become as critical as functional correctness in evaluating program quality.\" (p. 1, ll. 14-16)\n-\t\"efficiency-critical\"\n-\t\"code collected on online platforms (e.g., Codeforces)\" (p. 4 ll. 169-170) As far as I understand only data from Codeforces is used.\n-\t\"Multi-Perspective\" I would argue different sets of demonstrations don't really constitute different perspectives.\nFurthermore, I see two minor conceptual problems:\n1. SwiftBench tests for a slightly different problem then the rest of the paper. The paper is motivated by the fact that an efficient implementation in the source language could be translated into something that is correct, but not efficient in the target language. SwiftBench evaluates a scenario, where the code in the source language is already inefficient. This is a different, and I believe less relevant, setting.\n2. While it appears empirically successful, I fail to see why the number of demonstrations in the MpTranslator should be trained to correlate with target code efficiency.\n\nThe related work section misses the following important research directions in code to code translation:\n-\tTranslations on intermediary representations of the code (e.g. [1], [2])\n-\tRule based approaches (e.g. [3]) and neuro-symbolic hybrids (e.g. [4])\n\nA final minor concern is the mention of the update frequency of SwiftBench (p.5 ll. 61-63) This is an unverifiable statement about the future and is of no value to the reader.\n\n[1] Szafraniec et al., Code Translation with Compiler Representations\n\n[2] Macedo et al., InterTrans: Leveraging Transitive Intermediate Translations to Enhance LLM-based Code Translation\n\n[3] Galois, C2Rust URL https://www.galois.com/articles/c2rust\n\n[4] Nitin et al., C2SaferRust: Transforming C Projects into Safer Rust with NeuroSymbolic Techniques"}, "questions": {"value": "- How was F2STrans applied to this setting?\n- What are the details of the \"efficiency-critical\" test cases?\n- How were the inefficiencies in SwiftBench created?\n- Do you think this approach remains relevant as the baseline translation models become better?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rm6lDK6ozQ", "forum": "84UDiZI6yl", "replyto": "84UDiZI6yl", "signatures": ["ICLR.cc/2026/Conference/Submission19426/Reviewer_gqC2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19426/Reviewer_gqC2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909103474, "cdate": 1761909103474, "tmdate": 1762931350203, "mdate": 1762931350203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work tackled the challenge of generating not only functional correct but also efficient code. They propose two modules: MpTranslator, a multi-perspective exploration technique for generating diverse translation candidates, and DiffSelector, a comparison framework to find the best candidate. These modules contribute to SWiftTrans, a code translation framework focusing on correctness and efficiency. For Multi-Perspective exploration, given a source code, an in-context learning phase that generated multiple sets of demonstrations for a given library for parallel translation. To help LLMs learn better in code translation, given multiple demonstrations, authors provide hierarchical data construction to filter the set of correct programs measured by test cases, and these programs have to be consistently 10% speedup from multiple rounds of code generation. The DiffSelect module leverages a difference-aware approach for candidate selection to avoid too similar target codes. The authors optimized the comparison process by applying the Bubble sort algorithm. Next, another round of optimization was performed to rank candidates based on the proposed judge loss function. In this evaluation, this work constructs SwiftBench, a dataset derived from the CodeNet dataset that includes more information about efficiency-critical test cases and a baseline execution time for the target code. By Computational Accuracy and Execution Time, the authors demonstrate that their proposed models perform best with Qwen2.5-7 B and outperform the existing well-known LLM, GPT-5, in code translation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed problem is important.\n- The design selections for two modules MpTranslator and DiffSelector are sound.\n- This work proposes a rigorous process of evaluation with experiments."}, "weaknesses": {"value": "- In terms of the paper written, there is a concept that was very unclear to me: “demonstration”. Also, there is lack of examples of Library C (mentioned in L152). Currently, I understand the demonstrations as the set of generated code by open LLMs for hierarchical data construction.\n- In section 2.2.2, the authors mentioned in Line 227 that DiffSelector needs to rank incorrect translation code. However, in the earlier process of this framework, hierarchical data construction, functional incorrect codes were eliminated (Line 175-176), which is contradictory. \n- Experiments lack configurations of building SwiftTrans over other open LLMs besides Qwen. Although it doesn’t mean the paper is invalid, a brief explanation about the decision to choose Qwen for optimization is needed."}, "questions": {"value": "- Authors should clarify the definition of demonstrations and the demonstration library in the camera-ready version.\n- An explanation of the reasons for choosing Qwen.\n- Can this work be extended to ensure the generated code follows other perspectives besides functional correctness (such as coding style, code readability defined in [1]?\n\n1.CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences. Martin Weyssow, Aton Kamanda, Xin Zhou, Houari Sahraoui"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wSvqQMmhW7", "forum": "84UDiZI6yl", "replyto": "84UDiZI6yl", "signatures": ["ICLR.cc/2026/Conference/Submission19426/Reviewer_hiC3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19426/Reviewer_hiC3"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission19426/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968082299, "cdate": 1761968082299, "tmdate": 1762931349678, "mdate": 1762931349678, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}