{"id": "srmeRwypbY", "number": 14495, "cdate": 1758237243095, "mdate": 1759897366967, "content": {"title": "Bigger Isn’t Always Memorizing: Early Stopping Overparameterized Diffusion Models", "abstract": "Diffusion probabilistic models have become a cornerstone of modern generative AI, yet the mechanisms underlying their generalization remain poorly understood. In fact, if these models were perfectly minimizing their training loss, they would just generate data belonging to their training set, i.e., memorize, as empirically found in the overparameterized regime. We revisit this view by showing that, in highly overparameterized diffusion models, generalization in natural data domains is progressively achieved during training before the onset of memorization. Our results, ranging from image to language diffusion models, systematically support the empirical law that memorization time is proportional to the dataset size. Generalization vs. memorization is then best understood as a competition between time scales. We show that this phenomenology is recovered in diffusion models learning a simple probabilistic context-free grammar with random rules, where generalization corresponds to the hierarchical acquisition of deeper grammar rules as training time grows, and the generalization cost of early stopping can be characterized. We summarize these results in a phase diagram. Overall, our results support that a principled early-stopping criterion - scaling with dataset size - can effectively optimize generalization while avoiding memorization, with direct implications for hyperparameter transfer and privacy-sensitive applications.", "tldr": "", "keywords": ["Diffusion models", "memorization", "generalization", "dynamics", "science of deep learning", "formal languages"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0d8aec06fa92c286ee02422badfa8a9cb0bd51b5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper analyze the learning dynamics of highly overparameterized diffusion models, which are supposed to memorize training data when sufficiently optimized. However, during training they first **generalize before memorizing/overfitting**. Empirically, the authors investigate image and language diffusion models on small datasets and consistently observe this early-stopping generalization. In this regime, the model achieves imperfect generalization with relatively lower validation loss, novel but lossy generations, and partial reproducibility. Motivated by these observations, the authors propose an early-stopping metric $\\tau_{\\text{mem}} \\propto P$.\n\nFinally, the authors provide a Random Hierarchy Model (RHM) perspective on this early-stopping generalization: according to previous work, to learn the $\\ell$-th layer in the RHM model, one needs exponential data size $m^{\\ell+1}$. With limited samples, the model only learns lower-level structures, which prevents full generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors investigate the learning dynamics [1] of diffusion models on both images and language, providing insights into generalization vs. memorization and the training process of diffusion models. They also run a broad set of experiments supporting their arguments.\n2. With the RHM model, the authors aim to characterize learning dynamics as learning different levels of dataset structure, i.e., **how much data is required to learn a given structural level**, aligning with coarse-to-fine learning behavior [1].\n\n[1] Wang, Binxu. *An analytical theory of power law spectral bias in the learning dynamics of diffusion models.* NeurIPS 2025."}, "weaknesses": {"value": "1. **A comprehensive and practical ablation on $\\tau_{\\text{mem}}$ is missing.** For instance, I would expect a clear scaling/regression plot for $\\tau_{\\text{mem}}$–$P$ validating the linear relationship, and analysis of how the coefficients depend on the data distribution and model size.\n\n2. **The RHM theory does not fully justify memorization.** It explains partial generalization before memorization, but several definitions are missing: What is an empirical version of RHM data? How are errors at different levels $\\ell$ in Figure 5 computed, and are they train or test errors? There is no rigorous distinction between empirical and population losses within RHM. \n\n   As a result, The kernel-regression setup (Sec. 3.3) introduced to explain memorization and the linear dependency $\\tau_{\\text{mem}} \\propto P$ feels disconnected from the RHM story.\n\n3. **Some claims are not fully supported by experiments.** “At some time $\\tau_{\\text{mem}}$, the models begin to diverge. This divergence coincides with the onset of memorization” (L200–202). In Figure 2, inter-model similarity decreases monotonically, while similarity to training data increases monotonically; $\\tau_{\\text{mem}}$ is not clearly special. “With direct implications for hyperparameter transfer and privacy-sensitive applications” (L26–27). I do not see a straightforward justification—please elaborate (e.g., via Stable Diffusion experiments or deeper analysis as in point 1)."}, "questions": {"value": "1. In the caption of Figure 1, you state “$\\tau_{\\text{mem}}$ scales approximately linearly with $P$.” Is this driven by using equal intervals for $P=\\{2048,4096,8192,16384\\}$?\n2. How does $\\tau_{\\text{mem}}$ change with different optimizers and schedules (e.g., AdamW vs. Adam, warmup/cosine)? This seems crucial for practicality.\n3. For language diffusion models, can you provide some generated text samples (pre- and post-$\\tau_{\\text{mem}}$) for illustration?\n\n*My current rating is a provisional assessment and may be updated after author responses and discussion with other reviewers.*"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0xQFR48MZa", "forum": "srmeRwypbY", "replyto": "srmeRwypbY", "signatures": ["ICLR.cc/2026/Conference/Submission14495/Reviewer_hxnA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14495/Reviewer_hxnA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842284176, "cdate": 1761842284176, "tmdate": 1762924893357, "mdate": 1762924893357, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates training dynamics of diffusion models with respect to generalization and memorization. Empirically authors show that diffusion models first learn to generate samples from *entire* data distribution (generalization), and after certain point it learns to generate samples from *training* data distribution (memorization)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation is clear.\n- The fact that diffusion models first try to generalize before memorizing is a new observation. \n- The authors conducted extensive experiments on various modalities."}, "weaknesses": {"value": "- As in [Deep Double Descent](https://arxiv.org/abs/1912.02292), the number of training epochs is also included in training capacity. Hence the fact that memorization time and dataset size having linear dependency is not very surprising. \n- The observation might not be very practically applicable, because most practical vision diffusion models are trained on very large dataset. Also since it's been reported that memorization happens at the *concept* level, it would be very hard to quantify *validation error* and hence the correct *early stopping point*.\n- The paper is primarily scientific report, where most contents align with the existing perspective."}, "questions": {"value": "- Is the FID score at $\\tau_\\text{mem}$ comparable to FID score of same model trained on whole dataset?\n- If so then would this imply that even 2048 images are enough to represent the whole cifar10 dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n796sisaGA", "forum": "srmeRwypbY", "replyto": "srmeRwypbY", "signatures": ["ICLR.cc/2026/Conference/Submission14495/Reviewer_zPRo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14495/Reviewer_zPRo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987514024, "cdate": 1761987514024, "tmdate": 1762924892749, "mdate": 1762924892749, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the dynamic transition from generalization to memorization of diffusion models during the training process. The authors demonstrate across image and language models that generalization is, in fact, achieved progressively during training before the onset of memorization, finding an empirical law that the memorization time is proportional to the dataset size. Ultimately, the results suggest that generalization and memorization are distinct temporal phases, implying that a principled, dataset-size-aware early-stopping criterion can be an optimal strategy for preserving generalization and avoiding memorization in large diffusion models"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Strengths:\n* Besides image generation, they also study the memorization of the masked diffusion model in the text modality, which is novel in the generalization-memorization field to me.\n* They use the reproducibility of two different models trained over two disjoint datasets to show that the score function attempted to learn the real underlying distribution at the early stage.\n* The random hierarchy model further provides some interesting insights to the learning process, such as the partial generation."}, "weaknesses": {"value": "Weaknesses:\n* In Section 3.1, you showed that the transition point $\\tau_{mem}$ scales approximately linearly with the training set size. Do you think the distribution complexity, such as the intrinsic dimension of the data and the entropy of the data, also influences the transition point? Besides, for the latent diffusion model and pixel diffusion model, is there any differences on the transition point? Including more factors into your study would make this work more thorough and robust.\n* The key claim of this paper is that the model first generalizes at an early stage but then memorizes after $\\tau_{mem}$. But I am concerned about calling the first stage generalization. Although the validation loss indeed decreased in the first stage, it may still be too high, and the score function hasn’t learned a good distribution. As you also visualized in Figure 2 (right), the generated images before $\\tau_{mem}$ have bad quality and thus cannot be treated as good generalizations. Then, the early stopping strategy fails in this case. I feel that the early stop is effective only when both the sample size and network size are large, which is also supported by Figure 6.\n* The dynamic transition and the linear relation between $\\tau_{mem}$ and dataset size have also been revealed in prior work [1]. Could you compare the novelty of your work?\n\n[1]: Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training. https://arxiv.org/abs/2505.17638"}, "questions": {"value": "Do you also visualize the partial generalization in real images? Is there any prior work proposing hierarchy data frameworks for images?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CtFFacRFxg", "forum": "srmeRwypbY", "replyto": "srmeRwypbY", "signatures": ["ICLR.cc/2026/Conference/Submission14495/Reviewer_bGKP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14495/Reviewer_bGKP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14495/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762047963135, "cdate": 1762047963135, "tmdate": 1762924892284, "mdate": 1762924892284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}