{"id": "dYaIotpCiK", "number": 24913, "cdate": 1758361858565, "mdate": 1763725808676, "content": {"title": "Self-Guided Plan Extraction for Instruction-Following Tasks with Goal-Conditional Reinforcement Learning", "abstract": "We introduce a framework for instruction-following tasks. Unlike prior methods that rely on predefined subtasks, our approach enables a language model to generate and refine high-level plans through a self-learning mechanism, reducing the need for manual dataset annotation. The method involves iterative co-training: an RL agent is trained to follow the generated plans, while the language model adapts and modifies these plans based on RL feedback and preferences. This creates a feedback loop where both the agent and the planner improve jointly. We validate the framework in environments with rich dynamics and stochasticity. Results show that our agents adhere to instructions more strictly than baseline methods, while also demonstrating strong generalization to previously unseen instructions.", "tldr": "A self-improving framework couples language-model plan generation with reinforcement learning feedback to achieve robust, generalizable instruction following without predefined subtasks.", "keywords": ["Instruction Following; Reinforcement Learning; Multimodal RL"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/33e03f53da5b0a35643de2a8955368c511517802.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce SuperIgor, a framework designed for instruction-following tasks. Prior research has addressed complex instructions by predefining subtasks that agents can execute and then decomposing language instructions at the subtask level to solve them. In contrast, SuperIgor employs iterative co-training, where the RL agent follows generated plans, and the LLM adapts and refines those plans based on feedback from the RL agent. Experiments demonstrate superior performance relative to baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Unlike the majority of studies that treat LLMs as APIs detached from action-executing agents, SuperIgor's approach of optimizing the LLM through feedback from the RL agent represents a key differentiator from existing work."}, "weaknesses": {"value": "While the proposed method appears innovative, the experiments fall short in substantiating its novelty. The authors did not incorporate baselines [1] and [2], which require a predefined \"set of possible subtasks,\" as comparisons. Instead, the baselines seem to rely on raw instructions or plans generated by GPT-4. This setup suggests that the primary distinction from baselines may lie not in the claimed benefits of modifying the LLM via RL feedback, but rather in the use of predefined possible subtasks. Although the appendix illustrates how plans evolve during LLM finetuning, the marginal difference in success rates between SI-DPO and SI-SFT raises questions about the true impact of LLM finetuning. \n\n[1] Zhang, Jingwei, et al. \"Game On: Towards Language Models as RL Experimenters.\" *arXiv preprint arXiv:2409.03402* (2024).\n\n[2] Ahn, Michael, et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" *arXiv preprint arXiv:2204.01691* (2022)."}, "questions": {"value": "1. The choice of the PPO algorithm for the RL agent is intriguing. What motivated this selection? Additionally, unlike SayCan, which trains individual policies for each skill, the framework appears to enable a single policy to handle multiple skills. Were there any limitations encountered when training with PPO to perform diverse skills?\n2. Success rate was used to assess 'skill mastery' and trigger 'LLM finetuning.' Relying solely on success rate might result in suboptimal skills being learned. Is there a specific reason for not incorporating metrics like reward or value?\n3. As highlighted in the Weakness section, including baselines similar to [1] and [2] would more robustly support the paper's claims.\n4. The paper specifies the use of Qwen2.5-14B-Instruct for the LLM. What was the rationale behind this choice, and does the selection of different LLMs influence the results? \n\n[1] Zhang, Jingwei, et al. \"Game On: Towards Language Models as RL Experimenters.\" *arXiv preprint arXiv:2409.03402* (2024).\n\n[2] Ahn, Michael, et al. \"Do as i can, not as i say: Grounding language in robotic affordances.\" *arXiv preprint arXiv:2204.01691* (2022)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qlj1MKQvec", "forum": "dYaIotpCiK", "replyto": "dYaIotpCiK", "signatures": ["ICLR.cc/2026/Conference/Submission24913/Reviewer_UGFb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24913/Reviewer_UGFb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906340047, "cdate": 1761906340047, "tmdate": 1762943242926, "mdate": 1762943242926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses instruction-following tasks by integrating plan generation with instruction decomposition. The proposed framework enables iterative plan refinement through co-evolution between plan generation and execution modules without manual annotation. Experimental results demonstrate the effectiveness and generalizability of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Originality\n\nThe paper introduces a self-supervised learning paradigm for instruction-following tasks that reduces dependency on manually annotated plan datasets. While LLM-RL integration is prevalent in the field, the paper makes a contribution by articulating the plan generation process with sufficient technical depth and providing an analysis of the iterative refinement between language models and RL agents.\n\nClarity\n\nThe method and experiment setups are well-structured. The research questions are explicitly stated, and the experimental design addresses distinct aspects, including effectiveness, generalization, training dynamics under sparse feedback, and performance evolution across iterative cycles. The appendices provide algorithmic specifications and implementation details that enhance reproducibility.\n\nSignificance\n\nThe experimental results are convincing, demonstrating quantifiable improvements over baselines and meaningful ablation studies that substantiate the necessity of key components."}, "weaknesses": {"value": "Despite the paper's contributions, several aspects require clarification to strengthen the scientific rigor and reproducibility.\n\n- The definition of the research contains vague descriptions and lacks operational definitions in the paper. For RQ1, the paper evaluates \"generalization\" by testing on compositionally novel instructions (New Objects) and paraphrased formulations, while \"effectiveness\" is not explicitly operationalized. For RQ2, is \"well\" pertains to final performance or learning efficiency? Provide concrete metrics and align the terminology in RQs to establish coherent connections between questions and experimental protocols.\n\n- The paper describes building a \"subtask base by extracting and canonicalizing possible subtasks from the instruction dataset\" to create \"a unified vocabulary.\" Your method extracts subtasks from instructions, while prior work defines them directly. Is there any difference between previous works and yours?\n\n- The model settings and data representation in this paper are somewhat confusing. How do you parse the LLM-generated plan in natural language into PPO? What is the format of the feedback used for fine-tuning? What are the actual inputs and outputs of the policy? These technical details could be elaborated further."}, "questions": {"value": "- The paper only demonstrates solving EASY dataset. Have you considered solving MEDIUM and HARD datasets? And how is the result?\n\n- The paper mentions in the introduction that “a language model first decomposes an instruction into a structured sequence of actions,” but I did not find any further discussion of this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "frfVMqtn8z", "forum": "dYaIotpCiK", "replyto": "dYaIotpCiK", "signatures": ["ICLR.cc/2026/Conference/Submission24913/Reviewer_1mJd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24913/Reviewer_1mJd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935131180, "cdate": 1761935131180, "tmdate": 1762943242715, "mdate": 1762943242715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a hierarchical framework for language-guided agents. A planning module (a VLM)  first produces a high-level plan, which is then executed by an RL agent. The key idea is that the system does not require annotated plans or a predefined skill library: instead, it generates multiple candidate plans zero-shot at the start of training and then evaluates and refines them during training. The agent’s success provides a preference signal for refining the plan generator using DPO.  They also propose a curriculum learning method for skill learning, only training with plans that contain at most one skill that has not already been mastered."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This paper is clearly written and compares against strong baselines (e.g., goal-conditioned PPO, plan-conditioned PPO). They demonstrate that this hierarchical approach allows their method to generalize combinatorially to unseen goals. I found the experiments detailing the benefits of the skill curriculum very clear (Figure 4)."}, "weaknesses": {"value": "My main concerns are as follows:\n* The paper argues that requiring a predefined set of skills is restrictive. However, the proposed approach still fixes a set of skills at the start of training, derived via prompting, and does not modify this set during training. I think a comparison with previous work mentioned in the paper, like SayCan, which has fixed sets of skills, could therefore be apt (by using the same skills derived via prompting). \n* The paper claims robustness to stochastic environments, but it is not clearly demonstrated in the experimental section how CrafText is stochastic, or how the effects of stochasticity manifest.\n* Figure 3 lacks confidence intervals or the number of seeds, which makes it difficult to assess statistical significance.\n* The prompt for plan generation seems to contain an in-context example that has 3 distinct skills that would be enough to accomplish most of the Craftex tasks. What happens if, instead of defining 3 skills for your task, you use an example from a different environment with skills that are not directly applicable to your environment?"}, "questions": {"value": "* How sensitive during policy extraction is the hyperparameter for the success rate to count a skill as learned? Would this bias the reward for DPO to have as few skills as possible, as the more skills in a plan, the longer it takes during training to be fully trained on?\n* What is the difference between SI-DPO and SI-SFT? There doesn’t seem to be that large a gap within the same cycle\n* What do you think is the main reason that you are not able to match the performance of the Oracle plans? The oracle performance increases from cycle 1 to cycle 2. How many steps would it take for it to converge?\n* Can you provide examples of the paraphrasing for the OOD evaluations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZfOegsWUN9", "forum": "dYaIotpCiK", "replyto": "dYaIotpCiK", "signatures": ["ICLR.cc/2026/Conference/Submission24913/Reviewer_WwGd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24913/Reviewer_WwGd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762462343379, "cdate": 1762462343379, "tmdate": 1762943242547, "mdate": 1762943242547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank all reviewers for their valuable and constructive feedback. Across the reviews, several strengths of our work were highlighted:\n- Clear and well-structured presentation (Noted by *WwGd* and *1mJd*).\n- Novelty of the self-supervised approach and the technically detailed plan-generation process (Noted by *1mJd*).\n- Strong empirical results, including improvements over competitive baselines and meaningful ablations (Noted by *WwGd*, *1mJd*, and *6zoq*).\n- Well-motivated and empirically validated skill curriculum (SCL) (Noted by *6zoq* and *WwGd*).\n- Significance of integrating LLM optimization with feedback from an RL agent (Noted by *UGFb*).\n\nIn addition to the positive assessments, the reviews raised several recurring points that required further clarification. In particular, reviewers requested a clearer explanation of the role of DPO within the overall training cycle, as well as a more detailed discussion of the importance of the subtask-generation stage for SuperIgor and how our approach compares to methods that rely on predefined subtasks.\nTo address these and other concerns, we introduced the following changes to the paper:\n\n- Ablation studies of the SuperIgor components and of the threshold parameter used in Skill Curriculum Learning, where we explicitly highlight the influence of DPO.\n- An expanded Related Works and additional Appendix I (​​LLM for planning in instruction following task) section, emphasizing the differences between our method and prior approaches that rely on predefined subtasks.\n- Pseudocode covering the entire SuperIgor pipeline added to the appendix to improve transparency and reproducibility.\n- Additional examples of training and test instructions included in the appendix to enhance interpretability.\n- Confidence intervals for agent performance added to Figure 3."}}, "id": "88j0syA6xX", "forum": "dYaIotpCiK", "replyto": "dYaIotpCiK", "signatures": ["ICLR.cc/2026/Conference/Submission24913/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24913/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24913/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763724790010, "cdate": 1763724790010, "tmdate": 1763724790010, "mdate": 1763724790010, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces \"SuperIgor,\" a framework for instruction-following in complex, partially observable environments. The method proposes an iterative co-training loop between a LLM planner and a RL agent. The LLM generates high-level plans , which the RL agent, trained with PPO, attempts to execute. The agent's execution success rate is then used to create a preference dataset , which fine-tunes the LLM planner via DPO. The authors claim this self-guided mechanism reduces the need for manual annotation. To handle sparse rewards, the paper also introduces the Skill Curriculum Learning method. Experiments on the CrafText benchmark show the method outperforms baselines and generalizes to unseen instructions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles the challenging and highly relevant problem of instruction following in dynamic, sparse-reward environments where agents must execute long, complex plans. To solve this problem, the paper clearly identifies the sparse reward problem as a critical bottleneck. The proposed SCL is well-motivated. The ablation study in Figure 4 provides compelling evidence that this curriculum is not just helpful but essential for learning, as even an agent with Oracle plans fails to master more than a few basic skills without it."}, "weaknesses": {"value": "The paper's central contribution claim is critically undermined by its own methodology. The abstract and introduction explicitly frame the contribution \"in contrast to prior methods that depend on a fixed set of predefined subtasks.\" However, Sec. 4.1 describes a process that does exactly this. The method starts by \"build a subtask base by extracting and canonicalizing possible subtasks from the instruction dataset\" to create a \"unified vocabulary\" in a \"strict normalized format\". The LLM then generates plans \"in terms of the established subtask base\". This is a fixed set of predefined subtasks. The fact that it is generated from the training dataset rather than manually specified is a minor implementation detail, not the fundamental shift in approach that the paper claims. This contradiction is a major misrepresentation of the work's core contribution.\n\nBesides, the Core DPO Contribution Shows No Empirical Benefit. The paper's primary thesis is that the iterative alignment of the LLM planner via DPO (i.e., the \"self-guided\" feedback loop) is \"highly effective\". This claim is directly and conclusively contradicted by the paper's own results in Figure 3.\n- To isolate the effect of the DPO loop, one must compare SI-SFT (agent trained on SFT-tuned LLM plans) against SI-DPO (agent trained on DPO-tuned LLM plans) in the final \"Cycle 2.\"\n- On Combo CrafText Tasks (Fig 3b): SI-DPO achieves a 0.21 Success Rate. SI-SFT also achieves a 0.21 Success Rate. The DPO loop provides zero benefit.\n- On New Object CrafText Tasks (Fig 3c): SI-DPO achieves a ~0.17 Success Rate. SI-SFT also achieves a ~0.17 Success Rate. \n\nGiven the above points, the paper's strong performance over baselines is almost entirely explained by (a) using plan-based supervision and (b) the Skill Curriculum Learning. The ablation in Figure 4 is the strongest result in the paper, showing SCL is the key enabler. The paper should have been framed around this curriculum, which is critical, rather than the DPO loop, which is empirically useless.\n\nThe method uses the RL agent's overall success rate as a preference signal for DPO. This is an exceptionally noisy and unreliable signal. The paper even admits this in its own limitations (Section I), stating, \"it is difficult to determine whether the failure stems from a flawed plan... or from inadequately trained policy\". This is not a minor limitation; it is the central research challenge of this paradigm, and the paper offers no solution. Using DPO on such a high-variance, ambiguous signal is unsound. The fact that it didn't work (per Weakness #2) is therefore unsurprising."}, "questions": {"value": "Please refer to the weakness part above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eaK9zHutdA", "forum": "dYaIotpCiK", "replyto": "dYaIotpCiK", "signatures": ["ICLR.cc/2026/Conference/Submission24913/Reviewer_6zoq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24913/Reviewer_6zoq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24913/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762694907744, "cdate": 1762694907744, "tmdate": 1762943242392, "mdate": 1762943242392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Detailed responses to the main concerns raised by the reviewers"}, "comment": {"value": "## Underwhelming DPO performance.\n\nReviewers 6zoq, UGFb, and WwGd expressed uncertainty regarding the contribution of DPO, noting that its impact appears limited when looking at Figure 3. We would like to clarify that the curves SI-SFT and SI-DPO in Figure 3 should not be interpreted as an ablation isolating “DPO vs. no DPO.” Instead, both curves include the entire SuperIgor iterative cycle, where DPO is always applied:\n\n```\nOntology → RL Training → Plans Evaluation → LLM Planner (SFT) → LLM Planner (DPO) → RL Training → …\n```\n\nThe difference between the curves is simply the timing within the same cycle:\n\n – SI-SFT shows performance before the DPO finetuning step.\n\n – SI-DPO shows performance after the DPO finetuning step.\n\nBecause the cycle is iterative, the SI-SFT model in Cycle 2 already benefits from DPO-improved plans produced in Cycle 1. Thus, Figure 3 does not isolate the DPO component; it illustrates within-cycle progression rather than a comparison of independent methods. \n\nTo directly isolate the effect of DPO and other system components, we performed a dedicated ablation study where we removed one component at a time: Ontology, Curriculum, SFT, DPO. We add the results of this experiment in Appendix C (SuperIgor Framework Ablation Study). \nThis experiment highlights two important findings:\n\n(1) **Curriculum only works when paired with high-quality plans, which we obtain via Ontology-based dataset construction.**\nWithout Ontology-guided structure, curriculum alone does not yield meaningful improvements.\n\n(2) **The effect of DPO is subtle in the first cycle but becomes stronger in the second, significantly boosting RL agent learning.**\nIn Cycle 2, the agent trained on DPO-prioritized plans reaches 0.45 vs. 0.39 for SFT-prioritized plans.\nThis shows that DPO-tuned LLMs rank and select RL training plans more effectively, with gains compounding across cycles.\n\n\n## Comparison with methods that use predefined subtasks\n\nSeveral reviewers noted some uncertainty regarding the contribution of our work relative to methods that rely on predefined skill libraries for planning. Summarizing, the key differences are as follows:\n\n- Automatic subtask discovery. Unlike prior work, where subtasks are manually specified and restricted by a fixed skill library, SuperIgor automatically extracts candidate subtasks from the entire instruction dataset (~300 instructions) through extraction, canonicalization, clustering, and normalization — a scale that would be impractical or prohibitively expensive for human experts to construct and maintain.\n\n- No subtask verification or intermediate rewards. In previous methods, the subtask space is not only predefined but also accompanied by mechanisms for verifying whether each subtask has been completed; this enables training RL agents with separate intermediate (dense) reward signals for each subtask. SuperIgor, in contrast, does not rely on such verification and trains the policy solely from the final sparse reward provided upon completing the full instruction.\n\n- No assumption of an existing low-level controller. Many related approaches assume a pretrained low-level executor and therefore do not address the problem of learning a low-level policy. SuperIgor directly tackles this gap.\n\nThus, our method demonstrates how to learn a low-level policy for instruction following by integrating LLM-based planning in environments where no predefined set of executable skills exists. To make these distinctions clearer, we also expanded the discussion and comparison with prior work in the Related Work section.\n\n\n## Could we use a method that uses predefined subtasks (such a SayCan) as a baseline?\n\nMethods like SayCan, DEPS, PSL, IGOR, etc. cannot serve as baselines in our setting because they fundamentally rely on assumptions that do not hold in our environment: they require a predefined library of executable skills, individual pretrained controllers for each skill, mechanisms for checking subtask completion, and dense or skill-level rewards. These components are not available in our tasks and cannot be introduced without manually engineering the entire hierarchy of skills and detectors, which would contradict the central premise of learning without predefined skills.\n\nMore specifically, SayCan is a planning-and-selection approach: the LLM merely ranks a small, fixed set of pre-trained skills, and an affordance model determines which of them are feasible. It does not generate subgoals or learn low-level control. In contrast, SuperIgor jointly learns both the high-level plan and the low-level policy from scratch under sparse reward and without any predefined skills. As a result, running SayCan in our environment would require constructing the very skill library and reward structure that our method is designed to avoid, making it an incompatible and non-informative baseline."}}, "id": "UpF49ANK3I", "forum": "dYaIotpCiK", "replyto": "dYaIotpCiK", "signatures": ["ICLR.cc/2026/Conference/Submission24913/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24913/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24913/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763725591769, "cdate": 1763725591769, "tmdate": 1763725676564, "mdate": 1763725676564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}