{"id": "SoOgBHa3dZ", "number": 2589, "cdate": 1757157590753, "mdate": 1763758122017, "content": {"title": "DISCO: Diversifying Sample Condensation for Accelerating Model Evaluation", "abstract": "Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact.\nTo address the growing cost of standard evaluation, new methods focused on efficient evaluation have started to appear. The typical approach follows two steps.\nThe typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that *maximise diversity in model responses*. \nOur method, **Diversifying Sample Condensation** **(DISCO)**, selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. **DISCO** shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC.", "tldr": "", "keywords": ["Efficient evaluation", "Large Language Models", "anchor point", "fingerprint"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b031b91181865b5546b4b72ed33225c19532f873.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes DISCO, an efficient evaluation framework that (i) selects a tiny subset of test items by ranking samples via model disagreement (measured with JSD or the Predictive Diversity Score, PDS), and (ii) predicts a target model’s full-benchmark performance from its model signature (concatenated outputs on the selected subset) using simple predictors (kNN or Random Forest). The key thesis is that we should focus on diversifying model responses, rather than on the representativeness of samples, because disagreement maximizes the information about model performance. A formal proposition links mutual information about a model’s score to the Jensen–Shannon divergence across model predictions, motivating the selection rule; DISCO then shows strong empirical results on MMLU, HellaSwag, Winogrande, ARC, and also on ImageNet, often compressing datasets to 100 samples while preserving rank/accuracy with low error. This allows model evaluation with ~99% less inference costs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear practical value: Achieving 99%+ cost reduction while maintaining prediction accuracy addresses a real problem in modern ML evaluation where benchmarks require thousands of GPU hours.\n- Elegant simplicity: The approach is conceptually simpler than prior work—avoiding complex clustering algorithms and IRT parameter estimation in favor of greedy sample selection and direct prediction from model signatures.\n- Solid empirical validation: Comprehensive experiments across multiple benchmarks (MMLU, HellaSwag, Winogrande, ARC, ImageNet) with hundreds of models, demonstrating consistent improvements over baselines.\n- Theoretical motivation: Proposition 1 provides an information-theoretic justification linking mutual information to JSD, showing why model disagreement is informative for performance prediction.\n- Domain agnostic: Demonstrates effectiveness in both language (LLMs) and vision (ImageNet) domains.\n- Thorough ablations: Table 2 systematically analyzes design choices (model split, stratification, number of source models, dimensionality reduction, prediction model).\n- Clarity: The paper is well written, the claims are clear and the experiments support these claims adequately."}, "weaknesses": {"value": "- The empirical results sections are somewhat opaque. I find it hard to understand exactly how the experiments were conducted from the text alone. The experimental set-up seems valid, but perhaps a reworking of the text or an additional, more detailed, description of the empirical set-up could be added in the appendix.\n- Statistical rigor: No confidence intervals or significance tests on reported metrics. Single train/test split for chronological evaluation. Unclear if the improvements are statistically significant.\n- Limited failure analysis: Insufficient discussion of when/why DISCO might fail. No analysis of which types of benchmarks or tasks are unsuitable. The acknowledged limitation about distribution shift in model populations lacks empirical investigation (see questions, the performance split would be useful)\n- Missing cost analysis: No discussion of computational cost for computing PDS scores over full dataset. Cost of training metamodel not reported. How does total cost (sample selection + prediction) compare to just running models on more samples?\n- From my understanding the current DISCO pipeline relies on the signature being of the same size (i.e. same size of model outputs and same number of samples), the predictor is then trained on this fixed size representation (that is reduced with PCA). Currently, if one wants to change the number of samples used it would require the entire re-training of the predictor, the main reason seems to be the concatenation of the predictions which treats each scalar prediction *of each sample* as a different variable. I would be very interested in seeing a discussion on how this could be changed so that the pipeline is adaptable with respect to the number of samples (i.e. being able to use a predictor trained on N samples with less or more samples)."}, "questions": {"value": "- Missing sentence in the abstract? Before \"The typical approach follows two steps.\" There must be something about efficient sampling (or in that sentence itself) otherwise it's unclear what we're talking about.\n- The two assumptions for Proposition 1 should be added in the main text so that result is complete, I believe there is enough space in the paper for this.\n- Typo?: line 288 \"the stolen\"\n- Table 2: The PCA results are missing (supposed to be in subplot (d)) while the Prediction model ablation is said to be in (e) while actually being in (d)\n- In section 5.3 it is stated that \"performance-based splits create an artificial stress test\" and that the chronological split better reflects real-world usage. While I generally agree with this statement, I still think a performance-based split should be included for comparison since this is a known failure mode of efficient LLM evaluation methods.\n- Why are there no other baselines besides random selection with direct eval. in Table 3 for vision models? Presumably there is nothing preventing past benchmarks to being adapted to this setting.\n\n# Summary\nI think this is a strong paper, well written and well justified both theoretically and relative to past works and with adequate empirical evaluations. I would be happy to increase my score if the authors address the identified weaknesses and points in the questions section. Most of these represent relatively small, although important, additions / modifications."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ghQJyQYqQ9", "forum": "SoOgBHa3dZ", "replyto": "SoOgBHa3dZ", "signatures": ["ICLR.cc/2026/Conference/Submission2589/Reviewer_zKXy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2589/Reviewer_zKXy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838946109, "cdate": 1761838946109, "tmdate": 1762916291509, "mdate": 1762916291509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DISCO (Diversifying Sample Condensation), an efficient evaluation approach that improves both steps common to prior frameworks: subset selection and performance prediction. Instead of clustering or relying solely on model confidence/correctness, DISCO selects samples that induce maximal inter-model disagreement, and then learns a direct mapping from a model signature—the concatenated raw outputs of a model on these selected samples—to predict full benchmark performance. Evaluated on 424 LLMs across MMLU, HellaSwag, Winogrande, and ARC, and 400 vision models on ImageNet-1k, DISCO achieves low MAE and high ranking fidelity with very small subsets (e.g., 100 samples), showing strong efficiency and competitive performance relative to prior methods."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper identifies a clear and timely gap in the literature on efficient evaluation and proposes a simple, well-motivated solution that is both conceptually clear and practically effective (specially in vision domain and for low budget regimes).\n\n- The mathematical framing provides valuable intuition for why the proposed approach works and strengthens the overall narrative without adding unnecessary complexity.\n\n- The experimental evaluation is thorough and well executed, assessing the performance of the proposed method across multiple benchmarks and model families, and comparing it against several baselines.\n\n- Table 2 (factor analysis) offers additional insight into how specific design choices affect performance.\n\n- The paper is well written and structured, making the methodology and results easy to follow."}, "weaknesses": {"value": "While the paper is clearly executed and well motivated, I have some reservations about the practical efficiency of the proposed approach, particularly for LLM evaluation.\n\n- This first point is more of a critique of the overall line of work rather than one targeted specifically at DISCO. From Figure 5, it appears that randomly selecting slightly over 1000 samples with direct evaluation achieves on-par or even slightly better performance on both MAE and rank correlation compared to more sophisticated sample-selection techniques. This raises the question of whether the added complexity of sample-selection mechanisms is justified. To identify the most informative samples, one must evaluate a large pool of models, store their outputs, and compute disagreement statistics between their predictions for each sample. This process seems computationally and memory intensive, potentially offsetting the efficiency gains claimed from evaluating fewer samples. In contrast, simply randomly selecting a slightly larger subset (e.g., 2 000 samples) and then combining it with the proposed model-signature representation and simple predictors (e.g., linear or random forest) might yield even better results with significantly lower overhead.\n\n- Relatedly, Table 1 shows that while the proposed method consistently improves over baselines across datasets, the magnitude of improvement in MAE appears modest—though I am open to being proven wrong, as this may depend on implementation details or evaluation scale. The improvement in rank fidelity is clear but numerically small given the additional computation required.\n\n- Moreover, the paper does not include a quantitative analysis of computation or memory cost across methods. While this may not yet be standard practice in this research line, such an analysis would substantially strengthen the claim of “efficiency” and help contextualize the trade-off between computational expense and predictive accuracy.\n\n- Finally, while the vision results (Table 3) are particularly strong and show a clear improvement in ranking fidelity, it remains unclear whether this work is the first to apply subset-based evaluation techniques to vision or if comparable baselines exist. Clarifying this point would help position the contribution more clearly within the broader literature.\n\nI would like to note that I am open to revising my assessment if the authors can address these concerns. I appreciate the quality of the work and would genuinely welcome clarification that changes my current view."}, "questions": {"value": "Please see the weaknesses section for a more detailed explanation of my concerns, but here are a few specific questions:\n\n- Regarding the vision experiments: could you clarify whether this is the first application of subset-based evaluation techniques to vision models (e.g., ImageNet-1k), or if there are existing baselines in that domain that you compared against or were inspired by?\n\n- About calibration: since the proposed method relies on model output probabilities both for computing disagreement (PDS) and constructing model signatures, how sensitive is it to differences in model calibration? \n\n- On ranking fidelity: how should we interpret the magnitude of change in rank correlation you report? For instance, is improving from 0.916 (baseline) to 0.987 (your method on MMLU) a substantial and practically meaningful difference in ranking quality, or does it mainly indicate incremental improvement at the high end of the correlation scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bKtbySUpNa", "forum": "SoOgBHa3dZ", "replyto": "SoOgBHa3dZ", "signatures": ["ICLR.cc/2026/Conference/Submission2589/Reviewer_dg58"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2589/Reviewer_dg58"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852333194, "cdate": 1761852333194, "tmdate": 1762916290988, "mdate": 1762916290988, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper sheds light on a important problem of efficiently estimating a model's performance of large contemporary benchmarks $D$ by evaluating the model performance on a much smaller selected subset $|A| = K \\ll D$. The core idea, DISCO, puts forward the notion that samples should be selected where models disagree the most (to maximize inter-model response diversity), not necessarily samples that are \"representative\" in input latent space. This is done using either Jensen-Shannon divergence (JSD) or a computationally simpler proxy, Predictive Diversity Score (PDS). \n\nAfter selecting $A$ each model is represented by its \"signature\" which is then mapped using a lightweight predictor (k-NN/Random Forest/Regression) to estimate actual dataset performance. The paper also provides theoretical justification for using PDS by sandwiching bounds with JSD and connecting it with mutual information. \n\nThe empirical performance is strong across language datasets like MMLU, HellaSwag, ARC, Winogrande and also in Vision (Imagenet) while dramatically reducing inference cost with $K = 100$."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The insight of shifting focus from representativeness of samples to disagreement of models on these samples is a novel and fresh perspective.\n2. PDS is computationally cheap to calculate from logits and the final predictor from the model signature is straightforward to implement in existing benchmark evaluation pipelines.\n3. The authors provide reasonable theoretical bounds to use PDS as a proxy for JSD for the subset dataset selection.\n4. The contribution includes strong empirical results and generality across domains, which adds to the fact that this method is model agnostic, thus is more likely to be used in the real world."}, "weaknesses": {"value": "1. In proposition 1, the authors implicitly use the assumption - uniform prior over source models. This is not examined fully in practice because real model pools are heterogeneous and non-uniform. How sensitive are the PDS/JSD selection and signature predictors to such non-uniformities?\n2. Computing PDS/JSD requires running $M$ source models across the entire benchmark dataset $D$ which is likely significant for large datasets. The authors should report absolute compute (GPU-hours), memory/storage for signatures and break-even points (how many target models or evals justify the offline cost) for the method to be more practically used.\n3. Minor typo in section 4.2.2. \"we also compare against a rather complex prior work\nand show that our simple method wins against the stolen\" - The word stolen is out of context."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vxs6q2YXMR", "forum": "SoOgBHa3dZ", "replyto": "SoOgBHa3dZ", "signatures": ["ICLR.cc/2026/Conference/Submission2589/Reviewer_W8ah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2589/Reviewer_W8ah"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2589/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761941249753, "cdate": 1761941249753, "tmdate": 1762916290717, "mdate": 1762916290717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}