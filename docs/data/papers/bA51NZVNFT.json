{"id": "bA51NZVNFT", "number": 3845, "cdate": 1757551196864, "mdate": 1759898066719, "content": {"title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced Performance Gap", "abstract": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for evaluating reasoning ability in voice-interactive systems under real-time conversational constraints. VERA comprises 2,931 voice-native episodes derived from established text benchmarks and organized into five tracks (Math, Web, Science, Long-Context, Factual). Each item is adapted for speech interaction while preserving reasoning difficulty. VERA enables direct text–voice comparison within model families and supports analysis of how architectural choices affect reliability. We assess 12 contemporary voice systems alongside strong text baselines and observe large, consistent modality gaps: on competition mathematics a leading text model attains 74.8% accuracy while its voice counterpart reaches 6.1%; macro-averaged across tracks the best text models achieve 54.0% versus 11.3% for voice. Latency–accuracy analyses reveal a low-latency plateau, where fast voice systems cluster around ~10% accuracy, while approaching text performance requires sacrificing real-time interaction. Diagnostic experiments indicate that common mitigations are insufficient. Increasing \"thinking time\" yields negligible gains; a decoupled cascade that separates reasoning from narration improves accuracy but still falls well short of text and introduces characteristic grounding/consistency errors. Failure analyses further show distinct error signatures across native streaming, end-to-end, and cascade designs. VERA provides a reproducible testbed and targeted diagnostics for architectures that decouple thinking from speaking, offering a principled way to measure progress toward real-time voice assistants that are both fluent and reliably reasoned.", "tldr": "", "keywords": ["voice language models", "voice reasoning gap", "real-time conversation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/abb809473d11eac063e119417c8bafcbf7a3dced.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed the first benchmark to focus on real-time reasoning tasks for voice models. The benchmark contains 2931 voice-native episodes derived from common existing text benchmarks, including topics such as Math, Web, Science, Long-Context and Factual. By evaluating on 12 current existing voice systems, they visualize the Latency-accuracy curve. In this work, they not only identified the gap between text and speech models for reasoning tasks over all models but also answered how and why such degradation exists in different models."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed benchmark VERA tackled a novel and important problem in the field of real-time voice models.\n2. They evaluate multiple voice models and show detailed results with latency-accuracy tradeoff as well as reasonable toplines and cascaded counterparts.\n3. Their analyses are detailed and informative for the field. I believe these insights are valuable for the future research community to build better real-time voice models."}, "weaknesses": {"value": "1. The authors measure the time-to-first-response as the latency in their benchmark. However, this latency might not be the only thing that the user cares about. How fast the user gets the desired response is also equally important. Reporting only the time-to-first-response has problems for cases where the model directly speaks something (probably trivial stuff like filler words) after the user question finishes. Through my personal experience, Moshi almost starts to speak right after the user stops, but it often does not give the answer right away. It starts to generate some reasoning stuff in the output speech and then arrives at a final answer. This concern might cause concern about the conclusion, since probably the actual “thinking time” is not confined to the first spoken response. It mightbe the case where the model outputs some speech response in order to get more time to “think”. \n2. How is the latency calculated in this paper? I do not see a detailed explanation (correct me if I’m wrong). I’m wondering if the reported latency is affected by hardware specification or different engineering techniques. I’m concerned if these issues affect the fairness of comparing these models on the accuracy-latency figure and also the conclusion drawn from these results."}, "questions": {"value": "How is LiveAnswer implemented? I saw some description in Appendix D.5. It seems like the Core Reasoner is responsible for thinking, and it will incrementally push information to the Narration Synthesizer. What does “incrementally” mean in this context? Will the different design of this incremental update affect the latency of the LiveAnswer method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8Cz4rozi8n", "forum": "bA51NZVNFT", "replyto": "bA51NZVNFT", "signatures": ["ICLR.cc/2026/Conference/Submission3845/Reviewer_acn2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3845/Reviewer_acn2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761234541319, "cdate": 1761234541319, "tmdate": 1762917062394, "mdate": 1762917062394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Voice Evaluation of Reasoning Ability (VERA), a benchmark designed to measure the gap between text interaction models and voice interaction models. The paper addresses three critical questions: (1) What is the gap? (2) Why does the gap exist? and (3) How do the models fail differently? Overall, the authors provide thorough benchmarking and insightful analysis of the gap."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper tackles an important and timely problem: the gap between text-based and voice-based interaction models.\n- The evaluation is comprehensive, covering various voice-based systems across different categories."}, "weaknesses": {"value": "- This reviewer is not fully convinced by the design of the cascaded pipeline (LiveAnswer). If the narration synthesizer's main task is simply to prompt users to wait for additional time to think, then a rule-based system could suffice. Moreover, GPT-5 is capable of summarization, so it is unclear why GPT-5 is not used as the narration synthesizer, as it may significantly boost the model performance.\n- In Table 6, the authors present a human evaluation study demonstrating strong agreement between human raters and GPT-4o. However, the LLM problem solver, Gemini-2.5-Flash, is a text-only LLM. The authors should compare human and GPT-4o judgments on end-to-end voice systems, as transcribing voice outputs to text may introduce additional errors.\n- The authors aim to provide a comprehensive survey and comparison of previous voice-related benchmarks in Table 4. However, several relevant and influential benchmarks are missing, such as [SD-Eval, 2024], [VoxEval, 2025], [MMSU, 2025], and [VoxDialogue, 2025]."}, "questions": {"value": "- In Section 3.2, which LLM is used as the quality validator?\n- In Section 4.1, an LLM is used as the LLM-based normalizer. Have the authors evaluated whether the LLM-based math notation standardization process introduces any errors?\n- How did the authors obtain the ground truth ASR transcripts used to calculate the Word Error Rate (WER) in Table 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VJUXLcMrXC", "forum": "bA51NZVNFT", "replyto": "bA51NZVNFT", "signatures": ["ICLR.cc/2026/Conference/Submission3845/Reviewer_5RZf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3845/Reviewer_5RZf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761446756191, "cdate": 1761446756191, "tmdate": 1762917061972, "mdate": 1762917061972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark for assessing reasoning in real-time voice agents and demonstrates a persistent “Voice Reasoning Gap” (VRG). That is, models that reason well in text collapse when required to listen and speak under low-latency constraints. The authors convert diverse text tasks into speech-native prompts across five tracks—Math, Web, Science, Long-Context (Context QA), and Factual. Results show that even with clear speech (low WER), voice models remain far below text-only performance, and a pronounced latency–accuracy frontier emerges (fast systems cluster at low accuracy).  The paper argues that simply improving audio quality or granting more “thinking time” is insufficient."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark is well-motivated and identifies a fundamental problem in current voice systems.\n2. The experiments cover a wide range of models."}, "weaknesses": {"value": "1. The results are intuitive. Compared to text baselines, spoken language models usually lag behind text models because they rely on the capability of their textual backbone, and must also correctly perceive the input, reason about it, and produce accurate spoken output. The controlled experiments on Factual QA further highlight this issue. Most voice models achieve low performance.\n2. The reported accuracy seems surprisingly low. To better disentangle the sources of failure, it would be helpful to also report the accuracy based on the textual responses of the voice models.\n3. The authors claim that experiments on voice models with text input are not important. However, I believe such experiments are essential, as they help identify which component of the voice pipeline contributes most to the observed performance drop.\n4. The claim that “increasing thinking time yields negligible gains” seems too arbitrary. This conclusion is based only on a comparison between AF3 with and without the “thinking mode,” which may not generalize to other systems. Please revise or clarify this claim.\n5. As a benchmark, the manual inspection of about 200 episodes (6.8%) seems insufficient. Considering that manual inspection is not highly labor-intensive and would not require excessive effort, it would be better to increase the number of inspected samples to strengthen data reliability."}, "questions": {"value": "### Suggestion\n1. It would be better to mention the appendix in the main text, as otherwise it becomes a bit difficult for readers to follow.\n\n### Questions\n1. In Section 3.2, what is the specific LLM agent used for dataset construction and filtering?\n2. In Appendix G, does the described filtering correspond to the Voice Suitability Filter mentioned in Section 3.2?\n3. When conducting tasks such as Factual QA or Science QA, is web search enabled for the models?\n4. Is there any possible reason why UltraVox achieves substantially higher performance on Context QA? What version of the model was used in your experiments?\n5. What is the source of human annotators?\n6. In Lines 245–248, what is the track distribution of the 200 manually inspected samples? Also, what do $\\mu$ and $\\sigma$ refer to in this context?\n7. For Accuracy Evaluation (Lines 259–264), what procedure is applied when the three independent evaluations result in a tie?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wj9IJZAEcU", "forum": "bA51NZVNFT", "replyto": "bA51NZVNFT", "signatures": ["ICLR.cc/2026/Conference/Submission3845/Reviewer_VDP8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3845/Reviewer_VDP8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640113944, "cdate": 1761640113944, "tmdate": 1762917061709, "mdate": 1762917061709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper creates a benchmark for comparing text and voice models for reasoning tasks. It then shows that, generally, voice models perform significantly worse compared to top-the-line text models. That authors then attempt to explain this gap."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- I think the problem and setup are sound. \n- The benchmark has a significant size, and I think it will be a useful resource. \n- The paper has very clear figures and tables, which really help the presentation."}, "weaknesses": {"value": "- I am a little worried about the level of automation in this paper. I know that some human validation was done, but I am not sure if it was done for every step or end-to-end for any sample. It is therefore very hard to estimate the prevalence of mistakes in the benchmark. This is more worrying as mistakes will only impact the voice-activated performance, not the text one, as those are existing benchmarks. \n\n- I do not understand the choice of not comparing the same models via text and voice input. It would have added so much to this paper, and it is easy to do. GPT-5 can be used with voice input (directly, or by other services, e.g., POE), and some of the voice models have text input. The reasoning behind not doing this was vague, and I really found this comparison missing from the paper. \n\n- The paper would benifit from a limitations section, espacially with respect to the first point above."}, "questions": {"value": "- Are you able to add more clarity about the overall prevalence of mistakes in the dataset (by human validation, like done for one of the steps, not by estimation)?\n\n- Are you able to add the direct comparison as mentioned above?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uFcqhFskC4", "forum": "bA51NZVNFT", "replyto": "bA51NZVNFT", "signatures": ["ICLR.cc/2026/Conference/Submission3845/Reviewer_VjTP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3845/Reviewer_VjTP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3845/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761732496005, "cdate": 1761732496005, "tmdate": 1762917061453, "mdate": 1762917061453, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}