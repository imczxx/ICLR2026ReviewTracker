{"id": "sbfiK1nHOB", "number": 13714, "cdate": 1758221448269, "mdate": 1759897417694, "content": {"title": "Chain-of-Thought Degrades Abstention in Large Language Models, Unless Inverted", "abstract": "For Large Language Models (LLMs) to be reliably deployed, models must effectively know when not to answer: *abstain*. Chain-of-Thought (CoT) prompting has been gained popularity for improving model performance by ensuring structured outputs that follow a logical sequence. In this paper, we first investigate how current abstention methods perform with CoT outputs, finding that direct use of reasoning traces can degrade performance of existing abstention methods by more than 5%. As a result, we introduce a new framework for thinking about hallucinations in LLMs not as answering a question incorrectly but instead as LLMs answering the *wrong* question. Based on this framework, we develop a new class of state-of-the-art abstention methods called **Trace Inversion**. First, we generate the reasoning trace of a model. Based on only the trace, we then reconstruct the most likely query that the model responded to. Finally, we compare the initial query with the reconstructed query. Low similarity score between the initial query and reconstructed query suggests that the model likely answered the question incorrectly and is flagged to abstain. We perform extensive experiments to find impressive performance gains with our Trace Inversion methods. The code is publicly available at: https://anonymous.4open.science/r/trace-inversion-9EE0/.", "tldr": "", "keywords": ["abstention", "model safety", "chain of thought"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f95aed718b525aea5c4032db9bd14e1a200d704f.pdf", "supplementary_material": "/attachment/a3d4024a87e0b5daf14ea6c22011a029a839acb9.pdf"}, "replies": [{"content": {"summary": {"value": "This paper explores the performance of abstention methods when including Chain-of-Thought (CoT) traces. The authors evaluate several baseline abstention methods on 8 datasets and using 5 open-weights models, and find that including CoT traces generally worsens abstention. They then introduce Trace Inversion, a new abstention method that regenerates a prompt given the current reasoning trace, and then uses distance between the original prompt and the regenerated prompt as an abstention metric, where high distances should be abstains. The authors test three different ways of measuring prompt distance, and find promising results for their approach overall."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper's empirical observation that passing CoT reasoning traces into abstention methods worsens their performance is important.\n* The idea behind \"Trace Inversion\", i.e., to regenerate the query from the sampled reasoning trace and compute its distance from the original query as an abstention signal, is novel, and the results seem promising overall.\n* The authors select a wide variety of baseline methods, and evaluate on 8 datasets and with 5 open-weights models."}, "weaknesses": {"value": "* The authors present very large results tables and only limited aggregated statistics, which I fear may obfuscate the scale of the problem. Table 1, for example, has no mean accuracy over models or methods, while Table 2 has the mean per method. Particularly given the variability of the results (i.e., there's no clear winner), finding a more appropriate way to present the high-level results---either as a table of aggregated results, or as a figure---would make this paper much stronger.\n* The authors test three different implementations of Trace Inversion using different distance measures. However, there is no clear winner as to which distance measure works best. In Figure 4, for example, the best-performing method for GPT-OSS on Misconceptions is the worst for DeepSeek-Distill-Qwen on MMLU. How should someone who wants to apply Trace Inversion to a model decide which to use?\n* \"Reliable Accuracy\" seems improperly specified, and would reward over-abstention (a trivial example would be abstaining for all but one answerable questions). A more appropriate choice, given the threshold-based formulation in section 3.1, would be something like an Accuracy-Rejection Curve.\n* No large-scale, API models are evaluated. While this is perhaps understandable given cost constraints, several of the abstention methods provided could be used on closed models. This makes it hard to understand whether the results hold on large-scale frontier models, and should at least be discussed as a limitation."}, "questions": {"value": "1. Kirichenko et al. [1] have previously reported that including CoT traces can inflate the performance of an LLM-as-judge abstention detector. How do you reconcile this result with your finding that including the trace worsens abstention, in particular for the (relatively similar) REFLECT method?\n2. How is the correctness of final answers verified for the non-multiple-choice benchmarks? Exact match or some other approach?\n3. A threshold parameter is introduced on line 179. How is this applied for methods that don't emit a probability, such as REFLECT?\n4. On the datasets which only contain answerable questions, the authors state that \"the model is expected to abstain when it does not have the knowledge to answer\" (line 259). How is this determined, and when should the models actually abstain for these 6 datasets?\n\n\nReferences\n\n[1] https://arxiv.org/abs/2506.09038"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VEdfiYuAif", "forum": "sbfiK1nHOB", "replyto": "sbfiK1nHOB", "signatures": ["ICLR.cc/2026/Conference/Submission13714/Reviewer_fpXh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13714/Reviewer_fpXh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760543922220, "cdate": 1760543922220, "tmdate": 1762924262667, "mdate": 1762924262667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new method called Trace Inversion to determine whether a large language model (LLM) answered the intended question in order to abstain from providing a response. The authors also benchmark several baseline abstention methods including confidence-based and answer reviewing approaches, showing including reasoning traces in the model responses degrades abstention. The authors benchmark eight datasets including commonly used benchmarks such as MMLU across several open models including DeepSeek R1, Qwen, and GPT OSS."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors explore the important problem of abstention, particularly for models that produce chain-of-thought reasoning traces. The authors include a reasonble set of baseline methods includign confidence based as well as answer-reviewing methods. The authors confirm prior findings that reasoning degrades abstention. \n\nThe proposed method, Trace Inversion, is clear and well presented. The method covers the issue of models responding to the wrong query using self-review method and response similarity. The authors show Trace Inversion, depending on the choice of similarity between the original and reconstructed query, can outperform existing methods\n\nI appreciate the authors‚Äô inclusion of the code, integration with VLLM, and straightforward README."}, "weaknesses": {"value": "# Scope of claim is far out of what is reasonably supported by experiments\n\n- The author's frame abstention as \"‚ÄúHallucinations are the result of models answering a different question that the intended one.‚Äù While this is part of abstention, models can also hallucinate context or facts while answering the intended query. The authors inappropriately recast all of abstention under this narrow umbrella. \n\n# Experimental Setup\n\n- Table 1 missing baseline abstention without abstention methods. This is key to verify that these method do in fact boost abstention and that they provide reasonable baselines against which to compare Trace Inversion.\n- Choice of datasets is suspect? Table 3 shows only 2 out of the 8 datasets used in evaluation have unanswerable questions, which is not the best setting for evaluating abstention (the ability to recognize unanswerable questions). \n\n# Trace Inversion's gains are not systematic or robust\n\n- Claim on line 453 ‚ÄúTrace Inversion provides a systematic and robust enhancement to abstention strategies‚Äù is not reflected in experimental results. The best Trace Inversion method varies by model and by benchmark‚Äîand trace inversion is in fact not always the best depending on the similarity metric compared to baselines as shown in table 1. In some cases inversion methods are worse than uncertainty or answer reviewing baselines. For example, Figure 4: Trace Inversion Ground is tied for best for GPT-OSS but is worse than baselines for DeepSeek R1\n\n# Computational cost \n\n- The method is quite costly as we have to run inference then process all the chain fo thought again to regenerate the question adn compare the similar fo the generated and original query.  This isn't discussed or measured in the paper."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Yhw1FWt4mu", "forum": "sbfiK1nHOB", "replyto": "sbfiK1nHOB", "signatures": ["ICLR.cc/2026/Conference/Submission13714/Reviewer_D1CA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13714/Reviewer_D1CA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852948464, "cdate": 1761852948464, "tmdate": 1762924262194, "mdate": 1762924262194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how Chain-of-Thought (CoT) reasoning affects LLM abstention and proposes \"Trace Inversion\", a method that reconstructs the query from its reasoning trace, then flags abstention when this differs from the original query. They evaluate across 8 datasets and 5 models, showing improvements in 28/40 settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong Empirical Results: Consistent improvements across diverse models and domains, with significant gain in reliable accuracy across 40 evaluation settings.\n2. Comprehensive Evaluation: Thorough testing on multiple model families (7B to 32B parameters), diverse domains (math, reading comprehension, bias detection), and various abstention baselines."}, "weaknesses": {"value": "1. The observation that CoT harms abstention is intuitive and not novel. Prior work cited and not cited already demonstrated that reasoning models degrade in abstention ability significantly.\n2. The paper's two parts (CoT degrades abstention and Trace Inversion improves abstention) lack logical connection. The authors appear to have concatenated two separate observations without establishing why observing CoT degradation motivates the specific design of Trace Inversion. The method would work regardless of whether CoT helps or hurts.\n3. The paper shows that Trace Inversion improves performance but provides no insight into why. More alabtion studies or failure / success case analysis would be helpful."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8Boj1uOSVk", "forum": "sbfiK1nHOB", "replyto": "sbfiK1nHOB", "signatures": ["ICLR.cc/2026/Conference/Submission13714/Reviewer_51RG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13714/Reviewer_51RG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979004621, "cdate": 1761979004621, "tmdate": 1762924261726, "mdate": 1762924261726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how reasoning traces in large language models (LLMs) affect their ability to abstain, that is, to decide when not to answer. The authors demonstrate that while Chain-of-Thought (CoT) prompting improves reasoning accuracy, it consistently harms abstention reliability by making models more prone to overconfident or misleading answers. Across eight established benchmarks and five model families, they show that adding CoT reduces both reliable accuracy (accuracy when answering) and abstention accuracy (correctness of abstain or answer decisions). To address this, they introduce Trace Inversion, a new abstention framework that treats hallucination as the model ‚Äúanswering the wrong question.‚Äù Instead of relying on confidence scores, Trace Inversion reconstructs the question implicitly answered by the model‚Äôs reasoning trace and measures its similarity to the original query; if misaligned, the model should abstain. Three variants of this method are explored: embedding-based, LLM-based, and grounded detection. Experiments show that these methods outperform all existing abstention baselines and improve reliability"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper correctly identifies a key issue overlooked in prior work: improvements in reasoning performance can sometimes come at the cost of reduced abstention reliability. To address this, it introduces an elegant and effective method called Trace Inversion. The approach is conceptually simple yet broadly applicable, demonstrating strong performance across multiple models and benchmarks. Trace Inversion is particularly compelling because it frames model errors as instances of ‚Äúanswering a different question,‚Äù offering a meaningful bridge between interpretability and uncertainty estimation."}, "weaknesses": {"value": "- The approach relies heavily on the reconstructed query and on the hypothesis that hallucinations arise when a model answers an incorrect question. However, this assumption may not always hold. The reconstructed query itself can be inaccurate or incoherent, leading to spurious or unjustified abstentions.\n\n- Using semantic similarity to compare the original question \nùëû\n and the reconstructed question \nùëû\n‚Ä≤\n is a fragile proxy for alignment. Similarity metrics may conflate paraphrasing with correctness and fail to capture deeper logical or causal mismatches. How is this handled? \n\n- Trace Inversion still depends on the model‚Äôs Chain-of-Thought, making it unable to detect or correct flawed reasoning that appears coherent on the surface. If the CoT itself is wrong, inversion merely reflects that error.\n\n- The paper does not analyze how model accuracy changes with or without Chain-of-Thought. Exploring this trade-off could reveal when one might prefer a model that is slightly less accurate but more cautious or better at abstaining.\n\n- All the models experimented upon are trained with instruction tuning to generate a reasoning chain. My assumption is that they may struggle to generate an answer directly. Wouldn't experiments with base models work better? \n\n- The presentation of the results and experimental variants is somewhat difficult to follow. Clearer tables, visualizations, or structured explanations of the baselines and Trace Inversion variants would make the work more accessible.\n\n- Overall this is an interesting paper and I'm happy to reconsider once my concerns have been addressed"}, "questions": {"value": "In weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MVHMYNBAKn", "forum": "sbfiK1nHOB", "replyto": "sbfiK1nHOB", "signatures": ["ICLR.cc/2026/Conference/Submission13714/Reviewer_ohYZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13714/Reviewer_ohYZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13714/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762134501476, "cdate": 1762134501476, "tmdate": 1762924261087, "mdate": 1762924261087, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}