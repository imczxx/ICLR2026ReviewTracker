{"id": "TsdlOjcQNu", "number": 20884, "cdate": 1758311402880, "mdate": 1763724113323, "content": {"title": "UALM: Unified Audio Language Model for Understanding, Generation and Reasoning", "abstract": "Recent advances in the audio language modeling (ALM) domain tackle audio understanding and text-to-audio generation as separate tasks. Very few studies attempt to unify these tasks -- an essential step toward advanced multimodal reasoning. This paper introduces Unified Audio Language Model (UALM), which aims to unify audio understanding, text-to-audio generation, and multimodal reasoning in a single model. To achieve this goal, we first present UALM-Gen, a text-to-audio language model that directly predicts audio tokens and is comparable to state-of-the-art diffusion-based models. We then demonstrate, using proper data blending, training recipes, and inference techniques, that our single UALM model matches the quality of state-of-the-art specialized models in audio understanding, text-to-audio generation, and text reasoning.  Furthermore, we present UALM-R1, a multimodal reasoning model that utilizes both text and audio in the intermediate thinking steps to facilitate complex generation tasks. To our knowledge, this is the first demonstration in audio research of cross-modal generative reasoning, with its effectiveness confirmed by subjective evaluations.", "tldr": "This paper introduces UALM, an audio language model designed to unify audio understanding, generation, and reasoning", "keywords": ["Audio Language Model", "Audio Understanding", "Audio Generation"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a6272b53c1658638d32eb4d5316bcfbc8f562d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces UALM : a joint LLM model for audio understanding, text-to-audio generation, and text reasoning.\nThey train an audio-reasoning model UALM-R1 that has the ability to correct its own generation. They show that LLMs can beat diffusion models at text-to-audio generation, but have different scaling laws and require much more data. \n\nMain contributions:\n- They introduce a very carefully crafted training recipe, with modality alignment, multi-task pre-training, SFT and DPO. Showing that LLM based audio gen models need 10x data compared to diffusion audio gen models.\n- Unified understanding + generation model with state of the art results on text to audio generation- the authors plan to open-source model weights upon publication\n- First attempt at reasoning for audio generation (UALM-R1) with decent increase in performance compared to UALM"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- State of the art results on text-to-audio generation with a LLM, whereas all previous work pointed towards diffusion models being the strongest models for text-to-audio. The authors show that with ~10X more data, LLMs can outperform diffusion models. This is a very promising direction for future research around audio-gen LLMs and scaling laws\n- A very well engineered pipeline for model training and post-training - with details clearly laid out in the paper.\n- Ablation studies of newly introduced parameters : CFG for LLM sampling, self-adaptation before DPO,  cross entropy loss regularization for DPO\n- Comparison to previous state of the art audio-generation models on established baselines\n- Supplemental material shows audio generation of excellent quality + comparison to state of the art models for the same prompts, very well documented"}, "weaknesses": {"value": "- The reasoning model UALM-R1 is only evaluated on \"instruction following\" tasks but its audio quality isn't compared directly with audio metrics to the base model. Reasoning is done mostly through this rich caption text abstraction but there's no \"soft\" signal extracted from the generation to improve it.\n- The authors do not show synergies between audio-understanding and audio-generation joint training - the synergy is exploited for the reasoning model but they don't discuss if audio-understanding improves audio-generation performance or the other way around."}, "questions": {"value": "- For DPO, you mention the cross entropy loss is necessary to adapt to generation. Do you adapt to generations (generated with CFG) from the CFG=1 model? How does this regularization compare to using different DPO beta to stick closer to base policy?\n- For the RVQ and delay pattern appendix - you describe quite clearly how this works at the output layer, how does this work at the input layer?\n- For the enhancement VAE - why use a VAE instead of e.g a GAN or diffusion model to upsample and correct the audio? This looks like a key step in getting SOTA FD metric"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hT0z8gkVHT", "forum": "TsdlOjcQNu", "replyto": "TsdlOjcQNu", "signatures": ["ICLR.cc/2026/Conference/Submission20884/Reviewer_tZVa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20884/Reviewer_tZVa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408258275, "cdate": 1761408258275, "tmdate": 1762999993346, "mdate": 1762999993346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UALM, a single AI model that unifies three audio tasks: understanding, generation, and reasoning. Instead of using separate models, UALM uses one large language model to handle everything. The special version, UALM-R1, pioneers multimodal reasoning by using intermediate \"rich captions\" (detailed audio plans) and a \"self-reflection\" capability, where it generates an audio clip, \"listens\" to its own output, critiques flaws, and then generates an improved version."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.This paper uses one model to do three jobs: audio understanding, audio generation, and reasoning. This is simpler and more efficient than making a separate model for each task.\n\n2.The paper introduces a \"self-reflection\" feature. The model can \"listen\" to the audio it generated, find what's wrong, and then make a better version. This helps improve the final audio quality and accuracy."}, "weaknesses": {"value": "1.The comparison in many places such as Table 2 (Audio Understanding) lacks rigor. The paper does not list the specific parameter counts for competing models like SALMONN or Step-audio-2. These models often have multiple versions of different sizes (e.g., 1B, 7B, 72B). Without this, we cannot know if the comparison against UALM is fair, making the results inconclusive.\n\n2.The paper's most novel feature, \"self-reflection\" (UALM-R1), lacks strong evidence. Its evaluation (Table 4 & Appendix C.3) relies only on a tiny subjective test (20 prompts per scenario), which is not enough to prove it works reliably. Furthermore, in the demos, the \"improvement\" sounds like it just adds new audio on top of the original, rather than truly \"correcting\" or \"replacing\" the mistake.\n\n3.Limited innovation and poor reproducibility. The paper's approach relies heavily on combining existing techniques (like the Encoder-Adapter-LLM architecture and DPO) rather than introducing new methods. Its strong performance seems to come from brute-forcing the training with a massive 30 million sample (80k hours) dataset. The paper does not detail how this dataset was collected or created, and the dataset is private. This makes the work difficult to reproduce, and it's unclear if the success is due to the model's design or just the enormous amount of private data."}, "questions": {"value": "1.In Table 2 (Audio Understanding), the paper did not provide the parameter counts for competitor models (like Audio Flamingo 3 or Qwen2.5-Omni). Can the paper provide these numbers?\n\n2.The reasoning (SFT-1/SFT-2) and generation (30M) data rely heavily on synthetic creation or pseudo-labels. Could the author detail the exact data generation pipeline for reproducibility? For example, what models and prompts were used to generate the SFT data, and how were the initial 250k \"rich captions\" created?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9XUqcS0tEj", "forum": "TsdlOjcQNu", "replyto": "TsdlOjcQNu", "signatures": ["ICLR.cc/2026/Conference/Submission20884/Reviewer_9tTC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20884/Reviewer_9tTC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761829354920, "cdate": 1761829354920, "tmdate": 1762999993341, "mdate": 1762999993341, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UALM, a unified audio-language model designed for understanding, generation, and reasoning. The authors further introduce UALM-R1, which demonstrates enhanced reasoning capabilities. Overall, the paper is clearly written and easy to follow."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is clearly written and well organized. The accompanying demo page provides convincing qualitative results.\n\n- The three main challenges are well defined and clearly articulated.\n\n- The proposed methodology—pre-extracting audio tokens and continuing the training of a pretrained LLM—appears effective and contributes to the model’s success.\n\n- The paper provides several insightful observations, such as the comparable data requirements between LLMs and diffusion models, and the use of classifier-free guidance. These are valuable takeaways for the research community."}, "weaknesses": {"value": "- The main concern lies in the dataset preparation and processing, which currently lack sufficient detail. I would be like to accept this paper if the dataset construction process is clearly described.\n\n- The paper should provide detailed information on how the 30M text–audio pairs were created, including data sources, filtering, and preprocessing steps.\n\n- In Equation (1), the meaning of the symbol \\pi is unclear — does it denote a probability distribution?\n\n- The reproducibility of the research is uncertain, as important implementation and dataset details are not provided."}, "questions": {"value": "N.A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qDZitZ0vGV", "forum": "TsdlOjcQNu", "replyto": "TsdlOjcQNu", "signatures": ["ICLR.cc/2026/Conference/Submission20884/Reviewer_jfVq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20884/Reviewer_jfVq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832823379, "cdate": 1761832823379, "tmdate": 1762999993349, "mdate": 1762999993349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UALM, a single decoder only LLM (initialized from Qwen2.5 7B) that is extended with an audio encoder + MLP adapter for inputs and an audio codec head for outputs. It aims to unify three capabilities in one model: (i) audio understanding, (ii) text to audio generation, and (iii) multimodal reasoning that interleaves text and audio in intermediate steps. Empirically, UALM matches or outperforms strong baselines on AudioCaps and SongDescriber."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1. \nThe design choices (25 Hz input frames, 50 Hz codec frames with 8 RVQ codes, loss scaling so one audio frame ≈ one text token) are well motivated and concrete.  \n\nS2. \nUALM Gen and UALM deliver competitive objective scores vs. leading diffusion models on both SongDescriber and AudioCaps.\n\nS3. \nThe rich caption representation (keywords, temporal layout, and detailed descriptions) plus enrichment, dialogue, and self reflection workflows are well illustrated."}, "weaknesses": {"value": "W1.\nThe generation corpus is largely pseudo labeled, and DPO pair selection uses CLAP + aesthetics metrics. This raises a risk that the model (and the enhancement VAE) overfit to the judge/feature space that also underlies the reported objective scores. \n\nW2. \nUALM R1’s core claim is generation oriented reasoning, yet the subjective study uses only 20 prompts per scenario with AMT raters, and there is no standardized benchmark for audio CoT controllability (e.g., temporal ordering, count, spatial positioning) beyond demos. \n\nW3. \nThe alignment warm up is asserted to be critical, but there’s no ablation isolating its contribution vs. simply training longer/fewer steps or different adapter capacity.\n \nW4. \nWeights/code are promised “upon publication\", but several key assets (the 30M corpus, rich caption pairs, selection heuristics for DPO, and the enhancement VAE checkpoints) are not available. Given the centrality of the enhancement VAE and the data scale, releasing exact configs and checkpoints (even if data cannot be released) is important."}, "questions": {"value": "Please refer to the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1MJ5fK9pmO", "forum": "TsdlOjcQNu", "replyto": "TsdlOjcQNu", "signatures": ["ICLR.cc/2026/Conference/Submission20884/Reviewer_jHYo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20884/Reviewer_jHYo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965582583, "cdate": 1761965582583, "tmdate": 1762999993192, "mdate": 1762999993192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ALM), an ambitious framework designed to unify audio understanding, text-to-audio generation, and multimodal reasoning within a single model. The work presents a three-stage approach: (1) It first develops UALM-Gen, a language model-based text-to-audio generator that, through extensive data scaling and techniques like Classifier-Free Guidance (CFG) and Direct Preference Optimization (DPO), achieves quality comparable to state-of-the-art diffusion models. (2) It then presents UALM, a single model trained via a carefully designed data blending and curriculum learning strategy to concurrently handle audio understanding, generation, and text-only reasoning tasks without significant performance degradation. (3) Finally, it introduces UALM-R1, a reasoning-enhanced model post-trained to perform novel multimodal generative reasoning tasks, such as generating audio from abstract prompts (Enrichment), interactive refinement (Dialogue), and iterative self-correction (Self-Reflection). The authors claim that UALM achieves state-of-the-art or competitive performance across all three domains in a unified manner."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Ambitious and Visionary Goal:** The paper sets out with a highly ambitious and forward-looking vision: to create a single, unified model for audio intelligence that integrates understanding, generation, and reasoning. This conceptual goal is well-motivated by parallels with human cognition and represents a significant step towards more general and capable AI systems.\n\n2.  **Impressive System-Building and Engineering:** The sheer scale and complexity of the engineering effort are commendable. The authors successfully tackle numerous technical challenges, from scaling up a language model-based audio generator to rival diffusion models, to carefully balancing a multi-task, multi-modal training regime. The methodical approach, breaking down the grand vision into manageable technical problems and providing effective solutions for each, is a major strength.\n\n3.  **Pioneering Exploration of Generative Reasoning:** The introduction of multimodal reasoning for audio *generation* is the most novel and exciting contribution of this work. The concept of \"Self-Reflection\"—a cycle of generating, understanding, critiquing, and refining—is a powerful paradigm that mimics human creativity and problem-solving. This pushes the boundary beyond simple input-output generation towards more intelligent, iterative systems."}, "weaknesses": {"value": "Despite the impressive scope and results, the paper suffers from several significant weaknesses that temper its contributions and raise questions about the validity of some of its core claims.\n\n1.  **Misleading Performance Claims Due to External Enhancement Module:** The paper's headline claim of achieving state-of-the-art generation quality is confounded by its reliance on a powerful, external \"Enhancement VAE.\" This module, which upsamples the model's 16kHz mono output to 48kHz stereo, is a separate, large post-processing network. The ablation study in Table 8 reveals that this VAE is responsible for a massive improvement in objective scores (e.g., FD). This raises two critical issues:\n    *   **Lack of Fairness:** The performance of the core `UALM` is conflated with the performance of this post-processor. Comparisons against other SOTA models are unfair unless those models are also afforded the same enhancement module. The true generative capability of the unified model itself remains unclear.\n    *   **Contradiction of \"Unified\" Claim:** The necessity of a separate, large enhancement module contradicts the central premise of achieving all capabilities \"in a single model.\" A truly unified model should be able to generate high-fidelity audio end-to-end.\n\n2.  **Lack of Evident Synergy in the \"Unified\" Model:** The paper’s foundational argument is that unifying understanding, generation, and reasoning will create a synergistic system where abilities mutually reinforce each other. However, the paper provides little to no evidence that this synergy occurs during the unified pre-training of the main `UALM` model. The training appears to be a form of multi-task learning, demonstrating that a single model *can* perform these tasks concurrently, but not that it performs them *better* because of the unification. The most compelling synergy (Self-Reflection) only emerges after extensive, specialized post-training in `UALM-R1`, which calls into question the actual benefits of the unified pre-training stage itself.\n\n3.  **Incomplete Literature Review:** The paper's literature survey for audio generation is not up-to-date. It omits several recent and highly influential text-to-audio generation models, such as **Make-An-Audio** and **AudioLCM**. These models represent the cutting edge of the field. To robustly claim that a language model-based approach can be \"comparable to state-of-the-art diffusion-based models,\" it is essential to benchmark against these latest and strongest competitors. The current comparison set, while respectable, is incomplete.\n\n4.  **Heavy Reliance on a Non-Transparent Intermediate Representation:** The advanced reasoning capabilities of `UALM-R1` are heavily dependent on a manually designed intermediate representation, the \"Rich Caption.\" This structured format simplifies the reasoning task into what can be seen as a sophisticated form of \"template filling.\" This raises questions about the generality of the learned reasoning. It is unclear how the model would perform on complex tasks that cannot be easily decomposed into the predefined \"Keywords, Layout, Description\" structure. This suggests the model may be learning a specific workflow rather than a general, flexible reasoning ability."}, "questions": {"value": "1.  Given the significant performance boost from the Enhancement VAE, would it be more transparent to present the primary results of `UALM-Base` (without enhancement) and compare those directly to other models? How does the core unified model's raw output quality stand against SOTA baselines before any post-processing?\n2.  The paper argues for the \"synergy\" of unification. Can you provide any analysis or evidence showing that the unified pre-training of `UALM` leads to improvements in one capability (e.g., generation) as a direct result of being trained on another (e.g., understanding), compared to training them separately on the same data?\n3.  The \"Self-Reflection\" mechanism is a highlight of the paper. However, it operates on a macro, \"post-hoc\" level. Have you considered or explored more fine-grained, internal self-correction mechanisms that could operate during the token-by-token generation process itself?\n4.  The success of the reasoning abilities seems tied to the structure of the \"Rich Caption.\" How robust is this approach to tasks that do not fit neatly into this predefined structure? Does the model exhibit more general, free-form planning capabilities beyond this format?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pdcg7fjyLh", "forum": "TsdlOjcQNu", "replyto": "TsdlOjcQNu", "signatures": ["ICLR.cc/2026/Conference/Submission20884/Reviewer_Gqug"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20884/Reviewer_Gqug"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762156934395, "cdate": 1762156934395, "tmdate": 1762937479194, "mdate": 1762937479194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a audio language model that unifies audio understanding, text-to-audio generation, and text-only reasoning within a single framework. The model jointly learns to perceive, generate, and reason across audio and text modalities by extending a text LLM with modal alignment and post-training. The paper also introduce a caption-rich multimodal dataset that links audio with diverse textual descriptions, which enables downstream tasks, such as controllable audio generation. Experiments demonstrate competitive results on benchmarks for audio understanding, text-to-audio generation, and text-based reasoning. The paper is well written."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "*Unified multimodal design and empirical performance*. The paper presented a unified framework for audio understanding, text-to-audio generation, and reasoning. The proposed model achieves competitive or comparable results across diverse tasks, showing that the multimodal post training is effective.\n\n*Data contribution*. The introduction of a caption-rich dataset with detailed text descriptions for audio clips is an important resource. It supports both controllable generation and fine-grained understanding, addressing a bottleneck in current audio-language research."}, "weaknesses": {"value": "*Description of data curation*. As described in the paper, the desirable performance appears to rely heavily on the richness of the captions; however, the process of dataset curation remains unclear. A more detailed explanation beyond Sections 2.4.2 and 3.1 would strengthen the paper, particularly regarding data filtering, caption generation, and quality control.\n\n*Preference design in stage 1*. I am somewhat concerned about the use of the CLAP model for DPO training, as it may not accurately capture human preferences in text–audio alignment. The authors could consider or at least discuss preference-aware alternatives such as Human-CLAP [1].\n\n*Lack of ablation*. It is also unclear how much the model benefits from each post-training stage (e.g., instruction tuning, alignment tuning, DPO). Providing ablation studies or quantitative comparisons would help clarify their relative contributions.\n\nOther minor issue:\n*Comparison with speech language model* It is recommended that the authors discuss speech language models (SLMs) and clarify how their proposed framework differs conceptually and technically from SLMs."}, "questions": {"value": "The authors are encouraged to address or answer the question raise in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "na"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hgzqC4a1HV", "forum": "TsdlOjcQNu", "replyto": "TsdlOjcQNu", "signatures": ["ICLR.cc/2026/Conference/Submission20884/Reviewer_BqjX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20884/Reviewer_BqjX"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission20884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762737283372, "cdate": 1762737283372, "tmdate": 1762937477599, "mdate": 1762937477599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}