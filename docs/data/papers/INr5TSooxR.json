{"id": "INr5TSooxR", "number": 13963, "cdate": 1758226016519, "mdate": 1759897399796, "content": {"title": "SUSD: Structured Unsupervised Skill Discovery through State Factorization", "abstract": "Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. \nIn this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent’s focus toward underexplored factors.\nThis structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL).\nOur experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is available at the anonymous repository: [https://anonymous.4open.science/r/SUSD](https://anonymous.4open.science/r/SUSD).", "tldr": "We present a novel factorized method that exploits the compositional structure of environments to acquire diverse and dynamic behaviors.", "keywords": ["Unsupervised Skill Discovery", "Hierarchical RL"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/572cc2fd056ef1480b57de9623bcca1ff58e8c53.pdf", "supplementary_material": "/attachment/4aa9bb2b86d566ca8c17c9dd4c9838f81161c7ab.zip"}, "replies": [{"content": {"summary": {"value": "This paper has proposed a new unsupervised skill discovery method. Different from the previous mutual information-based and distance-based skill discovery methods, the proposed approach learn skills through state factorization. The proposed approach is evaluated in the Mujoco environment and the MPE environment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper is well written, which clearly conveys the importance of the unsupervised skill learning problem.\n\n2.\tThe proposed approach is extensively evaluated in various domains, such as Mujoco and MPE."}, "weaknesses": {"value": "1.\tIt is confusing what aspect the state factorization based on, which is related to the definition of skills. As stated in Section 4.2, the skills are learned with the curiosity-based rewards. However, curiosity can vanish with learning, so will the skills collapse to single action? How to prevent the collapse of the skills?\n\n2.\tThe experiment results demonstrate returns in the downstream tasks. However, as this paper aims at unsupervised skill learning, it is more important to show the comparison of the learned skills among the baselines. Can the skills learned in the Kitchen environment manipulate the task-related objects?"}, "questions": {"value": "Please see the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "qsaeDtkfeq", "forum": "INr5TSooxR", "replyto": "INr5TSooxR", "signatures": ["ICLR.cc/2026/Conference/Submission13963/Reviewer_1HYa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13963/Reviewer_1HYa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761558678125, "cdate": 1761558678125, "tmdate": 1762924464987, "mdate": 1762924464987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Introduces factorization to distance maximizing skill discovery from mutual-information skill discovery by giving each factor a different skill factor, and then introducing a balancing strategy based on the learning level of the skills for that factor. This is done by working in the FMDP framework and adding a curiosity bonus. Illustrates results on a few factored environments, including 2d modified gridworld and 2d object manipulation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Adds a novel combination of techniques (factored learning to distance-based skill discovery).\n\nProvides adequate empirical evidence to support the method\n\nhandles the load-balancing question for factors in a sufficient way."}, "weaknesses": {"value": "Provides marginal change from existing methods in the unsupervised skill discovery space, since both factorization and skill learning have been tried before.\n\nDoes not provide clear ablations for which change produces the gain in improvement.\n\nProvides a somewhat limited view of skill learning, focused only on unsupervised skill discovery."}, "questions": {"value": "Skill discovery, even in its modern form, is not introduced by the MISL framework but has been part of hierarchical reinforcement learning for a long time, so the choice of citations is quite narrow for simply calling it \"Unsupervised Skill Discovery.\" The references focus on mutual information based skills.\n\nAnother important limitation of using state-space distances is that these distances can often be either hard to learn, such as in the case of temporal distances, or not useful, such as in the case when the state spaces is images. This seems like a more significant issue than simple controllable factors, which is a challenge for both DSD and MISD methods. Thus the motivation does not entirely make sense.\n\nIt is probably worth highlighting early in the work that the primary contribution of this work is to apply concepts from existing MISL methods, such as DUSDi or SkiLD, to DSD methods, as factorization and load balancing have been previously addressed.\n\nBy assigning each factor a skill parameter, even those that are not controllable, if there is a significant amount of background noise, this could result in poor performance, since the changes from these uncontrollable factors would wash out the reward from the controllable ones.\nIt seems like curiosity-based factor weighting could result in poor performance in cases where there are factors that change constantly, but are not easy to control. Thus, this relies on a kind of quasistatic assumption where hard-to-control factors are generally stationary, so that getting reward from them will generally be constant until the agent learns to manipulate that factor. This is a challenge with all curiosity-based methods, but moreso in a case where the agent needs to manipulate multiple factors at once. \n\nBy giving each factor one skill, this seems like it could fail to load balance factors that require more fine-grained control, and those which require less fine grained control. \n\nThis method assumes that all factors are along a flat hierarchy of control: skills learned on one factor are not useful for downstream factors, but this is often not the case (the easy-to-control factors are often useful for downstream factors), and it is not clear if there is a solution for this. \n\nIt is not clear why SUSD should outperform any of the methods in the Ant or Halfcheetah environments, yet it appears to do so in Ant. This suggest perhaps a lack of variation or failure to sufficiently optimize baselines. \n\nFor factor decoding, doesn't SUSD learn a separate feature map for each factor using ground truth information? Especially since in the FMDP setting the factors are already given. So isn't SUSD given privileged information? A fairer comparison might be a comparison against representations from DUSDi, but that would require significant changes to that algorithm. Thus, it seems unneessary to have this section.\n\nConsidering factored MISD methods like SkiLD have been applied to 3D environments like iGibson, it would be good to have a more comprehensive evaluation in a more complex domain to demonstrate that DSD improves performance even in 3D."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XYMFZPprpl", "forum": "INr5TSooxR", "replyto": "INr5TSooxR", "signatures": ["ICLR.cc/2026/Conference/Submission13963/Reviewer_jqsH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13963/Reviewer_jqsH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761604230457, "cdate": 1761604230457, "tmdate": 1762924464445, "mdate": 1762924464445, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SUSD, a new framework for unsupervised skill discovery (USD) that aims to exploit the compositional structure of environments. It builds upon Distance-Maximizing Skill Discovery (DSD) methods like METRA, introducing (1) state space factorization to structure the skill space (similar to DUSDi) and (2) a curiosity-based factor weighting mechanism that prioritizes underexplored factors (similar to CSD). The paper evaluates SUSD across five environments (Ant, HalfCheetah, 2D-Gunner, Kitchen, and Multi-Particle), showing improvements over recent USD baselines, including DIAYN, LSD, CSD, METRA, and DUSDi."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposed a method with strong empirical results, combining the strengths of skill disentanglement (e.g. DUSDi) and Distance-Maximizing Skill Discovery (e.g. METRA). Ablation studies and factorization sensitivity provide useful insight into component contributions.\n\nThe factorized embedding formulation is clean and intuitively appealing; it aligns well with factored MDP structure.\n\nThe curiosity-based factor weighting is a natural and well-motivated extension to encourage balanced skill learning.\n\nThe combination of DUSDi, CSD and METRA is new.\n\nThe paper is well-written, where the presentation is clear and easy to understand"}, "weaknesses": {"value": "My main concern with this paper is its novelty and conceptual contribution: while the idea of combining state factorization with distance-based skill discovery is sensible, it is not clear how much SUSD goes beyond a straightforward integration of DUSDi, CSD and METRA: \n- The factorized skill structure is conceptually almost identical to DUSDi, except that it is applied within a DSD objective rather than a mutual-information one.\n- The curiosity-based weighting resembles the controllability weighting from CSD and METRA, but applied per-factor.\n\nAdditionally, while the proposed factorization makes intuitive sense in multi-entity environments, the paper doesn’t clearly characterize when this inductive bias is beneficial.\nFor instance, is the method sensitive to over- or under-factorization? Does performance degrade if the factor decomposition is slightly misaligned with the true controllable entities? A brief sensitivity analysis or discussion would make the claims more general."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "x3frMzjchx", "forum": "INr5TSooxR", "replyto": "INr5TSooxR", "signatures": ["ICLR.cc/2026/Conference/Submission13963/Reviewer_ThUn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13963/Reviewer_ThUn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915841330, "cdate": 1761915841330, "tmdate": 1762924463987, "mdate": 1762924463987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on unsupervised skill discovery and aims to improve distance-maximizing skill discovery performance in environments in multiple controllable objects. To this end, it assumes the state factorization is given and incoroporates the factorization into METRA objective. The proposed method further weights each factorization's contribution to the intrinsic reward based on its prediction accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well written and easy to follow, especially in the method description and the experiment results.\n\nThe proposed method of improving METRA's controllbility over multiple objects, to the best of my knowledge, is novel in the field of unsupervised skill discovery.\n\nThe ablations and extra experiment results in the appendix are throughout and provides good insight of each component's importance."}, "weaknesses": {"value": "My major question is the empirical evaluation for the motivation of the paper -- learning skills that engage all controllable factors in the environment.\nSpecifically, in Sec 5.4.1, since the state space is continuous, the # of unique states is infinite, so the policy may still visit many unique states while only covering a small portion of the state space. It's more appropriate to discretize the state space into bins and see the % of bins that are covered by the policy. I notice the authors mentioned that they \"round\" the state, kindly correct me if you are already doing that (would be helpful to explain how exactly the rounding is conducted). In addition, visualizing the each factor's coverage is another valid way to show better controllability.\n\nMinor:\nmissing space on Line 478."}, "questions": {"value": "Different state factors can have different scales and thus different $-\\log(s^i_{t+1}|s^i_t)$ scales, how do you account for this scale differences when weighting the intrinisc reward?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ISjcDPVbmI", "forum": "INr5TSooxR", "replyto": "INr5TSooxR", "signatures": ["ICLR.cc/2026/Conference/Submission13963/Reviewer_8o7W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13963/Reviewer_8o7W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979327647, "cdate": 1761979327647, "tmdate": 1762924463344, "mdate": 1762924463344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}