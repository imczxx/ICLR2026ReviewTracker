{"id": "8CZshTogXk", "number": 21348, "cdate": 1758316494801, "mdate": 1763292161664, "content": {"title": "ArabiDoc: A Holistic Arabic-English Evaluation Suite for End-to-End Document Processing", "abstract": "Document intelligence sits at the intersection of computer vision and natural language processing, where the goal is to transform complex real-world documents into structured, machine-readable representations. While progress has been made, current benchmarks for low-resource languages such as Arabic remain limited, typically emphasizing individual components, like text, tables, or charts, without providing a comprehensive evaluation of full-document parsing. To address this gap, we present a new bilingual (Arabic-English) benchmark that brings together diverse document elements within a single evaluation framework for end-to-end document parsing. Our benchmark offers three main contributions. First, it preserves reading order information, which allows models to better capture the natural flow of documents. Second, it supports visual content parsing, encompassing not only text blocks but also tables, charts, and figures, thereby reflecting the full range of document structures. Third, it introduces relaxed evaluation metrics that more fairly assess model performance by tolerating minor deviations in reading order or localized errors in table and chart parsing, ensuring the evaluation reflects practical usability rather than strict exactness. Constructed through a two-step annotation process, layout segmentation followed by object-level labeling, our dataset includes 137 pages that have been carefully segmented and verified by human annotators. By unifying previously separate evaluation tracks, this benchmark establishes the first comprehensive standard for structured document parsing in Arabic and provides a more realistic basis for bilingual evaluation with English. We expect this resource to foster progress in multimodal reasoning, enable stronger baselines, and support the development of vision-language models that generalize robustly across languages and document types.", "tldr": "", "keywords": ["End-to-end document parsing", "Bilingual Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8764c1a6130b8aa78f6cb175486e79af5a038b23.pdf", "supplementary_material": "/attachment/e4d5f8c15f76b2d3458e60ff898e27d768f7c1ec.pdf"}, "replies": [{"content": {"summary": {"value": "This paper introduces ArabiDoc, a bilingual (Arabic-English) benchmark for evaluating end-to-end document parsing models. The benchmark consists of 137 pages (68 Arabic, 69 English) annotated with text, tables, charts, and reading order information. The authors evaluate both expert document parsing models (Granite-Docling, DotsOCR) and general-purpose vision-language models (Gemini 2.5, GPT-4.1, Qwen2.5-VL, InternVL3.5) on this benchmark. The evaluation employs relaxed metrics including LCS for reading order, TEDS for tables, ChartEx for charts, and word-level F1 for text. Results show that models perform substantially worse on Arabic documents compared to English, with general-purpose models like Gemini 2.5-Pro achieving the best overall performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The benchmark addresses an important gap by providing end-to-end evaluation that jointly considers text, tables, and charts within full-page documents, moving beyond isolated subtask evaluation.\n \n2. The benchmark includes reading order evaluation which is valuable for assessing holistic document understanding, particularly for Arabic's right-to-left directionality.\n\n3. The relaxed metrics for tables (order-invariant) and text (word-level alignment) better reflect real-world requirements than strict exact matching."}, "weaknesses": {"value": "1. **Insufficient dataset scale:** With only 137 pages, the benchmark is too small to serve as a robust evaluation standard or support meaningful statistical analysis. This severely limits its utility for assessing model generalization. Compared to KITAB-Bench (Arabic) with 8,809 samples and OmniDocBench (English) with 981 samples. \n\n2. **Lack of diversity and fine-grained analysis:** The paper provides no breakdown of:\n- Page layouts (single vs. multi-column)\n- Document types (financial reports, academic papers, presentations, books)\n- Table complexity (spanning cells, embedded images)\n- Chart types (line, bar, pie, etc.)\n- Font variations (Ruq'ah, Naskh for Arabic)\n- Image quality attributes (scanned vs. digital, watermarks, tilt)\n\nWithout these statistics, it's impossible to assess the benchmark coverage or identify specific failure modes.\n\n3. **Missing comparative analysis:** Section 3.2 claims the benchmark provides \"a balanced mixture\" but offers no comparison to existing benchmarks (OmniDocBench, KITAB-Bench). Comparative statistics are essential to justify design choices.\n\n4. **Incomplete model evaluation:**\n\n- Pipeline-based methods entirely omitted without justification\n- Granite-Docling chart evaluation missing despite the model producing bounding boxes (charts could be cropped and re-fed for annotation extraction)\n- No qualitative examples or failure analysis explaining why models perform poorly on Arabic\n\n5. **Limited scope** compared to OmniDocBench: Missing formula recognition, which is critical for academic and scientific documents.\n\n6.**Vague methodology descriptions:**\n\n- Line 277: \"sequence matching algorithm\" not specified\n- Lines 213-215: Refinement criteria and its impact on reading order not detailed\n- Line 290-292: Unclear how fuzzy matching differs from KITAB-Bench implementation\n\n\n5. **Insufficient metric justification:** Text evaluation uses word-level alignment but provides no examples demonstrating why this is superior to alternatives or how it handles Arabic morphology.\n\n6. **No qualitative analysis:** Paper lacks examples from either the benchmark or model predictions in the main text, making it difficult to understand annotation quality or model failure patterns.\n\n7. **Shallow analysis:** Results tables show scores but provide minimal insight into what causes performance degradation (hard fonts? scanned quality? complex layouts?). The ablation study (Section 4.1) is the only analytical component."}, "questions": {"value": "1. **Code-switching**: Does the benchmark include code-switched Arabic-English content common in modern Arabic documents?\n\n2. **Refinement process** (Lines 213-215): What specific criteria guide the manual refinement of bounding boxes and object merging? How does this refinement quantitatively improve reading order accuracy?\n\n3. **Picture annotations** (Line 247): You mention pictures can include infographics. Are infographics fully annotated? What about other picture types (natural images, logos, screenshots), are they annotated or ignored?\n\n4. **Fuzzy matching** (Lines 290-292): How does your fuzzy column matching algorithm differ from KITAB-Bench's approach? What makes it more suitable for your scenario?\n\n5. **Granite-Docling charts**: Since the model outputs chart bounding boxes, why not crop and re-feed these regions to evaluate chart parsing capability?\n\n6. **Evaluation completeness**: Why were pipeline-based methods excluded? They remain widely used and could provide important baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x49uDXKhd7", "forum": "8CZshTogXk", "replyto": "8CZshTogXk", "signatures": ["ICLR.cc/2026/Conference/Submission21348/Reviewer_T7sy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21348/Reviewer_T7sy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761144846632, "cdate": 1761144846632, "tmdate": 1762941712714, "mdate": 1762941712714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "N8EHhQDttJ", "forum": "8CZshTogXk", "replyto": "8CZshTogXk", "signatures": ["ICLR.cc/2026/Conference/Submission21348/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21348/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763292160944, "cdate": 1763292160944, "tmdate": 1763292160944, "mdate": 1763292160944, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an arabic-english document parsing evaluation benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "This work introduces a new benchmark for Arabic document analysis and makes a certain contribution. However, its overall impact is restricted by limitations in areas like dataset quality, which constrains the scope of its contribution."}, "weaknesses": {"value": "1. The main text does not meet the requirement of 9 pages.\n\n2. Too few evaluation methods are included. Too many mainstream multilingual OCR solutions (e.g., mineru, olmocr, nanonets, etc.) have not been evaluated.\n\n3. No results for formulas are provided in the task-specific section.\n\n4. Results are not categorized by different document types.\n\n5. The dataset analysis and visualization results are insufficient, making it difficult to demonstrate the contribution of the dataset. The overall contribution of the work is not convincing. How can we verify the diversity of Arabic documents in the evaluation set? How can we understand the performance differences among different models?"}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MmFOY0JPJN", "forum": "8CZshTogXk", "replyto": "8CZshTogXk", "signatures": ["ICLR.cc/2026/Conference/Submission21348/Reviewer_xj4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21348/Reviewer_xj4s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795152633, "cdate": 1761795152633, "tmdate": 1762941712433, "mdate": 1762941712433, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Existing low-resource language (e.g., Arabic) document benchmarks lack comprehensive full-document parsing evaluation, so this work proposes an Arabic-English bilingual benchmark integrating diverse elements for end-to-end parsing. With 137 human-annotated pages and three key features (reading order preservation, multi-type visual content support, relaxed metrics), it establishes the first comprehensive standard for structured Arabic document parsing and cross-lingual evaluation."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper shows originality in addressing low-resource (e.g., Arabic) document parsing gaps with a bilingual framework, and high quality through rigorous human-annotated data."}, "weaknesses": {"value": "I strongly recommend that the authors rewrite this paper.\n1. What I truly fail to comprehend is that over a dozen OCR VLMs have emerged in the past six months, yet the authors only evaluated six models (including general-purpose VLMs)—which is simply inadequate for a benchmark. \n2. Additionally, why have the authors failed to provide even a single sample image of the dataset? \n3. Furthermore, is there any real necessity to incorporate English documents into the dataset?"}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "542oFUDeKf", "forum": "8CZshTogXk", "replyto": "8CZshTogXk", "signatures": ["ICLR.cc/2026/Conference/Submission21348/Reviewer_S7nn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21348/Reviewer_S7nn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761881962474, "cdate": 1761881962474, "tmdate": 1762941712143, "mdate": 1762941712143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ArabiDoc, a new bilingual (Arabic-English) benchmark dataset for end-to-end document processing. The authors' goal is to address a gap in existing benchmarks, which they argue are often fragmented into single-task evaluations (like OCR or table recognition) and lack support for low-resource languages like Arabic.\n\nThe benchmark consists of 137 document pages (68 Arabic, 69 English) sourced from finance and executive reports. Its key contributions are:\n\n1. Holistic Evaluation: It assesses the parsing of diverse elements—text, tables, and charts—within a single, unified framework.\n\n2. Reading Order: It explicitly preserves and evaluates the natural reading order of document elements.\n\n3. Relaxed Metrics: It proposes evaluation metrics that prioritize \"practical usability\" over \"strict exactness\". For example, the table metric (a modified TEDS) heavily weights content accuracy (80%) over structural accuracy (20%) , reflecting the needs of downstream tasks like Retrieval-Augmented Generation (RAG).\n\n\nThe paper evaluates several \"expert\" and \"general-purpose\" VLMs, concluding that while modern models like Gemini 2.5 show strong performance, a significant performance gap persists, especially on Arabic documents."}, "soundness": {"value": 1}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Problem Identification:** The paper correctly identifies a clear and important gap in the literature: the lack of comprehensive, end-to-end document parsing benchmarks, especially for low-resource languages like Arabic.\n2.  **Pragmatic Metric Philosophy:** The authors' argument for \"relaxed\" metrics that prioritize content accuracy for downstream tasks (like RAG) is a strong, pragmatic, and welcome contribution. The modified TEDS score (80% content, 20% structure) is a good example of this.\n3.  **Cross-Lingual Analysis:** The bilingual nature of the dataset (Arabic and English) allows for a direct and valuable comparison of model performance, clearly highlighting the cross-lingual generalization gap.\n4.  **Ablation Study Finding:** Though it contradicts the paper's main premise, the finding in Section 4.1 that pre-cropped inputs are \"dramatically\" (91% relative improvement in English) better than full-page ROI parsing is a significant and actionable insight.\n5.  **Clear Presentation:** The paper is well-written, clear, and easy to follow. The motivation for the work is well-argued.\n6.  **Helpful Visuals:** Figures like the radar chart (Figure 1), the annotation pipeline (Figure 2), and the dataset construction diagram (Figure 3) are helpful for understanding the evaluation and data creation process. Table 1 also provides a useful comparison against prior work."}, "weaknesses": {"value": "1.  **Critically Insufficient Dataset Size:** The primary weakness is that **137 pages is not a benchmark**; it is a small test set. This size is insufficient to capture the diversity of \"complex real-world documents\" or to provide a stable, generalizable measure of VLM performance. The authors themselves acknowledge this as a limitation.\n2.  **Fatal Annotation Bias:** The decision to use an \"expert annotation model\", specifically Gemini, to generate the ground-truth annotations is a fundamental methodological flaw. This creates an unmitigated conflict of interest when Gemini models (Gemini 2.5-Pro/Flash) are then evaluated and achieve the highest scores. The paper provides no quantitative analysis of the \"manual verification\" step to prove that this bias was removed.\n3.  **Contradictory Narrative and Flawed Main Experiment:** The paper promotes \"holistic\" and \"end-to-end\" parsing, yet its own ablation study (Section 4.1) proves this approach is deeply flawed and **vastly inferior to a modular detect-and-crop pipeline**. The paper fails to reckon with this contradiction and proceeds to use the inferior full-page method for its main experiments, rendering the results in Tables 2 & 3 uninformative of *actual* SOTA capability.\n4.  **Superficial Evaluation of \"Reading Order\":** The evaluation of reading order via the Longest Common Subsequence (LCS) is overly simplistic. It treats the document as a linear sequence, ignoring the far more complex *logical* and *hierarchical* structure (e.g., 'this is a caption for that table,' 'this list belongs to this header'). This misses a key component of true document understanding.\n5.  **Weak \"Relaxed\" Text Metric:** For a paper that (correctly) champions pragmatic metrics for RAG, the choice of a \"relaxed word-alignment metric\" is weak. For RAG, *semantic* equivalence is what matters, not lexical overlap. A metric like BERTScore would have been far better aligned with the paper's stated philosophy.\n6.  **Buried Key Finding:** The paper's most significant finding—the superiority of the detect-and-crop pipeline—is buried in an \"Ablation Study\" (Section 4.1) rather than being a central part of the results and discussion."}, "questions": {"value": "1.  **On Annotation Bias:** Can you provide a quantitative analysis of the \"manual verification\" step? What percentage of the \"expert annotation model's\" outputs were corrected by human annotators? What were the common error types? How can you assure the community that the final ground truth is free of \"Gemini-flavor\" artifacts?\n2.  **On Dataset Creation:** Why was a competitor model (e.g., GPT-4.1, Claude 3) not used to generate a portion of the annotations, or why was a fully-manual-from-scratch annotation process not used on a subset, to measure and control for this obvious annotator-model bias?\n3.  **On Experimental Setup:** Your ablation study (Section 4.1) provides strong evidence that a detect-and-crop pipeline is \"dramatically\" better than the full-page parsing method used in your main experiments (Tables 2 & 3). Why did you not use this superior pipeline-based approach for your main evaluation? The current results seem to evaluate a sub-optimal \"strawman\" version of these models.\n4.  **On Dataset Scale:** How can the authors justify that 137 pages are sufficient to form a \"benchmark\" that can produce reliable and generalizable conclusions, especially when the authors themselves state in their limitations that this modest size \"may restrict the ability of large-scale models to fully demonstrate their generalization capacity\"?\n5.  **On Metric Choice:** Given the stated focus on \"practical usability\" for RAG, why did you choose a \"word-alignment\" metric for text evaluation instead of a *semantic* similarity metric (e.g., BERTScore), which would better capture whether the *meaning* was extracted correctly?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper uses an \"expert annotation model\", which it identifies as a Gemini model, to generate the ground-truth data for the benchmark. It then evaluates other Gemini models (Gemini 2.5 Pro/Flash) on this same benchmark, where they achieve the highest scores.\n\nThis represents a significant potential conflict of interest and a \"data contamination\" or benchmark bias issue. The benchmark may be measuring similarity to the annotation model rather than true task performance. This is a responsible research practice concern, as the validity and fairness of the benchmark are questionable. While the authors disclose using LLMs for *writing*, the use of an LLM for *ground-truth generation* is a far more significant methodological issue that is not adequately transparently addressed or mitigated."}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "MgMzyP8aLh", "forum": "8CZshTogXk", "replyto": "8CZshTogXk", "signatures": ["ICLR.cc/2026/Conference/Submission21348/Reviewer_T5vW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21348/Reviewer_T5vW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908008284, "cdate": 1761908008284, "tmdate": 1762941711765, "mdate": 1762941711765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}