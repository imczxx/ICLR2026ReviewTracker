{"id": "R5L1TD1Z58", "number": 25140, "cdate": 1758364609391, "mdate": 1759896732709, "content": {"title": "ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs", "abstract": "Code runtime optimization$\\textemdash$the task of rewriting a given code to a faster one$\\textemdash$remains challenging,\nas it requires reasoning about performance trade-offs involving algorithmic and structural choices.\nRecent approaches employ code-LLMs with slow-fast code pairs provided as optimization guidance, but such pair-based methods obscure the causal factors of performance gains and often lead to superficial pattern imitation rather than genuine performance reasoning.\nWe introduce ECO, a performance-aware prompting framework for code optimization.\nECO first distills runtime optimization instructions (ROIs) from reference slow-fast code pairs; \nEach ROI describes root causes of inefficiency and the rationales that drive performance improvements.\nFor a given input code, ECO in parallel employs (i) a symbolic advisor to produce a bottleneck diagnosis tailored to the code, and (ii) an ROI retriever to return related ROIs.\nThese two outputs are then composed into a performance-aware prompt, providing actionable guidance for code-LLMs.\nECO's prompts are model-agnostic, require no fine-tuning, and can be easily prepended to any code-LLM prompt.\nOur empirical studies highlight that ECO prompting significantly improves code-LLMs' ability to generate efficient code, achieving speedups of up to 7.81$\\times$ while minimizing correctness loss.", "tldr": "", "keywords": ["Code optimization", "performance-aware", "code-llm"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fe8783ce7a25b920e350a47db8ededbec6c6873.pdf", "supplementary_material": "/attachment/cc72ea1837fb152f937e694e197944f11fe5ecd3.zip"}, "replies": [{"content": {"summary": {"value": "This paper targets an important problem of generating efficient code with Large Language Models (LLMs). It proposes a new method with runtime optimization instructions (ROIs) to guide LLMs to generate more efficient code. Evaluation on both PIE datasets and out-of-distribution Codeforce C++ datasets shows that the proposed method outperforms prior approaches in generating efficient code."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Targets an important and practical problem of generating efficient code with LLMs.\n- Novel idea of using runtime information to build a knowledge base for generating efficient code.\n- Clear writing and well-structured presentation."}, "weaknesses": {"value": "- The baselines in the evaluation seem weak and outdated.\n- The dataset selection for evaluation is limited in diversity."}, "questions": {"value": "This paper focuses on an important and relevant problem of improving code efficiency generated by LLMs. By leveraging runtime optimization instructions (ROIs) to build a knowledge database, and using symbolic advisor for retrieval, the proposed method tackles limitations of prior approaches. The paper is well-presented and easy to follow. There are several concerns that need to be addressed, particularly concerning the soundness of the evaluation.\n\nThe baseline comparison experiments do not look convincing. Three of the selected baselines are from 3 years ago (2020-2022), which may not reflect the most recent advances; the selected two recent baselines (Supersonic and SBLLM) seem to be weak baselines, as their performance is significantly worse than even the vanilla instruction-only baseline (see Table 2). It is not clear whether such weak comparisons can truly reflect the effectiveness of the proposed method. Are there more recent baselines in the literature that can be compared against?\n\nThe datasets selected for evaluation lack diversity. From Table 4, the reasons for performance degradation are most simple and superficial (e.g., using cin rather than scanf, not using sort function). It would be more convincing to show more complex and nuanced examples where the performance degradation is not easily fixed by simple code changes. The following dataset is more recent and contains complex algorithmic problems that can be a good target for evaluation:\n\n[How efficient is LLM-generated code? A rigorous & high-standard benchmark](https://arxiv.org/html/2406.06647v4)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p9GFPcYMre", "forum": "R5L1TD1Z58", "replyto": "R5L1TD1Z58", "signatures": ["ICLR.cc/2026/Conference/Submission25140/Reviewer_5KXa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25140/Reviewer_5KXa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980652602, "cdate": 1761980652602, "tmdate": 1762943341395, "mdate": 1762943341395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ECO (Enhanced Code Optimization), a performance-aware prompting framework designed to improve code large language models’ (code-LLMs) ability to generate efficient code. ECO addresses a key limitation of existing LLM-based code optimization methods—overreliance on raw slow-fast code pairs, which leads to superficial pattern imitation rather than targeted performance reasoning. The framework first distills Runtime Optimization Instructions (ROIs) from reference slow-fast code pairs, capturing the root causes of inefficiencies and optimization rationales. At inference, ECO combines two complementary modules: a symbolic advisor (rule-based, using Code Property Graphs to diagnose code-specific bottlenecks) and an ROI retriever (retrieving performance-aligned ROIs from a prebuilt database). These outputs form a model-agnostic prompt that guides code-LLMs to optimize input code. Experiments on the in-distribution PIE benchmark and out-of-distribution Codeforces dataset show ECO achieves speedups of up to 7.81× (e.g., with GPT-o4-mini) while minimizing correctness loss, outperforming baselines like instruction-only prompting, in-context learning (ICL), and retrieval-augmented generation (RAG)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Targeted Performance Reasoning Over Pattern Imitation: By distilling ROIs (which explicitly link inefficiencies to optimization rationales) instead of using raw slow-fast code pairs, ECO addresses a critical flaw in prior work. This design enables code-LLMs to focus on why optimizations work rather than just copying surface-level edits, reducing semantic drift and improving generalization.\n2. Complementary Dual-Module Design: The symbolic advisor (deterministic, rule-based bottleneck detection) and ROI retriever (contextual, example-driven guidance) balance precision and flexibility. Ablation studies confirm that each module compensates for the other’s weaknesses—e.g., the symbolic advisor ensures high correctness via fixed rules, while the ROI retriever enables broader optimization coverage—resulting in a more robust framework than single-module alternatives.\n3. Model-Agnostic and Practical Deployment: ECO requires no fine-tuning or model-specific adaptation, working seamlessly with both open-source (Qwen2.5-Coder) and closed-source (GPT-4o-mini, GPT-o4-mini) code-LLMs. This plug-and-play design makes it highly practical for real-world use, as it avoids the computational costs of retraining and can be integrated into existing LLM workflows with minimal effort."}, "weaknesses": {"value": "1. Incremental Improvement Over Component Technologies: ECO’s key components rely on well-established tools and methods without significant advancements. The symbolic advisor uses Joern (a standard tool for Code Property Graph analysis) and manually curated rules—there is no novel static analysis technique here. The ROI retriever uses off-the-shelf embedding models (Qodo-Embed-1-1) for similarity matching, with no innovation in retrieval logic. The framework’s value comes from integrating these components, not advancing the components themselves.\n2. Manual Effort in Rule and ROI Curation: The symbolic advisor’s rules are manually clustered from ROIs, and ROI distillation requires human-designed prompt templates (Appendix A.1). This manual effort limits scalability—adapting ECO to new programming languages (beyond C++) or new types of inefficiencies (e.g., GPU-specific bottlenecks) would require redefining rules and ROI templates. The paper does not address how to automate this process, which reduces ECO’s utility for diverse or emerging optimization tasks.\n3. Lack of Comparison to Compiler-Driven Optimization: The paper positions ECO as a complement to compiler optimizations (e.g., GCC’s -O3 flag) but does not explicitly compare ECO’s gains to those from compilers alone. It is unclear whether ECO’s optimizations (e.g., replacing cin with scanf, loop invariant hoisting) are already captured by modern compilers—or if ECO adds value beyond what compilers can achieve. This gap weakens the case for ECO’s practical impact."}, "questions": {"value": "1. Scalability to New Languages and Inefficiencies: The symbolic advisor’s rules and ROI templates are designed for C++ (e.g., detecting cin/cout inefficiencies, std::vector misuse). How would ECO need to be modified to support other languages (e.g., Python, Java) with different inefficiency patterns (e.g., Python’s for loop overhead, Java’s garbage collection bottlenecks)? Is there a path to automate rule/ROI curation for new domains, or would this require continuous manual effort?\n2. Performance on Smaller Models: ECO’s accuracy decreases with small models (3B parameters) because they cannot follow complex prompts. Have the authors tested simplified prompt variants (e.g., shorter bottleneck diagnoses, fewer ROIs) for smaller models? If such adaptations improve correctness without sacrificing speedup, this would expand ECO’s utility—yet the paper does not explore this direction.\n3. ROI Distillation Reliability: ROIs are distilled using a reasoning-oriented LLM (DeepSeek-r1:32b) with a manually designed prompt. How reliable is this process? For example, do ROIs ever misidentify the root cause of inefficiencies (e.g., blaming a loop instead of a data structure)? If so, how does this error propagate to ECO’s performance, and have the authors considered validating or filtering ROIs to reduce noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0hB9csSabr", "forum": "R5L1TD1Z58", "replyto": "R5L1TD1Z58", "signatures": ["ICLR.cc/2026/Conference/Submission25140/Reviewer_z5NH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25140/Reviewer_z5NH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990305147, "cdate": 1761990305147, "tmdate": 1762943341019, "mdate": 1762943341019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ECO, a framework for LLM code optimization. ECO first distills runtime optimization instructions (ROIs) from slow-fast code pairs. For a new input program, it uses a symbolic advisor to find deterministic bottlenecks and an ROI retriever to find relevant optimization rationale. These are combined into a performance-aware prompt that any LLM can follow without fine-tuning, allowing coding LLMs to generate more optimized code. The experimental results indicate that ECO works with both open and closed source models and shows speedup on the PIE and the OOD Codeforces datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This work is well motivated and addresses a meaningful task, the deterministic symbolic advisor provides a more reliable diagnosis of the code's bottlenecks, and the method itself is training-free, allowing easy deployment."}, "weaknesses": {"value": "- For the comparisons with fine-tuning methods (D.2), only qwen2.5-coder-7B is discussed, it would be interesting to see how the OpenAI models do after fine-tuning. (There actually is a fine-tuning API for both 4o-mini and o4-mini)\n- This paper should differentiate itself from \"LLM Program Optimization via Retrieval Augmented Search\" (Anupam et al., 2025) , which was posted on arXiv in January 2025. Specifically, ECO's core premise is to distill natural language ROIs because raw code pairs is insufficient, which is also the central contribution of Anupam et al. Their RAS framework introduced contextual retrieval, and their AEGIS framework explicitly decomposes training examples into 'atomic edits' with natural language explanations."}, "questions": {"value": "- Could the authors precisely state the novelty of the ROI distillation and ROI retriever in light of the contextual retrieval (RAS) and atomic edits (AEGIS) framework from Anupam et al., 2025?\n- Under identical conditions (LLM, k, same gem5 config, PIE dataset), how does ECO's ROI-retrieval + symbolic advisor compare to RAS's contextual retrieval + beam search or AEGIS's atomic-edit search?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JB5aduMS1w", "forum": "R5L1TD1Z58", "replyto": "R5L1TD1Z58", "signatures": ["ICLR.cc/2026/Conference/Submission25140/Reviewer_kEGt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25140/Reviewer_kEGt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25140/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993533904, "cdate": 1761993533904, "tmdate": 1762943340726, "mdate": 1762943340726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}