{"id": "51Fr7zIA87", "number": 6746, "cdate": 1757994286087, "mdate": 1759897897086, "content": {"title": "Towards the Decisive Factor of Symbolic Generalization of DNNs", "abstract": "The decisive factor that drives deep neural networks (DNNs) to learn non-generalizable representations (*i.e.,* non-generalizable interactions between input variables) has been a persistent challenge in the field of symbolic generalization. In this paper, we quantify the generalization power of such interactions encoded by DNNs, and we discover that DNNs usually learn non-generalizable interactions from a few samples, referred to as *confusing samples*. The emergence of confusing samples during the training process explains the overfitting of a DNN. We further discover that the composition of confusing samples is determined by the randomness of parameter initialization in the low layers of a DNN. In contrast, other factors, such as high-layer parameters and network architecture, have much less impact on the composition of confusing samples. Consequently, two DNNs initialized with different low-layer parameters will eventually learn entirely different sets of confusing samples, even though they have similar performance.", "tldr": "This study proves that it is the randomness of parameter initialization in the low layers of a DNN that determines the composition of its confusing samples.", "keywords": ["Model Generalization", "Overfitting", "Deep Learning Theory"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/696868b1012dd49478e981aa5fe39d0b7a1f41bb.pdf", "supplementary_material": "/attachment/c2eaed749eb23ae5ab49e0e04bcbb41b88ff716d.zip"}, "replies": [{"content": {"summary": {"value": "The paper identifies the cause of overfitting in symbolic generalization to be the emergence non-generalizable interactions in DNNs. Confusing samples lead to non-generalizable interactions and are distinct from samples with high loss. It then investigates the impact of different aspects of model architecture. In particular, it discovers that low-layer parameter initializations have a more pronounced effect on the average interaction order, giving rise to more confusing samples."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The paper takes a decisive step towards understanding symbolic generalization. In particular, it relates emergence of non-generalizable interactions to overfitting and shows clear evidence of this correlation.\n2) Their observation that samples with high loss is not equivalent to confusing samples, is important and well-supported.\n3) The authors also investigate the impact of inherent data quality, initializations and architectural differences on overfitting."}, "weaknesses": {"value": "1) I'm concerned about the approach taken to discover these non-generalizable interactions by using a baseline model trained only on testing data. In particular, the interactions developed after such a training may correspond to memorizing interactions on the test data. More details on training for this baseline model (which I could not find in the appendix either) is needed to address this issue. The cited technical report He et. al. (2025) also does not talk about the training procedure.\n2) The authors mention that the confusing samples were conducted on the testing samples. Can they clarify what this means? If indeed they obtain the confusing samples from a test pool, how is this relevant to the training phase?\nIn general, I think any claims made by conducting experiments on a test set must be very carefully made if there is any sort of training involved on the test set, due to subtleties related to data leakage or overfitting on test set.\n3) There is no discussion of limitations in the main text.\n4) While I believe the findings of the paper are significant from an explainability perspective, it seems difficult to transfer knowledge about confusing samples to a new dataset or a new initialization."}, "questions": {"value": "1) The experiment on architectural difference seems to have a distinction only in terms of parameter count (Resnet-32 and Resnet-56). Renaming this section might be helpful. As a follow-up the authors can try training models without residual connections in the upper or lower or all layers.\n2) The last subplot in Fig. 6(c) is missing the results for TinyBert. \n3) A small suggestion to improve readability would be to summarize the plots in Figs 5-7. Could you report the average deviation from the diagonal for each plot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R7E0GVoOzr", "forum": "51Fr7zIA87", "replyto": "51Fr7zIA87", "signatures": ["ICLR.cc/2026/Conference/Submission6746/Reviewer_A3cL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6746/Reviewer_A3cL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761590043149, "cdate": 1761590043149, "tmdate": 1762919030716, "mdate": 1762919030716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper describes DNN overfitting as an example-level phenomenon: a small number of \"perplexed examples\" are introduced during training, inducing the model to learn higher-order interactions that cancel out and are not generalizable, thus increasing the training-test gap. We further find that the random initialization of low-level parameters almost determines which samples will be confused, while the impact of the data itself, high-level parameters, or architecture is relatively small. Based on these observations, we also show that suppressing nongeneralizable interactions during training can lead to small performance gains, and propose the practical implication that data valuation should vary from model to model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. We shift the fine-grained attribution of \"overfitting/generalization\" from \"sample difficulty\" to the chain of \"low-level initialization -> confusing samples -> non-generalizable interactions\", which extends the perspective of the lottery ticket hypothesis.\n\n2, we validate our conclusions to be robust across vision /NLP, different architectures, and demonstrate practical gains from the training intervention."}, "weaknesses": {"value": "1. The generalization test relies on the \"baseline model trained on the test set\": using a baseline trained only on the test set to determine whether an interaction generalizes or not can lead to the question of \"evaluation bias/information leakage\" in methodology. The authors need to discuss more fully the necessity of this setting and alternatives (e.g., cross-fold verification, test-free inversion criteria).\n\n2. Universality of \"only sharing low-level is aligning confusing samples\": Current examples are mainly ResNet/BERT Tiny, etc. It is recommended to verify the conclusion boundary on larger models and more complex data.\n\n3. Heuristic nature of $\\eta_{avg}$: While the appendix provides evidence of correlation, it is recommended to supplement it with a more systematic sensitivity analysis (threshold τ, mask strategy, etc.) and alignment with the \"true generalization failure rate\"."}, "questions": {"value": "1. Confusing vs. difficult samples: Can you provide systematic metrics (e.g., intersection over union, conditional error rate improvement), rather than just graphs, to support the claim that the two are not equivalent?\n\n2. It is mentioned in the article that \"low-level initialization is the decisive factor\". What is the reference for this \"decisive\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3eEbEO16hl", "forum": "51Fr7zIA87", "replyto": "51Fr7zIA87", "signatures": ["ICLR.cc/2026/Conference/Submission6746/Reviewer_e8C8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6746/Reviewer_e8C8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915061647, "cdate": 1761915061647, "tmdate": 1762919030189, "mdate": 1762919030189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the decisive factor that hinders symbolic generalization in deep neural networks (DNNs). Building on prior work that interprets a DNN’s output as a logical model composed of AND-OR interactions, the authors explore how networks internally form inference patterns. They introduce the concept of confusing samples, which is a small subset of training examples that lead DNNs to learn non-generalizable interactions. The key and surprising claim of the paper is that randomness in the initialization of the low-level parameters, rather than characteristics of the dataset itself, determines which samples become confusing and consequently induce non-generalizable interactions. The authors empirically demonstrate that two networks with identical architectures and similar overall performance, but different low-layer initializations, develop almost entirely different confusing sample sets. Conversely, networks sharing the same low-layer initialization exhibit much greater overlap in confusing samples, even when their high-layer configurations or training data differ. Based on these results, the paper argues that low-layer parameter initialization is the decisive factor influencing symbolic generalization in DNNs, providing new insights into the mechanisms of overfitting and generalization failure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **S1.** The paper introduces a novel methodological framework for identifying the cause of non-generalizable interactions in DNNs, including the concept of confusing samples.\n- **S2.** By comparing confusing samples with commonly discussed hard samples, the paper provides symbolic generalization researchers with fresh perspectives and meaningful insights that challenge conventional assumptions.\n- **S3.** The work effectively links symbolic generalization analysis to practical overfitting behavior, thereby offering a deeper understanding of training dynamics.\n- **S4.** The ablation studies convincingly support the main claim by demonstrating that the variability in confusing samples is driven not by high-layer parameters or architectural differences, but specifically by the initialization of low-layer parameters."}, "weaknesses": {"value": "- **W1.** The emphasis on low-layer parameter initialization is intriguing, but the experiments are limited to image classification tasks (CIFAR-10, Tiny-ImageNet) and a medium-scale BERT model on SST-2. It remains unclear whether the findings can be generalized to larger-scale architectures, other modalities (e.g., language generation), or more diverse training paradigms.\n- **W2.** The definition of confusing samples depends on choices such as interaction-order thresholds and saliency measures, which can be sensitive to hyperparameters. This raises concerns about the stability and reproducibility of the concept under different configurations.\n- **W3.** Extracting high-order interactions is computationally expensive, which may limit the scalability of the proposed analysis to modern, large-scale DNNs. This constraint could reduce the practical applicability of the approach."}, "questions": {"value": "- **Q1.** To what extent does the observed phenomenon generalize to other domains and learning settings? It would be valuable for the authors to provide further discussion on its applicability beyond the current experimental scope.\n- **Q2.** Are there ways to reduce the sensitivity of confusing-sample identification to hyperparameters, such as thresholds for interaction extraction, to ensure more stable and reliable characterization?\n- **Q3.** In Appendix D, were the low-layer parameters kept fixed during training? If so, then the set of confusing samples would still depend on the particular initialization used. Could this mean that an unfavorable initialization might incorrectly classify important samples as confusing and lead the framework to remove them, ultimately degrading the model’s performance? Clarification from the authors on this potential risk would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "kAyO3EP7S3", "forum": "51Fr7zIA87", "replyto": "51Fr7zIA87", "signatures": ["ICLR.cc/2026/Conference/Submission6746/Reviewer_FDLL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6746/Reviewer_FDLL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6746/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994247078, "cdate": 1761994247078, "tmdate": 1762919029656, "mdate": 1762919029656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}