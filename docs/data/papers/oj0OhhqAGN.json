{"id": "oj0OhhqAGN", "number": 18945, "cdate": 1758292242128, "mdate": 1759897071396, "content": {"title": "SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale", "abstract": "The resource requirements of Neural Networks can be significantly reduced through pruning---the removal of seemingly less important parameters. However, with the rise of LLMs, full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible not only due to i) the size of the search space, but also because ii) caching all intermediate values of the matrix multiplication needed to specify the optimization objective is already prohibitive. Existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we leverage three key insights: a) enforcing equal sparsity levels per row decouples the rows without harming performance, b) the dimensionality of the problem can be reduced by leveraging the unitary invariance of the Frobenius norm objective and transforming the calibration data accordingly, and c) computing optimal 1-swaps (exchanging one kept and one pruned weight) can be realized efficiently. These insights enable us to implement a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at llm scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.", "tldr": "", "keywords": ["pruning", "llm", "sparsity", "wanda", "sparsegpt", "efficiency"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e64603e25611b4a8cc1b1ebe0542c79e6863e3e6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a heuristic local optimization algorithm for finding better pruning masks for LLMs.\nAchieves better results than DSnoT."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The proposed algorithm is sound and correct."}, "weaknesses": {"value": "- \"EQUAL SPARSITY-LEVEL ACROSS ROWS MUST NOT BE DETRIMENTAL\" is supported by the findings in Wanda, but it is not supported in SparseGPT and ADMM pruning [1]\n- SVD trick on calibration data might be unnecessary, since the reconstruction error can also be written as $tr((W_p - W)(X^TX)(W_p - W)^T)$ and $X^TX$ has $d \\times d$ shape. Or in other words, why do costly SVD, when one can just store Hessian ($X^TX$) for the layer-wise reconstruction problem?\n- There is no explicit runtime mentioned, only something vague in the last sentence.\n- Why would I use the SparseSwaps algorithm over SparseGPT/ADMM? For example, SparseSwaps achieves 19.75 perplexity on 60% Llama-3.1-8B, while ADMM achieves 13.92. And ADMM/SparseGPT are fast.\n- Also, something weird is happening for 50% sparsity (Table 2), where SparseSwaps did not provide any benefit over Wanda.\n- \"Optional Weight Reconstruction\" section does not make much sense, since computing $(X_uX_u^T)^-1$ for each row would need way too many matrix inverses. Approaches such as [1] are much better.\n- Metrics for original dense models should also be presented (e.g., in Table 1)\n\n[1] Boža, Vladimír. \"Fast and Effective Weight Update for Pruned Large Language Models.\" Transactions on Machine Learning Research."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YtnnRQ8A64", "forum": "oj0OhhqAGN", "replyto": "oj0OhhqAGN", "signatures": ["ICLR.cc/2026/Conference/Submission18945/Reviewer_1jXE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18945/Reviewer_1jXE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761486363769, "cdate": 1761486363769, "tmdate": 1762931002941, "mdate": 1762931002941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies layer-wise pruning for LLMs by reframing the mask-selection objective as a GPU-friendly local optimization problem that monotonically reduces the reconstruction loss. It is argued that exactly solving the combinatorial mask selection is intractable, hence existing methods must rely on surrogates that ignore within-row interactions. The authors propose to make the per-row objective separable by enforcing equal sparsity per row or N:M blocks, compress the calibration activations via an SVD-based unitary transformation to shrink the data dimension, and then apply 1-swap evaluations with efficient incremental updates to greedily pick the best kept or pruned exchange per row. The proposed method caches the weighted contributions and maintains a running sum of pruned rows; each candidate 1-swap is scored via a precomputed norm and a dot-product with the current residual, which gives fast monotone improvements under per-row or N:M constraints. For implementation, the algorithm warm-starts from any mask, performs up to certain swap iterations per row, and optionally applies a least-squares weight reconstruction on the kept indices. Experiments show that the proposed method yields up to 70% reductions in per-layer pruning error over previous method and attain consistent performance gains at higher sparsity. Improvements at milder sparsity can be lower."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well-written.  The main observations on row separability, unitary invariance and SVD compression, and exact 1-swap with incremental updates are convincing and directly related to the complexity bottlenecks of pruning LLMs. Using exact 1-swap search over the true objective is a new and interesting approach compared to previous LLM pruning methods which often optimize surrogates.  The proposed method is well-motived based on the observations, with detailed discussion on complexity and memory trade-offs. Experiments on several LLMs show good error reduction and performance gains. Discussion is also given for the cases where local loss reductions don’t translate to performance gains."}, "weaknesses": {"value": "I don't see any major weaknesses. Perhaps the authors should consider taking account of structures within q/k/v or MLP sub-blocks into their approach to understand why some layers benefit more than others."}, "questions": {"value": "1. It might be helpful to see peak per-layer GPU memory, wall-clock and GPU cost for different T_max's and sparsities.   \n2. Are perplexity and zero-shot accuracy sensitive to calibration corpus domain shifts and the number of calibration tokens?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kQu3CTPCXY", "forum": "oj0OhhqAGN", "replyto": "oj0OhhqAGN", "signatures": ["ICLR.cc/2026/Conference/Submission18945/Reviewer_Q6G3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18945/Reviewer_Q6G3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606914838, "cdate": 1761606914838, "tmdate": 1762931002238, "mdate": 1762931002238, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of finding optimal pruning masks for Large Language Models (LLMs) in a post-training, retraining-free setting. The authors correctly identify that state-of-the-art methods solve a layer-wise reconstruction error minimization problem, but that solving this problem exactly is computationally intractable due to both the combinatorial search space and, more critically, the prohibitive memory cost of caching intermediate values required for the optimization.\n\n1. The paper introduces SparseSwaps, a method to refine an existing pruning mask by making this optimization problem tractable. The core of the work rests on three key insights:\n2. Row-wise Decoupling: Enforcing equal sparsity per row (a common practice in LLM pruning) decouples the optimization problem, allowing each row of the weight matrix to be handled independently.\n3. SVD-based Compression: Leveraging the unitary invariance of the Frobenius norm, the high-dimensional calibration data matrix X can be compressed via SVD into a much smaller matrix X' without changing the optimization objective. This elegantly solves the memory bottleneck.\n\nEfficient 1-Swap Local Search: The authors propose an iterative local search algorithm that efficiently evaluates all possible \"1-swaps\" (exchanging one pruned weight for one unpruned weight) by pre-computing intermediate values and using incremental updates. This allows for monotonic improvement of the true row-wise reconstruction error.\nThe proposed SparseSwaps algorithm is presented as a post-hoc refinement step that can be applied to row-wise sparse masks (e.g., from Wanda or RIA). Extensive experiments on a suite of modern LLMs (Llama-3.1, Gemma-2, etc.) demonstrate that SparseSwaps consistently improves perplexity and zero-shot accuracy over strong baselines for both unstructured and semi-structured (2:4) sparsity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper correctly identifies a major practical limitation of sota layer-wise LLM pruning methods: the computational intractability.\n2.This paper proposes  three clever insights includes Row decouping, SVD Compressing and 1-Swap optimization that significantly reduce the problem's complexity with clear mathematics analysis.\n3.The paper provides compelling evidence for the effectiveness of SparseSwaps across multiple modern LLM architectures and sparsity patterns (unstructured, 2:4 N:M)."}, "weaknesses": {"value": "1. Constraint to Per-Row Sparsity: The first insight, which enables the method's tractability, is also its main limitation. By decoupling the rows, the algorithm cannot reallocate sparsity between different rows of a weight matrix. This restricts its ability to find a truly optimal unstructured mask at the layer level, as the sparsity budget for each row is fixed by the warm-start mask. The authors acknowledge this limitation in the conclusion.\n2. Computational Overhead: While the paper argues the cost is amortizable, SparseSwaps is inherently more computationally expensive than the one-shot methods it refines. A more detailed analysis of the practical wall-clock time and peak memory usage on standard hardware (e.g., for a 7B model on an A100) would be beneficial for practitioners to gauge the trade-off between performance gain and computational cost. The theoretical complexity is given, but its real-world implication remains somewhat abstract.\n3. Lacks ablation on the key findings and design choices i.e. p-u interaction and Tmax."}, "questions": {"value": "1. Practical Cost Analysis: Could you provide concrete wall-clock timings for running SparseSwaps (e.g., for T_max=100 iterations) on a model like Llama-3.1-8B and compare it to the runtime of the baseline methods it refines (Wanda/RIA)? This would provide a clearer picture of the practical cost involved.\n2. Overfitting and Regularization: Your analysis showing that minimizing local error can sometimes hurt perplexity (Table 2, 50% sparsity) is very interesting. Have you considered any mechanisms to mitigate this? For example, could one use a small validation set of calibration data to implement early stopping for the swap iterations on a per-layer basis?\n3. Exploring Inter-Row Swaps: Given the limitation of the per-row constraint, have you considered a hybrid approach? For instance, after the per-row optimization converges, one could perform a limited number of swaps between rows (e.g., swapping a pruned weight in a \"low-impact\" row for a kept weight in a \"high-impact\" row). Do you believe such an extension would be feasible and/or beneficial?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HHtN2zXU6K", "forum": "oj0OhhqAGN", "replyto": "oj0OhhqAGN", "signatures": ["ICLR.cc/2026/Conference/Submission18945/Reviewer_ohzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18945/Reviewer_ohzU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761723807957, "cdate": 1761723807957, "tmdate": 1762931001880, "mdate": 1762931001880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SparseSwaps, a scalable and tractable post-training mask refinement algorithm for pruning LLMs. Empirical validation is performed across various large-scale open-source LLM, reporting significant improvements over existing pruning methods in terms of per-layer pruning error, perplexity, and zero-shot accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. To address the core bottlenecks in LLM pruning, this paper propose an integrated framework that combines row decoupling, SVD-based compression, and a 1-swap strategy. This approach achieves substantial improvements over existing pruning methods such as DSnoT and Wanda.\n2. This paper is well-motivated by three insights in Sec. 2.\n3. The experiments report consistent and sometimes substantial improvements on both local pruning loss and downstream task metrics across multiple model families.\n4. The theoretical analysis in this paper is convincing, as it not only explains the effectiveness of SparseSwaps but also clarifies why previous methods are suboptimal."}, "weaknesses": {"value": "1. The experiments in this paper are somewhat limited. Although results are provided for five LLM models, all of them are language models. It remains unclear how SparseSwaps performs on vision models or other types of Transformer architectures. This limitation constrains the generality and comprehensiveness of the evaluation.\n\n2. The paper does not provide the runtime of SparseSwaps on different models or comparisons with baselines, which makes it difficult to evaluate the proposed method.\n\n3. The paper lacks a theoretical or experimental characterization of the convergence behavior of 1-swap.\n\n4. The experiments in the paper appear to use a fixed setup (see line 350), lacking evaluations of the method under different sequence lengths and on other datasets."}, "questions": {"value": "1. Can the authors provide a comparison of runtime and memory usage between SparseSwaps and other baselines, using the same experimental setup?\n2. Is the effectiveness of SparseSwaps limited to specific data distributions and experimental setups? Could the authors provide additional experimental results under different calibration data settings?\n3. Since SparseSwaps depends on warm-start masks (such as outputs from Wanda or RIA), how sensitive is the method to the quality of these masks? If the warm-start mask is of low quality (e.g., randomly generated masks or poor heuristic pruning masks), can SparseSwaps still effectively minimize pruning errors?\n4. Are there any plans to expand the theoretical work on 1-swap? For example, could the authors derive a quantitative relationship between the number of 1-swap iterations and the reduction in pruning error, or prove the degree of approximation to a local optimum under specific conditions (such as row-level sparsity or SVD compression)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UkC644GmkP", "forum": "oj0OhhqAGN", "replyto": "oj0OhhqAGN", "signatures": ["ICLR.cc/2026/Conference/Submission18945/Reviewer_svrG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18945/Reviewer_svrG"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18945/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761733206768, "cdate": 1761733206768, "tmdate": 1762931001434, "mdate": 1762931001434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}