{"id": "4yHJLiz8dm", "number": 13901, "cdate": 1758224658750, "mdate": 1759897405029, "content": {"title": "Toward Greater Autonomy in Materials Discovery Agents: Unifying Planning, Physics, and Scientists", "abstract": "We aim at designing language agents with greater autonomy for crystal materials discovery. While most of existing studies restrict the agents to perform specific tasks within predefined workflows, we aim to automate workflow planning given high-level goals and scientist intuition. To this end, we propose Materials Agent unifying Planning, Physics, and Scientists, known as MAPPS. MAPPS consists of a Workflow Planner, a Tool Code Generator, and a Scientific Mediator. The Workflow Planner uses large language models (LLMs) to generate structured and multi-step workflows. The Tool Code Generator synthesizes executable Python code for various tasks, including invoking a force field foundation model that encodes physics. The Scientific Mediator coordinates communications, facilitates scientist feedback, and ensures robustness through error reflection and recovery. By unifying planning, physics, and scientists, MAPPS enables flexible and reliable materials discovery with greater autonomy, achieving a five-fold improvement in stability, uniqueness, and novelty rates compared with prior generative models when evaluated on the MP-20 data. We provide extensive experiments across diverse tasks to show that MAPPS is a promising framework for autonomous materials discovery.", "tldr": "", "keywords": ["AI4Science", "LLM agents", "Materials Science Discovery"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8bb404ad484c0bbd9ddb16d0557760753dc0514e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper Materials Agent unifying Planning, Physics, and Scientists, known as MAPPS, for greater autonomy in material discovery. Different from prior works which rely on predefined step-by-step procedures, MAPPS has the autonomy to determine the sequence of actions required to achieve discovery objectives based on the high-level goals and scientific intuition. Meanwhile, MAPPS is designed to be physics-informed and include human experts in the loop, which enriches the agent’s scientific knowledge beyond textual data, mitigates risk, and allows expert guidance to influence the agent’s decisions. Extensive experiments show five-fold performance improvement in material discovery tasks compared to prior generative models, demonstrating that MAPPS is a promising framework for autonomous materials discovery."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- MAPPS uses a “Scientific Mediator” that enables human intuition and review, successfully involving human in the loop and enhancing scientific fidelity.\n\n- The authors define three levels of autonomy for agents in materials science, characterized by the agent’s freedom in planning workflows. This helps the audience understand the difficulty of tasks and provides guidance for future development for LLM agents. \n\n- The Tool Code Generator can automatically revise and regenerate python code in response to runtime errors, enabling autonomous debugging and more robust workflow execution."}, "weaknesses": {"value": "- The framework retrieves and reconstructs structures via ASE and MLFF relaxation rather than learning a generative distribution, hence we cannot draw any insight from the comparison with generative models. A more meaningful evaluation would measure its efficiency and autonomy relative to human-designed or static workflows.\n\n- MAPPS lacks a formal shared memory to maintain key information accessible to agents, thus resulting in a higher risk of communication-related hallucinations. \n\n- The system cannot autonomously diagnose or resolve simulation failures like non-converged relaxations or unstable configurations, instead relying on manual intervention to correct and rerun failed steps. The Tool Code Generator can self-correct coding errors, but not scientific or numerical failures, which still need human oversight.\n\n- The proposed agentic modules (planner, code generator, mediator) are largely existing and have already been demonstrated in prior LLM agent works, offering limited architectural or algorithmic innovation. Meanwhile, the work does not yield any new understanding of materials science or agent reasoning—its contributions remain primarily at the level of system integration and benchmarking."}, "questions": {"value": "As discussed in section 4.4, the authors argue that current LLMs fundamentally cannot achieve true Level-3 scientific autonomy without human expertise. Could you elaborate on whether this limitation is viewed as intrinsic to model architectures, or primarily due to missing system components (e.g., persistent memory, convergence feedback, or hierarchical control)? In other words, do you see a pathway toward full autonomy, or do you believe human supervision will remain indispensable in the foreseeable future?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "aNicMbIu3m", "forum": "4yHJLiz8dm", "replyto": "4yHJLiz8dm", "signatures": ["ICLR.cc/2026/Conference/Submission13901/Reviewer_S644"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13901/Reviewer_S644"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760994893540, "cdate": 1760994893540, "tmdate": 1762924408934, "mdate": 1762924408934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAPPS, a multi-agent system for autonomous materials discovery that integrates workflow planning, physics-based simulation, and human-scientist feedback. The framework consists of a Workflow Planner, a Tool Code Generator using domain-specific physics APIs (e.g., CHGNet, ASE, Pymatgen), and a Scientific Mediator coordinating inter-agent communication and human input. The authors claim MAPPS achieves “Level 2” autonomy, beyond single tool execution, through coordinated reasoning and planning. Experiments on MP-20, Matbench, and MPTS-52 show improved material discovery metrics compared to generative and retrieval-based baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Clear modular design (planner, code generator, mediator) with interpretable roles.\n- Useful framing of autonomy levels for AI-for-Science systems.\n- Strong empirical results demonstrate gains on several benchmark datasets."}, "weaknesses": {"value": "- The related work section lacks depth and coverage of recent AI-for-Science agent systems [1, 2, 3]. It only briefly discusses Level-1 tool-executing systems without situating MAPPS among other emerging human-in-the-loop or planning-based frameworks. A broader comparison to general AI planning, reflective agents, and scientific RAG systems would strengthen the related work.\n- The framework designm, combining workflow planning, code generation, and a human mediator, closely resembles existing multi-agent or hierarchical architectures. The main novelty appears to be the “Level-2 autonomy” framing and inclusion of explicit human feedback, which may be more incremental than conceptual.\n- All baselines are domain-specific generative models (CDVAE, DiffCSP, FlowLLM, etc.), not comparable agentic systems. Without benchmarking against other agent frameworks such as LLaMP, SciAgents, or LLMatDesign, it is unclear whether MAPPS’s improvements arise from the agent structure or from task-specific heuristics and retrieval setups.\n- The role of the “human provision” and “scientific mediator” is not clearly quantified. It is unclear how frequently humans intervene, how long such feedback takes, or how much performance depends on expert input versus autonomous reasoning. This makes it difficult to assess the degree of true “Level-2 autonomy.”\n- The backbone model referred to as “LRM” (large reasoning model) is not specified, nor are its prompting strategies, temperature, or tool-calling interface. This omission limits reproducibility and interpretability of the results.\n- Although Tables 5 and 6 explore human ablations, there is limited qualitative analysis of typical failure modes (e.g., physically invalid workflows, failed code execution, or incorrect property predictions). Such examples would clarify the limits of current LLM planning.\n\n[1] Zhang, Huan, et al. Honeycomb: A flexible LLM-based agent system for materials science. arXiv preprint arXiv:2409.00135 (2024).\n\n[2] Chiang, Yuan, et al. LLaMP: Hierarchical multi-agent framework for scientific reasoning and materials modeling. arXiv preprint arXiv:2401.17244 (2024).\n\n[3] Ghafarollahi, Alireza, et al. SciAgents: Automating scientific discovery through multi-agent intelligent graph reasoning. arXiv preprint arXiv:2409.05556 (2024)."}, "questions": {"value": "- What exactly constitutes “human provision” in Table 6—prompting, correction, or code editing?\n- How do modules communicate, through structured messages, shared memory, or sequential prompting?\n- How sensitive is MAPPS to the human scientist’s expertise level?\n- What is the one-shot success rate of the workflow planner before human feedback?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cygsEWVsni", "forum": "4yHJLiz8dm", "replyto": "4yHJLiz8dm", "signatures": ["ICLR.cc/2026/Conference/Submission13901/Reviewer_821m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13901/Reviewer_821m"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940864798, "cdate": 1761940864798, "tmdate": 1762924408346, "mdate": 1762924408346, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MAPPS (Materials Agents unifying Planning, Physics, and Scientists), a multi-agent framework for autonomous crystal materials discovery. The system consists of three components: a Workflow Planner that generates multi-step workflows using LLMs, a Tool Code Generator that synthesizes executable Python code incorporating physics-based tools (e.g., ML force fields), and a Scientific Mediator that coordinates human-agent interaction. The authors define three levels of agent autonomy and position MAPPS at Level 2 (human-guided planning). Experiments on crystal generation, structure prediction, and property-guided generation tasks show improvements over existing generative models, with a five-fold improvement in stability, uniqueness, and novelty rates on MP-20 data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses an important problem of enabling greater autonomy in computational materials discovery, and the formulation of three levels of autonomy provides a useful framework for understanding agent capabilities in scientific domains.\n\n- The experimental evaluation is comprehensive, covering multiple realistic tasks (crystal generation, structure prediction, property-guided generation) across several benchmark datasets, with appropriate metrics including DFT validation."}, "weaknesses": {"value": "- The novelty appears limited since the system primarily orchestrates existing components (LLMs for planning, existing physics tools, human feedback loops) without significant algorithmic innovation, and it seems the main contribution is engineering a working system rather than advancing fundamental understanding of autonomous agents.\n\n- The comparison with baselines is not entirely fair because MAPPS uses a fundamentally different paradigm (retrieval + refinement with human-in-the-loop) compared to end-to-end generative models, making it difficult to isolate whether improvements come from the agent framework itself or simply from having access to database retrieval and human guidance."}, "questions": {"value": "Could you provide an ablation study comparing MAPPS without retrieval access versus baselines with retrieval access to isolate the contribution of the agent framework from the database retrieval component?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uNFk0W3Idb", "forum": "4yHJLiz8dm", "replyto": "4yHJLiz8dm", "signatures": ["ICLR.cc/2026/Conference/Submission13901/Reviewer_Xb8S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13901/Reviewer_Xb8S"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964684614, "cdate": 1761964684614, "tmdate": 1762924407811, "mdate": 1762924407811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes an LLM-based multi-agent framework MAPPS for autonomous materials discovery. The authors position their work as advancing from Level 1 autonomy of tool-executing agents to Level 2 autonomy for human-guided planning agents. The framework consists of a Workflow Planner that decomposes high-level goals into multi-step workflows, a Scientific Mediator that coordinates execution with human oversight, and a Tool Code Generator that translates workflow steps into code that calls physics tools."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Multiple tools are being integrated in the work to perform materials property evaluation and structure optimization, including CHGNet, M3GNet and DFT.\n2. The multi-agent modular architecture is well designed to allow for different task types in the same framework by adjusting high-level goals and human guidance."}, "weaknesses": {"value": "1. The demonstrated results are too weak to justify the complexity of the agentic system design. The marginal improvements on the refined MP-20 and MPTS-52 for CSP failed to demonstrate the value of contributions. The task itself does not clearly require the multi-agent planning architecture, which adds unjustified complexity.\n2. A lot of the critical implementation details are missing, including: the specific parameters and input/output handling when prompting the multiple LLMs for different tasks, any detail for human interaction with the Scientific Mediator, the exact format of information being passed between agents, e.g. $r_t$.\n3. The Scientific Mediator is described as a \"lightweight human-in-the-loop mechanism.\" But the system requires human supervision and feedback at multiple steps which would become a bottleneck, e.g. expert-level human intuition at the beginning, explicit approval of generated workflows before execution, etc. \n4. The paper provides almost no ablation studies on the system design components."}, "questions": {"value": "1. In Section 2.1, DFT calculation is mentioned as one of the tool-executing agents in the proposed workflow. But the only experiments with DFT I can see is an extra evaluation step for the 500x2 generated structures, which do not provide feedback to any of the agents in the workflow. Is DFT considered part of the workflow? If so, how is the tool-call execution time handled considering it can take days?\n2. How is the validity evaluated exactly in Table 5 for the three tasks? What is the specific human intuition in the tasks? Any analysis into why O3-mini achieving perfect 100% validity in CSG? How many examples tested? How are the input and output handled fro each task?\n3. Section 2.4 described the Scientific Mediator with no details into its implementation. What exactly are the \"human intuition\"? How are they collected and from whom? Any analysis into its format and quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AHzCr3qDJP", "forum": "4yHJLiz8dm", "replyto": "4yHJLiz8dm", "signatures": ["ICLR.cc/2026/Conference/Submission13901/Reviewer_ygxH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13901/Reviewer_ygxH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13901/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762112835776, "cdate": 1762112835776, "tmdate": 1762924407275, "mdate": 1762924407275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}