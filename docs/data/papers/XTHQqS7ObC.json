{"id": "XTHQqS7ObC", "number": 8037, "cdate": 1758053727893, "mdate": 1763768464920, "content": {"title": "Proximal Diffusion Neural Sampler", "abstract": "The task of learning a diffusion-based neural sampler for drawing samples from an unnormalized target distribution can be viewed as a stochastic optimal control problem on path measures. However, the training of neural samplers can be challenging when the target distribution is multimodal with significant barriers separating the modes, potentially leading to mode collapse. We propose a framework named **Proximal Diffusion Neural Sampler (PDNS)** that addresses these challenges by tackling the stochastic optimal control problem via proximal point method on the space of path measures. PDNS decomposes the learning process into a series of simpler subproblems that create a path gradually approaching the desired distribution. This staged procedure traces a progressively refined path to the desired distribution and promotes thorough exploration across modes. For a practical and efficient realization, we instantiate each proximal step with a proximal weighted denoising cross-entropy (WDCE) objective. We demonstrate the effectiveness and robustness of PDNS through extensive experiments on both continuous and discrete sampling tasks, including challenging scenarios in molecular dynamics and statistical physics.", "tldr": "", "keywords": ["Neural sampler", "proximal gradient descent", "diffusion models", "discrete diffusion models", "cross entropy"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d8ab08aa5833a37b780ac5112352a153e1457306.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work proposes the “Proximal Diffusion Neural Sampler”. The authors consider the setting of sampling from distribution with given unnormalized densities on both discrete and continuous state spaces. Rather than solving the SOC/sampling problem in one step, they propose an iterative procedure where an intermediate set of path measures are approximated, starting with a simple path measure and a final target path measure. It is shown that these intermediate path measure correspond to a geometric annealing in path space (proposition 3.1). Every step of the optimization problem is solved with proximal WDCE, a variant of WDCE designed for the proximal setup of this work."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Novel idea that is well-motivated with a technically good presentation.\n- A nice and simple unifying framework of existing neural sampler methods. They consider both discrete and continuous setting in one (instead of writing two separate papers, one general principle is presented).\n- Great illustration of mode collapse (section 3.1) motivating the work.\n- Experiments show improvements that illustrate the theoretical innovations."}, "weaknesses": {"value": "- For a less-experienced reader, the paper would potentially be hard to follow. I recommend including more sentences explaining intuitions and motivations at intermediate paragraphs.\n- The experiments are only on simple distributions. While this unfortunately common in this line of literature, a more complex setting of higher-dimensional distributions would have been more convincing."}, "questions": {"value": "- Line 72: “path space, which contains all functions from [0, T] to X”. A path measure is only defined for a subset of functions usually (there is a measurable space, etc.)\n- The ESS for the LEAPS algorithm reported here is significantly lower than report in their work. Comparing the setting, you have slightly changed the settings (slightly different grid size) or q=4 instead of q=3 for the Potts model. Why did you change this setting? It would be more compelling if you compare to exactly the same parameters than their work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BptwPKe5wh", "forum": "XTHQqS7ObC", "replyto": "XTHQqS7ObC", "signatures": ["ICLR.cc/2026/Conference/Submission8037/Reviewer_h5wm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8037/Reviewer_h5wm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760560725799, "cdate": 1760560725799, "tmdate": 1762920032176, "mdate": 1762920032176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We would like to convey our sincere gratitude to all reviewers for carefully reading our paper and providing thoughtful comments and constructive feedback. We are glad to hear that the reviewers acknowledged the novelty (h5wm) and contribution (Kemc) of this work, *unanimously* agreed on the general applicability of our proposed methodology, commented that the paper is clean (ptdZ, prWn), easy to follow (Kemc, prWn), and well-motivated (h5wm) for solving a common practical issue (Kemc). Finally, the reviewers found most of the experimental results convincing (ptdZ, prWn, h5wm). Our paper proposed a unified framework for stable and efficient training of diffusion-based neural samplers through proximal gradient descent in the space of path measures, which we believe would be of significant interest to the community, and hopefully, could open up new avenues for future research.\n\nThe main concerns raised by the reviewers are summarized as follows:\n\n1. Insufficient quantification of the computational complexity of the proposed method, due to the outer loops for solving each subproblem. (ptdZ, Kemc)\n\n2. Complexity of notations regarding the path measures and insufficiency of explanations that may pose challenges for the general audience. (Kemc, h5wm)\n\nMoreover, the reviewers ptdZ and Kemc also raised questions regarding the memoryless assumption on the reference path measure, which is an interesting point for further discussion and future work.\n\nWe will address specific concerns raised by the reviewers point-by-point in the individual response. Please also see the updated manuscript for changes made in response to the reviews, which are highlighted in $\\color{orange}{{\\rm orange}}$ color. Notable changes include: \n\n1. We included further experimental results and ablation studies in Apps. B (weight-based v.s. resampling-based realization of WDCE), Sec. 5 (alanine dipeptide), and E.3 (ablation studies on discrete target distributions).\n\n2. We added a table after Prop. 3.1 to summarize the meaning of different path measures used in the paper, and polished the related discussions in Secs. 3 and 4 for better readability.\n\n3. We modified the proof of (3), the construction of optimal path measure $\\mathbb{P}^*$, and included a discussion to highlight the purpose of the memoryless assumption of the reference path measure $\\mathbb{P}^\\text{ref}$.\n\nWe sincerely hope that the revisions and responses adequately address the reviewers' concerns and show our commitment to improving the quality of our work. We also welcome any further questions or comments from the reviewers."}}, "id": "MnnXVEMmUe", "forum": "XTHQqS7ObC", "replyto": "XTHQqS7ObC", "signatures": ["ICLR.cc/2026/Conference/Submission8037/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8037/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8037/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763767924479, "cdate": 1763767924479, "tmdate": 1763767924479, "mdate": 1763767924479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Proximal Diffusion Neural Sampler (PDNS) to learn a neural sampler for sampling from unnormalised density functions.   Both continuous and discrete formulation is proposed and verified in this paper.\nThe paper argus that the reason behind model collpasing is that the sampling task is too far from the prior and hence PDNS breaks down the entire problem with proximal point method, progressively driving the neural sampler towards the target.\nThis proposed method enriches the family of neural samplers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clearly written and easy to follow. It is complete and well structured. It covers both discrete and continuous domains and presents results for each. The proposed algorithm is interesting and offers insights into mode collapse. The empirical evaluation indicates that the approach is promising."}, "weaknesses": {"value": "While the paper is motivated by mitigating mode collapse, the experimental section does not convincingly demonstrate this. Several benchmarks (e.g., LEAPS) do not appear to exhibit severe collapse. Do you have any explanation to this?\n\n\nOn line 780: \n> Blessing et al.  (2025) couple CE training... In contrast,PDNS utilize\n proximal algorithm with WDCE bjective,which can be applied uniformly to continuous-time\n and discrete-time diffusions,and is designed to be memory-efficient while retaining the benefits of\n denoising or score-matching objectives. \n\nThe mentioned algorithm has quite similar formulation and has strong connections. Blessing et al.  (2025)  works on continuous. But to my knowledge, it should also be able to work on discrete domain. Is this the main difference between PDNS and their approaches? Or is my understanding wrong?"}, "questions": {"value": "See my weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "C6dZb67mOq", "forum": "XTHQqS7ObC", "replyto": "XTHQqS7ObC", "signatures": ["ICLR.cc/2026/Conference/Submission8037/Reviewer_prWn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8037/Reviewer_prWn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814769842, "cdate": 1761814769842, "tmdate": 1762920031663, "mdate": 1762920031663, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Proximal Diffusion Neural Sampler proposes a method an stochastic optimal control (SOC) sampler that learns a target distribution based on having an energy function alone. It is an extension of the current developments in this area, specifically by applying the proximal point method on the space of path measures. They provide a method that works for both discrete and continuous cases. The end result is an algorithm where one can do a variable amount of optimization problems that empirically (and theoretically) converge to the target distribution without mode collapse. Essentially, a computationally tractable method for \"forward-KL\" solving with SOC.\n\nIn general, the contribution is strong and fairly easy to follow given the complexity (although I suggest improvements below). It seems applicable with fairly good results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "# overall\n- problem setting is well identified and known to be a practical issue\n- background is well explained and covers both discrete and continuous cases\n- In general, the key results from a hard topic are well presented... (The extensions from the paper need some notation work, in my opinion).\n- The method introduces a scheme of iterates to help exploration, including a proposed scheduler that dynamically decides the number of iterations based on a bounded KL term. This is nice and well motivated.\n\n# results\n- results on non-particle continuous models are very good.\n- results seem fairly good on particle models\n- results are strong on the discrete models"}, "weaknesses": {"value": "# clarity in actual objective\n- From lines 298 - 318 the paper does not make it extremely clear how the actual objective is computed. There are a number of identities used, in particular the right part of (15) that do not seem clearly motived by other equations in the text.\n- Similarly, about clarity, You use so many version of $\\mathbb{P}$ it becomes difficult to compare them. In particular the most confusing are the non-parametric ones: P^k, P^k*. P^k is a non-optimal, non-parametric path measure? It's not obvious why we even need this thing or how it is defined. Maybe it would be worth introducing alternative notation of P to help keep everything straight.\n- ... Similarly, (16) refers to a ratio of path measure derivatives that does not appear in the discrete objective on 322-323. Can you please write it out?\n\n# cost\n- can you better explain the cost of training your model versus others? Is it k-times as expensive?\n\n# unclear about some parts of method\n- ASBS, for example, is extremely hyperparameter sensitive. How about your method?\n- You use a memoryless path, correct? How does this interact with your optimization problem?\n- Can you use a non-memoryless distribution, e.g. like ASBS does?"}, "questions": {"value": "# results\n- can you please plot the energy histogram for the particle problems? It tells us much more than W2 and E() W2.\n- can you please work on notation in the methods you are proposing a bit? I know they're all path measures...\n- I know it's a lot to ask, but is there any molecular task you can apply this to? Alanine Dipeptide for example? Strong contender ASBS misses important modes, if your method gets them that would be great!\n- Can you better quantify how much excess training cost is required to solve the k proximal problems? Does it really slow everything down by a lot?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "s6ada3IuVN", "forum": "XTHQqS7ObC", "replyto": "XTHQqS7ObC", "signatures": ["ICLR.cc/2026/Conference/Submission8037/Reviewer_Kemc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8037/Reviewer_Kemc"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915833893, "cdate": 1761915833893, "tmdate": 1762920031118, "mdate": 1762920031118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses general-purpose sampling by simulating diffusion inference trajectories in both continuous and discrete settings. A central challenge is that fitting the noisy target distribution with a neural network often leads to mode collapse and training instability. To mitigate these issues, the authors augment standard SOC objectives (e.g., CE, WDCE) with a KL-regularized local objective, yielding a proximal point method in path space. They instantiate this “path-measure proximal point” framework (PDNS) for both continuous SDEs and discrete CTMC diffusion samplers, demonstrating its generality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proximal-in-path-measure perspective is clean and broadly applicable across continuous SDE and discrete CTMC settings. In particular, the proximal WDCE naturally tempers importance weights, directly addressing mode collapse and high-variance gradients. \n2. The paper presents the SOC, CE, WDCE, proximal formulations, and Girsanov/CTMC Radon–Nikodym weights with clarity. Practical guidance (e.g., OU reference bridges, conditional score formulas, and step-size schedulers) facilitates reproducibility.\n3. Experiments span synthetic multimodal and high-dimensional mixtures, challenging particle systems (LJ-13/55), discrete Ising/Potts models near criticality, and combinatorial max-cut. PDNS achieves state-of-the-art or near-state-of-the-art results on multiple tasks, measured by domain-relevant metrics (Sinkhorn/MMD, $W_2$, energy $W_2$, magnetization, two-point correlations)."}, "weaknesses": {"value": "1. Proximal WDCE introduces overhead from replay buffers, weight computation (Girsanov/CTMC), and resampling. While results are strong, the paper does not quantify compute and memory costs relative to baselines (e.g., wall-clock, NFE, memory footprint), especially on the largest tasks (LJ-55, Potts). Resource-normalized comparisons would clarify efficiency. \n2. Many derivations rely on independence between initial and terminal distributions (Eq.~(2)). The framework assumes (nearly) memoryless references to obtain closed forms and reciprocal sampling, which may be unavailable for some targets. Although non-memoryless extensions are mentioned in related work, PDNS is not analyzed under such references. \n3. The proximal step size $\\eta_k$ (or $\\lambda_k$) critically trades off stability and speed. Despite an adaptive KL-based scheduler, the paper lacks systematic sensitivity studies and practical heuristics for choosing $\\epsilon$ across tasks and scales. Additional ablations on $\\eta_k$, buffer size, and resampling vs.\\ weighting would aid practitioners.\n\nIf the authors resolve my concerns, I am willing to raise my rating."}, "questions": {"value": "1. Did authors consider using the Wasserstein two distance as a regularizer to replace the KL divergence between $p^{\\theta}$ and $p^{\\theta_{k-1}}$, by coupling the JKO scheme, the proximal problem formulation may be more intuitive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tV3behGl4n", "forum": "XTHQqS7ObC", "replyto": "XTHQqS7ObC", "signatures": ["ICLR.cc/2026/Conference/Submission8037/Reviewer_ptdZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8037/Reviewer_ptdZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8037/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964730012, "cdate": 1761964730012, "tmdate": 1762920030352, "mdate": 1762920030352, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}