{"id": "aiZG7mRi43", "number": 13281, "cdate": 1758215961464, "mdate": 1759897448973, "content": {"title": "MultiHaystack: Benchmarking Multimodal Reasoning over 40K Images, Videos, and Documents", "abstract": "Multimodal large language models (MLLMs) have advanced rapidly on benchmarks involving isolated text, image, or video tasks, but such settings overlook a crucial step in real-world applications: retrieving evidence from large, heterogeneous corpora before reasoning. Existing benchmarks typically provide only hundreds or thousands of candidates, making retrieval trivial and overstating model reliability. To address this gap, we introduce MultiHaystack, the first benchmark for large-scale, realistic cross-modal retrieval and reasoning. It contains over 46,000 documents, images, and videos paired with 747 uniquely verifiable questions, ensuring unambiguous evaluation while requiring both modality selection and fine-grained reasoning. Our experiments reveal a consistent pattern: models perform competitively when directly given the answer-containing file, but their performance drops sharply once evidence must be retrieved at scale. The best retriever (E5-V) achieves only 40.8% Recall@1, while even GPT-5 reaches just 51.4% VQA accuracy under top-5 retrieval. These results reveal that retrieval, rather than reasoning, is the dominant bottleneck, establishing MultiHaystack as a rigorous benchmark that exposes weaknesses hidden by small-scale evaluations and highlights retrieval as the key frontier for advancing MLLMs.", "tldr": "A benchmark used for benchmarking MLLMs on Large-Scale (over 40K) Cross-Modal Retrieval and Reasoning", "keywords": ["Multimodal Haystack", "Retrieval", "VQA"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c0708f65b5b3e1297d7c52dc73308103288fdbc3.pdf", "supplementary_material": "/attachment/f95442e1482372048e3a3242e3432dd2e7851b4e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduce a new benchmark MultiHaystack for cross-modal  retrieval and reasoning, 46K documents, images, and videos. The authors curated their benchmark from existing datasets and then evaluated popular VLM models on their benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduce MultiHaystack, the first large-scale benchmark for realistic cross-modal retrieval and reasoning, spanning 46K documents, images, and videos.\n2. The  paper conducted comprehensive experiments on 5 state-of-the-art MLLMs."}, "weaknesses": {"value": "1. The paper is not very clear, see the questions below.\n2. This benchmark is curated from a combination of several other benchmarks.\n3. Lack of significant experimental findings from this benchmark."}, "questions": {"value": "1. What the composition of the retrieval corpus?\n2. What are the retrieved informations, eg videos, text and images? How did the paper guarantee the retrieved context are useful?\n3. Does this benchmark provide ground truth retrieval data? \n4. How did the ground truth bounding boxes collected?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0sifBtpZic", "forum": "aiZG7mRi43", "replyto": "aiZG7mRi43", "signatures": ["ICLR.cc/2026/Conference/Submission13281/Reviewer_nMyE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13281/Reviewer_nMyE"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459407358, "cdate": 1761459407358, "tmdate": 1762923956183, "mdate": 1762923956183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MultiHaystack, a large-scale multimodal retrieval and QA benchmark designed to evaluate realistic retrieval and reasoning capabilities of MLLMs. The benchmark comprises over 46,000 multimodal items, including text, image, and video documents with 747 uniquely verifiable questions that demand both accurate modality selection and fine-grained reasoning. Unlike existing benchmarks with limited candidates, MultiHaystack focuses on long-context, large-scale retrieval, better reflecting real-world challenges. Experimental results demonstrate that models perform well when provided with relevant evidence but show a significant performance drop when retrieval is required. Moreover, cross-modal retrieval lags far behind single-modal performance, indicating retrieval is the primary bottleneck."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental design and analysis are comprehensive, evaluating both the multimodal retrieval capabilities of retrieval models and the reasoning abilities of MLLMs under conditions with and without retrieval.\n2. The proposed benchmark is the first long-context benchmark covering text, image, and video modalities, representing a clear and valuable contribution to benchmark design and innovation."}, "weaknesses": {"value": "No significant weaknesses are identified. \n\nThings to improve the paper that did not impact the score:\n\nThe open-source MLLMs used in the paper are not the latest models (e.g., InternVL-3.5 or Qwen2.5-VL). However, considering that some of these newer models were released very close to or after the submission date, the results reported in the paper remain acceptable."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VY31l1aeH0", "forum": "aiZG7mRi43", "replyto": "aiZG7mRi43", "signatures": ["ICLR.cc/2026/Conference/Submission13281/Reviewer_cRtU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13281/Reviewer_cRtU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660530824, "cdate": 1761660530824, "tmdate": 1762923955911, "mdate": 1762923955911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a new challenging benchmark that containing large-scale collection of images, videos and documents. The benchmark converts pre-existing corpus and annotate QA pairs and evidence via multi-stage pipelines. The benchmark first tests the retriever's capability to identify the correct relevant item among the three types of evidence. Then, reasoning capability is evaluated on the VQA tasks. Author conducts comprehensive experiments by benchmarking current retrievers and multimodal LLMs for retrieval and QA performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Propose a novel and challenging benchmark containing different types of evidences\n- Benchmark various retriever and LMMs on fine-grain categories to help understand performance gap."}, "weaknesses": {"value": "- Though proposing a new benchmark, and evaluating many methods' performance, this work does not introduce a novel methods to tackle this cross-modal (i.e., as referred by the author, where the retrieval pool contains image, videos and documents) retrieval tasks specifically. Could the author provide more insights regarding how to improve on these specific challenge settings?\n- Title slightly misleading, as the paper focuses largely on the importance of retrieval, but the title focuses on reasoning instead.\n- Line 294, Sec4.2, and Table 2 regarding single modality and cross-modality are quite confusing. Although I do understand that the single-modality refers to the fact that when the retrieval database contains only one type of data, say, the video. The author should still make sure this is specifically described in the paper. Furthermore, I wonder if this is actually a fair comparison when the author compares the cross-modality vs single-modality and argues that single-modality is less challenging. I suspect this could simply due to the fact that the retrieval pool is smaller and thus easier, rather than the fact of the \"cross-modality\" and \"single-modality\" itself. \n- Given that both the reasoning and retrieval performance are uneven for each tasks, could you provide more intuition regarding how to tackle each tasks, or how to tackle the tasks overall?"}, "questions": {"value": "- While I understand page/frame level vs item level, but I still believe this should be clarified (linme 262)\n- Which retrieval system's results are used for Table 3 (Line 312, Sec4.3)?\n- I agree that split the report of metric into different sub-tasks helps, but sitll For Table 4 and Table 5, I suggest add the overall results across all tasks as well. \n- Overall, I believe the benchmark could be interesting to the community as it does merge three types of data for retrieval together and become challenging for previous retriever. However, the paper does not provide enough insights into how to better tackle this real-world task, for example, could the author try agentic search/retrieval agents or simply query rewriting, or simple retrieval database (keys) augmentation to boost performance over a naive retriever model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OnGzRp96Mf", "forum": "aiZG7mRi43", "replyto": "aiZG7mRi43", "signatures": ["ICLR.cc/2026/Conference/Submission13281/Reviewer_93AH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13281/Reviewer_93AH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777981610, "cdate": 1761777981610, "tmdate": 1762923955373, "mdate": 1762923955373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MultiHaystack, a 46K-item corpus of documents, images, and videos coupled with 747 questions that target cross-modal retrieval-plus-reasoning at scale. It is designed to address common gaps in prior work, small candidate pools, single-modality focus, and loosely specified questions by ensuring that every query is evidence-grounded with a uniquely verifiable answer. The construction pipeline (i) aggregates videos, images, and documents from existing datasets; (ii) converts PDFs to images and samples each video into 8 frames, then uses GPT-4o to draft QA pairs; (iii) filters ambiguous items with GPT-4o and Gemini-2.5-Flash, conducts manual review to discard cases lacking explicit visual/textual anchors, and applies a retrieval-independence test to remove questions solvable without consulting the linked item; and (iv) increases retrieval difficulty via targeted distractors. The final set contains 747 questions spanning 433 images, 105 videos, and 209 documents."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear, well-structured writing.\n2. Effective figures that make the setup, pipeline, and findings easy to follow.\n3. Each query is validated with retrieval models and human checks to enforce unique, evidence-grounded answers.\n4. Careful analysis of performance variation across task types and the associated bottlenecks.\n5. Sensible LM-as-Judge validation, including consistency checks between GPT-4o-mini and human annotations.\n6. Comprehensive qualitative examples and a detailed appendix."}, "weaknesses": {"value": "1. LLM dependence in data construction. Because GPT-4o/Gemini-2.5-Flash generate and filter the QA pairs, dataset quality and distribution inherit these models’ biases and preferences.\n2. Question realism (especially for video frames). Several examples feel overly specific for web-scale retrieval, e.g., “What kind of personal item is on the face of the girl who is applying dye to her hair?” Such prompts seem better suited to within-video search over many frames rather than a cross-modal, cross-corpus retrieval scenario.\n3. Distractor design may inflate difficulty in an unnatural way. For instance, asking “What product model is visible on the VP Racing Fuels canister next to the laptop in the workshop setting?” and then seeding many near-miss distractors across a huge pool can feel optimized to depress retrieval metrics rather than reflect realistic user queries. The benchmark risks being optimized for failure rather than for real-world retrieval relevance. This imposes an unnaturally precise retrieval challenge that may overstate the difficulty compared to authentic multimodal information-seeking behavior."}, "questions": {"value": "1. Gold-in-Top-1 gap (Fig. 6). How do you explain cases where models like GPT-5 and Gemini-2.5-Flash fail to achieve perfect accuracy even when the gold item is ranked first? If the QA was generated/filtered by the same or earlier models, what prevents them from answering correctly when the evidence is already surfaced?\n2. Have you considered incorporating a human-authored subset of QA pairs to calibrate the realism and linguistic variety of the dataset, thereby mitigating the dependency on generative model biases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "inI00CkQgI", "forum": "aiZG7mRi43", "replyto": "aiZG7mRi43", "signatures": ["ICLR.cc/2026/Conference/Submission13281/Reviewer_MoTo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13281/Reviewer_MoTo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13281/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762132946948, "cdate": 1762132946948, "tmdate": 1762923954618, "mdate": 1762923954618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}