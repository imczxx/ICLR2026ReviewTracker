{"id": "h9kXyRWZGQ", "number": 23133, "cdate": 1758340039335, "mdate": 1759896831032, "content": {"title": "Sublinear iterations can suffice even for DDPMs", "abstract": "SDE-based methods such as denoising diffusion probabilistic models (DDPMs) have shown remarkable success in real-world sample generation tasks. Prior analyses of DDPMs have been focused on the exponential Euler discretization, showing guarantees that generally depend at least linearly on the dimension or initial Fisher information. Inspired by works in log-concave sampling (Shen & Lee, 2019), we analyze an integrator -- the denoising diffusion randomized midpoint method (DDRaM) -- that leverages an additional randomized midpoint to better approximate the SDE. Using a recently-developed analytic framework called the \"shifted composition rule\", we show that this algorithm enjoys favorable discretization properties under appropriate smoothness assumptions, with sublinear $\\widetilde{O}(\\sqrt{d})$ score evaluations needed to ensure convergence. This is the first sublinear complexity bound for pure DDPM sampling --- prior works which obtained such bounds worked instead with ODE-based sampling and had to make modifications to the sampler which deviate from how they are used in practice. We also provide experimental validation of the advantages of our method, showing that it performs well in practice with pre-trained image synthesis models.", "tldr": "", "keywords": ["DDPM", "randomized midpoint", "convergence guarantees"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c5433f551187e163bda53fd6da30d09d8eed1477.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper present an analysis of a stochastic integrator for the DDPM SDE.\nIt is based on an intermediate random middle point that is used to estimate evaluate the score in place of the initial point of the time interval.\nA sublinear convergence theorem is proven: the bound is in KL divergence towards an intermediary distribution that is close to the data distribution in Wasserstein distance.\nNumerical experiments tend to support the superiority of the proposed sampler."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The proposed sampler is sound and rely on previous literature.\nTheorem 3 provides sublinear complexity bound (towards a somewhat obscure intermediate probability).\nThe appendix material for the proof of Theorem 3 seems well-written and documented (did not check the proof)."}, "weaknesses": {"value": "From the abstract one can read \"prior works which obtained such bounds worked instead with ODE-based sampling and had to make modifications to the sampler which deviate from how they are used in practice.\" \nA similar claim line 304 \"This is an option that we cannot afford in this work, as our goal is to simply analyze a discretization of the vanilla DDPM reverse process without further algorithmic modifications.\" \nBut the proposed work studies Algorithm 1 that: \n* requires two score evaluation per iteration (OK but should be hilighted)\n* is proven convergent using some specific decaying step size only discussed in Appendix (Equation A.2 line 954)\n* In addition, the convergence is only proven through the use of an intermediate distribution $\\pi^{\\mathrm{approx}}$, with a mixed role for KL divergence and $W_2$-distance (see discussion line 284).\n\nDue to the difference in sampling schemes, the comparison experiments in Section 5 lack clarity.\nFor the OU process, what are the step size used for EMD and EED?\nFor RMD, is it the step size from Equation (A.2)? \nWhy isn't this choice discussed in the main paper?\nWhy figure 1 stops at 64 NFEs while standard DDPM would use 1k or 2k steps (Ho et al 2020, Song et al 2021)?\nWhy is there no comparison with a predictor-corrector scheme that is computationally closer (two score evaluations and two Gaussian noise per iteration)?\n\n\"Figure 3: Quantitative results: Deterministic sampling:\" Is RMD deterministic?\n\nMinor remarks: \n* the presentation of Equation RMD could be made more consistent with Algorithm 1 by using $t_{k-1}+\\tau_k$ instead of $t$\n* DDRaM not defined in the main text (but in abstract)\n* Figure 4 and 5 are in the appendix, which is not clear when reading the text line 463 and 470.\n* line 744: $Y^{\\mathrm{aux}}$ or $Y^{\\mathrm{alg}}$ ?\n* line 755: Thenn"}, "questions": {"value": "See questions in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rrlQOxK7li", "forum": "h9kXyRWZGQ", "replyto": "h9kXyRWZGQ", "signatures": ["ICLR.cc/2026/Conference/Submission23133/Reviewer_qw3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23133/Reviewer_qw3a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761775149516, "cdate": 1761775149516, "tmdate": 1762942525738, "mdate": 1762942525738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the Denoising Diffusion Randomized Midpoint (DDRaM) method, a new SDE-based integrator for diffusion models that achieves sublinear $\\sqrt{d}$ computational complexity in score evaluations under smoothness assumptions. Using the shifted composition rule, the authors provide the first theoretical sublinear convergence guarantee for pure DDPM sampling."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "(1) The paper analyzes a stochastic sampler with the random point method, and proves that the KL and $W_2$ divergence can be controlled with iteration complexity that has a sublinear dependence on the dimension $d$.\n\n(2) It conducts experiments to demonstrate the superiority of the randomized point method."}, "weaknesses": {"value": "(1) The equations for reverse SDE seem incorrect. It can be seen from related works, e.g., [1], that there is no $\\gamma$ there.\n\n(2) The keyword, DDPM, is not very accurate. Actually, it should be termed as sampling diffusion with stochastic samplers, or SDE. This is because DDPM is only a special case of score-based diffusion when taking the limit in the length of time steps. However, the denoising diffusion model introduced in this paper is more related to the score-based SDE, instead of the original DDPM paper.\n\n(3) The argument regarding why we approximate some distribution $\\pi_{approx}$ is not convincing. The readers compare it with $\\pi_{\\delta}$ with early stopping. However, this is because when $\\delta$ is small, $\\pi_{\\delta}$ is at least close to $\\pi_0$ in $W_2$ distance. To make your argument reasonable, at least some similar $W_2$ guarantee should be provided. In contrast, in the main text, how it is defined is never specified. When I check the proof (Lemma 6), I find that this distribution even depends on the transition kernel of $P_{alg}$. As a result, the kind of guarantee is very unusual. And the comparison with prior works under this metric is unfair.\n\n(4) The paper misses a closely related work [2] when introducing the analysis of DDIM.\n\n---\n\n[1] Nearly d-linear Convergence Bounds For Diffusion Models Via Stochastic Localization Benton et al., 2024 ICLR\n\n[2] Unified Convergence Analysis for Score-Based Diffusion Models with Deterministic Samplers ICLR"}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CumYb6HpxL", "forum": "h9kXyRWZGQ", "replyto": "h9kXyRWZGQ", "signatures": ["ICLR.cc/2026/Conference/Submission23133/Reviewer_wzAK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23133/Reviewer_wzAK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761781986272, "cdate": 1761781986272, "tmdate": 1762942525457, "mdate": 1762942525457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an important theoretical result for SDE-based diffusion samplers (like DDPMs). Until now, the theoretical complexity for ODE-based samplers was well-studied, but the limit for SDE-based samplers was still O(d), which is linear to the data dimension d. This created a large gap from practice, where O(1) steps can make good samples. This paper uses a new analysis called Denoising Diffusion Randomized Midpoint Method (DDRaM) to prove for the first time that SDE-based DDPM samplers can achieve O(sqrt(d)) sublinear complexity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The biggest contribution is providing the first sublinear O(sqrt(d)) proof for DDPM (SDE). This is a very important step forward in diffusion model theory. Many prior works (including Li & Jiao, ICLR 2025) achieved sublinear complexity for ODE (DDIM), but they could not solve the problem for DDPM because of its stochasticity. This paper successfully fills this important theoretical gap.\n\n2. Besides the theory, the paper shows that the proposed DDRaM method works well in practice. In experiments on AFHQv2 (Figures 1, 2), DDRaM consistently shows better performance (lower FID, FD DINOv2) than standard samplers like Euler-Maruyama (EMD) or Exponential Euler (EED). This shows the proposed analysis is not just for theory but also has practical benefits."}, "weaknesses": {"value": "My only one concern is about the decreasing importance of the DDPM sampler itself. In practice, many researchers are trying to develop samplers with very small NFE (like DDIM, DPM-Solvers) to make generation faster. Or, they train the model differently from the beginning (like Consistency Models or Rectified Flow). It is clear that proving sublinear complexity for DDPM was a very difficult problem, but I am a little unsure if solving this problem is as important as prior works on DDIM, which is more widely used in practice."}, "questions": {"value": "In L1355 (Appendix C.2.1), you mentioned using numerical functions like `scipy.integrate.quad` and `scipy.optimize.root_scalar` for quadrature and root finding. Doesn't this add significant latency to the sampling process at each step?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SdjJqwCgIY", "forum": "h9kXyRWZGQ", "replyto": "h9kXyRWZGQ", "signatures": ["ICLR.cc/2026/Conference/Submission23133/Reviewer_NdkL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23133/Reviewer_NdkL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761825010428, "cdate": 1761825010428, "tmdate": 1762942525228, "mdate": 1762942525228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes the denoising diffusion randomized midpoint method (DDRaM) â€” an SDE-based integrator for denoising diffusion probabilistic models (DDPMs), inspired by log-concave sampling (Shen & Lee, 2019). Using the \"shifted composition rule\" framework, it shows DDRaM needs sublinear score evaluations for convergence (the first sublinear complexity bound for pure DDPM sampling. Experimental validation confirms DDRaM works well with pre-trained image synthesis models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- As far as I know, this is indeed the first $O(\\sqrt{d})$ order error bound for a stochastic sampler in the diffusion model area.\n- The paper is clearly written, and the idea is easy to follow."}, "weaknesses": {"value": "- In the paper of Shen & Lee (2019), their analysis requires the target distribution to be a log-concave one. I am not sure if the Assumptions 1,2,3 in this paper can lead to the conclusion that the marginal distribution will be log-concave. Or could you explain how you could surpass this condition?\n- The experiments validate the usage of the DDRaM method, but it does not involve popular stochastic samplers like DDPM itself, EDM-stochastic, PNDM-Stochastic, and DPM-Solver-Stochastic for comparison. Actually, I suppose these high-order stochastic samplers may also possess the property of requiring only sublinear score evaluations to ensure convergence. I encourage the authors to analyze these high-order samplers in practical use."}, "questions": {"value": "Please refer to Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "p9Dbf0ranH", "forum": "h9kXyRWZGQ", "replyto": "h9kXyRWZGQ", "signatures": ["ICLR.cc/2026/Conference/Submission23133/Reviewer_91Te"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23133/Reviewer_91Te"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23133/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978336071, "cdate": 1761978336071, "tmdate": 1762942523937, "mdate": 1762942523937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}