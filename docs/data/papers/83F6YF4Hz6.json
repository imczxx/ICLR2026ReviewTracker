{"id": "83F6YF4Hz6", "number": 7546, "cdate": 1758026978885, "mdate": 1759897847179, "content": {"title": "SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models", "abstract": "Progress in long-context reasoning for large language models (LLMs) has lagged behind other recent advances. This gap arises not only from the intrinsic difficulty of processing long texts, but also from the scarcity of reliable human annotations and programmatically verifiable reward signals. In this paper, we propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles—questioner, responder, and verifier—within a single model to enable continual self-improvement. The questioner generates questions from raw documents paired with reference answers; the responder learns to solve these questions based on the documents; and the verifier evaluates semantic equivalence between the responder’s output\nand the questioner's reference answer, producing reward signals to guide continual training. To stabilize training, we introduce an automated curriculum that gradually increases document length and a reward function that adapts question difficulty to the model’s evolving capabilities. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data. Notably, SPELL achieves an average 7.6-point gain in pass@8 on the strong reasoning model Qwen3-30B-A3B-Thinking, raising its performance ceiling and showing promise for scaling to even more capable models.", "tldr": "A label-free RL framework that drives the autonomous evolution of LLMs in long-context reasoning", "keywords": ["Self-Play", "Reinforcement Learning", "Long-Context Reasoning", "Large Language Models"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7933eaa510ffa3672fd112dcc3da15ac02fa7897.pdf", "supplementary_material": "/attachment/fc21b2fe618345f58d45e3a6a95ddc4eb5028124.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a self-play RL framework for training LLMs to do long-context reasoning. Since there is little human annotations for long-context reasoning questions, they design a framework which includes a questioner, a responder and a verifier. Each module is trained via RLVR with a specially designed reward function. Overall the experiment results demonstrate that their proposed SPELL method is able to improve the long-context ability of LLMs and surpass the vanilla RLVR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of long-context reasoning is very relevant to the field. The self-play framework which includes three well-specified roles for generating questions, generating responses and verifying responses is carefully designed with reasonable rewards.\n- An interesting design is the iterative curriculum which keeps generating more and more difficult questions. They demonstrate in experiment results that the increasing difficult questions are better than static questions in RLVR.  \n- Their experiments show that SPELL has consistent advantages over RLVR on most datasets and both long and short context settings. In addition, the verifier and the consistency rewards are useful for the overall model training."}, "weaknesses": {"value": "- The paper aims to improve long-context reasoning ability. The method is quite effective but seems more like a general RLVR training data generation method, and less relevant to the problem of long-context. It would be better to compare their SPELL method with at least one long-context alignment method other than only RLVR. For example, authors mentioned LongPO, SoLoPO and QwenLong-L1, which seems to be good baselines (and can be compared with SPELL given same documents as the starting point)."}, "questions": {"value": "- For the questioner reward, why do you use a Gaussian-shaped reward instead of the linear piecewise reward as in R-zero? Is there a specific reason that the Gaussian reward is better? Why do you set σ = 0.5/3 and are there any ablation studies on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6ugn3hHec0", "forum": "83F6YF4Hz6", "replyto": "83F6YF4Hz6", "signatures": ["ICLR.cc/2026/Conference/Submission7546/Reviewer_Zsjp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7546/Reviewer_Zsjp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760499283039, "cdate": 1760499283039, "tmdate": 1762919642461, "mdate": 1762919642461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a self-play RL framework for training LLMs to do long-context reasoning. Since there is little human annotations for long-context reasoning questions, they design a framework which includes a questioner, a responder and a verifier. Each module is trained via RLVR with a specially designed reward function. Overall the experiment results demonstrate that their proposed SPELL method is able to improve the long-context ability of LLMs and surpass the vanilla RLVR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem of long-context reasoning is very relevant to the field. The self-play framework which includes three well-specified roles for generating questions, generating responses and verifying responses is carefully designed with reasonable rewards.\n- An interesting design is the iterative curriculum which keeps generating more and more difficult questions. They demonstrate in experiment results that the increasing difficult questions are better than static questions in RLVR.  \n- Their experiments show that SPELL has consistent advantages over RLVR on most datasets and both long and short context settings. In addition, the verifier and the consistency rewards are useful for the overall model training."}, "weaknesses": {"value": "- The paper aims to improve long-context reasoning ability. The method is quite effective but seems more like a general RLVR training data generation method, and less relevant to the problem of long-context. It would be better to compare their SPELL method with at least one long-context alignment method other than only RLVR. For example, authors mentioned LongPO, SoLoPO and QwenLong-L1, which seems to be good baselines (and can be compared with SPELL given same documents as the starting point)."}, "questions": {"value": "- For the questioner reward, why do you use a Gaussian-shaped reward instead of the linear piecewise reward as in R-zero? Is there a specific reason that the Gaussian reward is better? Why do you set σ = 0.5/3 and are there any ablation studies on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6ugn3hHec0", "forum": "83F6YF4Hz6", "replyto": "83F6YF4Hz6", "signatures": ["ICLR.cc/2026/Conference/Submission7546/Reviewer_Zsjp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7546/Reviewer_Zsjp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760499283039, "cdate": 1760499283039, "tmdate": 1763570330058, "mdate": 1763570330058, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a SPELL (Self-Play Reinforcement Learning for Evolving Long-Context Language Models), a self-play RL framework for long-context reasoning, in which a unified policy is trained using reward signals derived from three roles: the questioner, the responder, and the verifier. The reward functions involve no human supervision and drive self-improvement via an easy-to-hard curriculum. The questioner generates ground-truth answers for the responder and is incentivized to produce questions of intermediate difficulty. The verifier is trained via self-consistency (e.g., agreement across multiple samples). The responder is optimized to match the ground-truth answer and to satisfy the verifier’s majority vote.\n\nSPELL yields consistent gains over a strong RLVR (reinforcement learning with verifiable rewards) baseline across 12 open-source models on six long-context QA benchmarks, evaluated at maximum input lengths of 16K and 100K tokens. SPELL also generalizes to longer contexts and improves test-time exploration and performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper’s originality lies in extending the single-policy self-play paradigm to long-context understanding and reasoning through a unified tri-role self-play formulation: \n\n(1) A single policy is supervised under three distinct roles, which form a self-improvement loop.\n\n(2) All reward signals are generated entirely through the model’s own self-evaluation, without any human knowledge or supervision. This design fully leverages the base model’s inherent potential to improve and evolve autonomously.\n\nThe experimental comparison with RLVR is solid, and the core components of the questioner and verifier are thoroughly examined through ablation studies.\n\nThe paper is clearly presented and easy to follow."}, "weaknesses": {"value": "**Baseline breadth.** A major weakness is the absence of comparisons against recent long-context SOTA methods. The main RL baseline is RLVR on static data synthesized by DeepSeek-R1-0528. Adding head-to-head comparisons with long-context SOTA methods would better position SPELL among competitive approaches. In particular, SoLoPO (short-to-long preference optimization) and LongPO (long-context self-evolution via preference optimization) report substantial gains and improved length/domain generalization on long-context benchmarks. \n\n**Risk of self-reinforcing errors.** Because the self-consistency and mutual dependencies among the questioner, responder, and verifier drive the co-evolution, one potential weakness of the SPELL framework is its susceptibility to self-reinforcing errors over time. It would strengthen the work to discuss or incorporate mechanisms that mitigate it."}, "questions": {"value": "1. How often do the verifier and the rule-based judge disagree? The paper mentions that “the CEM-based reward function is brittle and can penalize semantically correct but lexically different answers”. What happens if we replace CEM with a stronger external judge (e.g., an independent 120B model)? In that setting, what is the relative importance of the verifier versus the rule-based judge?\n\n2. How sensitive is performance to G (number of responder and verifier rollouts)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eUo1wfilRG", "forum": "83F6YF4Hz6", "replyto": "83F6YF4Hz6", "signatures": ["ICLR.cc/2026/Conference/Submission7546/Reviewer_czEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7546/Reviewer_czEk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761712818989, "cdate": 1761712818989, "tmdate": 1762919642057, "mdate": 1762919642057, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPELL (Self-Play Reinforcement Learning for Evolving Long-Context Language Models), a framework designed to address the scarcity of human annotations and verifiable rewards in long-context reasoning. The core idea is a multi-role self-play loop where a single LLM policy alternates between three roles:\n\nQuestioner ($\\pi^{que}_\\theta$): Generates question-answer pairs from raw documents, conditioned on a growing history memory to increase difficulty.\n\nResponder ($\\pi^{res}_\\theta$): Answers the generated questions based on the full document context.\n\nVerifier ($\\pi^{ver}_\\theta$): Assesses the semantic equivalence between the responder's output and the reference answer via majority voting over multiple rollouts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-Motivated Problem: Tackles the critical and underexplored challenge of applying RL to long-context reasoning without reliable human annotations or programmatic verifiers.\n\nComprehensive Evaluation: The scale of the experimentation is impressive, spanning 12 models of various sizes and architectures across 6 benchmarks. This strongly supports the claim of generalizability.\n\nStrong Ablations: The ablation studies are a key strength, providing convincing evidence for the necessity of each core component (history memory, verifier updates, Gaussian reward).\n\nEffective Methodological Design: The three-role framework is elegant. The Gaussian reward for targeting the \"competence frontier\" and the hybrid responder reward are intelligent designs that directly address stability and learning efficiency in self-play.\n\nCompelling Results vs. Baselines: Demonstrating superiority over a strong RLVR baseline and instruction-tuned models effectively highlights the value of the adaptive, self-play curriculum."}, "weaknesses": {"value": "Limited Analysis of Scaling Laws: The paper shows results up to 32B models. A key question for a \"scalable\" method is its behavior on larger, state-of-the-art models (e.g., 70B+). Does the performance gap over baselines widen or narrow?\n\nSuperficial Treatment of Computational Cost: The framework requires $G + G^2$ generations per data point, which is computationally expensive. A discussion of training cost (e.g., GPU hours compared to the RLVR baseline) and its trade-off with performance is missing but crucial for a complete assessment.\n\nPotential for \"Covariate Shift\" in Verification: The verifier is trained and evaluated on data generated by the same, evolving policy. There is a risk of the verifier overfitting to the specific style or errors of its companion roles, leading to a degenerate equilibrium. The paper does not investigate this potential pathology.\n\nNarrow Benchmarking: While the benchmarks are diverse, they are all QA-style tasks. It is unclear if the improvements translate to other long-context tasks like summarization, retrieval-augmented generation, or long-form dialogue."}, "questions": {"value": "1. Computational Overhead: Could the authors provide an estimate of the computational cost (e.g., FLOPs or GPU hours) of SPELL compared to the RLVR baseline? Given the $O(G^2)$ verifier rollouts, how do you justify the cost-benefit trade-off, and are there strategies to reduce this overhead?\n\n2. Verifier Degeneration: The verifier's training signal is based on self-consistency and alignment with a rule-based check. How does the authors ensure the verifier does not degenerate, for example, by developing a biased \"style preference\" aligned with the responder instead of true semantic equivalence, especially on non-verifiable tasks?\n\n3. Scaling to Larger Models: The results are compelling on models up to 32B. What is your hypothesis regarding how SPELL would perform on much larger foundation models (e.g., 70B or 140B parameters)? Would the adaptive curriculum provide even greater relative gains, or would the need for high-quality data diminish its advantage?\n\n4. Comparison to Simpler Baselines: Did you consider comparing against a simpler \"self-training\" baseline, where you generate synthetic QA pairs with a teacher model (like DeepSeek-R1) once and then fine-tune the student model on this static dataset? This would help isolate the benefit of the online, adaptive self-play loop from the mere use of synthetic data.\n\n5. Failure Mode Analysis: Beyond aggregate metrics, can you provide a qualitative analysis of common failure cases? For instance, what types of questions does the evolved questioner struggle to generate, and what mistakes does the verifier consistently make? This would greatly inform future iterations of the framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bdiXIyH47M", "forum": "83F6YF4Hz6", "replyto": "83F6YF4Hz6", "signatures": ["ICLR.cc/2026/Conference/Submission7546/Reviewer_HDua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7546/Reviewer_HDua"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975021438, "cdate": 1761975021438, "tmdate": 1762919641491, "mdate": 1762919641491, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPELL (Self-Play Reinforcement Learning for Evolving Long-Context Language Models), a framework designed to address the scarcity of human annotations and verifiable rewards in long-context reasoning. The core idea is a multi-role self-play loop where a single LLM policy alternates between three roles:\n\nQuestioner ($\\pi^{que}_\\theta$): Generates question-answer pairs from raw documents, conditioned on a growing history memory to increase difficulty.\n\nResponder ($\\pi^{res}_\\theta$): Answers the generated questions based on the full document context.\n\nVerifier ($\\pi^{ver}_\\theta$): Assesses the semantic equivalence between the responder's output and the reference answer via majority voting over multiple rollouts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Well-Motivated Problem: Tackles the critical and underexplored challenge of applying RL to long-context reasoning without reliable human annotations or programmatic verifiers.\n\nComprehensive Evaluation: The scale of the experimentation is impressive, spanning 12 models of various sizes and architectures across 6 benchmarks. This strongly supports the claim of generalizability.\n\nStrong Ablations: The ablation studies are a key strength, providing convincing evidence for the necessity of each core component (history memory, verifier updates, Gaussian reward).\n\nEffective Methodological Design: The three-role framework is elegant. The Gaussian reward for targeting the \"competence frontier\" and the hybrid responder reward are intelligent designs that directly address stability and learning efficiency in self-play.\n\nCompelling Results vs. Baselines: Demonstrating superiority over a strong RLVR baseline and instruction-tuned models effectively highlights the value of the adaptive, self-play curriculum."}, "weaknesses": {"value": "Limited Analysis of Scaling Laws: The paper shows results up to 32B models. A key question for a \"scalable\" method is its behavior on larger, state-of-the-art models (e.g., 70B+). Does the performance gap over baselines widen or narrow?\n\nSuperficial Treatment of Computational Cost: The framework requires $G + G^2$ generations per data point, which is computationally expensive. A discussion of training cost (e.g., GPU hours compared to the RLVR baseline) and its trade-off with performance is missing but crucial for a complete assessment.\n\nPotential for \"Covariate Shift\" in Verification: The verifier is trained and evaluated on data generated by the same, evolving policy. There is a risk of the verifier overfitting to the specific style or errors of its companion roles, leading to a degenerate equilibrium. The paper does not investigate this potential pathology.\n\nNarrow Benchmarking: While the benchmarks are diverse, they are all QA-style tasks. It is unclear if the improvements translate to other long-context tasks like summarization, retrieval-augmented generation, or long-form dialogue."}, "questions": {"value": "1. Computational Overhead: Could the authors provide an estimate of the computational cost (e.g., FLOPs or GPU hours) of SPELL compared to the RLVR baseline? Given the $O(G^2)$ verifier rollouts, how do you justify the cost-benefit trade-off, and are there strategies to reduce this overhead?\n\n2. Verifier Degeneration: The verifier's training signal is based on self-consistency and alignment with a rule-based check. How does the authors ensure the verifier does not degenerate, for example, by developing a biased \"style preference\" aligned with the responder instead of true semantic equivalence, especially on non-verifiable tasks?\n\n3. Scaling to Larger Models: The results are compelling on models up to 32B. What is your hypothesis regarding how SPELL would perform on much larger foundation models (e.g., 70B or 140B parameters)? Would the adaptive curriculum provide even greater relative gains, or would the need for high-quality data diminish its advantage?\n\n4. Comparison to Simpler Baselines: Did you consider comparing against a simpler \"self-training\" baseline, where you generate synthetic QA pairs with a teacher model (like DeepSeek-R1) once and then fine-tune the student model on this static dataset? This would help isolate the benefit of the online, adaptive self-play loop from the mere use of synthetic data.\n\n5. Failure Mode Analysis: Beyond aggregate metrics, can you provide a qualitative analysis of common failure cases? For instance, what types of questions does the evolved questioner struggle to generate, and what mistakes does the verifier consistently make? This would greatly inform future iterations of the framework."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bdiXIyH47M", "forum": "83F6YF4Hz6", "replyto": "83F6YF4Hz6", "signatures": ["ICLR.cc/2026/Conference/Submission7546/Reviewer_HDua"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7546/Reviewer_HDua"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975021438, "cdate": 1761975021438, "tmdate": 1763600711210, "mdate": 1763600711210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper propose SPELL, a multi-role self-play reinforcement learning framework that enables scalable, label-free optimization for long-context reasoning. SPELL integrates three cyclical roles—questioner, responder, and verifier—within a single model to enable continual self-improvement. Extensive experiments on six long-context benchmarks show that SPELL consistently improves performance across diverse LLMs and outperforms equally sized models fine-tuned on large-scale annotated data."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The motivation of this paper is well-grounded.\n2. The multi-agent RL system designed is both novel and sound. The reward designed is intuitively justified and appears effective for long-context reasoning.\n3. The experiments is extensive and carefully designed."}, "weaknesses": {"value": "Given the unreliability of human annotations in long-context reasoning, how do you ensure that the questions and their corresponding answers provided by the questioner are correct?"}, "questions": {"value": "Given that the entire multi-agent RL pipeline is largely hand-engineered, the paper should discuss whether reward hacking could theoretically occur. For example, the response generator might collapse to a short, low-effort policy while the questioner learns to ask only trivial questions, yet the pair still receives a high combined reward."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pTZiM2nFLQ", "forum": "83F6YF4Hz6", "replyto": "83F6YF4Hz6", "signatures": ["ICLR.cc/2026/Conference/Submission7546/Reviewer_wF6e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7546/Reviewer_wF6e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7546/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011102564, "cdate": 1762011102564, "tmdate": 1762919641045, "mdate": 1762919641045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}