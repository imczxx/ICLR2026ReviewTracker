{"id": "XhqoDBouWS", "number": 22783, "cdate": 1758335389296, "mdate": 1759896846281, "content": {"title": "Why Attention Patterns Exist: A Unifying Temporal Perspective Analysis", "abstract": "Attention patterns play a crucial role in both training and inference of large language models (LLMs). Prior works have identified individual patterns—such as retrieval heads, sink heads, and diagonal traces—but these observations remain fragmented and lack a unifying explanation. To bridge this gap, we provide a unifying framework to explain the existence of diverse attention patterns by analyzing their underlying mathematical formulations with a temporal continuous perspective. Our work can both deepen the understanding of attention behavior and guide inference acceleration approaches. Specifically, this framework characterizes attention patterns as either predictable patterns, characterized by clear regularities, or unpredictable ones that appear random. Our analysis further reveals that the distinction between them can be explained by variations in query self-similarity across the temporal dimension. Focusing on the predictable patterns, we further provide a detailed mathematical analysis of three representative predictable patterns in terms of the joint effect of queries, keys, and Rotary Positional Embeddings. To validate the framework, we apply it to KV cache compression and LLM pruning tasks. In these experiments, a simple metric inspired by our theory consistently improves performance over baseline methods.", "tldr": "", "keywords": ["Large Language Model", "KV Cache Compression", "Attention Pattern"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7d92c884f8d28c929c4db837fc605fd734901d60.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a unifying temporal perspective to explain the emergence of diverse attention patterns in LLMs. The authors categorize patterns into predictable (re-access, sequential, seasonal) and unpredictable (retrieval-like) types, attributing the distinction to variations in query self-similarity over time. They provide theoretical analyses linking pattern formation to query-key continuity and RoPE, and validate their framework through applications in KV cache compression and LLM pruning. Experiments on models like Llama and Qwen show consistent improvements over baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-organized, with clear motivations, methodical explanations, and thorough appendices.\n- The paper offers a coherent perspective that integrates previously fragmented observations (e.g., sink heads, diagonal patterns, retrieval heads) under a single temporal continuity lens.\n- The paper provides detailed mathematical proofs for each pattern type, clearly linking RoPE mechanics and query-key dynamics to observable attention structures.\n- The proposed q-similarity metric is effectively applied to downstream tasks (KV cache compression, pruning), demonstrating improved performance over strong baselines."}, "weaknesses": {"value": "- The entire framework hinges on the assumption of temporal continuity in queries and keys. While this is likely a reasonable assumption for many layers and tasks, its universality is not thoroughly explored. The analysis might be less applicable in layers or for inputs where representations change abruptly. A discussion of the boundaries of this assumption would strengthen the work.\n- The q-similarity metric is central to the applications, but its specific formulation (e.g., the choice of cosine similarity) lacks a comprehensive ablation study. It remains unclear how sensitive the performance gains are to these choices, or if an even more effective metric derived from the same theory could be designed.\n- The proofs provided in the appendix, while a valuable effort, contain significant weaknesses that undermine their theoretical rigor."}, "questions": {"value": "1.How does q-similarity vary across different layers and heads? Is it consistent across models, or does it require per-model calibration?\n2.The paper claims that high q-similarity implies redundancy in pruning. Is this always true? Could some stable patterns be critical for certain tasks (e.g., syntax parsing)?\n3.In the proof of vertical stability (Theorem 5.1, Appendix B), a crucial step bounds the change in the angle between the query and a fixed key:\n$$|\\phi_{t+1,i}^{(m)}-\\phi_{t,i}^{(m)}|\\leq\\frac{\\|q_{t+1}^{(m)}-q_t^{(m)}\\|}{r_m}$$\nHowever, consider a simplified scenario in 2D: let q_{t}=(1,0) and q_{t+1}=(cos(2arcsin(ε/2)), sin(2arcsin(ε/2))). Here ||q_{t+1}-q_{t}|| ≤ ε， the angle change is 2arcsin(ε/2) ＞ ε. This suggests the above inequality does not hold. \n\nIn the proof of Theorem 5.4 (Seasonal Pattern), the derivation for non-dominant channels contains a critical error. The term (i-t)θ_m is incorrectly repeated in both cosine functions when calculating |a_{t+L,i}^{(m)} - a_{t,i}^{(m)}|. The correct expression for a_{t+L,i}^{(m)} should have a phase of (i-t)θ_m - Lθ_m. More critically, the standard inequality used to bound the difference for non-dominant channels is misapplied. This inequality, | ||u|| ||v|| cos φ - ||u'|| ||v'|| cos φ' | ≤ ..., is valid only when φ and φ' are the geometric angles between the vector pairs (u,v) and (u',v'), respectively. In your proof, you apply it with angles φ = φ_{t,i}^{(m)} + (i-t)θ_m and φ' = φ_{t+L,i}^{(m)} + (i-t)θ_m. However, these are not pure geometric angles but include an additive positional phase. This misapplication renders the subsequent bound on non-dominant channels invalid."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2g4ngq3cbv", "forum": "XhqoDBouWS", "replyto": "XhqoDBouWS", "signatures": ["ICLR.cc/2026/Conference/Submission22783/Reviewer_Qj6A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22783/Reviewer_Qj6A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761100124817, "cdate": 1761100124817, "tmdate": 1762942386519, "mdate": 1762942386519, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper analyzes sparse attention patterns from the perspective of query similarity. It derives theoretical relations between the similarity of consecutive query vectors and the corresponding changes in attention values. The paper further shows how certain query distributions lead to typical attention patterns. It shows potential applications of these insights in attention budget allocation and layer pruning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a new perspective for explaining existing attention patterns from the query point of view.\n\n2. The paper demonstrates how the proposed query-level observations can inform the design of sparse attention and layer pruning, adding practical value to the theoretical analysis."}, "weaknesses": {"value": "1. The claim of analyzing the *joint effect of input dynamics and positional encoding* seems overstated. While it would be valuable to disentangle and quantify their respective contributions, the paper instead merges them into the query with post-encoding. This makes the connection to the original input less clear than the abstract and introduction suggest.\n\n2. Several assumptions used in the derivations are not carefully validated, which raises concerns about the reliability of the conclusions. For instance, the assumption of a dominant channel weight in Theorem 5.1 requires empirical support.\n\n3. The empirical section lacks comparisons with more direct and recent baselines, such as DuoAttention [1], which explicitly distinguishes retrieval heads.\n\n4. Some key concepts (e.g., continuity, predictability) are introduced without sufficient explanation in the introduction. Brief definitions would help prevent confusion.\n\n[1] Xiao, Guangxuan, et al. \"Duoattention: Efficient long-context llm inference with retrieval and streaming heads.\" arXiv preprint arXiv:2410.10819 (2024)."}, "questions": {"value": "1. Is the query similarity computed after applying RoPE?\n\n2. Could you provide more justification for using attention patterns to guide layer-wise FFN pruning in Section 6.2? In particular, why does high query similarity (stability) imply that “the layer extracts less novel information”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hUiwhv5pHw", "forum": "XhqoDBouWS", "replyto": "XhqoDBouWS", "signatures": ["ICLR.cc/2026/Conference/Submission22783/Reviewer_V8Hz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22783/Reviewer_V8Hz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761700633792, "cdate": 1761700633792, "tmdate": 1762942386106, "mdate": 1762942386106, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework to explain diverse attention patterns in LLMs through temporal continuity analysis. The authors categorize patterns as predictable (re-access, sequential, seasonal) or unpredictable, attributing the distinction to query self-similarity. They provide mathematical analysis of how query/key continuity and RoPE jointly produce these patterns, and validate their framework through KV cache compression and (fewer) LLM pruning experiments. While the paper makes a reasonable attempt to unify attention pattern analysis, the contributions are incremental over existing work (particularly AttentionPredictor). The theoretical analysis, though rigorous, doesn't yield sufficiently novel insights—the sequential pattern analysis overstates its departure from prior work, and the seasonal pattern analysis lacks empirical grounding. The downstream experiments show only marginal improvements."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The temporal continuity perspective provides a systematic way to understand previously fragmented observations about attention patterns. The decomposition view connecting query similarity to pattern stability is intuitive.\n- Rigorous mathematical treatment: the theorems provide formal proofs for the emergence of different pattern types, with explicit bounds relating pattern stability to query/key properties and RoPE parameters.\n- Novel insight on periodic sequential patterns: The analysis of diagonal spacing and experimental validation by manipulating dominant channel locations is particularly interesting.\n- The evaluation on KV Cache compression is performed with different budgets."}, "weaknesses": {"value": "- Limited novelty: the observation that query continuity drives attention stability was already made by AttentionPredictor (and the authors acknowledge that). While this paper provides mathematical formalization, the fundamental insight is not new. \n- KV cache compression: the improvements over CAKE are marginal and seem to be within noise margins. Other state of art methods such as DuoAttention, Expected Attention could be a stronger baseline.\n- LLM pruning is only compared against a single baseline. There are no other comparisons to other structured pruning methods. If the authors think this makes sense, could they explain why they considered only this one baseline.\n- The hyperparameters  appear hand-tuned without ablation studies."}, "questions": {"value": "- What is the computational overhead of computing q-similarity scores during inference?\n- What is the precise impact of this KV cache compression method on memory footprint ? And on latency ? \n- How are the hyper-parameters selected ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jMpi5uzyrL", "forum": "XhqoDBouWS", "replyto": "XhqoDBouWS", "signatures": ["ICLR.cc/2026/Conference/Submission22783/Reviewer_yc4e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22783/Reviewer_yc4e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902958643, "cdate": 1761902958643, "tmdate": 1762942385782, "mdate": 1762942385782, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for analysing attention patterns in Transformer-based models; author identify three types of patterns (namely re-access, sequential, and seasonal) from a subset of the heads (referred to as \"unpredictable\") based on query self-similarity and positional embeddings. Based on the proposed framework, authors then propose a method for efficiently allocate KV cache budgets and structured layer pruning, improving over baselines like CAKE and ShortGPT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The analysis in e.g. Proposition 4.1 that links attention stability to query self-similarity, and how query drift induces changes in the logit changes, is novel and interesting; likewise for Th. 5.2 and 5.3, which provide conditions under which sequential/periodic diagonals appear\n- Using q-similarity in CAKE and ShortGPT yields sigificant improvements in several settings"}, "weaknesses": {"value": "- Improvements in CAKE (Tab. 1) seem very marginal, are they statistically significant? Averages are not clearly reported\n- Computing q-similarities for every layer/head seems computationally expensive, but runtimes/costs are not discussed in-depth"}, "questions": {"value": "- Can you please expand more on the runtime/costs of computing q-similarities and what kind of overheads they add to methods like CAKE?\n- Are there cases where q-similarity fails to misidentify retrieval heads? What does it happen in those scenarios?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "me4PtEqSGk", "forum": "XhqoDBouWS", "replyto": "XhqoDBouWS", "signatures": ["ICLR.cc/2026/Conference/Submission22783/Reviewer_UTJo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22783/Reviewer_UTJo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22783/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189555121, "cdate": 1762189555121, "tmdate": 1762942385434, "mdate": 1762942385434, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}