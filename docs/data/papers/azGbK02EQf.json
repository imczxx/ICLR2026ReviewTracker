{"id": "azGbK02EQf", "number": 18184, "cdate": 1758284811614, "mdate": 1759897121171, "content": {"title": "Ambiguity in LLMs is a concept missing problem", "abstract": "Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods to ambiguity handling either rely on the ReACT framework to obtain correct mappings through trial and error, or on supervised fine-tuning to bias models toward specific tasks. In this paper, we adopt a different approach that characterizes representation differences of ambiguous text in the latent space and leverages these differences to identify ambiguity before mapping them to structured data. To detect sentence-level ambiguity, we focus on the relationship between ambiguous questions and their interpretations. Unlike distances calculated by dense embeddings, we introduce a new distance measure based on a path kernel over concepts. With this measurement, we identify patterns to distinguish ambiguous from unambiguous questions. Furthermore, we propose a method for improving LLM performance on ambiguous agentic tool calling through missing concept prediction. Both achieve state-of-the-art results.", "tldr": "", "keywords": ["Ambiguity Detection", "Large Language Model", "Sparse autoencoder", "Kernel Machine"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/535a646047953c6aabefe286f48fb241bdd579ab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes leveraging a subset of SAE representations for ambiguity detection, instead of relying on conventional dense embeddings. The authors first demonstrate, through intuitive examples, that large language models (LLMs) struggle to generate diverse interpretations for ambiguous questions, and that slightly modifying the input (e.g., by introducing a [MASK] token) can help alleviate this limitation. Building on these empirical observations, the authors propose using SAE embeddings to measure the distance between different interpretations. Experimental results show that the proposed method outperforms standard dense embeddings on two tasks: text-to-SQL and tool calling."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This work highlights the lack of research on the representational differences of ambiguous text and tackles this underexplored yet important problem, which I also find crucial.\n- The case studies presented in Section 3.1 are intuitive and help readers clearly understand the problem that this work aims to address.\n- The idea of using representations derived from SAEs for ambiguity detection is relatively novel and valuable, as it connects theoretical probing studies using SAEs with practical scenarios where models must identify the ambiguity of a given question.\n- When evaluated on two datasets, AMBROSIA and Gorilla, the proposed method demonstrates reasonable performance compared to typical embedding-based approaches."}, "weaknesses": {"value": "- First of all, I would like to mention that I basically like the overall idea of this work. However, given the current state of the draft, there are several aspects that need improvement to better clarify the paper’s contributions.\n- There is room for clarification and refinement in the notations and equations. Some notations appear to be directly adopted from previous works without sufficient caution. For example, in Eq. (2), what does $\\mathbf{w}$ represent? I could not find a definition in the paper. Similarly, SAE in Eq 2 $\\rightarrow$ $f_\\text{SAE}$ ?\n- It is also unclear how the mask $M$ is constructed—what proportion of masking is applied? The explanation seems rather abstract, remaining at a conceptual level without sufficient implementation detail.\n- Regarding the Path State Approximation, using a linear approximation might be reasonable and perhaps the only feasible option. However, there is no discussion of the potential risks of this abstraction. Most researchers would agree that the training path is typically non-linear, so additional discussion on the implications of approximating such a non-linear trajectory with a linear one would be valuable.\n- Moreover, the experimental section feels somewhat limited. The results are presented for only two tasks, without detailed discussion or ablation studies on the components of the proposed method. Adding such analyses could significantly strengthen the paper.\n- Studies on ambiguity handling generally focus on (open-ended) question answering benchmarks such as NQ and TriviaQA. I appreciate that this work takes a different direction by applying the proposed method to more diverse and underrepresented tasks, such as text-to-SQL and tool calling. However, since most prior methods are evaluated on QA datasets, it would make the comparison more convincing if the proposed method’s performance were also reported in that setting.\n- It is also unclear whether simply averaging all distances—$D(q, i_1)$, $D(q, i_2)$, and $D(i_1, i_2)$—is sufficient. Intuitively, the effect of each term may be diluted by the others. Could there be a more principled way to handle this part? Perhaps additional case studies could help justify that the current simple averaging approach is indeed reasonable (or reveal that it may not be).\n- It would also be valuable if the draft explicitly discussed the pros and cons of the proposed approach. In practice, training SAEs from scratch is quite cost-intensive, and most studies instead rely on utilizing pre-trained and open-sourced SAEs available for only a limited number of models. Furthermore, incorporating SAEs inevitably introduces additional computational costs, as briefly mentioned in Section 4.2. Clearly addressing this efficiency aspect would make the paper more transparent and substantively stronger.\n- It appears inconsistent to rely on vanilla LLMs for generating interpretations in Section 4.1, considering that Section 3.1 already shows their limited capability in handling the task."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Rxkim0DFT0", "forum": "azGbK02EQf", "replyto": "azGbK02EQf", "signatures": ["ICLR.cc/2026/Conference/Submission18184/Reviewer_tKNc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18184/Reviewer_tKNc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761787087283, "cdate": 1761787087283, "tmdate": 1762927936937, "mdate": 1762927936937, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the origin of ambiguity in LLM and proposes that it arises from missing concepts in the model’s latent space. Building on recent work in mechanistic interpretability, the authors employ a Sparse Autoencoder (SAE) to extract human-interpretable latent concepts from LLM activations. They then introduce a path-kernel formulation that measures distances between questions and their possible interpretations by integrating gradients along the parameter trajectory of the model. Based on this hypothesis, the paper develops a concept predictor that identifies and injects missing concepts to mitigate ambiguous behavior. Experiments are conducted on the AMBROSIA dataset for ambiguity detection and on Gorilla TensorFlow Hub for tool-calling under ambiguous instructions. The proposed SAE-based path kernel shows higher accuracy than dense embedding baselines and demonstrates improved API retrieval performance when missing concepts are predicted and supplied. Overall, the work presents an interesting conceptual connection between ambiguity, interpretability, and kernel methods, and serves as a proof-of-concept study toward understanding and reducing ambiguity in LLM reasoning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents an interesting and novel view that ambiguity in LLMs can be interpreted as a missing concept problem, linking ambiguity detection with model interpretability.\n- Integrating sparse autoencoders with path kernels is a creative idea that provides a new way to measure semantic differences beyond dense embeddings.\n- The proposed method achieves clear performance gains on AMBROSIA and Gorilla benchmarks, showing empirical value beyond conceptual novelty."}, "weaknesses": {"value": "1. Incomplete methodological exposition.\nSection 3.4 (“Predicting Missing Concepts to Mitigate Ambiguity”) is poorly explained. The paper does not specify how labeled data are obtained, what features are used as input to the concept predictor. The integration between the predictor and retrieval module (“union joint”) is vague, and the role of the path kernel in this stage is unclear. Figure 4 is also oversimplified, leaving the data flow between modules undefined and the overall pipeline difficult to reproduce.\n\n2. Speculative core hypothesis without rigorous validation\nThe central claim that “ambiguity can be treated as a missing concept problem” (Section 3.1) is primarily a theoretical conjecture rather than an empirically verified mechanism. The paper demonstrates this connection through a single case study with the [MASK] token (Figure 1 and Section 3.1), but does not include systematic experiments or statistical evidence showing that missing-concept activation consistently correlates with ambiguity across datasets. The insight is interesting but remains under-supported.\n\n3. Unclear definition and computation of semantic entropy\nSection 3.2 introduces “semantic entropy” to quantify ambiguity but does not clearly explain how it is computed in practice.\n\n4. Simplistic approximation of training trajectory\nThe approximation of the training path in Section 3.3 by a “straight-line interpolation in parameter space” (Eq. 7–8) is a strong simplifying assumption. Since the entire path kernel formulation hinges on gradient-trajectory alignment, this approximation could drastically alter kernel behavior. The paper should have included experimental validation comparing results with true gradient trajectories (even on smaller models) to justify the substitution.\n\n5. Unclear parameter initialization and reproducibility gaps\nThe paper references initialization parameters θ^((0))  in the path kernel formulation but does not specify how these parameters are obtained, whether they come from pretrained SAE checkpoints or random seeds, or whether results are stable across different initializations. Such omissions raise reproducibility concerns, especially because kernel values depend on initialization when straight-line interpolation is used.\n\n6. Limited experimental diversity and external validation\nThe experiments are primarily conducted on AMBROSIA and Gorilla TensorFlow Hub datasets. Both are specialized and relatively small in scope. There is no evidence that the proposed method generalizes to other ambiguity types (e.g., lexical, referential, pragmatic) or to other tasks (beyond SQL parsing and tool calling). A cross-domain evaluation or at least a synthetic control experiment would significantly strengthen the claims.\n\n7. Missing ablation and sensitivity analyses\nImportant hyperparameters, such as the number of interpolation steps, concept-mask size, or LightGBM predictor depth, are not explored. The paper also lacks ablations to disentangle the effects of (a) path kernel vs. dense embeddings and (b) concept masking vs. full concept sets. Such analyses are necessary to verify that observed gains stem from the proposed mechanism rather than incidental tuning.\n\n8. Presentation and clarity issues\nThe overall exposition suffers from poor organization and missing transitions between conceptual sections. Figures 2–4 are not fully explained in the text, and some equations are introduced without definitions of symbols or variable ranges. The paper would benefit from a clearer flow of ideas, with intuitive examples accompanying mathematical sections."}, "questions": {"value": "1.\tOn dataset diversity and coverage\nYour experiments focus mainly on AMBROSIA and Gorilla datasets, which represent scope and structural ambiguity. How would your method handle other ambiguity types, such as referential or pragmatic ambiguity? Do you expect the same “missing concept” mechanism to hold there?\n2.\tOn reproducibility details\nCould you provide essential implementation details, such as SAE layer configuration, number of interpolation steps in the path kernel, LightGBM feature inputs, and training data sources for the concept predictor, so that others can reproduce your results?\n3.\tOn hypothesis validation\nThe paper posits that “ambiguity = missing concept” in the latent space, but this is demonstrated with only one case and activation examples. Can you provide quantitative evidence (e.g., correlation between missing-concept activations and ambiguity labels) to substantiate this causal link?\n4.\tOn the path kernel approximation\nSince you replace the true gradient path with linear interpolation, have you evaluated how this approximation affects results? Would performance degrade or patterns change if a real or simulated training trajectory were used?\n5.\tOn the semantic entropy metric\nCould you clarify how semantic entropy is computed in practice, which embeddings, clustering thresholds, and sample sizes are used, and whether you observed sensitivity to these parameters?\n6.\tOn ablation and robustness\nHave you performed ablations to isolate the contribution of (a) concept masking vs. full concept sets and (b) path kernel vs. standard cosine similarity? Such analysis could confirm that observed gains indeed arise from your proposed mechanisms.\n7.\tOn the stage of validation\nYou describe the approach as a “proof-of-concept.” What would be the next step toward turning it into a more generalizable framework, e.g., scaling to larger or more diverse corpora, or testing in real interactive ambiguity-resolution settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c0Jd6hwEvh", "forum": "azGbK02EQf", "replyto": "azGbK02EQf", "signatures": ["ICLR.cc/2026/Conference/Submission18184/Reviewer_BofT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18184/Reviewer_BofT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004574339, "cdate": 1762004574339, "tmdate": 1762927936293, "mdate": 1762927936293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method from distinguishing ambiguous from unambiguous queries. The method involves (2) prompting an LLM to generate two interpretations $i_1$ and $i_2$ for a given query $q$, then (2) computing the distances $D(q,i_1),D(q,i_2), D(i_1,i_2)$ using a \"path kernel-based method (with SAE)\". The average of these three distances is essentially higher for ambiguous queries, and lower for unambiguous queries. In experiments, they show that their detection method achieves 86% accuracy on the AMBROSIA dataset, outperforming simpler distance metrics based on LLM-produced dense embeddings. Moreover, they show that by using the generated interpretations to support agentic tool calling, the overall performance can be improved."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a method for ambiguity detection, and show that there are benefits both in detecting ambiguity and augmenting agentic workflows with alternative interpretations of the user query."}, "weaknesses": {"value": "1. Overall, the clarity of the technical writing is weak, making it difficult to understand what was actually done.\n\n- The following two interpretations have the same flaw to me, which is that they are both ambiguous themselves. How come the first one is considered wrong and the second one is correct?\n\n\"Show all gate agents and pilots who speak Spanish\"\nand\n\"Show all gate agents and pilots who are Spanish-speaking\"\n\n- L. 176: How was the SAE trained? How is it decoding concepts in natural language?\n- How did you clamp activation value of a given concept, and why does the figure instead say that it's *activating* the concept? In particular, the activation values where, and how do you know they correspond to some concept?\n- §3.2: How do you calculate semantic entropy? Where are the 20 queries from?\n\n2. Most of the paper (e.g., Figure 1, the motivation example in §3.1) seemed to say that a query is ambiguous when the distance between the two interpretations is larger than the distance between the query and either interpretation. However, if I understand correctly, the experiments actually use the straight average of the three distances as an indicator of ambiguity.\n\n3. It is really difficult to understand whether the significant added complexity to the method was necessary or made sense as a solution to the problem of ambiguity detection. Perhaps some higher-level intuition for how the method works would be beneficial."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4SecPPmLDs", "forum": "azGbK02EQf", "replyto": "azGbK02EQf", "signatures": ["ICLR.cc/2026/Conference/Submission18184/Reviewer_MqEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18184/Reviewer_MqEV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762022693655, "cdate": 1762022693655, "tmdate": 1762927935768, "mdate": 1762927935768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an approach for characterizing representational differences of ambiguous text within latent space and identifies ambiguity before mapping the text to structured data. To address sentence-level ambiguity, the authors examine the relationship between ambiguous questions and their corresponding interpretations. It also introduces an alternative metric based on a path kernel over conceptual structures and proposes a method to enhance the performance of large language models (LLMs) in handling ambiguous agentic tool-calling tasks by predicting missing concepts."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper posits that linguistic ambiguity stems from missing conceptual representations within the latent space of large language models (LLMs) and introduces a distance metric to improve interpretability while capturing specific semantic patterns.\n2. This paper identifies systematic patterns that effectively differentiate ambiguous questions from unambiguous ones.\n3. This paper further proposes a comprehensive framework aimed at enhancing LLM performance in managing ambiguous agentic tool-calling tasks through the prediction of missing concepts."}, "weaknesses": {"value": "1. The evaluations are conducted solely on the AMBROS (text-to-query) and Gorilla (tool-calling) datasets, which raises two primary concerns: (1) this limited scope renders the study somewhat fragile and lacking in coherence, and (2) the absence of broader testing on additional QA tasks restricts the generalizability of the approach.\n2. The evaluation is further weakened by the omission of key baseline comparisons. Specifically, methods referenced and critiqued in the Introduction section, such as ReACT, ClarifyGPT, and the approaches by Kamath et al. (2024) and Saparina & Lapata (2025), are not included in the experimental analysis. Without these comparative baselines, it is difficult to determine whether the proposed justifications and methods provide a genuine performance advantage.\n3. The paper contains presentation issues, such as misuse of `\\citep{}` and `\\citet{}`, and a lack of space before citations."}, "questions": {"value": "See \"Weaknesses.\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tufO32RKdr", "forum": "azGbK02EQf", "replyto": "azGbK02EQf", "signatures": ["ICLR.cc/2026/Conference/Submission18184/Reviewer_iMih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18184/Reviewer_iMih"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18184/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762059995850, "cdate": 1762059995850, "tmdate": 1762927935330, "mdate": 1762927935330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}