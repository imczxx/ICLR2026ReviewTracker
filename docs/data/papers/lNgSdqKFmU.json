{"id": "lNgSdqKFmU", "number": 14877, "cdate": 1758244954186, "mdate": 1759897343934, "content": {"title": "Self-Evolving Curriculum for LLM Reasoning", "abstract": "Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose \\textit{Self-Evolving Curriculum (SEC)}, an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.", "tldr": "", "keywords": ["Large Language Model", "Reasoning", "Reinforcement Learning", "Curriculum Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ef27194231b60f42775c4446a04fb05679fc2d5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Self-Evolving Curriculum (SEC), an automatic curriculum learning framework for fine-tuning Large Language Models (LLMs) with Reinforcement Learning (RL). The authors formulate curriculum selection as a non-stationary Multi-Armed Bandit (MAB) problem, where each \"arm\" corresponds to a category of problems. A key contribution is the use of the absolute advantage value from the policy gradient update as a computationally efficient proxy for immediate learning gain, which serves as the reward signal for the MAB. The arm selection policy is updated using the TD(0) method. Through extensive experiments on planning, inductive reasoning, and mathematics tasks with Qwen2.5 models, the paper demonstrates that SEC outperforms standard random and difficulty-ordered curricula."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  The motivation to frame the curriculum as a non-stationary MAB is intuitive. Using the absolute advantage as a reward signal is a natural choice.\n2.  The experimental setup is thorough. The authors evaluate SEC across three distinct reasoning domains, using two different model sizes. Further ablations showing the method's effectiveness with different RL algorithms, and with automatically inferred curriculum categories strengthen the paper's claims.\n3.  The results show the benefits of SEC, especially for improving generalization on out-of-distribution (OOD) test sets."}, "weaknesses": {"value": "1.  The paper notes that the performance gap between the SEC and a random curriculum narrows for the larger Qwen2.5-7B model on several tasks. This raises an important question about the scalability of the approach's benefits. As foundation models become increasingly capable, the inherent need for a carefully curated curriculum might decrease, potentially limiting the long-term impact of this method.\n\n2.  The paper employs a TD(0) update to handle the non-stationary nature of the MAB problem. While empirically successful here, this is a relatively simple approach that doesn't guarantee optimal adaptation. The non-stationary bandit literature offers more sophisticated algorithms (e.g., Sliding-Window Thompson Sampling[a]) that might provide more robust adaptation and stronger theoretical grounding.\n\na. Trovo, Francesco, Stefano Paladino, Marcello Restelli, and Nicola Gatti. \"Sliding-window Thompson sampling for non-stationary settings.\" Journal of Artificial Intelligence Research 68 (2020): 311-364."}, "questions": {"value": "1.  Following up on the observation that gains are smaller for the 7B model: How do you foresee SEC performing on even larger, state-of-the-art models (e.g., Qwen3-235B)?\n\n2.  Could you comment on why TD(0) was chosen over other non-stationary bandit algorithms, such as sliding-window Thompson Sampling[a], which can offer stronger theoretical guarantees for adapting to distribution drift? Did you experiment with any alternatives?\n\na. Trovo, Francesco, Stefano Paladino, Marcello Restelli, and Nicola Gatti. \"Sliding-window Thompson sampling for non-stationary settings.\" Journal of Artificial Intelligence Research 68 (2020): 311-364."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h66DtRTVjg", "forum": "lNgSdqKFmU", "replyto": "lNgSdqKFmU", "signatures": ["ICLR.cc/2026/Conference/Submission14877/Reviewer_6S7k"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14877/Reviewer_6S7k"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659388099, "cdate": 1761659388099, "tmdate": 1762925226707, "mdate": 1762925226707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates reinforcement fine-tuning for reasoning models, aiming to improve training efficiency through better curriculum learning strategies. The authors highlight the limitations of existing approaches: random curricula (suboptimal), manually designed curricula (heuristic), and online filtering methods (computationally expensive). To address these issues, they propose an automatic curriculum framework (called SEC) that formulates curriculum selection as a non-stationary multi-armed bandit (MAB) problem. The SEC policy leverages absolute advantage as a reward function. Experiments on reasoning domains demonstrate that SEC improves reasoning capability and generalization to out-of-distribution (OOD) tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow. \n- It tackles an important problem in fine-tuning reasoning models. \n- The proposed solution is practical and has potential real-world impact."}, "weaknesses": {"value": "The experimental comparison is limited to simple baselines (random and easy-to-hard curricula). However, several zone of proximal development (ZPD) and self-paced learning based curriculum RL methods (Florensa et al., 2018; Klink et al., 2020; Eimer et al., 2021; Tzannetos et al., 2023) exist with comparable computational cost to SEC. These approaches are also not discussed in the related work section.\n\nIn particular, ProCuRL (Tzannetos et al., 2023) provides a simple baseline. It applies to both binary and non-binary rewards and can be integrated into the paper's Algorithm 1 framework. Specifically, instead of Eq. (3), ProCuRL uses $r_t(c) = \\mathbb{E}_{\\tau \\sim \\pi_t(x), x \\sim c}[R(\\tau)]$ (where $R$ is the true reward function), with the curriculum policy defined as $p(c) \\propto \\exp (Q_t(c) \\cdot (1 - Q_t(c)) / \\tau)$. When the reward is non-binary, $Q_t(c)$ requires normalization. The computational cost of ProCuRL is comparable to SEC and could be considered as an additional baseline.\n\nReferences:\n\nFlorensa et al., 2018: Automatic Goal Generation for Reinforcement Learning Agents.\n\nKlink et al., 2020: Self-Paced Deep Reinforcement Learning.\n\nEimer et al., 2021: Self-Paced Context Evaluation for Contextual Reinforcement Learning.\n\nTzannetos et al., 2023: Proximal Curriculum for Reinforcement Learning Agents."}, "questions": {"value": "It would be useful to report end-to-end wall-clock time comparisons across curriculum methods to assess computational overhead. \n\nWhat is the novelty of SEC compared to existing MAB-based curriculum approaches such as Matiisen et al. (2020)?\n\nIn experiments, why is evaluation done using the best checkpoint rather than the last checkpoint? Does the curriculum cause any training instability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "q0qcs2ntwn", "forum": "lNgSdqKFmU", "replyto": "lNgSdqKFmU", "signatures": ["ICLR.cc/2026/Conference/Submission14877/Reviewer_UpBW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14877/Reviewer_UpBW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761689118586, "cdate": 1761689118586, "tmdate": 1762925226185, "mdate": 1762925226185, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Self-Evolving Curriculum (SEC), a method that dynamically adjusts the training data sequence during Reinforcement Learning (RL) fine-tuning of Large Language Models (LLMs). The core idea is to formulate curriculum selection as a non-stationary Multi-Armed Bandit (MAB) problem, where each \"arm\" represents a category of problems (e.g., a difficulty level). The curriculum policy is updated concurrently with the RL policy using the average absolute advantage of a training batch as a reward signal, aiming to select data that maximizes immediate learning gain. Experiments across planning (Countdown, Zebra Puzzle), inductive reasoning (ARC-1D), and mathematics (MATH) domains demonstrate that SEC consistently improves generalization, particularly on out-of-distribution (OOD) tasks, compared to random or fixed-order curricula. The method also shows effectiveness in multi-task settings and with different RL algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novelty: The formulation of adaptive curriculum learning as a non-stationary MAB problem is novel in the context of LLM RL-finetuning.\n\n2. Strong Empirical Results: The paper provides extensive experiments across three distinct reasoning domains (planning, inductive reasoning, mathematics) and two model scales (3B and 7B parameters).\n\n3. Clarity and Reproducibility: The paper is well-written, and Algorithm 1 provides a clear outline of the method. The authors have also included details on model variants, dataset sources, and hyperparameters, which aids reproducibility."}, "weaknesses": {"value": "1. While the core method is well-evaluated, a more detailed ablation study would strengthen the paper. For instance, how crucial is the specific choice of the absolute advantage? How do the performance gains compare to the computational cost of maintaining and updating the MAB policy? Furthermore, the hyperparameters for the MAB (learning rate, temperature) are provided but not discussed in terms of their sensitivity or impact on final performance. A brief analysis would be valuable.\n\n2. The connection between the absolute advantage and the actual learning progress is intuitively explained but lacks a rigorous theoretical justification. A more formal discussion or bound on how this proxy relates to the ultimate goal of maximizing final task performance would enhance the paper's foundation.\n\n3. The method relies on a predefined or automatically inferred categorization of problem difficulty. The paper could more deeply discuss the limitations of this approach—for instance, how SEC might perform when \"difficulty\" is not easily quantifiable or when the model's perception of difficulty changes in non-monotonic ways during training."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lEGD12TemK", "forum": "lNgSdqKFmU", "replyto": "lNgSdqKFmU", "signatures": ["ICLR.cc/2026/Conference/Submission14877/Reviewer_XxBp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14877/Reviewer_XxBp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919899085, "cdate": 1761919899085, "tmdate": 1762925225212, "mdate": 1762925225212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an automatic curriculum learning method for large language models reinforcement learning. The curriculum signal is the batch average absolute advantage from the same on policy rollouts used for training, so no extra probing is required. Experiments on logic puzzles and mathematics with Qwen models at three and seven billion parameters show ID (in distribution) and OOD (out of distribution) gains over random or fixed curriculum."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The studied problem is important.\n\n* Different tasks (Inductive reasoning, Planning, and Math) are involved in the experiments.\n\n* Both ID (in distribution) and OOD (out of distribution) settings are considered in the experiments.\n\n* This paper is well-written."}, "weaknesses": {"value": "* This paper does not compare against many existing curriculum-learning methods for reinforcement learning, despite a growing body of existing works (see references below). The related work section also listed many existing RL curriculum learning methods, but they are not compared in the experiments. The baselines used in the experiments are RFT method without curriculum or that with naive curriculum, which makes it hard to compare the proposed method and other advanced curriculum learning methods. A better evaluation should includes more existing RL curriculum learning methods.\n\n* The high level idea that learning from easy to hard using a curriculum signal and formulating curriculum learning as a Multi-Armed Bandit problem is not new, as it's already commonly used in both traditional RL and LLM RL. What is the fundamental differences and advantages of the proposed method comparing to these existing works? \n\n* The proposed method introduces multiple hyper-parameters. Based on Table S2, this paper has per-dataset hyperparameter choices, but does not include systematic sensitivity analyses or a clear selection protocol. Without sweeps or robustness tables, it is unclear whether the reported gains reflect the curriculum mechanism or tuning effects tailored to each dataset. Several key hyperparameters—especially decoding temperature—differ across datasets. Why are these settings not unified, and does this indicate the method is sensitive to hyperparameters? A stronger evaluation could justify how each hyperparameter was chosen (search vs. heuristic vs. validation tuning), report results under a single fixed configuration shared across datasets, and include sensitivity analysis.\n\n\nZhang et al., Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation. EMNLP 2025.\n\nTzannetos et al., Proximal Curriculum for Reinforcement Learning Agents. TMLR 2023.\n\nNarvekar et al., Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey. JMLR 2021.\n\nParashar et al., Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning"}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QmerBsTcEn", "forum": "lNgSdqKFmU", "replyto": "lNgSdqKFmU", "signatures": ["ICLR.cc/2026/Conference/Submission14877/Reviewer_qVdB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14877/Reviewer_qVdB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14877/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762164018963, "cdate": 1762164018963, "tmdate": 1762925224330, "mdate": 1762925224330, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}