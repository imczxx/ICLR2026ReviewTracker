{"id": "zIT4MLbjlO", "number": 8623, "cdate": 1758092851033, "mdate": 1759897772778, "content": {"title": "MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents", "abstract": "AI agents with advanced reasoning and tool use capabilities have demonstrated impressive performance in web browsing for deep search. While existing benchmarks such as BrowseComp evaluate these browsing abilities, they primarily focus on textual information, overlooking the prevalence of multimodal content. To bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising 224 challenging, hand-crafted questions specifically designed to assess agents' multimodal retrieval and reasoning capabilities. These questions often incorporate images in prompts, and crucial information encountered during the search and reasoning process may also be embedded within images or videos on webpages. Consequently, methods relying solely on text prove insufficient for our benchmark. Additionally, we provide a verified checklist for each question, enabling fine-grained analysis of multimodal dependencies and reasoning paths. Our comprehensive evaluation of state-of-the-art models on MM-BrowseComp reveals that even top models like OpenAI o3 with tools achieve only 29.02\\% accuracy, highlighting the suboptimal multimodal capabilities and lack of native multimodal reasoning in current models.", "tldr": "MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents", "keywords": ["Multimodal Agent", "Large Language Models", "Vision- Language Models", "Deep Research", "Agents"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/673f9d9ff963814f1791a0dd3a235750dbf5a5e8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "MM-BrowseComp consists of 224 challenging, hand-crafted questions distributed across 22 distinct subtasks covering five broad categories (Media, Technology, Society, Geography, and Academics). The questions are intentionally multi-hop and difficult, with a construction criteria ensuring they remain unanswerable by strong VLMs with web search in a single attempt, or by unfamiliar human annotators within five minutes. A crucial component is the verified checklist provided for each question, which represents the minimal irreducible reasoning path required to reach the correct answer. This checklist enables the use of Strict Accuracy (SA) alongside Overall Accuracy (OA), allowing for a fine-grained analysis that distinguishes genuine reasoning from \"lucky guessing\". Experimental results demonstrate that the top performer, OpenAI o3 with tools, achieved only 29.02% OA and 19.64% SA, confirming the benchmark’s challenging nature."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The benchmark successfully bridges the gap left by previous textual benchmarks (like the original BrowseComp).\n- The checklists provide fine-grained evaluation, moving beyond simple correctness to assess the path taken.\n- Evaluates 18 models across multiple dimensions with detailed error taxonomy and modality-specific performance breakdown."}, "weaknesses": {"value": "- While the authors convincingly justify the size through the rigor of construction and high filtering rate, a total of 224 instances across 22 distinct subtasks may be insufficient for reporting reasonable scores at this granularity.\n- Heavy reliance on GPT-4o-2024-11-20 as the sole evaluator for checklist, and I believe this might add certain evaluation bias."}, "questions": {"value": "- Could the authors elaborate on the strict criteria used by annotators to ensure the reasoning checklist is truly \"irreducible\"?\n- Why evaluate open-source agents on only 54 instances? This seems too limited for reliable conclusions. What were the selection criteria?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8hjvP9Nr7Q", "forum": "zIT4MLbjlO", "replyto": "zIT4MLbjlO", "signatures": ["ICLR.cc/2026/Conference/Submission8623/Reviewer_dmLk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8623/Reviewer_dmLk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761429383871, "cdate": 1761429383871, "tmdate": 1762920457750, "mdate": 1762920457750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MM-BrowseComp for evaluating agents that must browse the web and reason over multimodal content. Each instance comes with an irreducible reasoning checklist that specifies the minimal sequence of retrieval and reasoning steps required to reach the answer, enabling fine-grained assessment beyond final-answer accuracy. On this benchmark, strong systems achieve only ~29% accuracy, underscoring the challenge and current gap in native multimodal browsing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clarity: The paper is readable and well-structured, with intuitive examples and comprehensive task taxonomy/mixture. Construction principles and validation steps are communicated with sufficient detail.\n\n2. Significance: Addresses a timely need: deep web browsing with native multimodality—central for real-world assistants. The results and analyses (e.g., modality-specific performance, test-time scaling, error taxonomy) are likely to shape evaluation practices and agent design."}, "weaknesses": {"value": "1. Scale: 224 instances is on the small side for a general-purpose benchmark spanning 22 subtasks; per-subtask sample sizes are too thin for robust statistics. Consider releasing a larger dev/test split or staged expansions, and report confidence intervals (e.g., bootstrap over items) in the main text. The dataset probably won't be very meaningful if the data size is too small.\n\n2. Potential construction bias and leakage checks. During dataset construction, there could be several stages with risk of potential biases. Difficulty criteria include “unanswerable by strong models in one attempt.” can be subjective. The dataset construction lacks inter-annotator agreement. This risks encoding model-specific blind spots. Add contamination audits, time-stamped sources, and a multi-attempt human check protocol report (agreement, time-to-solve). \n\n3. While the paper has some analysis on the evaluation results, the LLM-judge based analysis seems not very scalable if the data size grows large. Also the llm judge backbone may also introduce extra bias for analysis."}, "questions": {"value": "1. Checklist design & validation. How do you ensure minimality and non-redundancy of checklists across annotators? Report inter-annotator agreement on checklists and provide a public rubric of what counts as “completed.” (This would reduce reviewer subjectivity when others extend the benchmark.) \n\n2. Tool standardization. Could you release a reference tool suite (OCR, layout/grounding, video frame sampler) and a tool-capability checklist per model/agent so results are not confounded by missing/different tools? This would also clarify where o3’s edge stems from (backbone vs. toolset). \n\n3. Dataset growth & governance. Any plan for a continually updated MM-BrowseComp with frozen yearly snapshots and public leaderboards? Is there any plan to make the data collection pipeline more scalable and generalizable.\n\n4. Error taxonomy reliability. The failure analysis uses GPT-4o to label errors. Please report labeling agreement (e.g., dual-judge consistency) and try to use different LLM to label the errors. Would GPT-4o have bias towards OpenAI models like o3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OmhrrpbzQK", "forum": "zIT4MLbjlO", "replyto": "zIT4MLbjlO", "signatures": ["ICLR.cc/2026/Conference/Submission8623/Reviewer_WJHa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8623/Reviewer_WJHa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967995364, "cdate": 1761967995364, "tmdate": 1762920457442, "mdate": 1762920457442, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors introduce a new benchmark to evaluate web-browsing agents in multimodal environments, where text shortcuts are not available. It contains 224 carefully designed questions that often include images in the prompt or require agents to pull information from visuals on real web pages. Each question is paired with a verified checklist of reasoning steps and supporting evidence. The best system only achieves only about 29% accuracy. The findings show that current models still struggle significantly with multimodal browsing and reasoning.Models perform much worse on visual content than on text and tend to depend on shallow image-captioning shortcuts rather than genuine visual understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The data construction process ensures questions require multimodal browsing, effectively eliminating text shortcuts.\n- Queries in the dataset go through rigorous difficulty-based filtering.\n- The human-verified checklist of minimal finegrained reasoning steps provides a valuable signal, it provides a way for evaluation to go beyond just right/wrong final answers."}, "weaknesses": {"value": "- There is missing a human baseline to calibrate what model accuracy means. It would provide an estimate for the performance ceiling of this task.\n- In 3.1.1, authors assert that essential information to solve a task should not appear in any text source. However, there is no mention of how this verification is done.\n- Although the authors repeatedly refer to “video-dependent” tasks, the paper never specifies how models are expected to engage with videos. Are agents intended to interact with video content directly, or are they simply expected to rely on accompanying textual information, like transcripts or descriptions?"}, "questions": {"value": "- Figure 3 could be clearer. Rather than focusing on whether each input includes an image, it might be more informative to present statistics on the actual modalities required or used by each task.\n- Relevant work from earlier this year with a similar objective of measuring multimodal interactions with no textual shortcuts: `BEARCUBS: A benchmark for computer-using web agents`"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h5KjgjjnaA", "forum": "zIT4MLbjlO", "replyto": "zIT4MLbjlO", "signatures": ["ICLR.cc/2026/Conference/Submission8623/Reviewer_3Njv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8623/Reviewer_3Njv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969191246, "cdate": 1761969191246, "tmdate": 1762920457171, "mdate": 1762920457171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MM-BrowseComp, a new benchmark designed to evaluate multimodal browsing agents that integrate reasoning and tool use. It comprises 224 hand-crafted questions across 22 subtasks, requiring retrieval and reasoning over both textual and visual information. Each question includes a verified checklist that tracks reasoning steps, allowing fine-grained analysis beyond final-answer accuracy. Experimental results show that even state-of-the-art models like OpenAI’s o3 achieve an accuracy of 29%, highlighting the difficulty of multimodal reasoning and the limitations of current models. Overall, the paper provides a challenging dataset that fills an important gap in multimodal browsering agent evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The proposed benchmark is constructed through multiple rigorous verification phases.\n* The experiment part systematically compares a wide range of state-of-the-art closed- and open-source models, offering a clear view of current limitations and performance gaps."}, "weaknesses": {"value": "* The tasks in this benchmark are often intentionally complex and involve multi-hop reasoning, which may not accurately reflect the typical multimodal search behaviors encountered in real-world web browsing scenarios.\n* The heavily hand-crafted nature of the benchmark may limit real-world generalizability."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tjN1ubdvML", "forum": "zIT4MLbjlO", "replyto": "zIT4MLbjlO", "signatures": ["ICLR.cc/2026/Conference/Submission8623/Reviewer_cbDe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8623/Reviewer_cbDe"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8623/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979570938, "cdate": 1761979570938, "tmdate": 1762920456791, "mdate": 1762920456791, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}