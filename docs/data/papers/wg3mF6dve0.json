{"id": "wg3mF6dve0", "number": 5855, "cdate": 1757940749297, "mdate": 1759897949499, "content": {"title": "Pinpointing Crowd in Bird's Eye View via Proximal Contexts", "abstract": "Bird’s-Eye-View (BEV) perception has emerged as a promising paradigm for scene understanding and it has recently been applied for multi-human activity analysis. We formulate multi-human BEV standing localization as a regression task under severe inter-occlusion in crowded scenes. To address this, we propose a unified dual-space collaborative learning framework, denoted as BEVCrowdLocator, that jointly reasons over image and BEV spaces to augment monocular inference. We incorporate standing point queries that explicitly model spatial relationships between adjacent individuals as proximal contexts, disambiguating foot-keypoint localization via proximity-aware attention. Additionally, we present the proximity-aware suppression mechanism that prioritizes consensus predictions during regression, improving robustness to occlusions. We validate our model's performance in terms of social distance measurement on the real-world surveillance benchmark. Moreover, we quantify the BEV localization performance on real soccer videos, demonstrating its potential in sports analysis.", "tldr": "", "keywords": ["multi-human BEV localization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01937914aac99cc831e5d0cffb1b7e709de9ba97.pdf", "supplementary_material": "/attachment/0bedcfa169599e34b7617e8b6d3f218891d701fa.zip"}, "replies": [{"content": {"summary": {"value": "1. This paper focuses on the challenging problem of multi-human localization in Bird's-Eye-View(BEV) from monocular images, especially in the case of crowded and occluded scenarios like sports fields or surveillance scenes.\n2. The authors propose BEVCrowd-Locator, a novel framework that formulates the task as a direct regression of individual standing points in image space, and converts to BEV space based on the predicted camera poses. \n3. The proposed dual-space learning scheme and the proximity-aware suppression mechanism are useful for mitigating duplicate predictions of the same individual.\n4. The result on the CityUHK-X-BEV dataset shows the effectiveness of the proposed framework."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed framework is novel. The dual-space learning scheme with CNN-Transformer hybrid network architecture, as well as the proximity-aware suppression mechanism, contributes to the improvement on the benchmark.\n2. The ablation studies are clear and well-designed. The ablation studies provide clear evidence for the necessity of each proposed component."}, "weaknesses": {"value": "1. While the state-of-the-art method for crowd analysis is BEV-Net, it was proposed in 2021, thus it would be nice to compare the proposed method with more recent and general proposed detectors like PairDETR.\n2. The image-to-BEV projection relies on the given camera poses estimated by the network. The framework may be limited when cameras are moving or changing. It would be nice if authors could discuss the related limitations."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DzYd4gzxsb", "forum": "wg3mF6dve0", "replyto": "wg3mF6dve0", "signatures": ["ICLR.cc/2026/Conference/Submission5855/Reviewer_uzQw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5855/Reviewer_uzQw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718610814, "cdate": 1761718610814, "tmdate": 1762918306971, "mdate": 1762918306971, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework termed BEVCrowdLocator, for multi-human BEV standing localization. Through dual-space collaborative learning between image space and BEV space, the model achieves precise prediction of individual standing points under monocular vision, thereby addressing severe occlusion issues in dense crowd scenarios. Key innovations include: Standing point regression: Incorporating trainable point queries within the Transformer architecture to model spatial relationships between adjacent individuals, enabling foot-level localisation based on neighbourhood context. Proximity-aware point matching: Merging duplicate detection points to enhance localisation robustness. Dual-space collaborative learning mechanism: During training, BEV features compensate for occlusion information in the image space, thereby improving overall prediction accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The approach exhibits a certain degree of innovation, as this paper proposes a novel architecture and corresponding design mechanism that together constitute a relatively well-structured overall framework. The experimental validation is relatively comprehensive, with the paper comparing multiple methods and conducting ablation experiments to demonstrate the effectiveness of the proposed approach. It also possesses a certain level of generalizability, as experiments were conducted on two distinct datasets, both yielding favorable results"}, "weaknesses": {"value": "1. Dual-space collaborative learning essentially extends the concept of image–BEV feature alignment (including feature projection and fusion), similar to existing approaches such as BEVFusion and the BEVFormer series. The proposed CNN–Transformer architecture follows a well-established framework and does not exhibit particular novelty. The authors are encouraged to provide a more in-depth analysis of the innovative aspects of their proposed framework, specifically clarifying its most significant distinctions or advancements compared with existing methods.\n\n2. The comparative methods employed in the experiments are relatively outdated, with only one originating from 2024 and most being based on approaches from 2022 or earlier. As a result, the comparisons do not sufficiently demonstrate the superiority of the proposed method. In addition, the experimental dataset is relatively small in scale, and the effectiveness of the proposed approach should be further verified on large-scale, real-world scenarios. Moreover, the ablation studies lack a detailed analysis of how varying the range of Top-K values influences the model’s performance."}, "questions": {"value": "1. Since both the overall concept of the collaborative framework and the CNN-Transformer architecture are established approaches lacking particular novelty, the authors are encouraged to focus on analysing the innovative aspects of the proposed framework. In particular, they should clarify the key distinctions and innovations that differentiate it from existing methods.\n\n2. The comparative methods employed in the experiments are relatively outdated, and the authors are requested to clarify the rationale behind this selection. In addition, the experimental results should be supplemented with comparisons to more recent methods. The experimental dataset is also relatively small in scale, whereas the network model contains a large number of parameters, which raises concerns about potential overfitting. Therefore, experiments on larger-scale datasets are recommended to further validate the effectiveness of the proposed method. Moreover, although the paper introduces the concept of Top-K, it does not provide corresponding experiments analysing the impact of different value ranges within this setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LfmFNHd4Gg", "forum": "wg3mF6dve0", "replyto": "wg3mF6dve0", "signatures": ["ICLR.cc/2026/Conference/Submission5855/Reviewer_LAAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5855/Reviewer_LAAS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788124650, "cdate": 1761788124650, "tmdate": 1762918306728, "mdate": 1762918306728, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of multi-human Bird's-Eye View localization from monocular images in crowded scenes with severe occlusions by proposing a novel method named BEVCrowd-Locator. Traditional approaches relying on coarse density maps or head detection—which is sensitive to height variations—struggle to achieve precise individual-level localization in dense crowds. To overcome this, the work reframes the task as a standing point (the center between the feet) regression problem and introduces a unified collaborative dual-space learning framework. This framework consists of a primary image-view branch and an auxiliary BEV branch, which compensate and provide feedback to each other via learnable mappings. By leveraging contextual information from the BEV space that is unaffected by visual occlusions, the model significantly enhances the robustness of image-space inference.\nThe core innovation lies in the introduction of trainable standing point queries. These embeddings explicitly learn proximal contexts, such as human body priors and spatial relationships between neighboring individuals, through the attention mechanisms in the Transformer architecture. This enables the model to effectively infer occluded foot locations and distinguish between adjacent persons. Furthermore, the paper proposes a proximity-aware suppression mechanism that aggregates redundant, overly close predicted points during regression, effectively mitigating duplicate detections of the same individual and further improving localization accuracy.\nComprehensive experiments validate the approach: On the CityUHK-X-BEV surveillance benchmark for social distancing compliance assessment, the method significantly outperforms state-of-the-art techniques, reducing the Chamfer Distance by over 50%. On the SoccerNet soccer video dataset, utilizing only point annotations and a simple cross-frame association module, the method achieves smooth BEV-based multi-human tracking, demonstrating exceptional performance especially in densely occluded areas like the goal box, highlighting its potential for dynamic sports analysis. Extensive ablation studies confirm the effectiveness of each proposed component. In summary, this work provides a powerful and efficient solution for precise multi-human BEV localization in complex scenes by cleverly leveraging collaborative information from both image and BEV spaces and modeling proximal relationships between individuals."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents the BEVCrowdLocator, a model aimed at multi-human localization in crowded environments, particularly addressing occlusion challenges. The core innovation is the use of dual-space collaborative learning, which combines image and Bird’s-Eye View (BEV) spaces to improve localization accuracy. The model introduces a context-driven feet localization method, utilizing proximal context via standing point queries to distinguish individual positions even under limited visibility. Additionally, a proximity-aware suppression mechanism is incorporated to reduce redundancy in predictions.\nThe paper is technically solid, with clear descriptions of the model architecture and its components. It employs transformers in both image and BEV spaces, demonstrating a strong understanding of the task at hand. Experimental results, including evaluations on CityUHK-X-BEV and SoccerNet, show that the proposed method outperforms existing approaches in terms of accuracy, particularly in occluded scenarios, while maintaining reasonable computational efficiency.\nThe structure of the paper is logical, with a clear explanation of the methodology and well-organized experimental sections. The approach is well-validated through extensive experimentation, and the model’s potential applications extend beyond surveillance and sports analytics, with relevance to areas such as autonomous driving and robotics. Overall, the paper contributes a meaningful advancement to multi-human localization in complex environments."}, "weaknesses": {"value": "While the paper presents a strong contribution, there are a few areas where further improvements could be made. First, the discussion of the model's limitations is brief, particularly regarding its performance under extreme occlusions or in highly dynamic environments. A more detailed analysis of such scenarios would help clarify the model's robustness. Additionally, the evaluation is mainly conducted on two datasets (CityUHK-X-BEV and SoccerNet), and more diverse datasets representing varied crowd behaviors and environmental conditions would better assess the model’s generalization capabilities.\nAnother weakness is the occlusion handling in moving scenarios; while the paper addresses occlusion in static settings, it lacks discussion on performance with moving cameras or in dynamic environments. The model's scalability for handling large crowds also needs more exploration, especially regarding performance when the number of individuals increases.\nFurthermore, while the paper compares feet localization with head detection in occluded settings, a deeper comparison of the two methods under varying levels of occlusion could provide a clearer understanding of the trade-offs. Finally, real-time performance and hyperparameter sensitivity are not fully addressed. Testing the model in real-time scenarios and evaluating the impact of different hyperparameters would provide a more complete picture of its practical deployment and tuning. Addressing these points would further enhance the paper’s impact and applicability."}, "questions": {"value": "To further improve the paper, I have a few questions and suggestions for the authors. First, could you provide additional details on how the BEVCrowdLocator model performs under extreme occlusion scenarios, such as when individuals are densely packed or largely obscured by other objects? Secondly, while the paper evaluates the model in static settings, how does it handle moving cameras or dynamic viewpoints? A discussion of performance with changing perspectives would be valuable. Third, the paper demonstrates good results with smaller crowds, but how does the model scale when dealing with larger crowds in real-world scenarios? It would be helpful to explore scalability in dense crowds. Additionally, while feet localization is emphasized, could you provide a deeper comparison with head detection, especially in more complex scenarios with high occlusion? A more thorough analysis of this trade-off would be beneficial. Regarding real-time performance, could you share how the model performs on live video feeds or in resource-constrained environments, such as edge devices? Lastly, it would be useful to include an ablation study or analysis of hyperparameter sensitivity, particularly regarding the number of standing point queries and transformer settings. Addressing these aspects would improve the understanding of the model’s limitations, robustness, and potential real-world deployment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TPVazUycTE", "forum": "wg3mF6dve0", "replyto": "wg3mF6dve0", "signatures": ["ICLR.cc/2026/Conference/Submission5855/Reviewer_8r8w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5855/Reviewer_8r8w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888191110, "cdate": 1761888191110, "tmdate": 1762918306375, "mdate": 1762918306375, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on multi-human standing (feet) localization in crowded scenes. Specifically, it introduces a unified dual-space collaborative learning with a dual-branch network. For the spaces, it uses both image/BEV spaces via image-to-BEV/BEV-to-BEV transformation with homography. For architecture, the primary branch (a hybrid CNN-transformer network) regresses standing locations in the image space. The auxiliary branch estimates standing locations in the BEV space and provides features for the primary branch. Lastly, proximity-aware suppression is used to distinguish close points to enhance point matching loss. Experiments on CityUHK-X-BEV and SoccerNet show that the proposed method outperforms existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The usage of both image/BEV spaces via image-to-BEV/BEV-to-BEV transformation can effectively combine the advantages of different spaces. This is a high-efficiency framework."}, "weaknesses": {"value": "1. The input is confusing. \n- L.28-38 emphasizes that this paper focuses on BEV perception, which is vague. L.178 only states that the input is an image. Based on Figs.1-4, the input is image view (IV) maps and only the results are shown in BEV. The primary branch is the IV branch, which is also confusing. Because it is not clear about the usage of BEV-to-image for the input (Fig.1). \n- Existing sotas are not designed for BEV, which may raise concerns about fairness. It would be better to compare with BEV methods (L.124-136) instead of method modification (L.320).\n\n2. The difference compared to BEV-Net should be discussed in more depth.\n- BEV-Net also uses the dual-space design with homography. The main difference seems to be the usage of Transformer (L.41).\n- the paper discusses about feet vs head localization (L.54 and 398). However, the methods that use the head for detection are limited. BEV-Net uses head and feet for detection.\n\n3. This paper highlights that the new formulation can handle occlusion in crowded scenes (L.14). However, there is no in-depth analysis. Quantitative or localization results like Fig.4 cannot reveal the reasons. It would be better to show the intermediate results like attention map in two spaces for occlusion."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pbnsogOAQM", "forum": "wg3mF6dve0", "replyto": "wg3mF6dve0", "signatures": ["ICLR.cc/2026/Conference/Submission5855/Reviewer_oK74"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5855/Reviewer_oK74"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077222313, "cdate": 1762077222313, "tmdate": 1762918306078, "mdate": 1762918306078, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}