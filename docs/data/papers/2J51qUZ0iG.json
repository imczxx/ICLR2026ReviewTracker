{"id": "2J51qUZ0iG", "number": 9895, "cdate": 1758147144755, "mdate": 1759897688519, "content": {"title": "Fantastic Pretraining Optimizers and Where to Find Them", "abstract": "AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2$\\times$ speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups.\nTo address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B–1.2B parameters) and data-to-model ratios (1--8$\\times$ the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training.\nFirst, optimal hyperparameters for one optimizer may be suboptimal for another, making blind hyperparameter transfer unfair. \nSecond, the actual speedup of many proposed optimizers over well-tuned baselines is lower than claimed and decreases with model size to only 1.1$\\times$ for 1.2B parameter models. Thirdly, comparing intermediate checkpoints before reaching the target training budgets can be misleading, as rankings between two optimizers can flip during training due to learning rate decay.\nThrough our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners --- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4$\\times$ over AdamW for 0.1B parameter models to merely 1.1$\\times$ for 1.2B parameter models.", "tldr": "Fair comparisons reveal that new optimizers only offer modest speedups over AdamW; matrix level optimizers' advantage shrinks with scale—from 1.4× at 0.1B to just 1.1× at 1.2B parameters—once proper hyperparameter tuning and evaluation are applied.", "keywords": ["optimizer", "benchmarking", "pretrain"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/644625deac134498a0d624373e075069873cf827.pdf", "supplementary_material": "/attachment/16d8aeabd609c47808be8933467ac1e7a351ddfd.zip"}, "replies": [{"content": {"summary": {"value": "The paper benchmarks 11 pretraining optimizers (e.g., AdamW, Muon, Soap, Mars, Kron) across model scales up to 1.2B parameters and multiple Chinchilla ratios. It introduces a three-phase tuning protocol to ensure fair comparison and finds that most reported speedups over AdamW shrink to ~1.1–1.4× after proper tuning, especially at larger scales. Matrix-based optimizers perform slightly better on small models but lose advantage as size grows."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper presents a comprehensive optimizer benchmark for pretraining, with consistent tuning and scaling analysis.\n* The proposed three-stage hyperparameter search ensures that each optimizer is fairly tuned, reducing the common bias of under-optimized baselines in prior studies.\n* The work clarifies that many claimed gains in prior work are overstated and provides actionable guidance for future optimizers' usage."}, "weaknesses": {"value": "* The work is entirely empirical. Although this is acceptable for benchmarking, it lacks a theoretical analysis explaining why matrix-based optimizers lose advantage with scale or high data-to-model ratios. Even basic intuition about curvature estimation or variance scaling would enrich interpretation.\n* Despite the emphasis on “LLM pretraining,” the largest model tested has 1.2B parameters. It remains unclear whether the diminishing returns of matrix-based optimizers persist at realistic scales (10B–70B+).\n* The paper normalizes speedup by “tokens to reach a loss,” assuming step overhead is <10%. Please also report wall-clock time, throughput, optimizer step latency and memory footprint. This will show whether matrix preconditioning’s modest token gains translate to real training gains at scale."}, "questions": {"value": "pls see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oCZjZu6FWy", "forum": "2J51qUZ0iG", "replyto": "2J51qUZ0iG", "signatures": ["ICLR.cc/2026/Conference/Submission9895/Reviewer_DRRU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9895/Reviewer_DRRU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943567304, "cdate": 1761943567304, "tmdate": 1762921356897, "mdate": 1762921356897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a comprehensive study of eleven deep learning optimizers to find the pareto frontier across four model sizes (0.1B–1.2B parameters) and data-to-model ratios (1–8× the Chinchilla optimum). It provides valuable practical insights and methodological guidance to find the optimal pretraining optimizer, though it is primarily an empirical study with limited theoretical contributions."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n- The paper’s deep dive into evaluating multiple optimizers by individually tuning hyper-parameters is very interesting.\n- The finding that speedups in newer optimizers over AdamW come down to 1.1X at 1.2B model scale at 8X Chinchilla is intriguing.\n- The paper is methodical and analyses the optimizers in an under explored regime (of 1.2B model size and 8X Chinchilla data size) in terms of both model and data size. It has the potential to serve as a good reference for the research community while making decisions on pre-training large language models.\n- The authors look at a range of benchmarks to evaluate the optimizers."}, "weaknesses": {"value": "Scope for improvement:\n- The largest models the authors experiment with is 1.2B parameters, while SOTA models are at-least 100X larger. This might not be directly applicable to SOTA open source models, because even the authors claim that ranking of optimizers changes with data / model scale.\n- On a similar note, a recipe to evaluate optimizers on bigger models and higher data ratios would be very interesting to look at.\n- The paper focusses a lot on empirical results, but doesn’t add any theoretical contributions."}, "questions": {"value": "Like mentioned in the scope for improvement section, I am curious what the authors think about:\n- a receipe to evaluate optimizers on SOTA models with even higher data ratios.\n- theoretical understanding about some of the trends are missing. For instance, in Figure 1, the last line says \"the three matrix-based optimizers converge to a similar loss in an overtrained setting.\" A more in depth theoretical analysis would be very helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NEyOInirXU", "forum": "2J51qUZ0iG", "replyto": "2J51qUZ0iG", "signatures": ["ICLR.cc/2026/Conference/Submission9895/Reviewer_pAs8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9895/Reviewer_pAs8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945344327, "cdate": 1761945344327, "tmdate": 1762921356654, "mdate": 1762921356654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A thorough pretraining evaluation study is proposed, comparing and contrasting recent matrix-based optimizers such as Muon, SOAP, etc., with scalar-based optimizers such as AdamW. Different model sizes (130M–1.2B) and data budgets in terms of Chinchilla optimal (1x–8x) with the Llama 2 architecture are considered for benchmarking. Key findings include, among others, a decrease in the performance of matrix-based optimizers relative to AdamW as model size increases, and the relative performance differences between different matrix-based optimizers (Muon, SOAP, Kron) disappear in the over-trained regime."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Optimizers are benchmarked under strong settings: diverse data mixtures (code, text, math), language, chinchilla-optimal data budgets (1-8x), various model scales (130m-1.2B) and modern matrix-based baselines (muon, scion, soap).\n- Tuning framework to identify scale-sensitive hyperparameters for each optimizer in small-scale settings with sweeps and a scaling law to predict optimal scale-sensitive hyperparameter values for large-scale experiments based on model size and data."}, "weaknesses": {"value": "- Optimizer optimal hyperparameters are chosen with a one-at-time sweep over hyperparameters which may miss correlation among hyperparameters as obtained from a dense grid search. This could have non-trivial downstream impact on the findings of the study, since optimal hyperparameters from a grid search could be quite different. The coordinate descent is done over too many hyperparameters including non-trivial ones with a few runs (~10-20 which is way less compared to prior work e.g. [1]), and I'm not sure if the final hyperparameters obtained from this one-at-a-time coordinate descent are actually optimal compared to a standard grid search over hyperparameters. \n- Schedule for Muon in appendix tables (Table 83-88) is mentioned as \"linear\" -- does it mean just a linear decay? On the other hand, figure 6 shows cosine decay with warmup for different optimizers. Can authors confirm the choice of scheduler and if it's the standard cosine decay?\n- It should be noted that benchmark is highly specific -- llama 2 architecture with text-only datasets, fixed sequence length (4096) and upto 1.2B scale. Hence, findings such as matrix-shaped optimizers decrease in speedup with model size, may change in multi-modal setup with vision tokens and different/modern architectures such as MoEs which are realistically used in the pretraining of modern LLMs.\n\n[1] Small Batch Size Training for Language Models (NeurIPS 2025), https://arxiv.org/abs/2507.07101"}, "questions": {"value": "- Can authors please elaborate on the search space of hyperparameters used for each optimizer in their sweeps and how was it chosen?\n- During coordinate descent - how are hyperparameters sampled and what scale is used (uniform or log-scale)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Mtwstng39", "forum": "2J51qUZ0iG", "replyto": "2J51qUZ0iG", "signatures": ["ICLR.cc/2026/Conference/Submission9895/Reviewer_3kks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9895/Reviewer_3kks"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762000809403, "cdate": 1762000809403, "tmdate": 1762921356271, "mdate": 1762921356271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper benchmarks popular optimizers on LLM pretraining, including AdamW as a baseline. It tunes their hyperparameters using a three phased methodology. In phase 1 and phase 2 the hyperparameters are searched using coordinate descent i.e., at every iteration only a single hyperparameter is searched, while all others are held fixed at their current best values, and this is repeated until convergence. In phase 2, which involves larger scale experiments, only a subset of the hyperparameters are tuned, based on if they are determined scale sensitive using phase 1 results. Finally, phase 3 fits scaling laws from the hyperparameters found in phase 2. The benchmark trains Llama 2 architectures of four sizes, from 0.1B to 1.2B parameters and reports validation loss on C4-EN and on downstream tasks. The paper concludes that, while newer optimizers show non-negligible speedups over AdamW, the speedups are more moderate than those stated in existing literature and decrease as model size increases."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important and timely topic. Pretraining is indeed the most resource-demanding stage in the LLM training process and thus increasing the optimizer's speed is of great research interest. I also agree with the authors that many works that propose new optimizers lack fair and rigorous evaluation, leading to unrealistic speedups (e.g., 2-3x speedups over AdamW). \n\nA wide range of popular optimizers is evaluated, from different representative categories.\n\nThe experimental settings are clearly stated, and the accompanying code with the sweep configs ensures reproducibility of the results."}, "weaknesses": {"value": "**Suitability of Coordinate Descent for hyperparameter tuning:**\n\nMy main concern is the use of Coordinate Descent (CD) for hyperparameter tuning. The paper's primary contribution, as stated in the introduction, is the claim that \"the hyperparameters are near-optimal for each setting\", and this is reiterated in the Phase 2 results section: \"Combined with the results in Phase I, we obtain a set of near-optimal hyperparameters\".\n\nHowever, it is well established in the literature that many optimizer hyperparameters are coupled. For example, the learning rate and batch size are often considered to follow  a  linear or square root relationship [1,2]. Recent studies suggest that the weight decay also shares a similar coupling with the batch size [2]. [3] also demonstrates a strong relationship between the Adam beta2 parameter and batch size. Similar couplings are likely to exist for other hyperparameters, such as learning rate and warmup steps (fewer warmup steps could likely benefit from smaller learning rates) or learning rate and gradient norm clipping threshold.   \n\nIn short, the objective (validation loss) is far from separable across coordinates (hyperparameters), making it uncertain whether CD can truly obtain \"near-optimal\" hyperparameters rather than getting stuck at \"bad\" local minima. In addition, the three phased methodology increases the likelihood of errors aggregating from one phase to another.\n\nThe paper includes no ablations or error analysis (e.g., as in [4]) to validate the tuning methodology and support the claim of \"near-optimal hyperparameters\". The ablations in the Appendix (\"1-dimensional ablations centered around the found configuration\") do not address such possible limitations of CD, since each step varies only a single parameter at a time. For example, the batch size is increased from 128 to 256 and 512, while the learning rate as well as all other hyperparameters remain fixed. \n\nIn short, since all of the paper's results rely on the robustness of the evaluation methodology, it is critical that this methodology is fully justified and potential limitations of CD in this setting be discussed. The paper makes a strong claim of \"near-optimal hyperparameters\", but absence of such analysis make this claim uncertain and this represents a major limitation of the work.\n\n**Speedup Results Comparability:** \n\nThe paper defines the speedups based on the number of tokens needed to reach a given loss, without considering the time each optimizer needs to reach that loss. However, since the batch size is treated as a tunable hyperparameter, some optimizers use larger batch size than others in certain settings. For example, on 300M model for 4x Chinchilla Data, AdamW uses batch size 256 (Table 102) and Adam-Mini 128 (Table 132). As a result, it is unclear if the reported results are directly comparable, since optimizers with larger batch size may have increased throughput (assuming sufficient hardware), which could lead to a faster convergence in wall-clock time. \n\nIn short, it is debatable if the batch size should be treated as a regular hyperparameter, given that it influences the throughput and can affect the speedup in terms of wall-clock time. Furthermore, it is common for users to want to choose the best optimizer for a given batch size constrained by memory limitations, and the results provided do not help with this decision-making process.\n\n**Single Architecture Type Used:**\n\nThe paper only tests the optimizers on four models of the same family (LLaMA 2). Including a broader range of model families would have been ideal to ensure the results are generalizable. For example, including more recent models (e.g., Gemma 2 or Qwen 3) or exploring different model types such as Mixture-of-Experts. Additionally, while scaling up to larger models (7B+) might be resource-prohibitive, testing on models at this range would more closely approximate production-level model sizes.\n\n[1] LARGE BATCH OPTIMIZATION FOR DEEP LEARNING: TRAINING BERT IN 76 MINUTES, 2020.\n\n[2] Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training, 2025.\n\n[3] Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful, 2025.\n\n[4] Practical Efficiency of Muon for Pretraining, 2025."}, "questions": {"value": "Is Coordinate Descent (CD) a common methodology for tunning optimizer hyperparameters? Could the authors cite relevant works (preferably in the setting of LLM pretraining) to support this? Could the authors provide supporting experimental evidence or analysis for the claim of \"near-optimal hyperparameters\" and also discuss potential limitations of CD (if any) for the given setting?\n\nIt would be useful if the authors could provide some statistics about the number of runs conducted per setting. For example, for each unique triplet (Optimizer, Model size, Chinchilla Ratio) how many pretraining experiments where performed in each phase? Also, could the authors provide how long each experiment took approximately (if possible translated in H100 GPU hours). This would help me get a better understanding of the scale of the benchmark.\n\nIt is mentioned in the paper that the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing to only 1.1x for 1.2B parameter models. Could the authors discuss possible reasons for this trend? Additionally, for models larger than 1.2B parameters, do the authors expect the speedup to decrease further?\n\nIn Table 5 (Scaling-Sensitive Hyperparameters), some optimizers appear to have more scale-sensitive hyperparameters than others. For example, AdamW appears scale-sensitive to learning rate, warmup, weight decay and batch size, whereas Muon only to learning rate. Also, Adam-Mini is not scale-sensitive to batch size, while AdamW is. Could the authors provide any further insights on these differences"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o9zv4D8UpC", "forum": "2J51qUZ0iG", "replyto": "2J51qUZ0iG", "signatures": ["ICLR.cc/2026/Conference/Submission9895/Reviewer_xiBh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9895/Reviewer_xiBh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9895/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762474168092, "cdate": 1762474168092, "tmdate": 1762921355803, "mdate": 1762921355803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}