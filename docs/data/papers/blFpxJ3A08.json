{"id": "blFpxJ3A08", "number": 17597, "cdate": 1758277985113, "mdate": 1759897165787, "content": {"title": "LPFQA: A Long-Tail Professional Forum-based Benchmark for LLMs' Evaluation", "abstract": "Large Language Models (LLMs) have made rapid progress in reasoning, question answering, and professional applications; however, their true capabilities remain difficult to evaluate using existing benchmarks. Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To address this gap, we propose LPFQA, a benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. LPFQA introduces four key innovations: fine-grained evaluation dimensions that target knowledge depth, reasoning, terminology comprehension, and contextual analysis; a hierarchical difficulty structure that ensures semantic clarity and unique answers; authentic professional scenario modeling with realistic user personas; and interdisciplinary knowledge integration across diverse domains. We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks. LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.", "tldr": "", "keywords": ["long-tail knowledge", "professional forum", "LLM evaluation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ce0fdc12feebdb659f72b23dfa25ac49e7572b6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LPFQA, a new benchmark designed to evaluate the ability of Large Language Models (LLMs) to handle professional, long-tail knowledge. The benchmark consists of 505 questions gathered from 20 professional forums across different disciplines. Based on tests conducted on 12 mainstream LLMs, the study reveals significant performance disparities among them."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.  The paper accurately identifies a critical gap in current LLM evaluation—the lack of assessment for real-world long-tail knowledge—and innovatively uses professional technical forums as a data source to address this challenge. This approach ensures that the benchmark's questions are grounded in the authentic needs of practitioners, lending them high authenticity.\n\n2.  The  construction process outlined in the paper, which combines automated generation (using MLLMs/LLMs) with expert verification, is systematic and well-defined. This framework provides a valuable and replicable methodology for creating similar benchmarks for other specialized long-tail knowledge domains.\n\n3.   The paper offers valuable preliminary insights through its ablation studies."}, "weaknesses": {"value": "1. The Core Concept of \"Long-Tail\" is Ill-Defined and Unsubstantiated: The paper's central claim rests on the term \"long-tail knowledge,\" yet this concept is never operationally defined or empirically verified. The authors operate on the unsubstantiated assumption that any question originating from a specialized professional forum automatically qualifies as \"long-tail.\" This is a critical conceptual flaw that lacks evidentiary support.\n\n2. The Data Curation and Verification Process Lacks Critical Transparency: The methodology section (3.2) describes a procedural pipeline but omits the crucial details required to assess its validity and potential biases. Key decisions regarding the selection of forums, the qualifications and instructions for the \"professional experts,\" and measures for inter-rater reliability are completely absent, making the process a \"black box\" and impossible to scrutinize.\n\n3. The Dataset is Insufficiently Small and the Filtering Logic is Contradictory: With only 505 questions spread across 20 domains, the dataset is too small to support robust conclusions about the capabilities of large-scale models. Furthermore, the decision to filter out questions that all models answered correctly is logically inconsistent with the \"long-tail\" premise; if a question is truly long-tail, it should not be easily solved by all general-purpose models. This suggests the filtering may be artificially creating difficulty rather than reflecting the data's natural properties.\n\n4. Ablation Studies are Oversimplified and Lead to Unjustified Conclusions: The paper draws strong, overly broad conclusions from simplistic ablation studies. Attributing the failure of a Code Interpreter solely to \"reasoning being irrelevant,\" or the failure of search tools solely to the \"nature of long-tail knowledge,\" ignores numerous significant confounding variables (e.g., model knowledge cut-off dates, poor query generation, the specific type of reasoning being tested).\n\n5. The Evaluation Protocol Has Unaddressed Limitations: The paper fails to discuss the well-documented limitations of its chosen evaluation method, particularly the use of an \"LLM-as-Judge\" for open-ended questions. It also omits essential information about the final composition of the benchmark (e.g., the ratio of multiple-choice to short-answer questions), hindering a complete interpretation of the results."}, "questions": {"value": "1. On Defining \"Long-Tail\":\nHow do you operationally define \"long-tail knowledge\" in a way that is measurable and falsifiable, beyond simply its origin in a professional forum?\nWhat empirical evidence can you provide to demonstrate that the concepts and questions within LPFQA are indeed \"low-frequency\" or \"rare\" within large-scale pre-training corpora?\nHow does your methodology distinguish between a question that is genuinely \"long-tail\" (rare knowledge) versus one that is simply \"difficult\" (requiring complex reasoning about potentially common knowledge)?\n\n2. On Data Curation Transparency:\nWhat were the specific, objective criteria used to select the source forums? This is essential for understanding the dataset's scope and potential biases.\nCould you please detail the qualifications of the \"professional experts\" for each domain? What specific instructions (manual) were they provided for validation, and what was the measured inter-rater reliability?\nHow do you account for the severe data imbalance across domains (e.g., 68 questions in Physics vs. 3 in Data Science)? Is this an artifact of the collection method or a reflection of the domains themselves?\n\n3. On Dataset Integrity and Filtering:\nGiven the small and imbalanced sample size, how can you justify making strong claims about a model's specific domain strengths or weaknesses?\nIf a significant portion of the initial data could be answered correctly by all models, doesn't this challenge the core premise that the data is inherently \"long-tail\"? What percentage of questions were removed during the \"all correct\" and \"all incorrect\" filtering stages?\n\n4. On the Interpretation of Ablation Studies:\nRegarding the Code Interpreter experiment, how did you rule out alternative explanations for the performance drop, such as the model's inability to translate non-computational problems into effective code?\nFor the search tool experiment, how did you control for confounding variables like the models' knowledge cut-off dates or the quality of the LLM-generated search queries?\n\n5. On the Evaluation Protocol:\nWhat steps were taken to validate the reliability of the \"LLM-as-Judge,\" especially for nuanced short-answer questions, and to mitigate its known biases (e.g., preference for verbosity)?\nWhat is the final distribution of multiple-choice versus short-answer questions in the LPFQA benchmark? How might this composition affect the interpretation of the overall model scores?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PMqVp4JrLD", "forum": "blFpxJ3A08", "replyto": "blFpxJ3A08", "signatures": ["ICLR.cc/2026/Conference/Submission17597/Reviewer_fdUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17597/Reviewer_fdUJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760693637544, "cdate": 1760693637544, "tmdate": 1762927460983, "mdate": 1762927460983, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LPFQA, a dataset containing 505 questions drawn from professional forums across various fields. The authors motivate their work by highlighting limitations in existing benchmarks and mention that current long-tail benchmarks lack complexity, while conversational benchmarks provide limited control over expertise and difficulty levels. To construct LPFQA, the authors propose a three-stage pipeline for dataset creation involving data collection, question generation, and expert verification. They then evaluate the dataset on 12 large language models and analyze their performance across different domains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-written and easy to follow.\n- The dataset is collected from professional forums, offering real-world scientific questions that add practical relevance.\n- The number of models used in evaluation is through."}, "weaknesses": {"value": "- The dataset size is limited. Although the authors claim to cover multiple fields, Figure 2 shows that many domains contain fewer than 20 questions, which is insufficient to support the performance evaluations made in Section 4.1.\n- While the authors state that the dataset creation process is end-to-end automated and scalable, the expert verification stage introduces a manual expert verification, which limits the overall scalability of the approach.\n- The evaluation design has several shortcomings:\n  - The paper claims that LPFQA assesses long-tail knowledge, but this concept is not formally defined, nor is it clearly demonstrated through experiments. It would help if the authors clarified whether the ablations in Section 4.2.2 aim to support this claim, and if so, a comparison with existing long-tail benchmarks would strengthen the argument.\n  - The paper states that the difficulty level of the dataset is adjusted, but no clear definition or validation of difficulty is provided. It is unclear how this factor influences LLM performance.\n  - Multiple-choice and short-answer questions are not analyzed separately, which could obscure insights about model performance across question types.\n  - The paper lacks a detailed error analysis, leaving unclear why models fail on questions in this dataset or what types of errors are most common for LPFQA."}, "questions": {"value": "- How do you ensure that the LLM-generated distractors are of high quality and guaranteed to be incorrect (i.e., that each multiple-choice question has only one valid answer)?\n- What portion of the dataset consists of multiple-choice questions versus short-answer questions?\n- How do you ensure that the LLM does not hallucinate or introduce errors when extracting questions from screenshots?\n- Could you provide more details about the experimental setting described in Section 4? Specifically, what temperature was used when prompting the LLMs to answer the questions, and did you experiment with chain-of-thought or few-shot prompting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OaHSFhdMdu", "forum": "blFpxJ3A08", "replyto": "blFpxJ3A08", "signatures": ["ICLR.cc/2026/Conference/Submission17597/Reviewer_wqkd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17597/Reviewer_wqkd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761702009709, "cdate": 1761702009709, "tmdate": 1762927459317, "mdate": 1762927459317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new benchmark dataset that targets at long-tail professional benchmark to evaluate LLMs. It proposes questions from 20 fields and 502 tasks in total. Experiments indicate that even the strongest LLMs still cannot fully solve this task."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Long-tail QA benchmark for professional domain, as long as we have clear definition, is an interesting and useful task."}, "weaknesses": {"value": "Unfortunately this is below the bar of this conference. \n1. The paper focuses on long-tail QA, but it lacks a clear definition of what constitutes “long-tail.” This concept should be distinguished from cases where an LLM lacks parametric knowledge. In my view, the proposed task does not align well with the common understanding of “long-tail” .\n2. The experiments require significant improvement. The current evaluation is only limited to running different LLMs or API-based systems, without providing meaningful insights into why these models fail.\n3. Significant amount of this paper is written by LLMs."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5cnD5EMJNu", "forum": "blFpxJ3A08", "replyto": "blFpxJ3A08", "signatures": ["ICLR.cc/2026/Conference/Submission17597/Reviewer_YZDZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17597/Reviewer_YZDZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913814417, "cdate": 1761913814417, "tmdate": 1762927457824, "mdate": 1762927457824, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LPFQA, a long-tail benchmark derived from professional forums. A semi-automated pipeline scrapes online forums, captures screenshots of the pages, then tasks an MLLM with extracting and re-writing a QA example, with examples then verified by a human. The authors then baseline ten models on this benchmark, doing simple prompting, and also exploring code-interpreter and search tool-enabled variants."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors pose an interesting framing of finding a sweet-spot in task difficulty that reflects real-world 'professional' settings, contrasting this with 'head' examples that were likely seen during pre-training and extreme examples that are too challenging (HLE). With that said, I think this significance of this niche target area could be motivated further with fuller comparison to existing works."}, "weaknesses": {"value": "This seems to be a *very* initial draft of this work, and requires significant extra work in both motivation, formulation, and evaluation. Further, the paper requires extensive writing and presentation improvements. The novelty of the contribution is especially questionable -- the authors largely scrape online data without truly justifying if it matches the long-tail distribution they target. In summary, the work needs comprehensive bolstering."}, "questions": {"value": "Some comments and questions that I hope are helpful in improving this work:\n\n- Some domains are extremely small (3 items). \n- Why screenshots instead of text during the scraping process? Is this simply to ease the parsing process? If screenshots are required for multimodal context, can quantify the proportion of items that truly require vision?\n- Can you better motivate why this particular data reflects the long-tail use case you target? It would be ideal to provide quantitative analysis of this, rather than simple qualitative comparisons with existing works. \n- Can you provide full details of your expert annotation process? Did you hire experts from each of the domains? Do you check annotator agreement in any way?\n- Is any diversity injected during rewriting, or is the QA re-writing step largely to clean-up the example and map it into text?\n- Have you ensure it is legal to scrape all of these websites and use their content for AI-related purposes? I would suggest detailing this clearly in the paper, as many forums now have restrictions.\n\nPresentation / Typos / Grammar Issues:\n- **Citation format is incorrect**. Citations should be placed within parentheses.\n- Inconsistent totals in dataset size (502 vs. 505). \n- Define LFPQA within in the paper text\n- Some of your phrasing is incorrect. E.g., “Diversity domains knowledge” → “Diverse domain knowledge”\n- Normalize quotation marks and spacing: fix curly vs. straight quotes and stray spaces (e.g., “LPFQA ’s”).\n- Simply the radar chart presentation. It is hard to grasp any key takeaways as there is too much information presented.\n- Section 3.3 can be moved to the Appendix\n\nI hope this feedback is helpful!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uvKisy2llu", "forum": "blFpxJ3A08", "replyto": "blFpxJ3A08", "signatures": ["ICLR.cc/2026/Conference/Submission17597/Reviewer_oqiX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17597/Reviewer_oqiX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17597/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762204077223, "cdate": 1762204077223, "tmdate": 1762927453034, "mdate": 1762927453034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}