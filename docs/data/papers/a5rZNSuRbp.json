{"id": "a5rZNSuRbp", "number": 14608, "cdate": 1758239897646, "mdate": 1763359739582, "content": {"title": "Free-View Robot Manipulation: Visuomotor Policy by Calibration Diffusion", "abstract": "Visuomotor policies have demonstrated great potential in robot manipulation tasks.\nHowever, current robot manipulation tasks are often observed from fixed viewpoints. \nOnce the viewpoints change, the trained policy becomes ineffective.\nThis limitation curbs the generalization of robot manipulation and impedes its application.\nTo address this issue, we make a comprehensive study by presenting novel free-view manipulation tasks that enables the robot to perform actions from any viewpoint.\nFirstly, we construct a free-view dataset, which encompasses 8 tasks with over 5,000 episodes sourced from the ISAAC SIM simulation  environment. \nEach episode records robot manipulation behaviors from different viewpoints.\nSecondly, we propose a calibration diffusion policy, which utilizes an additional calibration network to enhance the adaptability of the diffusion policy to different viewpoints. \nIn particular, we adopt two-stage curriculum training to make the calibration diffusion policy converge rapidly.\nFinally, we conduct a wealth of experiments on the free-view dataset. \nThe obtained results demonstrate the effectiveness of the calibration diffusion policy. \nThis also means that we have built a new benchmark for free-view manipulation.", "tldr": "The robot complete the manipulation under Free-View", "keywords": ["Diffusion Policy; Free View;Visuomotor Robot Manipulation;"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a952dd75859241255e059418e786ff6c3f7effdd.pdf", "supplementary_material": "/attachment/3eccac92c54491fd553e4ad1ab1bd6fe7a63bf27.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the free-view robot manipulation task to overcome the fixed viewpoint limitation in current visuomotor policies. The authors construct a new dataset with 8 tasks and over 5,000 simulation episodes, each from varying viewpoints. Their solution is a calibration diffusion policy, which uses a novel calibration network and a two-stage training curriculum to achieve effective, viewpoint-invariant manipulation. The method's success is validated through extensive experiments, establishing a strong new benchmark for this problem."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. the topic the paper focuses on is an interesting and crucial problem in current visuomotor policy for robot manipulation.\n2. the paper is well-written with little typos.\n3. the author try to build a benchmark to systematically analyze the free-view robot manipulation problem."}, "weaknesses": {"value": "1. The contribution of the proposed dataset is undermined by its limited scale and lack of detailed specification. The paper would be strengthened by providing a comprehensive description of the camera pose distribution used for data collection, including the ranges for azimuth, elevation, and distance from which viewpoints were sampled. \n2. The method lacks of specific design for the free-view robot manipulation problem. The author simply add the calibration parameters to the current diffusion policy pipeline without any insight of this problem. \n3. The statement of the two-stage training pipeline is confused.  The purpose of introducing random noise states in the first stage is unclear and requires elaboration. Please clarify the  training strategy with figure 3. \n4. The experimental evaluation lacks critical details regarding the viewpoint split between training and testing. It is essential to clarify whether the tested viewpoints were entirely unseen during training or were simply a held-out set from the same distribution. Please present more details of experiment evaluation.\n5. The most significant weaknesses of the paper is lack of real-world experiments. The absence of real-world experiments leaves the method's practicality, robustness to perceptual noise unproven."}, "questions": {"value": "Please see the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethic concerns."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cOxQ0Xu4KI", "forum": "a5rZNSuRbp", "replyto": "a5rZNSuRbp", "signatures": ["ICLR.cc/2026/Conference/Submission14608/Reviewer_dmaV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14608/Reviewer_dmaV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761399077175, "cdate": 1761399077175, "tmdate": 1762924990931, "mdate": 1762924990931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Upload a new version of the PDF"}, "comment": {"value": "Thank you for the reviewers' comments. \nWe have noted the reviewers' concerns regarding real-world experiments, and in fact, we have been continuously attempting to apply the Calibration Diffusion Policy method to real-world tasks.\n\nTherefore, we have updated the PDF to a new version. The key differences from the previous PDF version are as follows:\n1. Added two real-world tasks: Real Pick Cup and Real Close Box as shown in Figure 2.\nFor real-world tasks, we perform camera calibration before each data collection and collect 500 trajectories for each task.\nThe corresponding experimental results are also shown in Table 2 and Figure 5.\n2. Due to the length limit, we have moved the description of the tasks in the dataset to the APPENDIX A.1 and supplemented the camera position range for free-view settings.\n3. In Appendix A.5, we generate image pairs for both Free-View and Canonical-View, and add a comparison with multi-view methods."}}, "id": "mPhozPcq08", "forum": "a5rZNSuRbp", "replyto": "a5rZNSuRbp", "signatures": ["ICLR.cc/2026/Conference/Submission14608/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14608/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14608/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763361052983, "cdate": 1763361052983, "tmdate": 1763361583485, "mdate": 1763361583485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to deal with the viewpoint generalization problem for robotic manipulation. The author constructs a free-view dataset, which encompasses 8 tasks with over 5,000 episodes sourced from the Isaac Sim simulation environment. Then, a calibration diffusion policy training method is introduced to enhance the adaptability of the diffusion policy to different viewpoints. To verify their method, the author does some experiments based on Issac Gym."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method is novel and the problem you concern is important.\n2. For simulation, the author introduce well-designed free-view settings."}, "weaknesses": {"value": "1. No multi-view related method is included as baseline. There have been many methods proposed to deal with viewpoint disturbance setting, like RoboUniview[1], Maniwhere[2], ReViWo[3] or MV-MWM[4]. However, no kind of such baselines is included.\n\n2. We desire a real-world experiment to validate the effectiveness of your method.\n\n3. For real-world tasks, despite using self-collected data, we wonder if some open-source multi-view data could also be used to better improve your model capability.\n\nReferences:\n[1] RoboUniView: Visual-Language Model with Unified View Representation for Robotic Manipulation. \n[2] Learning to Manipulate Anywhere: A Visual Generalizable Framework For Reinforcement Learning.\n[3] Learning View-invariant World Models for Visual Robotic Manipulation.\n[4] Multi-View Masked World Models for Visual Robotic Manipulation."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zTxzentib6", "forum": "a5rZNSuRbp", "replyto": "a5rZNSuRbp", "signatures": ["ICLR.cc/2026/Conference/Submission14608/Reviewer_3urr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14608/Reviewer_3urr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761614475444, "cdate": 1761614475444, "tmdate": 1762924990113, "mdate": 1762924990113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Free-View Robot Manipulation framework that enables robots to perform visuomotor manipulation tasks from arbitrary camera viewpoints. It builds a new Free-View Dataset (8 tasks, 5000+ episodes) within Isaac Sim and proposes a Calibration Diffusion Policy (Cali. DP) that integrates camera calibration parameters into diffusion-based visuomotor policy learning. Extensive simulation experiments demonstrate improved generalization to viewpoint variations compared to baselines like ACT, Diffusion Policy, and Flow Policy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses an important and underexplored challenge (viewpoint generalization) in visuomotor policy learning, with clear motivation and relevance to real-world deployment.\n2. It constructs a well-designed free-view dataset with diverse manipulation tasks and calibration annotations, providing a valuable benchmark for future research.\n3. Experimental results are comprehensive, including comparisons with strong baselines, ablation studies, and multiple evaluation settings, demonstrating meaningful performance gains."}, "weaknesses": {"value": "1. All experiments are conducted purely in simulation (Isaac Sim) without any real-world validation; thus, the claimed generalization to varying viewpoints remains unverified under physical-world noise and imperfections.\n2. The proposed Calibration Diffusion Policy shows limited methodological novelty. It mainly adapts ControlNet-like conditioning to diffusion policy rather than introducing a fundamentally new idea.\n3. The dataset’s task diversity and complexity are relatively low (no deformable or dual-arm tasks), limiting its generalizability and practical relevance.\n4. The paper’s claim that existing methods fail under viewpoint changes is insufficiently supported; it lacks direct comparisons with modern multi-view or hybrid-view frameworks (e.g., OpenVLA, RDT, or $\\pi_0$) that already handle varying viewpoints effectively."}, "questions": {"value": "1. Can the proposed Calibration DP be validated on a real robot to assess robustness under real-world calibration noise?\n2. How does the method perform when calibration parameters are inaccurate or unavailable, as often occurs in practice?\n3. Could comparisons be extended to recent Vision-Language-Action systems or hybrid-view approaches to strengthen the motivation?\n4. Is the calibration network truly necessary, or could similar improvements be achieved with modern feature alignment or camera-pose estimation modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K3ZgREtrKv", "forum": "a5rZNSuRbp", "replyto": "a5rZNSuRbp", "signatures": ["ICLR.cc/2026/Conference/Submission14608/Reviewer_5TS4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14608/Reviewer_5TS4"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855815046, "cdate": 1761855815046, "tmdate": 1762924989683, "mdate": 1762924989683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework for viewpoint-invariant robot manipulation by introducing a calibration diffusion policy. The authors also propose a free-View robot manipulation dataset constructed in the Isaac Sim environment, covering eight manipulation tasks. The method integrates a calibration network into the diffusion policy to adapt to variable camera viewpoints. Extensive experiments and ablation studies are conducted to evaluate the approach against several baselines, including BC-T, ACT, Diffusion Policy, Flow Policy, and DP3."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper identifies a relevant and underexplored issue, that is the sensitivity of visuomotor policies to camera viewpoints, and formalizes the “free-view manipulation” as a benchmark task. This is novel to me.\n2. The Free-View dataset is clearly structured, includes calibration parameters for each episode, and provides an important testbed for evaluating viewpoint generalization.\n3. The experiments cover comparisons with multiple strong baselines, data size ablations, structure ablations, and calibration noise analyses. The evaluation protocol is clearly explained. and overall it is generally well organized and includes clear visualizations."}, "weaknesses": {"value": "1. The entire evaluation is done in simulation of Isaac Sim. There lack of physical robot validation limits the practical credibility of the results.\n2. The description of the calibration network could be improved to make it more clear. The method section is difficult to follow, with unclear notations and redundant equations. Key architectural details are missing\n3. The method requires accurate calibration parameters between the robot and each camera. As the authors themselves admit in Section A.3, this dependency makes the approach impractical for real-world scenarios, where calibration is noisy or unavailable."}, "questions": {"value": "1. How is the camera calibration represented in real-world tasks, and could the model generalize to approximate or partially incorrect calibrations?\n2. How scalable is the proposed two-stage training strategy when the number of viewpoints or tasks increases significantly?\n3. Could the calibration features be learned implicitly, rather than explicitly provided?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2WSrti0ocZ", "forum": "a5rZNSuRbp", "replyto": "a5rZNSuRbp", "signatures": ["ICLR.cc/2026/Conference/Submission14608/Reviewer_K2pn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14608/Reviewer_K2pn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14608/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991885123, "cdate": 1761991885123, "tmdate": 1762924989159, "mdate": 1762924989159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}