{"id": "u1OWn3ayY1", "number": 14802, "cdate": 1758244058176, "mdate": 1759897348449, "content": {"title": "CheXGenBench: A Unified Benchmark for Fidelity, Privacy and Utility of Synthetic Chest Radiographs", "abstract": "Structured benchmarks have advanced text-conditional image generation for real-world imagery, however, no such benchmark exists for synthetic radiograph generation. Despite being a highly active area of research, existing studies continue adopting inconsistent evaluation protocols and lack a unified assessment of the three most critical criteria: generative fidelity, privacy risk, and downstream utility. \nTo address these limitations, we introduce CheXGenBench, the first unified evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across frontier text-to-image (T2I) generative models. Our evaluation protocol, comprising over 20 quantitative metrics, covers 11 leading T2I architectures with plug-and-play integration for newer models. Through a rigorous and fair evaluation protocol, we establish a new SoTA in synthetic chest X-ray generation. Furthermore, our results uncover several critical limitations in the applicability of current generative models, which include (1) even SoTA models struggle with long-tailed medical distributions, (2) models pose high privacy risks regardless of fidelity quality, and (3) synthetic data offers limited utility for downstream multimodal tasks. Drawing from these results, we propose concrete research directions to advance the field.\nFinally, we curate and release SynthCheX-75K, a high-quality synthetic dataset comprising 75K radiographs generated by our top-performing model (Sana 0.6B).\nThe fine-tuned models and the SynthCheX-75K dataset would be released after acceptance, while the anonymised code is available at https://anonymous.4open.science/r/CheXGenBench-52F0/README.md", "tldr": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models", "keywords": ["Biomedical Imaging", "Text-to-Image Generation", "Medical Image Analysis", "Chest Radiographs", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/74ad9a4953bc9d7b0679d9dbaf91dbc7ba8e82e4.pdf", "supplementary_material": "/attachment/23aceb36410d05cd2f0ad3eccb3e094758528c0e.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a comprehensive evaluation framework for T2I model performance in medical imaging. It addresses a clear gap in the field: the absence of standardized, reproducible benchmarks for synthetic radiograph generation. CheXGenBench evaluates models along three complementary dimensions-fidelity, privacy, and utility-using over twenty quantitative metrics applied to eleven leading generative architectures, ranging from Stable Diffusion variants to modern transformer-based models.\n\nThe benchmark standardizes training and evaluation protocols, enabling equitable model comparison. It integrates domain-adapted encoders such as RadDino and BioViL-T for in-domain evaluation, improving over prior works that relied on natural-image metrics like Inception-based FID. Moreover, it introduces pathology-conditional evaluation, offering fine-grained insights into model performance on long-tailed medical distributions. The framework also incorporates privacy risk quantification through pixel, latent, and re-identification distance metrics, reflecting growing regulatory and ethical concerns in synthetic medical imaging.\n\nBeyond benchmarking, the authors release SynthCheX-75K, a synthetic dataset of 75000 chest radiographs generated by the top-performing model (Sana 0.6B), intended for downstream diagnostic research and data augmentation. The results show that while synthetic data can match real data performance in unimodal classification tasks, multimodal utility, such as report generation, remains limited.\n\nThe paper is well-structured, clearly written, and timely, offering a valuable contribution toward standardization in synthetic medical imaging. A minor limitation is that the benchmark, while broad, remains confined to chest X-rays and might not generalize to other modalities. Additionally, some metrics overlap conceptually, and further justification for their selection would strengthen clarity. Nonetheless, the unified design, release of reproducible code, and transparent reporting mark an important step forward for evaluating generative models in healthcare."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Original contribution introducing the first unified benchmark combining fidelity, privacy, and clinical utility for synthetic radiograph generation.\n- Addresses a clear and relevant gap in medical AI benchmarking with a well-motivated framework.\n- High methodological quality with standardized training and evaluation protocols ensuring fair model comparison.\n- Integration of in-domain models (RadDino, BioViL-T) improves validity of generative fidelity assessment.\n- Novel pathology-conditional evaluation provides granular insights into model behavior on long-tailed medical distributions.\n- Inclusion of privacy and re-identification risk metrics is timely and important for ethical AI in healthcare.\n- Clear presentation, consistent structure, and well-organized figures and tables aid readability.\n- Release of SynthCheX-75K dataset adds practical value and supports community reproducibility.\n- Significance lies in establishing a reproducible, extensible benchmark that can evolve with future generative model advances.\n- Balanced discussion of current model limitations and future research directions demonstrates maturity and impact."}, "weaknesses": {"value": "- Benchmark scope is limited to chest X-rays, reducing generalizability to other medical imaging modalities such as CT, MRI, or ultrasound.\n- The choice and weighting of some metrics are not fully justified, and the overlap between fidelity measures (FID, KID, PRDC) may obscure interpretability.\n- Privacy evaluation lacks ground truth for real-world re-identification risk, relying solely on similarity metrics without human or institutional validation.\n- Downstream utility experiments cover only classification and report generation, leaving out segmentation and localization tasks that are clinically relevant.\n- While standardized training is valuable, fixed hyperparameters across diverse architectures may disadvantage some models and affect fairness.\n- Synthetic data quality is discussed mainly quantitatively; more qualitative visual examples or expert reader assessments could strengthen conclusions.\n- Limited discussion on how the benchmark will be maintained or expanded, which may affect long-term community adoption."}, "questions": {"value": "1. How easily can CheXGenBench be extended to other modalities such as CT or MRI, and are there plans to do so?\n 2. How were the relative weights or importance of the 20+ metrics decided, and would metric correlation analysis help simplify interpretation?\n 3. For privacy evaluation, have any real re-identification or membership inference experiments been performed to validate metric reliability?\n 4. Could the benchmark include additional downstream tasks like segmentation or detection to better reflect clinical utility?\n 5. Were hyperparameters adjusted per model, or were all architectures evaluated under identical training settings?\n 6. What quality control or expert review was applied to SynthCheX-75K to ensure diagnostic plausibility?\n 7. Are there plans for regular updates or a public leaderboard to keep the benchmark relevant as new models emerge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 8}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "xXkc0Pt8oe", "forum": "u1OWn3ayY1", "replyto": "u1OWn3ayY1", "signatures": ["ICLR.cc/2026/Conference/Submission14802/Reviewer_nMkj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14802/Reviewer_nMkj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761408858815, "cdate": 1761408858815, "tmdate": 1762925153160, "mdate": 1762925153160, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "I reviewed a paper that presents CheXGenBench, a comprehensive benchmark designed to evaluate synthetic chest X-ray generation models across three critical dimensions: fidelity, privacy, and utility. The authors aim to establish a standardized evaluation framework for medical text-to-image (T2I) models, addressing the lack of unified assessment protocols in this rapidly growing area.\n\nThe benchmark covers 11 state-of-the-art generative models and evaluates them on a large-scale clinical dataset derived from MIMIC-CXR. \n\nIn addition, the authors introduce SynthCheX-75K, a dataset of 75,000 synthetic radiographs generated with their best-performing model (Sana 0.6B), along with detailed analysis of strengths and weaknesses across different pathologies. The study reveals that current medical T2I models produce high-fidelity but clinically shallow imagesthey often fail on rare conditions, exhibit potential privacy leakage, and offer limited downstream benefits compared to real data."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The authors identify a genuine gap in how we evaluate synthetic medical image generation and address it through a comprehensive, unified benchmark that integrates three critical evaluation criteria. This multidimensional perspective is highly valuable, as most prior studies have focused solely on visual fidelity or clinical realism, while neglecting privacy and downstream impact.\n\nThe benchmark is methodologically rigorous and transparently designed. I appreciate evaluation  across a  range of metrics (FID, KID, PRDC, classification AUROC, and privacy measures). The scope and consistency of the experimental setup make the results credible and reproducible. The authors also provide a new synthetic dataset (SynthCheX-75K) and code, reinforcing the benchmark’s long-term utility for the community."}, "weaknesses": {"value": "While the paper is very strong in scope and execution, its conceptual novelty is limited, as it primarily focuses on benchmarking and integration rather than proposing new modeling techniques or theoretical insights. The work’s contribution is therefore more infrastructural than algorithmic, which may place it slightly below the novelty threshold for top-tier venues like ICLR.\n\nA second limitation is that the benchmark is domain-specific, focusing exclusively on chest X-rays (MIMIC-CXR). This narrow scope raises questions about generalization. for instance, whether the same evaluation framework and findings would hold for other imaging modalities (CT, MRI, ultrasound) or non-medical domains. Extending the framework to more diverse data would substantially increase its impact.\n\nAlthough the privacy evaluation is interesting and well-motivated, the privacy attack scenarios remain somewhat shallow, focusing on direct pixel and latent re-identification. Stronger tests such as membership inference, model inversion, or attribute inference attacks could offer a more realistic picture of generative privacy risks."}, "questions": {"value": "While the paper is very strong in scope and execution, its conceptual novelty is limited, as it primarily focuses on benchmarking and integration rather than proposing new modeling techniques or theoretical insights. The work’s contribution is therefore more infrastructural than algorithmic, which may place it slightly below the novelty threshold for top-tier venues like ICLR.\n\nA second limitation is that the benchmark is domain-specific, focusing exclusively on chest X-rays (MIMIC-CXR). This narrow scope raises questions about generalization. for instance, whether the same evaluation framework and findings would hold for other imaging modalities (CT, MRI, ultrasound) or non-medical domains. Extending the framework to more diverse data would substantially increase its impact.\n\nAlthough the privacy evaluation is interesting and well-motivated, the privacy attack scenarios remain somewhat shallow, focusing on direct pixel and latent re-identification. Stronger tests such as membership inference, model inversion, or attribute inference attacks could offer a more realistic picture of generative privacy risks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bDnhiPAJW9", "forum": "u1OWn3ayY1", "replyto": "u1OWn3ayY1", "signatures": ["ICLR.cc/2026/Conference/Submission14802/Reviewer_NNTd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14802/Reviewer_NNTd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978732114, "cdate": 1761978732114, "tmdate": 1762925152727, "mdate": 1762925152727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes CheXGenBench, the first unified benchmark for synthetic chest radiograph generation that evaluates three critical dimensions: generation fidelity, privacy risk, and clinical utility through 20+ metrics. Unlike prior work that fragmented these evaluations, this benchmark provides standardized protocols to train and evaluate 11 Text-to-Image models, identifying Sana (0.6B) as the top performer. Key findings reveal that (1) even SOTA models struggle with long-tailed medical data distributions, (2) high privacy risks exist regardless of fidelity quality, and (3) synthetic data is useful for unimodal tasks (classification) but limited for multimodal tasks (report generation). The authors also release SynthCheX-75K, a synthetic dataset of 75K samples."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**1. Comprehensive and Novel Benchmark Framework**\n- First unified benchmark evaluating fidelity, privacy, and utility simultaneously for medical image generation\n- Pathology-specific conditional analysis is a crucial contribution. Previous work only reported global averages, missing clinically important performance variations across different conditions\n- Comprehensive evaluation of recent models (Sana, Pixart Sigma, Flux) for medical radiograph generation\n\n**2. Rigorous Methodology and Important Findings**\n- Standardized training protocol (identical data, epochs, batch size) ensures fair comparison across 11 models\n- Uses domain-specific state-of-the-art encoders (RadDino, BioViL-T) instead of outdated DenseNet-121, improving evaluation reliability\n- Introduces PRDC metrics to measure mode coverage.\n- Multi-level privacy assessment (pixel, latent space, patient re-identification)\n- Quantitatively demonstrates severity of long-tail distribution problem (correlation coefficient 0.947 between pathology prevalence and model performance). This is an important finding that reveals current models primarily memorize data distribution rather than learning to generate rare pathologies"}, "weaknesses": {"value": "**1. Unsubstantiated Claims About SynthCheX-75K and Lack of Augmentation Experiments**\n- Conclusion claims SynthCheX-75K can be used \"to augment existing datasets, particularly for rare conditions,\" but provides no experimental evidence. Table 4 only shows standalone synthetic-only training results, not real+synthetic augmentation. Given that Sana (which generated SynthCheX-75K) shows poor performance on rare pathologies like Pleural Other (FID 199.45 vs 44.60 for No Finding), how can this dataset augment rare conditions effectively? The paper's own finding of 0.947 correlation suggests all models struggle with rare pathologies, making this augmentation claim highly questionable without supporting experiments.\n- Similarly claims SynthCheX-75K serves as \"high-quality standalone training resource,\" but Table 5 shows all synthetic data underperforms real data for report generation. This contradicts the \"standalone\" characterization. It only works for classification, not for more complex multimodal tasks.\n\n**2. Multimodal Task Failure Without Deep Analysis**\n- Table 5 shows ALL synthetic data significantly underperforms real data for report generation (e.g., Original BLEU-1: 38.16 vs best Sana: 31.11, ~18% degradation). This is critical given \"downstream utility\" is a core evaluation dimension. However, the paper only provides superficial speculation (\"potentially... can alleviate\") without investigating root causes. Is it insufficient image-text alignment? Sana and Pixart Sigma have highest alignment scores but don't improve RRG performance. Is there a fundamental limitation in synthetic image semantics that prevents effective multimodal learning? This deserves deeper analysis.\n- No experiments on real+synthetic mixture, which would be the actual practical use case for augmentation and could reveal whether synthetic data has any value when combined with real data.\n\n**3. Incomplete Technical Details**\n- Critical reproducibility details missing: LoRA alpha values not specified (Table 11 only shows learning rates). Section 3.1 mentions LoRA on attention layers might be insufficient for large models, but no experiments applying LoRA to additional layers (e.g., MLP) are conducted.\n- Downstream task evaluation limited to classification and RRG. Other important tasks like object detection and few-shot learning not evaluated."}, "questions": {"value": "**1. Evidence for Rare Condition Augmentation Claim**\n- What evidence supports the claim that SynthCheX-75K can augment rare conditions? Table 2 shows Sana performs worst on rare pathologies (e.g., Pleural Other FID: 199.45). Did you conduct experiments showing that adding SynthCheX-75K to real data improves performance on rare pathologies? If not, on what basis is this claim made?\n- Can you provide distribution statistics of SynthCheX-75K? Does it faithfully reproduce the long-tail distribution of MIMIC-CXR, or does it have better coverage of rare pathologies?\n\n**2. Root Cause of Multimodal Failure**\n- Why does ALL synthetic data underperform real data for RRG despite some models showing high image-text alignment? Can you provide deeper analysis beyond the current speculation?\n- Did you experiment with real+synthetic mixtures for training? At what mixing ratios might synthetic data provide value?\n- Could the semantic information in synthetic images be fundamentally different from real images in ways that affect caption generation but not classification?\n\n**3. Technical Details**\n- What are the LoRA alpha values? Why was LoRA only applied to attention layers, did you test applying it to MLP layers for large models?\n- For SynthCheX-75K filtering (Appendix G): What percentage was classified as \"Low Quality\" or \"Not Relevant\"? How does this vary by pathology?\n- Are the classification improvements (Table 4) statistically significant and generalizable to other datasets (CheXpert, PadChest)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NmETTB1MhW", "forum": "u1OWn3ayY1", "replyto": "u1OWn3ayY1", "signatures": ["ICLR.cc/2026/Conference/Submission14802/Reviewer_UyBi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14802/Reviewer_UyBi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988381667, "cdate": 1761988381667, "tmdate": 1762925152311, "mdate": 1762925152311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a unified evaluation framework for synthetic chest radiograph generation that incorporates three main criteria: generative fidelity, privacy risk, and downstream utility. It includes around 20 quantitative metrics for these tasks, validated on 11 different text-to-image models.\nThe main contributions are:\n- It provides a unified platform to evaluate text-to-image generative models across three major criteria.\n- It presents a thorough study and extensive experiments demonstrating the applicability of different metrics for domain-specific outcomes, especially considering the long-tailed medical data distribution.\n- It commits to releasing fine-tuned models and a synthetic dataset that could be valuable for the research community."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Provides a detailed and well-reasoned study highlighting the clinical applicability of the three evaluation criteria.\n-  Evaluates 11 models, including both domain-specific and general-purpose text-to-image models.\n- The paper is clearly written and well-structured.\n- Addresses critical domain-specific challenges, such as the long-tailed distribution in medical data, and compares commonly used evaluation backbones like DenseNet (for FID) with RadDINO.\n- Commits to releasing a large-scale (20k) synthetic dataset that could significantly benefit the research community."}, "weaknesses": {"value": "- Some parts require further explanation:\na) In Section 2.2, it is mentioned that for FID they focus on radiology-specific and biomedical domain-oriented assessment. The same section also lists other criteria such as precision, recall, density, and coverage. However, it is unclear how these metrics are computed. Are domain-pretrained classifiers or existing approaches used to obtain these measures for generated medical samples?\nb) The paper emphasizes the importance of metrics that address the long-tailed distribution in medical data, but it does not explain how RadDino effectively handles such cases. If rare conditions are underrepresented in its training, would not RadDino also face similar limitations?\nc) In Table 1, the “Alignment score” is reported but not described. Its definition, computation method, and relevance to the evaluation framework should be clarified.\n- The distribution of samples across different categories in the 20k synthetic dataset should be discussed to better understand diversity and balance.\n- In the downstream task of radiology report generation, most metrics used are general NLG-based metrics. In the medical domain, these are often insufficient for evaluating the accuracy of clinical terminology and findings. If the framework aims to serve as a unified standard, domain-specific metrics should be included. Beyond RadGraph-F1, metrics like GREEN score and RaTEScore could provide a more clinically meaningful evaluation."}, "questions": {"value": "- In Section 2.2, it should be explained how precision, recall, density, and coverage are computed for the evaluation. It is unclear whether these metrics rely on a domain-pretrained classifier or any other existing approach tailored for medical image assessment.\n- The handling of long-tailed medical data distributions by RadDino should be clarified. It is important to explain whether RadDino is trained or fine-tuned on rare disease cases and how it effectively addresses imbalance, given that such cases are inherently underrepresented.\n- The definition and computation method of the “Alignment score” in Table 1 should be described to clarify its role and interpretation within the evaluation framework.\n- The distribution of samples across different categories in the 20k synthetic dataset should be presented to illustrate dataset diversity and ensure transparency regarding potential biases.\n- In the downstream task of radiology report generation, the choice of primarily NLG-based metrics should be justified. Since such metrics are often insufficient for clinical evaluation, the inclusion of domain-specific measures such as GREEN score and RaTEScore, in addition to RadGraph-F1, should be considered to strengthen the medical relevance of the evaluation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lSK7ZUK0z1", "forum": "u1OWn3ayY1", "replyto": "u1OWn3ayY1", "signatures": ["ICLR.cc/2026/Conference/Submission14802/Reviewer_QCXy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14802/Reviewer_QCXy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14802/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997336197, "cdate": 1761997336197, "tmdate": 1762925151834, "mdate": 1762925151834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}