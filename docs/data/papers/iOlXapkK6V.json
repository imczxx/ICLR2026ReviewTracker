{"id": "iOlXapkK6V", "number": 13243, "cdate": 1758215573570, "mdate": 1759897453233, "content": {"title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Implicit Chain-of-Thought Reveals Challenges of Learning Long-Range Dependencies", "abstract": "Language models are increasingly capable, yet still fail at a seemingly simple task\nof multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via \\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.", "tldr": "We study why transformers can't learn multi-digit multiplication by reverse-engineering a model that successfully learns multiplication via process supervision called implicit chain-of-thought.", "keywords": ["mechanistic interpretability", "arithmetic reasoning", "feature geometry"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a29922f1e2322835250c5645d2c4f726cbb33ca9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates why Transformers fail at multi-digit multiplication. The authors reverse-engineer an implicit chain-of-thought (ICoT) model that successfully learns 4×4 multiplication and compare it to a standard fine-tuned model that fails. Through mechanistic analysis, they discover that the ICoT model encodes necessary long-range dependencies by constructing a binary-tree-like attention pattern that caches pairwise partial products across timesteps for later retrieval. The model also uses sophisticated geometric representations, encoding digits with Fourier bases that form a pentagonal prism structure.\n\nIn contrast, standard fine-tuning gets stuck in a local optimum where only the first, last, and some early digits are learned correctly - the model fails to learn the middle digits that require tracking complex dependencies. To validate their understanding, the authors introduce an auxiliary loss that supervises the model to predict intermediate \"running sums,\" providing the inductive bias needed to achieve 99% accuracy without chain-of-thought supervision. This work reveals a fundamental challenge in how Transformers learn long-range dependencies under standard training and demonstrates that while task-specific fixes exist, more general solutions are needed."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is an excellent paper. \n\nThe mechanistic investigation of the ICOT model builds up over several layers, from theoretical analysis of long-range dependencies, demonstrated through logit attributions, linear regression probing, use of some attention heads to handle long-range dependencies, the outputs of some attention heads forming a Minkowski sum, a 3D two-level PCA analysis, and finally a Fourier analysis. \n\nThe pentagram prism structure, revealed in the Fourier analysis, is a novel insight into how the models encodes information about our base 10 counting system. This “tool-like” structure appears to embed: A) 10 is divisible by 5 and 2 B) concept of odd and even numbers C) potentially a way to calculate n+5 and n-5 (between layers) and D) n+4 and n-4 (around rings).\n\nThe Figures are excellent - particularly 1 & 4 as they represent hard-to-diagram concepts. Figure 6 is beautifully presented - the pentagon prism gives a really great insight into how this model represents base 10 digits in model space.\n\nThe ICOT model insights lead directly to a modified loss function allowing SFT of a model to perform multiplication accurately. This strengthens the case for the ICOT insights being valid."}, "weaknesses": {"value": "None to speak of. I think the Conclusion undersells what the paper achieves. \n\nMinor points: \n- Minkowski sums are first used without explanation. Consider adding “(adding sets of vectors geometrically)”.\n- Section 3.3, the notation “ATT superscript 2 subscript 3” is not explained before use, and differs from the “Layer 2 Head 3” notation used in Figure 4. Later “ATT superscript 1 (i, j)” is used with little introduction. Consider standardizing in some way to make reading easier. \n- Figure 5, consider swapping position of 2nd and 3rd image for easier left to right reading- making the zoomed-out and zoomed-in images sit side by side."}, "questions": {"value": "Q1: The pentagon prism structure appears to embed many facts about our base 10 counting system. Do you expect this tool-like structure to have been independently learnt by other models trained on base 10 math?\n\nQ2: The pentagon prism has a 5 ring and 2 layers. The primes of 10 are 5 and 2. If the model had been trained in base 15, what shaped structure would you expect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MGnA9IBmz2", "forum": "iOlXapkK6V", "replyto": "iOlXapkK6V", "signatures": ["ICLR.cc/2026/Conference/Submission13243/Reviewer_onsG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13243/Reviewer_onsG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761166052420, "cdate": 1761166052420, "tmdate": 1762923926295, "mdate": 1762923926295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how Transformers acquire long-range dependencies for multiplication via an implicit chain of thought (ICoT). Linear probing analyses indicate that ICoT encourages Transformer attention to cache pairwise products, providing evidence of long-range dependency capture. A 3D mapping of attention head outputs reveals nested representations of number tokens. The authors also propose a loss function that enables Transformers to learn multiplication efficiently without ICoT."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper thoroughly examines how a Transformer with ICoT processes input digits to perform multiplication, supported by extensive experiments.\n- Linear probing shows that the attention mechanism indeed stores intermediate computations.\n- The geometric analysis reveals an interesting pentagram-like structure."}, "weaknesses": {"value": "The paper has unclear points in several critical areas (premise, definitions, and mathematical exposition), which made it difficult to fully appreciate the contributions.\n\n---\n**Premise and baseline model.**\n\nFirst, while the authors claim that SFT models cannot learn multiplication, prior work reports that simple GPT models readily learn multiplication when using zero-padding (to fix the number of digits) and digit reversal. Indeed, [1] shows that GPT-2 small successfully learns multiplication up to 15×15 digits. The present paper appears to use the same technique, which was not used in [Yang et al., 2023]—the work cited as evidence that multiplication is hard to learn. Consequently, the premise of this paper seems to rest on an unfair contrast: the claimed hardness relies on [Yang et al., 2023], whereas the technique employed follows [1], which already addresses the hardness.\n\n[1] Shen et al., \"Positional Description Matters for Transformers Arithmetic,\" 2023.\n\nSecond, it is unclear what “SFT models” refers to. No precise definition is given. Line 71 only states “a model trained with standard fine-tuning,” but if it is fine-tuned, on what pretraining data? What is the architecture—encoder–decoder or decoder-only? In either case, why does the baseline fail to learn multiplication unlike in [1]? Reversing target digits already encourages a certain chain of thought. The baseline should be able to reach near-perfect accuracy on the target task (i.e., 4×4-digit multiplication).\n\n---\n\n**Presentation**\n\nThe presentation obscures several core results.\n\nThe main text states that Figure 3 shows mean absolute error, but the scatter plots do not appear to reflect that. What are the diagonal points? In the upper row, why are blue points clustered at the bottom right?\n\nSection 4 is also difficult to follow due to many undefined or cluttered symbols and operators.\n- [Eq. 4] $A, B, \\oplus$ are undefined. If $A$ is a matrix, the Minkowski sum between a matrix and a scalar $\\epsilon$ is not defined. Assuming broadcasting, the right-hand side is a matrix while the left-hand side is a set, so the statement “matrix includes a set” is not meaningful.\n- [Eq. 5] $\\mathrm{Cov}(A_i)$ is undefined. Is this the covariance over entries in $A_i$, or across the index $i=1,2,\\dots$? It is also unclear why $\\Sigma_A$ and $\\Sigma_B$ should share eigenvectors. The “local” covariance $\\Sigma_{\\mathrm{local}\\mid a_i}$ appears to be conditioned on $a_i$, but its definition is not.\n- [Fourier expansion] The index $k$ and the operator $*$ are undefined. $\\mathbb{1}(n)$ and $\\mathbb{p}(n)$ are not defined; the notation suggests vectors, but other terms are not vectors, which is inconsistent. What does $\\mathbb{1}(n) \\equiv 1$ mean?\n\n---\n**Insights**\n\nWhile the paper aims to explain why multiplication is hard for SFT but feasible with ICoT, the results do not fully support this claim. Linear probing shows that how the successful model (i.e., the ICoT model) processes the digits while the SFT model does not. This finding is interesting but describes what happens in successful versus unsuccessful models rather than explaining what causes success or failure. Section 5 states:\n\n> Despite only the middle digits receiving gradients (as they are the only sources of loss remaining), their losses plateau, suggesting that the model is stuck in a local optimum that lacks the long-range dependencies to properly learn the middle digits.\n\nHowever, attributing failure to being stuck in a local optimum—without theoretical justification—does not provide a satisfactory explanation.\n\n---\n\nOverall, the work appears to rest on an incorrect (or unfair) premise that SFT models cannot learn even 4×4-digit multiplication, thereby overestimating the task’s hardness and overstating the benefits of ICoT. In addition, the presentation requires substantial improvements. Finally, the claim to explain “why” should be made with greater care to avoid overstatement."}, "questions": {"value": "Please address the weaknesses raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ikYYpWgRVR", "forum": "iOlXapkK6V", "replyto": "iOlXapkK6V", "signatures": ["ICLR.cc/2026/Conference/Submission13243/Reviewer_cmTc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13243/Reviewer_cmTc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920506201, "cdate": 1761920506201, "tmdate": 1762923925889, "mdate": 1762923925889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies why certain models (standard fine-tuned) fail at successfully performing multi-digit multiplication. The authors gain insight into this by reverse engineering a model that *can* successfully perform multi-digit multiplication via implicit chain of thought (ICoT). They uncover that the successful ICoT model encodes long-range dependencies, while the standard fine-tuned (SFT) models seemingly do not. Given these insights, they develop an auxiliary loss that provides a helpful inductive bias for the SFT model to learn long range dependencies to perform multi-digit multiplication."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Develop an understanding that leads to an actionable intervention: the auxiliary loss that leads to the SFT model getting up to 99% after previously being unable to perform the multiplication task.\n2. The paper is well-written and the visualizations are well-made.\n3. The authors include code with their paper."}, "weaknesses": {"value": "W1. The present study is only conducted on 4-digit multiplication. Additionally, it's unclear how the auxiliary loss would generalize to multiplication with more digits e.g. 5x5, 6x6, etc.\n\nW2. It's unclear how the insights would generalize to other tasks."}, "questions": {"value": "Q1. Related to weakness 1: Do you have general results on multiplying numbers with more than 4 digits?\n\nQ2: Do you have any idea why the case with the auxiliary loss goes up to 99% but not to 100% like the ICoT model? How does the auxiliary loss model perform as the number of digits increases? Does it stay at 99% consistently as the length increases or are there any changes?\n\nQ3: How sensitive/robust is the performance with the auxiliary loss as you vary hyperparameters and random seeds?\n\nQ4: I'm not sure if this is known in the literature already, or if you have thought about this, but do you have any insight on why the SFT model fails at the task in the first place? My understanding is that you diagnose the failure being related to the difficulty of the middle digits, but is there any understanding as to why the transformer with an autoregressive loss isn't able to do this?\n\nNote: I find it intriguing that you use methods that on their own are not always robust for mechanistic understanding of exact model behaviour (e.g. PCA, linear probes, logit attribution, attention patterns), etc. but it still gives you insight to something important the model is doing because you're able to take a model that was previously unable to solve the task and significantly boost its performance on the task. Perhaps I'm not familiar enough with the mech interp literature in this area, but I personally find this interesting and it raises some philosophical questions for me on what is ``important'' for mechanistic understanding."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nseYauTJFR", "forum": "iOlXapkK6V", "replyto": "iOlXapkK6V", "signatures": ["ICLR.cc/2026/Conference/Submission13243/Reviewer_tMVK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13243/Reviewer_tMVK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144519966, "cdate": 1762144519966, "tmdate": 1762923925104, "mdate": 1762923925104, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies Transformers’ ability to learn tasks with long-range dependencies, focusing on 4x4 multiplication.\nIt compares a standard fine-tuned model (SFT) with a model trained using implicit chain-of-thought (ICoT).\nWith a 2-layer, 4-head architecture, ICoT reaches 100% accuracy, whereas SFT remains below 1%.\nThe core of the paper is a comparison of the internal mechanisms learned by SFT vs. ICoT. \nUsing logit attribution and linear probing, the authors argue that ICoT captures the required long-range dependencies while SFT does not.\nTo explain how ICoT computes these dependencies, the paper introduces an attention tree, revealing a sparse, binary-tree-like routing pattern that supplies the necessary tokens to compute $c_2$.\nThe paper then studies the geometry of ICoT’s hidden representations via PCA. \nIt finds that intermediate representations cluster by $a_i$ and $b_j$, and that the final hidden states exhibits a pentagonal-prism structure.\nAdditionally, the authors observe that SFT appears to get stuck in a local minimum, based on gradient norms and per-$c_k$ losses over training.\nFinally, they introduce an auxiliary loss to predict $\\hat{c}_k$, which enables SFT without CoT to learn the task successfully."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**S1.**\nThe paper is well written, with sufficient methodological detail.\n\n**S2.**\nClaims are validated through multiple analyses (logit attribution, linear probes, attention tree visualization, PCA, gradient norms and losses), which together provide strong support."}, "weaknesses": {"value": "**W1.**\nThe paper does not fully address the fundamental reason why Transformers fail to learn multiplication when trained with SFT. \nThe results convincingly indicate that SFT fails to capture long-range dependencies and that ICoT succeeds, but they do not explain why SFT fails to develop those dependencies in training (e.g., optimization landscape, inductive bias).\nFor example, in line 375, the paper states that “the model is stuck in a local optimum”, and does not clarify the underlying cause.\nAt the current stage, the work feels closer to “an analysis of differences between SFT and ICoT”, than to a full answer to the title “why can’t transformers learn multiplication”.\n\nAside from this limitation, the work is interesting and solid."}, "questions": {"value": "**Q1.**\nI’m slightly unsure how Figure 5 was produced. \nMy understanding is: (1) run many problems; (2) collect the output vectors of a specific first-layer attention head; (3) run PCA; (4) color points by the digit at $a_i$ or $b_j$.\nWhich timestep is used for extracting output vectors?\nAlso, what does the purple box indicate in Figure 5(b)?\n\n**Q2.**\nHow does Section 4 relate to the main message that ICoT learns multiplication while SFT fails? \n\n**Q3.**\nThe pentagonal prism in Figure 6 is interesting.\nDoes this structure consistently appear across different ICoT runs (e.g., different seeds/initializations)?\nAlso, if the task were posed in a different base (e.g., 11 (prime) or 30 (many divisors)), what geometry would PCA reveal? (the second question is just out of curiosity, so you don’t have to run extra experiments for this question.)\n\n**Q4.**\nIn Section 6, the SFT model with the auxiliary loss solves 4x4 problems successfully.\nCould you provide Figure 2/5/6 style visualizations for this model? \nThis would help assess whether its internal mechanism aligns with ICoT or differs in important ways."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Kl0oYMoDoh", "forum": "iOlXapkK6V", "replyto": "iOlXapkK6V", "signatures": ["ICLR.cc/2026/Conference/Submission13243/Reviewer_CsfC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13243/Reviewer_CsfC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13243/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762167835434, "cdate": 1762167835434, "tmdate": 1762923924242, "mdate": 1762923924242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}