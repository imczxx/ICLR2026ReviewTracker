{"id": "5ThIWuDkEf", "number": 5357, "cdate": 1757903855638, "mdate": 1763685497527, "content": {"title": "Anatomy-aware Representation Learning for Medical Ultrasound", "abstract": "Diagnostic accuracy of ultrasound imaging is limited by qualitative variability and its reliance on the expertise of medical professionals. Such challenges increase demand for computer-aided diagnostic systems that enhance diagnostic accuracy and efficiency. However, the unique texture and structural attributes of ultrasound images, and the scarcity of large-scale ultrasound datasets hinder the effective application of conventional machine learning methodologies. To address the challenges, we propose Anatomy-aware Representation Learning (ARL), a novel self-supervised representation learning framework specifically designed for medical ultrasound imaging. ARL incorporates an anatomy-adaptive Vision Transformer (A-ViT). The A-ViT is parameterized, using the proposed large-scale medical ultrasound dataset, to provide anatomy-aware feature representations. Through extensive experiments across various ultrasound-based diagnostic tasks, including breast and thyroid cancer, cardiac view classification, and gallbladder tumor and COVID-19 identification, we demonstrate that ARL significantly outperforms existing self-supervised learning baselines. The experiments demonstrate the potential of ARL in advancing medical ultrasound diagnostics by providing anatomy-specific feature representation", "tldr": "An anatomy-aware representation learning in medical ultrasound, introducing a large scale medical ultrasound dataset", "keywords": ["Foundation model", "medical ultrasound", "representation learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3052db98077cd2b176bd3c54a499e2af51fe13a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Anatomy-Aware Representation Learning (ARL), a self-supervised framework for ultrasound (US) imaging that integrates anatomical context into representation learning. The authors propose Anatomy-aware Vision Transformer (A-ViT), which incorporates an Anatomy-Conditioned Deformable Transformer (ACDT) to extract features according to the organ being analyzed. The authors evaluate ARL across multiple downstream tasks, including breast, thyroid, and gallbladder cancer classification, cardiac view classification, cardiac segmentation, and COVID-19 diagnosis. Experiments show that ARL consistently outperforms state-of-the-art (SoTA) self-supervised methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces a foundation model for ultrasound imaging, trained on 5.2M images, covering 16 anatomical categories.\n\nThe model is tested on six downstream tasks across both classification and segmentation. \n\nQuantitative results demonstrate consistent gains over state-of-the-art baselines."}, "weaknesses": {"value": "The authors emphasizes speckle noise and low color variation as major challenges unique to ultrasound imaging. However, the proposed A-ViT primarily introduces anatomy-conditioned deformable attention, which addresses anatomical variability rather than these low-level texture or color issues. The authors introduces an adversarial term to preserve high-frequency content. However, the paper lacks direct evidence that speckle-related distortions are mitigated or that low-color variation are effectively handled. \n\nAnatomy-aware conditioning was previously proposed in MRI/CT segmentation & registration. Deformable attention exists in general vision. MAE + distillation hybrid ideas has also been previously proposed. Therefore the paper doesn’t introduce a new model, but rather a new instantiation tailored to ultrasound images. However, there is limited explicit explanations on how each architectural choice is uniquely designed for ultrasound or directly tied to ultrasound physics. Most of the mechanisms introduced in A-ViT are general and could apply to other modalities. As a result, the authors should better explain the novelty by explicate how A-ViT differs from existing models for other modalities. \n\nIn Tables 2 and 3, the proposed model does not have the highest specificity in some cases and the highest specificity values are not bolded correctly"}, "questions": {"value": "1. Are there evidence that the proposed model addresses speckle noise and low color variation challenges in ultrasound imaging.\n\n2. How is the proposed model differed, comparing to existing models for other modalities? Is there any architectural choice that is uniquely designed for ultrasound or directly tied to ultrasound physics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EM3XMPy9uS", "forum": "5ThIWuDkEf", "replyto": "5ThIWuDkEf", "signatures": ["ICLR.cc/2026/Conference/Submission5357/Reviewer_cf7P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5357/Reviewer_cf7P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761865203943, "cdate": 1761865203943, "tmdate": 1762918021630, "mdate": 1762918021630, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General response"}, "comment": {"value": "We sincerely thank all the reviewers for their time and thoughtful feedback in reviewing our manuscript. In response to their valuable suggestions, we have carefully revised the manuscript and incorporated the following updates in the revised manuscript.\n\n- **(Related work)** We have expanded the discussion comparing the proposed A-ViT with prior anatomy-aware and ultrasound-specific representation learning methods.\n- **(Main text and Appendix A)** We have added detailed information about the proposed large-scale ultrasound dataset, as well as a description of the procedure used to annotate anatomy categories.\n- **(Appendix C)** We have included an ablation study that varies the size of the ultrasound pretraining dataset to assess the importance of large-scale data for effective representation learning.\n- **(Appendix D)** We have added an evaluation of existing self-supervised learning methods when pretrained on natural-image and ultrasound-image datasets.\n- **(Appendix E)** We have compared the proposed ACDT with alternative conditioning mechanisms, including LoRA, FiLM, and cross-attention.\n- **(Appendix F)** We have added an ablation study on the adaptive loss-weighting strategy.\n- **(Appendix G)** We have incorporated comparisons of masked-image reconstruction using a standard MAE and the proposed A-ViT.\n- **(Appendix H)** We have supplemented computational cost and inference speed analysis of the A-ViT.\n\nWe believe that these revisions have further enhanced the quality and completeness of the paper, and we hope that the updated version adequately addresses concerns raised by the reviewers."}}, "id": "f9rcw1NXvm", "forum": "5ThIWuDkEf", "replyto": "5ThIWuDkEf", "signatures": ["ICLR.cc/2026/Conference/Submission5357/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5357/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5357/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763609433184, "cdate": 1763609433184, "tmdate": 1763609433184, "mdate": 1763609433184, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Anatomy-aware Representation Learning (ARL), a self-supervised representation learning framework tailored specifically for medical ultrasound imaging. The authors identify key challenges in ultrasound diagnostics, including qualitative variability, reliance on expert knowledge, the unique texture/structural properties of the images, and the scarcity of large-scale datasets. The ARL framework is intended to address these issues. The work is evaluated by fine-tuning a Vision Transformer encoder and classification head on a diverse set of medical downstream tasks, including breast, thyroid, and gallbladder cancer classification, COVID-19 identification, and cardiac view classification, as well as a dense prediction task (echocardiography left ventricle blood-pool segmentation) using a UPerNet configuration."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The integration of anatomical conditioning into deformable attention for US is a novel and well-motivated architectural contribution. Unlike prior SSL methods that treat all images uniformly, ARL explicitly conditions feature extraction on anatomical context.\n2.The 5.2M-image dataset is a significant contribution, and the ablation studies (Table 3, Fig. 4) convincingly isolate the impact of each component (ACDT, adversarial loss, etc.).\n3.The paper is well-structured and clearly written, with intuitive figures (Fig. 1, 2, 3) and logical flow.\n4.Medical US is an underexplored modality in SSL, and ARL provides a strong foundation for future work. The dataset alone will likely become a community resource."}, "weaknesses": {"value": "1.The central weakness is the lack of a clear description of the Anatomy-aware Representation Learning (ARL) mechanism itself. While the goal is anatomy-aware learning, the specific self-supervised loss function or task design that enforces this awareness is not described, making it impossible to fully assess the work's technical depth or novelty.\n2.The submission provides an extensive experimental plan but no quantitative results (e.g., AUC, F1, Dice Score). Without empirical evidence, the claim of technical soundness and significance remains unverified. It is impossible to determine if the proposed ARL method actually advances the state-of-the-art or even works as intended.\n3.The paper claims to address the unique texture and structural attributes of ultrasound images. However, without detailing how ARL achieves this (i.e., the mechanism of anatomy-awareness), its true originality over existing self-supervised methods (like SimCLR, MAE, etc.) applied to medical imaging cannot be confirmed."}, "questions": {"value": "1.Please fully describe the proposed Anatomy-aware Representation Learning (ARL) framework. What are the specific self-supervised tasks or loss functions that compel the model to learn \"anatomy-aware\" features? How do these tasks specifically leverage or model the unique texture and structural attributes of ultrasound images better than standard methods?\n2.Please provide the full set of quantitative results (e.g., AUROC, F1-Score, Dice Similarity Coefficient) for all mentioned downstream tasks (classification and segmentation), and critically, compare your method against strong baselines such as ImageNet pre-trained models and non-anatomy-aware self-supervised learning methods applied to ultrasound data.\n3.Did the authors perform an ablation study to justify the anatomical-aware component? Showing results without the 'anatomy-aware' loss/mechanism would be critical evidence for the necessity and effectiveness of the proposed novelty.\n4.How are anatomical labels obtained for the 5.2M pretraining images? Are they derived from metadata, manual annotation, or automated prediction? If the latter, could label noise degrade representation quality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8wAzxEnmgV", "forum": "5ThIWuDkEf", "replyto": "5ThIWuDkEf", "signatures": ["ICLR.cc/2026/Conference/Submission5357/Reviewer_h8d3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5357/Reviewer_h8d3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761903686633, "cdate": 1761903686633, "tmdate": 1762918021308, "mdate": 1762918021308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an anatomy-aware representation learning framework (ARL) for medical ultrasound (US) imaging, based on a large-scale US dataset and a novel Anatomy-aware Vision Transformer (A-ViT). The method incorporates anatomical context via a deformable transformer and combines multiple self-supervised objectives to improve feature learning. Extensive experiments across multiple downstream tasks demonstrate improved performance over existing self-supervised learning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper introduces a large-scale, multi-source US dataset, which is a valuable contribution to the community.\n\n2.The proposed A-ViT model effectively integrates anatomical information and shows consistent improvements across diverse US tasks.\n\n3.The combination of MIM, adversarial loss, and self-distillation is well-motivated and empirically validated.\n\n4.Comprehensive evaluation on multiple organs and tasks (classification and segmentation) strengthens the claim of generalizability."}, "weaknesses": {"value": "1.The motivation for choosing certain design elements (e.g., deformable attention, specific loss weighting) could be better justified.\n\n2.The comparison to other anatomy-aware or medical-specific transformers is limited.\n[1]Anatomy-Aware Contrastive RepresentationLearning for Fetal Ultrasound\n[2]Anatomy-Aware Self-Supervised Learning for Aligned Multi-Modal Medical Data\n[3]SELF-SUPERVISED REPRESENTATION LEARNING FOR ULTRASOUND VIDEO\n\n3.The computational cost and inference speed of A-ViT are not discussed, which may limit practical deployment.\n\n4.Some baseline results (e.g., DINO v3) are strong, and the margin of improvement is not always substantial.\n\n5.The main text lacks detailed statistical information about the dataset, such as organ, image size, depth, and classes."}, "questions": {"value": "1.The authors are advised to check the title of the paper: “ANATOMY-AWARE REPRESENTATION LEARNING FOR MEDICAL ULTRASOUND.” There appears to be an extra symbol (a period at the end).\n\n2.Why was the deformable attention mechanism chosen over other spatial-aware transformers? Were alternatives considered?\n\n3.How does the model perform when the anatomical label is noisy or misassigned?\n\n4.Is the performance gain mainly from the proposed architecture or the large-scale dataset? An ablation on dataset scale would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qJq9PAIAIO", "forum": "5ThIWuDkEf", "replyto": "5ThIWuDkEf", "signatures": ["ICLR.cc/2026/Conference/Submission5357/Reviewer_BKQr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5357/Reviewer_BKQr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5357/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913127798, "cdate": 1761913127798, "tmdate": 1762918021073, "mdate": 1762918021073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}