{"id": "wOmjeBN6hP", "number": 9727, "cdate": 1758136849939, "mdate": 1759897701918, "content": {"title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning", "abstract": "Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging. Existing methods mainly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced learning rather than exploration and generalization. To address this issue, we propose **Parallel-R1**, the first reinforcement learning (RL) framework that instills parallel thinking for complex real-world reasoning tasks. Our framework employs a progressive curriculum that addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking behavior, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully elicits parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on difficult tasks with RL. Further analysis reveals a distinct shift in the model's thinking patterns: in the early stage, it utilizes parallel thinking as an exploration strategy, while in the later stage, it employs this ability for multi-perspective verification.\nMost significantly, we validate parallel thinking as a **mid-training exploration scaffold**, where this intermediate phase unlocks a higher performance ceiling after RL, yielding a **42.9%** improvement over the sequential RL baseline.", "tldr": "", "keywords": ["Reasoning Paradigms", "Parallel Thinking", "RL", "LLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fa432224338628c858da3b487b8190cadca36e57.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces Parallel-R1, the first reinforcement learning (RL) framework that successfully trains large language models (LLMs) to perform parallel thinking—i.e., exploring multiple reasoning paths simultaneously—on complex real-world tasks like advanced math problems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper identifies a critical gap: while test-time parallel reasoning (e.g., self-consistency, tree-of-thoughts) is common, models do not intrinsically learn this capability through training. By reframing parallel thinking as a trainable skill—not just an inference-time trick—the work addresses a foundational limitation in LLM reasoning. This aligns with cognitive science (humans use parallel exploration) and practical needs (e.g., robustness in math/STEM tasks), giving the research strong motivation and applicability.\n\nThe paper doesn’t just apply RL—it carefully interrogates how to reward parallel thinking. The alternating reward scheme (switching between accuracy-only and accuracy+parallelism rewards) demonstrates deep understanding of RL pitfalls:\n1. Pure accuracy rewards suppress parallelism;\n2. Pure structural rewards encourage “empty” parallelism without reasoning gains.\nExtensive ablations (e.g., removing Stage 1 RL, testing prompts, comparing reward schemes) provide strong causal evidence for design choices, enhancing scientific credibility."}, "weaknesses": {"value": "The paper operationalizes parallel thinking via syntactic control tags (<Parallel>, <Path>, <Summary>), but does not verify whether the content within <Path> blocks is meaningfully diverse or merely superficially duplicated reasoning. High “parallel ratio” (e.g., 95.6% in SFT models) could reflect template compliance without genuine exploration.\n\nEvidence: Table 2 shows SFT models achieve near-perfect parallel activation but underperform RL variants (e.g., Parallel-SFT-Seen: 36.0 avg vs. Parallel-R1-Seen: 48.9). This suggests format adherence ≠ functional parallelism. Yet the paper lacks metrics for semantic diversity across paths (e.g., path divergence, mutual information, or solution disagreement rates). \n\nAll experiments are confined to math reasoning benchmarks (GSM8K, MATH, AIME, AMC). While math is a natural domain for structured parallel exploration, the paper claims broader relevance to “complex real-world reasoning tasks” (Abstract, Sec 1). However, no evidence is provided for applicability to domains like commonsense reasoning, planning, or open-ended QA—where “critical steps” are less well-defined and verification is harder.\n\n While the paper claims parallel thinking leverages “negligible cost” via GPU parallelism (Sec 1), it reports no inference-time metrics: latency, memory usage, or FLOPs. In practice, generating multiple paths and summaries increases compute. Without this, the claim of practicality is unsubstantiated."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AZjoRBuYzH", "forum": "wOmjeBN6hP", "replyto": "wOmjeBN6hP", "signatures": ["ICLR.cc/2026/Conference/Submission9727/Reviewer_6K5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9727/Reviewer_6K5f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760935582811, "cdate": 1760935582811, "tmdate": 1762921228950, "mdate": 1762921228950, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Parallel-R1, the first framework that uses reinforcement learning to internalize parallel thinking capabilities in large language models, enabling them to adaptively explore multiple reasoning paths simultaneously during inference.\nThe framework employs a progressive curriculum learning approach, starting with supervised fine-tuning to learn the basic format, followed by reinforcement learning to stabilize and generalize this ability across tasks of increasing difficulty.\nExperiments demonstrate that Parallel-R1 significantly improves performance on multiple mathematical reasoning benchmarks and reveals the dynamic shift in parallel thinking during training—from an exploration tool to a verification mechanism.\nWhat's more, the paper identifies parallel thinking as a mid-training exploration scaffold, whose exploratory effects guide the model to a more optimal policy space, enabling it to achieve final performance far exceeding baselines even after the scaffold is removed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem is novel and significant. The paper targets a highly cutting-edge and important problem which is how to internalize the advanced reasoning capability of \"parallel thinking\" in large language models through training (rather than test-time techniques).\n2. The method is systematic, comprehensive, and insightful. Instead of proposing an isolated algorithm, the paper presents a complete, end-to-end solution framework.\n3. The experiments are thorough, and the depth of analysis exceeds conventional standards. The empirical analysis goes beyond superficial \"performance improvement\" to delve deeply into the underlying mechanisms."}, "weaknesses": {"value": "1. Potentially insufficient comparison with the strongest baselines. The paper's primary baselines are sequential-thinking RL methods. However, in the field of parallel reasoning, there exist other powerful test-time techniques, such as various variants of the \"Tree of Thoughts\" approach.\n2. Limited scope in validating generalization ability. All experiments in the paper are focused on the domain of mathematical reasoning. While mathematics serves as a touchstone for evaluating reasoning capabilities, this narrow focus restricts the support for the paper's claim of applicability to \"general reasoning tasks.\""}, "questions": {"value": "1. The study would benefit from a formal analysis of computational efficiency during training. \n2. It is recommended to incorporate comparative analyses with other parallel thinking algorithms\n3. The current evidence cannot distinguish whether the acquired \"parallel thinking\" is a general-purpose reasoning strategy or a specialized skill optimized for mathematical problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iAzPCDhB0F", "forum": "wOmjeBN6hP", "replyto": "wOmjeBN6hP", "signatures": ["ICLR.cc/2026/Conference/Submission9727/Reviewer_RoLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9727/Reviewer_RoLa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893701641, "cdate": 1761893701641, "tmdate": 1762921228569, "mdate": 1762921228569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Parallel-R1 is an RL framework designed to actually train parallel thinking in LLMs, rather than just prompting it, by letting the model explore multiple reasoning paths on complex tasks. The method uses a progressive curriculum: start with SFT on easy, prompt-generated trajectories to “teach” the parallel pattern, then switch to RL so the model can explore and generalize it on harder problems. The authors evaluate their method over several math benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe authors propose a framework specifically aimed at improving parallel reasoning in LLMs.\n2.\tThe introduced multiverse attention mechanism enables end-to-end training of parallel reasoning behaviors.\n3.\tThe authors validate the approach with experiments and report performance."}, "weaknesses": {"value": "1.\tThe authors should clarify why different paths are expected to produce genuinely different reasoning trajectories. As it stands, the only differences between trajectories are the position index and the number of <path> tokens, so it is not obvious that this design actually encourages the model to learn diverse reasoning.\n2.\tThe pass@16 improvement on AIME24 is noticeably smaller than on AIME25; this suggests the method may have high variance across datasets and needs more analysis.\n3.\tQwen3-4B is relatively small; it would strengthen the paper to report results on larger models to show the method scales."}, "questions": {"value": "Please check the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AiyqCKEdKM", "forum": "wOmjeBN6hP", "replyto": "wOmjeBN6hP", "signatures": ["ICLR.cc/2026/Conference/Submission9727/Reviewer_iyKL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9727/Reviewer_iyKL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762117497361, "cdate": 1762117497361, "tmdate": 1762921228126, "mdate": 1762921228126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "A solid RL framework for instilling LLM parallel reasoning.\n\nThe paper introducing a new reinforcement learning framework, `Parallel-R1`, to teach LLMs “parallel thinking.” Its motivation is clear and timely, aligning with the surge of reasoning-enhancement research (e.g., R1, Gemini IMO results). The writing is super clear and the organization is easy to follow.\n\nIt presents a simple yet effective curriculum RL training with parallel reasoning tags (<Parallel>, <Path>, <Summary>) and a multiverse attention variant. \n\nThe work’s significance is strong for mathematical reasoning benchmarks, but broader generalization beyond math remains unclear."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. All components / training method have a clear motivation. I admire authors release such adequate ablation study results and the design motivation behind each component.\n\n2. Strong empirical improvements: Table 2 show 10.2% sequential RL baselines v.s. 42.9% Parallel RL on AIME25\n\n3. Clear training methodology design: Section 3.3’s curriculum RL (SFT → easy RL → hard RL) effectively mitigates cold-start issues.\n\n4. Insightful analysis: Figure 4’s finding that parallel reasoning evolves from exploration to verification is conceptually valuable. We then can use it as a mid-training scaffold sounds promising."}, "weaknesses": {"value": "1. Experiments are restricted to math datasets (AIME, AMC, MATH). I understand that based on the proposed method, the dataset is still hard to obtain. However I still curious about the cross-domain validation.\n\n2, Section 3.4’s “Multiverse” model increases implementation difficulty but the final gain seems very limited. Just want to understand the compute cost."}, "questions": {"value": "1. The curriculum uses an SFT cold-start on easy math (GSM8K) followed by RL on harder tasks. But can you quantify and isolate how much of the final gain comes from the SFT stage vs how much from the RL stage (especially on hard problems)? For example: if you skip the SFT entirely, what happens?\n\n2. Their reward design alternates between a “parallel-structure” reward and an accuracy reward (Section 3.3.2). How sensitive is the final performance to the exact weighting, frequency, and window-size of these alternating rewards? Could a slightly different schedule eliminate the benefit of the “parallel thinking scaffold” effect?\n\n3. The work claims that the model transitions from “parallel thinking for exploration” to “parallel thinking for verification” (Figure 4). How robust is that finding across model sizes, domains (other than math), and random initial seeds? Seems we only conduct experiments on Qwen and math. I wonder if this is applicable to other models and domains.\n\n4. Regarding the “Multiverse” (Parallel-R1-Unseen) architecture: the authors note that performance actually drops in some configurations relative to the autoregressive variant (see Table 2). What hypotheses do authors have about why the explicit path-isolation hurts performance, and how generalizable is that to larger models/harder tasks?\n\n5. The experiments are purely within mathematical reasoning benchmarks (AIME, AMC, MATH). How well would this parallel thinking RL approach transfer to non-numeric reasoning tasks (e.g., commonsense reasoning, planning, multi-hop QA)? What adaptations might be required?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JrdzJMcscK", "forum": "wOmjeBN6hP", "replyto": "wOmjeBN6hP", "signatures": ["ICLR.cc/2026/Conference/Submission9727/Reviewer_EJPr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9727/Reviewer_EJPr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124006893, "cdate": 1762124006893, "tmdate": 1762921227709, "mdate": 1762921227709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Parallel-R1, a reinforcement learning framework that allows reasoning in parallel for complex real-world reasoning tasks. Specifically, it introduces a two-stage framework that performs SFT on prompt-generated trajectories from easier tasks to instill parallel thinking and then RL to explore and generalize on harder problems. Experiments show that the proposed method achieves improved performance on sequential RL baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is the first RL framework to learn parallel thinking on general reasoning tasks. Prior works either only use SFT or perform RL on tasks with synthetic data.\n* The work offers a complete recipe, including SFT and RL, to elicit parallel thinking. The recipe is compatible with existing popular reinforcement learning pipelines such as DAPO.\n* Benchmarks show performance improvements over sequential RL baselines."}, "weaknesses": {"value": "Comparisons with prior works:\n\n* No comparison with prior work Multiverse [1] that the work is heavily based on. Specifically, this work reuses the handing of attention masks and position ids from Multiverse (Sec 3.4). A comparison is needed since both the baseline and the proposed method use distilled reasoning trajectories from a reasoning model such as DeepSeek-R1.\n* The performance is much worse than prior works that do not use RL. Multiverse achieves 53.8% on AIME 24 (Table 2 in [1]), while the work achieves 19.0% (with Parallel-R1-Unseen). This work does not motivate why RL is necessary for parallel thinking, since prior works with only SFT is able to achieve much higher performance. It is true that Multiverse is based on Qwen2.5-32B-Instruct and this work is based on Qwen3-4B-Base, but the cost of running SFT on Qwen2.5-32B-Instruct is much lower than running SFT and RL on Qwen3-4B-Base.\n* The comparison is also much worse than Qwen3-4B, the sequential reasoning variant of the base model. Qwen3-4B achieves 76.0% on AIME 24 (Table 17 in [2]), which is much higher than the work's result. This makes the method less useful in real-world applications, as the performance demonstrated with the proposed parallel reasoning recipe is sub-par compared to existing sequential reasoning variant.\n* The evaluation for Qwen3-4B-Base is likely problematic. Qwen3-4B-Base achieves 54.10% in MATH without instruction tuning (Table 7 in [2]), which is much higher than the work's result, which is 13.9%.\n\nData pipeline:\n\n* In Sec 3.2, the authors mentioned that the LLM pipeline in Multiverse is computationally intensive and limited scalability. However, the proposed data pipeline also involves an LLM. Although the LLM used in the data pipeline is smaller than the one in Multiverse, the pipeline does not work with complex problems (e.g., those in DAPO, as indicated in Table 1). The advantage of the proposed data pipeline is not justified, as Multiverse is able to handle long trajectories with complex problems in the s1K-1.1 dataset.\n\nPerformance:\n\n* The method's baseline selection is not justified. An improved sequential reasoning baseline is to train Qwen3-4B-Base model with the proposed SFT dataset on GSM8K as a sequential model, treating the added special tokens as typical text tokens, and then perform DAPO on the model in a fair setting. The evaluation should be performed using a sequential inference engine. The reviewer believes this is different from \"Parallel-R1-Seen\", since \"Parallel-R1-Seen\" is inferenced with a parallel inference engine.\n* In Table 1, the \"Parallel-R1-Seen\", an autoregressive method (L342) that does not prevent paths from seeing each other in training, outperforms parallel methods (S1 and S2) by a large margin. Furthermore, there might be a discrepancy between training and inference, as the paths are generated in parallel at inference.\n\nEfficiency:\n\n* The method is proposed to improve both performance and efficiency (L129, L300). However, no efficiency comparisons are provided with the baseline methods. The # Parallel is not an efficiency metric because the model can output much longer sequences in parallel to achieve the same performance as in sequential, causing higher latency than in sequential. A valid comparison metric is the latency in terms of the tokens, or wall-clock time if an efficient inference engine is used.\n\nMinor point:\n\n* Writing is not clear: abstract and intro mentions a 42.9% improvement over sequential RL baselines, but this number is not reported in the experiments.\n* In Fig. 4, the parallel ratio goes to 0 starting at Step 300, but the parallel ratio is non-zero in the Parallel-R1 results in Table 2. In L308, the authors indicated that the model is trained with 300 gradient update steps in Stage 2 (which is around Step 500 in total in Fig. 4, as there is a 200-step RL training in Stage 1). These two results are inconsistent. Furthermore, if training for 300 steps leads to a parallel ratio of 0, it indicates that the model will not be able to perform parallel reasoning at inference and will not have improved efficiency compared to the sequential baseline.\n\n[1] Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation. Yang, et al. https://arxiv.org/abs/2506.09991\n\n[2] Qwen3 Technical Report. Qwen Team. https://arxiv.org/abs/2505.09388"}, "questions": {"value": "In addition to the questions in the weaknesses section, the reviewer has the following questions:\n\n1. What are the training time and hardware requirements for Parallel-R1-Unseen (4B)? How does it compare to Multiverse-32B?\n2. What is the exact inference setting for \"Parallel-R1-Seen\"? Is it with a parallel inference engine?\n3. Are there any interpretations for going fully sequential after 300 steps in Fig. 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "u45zz1OiO2", "forum": "wOmjeBN6hP", "replyto": "wOmjeBN6hP", "signatures": ["ICLR.cc/2026/Conference/Submission9727/Reviewer_CufE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9727/Reviewer_CufE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129816715, "cdate": 1762129816715, "tmdate": 1762921227306, "mdate": 1762921227306, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Parallel-R1, which uses RL to teach LLMs to perform \"parallel thinking\" (exploring multiple reasoning paths concurrently) for complex mathematical problems.\n\nTo overcome the difficulty of training this behavior from scratch (the \"cold-start\" problem), the framework uses a progressive curriculum: the model first learns the basic format of parallel thinking on easier math problems. Then, the model then uses RL to generalize this skill to more difficult problems, guided by a reward system that balances task accuracy with the use of parallel structures.\n\nThe authors show that Parallel-R1 improves accuracy on challenging math benchmarks (MATH, AIME) compared to standard sequential RL baselines. The model's strategy evolves during training, shifting from using parallel thinking for broad exploration in early stages to using it for multi-perspective verification in later stages. \n\nIn addition, this paper claims that Parallel thinking can serve as a \"mid-training exploration scaffold,\" where an initial phase of forced parallel exploration helps the model discover better strategies, leading to a higher final performance ceiling even after the model reduces its use of parallel structures."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- As far as I know, this is the first RL framework to teach parallel thinking for complex, real-world mathematical reasoning tasks, moving beyond prior work that focused on synthetic domains or supervised fine-tuning.\n- The paper presents a simple and scalable data pipeline for generating the initial training data, which avoids the computationally intensive and complex pipelines required by previous methods.\n- The proposed framework demonstrates significant and consistent performance improvements across multiple challenging mathematical reasoning benchmarks, including MATH, AIME, and AMC.\n- The research proposes a novel concept of using parallel thinking as a \"mid-training exploration scaffold,\" showing that this intermediate phase helps the model discover better policies and achieve a higher final performance."}, "weaknesses": {"value": "- The entire methodology is validated exclusively on mathematical reasoning tasks. The paper makes no attempt to demonstrate or even discuss how this \"parallel thinking\" capability would transfer to more ambiguous, open-ended, or creative domains like writing, where the notion of distinct, verifiable \"paths\" is far less clear.\n\n- The paper claims that parallel thinking can be achieved at \"negligible cost\" by exploiting GPU parallelism. This is an oversimplification. Generating multiple reasoning trajectories simultaneously consumes substantially more memory and computational resources (total FLOPs) than a single path. While wall-clock time might be similar, it ignores the increased hardware requirements and energy consumption.\n\n- The intriguing idea of parallel thinking as a \"mid-training scaffold\" is supported by a single experiment on one benchmark (AIME25) with an arbitrarily chosen cutoff point (200 steps). This is preliminary evidence at best. \n\n- The most effective reward scheme for the structured model involves a finely-tuned \"alternating\" schedule (80% accuracy, 20% parallel reward, window of 10 steps). This feels more like ad-hoc hyperparameter tuning than a principled approach, adding significant complexity to the training process."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DELu5tSnCR", "forum": "wOmjeBN6hP", "replyto": "wOmjeBN6hP", "signatures": ["ICLR.cc/2026/Conference/Submission9727/Reviewer_qZmU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9727/Reviewer_qZmU"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission9727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219809654, "cdate": 1762219809654, "tmdate": 1762921226640, "mdate": 1762921226640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Parallel-R1, the first Reinforcement Learning framework designed to instill parallel thinking in large language models for complex reasoning tasks. Parallel thinking allows the model to explore multiple reasoning paths concurrently, which is hypothesized to improve reasoning generalization. The key idea is a progressive curriculum: first, the model is trained on simpler tasks via supervised fine-tuning (SFT) to bootstrap parallel thinking behavior, and then RL is applied on more difficult tasks to generalize this skill.\n\nThe authors explore multiple reward schemes, including an alternating reward strategy that balances outcome-based accuracy with parallel thinking behaviors. They also introduce two structural variants—Parallel-Seen (no architectural changes) and Parallel-Unseen (incorporates inductive biases in the attention mechanism to enforce path isolation). Experiments on a suite of math benchmarks (MATH, AMC23, AIME24/25) demonstrate substantial gains: Parallel-R1 improves accuracy by 8.4% over sequential RL models and achieves a 42.9% improvement when parallel thinking is treated as a mid-training exploration scaffold. The paper also provides in-depth analyses of learning dynamics and ablation studies, showing that parallel thinking evolves from exploration to verification during training."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "The paper is highly novel, introducing the first RL framework explicitly designed to train parallel thinking in LLMs. The progressive curriculum that bootstraps learning from simple tasks to more complex problems is well-motivated and effectively addresses the cold-start problem in RL. The experimental results are strong and demonstrate substantial improvements on multiple established math benchmarks, including MATH, AMC23, and AIME24/25. Beyond empirical performance, the paper provides insightful analyses of learning dynamics, revealing that parallel thinking initially functions as an exploration mechanism and later shifts toward verification. The methodological rigor is evident in the extensive ablation studies, careful examination of reward schemes, and exploration of architectural variants. Furthermore, the framework achieves these gains while requiring minimal modifications to the model architecture, which enhances its practical applicability."}, "weaknesses": {"value": "Despite the strong contributions, there are several areas where clarification or further investigation would strengthen the work. First, the generality of Parallel-R1 beyond mathematical reasoning is unclear, and it would be valuable to know whether similar gains could be achieved in other complex reasoning domains, such as commonsense or scientific reasoning. Second, the computational cost of RL on LLMs is potentially significant, yet the paper provides limited discussion of efficiency or resource requirements compared to sequential RL or SFT-only approaches."}, "questions": {"value": "The paper demonstrates strong results on mathematical reasoning benchmarks, but it is unclear how well the Parallel-R1 framework would generalize to other domains requiring complex reasoning, such as commonsense reasoning or scientific problem-solving. Can the authors comment on the expected transferability of parallel thinking beyond math tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "03gHi8UBib", "forum": "wOmjeBN6hP", "replyto": "wOmjeBN6hP", "signatures": ["ICLR.cc/2026/Conference/Submission9727/Reviewer_HXbg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9727/Reviewer_HXbg"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission9727/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762320492154, "cdate": 1762320492154, "tmdate": 1762921225838, "mdate": 1762921225838, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}