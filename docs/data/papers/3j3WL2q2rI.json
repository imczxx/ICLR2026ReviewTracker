{"id": "3j3WL2q2rI", "number": 2436, "cdate": 1757085930961, "mdate": 1759898148174, "content": {"title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams", "abstract": "Most robotic grasping systems rely on converting sensor data into explicit 3D point clouds, which is a computational step not found in biological intelligence. This paper explores a fundamentally different, neuro-inspired paradigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that mimics the biological visuomotor pathway, processing raw, asynchronous events from stereo spike cameras, similarly to retinas, to directly infer grasp poses. Our model fuses these stereo spike streams and uses a recurrent spiking neural network, analogous to high-level visual processing, to iteratively refine grasp hypotheses without ever reconstructing a point cloud. To validate this approach, we built a large-scale synthetic benchmark dataset. Experiments show that SpikeGrasp surpasses traditional point-cloud-based baselines, especially in cluttered and textureless scenes, and demonstrates remarkable data efficiency. By establishing the viability of this end-to-end, neuro-inspired approach, SpikeGrasp paves the way for future systems capable of the fluid and efficient manipulation seen in nature, particularly for dynamic objects.", "tldr": "We propose SpikeGrasp is the first framework that directly estimates 6-DoF grasp poses from raw stereo spike streams", "keywords": ["Neuromorphic Camera", "6-DoF Grasp Pose Detection", "Spiking Neural Network", "Stereo Vision"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2dc9cbad37d314fb71e93fa331db49952b944a83.pdf", "supplementary_material": "/attachment/f0d1a408637e1f416105093a400862e57b44130a.zip"}, "replies": [{"content": {"summary": {"value": "This paper innovatively utilizes spike streams as perceptual signals to predict 7-DoF grasp poses. To achieve this, the paper proposes the SpikeGrasp framework and synthesizes a dataset in Blender for model training. The proposed method demonstrates promising performance on the test data. The main contributions of the paper are as follow:\n\n1.\tThis paper proposes the SpikeGrasp architecture which process spike streams and outputs grasp pose.\n\n2.\tThis paper presents the first large-scale synthetic spike stream dataset for 6-DoF grasp pose detection."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThis paper introduces an end-to-end 6-DoF pose detection network based on stereo spike streams, which is different from the previous works that are based on RGBD or point clouds.\n\n2.\tThis paper presents the first synthetic spike stream dataset for 6-DoF grasp pose detection,"}, "weaknesses": {"value": "1.\tThe dataset presented in this paper constitutes an incremental extension of the GraspNet-1Billion [1] dataset. It retains similarity with GraspNet-1Billion in terms of the included objects, the grasp pose annotations, and the data partitioning strategy. The primary enhancement lies in the incorporation of the spike stream modality. Furthermore, the evaluation benchmark remains consistent with that used for GraspNet-1Billion.\n2.\tThe paper does not clearly explain why spike streams are used as input. According to the results in Table 1, SpikeGrasp only achieves an 38.84 AP on the seen test set, while GSNet which gets single view point clouds already reaches 65.7 AP on the GraspNet-1Billion test set. Furthermore, I am curious why GSNet only achieves 34.52 AP on the synthetic dataset? What are the distinctive features of this synthetic dataset compared to GraspNet-1Billion?\n3.\tThe experiments presented in this paper have certain limitations. First, the proposed method is not evaluated on the widely adopted GraspNet-1Billion benchmark. This would support the results and make them stronger. Furthermore, the study lacks real-world experiments, which are essential for demonstrating the practical applicability and robustness of the approach in physical environments.\n\n[1] Fang, Hao-Shu, et al. \"Graspnet-1billion: A large-scale benchmark for general object grasping.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."}, "questions": {"value": "1.\tWhy does the paper use Stereo Spike Streams as the model input? What advantages does it offer compared to RGB-D or point cloud data for the 6-DoF grasp pose detection task?\n2.\tWhere does the gap between real and simulated Stereo Spike Streams data come from? Can the method proposed in the paper be used in real-world scenarios, and how effective is it?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2tRz5FDsi6", "forum": "3j3WL2q2rI", "replyto": "3j3WL2q2rI", "signatures": ["ICLR.cc/2026/Conference/Submission2436/Reviewer_uiZd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2436/Reviewer_uiZd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761478484371, "cdate": 1761478484371, "tmdate": 1762916237354, "mdate": 1762916237354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes SpikeGrasp, which uses stereo spike streams to obtain  6-DoF grasp poses. This differs from previous methods that depend on explicit geometry reconstruction. The paper also curates a synthetic grasping dataset and defines a custom evaluation protocol. The experiment in the synthetic environments suggests competitive performances against other baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The end-to-end “spikes to grasp” pipeline is technically coherent and clearly pipelined.\n\n2. The paper includes reasonable ablations showing the effect of objectness and graspness branches and spike choices."}, "weaknesses": {"value": "1. The story is not convincing. The introduction starts well, and the motivation is clear, but the subsequent parts, including the methodology, do not really effectively adhere to the points conveyed in the paper. The current writing lacks reasons for designing each module. I agree with the claim that humans do not have an explicit 3D sensing in the brain to manipulate objects.  I also feel interested in a Spike-driven solution. However, I do not feel a very strong connection to why the authors designed the network in this way. \n\n2. The central narrative is that direct “spikes → grasp” is viable and efficient. However, all validation is synthetic or simulation-only. There should be a real spike-camera experiment for evaluation.\n\n3. Some typos can be found under Supplementary Material. For example, A.2.1 PROBLEM STATEMENT is typed twice."}, "questions": {"value": "Can you report some real spike-camera experiments with SR@1/5/10 and PR-AUC, plus wall-clock latency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MrTAeC8NCO", "forum": "3j3WL2q2rI", "replyto": "3j3WL2q2rI", "signatures": ["ICLR.cc/2026/Conference/Submission2436/Reviewer_xtg1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2436/Reviewer_xtg1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691909019, "cdate": 1761691909019, "tmdate": 1762938717426, "mdate": 1762938717426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a bio-inspired 6-DoF grasp detection method called SpikeGrasp, which mimics the biological visuomotor pathway. Similar to retinas, the method processes raw and asynchronous events from stereo spike cameras to directly infer grasp poses and then refines grasp hypotheses via a recurrent spiking neural network without reconstructing a point cloud. The authors built a synthetic benchmark dataset to evaluate SpikeGrasp, and compared it with standard methods. Good results are reported."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a novel bio-inspired grasping method that uses stereo spike cameras, which is new in the field of grasping.\n\n2. This paper proposes a new synthetic spike stream dataset for spike-based grasping research.\n\n3. This paper conducts a set of experiments and demonstrates the effectiveness of the proposed method."}, "weaknesses": {"value": "1. The paper does not clearly analyze the benefits of spike-based grasping, and the principle of SpikeGrasp should be further elaborated.\n\n2. The paper lacks real-world experiments, which is hard to demonstrate the validity of grasping methods.\n\n3. The relation of this work and existing literature should be analyzed in more depth. What is the design rationale of the proposed method should be elaborated more."}, "questions": {"value": "1. How does SpikeGrasp deal with dynamic environments?\n\n2. The paper states that 6-DoF grasp annotations of objects are sourced from GraspNet-1Billion. Could the authors provide the detailed process of how to source grasp annotations?\n\n3. I am a bit confused why real-world experiments were not conducted. What are the challenges?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VXN2MWdB4K", "forum": "3j3WL2q2rI", "replyto": "3j3WL2q2rI", "signatures": ["ICLR.cc/2026/Conference/Submission2436/Reviewer_b9PB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2436/Reviewer_b9PB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838193816, "cdate": 1761838193816, "tmdate": 1762916236354, "mdate": 1762916236354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author proposes SpikeGrasp, a deep learning framework for 6-DoF grasp detection using raw spike streams. The model extracts features from the spike streams and then uses the features to generate objectness, graspness, and the final grasp pose. Also, a new synthetic spike stream dataset for 6-DoF grasp detection is proposed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The use of spike stream offers new insight into grasp detection. The approach eliminates the need for depth information, which may sometimes be inaccurate. \n\nThe author proposes a new synthetic dataset with spike streams, which fills the underserved evaluation gap for spike stream-based methods and is a useful resource for the community.\n\nThe model is computationally efficient in theory."}, "weaknesses": {"value": "The methods used to compare are mostly dated. The author should consider evaluatingmore state-of-the-art methods, e.g., HGGD or EconomicGrasp.\n\nThe author should conduct more real-world experiments instead of just using simulated settings. Real robot experiments are very important for validating the effectiveness of the proposed method."}, "questions": {"value": "Is the depth modal still in the new synthetic dataset? Since many other methods in the quantitative evaluation require depth as a input modal, the author should elaborate this point in the details of the dataset. \nWhy is the result of GraspNet on the synthetic dataset indentical to that on the GraspNet-1Billion dataset?\nGSNet has a relatively high AP on the GraspNet-1Billion seen dataset. Why is there a huge performance degradation on the seen synthetic dataset, namely 34.52. Is the dataset properly generated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Gu2G7oQzot", "forum": "3j3WL2q2rI", "replyto": "3j3WL2q2rI", "signatures": ["ICLR.cc/2026/Conference/Submission2436/Reviewer_gP3W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2436/Reviewer_gP3W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2436/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844671591, "cdate": 1761844671591, "tmdate": 1762916236126, "mdate": 1762916236126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}