{"id": "DM0Y0oL33T", "number": 8896, "cdate": 1758101645780, "mdate": 1763631087465, "content": {"title": "Generative Universal Verifier as Multimodal Meta-Reasoner", "abstract": "We introduce *Generative Universal Verifier*, a novel concept and plugin designed for next-generation multimodal reasoning in vision-language models and unified multimodal models, providing the fundamental capability of reflection and refinement on visual outcomes during the reasoning and generation process. This work makes three main contributions: (1) We build **ViVerBench**, a comprehensive benchmark spanning $16$ categories of critical tasks for evaluating visual outcomes in multimodal reasoning. Results show that existing VLMs consistently underperform across these tasks, underscoring a substantial gap from human-level capability in reliable visual verification. (2) We design two automated pipelines to construct large-scale visual verification data and train **OmniVerifier-7B**, the first omni-capable generative verifier trained for universal visual verification and achieves notable gains on ViVerBench(+$8.3$). Through training, we identify three atomic capabilities in visual verification and demonstrate how they generalize and interact synergistically. (3) We propose **OmniVerifier-TTS**, a sequential test-time scaling paradigm that leverages the universal verifier to bridge image generation and editing within unified models, enhancing the upper bound of generative ability through iterative fine-grained optimization. Beyond generation, we extend universal verifier to broader world-modeling interleaved reasoning scenarios. Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+$3.7$), and GenEval++(+$4.3$), outperforming existing parallel test-time scaling methods, such as Best-of-N. By endowing multimodal reasoning with reliable visual verification, OmniVerifier advances both reliable reflection during generation and scalable test-time refinement, marking a step toward more trustworthy and controllable next-generation reasoning systems.", "tldr": "", "keywords": ["Multimodal Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1c83185ff1b476816381b4646296a18a650e9b51.pdf", "supplementary_material": "/attachment/34427db3eed3abc11eb57364d95b024cfbbb7e12.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Generative Universal Verifier, a new approach to help AI models better verify and improve visual outputs during reasoning. The authors first create ViVerBench, a comprehensive benchmark showing current models struggle with visual verification tasks. They then develop automated methods to build training data and train OmniVerifier-7B, which significantly improves verification performance. Finally, they apply this verifier through OmniVerifier-TTS, a sequential refinement system that enhances image generation quality through iterative verification and editing."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The structure of this paper is clear, which investigates three central questions. \n2. The ViVerBench benchmark is well-designed with 16 diverse task categories and careful human validation, providing a solid foundation for evaluating visual verification capabilities.\n3. The identification of three atomic capabilities (Explicit Alignment, Relational Verification, Integrative Reasoning) offers important insights into how visual verification works and generalizes.\n4. The sequential TTS approach shows clear advantages over parallel methods like Best-of-N, achieving better results with fewer generation steps."}, "weaknesses": {"value": "1. Computational costs of running multiple verification and editing steps aren't adequately addressed, which could limit practical application. The authors should discuss the extra inference cost when adopting OmniVerifier-TTS.\n2. As shown in Table 1, the core component, OmniVerifier-7B, achieves an overall score of only 0.653 on ViVerBench. Relying on such a judge can introduce a fundamental risk into the entire iterative refinement process.\n3. While the authorsdemonstrate in Table 2 that their specifically trained OmniVerifier-7B judge outperforms QwenVL, they do not explore whether a judge with superior foundational abilities would yield further gains. For instance, as shown in Table 1, Gemini-2.5-Pro achieves the highest score on the ViVerBench benchmark. Would a more powerful judge like Gemini-2.5-Pro lead to significantly better TTS results？"}, "questions": {"value": "1. What are the specific failure cases where sequential TTS performs worse than parallel approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "8rAuvm6ZgN", "forum": "DM0Y0oL33T", "replyto": "DM0Y0oL33T", "signatures": ["ICLR.cc/2026/Conference/Submission8896/Reviewer_Q1VF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8896/Reviewer_Q1VF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760851852170, "cdate": 1760851852170, "tmdate": 1762920651774, "mdate": 1762920651774, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the \"Generative Universal Verifier,\" a concept aimed at endowing next-generation multimodal models with the crucial ability to reflect upon and refine their visual outputs. The work presents three core contributions:\n\n1）ViVerBench: A new, comprehensive benchmark spanning 16 task categories designed to evaluate the verification of visual outcomes. Experiments on this benchmark reveal that existing SOTA VLMs significantly underperform human-level capability.\n\n2）OmniVerifier-7B: The authors design two automated data construction pipelines to train a 7B generative verifier, which achieves a substantial +8.3 gain on ViVerBench. Through this process, they identify and analyze three \"atomic capabilities\" of visual verification: Explicit Alignment, Relational Verification, and Integrative Reasoning.\n\n3. OmniVerifier-TTS: A novel sequential test-time scaling (TTS) paradigm is proposed. This method uses the verifier to bridge image generation and editing within unified models, enabling iterative, fine-grained optimization. This sequential approach is shown to outperform parallel TTS methods like Best-of-N on benchmarks such as T21-ReasonBench and GenEval++."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1）Problem Importance: The paper addresses a highly important and timely problem. As MLLMs move towards complex, interleaved reasoning and generation, the ability to self-critique and refine *visual* outputs, not just text, is a fundamental requirement for building more reliable and controllable systems.\n\n2）High-Quality Benchmark: ViVerBench is a major contribution in itself. It is comprehensive, challenging, and meticulously constructed. The evaluations on it provide clear and actionable insights, identifying specific weaknesses in current SOTA models (e.g., mismatched world knowledge, underdeveloped critics for visual reasoning).\n\n3）Deep Capability Analysis: The investigation in Section 4.2 that leads to the identification of three \"atomic capabilities\" (Explicit Alignment, Relational Verification, Integrative Reasoning) is a standout feature. The discovery that the first two capabilities generalize well, while the third is task-specific, is a deep insight that provides clear guidance for training more efficient and effective universal verifiers.\n\n4）Novel Sequential TTS: The OmniVerifier-TTS framework is a significant methodological advance. It smartly reframes test-time refinement from a parallel *selection* problem (Best-of-N) to a sequential *optimization* problem. By using the verifier to generate concrete \"edit prompts,\" it enables iterative, fine-grained correction, leading to a higher performance ceiling than parallel methods."}, "weaknesses": {"value": "1）Unclear Data Construction Diagram: The diagram illustrating the data construction pipeline in Figure 2 is confusing, particularly for \"Method 2: Prompt-fixed Image-Inpainting\". As per the flowchart, the line from GPT-5 seems to originate only from the SAM-processed 'True Image'. The 'False Image', generated by FLUX inpainting, lacks a clear connection to the final \"Prompt & Explanation\" output. This makes the data-pairing process for false examples ambiguous and difficult to follow.\n\n2）Limits of \"Universality\": The paper honestly acknowledges that the \"Integrative Reasoning\" capability (e.g., Maze, Robotics) shows minimal cross-task generalization and requires task-specific data. This challenges the \"Universal\" claim to some extent. The paper would be strengthened by further discussing the implications of this finding for building a *truly* universal verifier."}, "questions": {"value": "1）Addressing Performance Drops: Why does the OmniVerifier-7B model, after being trained with RL on alignment and relational data, show such a performance drop on the 'Chart' task and stagnate on 'Static Physics' (Table 1)? These tasks (especially 'Chart') are prime examples of the atomic capabilities the model was supposedly trained to improve.\n\n2）Rationale for GPT-5 vs. Gemini 2.5 Pro: What was the rationale for using GPT-5 for the visual verification data construction pipelines? Given that your own evaluation (Table 1) identified Gemini 2.5 Pro as the superior model for these verification tasks, were there specific capabilities (e.g., superior prompt generation, better compliance with modification instructions) where GPT-5 was found to be more suitable for the data creation role?\n\n3）Path for Integrative Reasoning: Given the poor generalization of \"Integrative Reasoning\" tasks, do you see a path toward improving their generalization (e.g., through different training strategies or data augmentation), or do you believe a task-specific \"mixture-of-verifiers\" model is the more practical future direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "P8OJJZ1FUp", "forum": "DM0Y0oL33T", "replyto": "DM0Y0oL33T", "signatures": ["ICLR.cc/2026/Conference/Submission8896/Reviewer_Ctie"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8896/Reviewer_Ctie"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921094364, "cdate": 1761921094364, "tmdate": 1762920651297, "mdate": 1762920651297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets a missing skill in multimodal LMs: checking whether generated/used images actually match the prompt/reasoning. It builds ViVerBench to show current VLMs lag far behind humans, trains a 7B OmniVerifier via two automatic visual-verification data pipelines, and plugs the verifier into a sequential test-time scaling setup so the model can detect mistakes and issue edit prompts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated problem: visual-outcome verification is clearly under-served and the benchmark exposes real gaps.\n\n- Data pipelines are scalable and produce clean true/false supervision for several atomic verification skills.\n\n- Nice systems angle: using the verifier to improve generation (not just evaluate) via sequential TTS is practical and shows gains."}, "weaknesses": {"value": "- Overstated Universal Claim: The \"Universal Verifier\" title seems to overstate the model's capabilities, especially given the limitations candidly discussed by the authors. The admission that it \"generalize less effectively\" to tasks like mazes due to a large domain gap suggests the verifier is still highly dependent on task-specific data distributions.\n- Evaluation is mostly on the authors’ benchmark / close tasks; less evidence for robustness on messy, real-world images."}, "questions": {"value": "- In OmniVerifier-TTS, the iterative editing process depends on the “edit prompts.” How are hallucinated or semantically incorrect edit suggestions filtered or corrected during inference?\n\n- For the two automated pipelines you use to construct large-scale visual verification data, how sensitive is the final verifier’s performance to noise or inconsistencies introduced by the upstream models?  and did you consider any robustness or denoising steps beyond the current filtering?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "22uLZci2xN", "forum": "DM0Y0oL33T", "replyto": "DM0Y0oL33T", "signatures": ["ICLR.cc/2026/Conference/Submission8896/Reviewer_YEzL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8896/Reviewer_YEzL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973598444, "cdate": 1761973598444, "tmdate": 1762920650836, "mdate": 1762920650836, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response"}, "comment": {"value": "We sincerely thank all the reviewers for their thorough reviews and valuable feedback. We are glad to hear that we addresses an important and timely problem (Reviewer YEzL, Ctie), high-quality benchmark and data construction: (all reviewers), the experimental analysis is thorough and presents a novel approach (all reviewers).\n\nWe summarize our responses to the reviewers' comments as follows:\n\n- We additionally provide more evaluation on mainstream benchmarks to show the improvement of OmniVerifier and **updated our manuscript in the Appendix. B** (Reviewer YEzL)\n- We additionally conduct an in-depth analysis of OmniVerifier-TTS, including its runtime efficiency and a detailed examination of its strengths and weaknesses, and **updated our manuscript in the Appendix. C, D, and E** (Reviewer Q1VF)\n- We provide additional insights and analysis regarding the potential capabilities of future universal verifiers and their possible applications. (Reviewer YEzL, and Ctie)\n\nIn addition, we would like to take this opportunity to discuss and summarize what future Universal Verifiers could achieve and the potential benefits they may bring:\n\n1. **Verifier as a strong policy model.** In the future, we envision that the verifier will no longer be a standalone model. Instead, we aim to train powerful judgment capabilities on top of strong policy models, with the two complementing each other. We expect a multimodal judge that not only evaluates text outcomes but also demonstrates strong generalization across visual and omni-modality outcomes. We point out that self-improvement drives model progress, enabling self-evaluation and self-refinement without relying on labels, ultimately achieving self-evolution. OmniVerifier represents the first step toward this vision.\n2. **For traditional Text-to-Image generation.** OmniVerifier has the potential to enable fully automated end-to-end evaluation. Automated evaluation of text-to-image generation has historically been highly inaccurate, with precise assessment heavily dependent on human effort, which greatly limits further progress in T2I generation. We have been exploring how advanced vision-language models can facilitate full-process evaluation, and in the future, we plan to invest more in model scaling and exploring stronger training paradigms to advance toward this goal.\n3. **For the future era of interleaved reasoning and generation.** Crucially, a full-modality hybrid judge is essential, moving beyond the constraints of text-outcome verification. To address the current limitations in assessing visual outcomes, we are steering OmniVerifier toward sequence-level cross-modal verification. This development is particularly vital for complex tasks such as game reasoning, GUI interaction, and visual narratives.\n\nWe reply to each reviewer's questions in detail below their reviews. Please kindly check them out. Thank you and please feel free to ask any further questions."}}, "id": "vzcZGVFocg", "forum": "DM0Y0oL33T", "replyto": "DM0Y0oL33T", "signatures": ["ICLR.cc/2026/Conference/Submission8896/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8896/Authors"], "number": 13, "invitations": ["ICLR.cc/2026/Conference/Submission8896/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763632718277, "cdate": 1763632718277, "tmdate": 1763632718277, "mdate": 1763632718277, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}