{"id": "wue4blBoAx", "number": 15441, "cdate": 1758251365843, "mdate": 1762999830718, "content": {"title": "Toward Spatial Intelligence: A Unified Self-supervised Framework for 3D Representation Learning from Unposed Multi-View Images", "abstract": "Robust 3D representation learning forms the perceptual foundation of spatial intelligence, enabling downstream tasks in scene understanding and embodied AI. However, learning such representations directly from unposed multi-view images remains challenging. Recent self-supervised methods attempt to unify geometry, appearance, and semantics in a feed-forward manner, but they often suffer from weak geometry induction, limited appearance detail, and inconsistencies between geometry and semantics.\nWe introduce $\\textbf{\\textit{UniSplat}}$, a self-supervised framework designed to address these limitations through three complementary components. \nFirst, we propose a $\\textit{dual-masking strategy}$ that strengthens geometry induction in the encoder. By masking both encoder and decoder tokens, and targeting decoder masks toward geometry-rich regions, the model is forced to infer structural information from incomplete visual cues, yielding geometry-aware representations even under unposed inputs.\nSecond, we develop a $\\textit{coarse-to-fine Gaussian splatting strategy}$ that enhances appearance learning by progressively refining the radiance field, thereby enhancing appearance detail to produce high-fidelity representations.Finally, to enforce geometric–semantic consistency, we introduce a \\textit{pose-conditioned recalibration mechanism} that interrelates the outputs of multiple heads by reprojecting predicted 3D point and semantic maps into the image plane using estimated camera parameters, and aligning them with corresponding RGB and semantic predictions to ensure cross-task consistency and resolving geometry–semantic mismatches. \nTogether, these components yield unified 3D representations that are robust to unposed, sparse-view inputs and generalize across diverse tasks, laying a perceptual foundation for spatial intelligence.Finally, to enforce geometric–semantic consistency, we introduce a $\\textit{pose-conditioned recalibration mechanism}$ that interrelates the outputs of multiple heads by reprojecting predicted 3D point and semantic maps into the image plane using estimated camera parameters, and aligning them with corresponding RGB and semantic predictions to ensure cross-task consistency and resolving geometry–semantic mismatches. \nTogether, these components yield unified 3D representations that are robust to unposed, sparse-view inputs and generalize across diverse tasks, laying a perceptual foundation for spatial intelligence.", "tldr": "UniSplat is a self-supervised, feed-forward framework that jointly learns geometry, appearance, semantics, and camera calibration from unposed multi-view images.", "keywords": ["Self-supervised learning", "3D Gaussian splatting", "Feed-forward 3D reconstruction", "Spatial intelligence"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/8682f38ee9979dcee9d490d65e078abc0f71ed4f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces UniSplat, a self-supervised framework designed to learn a unified 3D representation of geometry, appearance, and semantics from unposed multi-view images. Specifically, the paper proposes:\n\n(1) a *dual-masking strategy* that uses gaussian-guided geometric cues to extract robust geometric information from partial visual inputs;\n\n(2) a *coarse-to-fine Gaussian splatting strategy* that progressively refines anchor, semantic, and appearance Gaussians, addressing the granularity mismatch between semantic and appearance representations in conventional 3D learning; \n\n(3) a *pose-conditioned recalibration mechanism* that leverages VLM and VGGT to provide supervision for semantic and geometric alignment, improving their consistency.\n\nOverall, the method shows excellent performance on both traditional 3D vision and embodied AI tasks, demonstrating its effectiveness and versatility."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and logically structured, focusing on the unified self-supervised learning of geometry, appearance, and semantics in 3D representation learning.\n\n2. By leveraging Gaussian splatting to provide geometric cues, the method effectively mitigates the issue of overfitting to trivial textures commonly seen in traditional approaches.\n\n3. The large gap between semantic fields and appearance fields is progressively bridged through the introduction of anchors, enabling better alignment between the two.\n\n4. The proposed method achieves strong performance across multiple benchmarks, with comprehensive evaluation metrics and thorough ablation studies, demonstrating the soundness and robustness of the approach."}, "weaknesses": {"value": "1. Although geometry, appearance, and semantics are all important factors for unified 3D representation, considering all of them within a single framework may lead to a potential issue of being *broad but not deep*.\n\n2. The experimental section lacks sufficient analysis or evidence to clearly demonstrate the advantages of UniSplat in achieving a unified representation across geometry, appearance, and semantic aspects.\n\n3. The overall framework of UniSplat is relatively complex. Although it might be efficient in practice, it is unclear whether the method is easy to reproduce.\n\n4. UniSplat involves a large number of hyperparameters, yet their selection process and sensitivity are not discussed or analyzed in the paper."}, "questions": {"value": "1. Section 3.4 introduces the fusion strategy between geometry and semantics, while Section 3.3 describes the alignment between semantics and appearance. Beyond these, does UniSplat explicitly consider the direct relationship between geometry and appearance?\n2. Although the paper claims that UniSplat is a self-supervised framework, its training relies on supervision signals from VLM and VGGT for semantic and geometric guidance, respectively. Does this contradict the self-supervised learning claim?\n3. What is the source of the GT shown in Figure 2? In some cases (e.g., Case 2 and Case 3), the GT results appear even worse than other methods.\n4. In Section 4.4, how is the **without self-supervised** variant implemented? Does it refer to using the base model directly for inference without additional self-supervised training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aWela2l8YP", "forum": "wue4blBoAx", "replyto": "wue4blBoAx", "signatures": ["ICLR.cc/2026/Conference/Submission15441/Reviewer_WiqJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15441/Reviewer_WiqJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468163437, "cdate": 1761468163437, "tmdate": 1762925724140, "mdate": 1762925724140, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "Y4jQ7nUd58", "forum": "wue4blBoAx", "replyto": "wue4blBoAx", "signatures": ["ICLR.cc/2026/Conference/Submission15441/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15441/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762999829396, "cdate": 1762999829396, "tmdate": 1762999829396, "mdate": 1762999829396, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniSplat, a self-supervised framework for learning unified 3D representations from unposed multi-view images. The method integrates three components: (1) a dual-masking strategy for geometry-aware learning, (2) a coarse-to-fine Gaussian splatting pipeline for refining appearance, and (3) a pose-conditioned recalibration mechanism for enforcing consistency between geometry and semantics.\n\nThe authors claim that UniSplat achieves robust 3D perception under unposed and sparse-view settings, generalizes across diverse tasks, and forms a “perceptual foundation for spatial intelligence.” Experiments are conducted on ScanNet, RealEstate10K, and a set of embodied AI benchmarks, showing improvements over prior pose-free baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) The architecture combines several meaningful ideas — e.g., multi-level Gaussian refinement and multi-head consistency — that are intuitively complementary.\n\n(2) The writing is clear, and the motivation for unified geometry–semantics–appearance modeling is well-articulated.\n\n(3) Results on ScanNet indicate measurable gains in segmentation, depth, and rendering metrics over prior self-supervised methods.\n\n(3) The idea of cross-task recalibration using estimated poses is interesting and could encourage better 3D–semantic alignment."}, "weaknesses": {"value": "(1) **Overclaiming conceptual novelty:**\nThe paper frames UniSplat as a “unified self-supervised foundation for spatial intelligence”, which overstates its conceptual reach.\nThe proposed components (masking, Gaussian splatting, and cross-head consistency) are incremental combinations of existing trends — notably, dual-masking from VideoMAE [Wang et al. 2023], coarse-to-fine Gaussian strategies from Scaffold-GS [Lu et al. 2024], and geometry-semantic reprojection from prior multi-task 3D learning works.\nThe “unified” claim appears to stem from integrating these techniques rather than introducing a fundamentally new paradigm.\n\n(2) **Ambiguity in “self-supervision”:**\nThe model relies heavily on distillation from pre-trained teachers (LSeg, VGGT) for both semantic and geometric priors.\nThis contradicts the “self-supervised” framing, as these teachers are trained with substantial supervision.\nThe resulting pipeline is more accurately pseudo-supervised or teacher-assisted pretraining, not purely self-supervised learning from raw unposed images.\n\n(3) **Insufficient experimental validation:**\nAblations are limited to ScanNet and RealEstate10K, with no evaluation on out-of-distribution datasets (e.g., CO3D, KITTI, Replica, or ACID). This makes it difficult to assess the claimed “generalization across domains and tasks.”\nThe pose-conditioned recalibration mechanism is central to the paper, yet there is no quantitative evaluation or visualization of its contribution beyond a single ablation line in Table 3. \nThe paper does not show whether pose estimates are accurate, stable, or beneficial under large viewpoint shifts.\n\n(4) **Lack of clarity on failure cases:**\nThe paper does not present qualitative examples where UniSplat fails or underperforms, which would be crucial for assessing its robustness to noise, textureless regions, or dynamic content.\nThe conclusion asserts that the model provides a “foundation for spatial intelligence,” yet there is no direct evidence of reasoning, planning, or long-horizon understanding tasks to support this statement."}, "questions": {"value": "(1) **Clarify supervision hierarchy:**\nTo what extent are VGGT and LSeg required during training? Could UniSplat be trained without these external priors, and if so, how would performance degrade?\n\n(2) **Pose estimation reliability:**\nHow accurate are the internally estimated poses? Have you compared them with ground truth on ScanNet to verify geometric plausibility?\n\n(3) **Scope of generalization:**\nHave you tested on outdoor or dynamic datasets to support the “spatial intelligence” claim? If not, the language should be toned down to “indoor scene understanding.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6q2Ifm84tx", "forum": "wue4blBoAx", "replyto": "wue4blBoAx", "signatures": ["ICLR.cc/2026/Conference/Submission15441/Reviewer_u436"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15441/Reviewer_u436"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761568171636, "cdate": 1761568171636, "tmdate": 1762925723612, "mdate": 1762925723612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UniSplat, a framework for unified 3D scene understanding from unposed multi-view images. The key contributions include: (1) a geometry-aware dual-masking strategy to encourage 3D reasoning, (2) a hierarchical coarse-to-fine Gaussian splatting representation that refines the radiance field, (3) pose-conditioned recalibration to enforce cross-task geometric consistency\n\nThe method achieves strong pose-free quality for novel view synthesis without requiring SfM or per-scene optimization. Moreover, as a frozen encoder, it attains top performance across multiple embodied tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces three key innovations: dual masking, a coarse-to-fine Gaussian hierarchy, and pose-conditioned recalibration. These components work together to enable a pose-free, feed-forward approach to learning 3D geometry from unposed multi-view images.\n2. Each module has a clear objective and is designed to contribute to the overall model. The method achieves strong results across ScanNet, RealEstate10K, and robotics benchmarks.\n3. The paper is clearly written, with helpful figures and formulas. Training details and ablation analyses are provided, and sufficient comparison results are included as well.\n4. The work offers an SfM-free and per-scene-optimization-free approach. It provides a framework for RGB-only 3D perception that benefits downstream 3D tasks while achieving strong performance."}, "weaknesses": {"value": "1. Heavy reliance on teacher models: The proposed self-supervised method relies on LSeg and VGGT distillation rather than purely self-supervised. This may introduce teacher biases and limits the claim of self supervision.\n2. Module efficiency vs. complexity: In ablation studies, dual masking and coarse-to-fine splatting bring only modest performance gains, yet introduce additional computational cost and system complexity.\n3. Hyperparameter sensitivity: The method performance depends on mask ratios and gaussian latent tokens. The current exploration is limited, raising concerns about robustness cross datasets."}, "questions": {"value": "1. What is the concrete advantage of distilling from LSeg/VGGT into UniSplat instead of directly using these SOTA models as the visual backbone?\n2. Dual masking and coarse-to-fine splatting add computational overhead but provide modest gains. Can you provide a compute comparison: with and without these modules — to illustrate the trade-off between efficiency and performance?\n3. How stable are the mask ratios and Gaussian token counts across different datasets? Can you provide variance analysis to demonstrate robustness of the current setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JZW4JGvOeU", "forum": "wue4blBoAx", "replyto": "wue4blBoAx", "signatures": ["ICLR.cc/2026/Conference/Submission15441/Reviewer_GKfm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15441/Reviewer_GKfm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852167492, "cdate": 1761852167492, "tmdate": 1762925722880, "mdate": 1762925722880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents UniSplat, a self-supervised framework that learns 3D spatial representations from unposed multi-view images without requiring camera calibration or explicit geometric priors. The method introduces a unified latent space that jointly encodes scene appearance, depth, and spatial consistency using a multi-view feature alignment objective and a geometry-guided splatting decoder."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1, Learning 3D representations from unposed data is a genuinely challenging and valuable problem, given the increasing prevalence of Internet-scale uncalibrated imagery.\n\n2, The framework is well-engineered, combining multi-view correspondence, depth estimation, and differentiable rendering into a cohesive pipeline.\n\n3, The proposed recalibration module is a well-motivated and technically sound component that explicitly enforces geometric–semantic–appearance consistency within the predicted 3D scene. By re-projecting 3D point and semantic maps into the image plane using the estimated camera parameters, the mechanism provides a concrete spatial anchor that ties together outputs from different decoder heads. This design elegantly compensates for the lack of ground-truth camera supervision in unposed settings and introduces an implicit self-calibration loop that encourages all modalities to converge toward a coherent spatial frame.\n\n4, UniSplat achieves reasonable results on benchmark datasets for both view synthesis and representation learning. Ablations show that each component contributes positively to performance, indicating a sound implementation."}, "weaknesses": {"value": "1, The proposed “unified framework” mainly integrates well-known components: multi-view feature alignment (as in Spatial-ViT, MV-SSL), differentiable splatting/rendering (as in Gaussian Splatting, IBRNet), and self-distillation (as in DINO / iBOT). The central idea is a combination rather than a fundamentally new principle. The paper does not provide a new learning theory, loss formulation, or representation insight.\n\n2, The paper assumes that multi-view consistency induces 3D awareness, but provides no analysis or quantification of learned geometry.\n\n3, While the paper attributes the model’s improved geometry awareness to the proposed dual masking strategy, the mechanism lacks convincing theoretical or empirical justification.  Masking encoder and decoder tokens may regularize training and improve feature robustness, but it does not explicitly enforce cross-view geometric consistency or physical projection constraints—the key requirements for genuine 3D structure learning.  Moreover, the “geometry-guided” second-stage mask depends on coarse Gaussian fields that are themselves predicted from unposed inputs and thus may not provide reliable structural cues, creating a circular dependence.  Consequently, it remains unclear whether the observed performance gains stem from enhanced geometry reasoning or simply from stronger feature regularization.  Additional ablations or visualization of learned depth and correspondence would be necessary to substantiate the claimed geometric benefit."}, "questions": {"value": "See the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PSPVsND784", "forum": "wue4blBoAx", "replyto": "wue4blBoAx", "signatures": ["ICLR.cc/2026/Conference/Submission15441/Reviewer_iuFM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15441/Reviewer_iuFM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15441/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762057228334, "cdate": 1762057228334, "tmdate": 1762925722187, "mdate": 1762925722187, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}