{"id": "PTTmPHS7OE", "number": 18432, "cdate": 1758287768258, "mdate": 1759897104037, "content": {"title": "LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning", "abstract": "Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation, leaving open the question of whether such reasoning skills generalize to complex real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs’ reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistic constraints. The benchmark covers carefully crafted queries of varying\ndifficulty levels and is supported by a sandbox environment with in-house tools for constraint-based location search. Automated verification further guarantees the scalability of the benchmark, enabling the addition of arbitrary number of queries. Extensive evaluations on real-world site selection data from Boston, New York, and Tampa reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://anonymous.4open.science/r/LocationReasoner-DC5D.", "tldr": "This paper proposes a real-world site selection benchmark for evaluating LLMs on location reasoning", "keywords": ["LLM", "reasoning", "site selection"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cbe01d4b9eadff4604dec2d80acbe0be9b21dd4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces LocationReasoner, a benchmark for evaluating LLMs on real-world site-selection reasoning. It provides (i) a sandbox with fixed, offline datasets of various cities and a set of “in-house” tools for constraint-based queries; (ii) automated query generation (rule-based and LLM-based) and deterministic verification; and (iii) evaluations across multiple LLM families and agentic workflows (ReAct, Reflexion). On these datasets the average pass rate for various LLMs is quite low and agentic methods often do worse than direct prompting. The authors analyze failure types (logic, edge cases, tool misuse, prompt misread, code errors) and argue that holistic, non-linear planning via direct code generation exceeds step-wise agentic loops."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Timely and unique benchmark and planning domain: Targets a gap between math/code reasoning and practical, multi-constraint decision-making; automated verification at scale is valuable. The urban planning domain is interesting.\n- Clear environment design: Offline sandbox, fixed toolset, and concrete tool taxonomy (loaders, zone, analysis, filters, population) make the task reproducible and interpretable and code is provided\n- Broad model coverage + difficulty controls: Multiple provider families, reasoning vs. non-reasoning variants, and simple/medium/hard splits are aligned with standard practice in the LLM + planning and sequential decision making literature so that is nice; results are consistent across cities with convergence analysis.\n- Useful error analysis: Breaks down failures (logic, edge, tool, prompt, code), gives concrete examples, and quantifies “correction rates” for reasoning models which is nice for Reflexion and React,"}, "weaknesses": {"value": "- External validity / coverage: The geography is limited to three U.S. cities with specific data sources (SafeGraph, OSM, Google Places). It’s unclear how well conclusions generalize to non-U.S., rural, or low-data regions and to domains beyond retail-like POI reasoning. I have provided some Urban Planning datasets in the question section which may be relevant to this.\n- Agentic fairness and scope. Agentic workflows are only tested on GPT-4o, while direct prompting includes multiple model families. This asymmetry makes it hard to conclude that “agentic strategies don’t help” in general rather than “agentic strategies built on this one base model + prompts didn’t help.” Broaden agentic experiments to at least one reasoning-tuned model per family, and report hyperparameters, step limits, memory policies, error-recovery prompts in a way that supports apples-to-apples comparisons.\n- There are more recent agentic strategies such as Self Discover [1] and LATS [2], it would have been interesting to see experiments on those for greater coverage\n- Attribution of the central claim. The paper argues “direct code generation (holistic, non-linear) performs better than ReAct (chunked, linear).” While plausible and supported by the results, the causality is mostly qualitative. Please add controlled ablations ie: same base model, matched temperature/decoding, code gen with enforced step-limits vs. ReAct with global-plan scaffolds; and instrumentation of failure transitions. The section explaining the \n- While the dataset and evaluation harness are useful, the pipeline itself appears to be a composition of existing LLMs and standard agentic patterns (e.g., ReAct/Reflexion) wrapped around tool calls. It is not clear to me what exactly is the novel algorithmic component, planning formalism, or execution/control innovation—so the novelty seems concentrated in the benchmark \n- If the main novelty indeed is the benchmark and the author's claim is that this is an example of a dataset where significant innovation is possible in the designing of new research to fix issues such as over-thinking or excessive tool usage/lack of convergence (as in React), please reposition claims accordingly and emphasize the benchmark’s design principles, reliability, and long-term value - for example providing a discussion or ablation studies to understand why LLMs-even with Reflexion/React-perform so poorly on this specific dataset and what could possibly be done to improve performance\n\n\n[1] https://arxiv.org/abs/2402.03620\n\n[2] https://arxiv.org/pdf/2310.04406"}, "questions": {"value": "- Tool/ground-truth coupling: Can you evaluate an agent that does not have access to the exact toolset used by the verifier (e.g., renamed arguments, perturbed APIs, or a mapping layer) to test reasoning robustness beyond tool-specific affordance learning?\n- There are some other prior datasets for urban planning in the ML literature - for example: UrbanDataLayer: A Unified Data Pipeline\nfor Urban Science [1] and some prior works on LLMs for urban planning [2] [3] [4]. Could the authors speak to how the dataset used in this paper compares to the Urban Science dataset as well as some prior LLM based approaches in urban planning? \n- Why were ReAct/Reflexion run only with GPT-4o? Do results hold with o4-mini, Gemini 2.5, or DeepSeek-R1 as the base? \n- Since spend data spans 2019–2025, would it be possible to clarify whether time-aware splits or year-withheld constraints?\n- Given the limitations the authors have noticed on their datasets across several language models perhaps the following works would be of interest regarding the limitations of popular agentic frameworks as in plan-and-execute frameworks, Interactive reasoning frameworks, self-refinement frameworks: [5] [6] \n\n\n[1] https://proceedings.neurips.cc/paper_files/paper/2024/file/0db7f135f6991e8cec5e516ecc66bfba-Paper-Datasets_and_Benchmarks_Track.pdf\n\n[2] https://arxiv.org/abs/2402.17161\n\n[3] https://www.nature.com/articles/s44284-025-00261-7\n\n[4] https://arxiv.org/abs/2406.13945\n\n[5] https://proceedings.neurips.cc/paper_files/paper/2024/file/fa080fe0f218871faec1d8ba20e491d5-Paper-Conference.pdf\n\n[6] https://arxiv.org/abs/2408.11326"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yWYseyB9DQ", "forum": "PTTmPHS7OE", "replyto": "PTTmPHS7OE", "signatures": ["ICLR.cc/2026/Conference/Submission18432/Reviewer_qSv8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18432/Reviewer_qSv8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761110227452, "cdate": 1761110227452, "tmdate": 1762928128176, "mdate": 1762928128176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new benchmark to evaluate multi-step reasoning capabilities of llms in site selection scenarios. This requires the model to understand geospatial cues, use different tools, think logically and come up with the final answer. The paper contributes to the existing benchmarks for understanding the capabilities and limitations of llms particularly reasoning."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The proposed benchmark is carefully designed to include a diverse range of problems (easy, medium, hard)\n- The benchmark tackles problems around tool use, reasoning chains, agentic pipelines, and structured output - all of which are essential in evaluating model capabilities.\n- The evaluation shows expected results between difference of reasoning and non-reasoning models. \n- The benchmark can be easily extended to include more data points and introduce further complexity.\n- The post analysis of model responses provides a lot of insight into the thought process and problems current llms face in challenging agentic tasks."}, "weaknesses": {"value": "- The dataset only includes a few cities in America. The authors should try to extend it to encompass more geographical diversity and analyze whether llms have any location bias"}, "questions": {"value": "- How many data points are in the benchmark at this time?\n- Could you explain how the error analysis was done and how model responses were classified into the different groups? Was this done manually by going through every response or in some automated way?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cgBiMyNUlu", "forum": "PTTmPHS7OE", "replyto": "PTTmPHS7OE", "signatures": ["ICLR.cc/2026/Conference/Submission18432/Reviewer_SoL7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18432/Reviewer_SoL7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915798912, "cdate": 1761915798912, "tmdate": 1762928127738, "mdate": 1762928127738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LocationReasoner, a benchmark to evaluate LLMs’ reasoning abilities on real-world site selection problems. The motivation is that current reasoning models are mostly tested on math or coding problems, leaving their practical reasoning unclear. The authors create a benchmark with queries of different difficulty levels and provide a sandbox with automated verification using real city data. Results show limited performance: even the best model (OpenAI o4-mini) achieves only 69.99% perfect pass rate. Ablation studies suggest reasoning helps somewhat on medium-difficulty queries, and better prompting gives small gains."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset is practical and scalable. Automated query generation and verification using real city data is reproducible.\n\n2. This paper provides comprehensive evaluation: Multiple LLM families, agentic strategies, and cities are tested, with detailed error analysis.\n\n3. They also provide insightful findings on agentic strategies. For example, they discover that ReAct and Reflexion can hurt performance because of over-reasoning, which is surprising and useful for future research."}, "weaknesses": {"value": "I am not an expert in this domain. Therefore, my concerns are raised based on high-level research perspective instead of specific task perspective.\n\n1. It’s unclear if the poor performance of Agentic methods is due to the base LLM limitations or Agentic workflow design. Showing their reasoning traces and the planner (LLM) output would help clarify this.\n\n2. The fixed 15 in-house tools might restrict performance. It would be good to test more general tools (like Python libraries) and see which failures are due to tools versus reasoning.\n\n3. How do we know “hard” queries are really harder? Some empirical validation, like human expert performance or inter-annotator agreement, would strengthen this.\n\n4. Data leakage concern: Since datasets are public and LLMs may have seen similar data, can the authors check performance on synthetic or fictional locations to rule out contamination?"}, "questions": {"value": "Please see my questions raised in each weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "FGZlyI275I", "forum": "PTTmPHS7OE", "replyto": "PTTmPHS7OE", "signatures": ["ICLR.cc/2026/Conference/Submission18432/Reviewer_2Tbz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18432/Reviewer_2Tbz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927834491, "cdate": 1761927834491, "tmdate": 1762928127257, "mdate": 1762928127257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduced a new benchmark for evaluating LLMs' capability in location selection under various constraints. The authors have conducted extensive evaluations of various types of LLMs with different settings (general/reasoning). Some insightful analysis is also presented to guide future research."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The experimental evaluation in this paper is extensive.\n- The attribute-based analysis is very useful for highlighting the key challenge.\n- The paper is well-written."}, "weaknesses": {"value": "**Some related works need to be discussed more extensively**\n\nIn general, I think the problem of site selection is a constraint satisfaction problem, and there have been benchmarks for such kinds of problems, for example, travel planning [1] and scene/agent-intention understanding [2]. I wonder what makes site selection especially challenging compared to a broader constraint-satisfaction benchmark literature? The authors should discuss these more extensively in the related works section.\n\n**Is this benchmark not challenging enough?**\n\nThe extensive evaluation in this paper is well-acknowledged. But I notice that even GPT4o has already achieved 37% success in hard mode, it is a relatively old model in the fast development of LLMs, how about GPT5? I am a bit concerned that the difficulty of this benchmark is not high enough that it might be saturated soon (is it easy to create harder problems using the mentioned data generation pipeline?). Also, what's the human performance on this benchmark? Providing that will better help readers understand the current limitations of LLMs.\n\n[1] Ju, Da, et al. \"To the globe (ttg): Towards language-driven guaranteed travel planning.\" arXiv preprint arXiv:2410.16456 (2024).\n[2] Li, Bowen, et al. \"LogiCity: Advancing neuro-symbolic ai with abstract urban simulation.\" Advances in Neural Information Processing Systems 37 (2024): 69840-69864."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J6YCkt2mp2", "forum": "PTTmPHS7OE", "replyto": "PTTmPHS7OE", "signatures": ["ICLR.cc/2026/Conference/Submission18432/Reviewer_KPTZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18432/Reviewer_KPTZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964397444, "cdate": 1761964397444, "tmdate": 1762928126854, "mdate": 1762928126854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a benchmark for evaluating how LLMs can reason over code on geospatial tasks, specifically for location and site selection given some specific constraints. The LLM is also given access to a library of functions/tools that it can use to generate the final code and the solution. The paper integrates multiple interesting datasets such as SafeGraph (which has information about points of interest, parking facilities, and consumer spending patterns), Google Places API which has information about demographics and population, and transportation data from OpenStreetMap. They also build a sandbox environment with a library of functions that can load, analyze, and filter the data in different ways. They additionally employ LLMs to construct the location selection queries, based on some set of logical constraints and then have the LLM phrase those queries in natural language. The actual task would then be to go from natural language to perhaps rediscover the logical constraints and produce the solution by generating and executing code using the given library. They classify the query as simple, medium or hard based on the number of constraints. \nThe paper created 316 queries and evaluated the performance of several frontier LLMs on this task with and without \"thinking\"/\"reasoning\" enables, and also evaluate if prompt engineering frameworks like React or Reflexion are helpful. The best performance (perfect pass rate) was by GPT-o4-mini which got ~70%."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The task of Location selection specifically framed to generate and execute code given a set of library functions seems like a good reasoning test bed.\n* The datasets integrated to create the task are rich and include a lot of diverse information such as parking facilities, consumer spending patterns (from SafeGraph), information about demographics and population (from Google Places API), and transportation data (from OpenStreetMap).\n* The experimental set up and evaluations are standard and straightforward.\n* The error analysis is reasonable. Some more constructive ways of improvement would have been nicer to see."}, "weaknesses": {"value": "1. While it is possible to understand the high level idea, the paper is lacking in clarity and examples to clearly describe the different contributions. Specifically,\n\n  1a. There is no example the fully illustrates the input query, the expected final solution and an example of the code that would generate the final solution. In particular, **lacking the outputs and expected code solution** (even in the supplement) makes it hard to get a feel for the complexity of the task. Section 2.2, 6, and A3 all give glimpses of the task but not quite a full example.\n\n2. **Query generation lacks clarity** Section 2.2 on Query design, does not clearly describe the details of how the queries were actually generated, and how the solutions were generated and verified. \n\n  2a. What are some examples of the rule-based generation? What is the algorithm that was used? \n\n  2b. What were the prompts for the LLM-based generation? What specific steps were involved in the LLM-based generation. Here again examples would be immensely helpful.  \n\n  2c. Are the rule-based queries substantially different from LLM-generated queries? In what way? \n\n  2d. How is the model performance when sliced by how the queries were generated, is there a difference? \n\n  2e. Which LLM was used for LLM-generated queries? (also how and why did you choose that LLM model to do the generation?) \n\n  2f. Also, how did you generate the ground truth solutions/code? Was there any human annotation? How were the ground truth solutions verified for correctness?\n\n3. **Missing details on characteristics of the problem**. The evaluation appears to be on 316 generated queries (and solutions) on 3 cities. Section 4 argues that 150-200 queries are sufficient to test robustness of performance for a city. Is there something about the location selection problem that makes it different for each geographical region being studied? Are there any constraints to where your data generation method (and library) may be applicable? If it is easy to scale this, why not scale the questions? What more would we learn by scaling the questions / what are we missing if we don't scale? \n\n4. Another major weakness is **lack of comparison to other geospatial code generation tasks and benchmarks**, and how this task and tooling is different and what this benchmark tests that is different from others geospatial code generation benchmarks. The paper references a range of generic reasoning and LLM+Agents related papers, but it seems to be missing more closely relevant and related works, Here are some geo code benchmarks I am familiar with:\n\n[1] GeoCode Eval, GeoCode-GPT: A large language model for geospatial code generation\n\n[2] Geollm-engine: A realistic environment for building geospatial copilots\n\n[3] Multi-Agent Geospatial Copilots for Remote Sensing Workflows\n\n[4] The Cloud-Based Geospatial Benchmark: Challenges and LLM Evaluation\n\n[5] Evaluation of code llms on geospatial code generation\n\n[6] An llm agent for automatic geospatial data analysis\n\n[7] GIS copilot: Towards an autonomous GIS agent for spatial analysis\n\n\n* Overall it seems the paper has some interesting contributions, but it's not clear how substantial these are. The lack of clarity and detailed information highlighted in weaknesses 1-4 make it difficult to determine the contribution have resulted low scores on presentation and contribution."}, "questions": {"value": "* Please address questions under weaknesses\n\n* Are there other domain related characterizations of the dataset (beyond the simple, medium, hard classification based on the complexity of the constraints)? E.g. a couple of the queries in section 2.2. pertained to \"restaurant\" site selection. If there are such domain related characterizations, based on the characterization and the query are there more domain specific nuances / \"knowledge\" that the model would have to know to apply the right constraints? -- E.g. in residential areas some zoning laws may apply whereas for commercial land some other laws might apply. Could you capture such information in the dataset description?\n\n**Other comments**\n\n* Presentation of results in Table 2 can be improved. It is very difficult to parse and identify which models are performing well and where. Perhaps just have a much smaller table of just the overall performance and move the details to the supplement or consider generating plots."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hfJCxazxur", "forum": "PTTmPHS7OE", "replyto": "PTTmPHS7OE", "signatures": ["ICLR.cc/2026/Conference/Submission18432/Reviewer_P7C5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18432/Reviewer_P7C5"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988000279, "cdate": 1761988000279, "tmdate": 1762928126169, "mdate": 1762928126169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}