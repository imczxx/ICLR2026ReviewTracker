{"id": "4NwlXFmbRe", "number": 8098, "cdate": 1758062804412, "mdate": 1763771884603, "content": {"title": "When Do Transformers Learn Heuristics for Graph Connectivity?", "abstract": "Transformers often fail to learn generalizable algorithms, instead relying on brittle heuristics. Using graph connectivity as a testbed, we explain this phenomenon both theoretically and empirically. We consider a simplified Transformer architecture, the disentangled Transformer, and prove that an $L$-layer model has capacity to solve for graphs with diameters up to exactly $3^L$, implementing an algorithm equivalent to computing powers of the adjacency matrix. We analyze the training-dynamics, and show that the learned strategy hinges on whether most training instances are within this model capacity. Within-capacity graphs (diameter $\\leq 3^L$) drive the learning of a correct algorithmic solution while beyond-capacity graphs drive the learning of a simple heuristic based on node degrees. Finally, we empirically demonstrate that restricting training data within a model's capacity leads to both standard and disentangled transformers learning the exact algorithm rather than the degree-based heuristic.", "tldr": "", "keywords": ["transformer", "graph connectivity", "learning theory", "training dynamics"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2da9a993c685d19c29f40c86b92a52046e5a65cf.pdf", "supplementary_material": "/attachment/05e2e9bfa5a595138751ca979fce7d02a0aee8aa.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes the capacity of transformers to generalize out of distribution on a toy task. Specifically, the paper analyzes the problem of predicting the connectivity matrix of a graph by supervising a transformer on a training set with ground-truth solutions. The paper aims to explain when the transformer learns an actual algorithm for solving this task, which can hence be transferred to any test distribution, and when the learned solution is overfitted to the training distribution. The analysis is based on a specialized transformer architecture, tailored to this task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Analyzing the capabilities of neural networks through toy tasks is a valid approach, often providing intuition that extends to more general cases. The paper is well-written, and the theoretical derivations appear to be sound."}, "weaknesses": {"value": "Some of the main results in this paper are interpreted in a misleading way. Specifically, one of the main contributions that the authors claim to have made is: \n\n**If the capacity is not exceeded in the training set, then training will lead the transformer to learn the right algorithmic solution of the problem.**\n\nThis is claimed often in the paper. However, when reading the actual theoretical construction, what was proven is much weaker. An appropriate interpretation of what is proven is: \n\n**The analysis suggests that if the transformer already implements the algorithmic solution, then training will not change this.**\n\nThe authors should delete all of the instances of the misleading claim from their paper, and instead write the more appropriate and modest interpretation of their theoretical contribution.\n\nIn more details:\n\nThe assumptions in Section 4.3 beat the whole point of this section. You want to show that if the capacity is not exceeded, then the training will lead the transformer to learn the right algorithmic solution of the problem, namely, to implement powers of the adjacency matrix. However, instead of proving this, in lines 276 – 278 you basically ASSUME that the transformer implements powers of the adjacency. More accurately, you assume something as strong, namely, that the transformer predicts the support of the powers of the adjacency, which is what you need to implement the algorithmic solution. \n\nThen, starting from line 297, the authors actually assume that the transformer exactly implements the algorithmic solution. It is assumed that one channel of the transformer implements the algorithm, and another channel does not. The analysis only shows that for data under the capacity, the channel with the algorithmic solution dominates. However, this is a very weak result. The actual question that the authors set out to answer is whether the transformer can **learn** the algorithmic solution. This question is not answered by the analysis.  Instead, the analysis shows that under strict conditions, if the transformer already implements the algorithmic solution, training will not ruin this. Hence, the analysis in this section is interpreted in a misleading way. The authors should not claim to have solved the aforementioned problem (proving that below the capacity, transformers learn the algorithmic solution).  The authors should delete any such claim from the whole paper. For example, line 475–478 should be deleted. Instead, the authors should write that their analysis suggests that if the transformer already implements the algorithmic solution, then training will not change this.\n\nAnother point is that the paper does not discuss what the special toy task can teach us about the capacity of general transformers on general tasks to avoid overfitting by learning heuristics.\n\n\nMinor comment:\n\nLine 662: change the index h to another notation, as h already represents the features."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UH5IDO9xJD", "forum": "4NwlXFmbRe", "replyto": "4NwlXFmbRe", "signatures": ["ICLR.cc/2026/Conference/Submission8098/Reviewer_E2Xj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8098/Reviewer_E2Xj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760873390597, "cdate": 1760873390597, "tmdate": 1762920082457, "mdate": 1762920082457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for their constructive feedback. To clarify the scope and novelty of our work, we summarize our central narrative below:\n\n### __1. Motivation: Existence/Expressivity is not Learning__\n\nPrior work (e.g., Merrill & Sabharwal) established that log-depth Transformers can express the connectivity algorithm. We address a fundamentally different question: \n\n__Even if the model is expressive enough to solve the task, why does it often fail to learn the algorithm?__\n\n*   Definition: We define \"learning an algorithm\" as acquiring a mechanism (matrix powering) that generalizes to Out-of-Distribution (OOD) graphs (e.g., 2-Chain and 2-Cliques) where heuristics fail.\n*   Phenomenon: Standard training achieves perfect in-distribution accuracy but 0% OOD accuracy (Figure 1), proving that transformer models trained with gradient descent prefer brittle heuristics over the robust algorithm. Our goal is to mechanically explain and fix this preference.\n\n### __2. Why the Exact $3^L$ Bound is Necessary__\n\nTo explain this failure, we derive an exact capacity limit, not just an asymptotic bound.\n*   Theorems 4.3 (Expressivity, lower bound) & 4.4 (Capacity, upper bound): We prove an $L$-layer model can solve graphs with diameter $\\le 3^L$, but must fail on graphs with diameter $> 3^L$.\n*   Why this is important: This exact threshold allows us to define a rigorous dichotomy (Definition 4.5): partitioning training data into within-capacity and beyond-capacity sets. Without the exact $3^L$ boundary, the subsequent dynamics analysis would be impossible.\n\n### __3. From Capacity to Training Dynamics__\n\nWe link this capacity limit directly to gradient descent behavior using a rigorous weight decomposition:\n*   Theorem 4.6 (Two Channels): Weights split into an Algorithmic Channel ($A \\otimes I$, matrix powering) and a Heuristic Channel ($B \\otimes J$, degree shortcuts). The only assumption we made for this Theorem is that the model should be permutation equivariant – a common assumption for permutation equivariant structures such as graphs. \n*   Theorems C.5 & C.9 (Dynamics): We prove a phase transition in the gradients:\n    *   Within-Capacity data ($\\le 3^L$) drives gradients toward the Algorithmic channel.\n    *   Beyond-Capacity data ($> 3^L$) actively drives gradients toward the Heuristic channel.\n\n### __4. Prescriptive Power__\n\nThis theory prescribes the \"Data Lever\": filtering data by diameter $\\le 3^L$ suppresses heuristics. Figure 6 validates that this theoretical insight successfully transfers to Standard Transformers, enabling perfect OOD generalization. We also would like to empathize this prescriptive power is only given if we have an exact bound $3^L$ to define a dichotomy (Definition 4.5), where asymptomatic tight bounds given by Merrill & Sabharwal couldn't. \n\n*William Merrill and Ashish Sabharwal. A little depth goes a long way: The expressive power of log-depth transformers.*"}}, "id": "dW2ndAVjqw", "forum": "4NwlXFmbRe", "replyto": "4NwlXFmbRe", "signatures": ["ICLR.cc/2026/Conference/Submission8098/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8098/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8098/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763710777218, "cdate": 1763710777218, "tmdate": 1763711738569, "mdate": 1763711738569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a study to detect whether (disentangled) transformers learn algorithms rather than shortcuts (i.e. brittle heuristics vs. correct algorithmic solutions). They design specific stress tests based on graph connectivity, because it is a fundamental algorithmic problem which admits simple heuristics and has a complexity landscape tied to Transformers depth.\n\nThe authors \n(i) prove that a $L$-layer model can solve graphs with diameter $\\leq 3L$ by effectively computing powers of the adjacency matrix; beyond this, it must fail on some instances.\n(ii) They characterize training dynamics via a *two-channel decomposition* (algorithmic ($A \\otimes I$) vs. heuristic ($B \\otimes J$), the latter encoding degree-based shortcuts), showing which channel wins depends on the *fraction of beyond-capacity graphs* in the training distribution. \n(iii) They propose a data lever training that suppresses heuristics and yields improved OOD generalization. They show that the effect *transfers to standard Transformers* as well as the disentangled model.\n\nExperiments on synthetic ER graphs and OOD \"two-chains/cliques\" support the theory."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "* The theoretical results are clear and well-defined. The *expressivity theorem* and *tight $3L$ capacity* (Theorems 4.3-4.4) present their main points clearly, including important assumptions like self-loop augmentation, depth L, and non-negativity of the capacity upper limit. These results come with proof sketches and detailed appendices.\n\n* The *layerwise permutation-equivariant parameterization* (Theorem 4.6) provides a clear method for understanding the difference between *algorithmic* and *heuristic* channels, which helps analyze *gradient dynamics* (Theorems C.5/C.9).\n\n* Figures from the experiments support three main findings: (i) the *$3L$ capacity breakpoint*, (ii) the *two-phase training behavior*, and (iii) the *data-lever mitigation*. They also show the *transferability* to standard Transformers. Although some assumptions, such as *non-negativity* and *equivariance form*, are idealized, the authors talk about these issues directly. The predictions they make also align well with the results from experiments conducted *without* enforcing those constraints. Overall, the claims are *well-supported* and consistent with each other.\n\n* The *data lever* is simple (restrict to *within-capacity* diameter) yet *effective*; it *transfers* to standard Transformers and improves OOD generalization on 'two-chains/cliques' \n\n* Careful delineation of assumptions; synthetic testbed isolates the phenomenon; reproducibility statement and code availability are noted.\n\nIn sum, the work offers a novel law for connectivity (diameter $\\leq 3L$), a mechanistic decomposition of training into algorithmic vs. heuristic channels with predictive training-dynamics theory, and a *simple, effective data intervention that enhances OOD generalization even for standard Transformers. Given the community's interest in *algorithmic reasoning*, *shortcut learning*, and *mechanistic interpretability*, these contributions are *timely* and *impactful* for ICLR."}, "weaknesses": {"value": "* The paper is well-structured overall, but the narrative is dense; adding a short executive overview at the end of the Introduction (5-6 sentences: problem -> $3L$ capacity result -> why tight -> training-dynamics story -> data lever -> empirical scope) would help orient readers. In addition, I suggest collecting all assumptions (self-loops, non-negativity, permutation-equivariance, depth L, task definition) in a single callout box with 1-line justifications and pointers to where they're relaxed/validated would increase the clarity and the scope of the author's contributions. I'd also suggest considering a simple running example (small 6-8 node graph) reused across sections to illustrate: (i) layer reach vs. diameter, (ii) algorithmic vs. heuristic channels, (iii) effect of the data lever.\n* Experiments rely on Erdős-Rényi graphs and a couple of OOD structures (two chains/cliques). Additional graph families (grids, small-world, scale-free, trees, expanders, barbell/lollipop) and directed/weighted variants would better probe the generality of the theory and the data lever.\n* In realistic settings, one may *not know capacity* (e.g., mixed tasks, unknown diameters) or be unable to *filter by diameter*. A discussion/experiment on *approximate proxies* (e.g., curriculum by *graph diameter estimates*, path-length sampling, synthetic augmentation) would make the prescription more deployable.\n*  Since connectivity is a classic *message-passing* problem, comparisons to *GNNs* (e.g., MPNNs with sufficient layers), *RNNs*, or *explicit DP modules* would contextualize the Transformer's behavior and whether the data lever similarly helps/hurts those models.\n* Limited analysis of *n-scaling* and sensitivity to *p* in ER(n,p); more *systematic sweeps* could test how the within/beyond-capacity mixture threshold ( $\\rho^\\star$ ) moves with size/density and training budget."}, "questions": {"value": "1. How sensitive is the *$3L$* upper bound to the *non-negativity* requirement? Do you observe empirical counterexamples to the $3L$ law when weights are unconstrained and trained longer/larger? \n2. If one cannot compute diameters exactly at scale, which *practical proxies* of the diameter (e.g., layered BFS samples, effective resistance bounds, eccentricity estimates) suffice to approximate the within-capacity filter? Any experiments with *noisy diameter filters*? \n3. Would the *two-channel* story extend to other algorithmic tasks (e.g., *shortest paths/reachability variations*, *cycle detection*, *parity/motif counting*)? Are there analogous *rank-1 broadcast* heuristic channels for these problems? \n4. How does performance change on *scale-free* and *small-world* graphs where *degree distributions* or *clustering* differ markedly from ER? Does the heuristic channel bias shift? \n\nI'm happy to revise my score upward if the authors (1) extend experiments to additional graph families and directed/weighted settings, (2) include a curriculum/proxy version of the data lever, and (3) add comparative baselines (e.g., MPNNs, GNNs) and a more systematic scaling analysis. Nonetheless, the work is solid and insightful, and I recommend it for acceptance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "juwPMrxNgs", "forum": "4NwlXFmbRe", "replyto": "4NwlXFmbRe", "signatures": ["ICLR.cc/2026/Conference/Submission8098/Reviewer_1s5u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8098/Reviewer_1s5u"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761564102378, "cdate": 1761564102378, "tmdate": 1762920082033, "mdate": 1762920082033, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the computational capabilities of Transformers without  CoTs. Specifically, the authors focus on the problem of graph connectivity, and analyze whether Transformer can learn to decide connectivity properties. They prove that constant-depth Transformers fail to solve connectivity, while log-depth Transformers can succeed. The paper also provides empirical evidence supporting these theoretical separations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Though [1] has already proven that  log-depth Transformers can solves graph connectivity, this paper gives a more fine-grained analysis. \n- The paper is clearly written overall.\n- The authors have conducted experiments to support the theoretical findings.\n\n[1] William Merrill and Ashish Sabharwal. A little depth goes a long way: The expressive power of log-depth transformers."}, "weaknesses": {"value": "1. The paper investigates the computational power of Transformers **without CoT**. However, as the authors themselves mentioned, it is already well known that Transformers without CoT are severely limited in expressive power. Moreover, these limitations can be easily removed by using CoTs: theoretically, Transformers with CoTs are known to be Turing complete, e.g. [2,3]; empirically, almost all practical Transformers make extensive use of CoTs. Therefore, the question explored here feels of limited relevance to either current practice or deeper theoretical understanding.\n\n2. The paper focuses on a single specific task, namely graph connectivity. Prior works [1] have already shown that log-depth Transformers can solve graph connectivity. As such, the contribution mainly confirms previously understood phenomena rather than discovering fundamentally new behavior. It is not clear whether the insights here can be generalized to broader classes of graph problems. This makes the work more of a case study than a general theoretical advance.\n\n[2] William Merrill and Ashish Sabharwal. The Expressive Power of Transformers with Chain of Thought.\n\n[3] Qian Li and Yuyi Wang. Constant bit-size transformers are Turing complete."}, "questions": {"value": "Can the analysis be extended to provide insights into whether current training procedures lead Transformers (with CoT) to simulate broader classes of algorithms?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OHIvtYR0XM", "forum": "4NwlXFmbRe", "replyto": "4NwlXFmbRe", "signatures": ["ICLR.cc/2026/Conference/Submission8098/Reviewer_VnQA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8098/Reviewer_VnQA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718653000, "cdate": 1761718653000, "tmdate": 1762920081696, "mdate": 1762920081696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates why Transformers  fail to learn true algorithmic reasoning and instead rely on unstable heuristics. Authors focus on  the graph connectivity task as a testbed, and introduce a simplified model called the disentangled Transformer and prove that an L-layer version can solve graph connectivity for graphs with diameters up to $3^L$ -- by effectively computing powers of the adjacency matrix.  When training data mostly include graphs within this “capacity”, the model learns the correct algorithm. However, when exposed to larger graphs beyond its capacity, it defaults to a heuristic based on node degrees. To validate this, authors provide experiments with data that is within the capacity of the model (with specific adjustments to the model) and observe that the true algorithm is emulated successfully."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Theoretically explaining the algorithmic alignment capacity of Transformers is interesting and important.\n- A tight bound for the *capacity* of Transformers for the graph connectivity problem.\n- Experiments are carefully designed and validate the presented theory.\n- The paper is generally well-written: it is easy to read and follow."}, "weaknesses": {"value": "- The scope of the contribution is very limited: the result applies only to graph connectivity and to the disentangled Transformer architecture. Of course, graph connectivity may serve as a good task setting for testing algorithmic alignment, but eventually we are interested in designing better architectures that generalize out of distribution, for a variety of task settings. Clearly we cannot hope to cover every task, and every domain, but the current scope is too limited in my opinion. \n- The technical construction proving $3^L$ capacity is straightforward, given the connection with the transitive closure. That being said, showing that this is also a matching lower bound requires a careful construction.\n- Experimental setup is clearly tailored to the specific task setting, which again makes sense in the context of this paper, but it keeps us in this extremely limited scope.\n- Authors seem to suggest that \"expressivity\" and the \"model capacity\" are two different things. This needs to be toned carefully in my opinion. In the end, these are all expressivity results, but they differ in the kinds of assumptions they make. Many of these expressivity results (e.g., universality results) are non-uniform and require to show: for any input, there exists a model parametrization that realizes the target function (i.e., we are allowed to change the parametrization based on the input size). This is clearly weaker than uniform results which require the existence of one model parametrization that captures the target function on any input. The setup authors study is somewhere in-between, because it is essentially saying that, there exists a  a model parametrization that captures the target function on any input with diameter $\\leq3^L$. This is all to say that a more delicate discussion is needed when it comes to expressivity. It is clear that uniform expressivity typically correlates with algorithmic generalization whereas non-uniform results do not inform us much (and similar for asymptotic ones)."}, "questions": {"value": "- Is there a way of generalizing the scope of this study to capture a broader class of functions? The bounds may be different for each function of course, but the theorems could be generally applicable to derive bounds -- based on the diameter/size of the graphs and the nature of the functions considered."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3VNHfRSEb3", "forum": "4NwlXFmbRe", "replyto": "4NwlXFmbRe", "signatures": ["ICLR.cc/2026/Conference/Submission8098/Reviewer_7GWi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8098/Reviewer_7GWi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8098/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929696187, "cdate": 1761929696187, "tmdate": 1762920081220, "mdate": 1762920081220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}