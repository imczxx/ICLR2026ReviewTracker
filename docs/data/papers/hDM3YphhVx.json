{"id": "hDM3YphhVx", "number": 1931, "cdate": 1756968135123, "mdate": 1759898177800, "content": {"title": "Advancing Complex Video Object Segmentation via Progressive Concept Construction", "abstract": "We propose Segment Concept (SeC), a concept-driven video object segmentation (VOS) framework that shifts from conventional feature matching to the progressive construction and utilization of high-level, object-centric representations. SeC employs Large Vision-Language Models (LVLMs) to integrate visual cues across diverse frames, constructing robust conceptual priors. To balance semantic reasoning with computational overhead, SeC forwards the LVLMs only when a new scene appears, injecting concept-level features at those points.\nTo rigorously assess VOS methods in scenarios demanding high-level conceptual reasoning and robust semantic understanding, we introduce the Semantic Complex Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160 manually annotated multi-scenario videos designed to challenge models with substantial appearance variations and dynamic scene transformations. Empirical evaluations demonstrate that SeC substantially outperforms state-of-the-art approaches, including SAM 2 and its advanced variants, on both SeCVOS and standard VOS benchmarks. In particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS, establishing a new state-of-the-art in concept-aware VOS.", "tldr": "", "keywords": ["SAM2", "LVLM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/efc5939f197a3fd20192ece1bcd8048e3aab23ed.pdf", "supplementary_material": "/attachment/fa3c95a41695c3deb3e2035c69121c8ad9c5f9de.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Segment Concept (SeC), a concept-driven framework for video object segmentation (VOS) that shifts from traditional appearance-based matching to progressive construction of object-level concepts using Large Vision-Language Models (LVLMs). The method selectively activates LVLM-based concept reasoning only when significant scene changes are detected, balancing semantic understanding with computational efficiency. Additionally, the authors introduce SeCVOS, a new benchmark designed to evaluate VOS models in semantically complex, multi-shot scenarios. Experiments show that SeC outperforms state-of-the-art methods, including SAM 2 and its variants, on both SeCVOS and standard VOS benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Benchmark Contribution (SeCVOS ).\n2. Rigorous Evaluation and Ablation. The paper includes extensive experiments, ablation studies, and qualitative analyses. The comparison with a wide range of methods and the inclusion of failure cases (Fig. 5) enhance credibility.\n3. Good performance. The results are consistent across multiple datasets and settings, demonstrating robustness and generalizability."}, "weaknesses": {"value": "1. Limited Comparison with LVLM-Based VOS Methods:\nWhile the paper compares with many traditional and SAM-based methods, it lacks a thorough comparison with recent LVLM-based VOS approaches (e.g., VISA, GLUS, VideoLISA). Table 8 in the appendix is a start, but it should be integrated into the main paper to better position SeC in the LVLM-VOS landscape.\n2. Scene Change Detection Simplicity: \nThe scene change detector is based on a simple HSV histogram difference. This may not be robust to semantically significant but visually subtle changes. A learning-based or more sophisticated detector could be more reliable.\n3. Computational Overhead: \nAlthough the authors emphasize efficiency, the use of LVLMs (even sparsely) still introduces significant computational cost. A more detailed analysis of latency, memory usage, and comparison with other LVLM-based methods would be helpful.\n4. Lack of User Study or Qualitative Justification for \"Concept\": \nThe term \"concept\" is central to the paper but remains somewhat abstract. A user study or qualitative analysis showing how the learned concept tokens align with human-understandable semantics would strengthen the claim.\n5. Lack of User Study or Qualitative Justification for \"Concept\": \nThe term \"concept\" is central to the paper but remains somewhat abstract. A user study or qualitative analysis showing how the learned concept tokens align with human-understandable semantics would strengthen the claim."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Hq7w5DQ9Qm", "forum": "hDM3YphhVx", "replyto": "hDM3YphhVx", "signatures": ["ICLR.cc/2026/Conference/Submission1931/Reviewer_V92j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1931/Reviewer_V92j"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833738514, "cdate": 1761833738514, "tmdate": 1762915960079, "mdate": 1762915960079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a SAM2 based methods for long term video object segmentation. To handle the underlying occusion and  object re-detection, the authors employ LVLMs to extract the high-level semantic information about the  segmentation target and realize the consistent tracking.  The authors evaluate the proposed method on several public VOS datasets as well as the self-collected SeCVOS dataset."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The statement is clear, and the figures are in good illustration."}, "weaknesses": {"value": "1. Novelty is a big issue. Token-level video summarization has been widely exploited for \nlong-term video understanding tasks, such as [1,2,3].\n[1]Online Video Understanding: A Comprehensive Benchmark and Memory-Augmented Method\n[2] InternVideo2.5: Empowering Video MLLMs with Long and Rich Context Modeling.\n[3]  Streaming Long Video Understanding with Large Language Models, neurips\n\n2. Fairness about the selected dataset called SeCVOS benchmark. This dataset is small with only 160 manually videos. \nAlso, compared to the existing vos dataset LVOS, the duration is shorter than LVOS. I think the authors can test the proposed methods on large-scale tracking datasets, such as TrackingNet and SportsMOT.\n\n\n3. In the experimental section, the compared methods are all before 2025. Many recent counterparts are not compared. Also, the performance advantage over these old methods is slight. \n\n4. The method section is too simple and straightforward. \n\n\nOverall, the contribution of the whole paper is weak."}, "questions": {"value": "The computation burden should be analyzed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Lw6J6BQhFK", "forum": "hDM3YphhVx", "replyto": "hDM3YphhVx", "signatures": ["ICLR.cc/2026/Conference/Submission1931/Reviewer_tKVK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1931/Reviewer_tKVK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834426978, "cdate": 1761834426978, "tmdate": 1762915959935, "mdate": 1762915959935, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Segment Concept, a new method for video object segmentation. Instead of only matching visual features, SeC utilizes large vision-language models to comprehend objects at a higher, conceptual level. This helps the model track objects even when scenes or appearances undergo significant changes. The authors also developed a new benchmark, called SeCVOS, which utilizes complex, multi-scene videos to test this ability. Experiments show that SeC performs significantly better than previous models, such as SAM 2, achieving an 11.8% improvement on SeCVOS."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper introduces a creative method that uses large vision-language models to understand objects by concepts instead of only appearances, showing strong technical quality. \n- It is clearly written and well-tested, and the new SeCVOS dataset plus strong results make it important for advancing video object segmentation research."}, "weaknesses": {"value": "When the video is too long, the memory bank fills up rapidly, which increases the computational cost. Currently, it uses a FIFO method to limit the buffer size. It lacks a clear strategy for summarizing or compressing the memory bank. Exploring efficient memory summarization could further enhance scalability for long videos."}, "questions": {"value": "Does the proposed method generalize well to real-world videos beyond the benchmark datasets, such as those with complex motion, occlusion, or lighting variations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pwerDxYFQP", "forum": "hDM3YphhVx", "replyto": "hDM3YphhVx", "signatures": ["ICLR.cc/2026/Conference/Submission1931/Reviewer_MC7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1931/Reviewer_MC7A"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897633077, "cdate": 1761897633077, "tmdate": 1762915959802, "mdate": 1762915959802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Segment Concept (SeC), a concept-driven video object segmentation framework that shifts from low-level appearance matching to high-level semantic reasoning. SeC leverages LVLMs to construct and refine object-level concepts by integrating information across keyframes over time. A learnable concept token distills the semantic essence of the target, which is then injected into the segmentation network via cross-attention to guide concept-aware predictions. To balance robustness and efficiency, SeC introduces a scene-adaptive activation strategy, invoking LVLM reasoning only under complex visual changes while using lightweight matching for stable scenes. Furthermore, the authors curate a new benchmark, SeCVOS, featuring complex multi-shot videos to evaluate semantic reasoning in video segmentation, where SeC outperforms existing memory-based models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper leverages LVLM-derived (InternVL-2.5) object-level embeddings for video object segmentation task. It combines low-level visual similarity and high-level semantic similarity to improve segmentation performance on challenging cases.\n2. This paper proposes a SeCVOS Benchmark, designed for semantic complex scenarios.\n3. Keeping only the transition frames in the concept memory bank is both efficient and effective, as it focuses on the most informative moments of semantic change while reducing redundant computation. \n4. The discussion section is promising, including two significant parts: progressive object-level representation and frequency of concept guidance."}, "weaknesses": {"value": "1. ***Table 5:*** Model parameters and the inference time in Table 5 should be included for clear and fair comparison.\n\n2.  In the qualitative analysis, the results of **SAMURAI** and **SAM2-Long** should also be included in Fig. 5 of the main paper and figures/videos in Supple materials.\n\n3. Some failure cases should be included in Sec. E of supple. materials to better illustrate the limitations of the proposed method and provide insights into potential areas for improvement."}, "questions": {"value": "1. This paper focuses on the Video Object Segmentation (VOS) task, and Table 5 compares methods specifically designed for VOS. However, several recent unified video segmentation approaches [1-3] have also demonstrated strong performance on VOS tasks. \n\n***Ref:***\n\n[1] TarVIS. Athar, Ali, et al. \"Tarvis: A unified approach for target-based video segmentation.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[2] UNINEXT. Yan, Bin, et al. \"Universal instance perception as object discovery and retrieval.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.\n\n[3] UniVS. Li, Minghan, et al. \"Univs: Unified and universal video segmentation with prompts as queries.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Please carefully check all videos (like 3qlRJLGGZy0.mp4) in the proposed benchmark. For any shots involving faces, ensure they comply with the relevant release and privacy requirements."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g0946IBOtO", "forum": "hDM3YphhVx", "replyto": "hDM3YphhVx", "signatures": ["ICLR.cc/2026/Conference/Submission1931/Reviewer_PtAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1931/Reviewer_PtAu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1931/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930716655, "cdate": 1761930716655, "tmdate": 1762915959457, "mdate": 1762915959457, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}