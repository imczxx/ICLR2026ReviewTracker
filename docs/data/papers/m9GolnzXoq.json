{"id": "m9GolnzXoq", "number": 24503, "cdate": 1758357459124, "mdate": 1759896762692, "content": {"title": "From sequences to schemas: How recurrent neural networks learn temporal abstractions", "abstract": "A fundamental challenge in neuroscience is to understand how neural systems extract and represent abstract structure from complex, time-varying input. From language and music to action planning and sensory prediction, behavior relies on the ability to recognize relational patterns in sequences. Yet it remains unclear how such abstract temporal schemas are learned and encoded in neural population dynamics. Here, we show that training recurrent neural networks (RNNs) on an abstract sequence classification task drives the emergence of internal representations that express the underlying hierarchical structure and are supported by low-rank recurrent dynamics, whereas training on a standard next-token prediction task does not. Using sequences generated by a binary branching tree to instantiate abstract structure, we trained RNNs to classify sequences based on their abstract class (e.g., aab, aad → AAB; aba, aca → ABA), providing a label only at the end of the sequence. Despite the absence of explicit supervision at the transition level, the networks developed low-dimensional, linearly separable internal representations, reflecting the underlying hierarchical tree structure of the data, and encoding information about the sequence's path through this structure. This enables generalization across different token instantiations of the same abstract pattern. In contrast, RNNs trained on a next-token prediction task fail to form such organized dynamics or recover the underlying tree structure. However, through transfer learning, we show that when initialized with weights from a classification-trained network, prediction models learn faster and generalize better. These findings demonstrate that task objectives critically shape internal representations and that abstract structure, once learned, can serve as a reusable scaffold for diverse temporal computations.", "tldr": "", "keywords": ["statistical learning", "abstract sequential pattern", "generalisation", "abstraction", "classification", "prediction", "transfer learning", "RNN", "mechanistic interpretability"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/29a82a7a819df017cc1e3e672ebafcc588ce5ed6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies how RNNs learn and represent abstract temporal schemas from a sequence of time-varying inputs. By training RNNs on a sequence classification with binary tree structure, the authors found low-dimensional, linearly separable population dynamics and low-dimensional weights that support generalization to other sequences and tasks. The authors also show an interesting comparison with RNNs trained on the next-token prediction task, which didn’t display tree-structured neural activity as the sequence classification-trained RNNs do."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I’d like to start by thanking the authors for their thoughtful and well-executed work. This paper investigates how RNNs learn structured neural dynamics and connectivity patterns over sequences, a question that highlights the temporal regularities in the task and has been relatively underexplored in the literature. The authors conduct detailed and in-depth analyses of both RNN connectivity and dynamics, and also show how tree-like dynamical structures support generalization at the functional level. This provides a systematic view of how RNNs form temporal abstractions across multiple levels of analysis. I also found the experiment that extracts the dimensionality of low-rank structure in the recurrent weights by comparing the dimensionality of rotated averaged weights to that of the original averaged weights particularly clever. Overall, the analyses are well-motivated, carefully executed, and clearly interpreted. And I believe this work will be relevant for the computational and system neuroscience community."}, "weaknesses": {"value": "1. To make prediction unambiguous the authors prune the tree heavily, which changes the data distribution and possibly the need for abstraction, and the observed lack of low-rank structure in prediction-based RNNs may be an artifact induced by the possibly unbalanced training data distribution. For example, can the authors compare the dimensionality / distribution of the training data for the prediction task, with that of the sequence prediction task?\n2. The binary-tree, fixed-length, two-token-type setting is clean for analysis but narrow. It’s unclear how robust the low-rank abstraction is under (i) probabilistic transition of the tokens, (ii) variable-length sequences, (iii) larger alphabets and more than m=2 latent items. Can the authors comment on it?"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jW6HYIVlzO", "forum": "m9GolnzXoq", "replyto": "m9GolnzXoq", "signatures": ["ICLR.cc/2026/Conference/Submission24503/Reviewer_3JvZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24503/Reviewer_3JvZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760561646610, "cdate": 1760561646610, "tmdate": 1762943106040, "mdate": 1762943106040, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper explores how RNNs learn temporal abstractions from sequences. This work uses synthetic sequence tasks generated from a binary branching hierarchy, and compared models that learn under different (classification vs. next-token prediction) objectives. The results show that RNNs can implicitly learn to encode the hierarchical structure of the sequences in the hidden representation. Further analysis shows that this hierarchical encoding is achieved through low-rank recurrent dynamics in the representations and learned weights."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work goes beyond representation analysis and offers a detailed understanding of how hierarchical representations are supported in the trained model. The rank-constrained baseline experiments are quite interesting.\n- I appreciate that this work explores and compares different training objectives and how they may link to different model behavior.\n- This work has the potential to offer interesting implications for analyzing temporal abstraction in biological networks."}, "weaknesses": {"value": "- Some conclusions seem potentially over-claimed. For example, next-token prediction was linked to a failure in inducing similar representational dynamics, but the next-token task formulation significantly changed the data distribution, which could have greatly reduced the learning pressure to encode hierarchical class structures. The transfer effect from the model weights learned under the classification objective also seems relatively weak from Figure 4C.\n- It seems that there might be some inconsistencies regarding what counts as \"low-rank\" or \"effective compression\". In some cases (e.g. Figure 2a), a dimensionality around 10 is defined to be reduced to low-rank representations over time steps, but other times (e.g. Figure 4d) a dimensionality of a little over 10 is described as weak compression. Results from rank-constrained training also seem to suggest that the truncated weight matrix can result in non-trivial generalization gap, which may suggest that additional dimensions are important for differentiating lower-level (close to leaf) classes.\n- I think this work could be greatly enhanced if there is further understanding of the factors contributing to the effective learning of these hierarchical, low-rank representations -- e.g., whether they develop in tandem/before/after memorized predictions, whether they're seeded by architectural complexity (see below)."}, "questions": {"value": "- What was the motivation for initializing in the lazy learning regime as opposed to the rich learning regime? It might be interesting to compare if a rich learning regime more consistently/quickly leads to a hierarchical representation.\n- What is the size of the hidden layer? Have you tried increasing/decreasing the hidden size and would you expect redundant representational resource (larger size) to potentially promote memorization and inhibit structure learning?\n- I'm curious what exact generalization failures the rank-constrained models exhibit -- these errors seem like potentially really interesting to understand if the leading dimensions are preserving more high-level class differences as opposed to later dimensions, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oK7G8IirLl", "forum": "m9GolnzXoq", "replyto": "m9GolnzXoq", "signatures": ["ICLR.cc/2026/Conference/Submission24503/Reviewer_TPAB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24503/Reviewer_TPAB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707245010, "cdate": 1761707245010, "tmdate": 1762943105759, "mdate": 1762943105759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how Recurrent Neural Networks (RNNs) learn abstract temporal structures, such as algebraic patterns (e.g., AAB, ABA), from sequential data. The authors compellingly demonstrate that a sequence classification task, where a label is provided only at the end of the sequence, successfully drives the emergence of low-dimensional, low-rank recurrent dynamics that mirror the hierarchical, tree-like structure of the generative process. This is supported by a thorough set of analyses, including PCA, weight matrix SVD, and ultrametric content. The interesting part is that this schema learning is task-dependent, as standard next-token prediction fails to form such structures. Finally, the learned low-rank weights provide a reusable scaffold that accelerates learning and improves generalization in prediction tasks."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and clearly articulates its fundamental research question: how neural networks learn abstract temporal schemas.\n\n- **Core Finding on Task-Dependence:** The paper's primary strength is the clear demonstration that abstract, low-rank representations emerge specifically from a global sequence classification task. This is compellingly contrasted with a local next-token prediction task, which fails to produce the same abstract structure.\n- **Thorough Analysis:** This central claim is supported by a convincing and multi-faceted analysis of the classification network's internal dynamics. The use of PCA , SVD of the recurrent weights, and ultrametricity analysis collectively builds a strong case that the network learns a low-dimensional, hierarchical representation of the abstract sequence classes."}, "weaknesses": {"value": "1. **Different data distributions in task comparison:** The paper's central claim is that the task objective (global classification vs. local prediction) dictates whether an abstract, low-rank representation emerges. However, the study may introduce a confound by training the two models on different data distributions. The classification network was trained on sequences from a full, branching tree, while the prediction network was trained on \"unambiguous\" sequences derived from an extreme pruning of that same tree. This makes it impossible to disentangle the effect of the task objective from the effect of the input data's structure. The prediction network may have failed to learn the hierarchical schema simply because its training data was explicitly pruned to remove that rich, branching structure.\n2. **Inadequate evidence for transfer learning:** The authors conclude that the low-rank structure learned during classification serves as a \"reusable scaffold\" for the prediction task. However, the paper's own analysis demonstrates that this scaffold is only beneficial when its weights are retrained. Crucially, during this retraining, the dimensionality increases, the recurrent weights become high-rank, and the representation's ultrametric content drops. This suggests that while the scaffold provides a good initialization, the abstract structure itself is not \"reused\" to solve the new task, which undermines the paper's conclusion about its functional transferability."}, "questions": {"value": "1. The transfer experiment where the classification weights are frozen results in performance at chance level (Fig 4C, red line). This is a surprisingly complete failure. Does this imply that the abstract representations in $h_t$ are not, by themselves, sufficient for the output weights of the prediction network to solve the task?\n2. Given that the low-rank, high-ultrametricity structure is lost upon retraining, how did the authors distinguish between the \"reusable scaffold\" hypothesis and the alternative that the classification pre-training merely provided a \"warm start\" in the prediction loss landscape?\n3. The transfer learning analysis could be strengthened by a more granular ablation study. For instance, the authors could test a condition where the transferred recurrent weights ($W_h$) are frozen to preserve the abstract scaffold, while the transferred input weights ($W_{in}$) are retrained.\n4. The paper links generalization accuracy to the Ultrametric Content (UC) score (Fig 3E). However, the UC score never approaches a perfect 1.0, and the similarity matrices (Fig 3A) show clear off-diagonal blocks. The authors attribute this to \"short-term memory effects\". Could this also be interpreted as the network learning a structure that is not a perfect hierarchy, but a more complex graph or schema?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3j78MTfK2w", "forum": "m9GolnzXoq", "replyto": "m9GolnzXoq", "signatures": ["ICLR.cc/2026/Conference/Submission24503/Reviewer_gRSD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24503/Reviewer_gRSD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761794080995, "cdate": 1761794080995, "tmdate": 1762943105372, "mdate": 1762943105372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how recurrent neural networks (RNNs) develop internal representations of abstract schemas during training. The authors design an abstract sequence classification task in which a binary branching tree generates structured sequences mapped to different concrete sequential units. RNNs are trained to classify the schema label, which is provided only at the end of each sequence.\nThe authors found that that RNNs trained on this end-of-sequence categorization task develop linearly separable internal representations that retain a tree-like structure. The authors further analyze the learned dynamics across randomly initialized RNNs, showing that independently trained networks converge to similar low-dimensional representations, differing only by random rotations. This shared low-rank structure, they argue, accounts for the network’s behavior. Finally, the authors noticed that jointly initializing both input and recurrent weights from an RNN trained on the schema classification improved performance for a network instructed to do a prediction task."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* This paper raises interesting questions about how RNNs internalize abstract schemas and converge to shared low-dimensional geometries. \n* The paper provides a clean experimental design probing how abstract schema in shaping internal representations.\n* The paper report experiments and analysis in a very honest and systematic way, which I appreciate. \n* The discovery of shared low-dimensional computational geometries across independently trained networks is intriguing and may shed light on representational alignment in recurrent systems."}, "weaknesses": {"value": "The observations are intriguing and thought-provoking, yet the theoretical grounding and causal interpretation remain underdeveloped. It also remains unclear how the findings on this particular form of schema generalize to many other forms of abstract patterns in sequential data. \n\n* Scattered Presentation. The paper reads as a collection of interesting but loosely connected observations rather than a unified theoretical account. One wonders about the causal link between low-rank geometry and generalization.\n\n* Lack of causal analysis: Section 2.2, I found the tree-like measure, and the correlation insufficient to justify the causal claim as the title for this subsection. As it is unclear whether a interference on the low rank recurrent activities will distroy/enhance RNN's representation of sequence relational structure. \n\n* This paper lacks discussion about a range of literature in analyzing low rank RNNs (See Maheswaranathan et al. 2019, Schuessler et al. 2020) in the neuroscience; schema learning in the human cognition (see Wu et al. 2025a); and the type of mechanistic analysis done on RNNs' internal representations suggesting the emergence of internal representations reflective of sequence structure (Wu et al. 2025b). See Ref.\n\nAdditiona comments:\n* Figure 1 C: there is no shaded area\n\n\nMoreover, the focus on RNNs constrains the broader relevance of the work, as the audience of this conference would be curious on other architecture types... i.e. the scope is quite narrow."}, "questions": {"value": "1.  What level of processing in the RNN reflects more abstract structure? Do later recurrent layers encode higher-level schema abstractions, while earlier ones capture more local bindings (e.g., abb = CDD/CFF/ACC)? \n2. How the low rank recurrent activities drvies relational structure of sequences, what would happen if artificially induce low rank activities in these RNNs?  The singular value decomposition (SVD) is used descriptively but its causal or interpretive value is unclear. Does it reveal meaningful components that contribute to generalization or interpretability? \n3. I am a bit surprised that the next-token-prediction RNNs do not form hierarchical representations. I suspect it has something to do with the way sequences are introduced to the RNNs, and a meta-learning type of set up may encourage the hierarchical representation to emerge in character predicting RNNs. \n4.  Figure 3E suggests a weak correlation between UC scores and generalization ability. However, from the figure it is not obvious to see the correlation. And the relationship is not statistically tested—please include hypothesis testing or correlation significance analysis to support the claim.\n5. Transfer. The final section suggests that networks trained for next-token prediction benefit from weights learned in schema classification networks. What is the role of low rank components in transfer? \n\n\nReference: \n\nHuman sequence schema learning: \n\n_Wu, S., Thalmann, M., & Schulz, E. (2025) Two types of motifs enhance human recall and generalization of long sequences. Communications Psychology_\n\nAnalyzing low rank RNN: \n\n_Maheswaranathan, N., Williams, A. H., Golub, M. D., Ganguli, S., & Sussillo, D. (2019). Universality and individuality in neural dynamics across large populations of recurrent networks. Nature Neuroscience_ \n\n_Schuessler, F., Mastrogiuseppe, F., Dubreuil, A., Ostojic, S., & Barak, O. (2020). Dynamics of random recurrent networks with low-rank structure. Physical Review Research_\n\nAnalyzing internal representations of RNNs: \n\n_Wu, S., Alaniz, S., Karthik, S., Dayan, P., Schulz, E., & Akata, Z. (2025). Concept-Guided Interpretability via Neural Chunking. Advances in Neural Information Processing Systems 2025_"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IvqTwy2SVI", "forum": "m9GolnzXoq", "replyto": "m9GolnzXoq", "signatures": ["ICLR.cc/2026/Conference/Submission24503/Reviewer_tYhh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24503/Reviewer_tYhh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940288786, "cdate": 1761940288786, "tmdate": 1762943104813, "mdate": 1762943104813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}