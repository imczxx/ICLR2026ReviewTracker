{"id": "FONB5dIxSB", "number": 5530, "cdate": 1757918224690, "mdate": 1759897969307, "content": {"title": "Online Alignment as Perceptual Loss", "abstract": "Online alignment (e.g., GRPO) is generally more performant than offline alignment (e.g., DPO)---but why? Drawing on prospect theory from behavioral economics, we propose a human-centric explanation. We prove that online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and PPO/GRPO-style clipping---originally introduced to just stabilize training---recovers a perceptual bias in how humans perceive probability. In this sense, PPO/GRPO act as perceptual losses already. Our theory further suggests that the online/offline dichotomy is itself incidental to maximizing human utility, since we can achieve the same effect by selectively training on any data in a manner that mimics human perception, rather than restricting ourselves to online on-policy data. Doing so would allow us to post-train more quickly, cheaply, and flexibly without sacrificing performance. To this end, we propose a design pattern that explicitly incorporates perceptual distortions of probability into objectives like DPO/KTO/GRPO, creating $\\textit{humanline variants}$ of them. Surprisingly, we find that these humanline variants, even when trained with offline off-policy data, can match the performance of their online counterparts on both verifiable and unverifiable tasks.", "tldr": "Online alignment objectives (e.g., GRPO) mimic how humans perceive probability. By modifying offline objectives to do the same, we can match the performance of online alignment with offline off-policy data, giving us the best of both worlds.", "keywords": ["alignment", "LLM", "LLM alignment", "prospect theory", "perceptual loss", "behavioral economics"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8543eb7dfee6f00e038a9139585951613889b07.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studied the problem of reasons and explanations of online alignment better than offline alignment. Through the lens of the prospect theory framework, the authors provided a human-centric explanation. By proving PPO/GRPO in online on-policy clipping recovers a perceptual bias, the authors showed that online alignment acts as a perceptual loss. With these findings, the post-training is more efficient without restriction on the data source and without the cost of performance. Finally, they also test their results in an unverifiable reward setting to follow open-ended instructions and a verifiable reward setting for the math reasoning task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The perspective to explain online alignment is interesting by adopting some methods from behavioral economics.\n2. The writing of the paper is clean. The authors clearly state the problem, methods, and their results.\n3. Most of the related work is cited and discussed. And the authors provided additional related work in Appendix A.\n4. The paper provided both theoretical results and empirical results."}, "weaknesses": {"value": "1. The assumption is strong, e.g., Assumption 4.3.\n2. Some statement is not rigorous. For example, \"If the success of PPO/GRPO can be ascribed to them being perceptual losses\"."}, "questions": {"value": "1. How is Assumption 4.3 in practice, especially \"The cumulative probability of outcomes with higher absolute surprisal than $z_i$ is negligible\"? Is this assumption also used in previous work? How did your experimental setting satisfy this assumption?\n2. What is the intuition of Definition 4.5 to propose humanline sampling?\n3. The term perception plays a central role in your framing, yet its precise mathematical definition is unclear. Could you provide a more formal explanation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "SgjVIrN9cy", "forum": "FONB5dIxSB", "replyto": "FONB5dIxSB", "signatures": ["ICLR.cc/2026/Conference/Submission5530/Reviewer_xZQd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5530/Reviewer_xZQd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482177466, "cdate": 1761482177466, "tmdate": 1762918113856, "mdate": 1762918113856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a human-centric explanation of why online, on-policy alignment (e.g., GRPO/PPO) often outperforms offline, off-policy alignment (e.g., DPO/KTO). The key idea is to model human utility via prospect theory and include probability weighting (capacity) in addition to the subjective value. From this, the authors argue that sampling from the current policy better matches the human-perceived outcome distribution, giving intuition on why online > offline.\n\nOn the practical side, they introduce a two-part humanline design pattern applicable to DPO/KTO/GRPO: Humanline syncing and Humanline clipping. Empirically, humanline variants trained on offline data close the observed performance gap with online variants on instruction-following and allow 64× less frequent sampling in math-reasoning. Ablations suggest syncing provides the bulk of the gain; upstream clipping adds a smaller but positive effect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**S1. Originality:** Extends alignment as prospect-theoretic optimization by explicitly modeling probability weighting (not just value).\n\n**S2. Practicality:** The humanline recipe (syncing and upstream asymmetric clipping) is simple to implement and integrate into existing algorithms.\n\n**S3. Quality/robustness:** Ablations isolate the roles of syncing vs clipping; results hold across tasks and model sizes.\n\n**S4. Significance:** If adopted, the approach can lower alignment cost and increase offline data usage without sacrificing quality."}, "weaknesses": {"value": "**W1. Dependence on offline data.**\nThe method depends on the choice of offline dataset (e.g., offline data generated by Gemma-9B vs Llama-8B). The paper attributes this to a possible violation of Assumption 4.1 (support overlap and bounded likelihood ratio), but never verifies it empirically. It would strengthen the work to measure these assumptions directly. For instance, by reporting a coverage or divergence metric and examining how they correlate with performance across datasets.\n\n**W2. Gains stem from syncing, not clipping.**\n\nThe ablation study shows that most performance improvements come from syncing the reference model ($\\pi_{\\text{ref}}$). In contrast, the proposed clipping mechanism, a central contribution of the paper, performs comparably to the offline baseline when used alone. This suggests that clipping contributes little to the observed gains while adding two extra hyperparameters and complexity.\n\n**W3. Unfair comparison to trust-region baselines.**\n\nThe comparison to trust-region methods is not hyperparameter-fair. The baseline’s update frequency $k_{tr}=1024$ was taken directly from the original paper, where it was tuned for a different task. In contrast, the proposed method’s equivalent parameter $k$ is shown to be highly sensitive and was tuned for this work. The trust-region baseline could likely perform better if $k_{tr}$ were tuned under the same conditions, so the reported advantage may be overstated.\n\n**W4. Weak theoretical justification.**\nProposition 3.4 only suggests that a small KL divergence is beneficial; it does not theoretically prove that online > offline."}, "questions": {"value": "**Q1.** Could the authors provide quantitative results (e.g., support overlap or KL divergence) to verify when Assumption 4.1 holds and how it relates to performance differences when using different datasets?\n\n**Q2.** See other questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KM3JrI6O2s", "forum": "FONB5dIxSB", "replyto": "FONB5dIxSB", "signatures": ["ICLR.cc/2026/Conference/Submission5530/Reviewer_r7Xi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5530/Reviewer_r7Xi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918905543, "cdate": 1761918905543, "tmdate": 1762918113429, "mdate": 1762918113429, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors provide a theoretical study of the reasons why online alignment is better than offline alignment. In particular, this work shows that online on-policy algorithms are good at approximating human perception. Moreover, the offline and online training have been shown they be identical in maximizing human utility. Motivated by these investigations, the authors propose a new design for the training algorithm, which is evaluated on experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work studies an important domain - trying to explain why online algorithms are usually better than offline algorithms.\n2. The presentation of this paper is easy to follow. \n3. The theoretical and empirical investigations are both provided."}, "weaknesses": {"value": "1. From the experimental results, the humanline indeed can help with offline alignments following the lack of human perceptions. However, it is not clear why the online alignments (the authors have shown that those online alignments have demonstrated the capacity for modeling human perceptions) can also be greatly improved with humanline.\n\n2. The modeling of the value function as well as the utility function is hypothetical, which means that if they reflect the true system is not clear; otherwise, the authors need to verify them.\n\n3. Based on the definitions under prospect theory, the authors provide a new analysis for answering the question about the comparison between the offline and online alignments. However, from another perspective, the offline off-policy is indeed a more challenging process compared to online on-policy, which is more computational cost but involves more exploration opportunities. In this case, how to validate the benefits of online alignments is mostly from the explanation under the prospect theory? \n\n4. As the performance difference between offline and online alignments is also witnessed in large model alignments. I am wondering how to verify that the phenomenon observed for small models can also be extended to large models."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EWDuQVYj6B", "forum": "FONB5dIxSB", "replyto": "FONB5dIxSB", "signatures": ["ICLR.cc/2026/Conference/Submission5530/Reviewer_fVrX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5530/Reviewer_fVrX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761935086556, "cdate": 1761935086556, "tmdate": 1762918112936, "mdate": 1762918112936, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper talks about why online on-policy methods tend to outperform offline off-policy methods, and offers a human-centric answer grounded in prospect theory. Specifically, the authors claim that (1) online on-policy sampling better approximates the human-perceived distribution of what the model can produce, and (2) PPO/GRPO recovers a perceptual bias in how humans perceive probability. They then propose a practical humanline design pattern applicable to DPO/KTO/GRPO: (1) humanline syncing (sync the reference with the previous policy every k steps) and (2) humanline clipping (asymmetric clipping of token log-ratio). Empirically, the authors show that offline + humanline matches online performance on instruction following and allows 64-times less frequent sampling without degradation on math reasoning, which gives large speedups versus standard online training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "## Originality\n1. The paper reframes the online/offline gap through prospect theory from behavioral economics, which is quite a unique point of view. Though it is not the first paper to extend prospect theory to alignment, it considers the probability weighting and the inverted-S curve shown in the paper intuitively explains why online sampling is closer to the human-perceived distribution. \n2. The paper theoretically links GRPO/PPO clipping to perceptual weighting via rejection sampling and limit arguments. This gives a fresh interpretation of a widely used heuristic.\n\n## Quality\n1. All claims are backed with theoretical proof. \n2. Empirically, the paper gives a practical yet minimal recipe to highlight the change. Such a plug-and-run recipe can be useful for future researchers. \n\n## Clarity\nThe presentation is clear and easy to follow. There is a good example throughout the prospect theory section making it easy to understand. \n\n## Significance\n1. If robust, the result meaningfully relaxes the need for fully online training. This would significantly improves the overall training walltime while maintains the same level of performance, greatly speed up the model iterations."}, "weaknesses": {"value": "1. Lack of implications of assumptions. The authors did not cover enough discussion on assumptions 4.1-4.3, e.g., what do those assumptions imply in the language of LLM alignment? Especially for 4.1, same support and finite likelihood ratio seem too restrict in LLM as the action space is enormous; for 4.3, does LLM usually gives you light tail? Any data/study support?\n2. Offline data quality dependency. The paper also shows that not all data could match the online performance, but stops discussing more on the implication and in practice how this could be resolve or any efficient ideas to detect an offline dataset with \"good quality\". Within good/bad data, do they share some common attributes?\n3. Data–method confound. Given the offline data quality dependency, can naive offline methods without humanline achieve comparable performance by a carefully selected set of data? If so, then how can we verify the efficacy of humanline? Some carefully designed abalation tests on this would be good."}, "questions": {"value": "See weakness. Happy to raise the score if authors could help clarify on the above issues."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LY0ByvZyBz", "forum": "FONB5dIxSB", "replyto": "FONB5dIxSB", "signatures": ["ICLR.cc/2026/Conference/Submission5530/Reviewer_Thz1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5530/Reviewer_Thz1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5530/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762055318294, "cdate": 1762055318294, "tmdate": 1762918112490, "mdate": 1762918112490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}