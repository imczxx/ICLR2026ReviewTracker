{"id": "QIgRLpdneh", "number": 19012, "cdate": 1758292735691, "mdate": 1759897066484, "content": {"title": "GRAM: Spatial general-purpose audio representations for real-world applications", "abstract": "Although audio foundations models have seen great progress on a wide variety of tasks, their application in real-world acoustic environments with reverberation and noise has been less successful. Moreover, as audio foundation models are typically trained on dry, single-channel audio clips, the inherent spatial nature of real-world sound scenes is overlooked and tasks involving sound localization ruled out. To address these limitations, we propose GRAM: a General-purpose Real-world Audio Model utilizing a multi-channel masked auto-encoder approach to efficiently learn spatial audio representations from high-quality simulated real-world scenes. To evaluate the performance of GRAM and other audio foundation models in real-world sound scenes, we release Nat-HEAR: A naturalistic version of the HEAR benchmark suite comprising a simulated real-world version, as well as two new sound localization tasks. We show that the performance of GRAM surpasses all state-of-the-art self-supervised audio foundation models and speech models on both HEAR and Nat-HEAR, while using only a fraction of the training data. GRAM also showcases state-of-the-art localization performance, surpassing even supervised sound localization approaches, and can be flexibly applied either to a two-channel, binaural sound format or a four-channel, Ambisonics format. Validating GRAM's performance on real-world sound recordings demonstrates robust transfer to real-world scenes. Taken together, GRAM presents a significant advancement towards robust, spatial audio foundation models for real-world applications.", "tldr": "We present an state-of-the-art audio foundation model that is capable of extracting robust general-purpose audio representations which encode spatial information.", "keywords": ["Spatial Audio Learning", "Audio Foundation Model", "Self-Supervised Audio Representation Learning", "Ambisonics", "Binaural Audio"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e8cee1612bd11b1a0413f5a50645a2ad310ebff0.pdf", "supplementary_material": "/attachment/c222ba10049a5159eb9b2b07726d8ce0f806bd67.zip"}, "replies": [{"content": {"summary": {"value": "The paper aims to provide a model that can handle spatial audio. To that end, a multi-channel masked auto-encoder is trained for two-channel and four-channel audio formats."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "There have significant advances on foundation models for speech. However, spatial audio has been largely overlooked. It is positive to see advances in this area. To this end, the paper provides a potentially valuable dataset containing binaural and ambisonics room impulse responses. The paper also provides a reasonable evaluation of the proposed model for several audio datasets, compared to some alternative audio models."}, "weaknesses": {"value": "I have two major concerns about the claims of this work: 1) A key challenge in spatial audio is the sheer variety of different microphone array geometries (ranging from binaural listeners to linear, circular, and spherical arrays of varying number of microphones). By definition, a foundation model needs to generalise across different array geometries. However, the proposed approach is limited to only two specific formats involving only two and four microphones. While the results are very promising for these specific formats, the model is not suitable for other, more general geometries. 2) The model is trained on spectrograms (or spectrograms + intensity vectors). Considering that spectrograms discard phase information (which is crucial for spatial audio), it is unclear why the model is not trained on raw waveforms, similar to recent foundation models for speech."}, "questions": {"value": "-\tCan the model be extended to more general microphone array geometries without the need for re-training?\n-\tHow does the model perform when trained on raw waveforms rather than spectrograms?\n-\tHow did you train the Mamba variant? Based on the discussion in the paper, I believe this trained using spectrograms as inputs. However, it is unclear why you would train a sequence model on image-like data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5iBjyO2fbN", "forum": "QIgRLpdneh", "replyto": "QIgRLpdneh", "signatures": ["ICLR.cc/2026/Conference/Submission19012/Reviewer_rz9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19012/Reviewer_rz9E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946927200, "cdate": 1761946927200, "tmdate": 1762931059549, "mdate": 1762931059549, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents GRAM (General-purpose, Real-world Audio Model), a multi-channel masked autoencoder (MAE) designed to learn spatially aware audio representations from simulated naturalistic sound scenes. Unlike existing single-channel foundation models such as Audio-MAE or BEATs, GRAM explicitly encodes spatial cues by processing both binaural (2-channel) and Ambisonics (4-channel) inputs. To support systematic evaluation, the authors introduce Nat-HEAR, an extended version of the HEAR benchmark that incorporates spatialized and localization-oriented downstream tasks. Experimental results show that GRAM achieves state-of-the-art performance across multiple baselines, both self-supervised and supervised, and demonstrates notable robustness to reverberation and environmental noise. Together, GRAM and Nat-HEAR represent an effort to push foundation audio models toward more realistic spatial understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "(1) The application of a masked autoencoder to multi-channel spectrograms is an important step toward spatially aware audio modeling. By allowing the network to learn from interaural time and intensity differences as well as room reverberation cues, the approach extends the MAE framework beyond monaural “dry” signals to capture the physical geometry of sound propagation. This opens the door to richer and more perceptually grounded audio representations.\n(2) The introduction of Nat-HEAR as a benchmark is a strong contribution to the field. By expanding HEAR to include spatialized datasets and localization-oriented downstream tasks, the paper provides a valuable resource for evaluating and comparing spatial audio foundation models. The release of 85,000 simulated room impulse responses (BRIRs/ARIRs) also contributes significantly to the reproducibility and standardization of future research in this area.\n(3) The paper demonstrates consistent performance improvements over both self-supervised and supervised baselines. Of particular note is that GRAM surpasses some supervised localization models trained with explicit direction labels, suggesting that the model’s spatial embeddings capture meaningful physical relationships without the need for handcrafted supervision.\n(4) The study includes thoughtful ablations on key design factors such as masking strategies (patch- versus time-based), ratios of dry to reverberant mixtures, and backbone architecture choices (Transformer versus Mamba). These analyses provide transparency and help readers understand how GRAM’s components influence its robustness and generalization behavior."}, "weaknesses": {"value": "(1) The architectural novelty of GRAM is limited. The model primarily extends existing single-channel frameworks such as Audio-MAE or BEATs to handle multi-channel inputs, without introducing a fundamentally new objective, representation mechanism, or training paradigm. While the extension is useful, the contribution is incremental in nature rather than conceptually transformative.\n(2) Despite claims of strong real-world generalization, the training and evaluation are predominantly conducted on simulated environments rather than real-world multi-channel recordings. Validation on measured datasets such as SONYC-UST, TAU Spatial Sound, or FAIR-Play would be necessary to substantiate the claim that GRAM transfers effectively to real acoustic conditions, including non-ideal sensor responses and background variability.\n(3) The paper’s positioning relative to recent spatial audio foundation models is not sufficiently clarified. Prior works such as Spatial-AST (Zheng et al., 2024) and Qwen-Audio (Chu et al., 2023) already explore similar multi-channel and spatial reasoning concepts within transformer frameworks. GRAM does not convincingly demonstrate performance superiority or distinct methodological advances compared to these approaches, especially on real-world localization or speech-related tasks.\n(4) The model’s treatment of multi-channel spectrograms as simple stacked image-like inputs misses opportunities for deeper spatial modeling. Unlike methods such as Spatial-SSL (2024) or SoundFieldNet, GRAM does not explicitly incorporate spatial phase correlation, coherence constraints, or directional energy cues, which are physically meaningful aspects of spatial sound. This omission limits both interpretability and potential generalization in physically complex environments."}, "questions": {"value": "(1) Can the authors provide quantitative results demonstrating GRAM’s performance on real-world multi-channel datasets to validate its generalization beyond simulated environments and strengthen the claim of real-world robustness?\n(2) Can the authors present a direct comparison or ablation against recent spatial audio foundation using a consistent evaluation setup to objectively demonstrate where GRAM offers performance or efficiency advantages, thereby clarifying its distinct contribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "IobcNJla0T", "forum": "QIgRLpdneh", "replyto": "QIgRLpdneh", "signatures": ["ICLR.cc/2026/Conference/Submission19012/Reviewer_ESjb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19012/Reviewer_ESjb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954996913, "cdate": 1761954996913, "tmdate": 1762931059048, "mdate": 1762931059048, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GRAM (general-purpose real-world audio model), a model which produces representations of spatial audio (from binaural or ambisonics inputs), capturing the main audio signal as well as other characteristics such as the reverberation, background noise, and spatial dimensions.\n\nGRAM is trained as a masked auto-encoder, on a custom dataset with 85,000 room impulse responses (respectively for binaural and for ambisonics). Additionally, the authors extend the HEAR benchmark to incorporate the acoustic properties (resulting in Nat-HEAR).\n\nEvaluations on the Nat-HEAR benchmark show GRAM-binaural achieving the highest combined score, followed by GRAM-Ambisonics. On the original HEAR benchmark, GRAM-clean (followed by GRAM-binaural, and GRAM-ambisonics) demonstrating GRAM's performance in natural sound-fields is not at the expense of performance on clean audio. Additionally, GRAM is show to capture directionality of sound with an 11.3° MAE on the TUT Sound Events 2018 REAL dataset."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* This paper tackles an important problem (representation learning for spatial audio) and does so thoroughly, ablating input format (binaural and ambisonics), architecture (transformer and mamba), patching strategy (frequency based and time based), masking ratio, and in-batch sampling factor.\n* The paper is well written and easy to follow (though see a few questions below)"}, "weaknesses": {"value": "* **W1**: Though GRAM includes extensive ablations on Nat-HEAR and HEAR (which measure the understanding of the audio signal) as well as localisation tasks, its impact would be further strengthened with other downstream tasks (particularly around acoustics). Examples of other acoustics tasks include: source separation (as the authors suggest in the introduction) or T60 estimation.\n\n\n* **W2**: There are some readability issues in the paper that need addressing (see questions below)."}, "questions": {"value": "* **Q1**: In line 225, if the hop window is 10ms, and the audio duration is 10s, why is the dimensionality of the spectrogram 1024x128 instead of 1000x128?\n* **Q2**: If I understand in-batch sampling correctly, it implies that for a sample it gets repeated 3.2 times on average in a single batch. That could give raise to a very noisy gradient. Could you run pre-training with a sampling factor of 1 and use batch accumulation of 16 (so that you get the same effective batch size but with more diverse samples? I understand this might be up to 16x times slower to train, but that may be offset by faster convergence.\n* **Q3**: GRAM-clean is not mentioned before line 251, and it is not clarified exactly what it does. I assume it's a variant of GRAM with the same Mel-spectrogram but using only a single channel?\n* **Q4**: The average score for GRAM-Binaural does not seem to add up. I'm not sure I follow the computation of $s(m)$. Would it not be simpler to take the average performance? Or the average of the improvements over the baseline?\n* **Q5**: [Minor] Note that TUT Sound Events 2018 REAL represents a real RIR convolved with clean audio. As such it is possible there is still a performance gap with actual ambisonics audio. I would suggest computing the direction-of-arrival error against STARSS'23 [1] which is fully recorded audio\n* **Q6** [Minor] It would be interesting to ablate Nat-HEAR and the localisation tasks depending on the SNR [5-10, 10-20, 20-40]. This would establish how robust is GRAM to distractor noise.\n* **Q6**: Some models seem to be missing from Fig 2.A (including Wav2Vec2 HuBERT and WavLM). Additionally, the caption of Fig 2 does not match its contents (the mentions two subfigures that are not there, and describes A in C, and B in A).\n\n\n------\n### Nitpicks (do not affect score, no need to follow up on these during rebuttal)\n\n* **N1**:  In lines 253-256, could you list the number of parameters of each model\n* **N2**: The points in Figs 2.A, 4, and 5 are pretty thick, can you make them smaller so that it's clear what is the actual performance of each setting?\n* **N3**: Line 407, please indicate Table 5 is in the appendix.\n* **N4**: At the end of section 4.1, please explicitly note in the main text that the results for the sampling factor and masking ratio are in Appendix F.\n-----\n\n[1] Shimada et al (2023) \"STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events\" NeurIPS Track on Datasets and Benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eA1NyQ3gTV", "forum": "QIgRLpdneh", "replyto": "QIgRLpdneh", "signatures": ["ICLR.cc/2026/Conference/Submission19012/Reviewer_zoEh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19012/Reviewer_zoEh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19012/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762196540108, "cdate": 1762196540108, "tmdate": 1762931058627, "mdate": 1762931058627, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}