{"id": "pelxC06W8U", "number": 14930, "cdate": 1758245662587, "mdate": 1763515841991, "content": {"title": "Block-sample MAC-Bayes generalization bounds", "abstract": "We present a family of novel block-sample MAC-Bayes bounds (mean approximately correct). While PAC-Bayes bounds (probably approximately correct) typically give bounds for the generalization error that hold with high probability, MAC-Bayes bounds have a similar form but bound the expected generalization error instead. The family of bounds we propose can be understood as a generalization of an expectation version of known PAC-Bayes bounds. Compared to standard PAC-Bayes bounds, the new bounds contain divergence terms that only depend on subsets (or \\emph{blocks}) of the training data. The proposed MAC-Bayes bounds hold the promise of significantly improving upon the tightness of traditional PAC-Bayes and MAC-Bayes bounds. This is illustrated with a simple numerical example in which the original PAC-Bayes bound is vacuous regardless of the choice of prior, while the proposed family of bounds are finite for appropriate choices of the block size. We also explore the question whether high-probability versions of our MAC-Bayes bounds (i.e., PAC-Bayes bounds of a similar form) are possible. We answer this question in the negative with an example that shows that in general, it is not possible to establish a PAC-Bayes bound which (a) vanishes with a rate faster than $\\mathcal{O}(1/\\log n)$ whenever the proposed MAC-Bayes bound vanishes with rate $\\mathcal{O}(n^{-1/2})$ and (b) exhibits a logarithmic dependence on the permitted error probability.", "tldr": "We propose a novel PAC-Bayes generalization bound for learning algorithms for that holds in expectation and show that it does not hold with high probability.", "keywords": ["PAC-Bayes bound", "MAC-Bayes bound", "KL divergence", "block-sample MAC-Bayes bound"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/66dc53b99f18d9df6a685f05a3c919902855d15a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "MAC-Bayes bounds (mean approximately correct)  are a  variation of PAC-Bayes bounds that are not necessarily valid with high probability but bound the expected generalization error.\nThe study considers „block-sampled“ MAC-Bayes bounds, in which the data is split into block/batches and the divergence term between prior and posterior is defined on block level.\nThe submitted paper studies the optimization of these „block-sampled“ MAC-Bayes bounds and whether they can be turned into\nPAC-Bayes versions which improve on current PAC-Bayes results, which the authors show to be not possible in general."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I found the general topic of the study interesting.\nI would like to stress that also the negative result is interesting."}, "weaknesses": {"value": "* The „impossibility result“ in Theorem 2 can be viewed as a general form of the properly cited results by \n\nHrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Formal limitations of sample-wise\ninformation-theoretic generalization bounds. In 2022 IEEE Information Theory Workshop (ITW),\npp. 440–445. IEEE, 2022.\n\n* I was wondering: how are the results related to the results by \n\nRecursive PAC-Bayes: A frequentist approach to sequential prior updates with no information loss\nYS Wu, Y Zhang, BE Chérief-Abdellatif, Y Seldin\nAdvances in Neural Information Processing Systems 37, 17947-17971\n\nwhich - in a different setting - consider a split of the training data and incrementally update the PAC-Bayes bound.\n\n* Section 4: I was somehow not convinced by this example that should show the usefulness of the new bounds. The prior leading to (11) depends on the „batch size“ m. I accept that in PAC-Baysian analysis the prior is not necessary some form of „prior belief“ as in stared Bayesian analysis, but rather a tool to get tight performance guarantees. Still, that the prior depends on m does not feel right to me - where should such a prior come from? Could the authors discuss this in more detail?  The fact that the results are better than without blocks-sampling is then also heavily dependent on the prior (end of  Section 4). Why should this prior be used for the  m=n case? Is the prior real „prior belief“  here? If yes, where does it come from for the m-n case? If not - if it is a tool to get tight bounds- why this choice for n=m?\n\n\nMinor comments:\n\n* Authors forgot to introduce the prior Q_W when introducing PAC-Bayes in equation (1).\n* Can the loss in the example 4 be viewed as some known robust loss function?"}, "questions": {"value": "Please see \"Weaknesses\" above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "8lrdAg2cPY", "forum": "pelxC06W8U", "replyto": "pelxC06W8U", "signatures": ["ICLR.cc/2026/Conference/Submission14930/Reviewer_vrZ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14930/Reviewer_vrZ6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760608265021, "cdate": 1760608265021, "tmdate": 1762925269412, "mdate": 1762925269412, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a mean posterior error bound that relates\n$$\\mathbb{E} \\\\{ d( \\rho\\\\{ \\hat L_n \\\\}, \\rho\\\\{ \\mathbb{E} L_n \\\\})  \\\\}$$\nto\n$$\\sum_{j=1}^J \\mathbb{E}\\\\{  D_{\\text{KL}}(\\rho_j\\\\|\\pi) \\\\} + J \\cdot \\text{``per block CGF''}$$\nwhere $\\mathbb E$ is over the randomness of the observations, $\\rho$ is the data-dependent posterior probability, and $\\pi$ a prior probability; importantly, the novelty of the paper lies in $\\rho_j$, the posterior probability *after observing the block #j*.\n\nThis implies novel generalization bounds that scale as\n$$\\sqrt{ \\frac 1 n  \\sum_{j=1}^J \\mathbb{E}\\\\{  D_{\\text{KL}}(\\rho_j\\\\|\\pi) \\\\}}$$\nwhere the usual ``complexity term'' is broken down linearly into block components."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "I found the block decomposition concept novel, and the authors did convince me that the technique drives tighter bounds.\n\nSome might argue that this is an interpolation between the individual-sample ($m=1$) and the bulk-sample ($m=n$) regimes (borrowing the leave-one-out analysis technique, equality (b), from the former). However, the resulting bounds are tighter as a result of this interpolation, with the \"optimal block rate\" clearly explored in Section 5."}, "weaknesses": {"value": "For those outside the PAC-Bayes community, Sections 1-2 do not provide a good introduction to the concepts involved. In particular, *what is $Q_W$*? PAC-Bayes people of course know this is the prior probability; however, even if explicitly mentioned, the role of this prior would usually still be quite confusing for generic readers. The authors did even less in this regard by expending 0 words on $Q_W$, and how the evolution to $P_{W|S_j}$ is to be construed.\n\nVery minor notation issue: $\\mathbb E_{P_S}$ vs. $\\mathbb E_{S}$."}, "questions": {"value": "In general, how does the quantity\n\n$$K_{n} := m^{-1}\\mathbb{E} \\\\{ D_{\\text{KL}} (\\rho_m \\\\|\\pi) \\\\}$$\n\nevolve as a sequence of $m$? Here $m$ agrees with the definition in your paper (i.e. block size), $\\pi$ is a prior probability, and $\\rho_m$ is the posterior probability evolved from $\\pi$ after seeing $m$ i.i.d. observations. I believe this is important to better conceptualize the benefits of the block decomposition in bounds e.g. (10). Some graphical exploration would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "M4Ob2xYz88", "forum": "pelxC06W8U", "replyto": "pelxC06W8U", "signatures": ["ICLR.cc/2026/Conference/Submission14930/Reviewer_7GiM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14930/Reviewer_7GiM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848739163, "cdate": 1761848739163, "tmdate": 1762925268704, "mdate": 1762925268704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the MAC-Bayes bounds. They proposed a family of novel block-sample MAC-Bayes bounds, which generalized the MAC-Bayes bounds in the sense that it aggregates the contributions of a collection of disjoint subsets of the sample (on the MAC-Bayes bound) instead of consider the sample a whole. They proved the convergence of the block-sample bound and showed its superior characteristic power compared with the original PAC-Bayes bounds in a simple application-Gaussian mean estimation. At last, the paper discussed how the block size could affect the convergence and the possibility of transforming the block-sample bound into a high probability form."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Although I did not verify all the proofs in detail, they appear to be sound overall.\n* While not significant, the block-sample bound seems to be a non-trivial generalization of the prior results."}, "weaknesses": {"value": "I think the authors should put more effort in the presentations of this paper because of the following reasons:\n* The authors throw out the definition of PAC-Bayes bounds at the very beginning, however, without giving enough explanation for the term in the expression. For instance, I do not see any description of $Q_W$ and have trouble in understanding what it means. Also, I advise the author to give a concrete example of $I(n,d)$ in addition to just saying it is proportional to $n$ and $d$. Similarly, instead of just saying what $P_{W|S}$ does in general, give a concrete example to demonstrate it.\n* The authors introduced the \"individual-sample bound\" in describing their first contribution but did not even explain what it is.\n* From my perspective, the authors did a poor job in motivating the audiences on their contributions. The author only briefly mentioned that their result is a generalization of the MAC-Bayes bound and individual-sample bound (which they did not define), they did not emphasize the significance of their generalization. In particular, the authors did not well explain what advantages this generalization can bring upon the original MAC-Bayes bound in the contribution part. \n* While the author gave an example on mean estimation of Gaussian in Section 4 to demonstrate the usefulness of the block-sample bound compared to the PAC-Bayes bound, I am not convinced by this example due to its overly simplicity. In particular, the PAC-Bayes bound exhibits a poor generalization for this application not because this task is hard. In fact, a simple Hoeffding bound can give a tight error bound on the mean estimation. Therefore, this example is not enough to demonstrate the significance of the generalization. With that being said, I still suggest to move this example into the introduction to give a more fluent and motivative presentation.\n* In Section 3, it would be more accessible for the audience if the author could briefly explain the assumption in the main theorem in plain language and sketch their proof idea of the theorem in advance.\n\nI will consider raise my rating if the authors address my concerns."}, "questions": {"value": "* What does the assumption (3) in the main theorem implies intuitively?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CHqUhLF1AH", "forum": "pelxC06W8U", "replyto": "pelxC06W8U", "signatures": ["ICLR.cc/2026/Conference/Submission14930/Reviewer_HaRN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14930/Reviewer_HaRN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762530980604, "cdate": 1762530980604, "tmdate": 1762925268228, "mdate": 1762925268228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Summary of changes in the revised version"}, "comment": {"value": "We are very grateful to the reviewers for the time and effort they have invested to provide comments on our paper that have helped us to make substantial improvements to the paper. We have taken the concerns raised by all three reviewers regarding the quality of the writing, especially of the introductory sections, particularly seriously. In this post, we briefly summarize the main changes we have made to the paper (for convenience, all text that is new or changed is printed in blue in the uploaded revised version). For detailed responses to individual comments made by the reviewers, please see our comments posted on each of the reviews.\n* We have re-written most of Section 1 to give a far more comprehensive and detailed introduction to the general topic of the paper, as well as to introduce and explain every notation used in full detail.\n* In the course of this, we are now illustrating the notions of statistical learning theory with an image classification example to make the paper more approachable and the overall motivation clearer.\n* We now also introduce the simple example from Section 4 already in Section 1.\n* Because this means that almost all of the notations that were previously introduced in Section 2 are now fully defined in Section 1, we have chosen to delete the former Section 2 and define the few remaining notations where they first appear.\n* To make the paper structure a bit more straightforward, we have promoted the former Section 1.1 (Related Works) to a new top-level section, forming now Section 2.\n* We have added many details and additional discussions into the paper which we are confident comprehensively address the other concerns raised by the reviewers. For details, see our individual responses to the reviews below."}}, "id": "03n2dMWA7r", "forum": "pelxC06W8U", "replyto": "pelxC06W8U", "signatures": ["ICLR.cc/2026/Conference/Submission14930/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14930/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission14930/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763515971568, "cdate": 1763515971568, "tmdate": 1763515971568, "mdate": 1763515971568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}