{"id": "m14YNdmPAh", "number": 16170, "cdate": 1758260955778, "mdate": 1759897256983, "content": {"title": "floq: Training Critics via Flow-Matching for Scaling Compute in Value-Based RL", "abstract": "A hallmark of modern large-scale machine learning techniques is the use of training objectives that provide dense supervision to intermediate computations, such as teacher forcing the next token in language models or denoising step-by-step in diffusion models. This enables models to learn complex functions in a generalizable manner. Motivated by this observation, we investigate the benefits of iterative computation for temporal difference (TD) methods in reinforcement learning (RL). Typically, they represent value functions in a monolithic fashion, without iterative compute. We introduce floq (flow-matching Q-functions), an approach that parameterizes the Q-function using a velocity field and trains it with techniques from flow-matching, typically used in generative modeling. This velocity field underneath the flow is trained using a TD-learning objective, which bootstraps from values produced by a target velocity field, computed by running multiple steps of numerical integration. Crucially, floq allows for more fine-grained control and scaling of the Q-function capacity than monolithic architectures, by appropriately setting the number of integration steps. Across a suite of challenging offline RL benchmarks and online fine-tuning tasks, floq improves performance by nearly 1.8x. floq scales capacity far better than standard TD-learning architectures, highlighting the potential of iterative computation for value learning.", "tldr": "We use iterative computation via flow matching to scale compute for value-based RL and obtain significant gains over monolithic critics under both parameter and compute matched settings.", "keywords": ["offline RL", "online fine-tuning", "flow-matching", "TD-learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/da6a5161c97b75ce47082884b1a8118e98756185.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a novel way of applying flow mathching in offline RL by predicting the Q-value function with TD learning: for that they introduce a latent variable $z$ that at $t=0$ is sampled from a uniform distribution and by following the learned velocity $v_{\\theta}(t, z)$ we should obtain $\\delta(Q(s,a))$  at $t = 1$. To learn the velocity field $v_{\\theta}(t,z)$, this approach suggests using the interpolant between the initial noise $z$ and TD-bootstrapped Q function at every step of numerical integration of the flow.  The paper builds on top of the previous work that considered flow matching for policy learning and showed that using flow matching for Q learning can bring up to x1.8 better results on some hard tasks from OGBench, where hardness is measured with respect to FQL baseline. This approach is contrasted to monolithic way of learning Q and over the series of ablation studies, authors demonstrate that this improvement cannot be attributed to just an increase of compute. Empirically, the authors show that the main ingredients to make flow matching work for TD learning are 1) the careful choice of support interval for $z$ , 2) using categorical representation for the latent input $z(t)$ with HL-Gauss encoding of high variance.\n\nThe overall idea is original and results in a clear improvement over the monolithic way of learning Q-function, but more explanation/investigation is needed to understand the degraded performance when large $K$ are considered. Moreover, the paper can improve its writing, while providing more details on the experimental setup in the main paper. For all these reasons, I recommend a weak accept."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The idea of using flow matching for TD learning is very interesting and further extends the works on adapting flow matching techniques to RL. The paper shows that this approach first of all allows us to reach new state-of-the-art results for many tasks from OBbench constrained to a single task. Secondly, they showed that this algorithm proposes a good way to further scale offline RL and it is compared to other ways of scaling such as increasing the compute power in sequential way (comparison to ResNet) and in parallel way (comparison to ensemble methods). The authors also perform a vast ablation study to justify their design choices and discovering a set of default hyper parameters that work uniformly well for all tasks considered in the experiments."}, "weaknesses": {"value": "I found that a lot of experimental details were not clear enough or transparent while reading the paper. What is success-rate in OGbench? Is success-rate the only metric, because in Figure 7 you mention a generic performance metric. How do you extract policy from the learned Q-furnction? Those details become more clear after consulting the appendix, but a short summary in the main paper is necessary. Moreover, it is only upon reading the appendix, it becomes clear that floq essentially builds on top of FQL and shares most of its implementation details except for training the Q function, which in FQL is done in monolithic way. Thus, in practice floq needs to distill its learned Q function from flow matching into a single MLP that can be used to obtain approximated Q in a single step. In this regard, I would like the authors to clarify whether they use a distilled Q function for all their experimental results or they use sometimes a Q function obtained directly from flow matching. \n\nMoreover, I found the study performed in Section 5.3 in the point 1) a bit unsatisfactory. First of all, I would like some more details on this experiment’s setup: does $K$ correspond to a number of integration steps used during the training and then the results are reported for the distilled Q function? From Figure 4, we see that using larger number of $K$ leads to a significant degradation on one of the tasks, and the only explanation that authors provide is that it might be overfitting, but no further analysis to confirm this hypothesis is performed. In this case, why cannot be underfitting? To my mind, you might need different number of training steps to train floq with small $K$ vs large $K$. Or you might need to consider larger networks for distillation to represent floq with $K>8$. I believe this question should be further explored, as, on the first glance, the choice of $K$ should only affect the granularity of the flow and thus having worse performance with larger $K$ is contra-intuitive.\n\nAnother possible limitation is that the authors only considered OGbench for their experiments, which is a dataset designed originally for goal-conditioned offline RL. The authors argue that the most common dataset such as D4rl has been saturated, but in this regard, why not considering d5rl for example?\n\nFinally, the authors only test their algorithm on 3 seeds, which is not enough to get reliable statistics, especially for multi-agent comparison https://arxiv.org/abs/2306.10882 (see App.G). Even if it is just for a few tasks, but performing comparisons with more seeds is important for more conclusive results.\n\nMinor:\n- L138 remove the in front of our\n- L158 refer to the papers that use averaging/doing ensembles for performing TD\n- Section 5.3: the order of the questions is non-linear: point 2) is after 3)\n- L1025: helps TO mitigate\n\n\n\nWeird sentences:\n- L194-195: Interestingly, we find that this problem can is a result of two peculiarities associated with applying flow-matching to TD\n- L235: When we then run (imperfect) TD-loss training on these interpolants, \n- L227-229: When the width of the interval $[l,u]$ is small, and the overlap between this interval and the range of target $Q$-values we hope to see is minimal, we would expect to see more straight flow traversals, that might be independent of interpolant $z$.\n- L262-263: We show in our experiments THAN doing so helps substantially by encouraging the network to meaningfully utilize this time.\n- L437: To answer this question, we trained a variant of floq where we supervised the velocity field was supervised only at t= 0 and used only a single flow step.\n- L444: oneintegration"}, "questions": {"value": "See weaknesses. More questions:\n\n  1. Can floq be adjusted to distributional RL setting, as currently flow matching is used to approximate a dirak distribution over $Q(s,a)$, while it can be seen as a natural framework to learn distributions over $Q$?\n2.  How do you measure Q_min and Q_max?\n3. What is a difference between default tasks and all tasks in OGbench?\n4. (L430-431) Can floq be parallelised though? Because, even if the cumulative compute time is the same, when it is parallizable you can expect that you can gain some speedup, so the actual time will reduce roughly by number of processes P. \n5. How online fine-tuning is performed, how the data is collected for online finetuning?\n6. If flow matching is done with an ODE integrator why not to try different integration schemes? According to you, can it remedy the degradation of the performance observed for the cases with the high curvature.\n7. How different are $Q(s,a)$ for fixed $s$ and $a$, but with different initial $z$? Is there a high discrepancy, even if they are trained to represent dirak distribution?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PRCNDtXApO", "forum": "m14YNdmPAh", "replyto": "m14YNdmPAh", "signatures": ["ICLR.cc/2026/Conference/Submission16170/Reviewer_Jc59"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16170/Reviewer_Jc59"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760977971233, "cdate": 1760977971233, "tmdate": 1762926334042, "mdate": 1762926334042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes *floq*, a flow-matching based Q-value estimation approach for offline and offline-to-online reinforcement learning. More specifically, it computes Q-values by iteratively running numerical integration of flow-matching's velocity field, allowing more fine-grained control and scaling than traditional monolithic architectures. The proposed method is evaluated on multiply OGBench tasks."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The overall paper is well written and organized, and some illustrations are clear.\n- The proposed flow-matching-based Q-learning is interesting.\n- The results are competitive, and the authors provide comprehensive analysis on their design choices."}, "weaknesses": {"value": "- I can understand that applying flow-match or diffusion models can improve policy learning due to the complex, multi-modal/-peak distribution of policies (e.g., you can take different actions at a state but still get similar rewards). However, when it comes to the Q function, I am confused that if Q-value is also multimodal given a certain action.\n- The authors state that previous diffusion-based methods require supervision at every iterative step. But this can be alleviated by directly approximating the final Q-value from noise prediction, similar to DDIM.\n- It would be better to also add the flow-matching to preliminary to help readers understand the following method smoothly.\n- Although OGBench tasks are generally more challenging than standard D4RL, the comparison is still needed to illustrate the robustness of *floq*. In addition, more diffusion-based approaches should also be added to the comparison.\n- There are repeated citations for the same \"flow q-learning\" paper."}, "questions": {"value": "- What is the $m$ in the TD-target equation $y(s, a)$ in line 179.\n- The authors state that Q_min and Q_max denote the minimal and maximal possible Q-value achievable on the task. But how do you calculate them in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yE6nQcb4yo", "forum": "m14YNdmPAh", "replyto": "m14YNdmPAh", "signatures": ["ICLR.cc/2026/Conference/Submission16170/Reviewer_eV3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16170/Reviewer_eV3j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761838535679, "cdate": 1761838535679, "tmdate": 1762926333604, "mdate": 1762926333604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Flow-matching Q-functions (floq). floq trains a time-dependent,state-action-conditioned velocity field $v_\\theta(t,z|s,a)$ such that at the forward operator $\\psi_\\theta$ defined by the vector field equals $\\psi_\\theta(j/K, z | s, a) \\ z + \\frac{1}{K}\\sum_{i=1}^j v_\\theta(\\frac{i}{K}, \\psi_\\theta(\\frac{i-1}{K}, z| s, a) | s, a)$ with $Q(s, a, z) \\triangleq \\psi_\\theta(1, z|s,a)$ and $z_0 \\sim\\text{Unif}[l,u]$. Furthermore, $z_t = (1-t) z_0 + t y(s,a)$ where $y(s,a)$ is the boopstrapped TD-target. The authors validate floq on 50 tasks from OGBench, demonstrating approximately 1.8× improvement over baseline FQL on challenging tasks, and show it scales better than ResNets or monolithic Q-function ensembles."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Strong ablation studies on floq \n2. Comprehensive evaluations with superior performance in difficult tasks such as puzzle-4x4 and cude-double\n3. Novel connection between flow matching and Q-learning\n4. Nice analysis of design choices in section 4.3. It gives very inspirational analysis on how to design the floq architecture."}, "weaknesses": {"value": "1. Lack of theoretical understanding\n2. All implementations of floq build on FQL\n3. The idea is similar to distributional RL, but it not widely compared\n4. While design choices are grounded, they are not systematic but more results from trial-and-error"}, "questions": {"value": "1. For a fair comparison, I think one should do scalar critic + gaussian policy, scalar critic + flow policy, floq critic + gaussian policy and floq critic + flow policy. Is there any result on floq critic + gaussian policy?\n2. floq's main contribution is on its critics. Is this applicable for other models such as REBRAC or only special to FQL?\n3. In Figure 5, why do ResNets show performance degradation at high FLOPs (e.g., antmaze-giant drops from 45% at 16 FLOPs to 31% at 32 FLOPs)? More importantly, floq also shows this degradation pattern in Figure 4 (antmaze-giant: 86% at K=4 → 70% at K=8 → 55% at K=16). What is the fundamental cause of this degradation?\n4. in Figure 6, floq achieves better performance than monolithic ensembles. Is ensemble also applicable for flow? If so, is there any benefit gain?\n5. I am not 100% convinced why HL-Gauss encoding helps here. Could you explain more in details why this helps in this scenario?\n6. The performance seems to depend heavily on hyperparameters. How easy/difficult is it to adopt the algorithm for a new test task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jKPycB9Wv7", "forum": "m14YNdmPAh", "replyto": "m14YNdmPAh", "signatures": ["ICLR.cc/2026/Conference/Submission16170/Reviewer_H9C5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16170/Reviewer_H9C5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972266873, "cdate": 1761972266873, "tmdate": 1762926333211, "mdate": 1762926333211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to improve scalability of Offline RL by introducing a flow model based Q-critic. The most closely related past work is FQL (ICML2025), which included a flow-based policy, but a regular Q-critic; the current work uses the same policy type, but adds a flow-model critic. The model works by uniformly sampling an initial noise value in the interval [Q_min, Q_max], then applying Euler integration steps to the noise with a learned velocity field for a few steps to end up at the final Q estimate. (The velocity field is also conditioned on the state and action.) Their architecture is called *floq*. Another major point in their discussion is the need for intermediate targets when training the flow which is done by a linear interpolant. The representation of the interpolant inputs is another major design choice: they use a categorical representation based on the HL-Gauss method from Farebrother (2024), and they also add a Fourier basis representation for the time t. Experimentally, they evaluate their method on OGBench (the same major testbed as used by FQL (ICML2025)), and consistently outperform FQL in terms of performance. Several ablation and sensitivity studies show the necessity of the different design choices in the current configuration, and how they alter the performance."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- Well written, and well presented.\n\n- Thorough experimentation. The work provides several sensitivity studies and much material for readers to examine regarding the performance. This can be a good reference for reproducing the results or building on the results.\n\n- Performance is compared on the same settings as FQL, which is an established method, the FQL results in the current paper match the published results, and the method of the current paper outperforms it clearly.\n\n- Some of the discussion was interesting, e.g., about the  usefulness of iterative computation for scaling in deeplearning, and how intermediate targets in such cases tend to be helpful."}, "weaknesses": {"value": "- The computation time increases. The runtime in Appendix A.8. shows that it is quite a bit slower than FQL (up to around 7 times slower for 16 integration steps). This may reduce the practicality of the method.\n\n- Many design choices appear ad hoc to me (e.g., using HL-Gauss, and other design choices). These appear mostly engineered by testing things, without a strong theoretical backing for the choices. Such advances are also useful, but it is not clear to me whether these are likely to stand the test of time, so I list it as a weakness."}, "questions": {"value": "I have no major question, but feel free to respond to my perceived weaknesses.\n\nA few typos:\n\nLine 138: “the our” change to “our”\n\nThe definitions of the v and q functions around lines 126 and 127 appear wrong. You add the factor $1/(1-\\gamma)$ in front, but at the same time, you added the $\\gamma^t$ inside the sum of the expectation. The original definition of these is: $\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t r_t]$, so there will not be a $1/(1-\\gamma)$ factor in front. You get this factor, if you instead define the discounted state visitation distribution, and define the objective as the expected reward under the normalized discounted state visitation distribution (where the discounting appears in the distribution, not as a multiplier to the reward)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PpyDMD8yWu", "forum": "m14YNdmPAh", "replyto": "m14YNdmPAh", "signatures": ["ICLR.cc/2026/Conference/Submission16170/Reviewer_GXf4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16170/Reviewer_GXf4"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16170/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997860434, "cdate": 1761997860434, "tmdate": 1762926332706, "mdate": 1762926332706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}