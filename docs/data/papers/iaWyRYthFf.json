{"id": "iaWyRYthFf", "number": 18674, "cdate": 1758289944346, "mdate": 1759897088560, "content": {"title": "Cost-Aware Dynamic Tree Construction for Efficient Large Language Model Inference", "abstract": "Large Language Models (LLMs) face significant inference latency challenges stemming from their autoregressive design and large size. To address this, speculative decoding emerges as a solution, enabling the simultaneous generation and validation of multiple tokens. While recent approaches like EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures, they often neglect the impact of crucial system variables such as GPU devices and batch sizes.\n\nTherefore, we introduce a new dynamic tree decoding approach called CAST that takes into account inference costs, including factors such as GPU configurations and batch sizes, to dynamically refine the tree structure. Through comprehensive experimentation across six diverse tasks and utilizing six distinct LLMs, our methodology demonstrates remarkable results, achieving speeds up to 5.2 times faster than conventional decoding methods. Moreover, it generally outperforms existing state-of-the-art techniques from 5% to 20%.", "tldr": "We propose a new dynamic tree speculative decoding method that leverage the inference cost and achieves improvements against baselines.", "keywords": ["Large Language Models", "Speculative Decoding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/79fdd6fb5e0895623d8e999632a33ce922fab53f.pdf", "supplementary_material": "/attachment/f75c08839b9e590e06bc2f14440ee9e502f582b6.pdf"}, "replies": [{"content": {"summary": {"value": "This paper addresses the latency challenge of autoregressive large language model inference by improving speculative decoding efficiency. Prior dynamic draft-tree methods such as EAGLE-2 and EAGLE-3 adapt token proposal structures, but they overlook system-level factors, including GPU characteristics and batch size. The proposed method models inference cost and dynamically adjusts tree depth, branching width, and token verification count to balance acceptance rates against computation overhead. Experiments demonstrate consistent gains and improvement over previous state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Adeaute related background along the timeline and solid motivation analysis.\n\n- The paper explains its concepts clearly, and the writing is smooth and well-organised.\n\n- The paper presents clear experimental settings and includes a comprehensive set of comparison models."}, "weaknesses": {"value": "- For the conclution \"merely increasing the tree depth and node numbers may not always result in better performance\", is there some quantitative analysis / results to support?\n- The results are mainly presented in table form. Is there a visualization of the performance trends? The paper also mentions a trade-off — are there additional experiments that explore multi-factor trade-offs in more depth? Just like Figure 5 in Appedndix E.1. Maybe more info in the Appendix could be included in the main body and cut off 1 table.\n- 3.2.2 typo \"valuesinterpreted\" \n- The core contribution of this paper lies in incorporating cost into the model’s decision process. However, the work does not provide concrete examples or detailed formulation for this component. Although the paper mentions that GPU hardware characteristics and batch size should be considered, it is necessary to further clarify the modeling procedure—for instance, which GPU parameters are included and how they quantitatively influence the cost function. This aspect is central to the method and requires empirical evidence to validate its effectiveness. Since the high-level idea is relatively straightforward, the design and justification of the cost function should serve as the main technical contribution."}, "questions": {"value": "- General questions are given in the weakness part.\n\n- Is there a theoretical upper bound or lower bound under specific settings? Furthermore, do the experimental results align with the theoretical analysis?\n\n- Can you provide an example of how to represent hardware, such as parameters like memory capacity and FLOPS? Is there a unified quantitative representation for hardware characteristics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gz6LYymySV", "forum": "iaWyRYthFf", "replyto": "iaWyRYthFf", "signatures": ["ICLR.cc/2026/Conference/Submission18674/Reviewer_Aptz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18674/Reviewer_Aptz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786909522, "cdate": 1761786909522, "tmdate": 1762928371315, "mdate": 1762928371315, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the inference latency of llms by improving speculative decoding. The authors argue that existing dynamic tree-structured methods, such as EAGLE-2 and EAGLE-3, are suboptimal because they ignore system-level inference costs, particularly the impact of GPU hardware and batch size. This paper introduces Cost-Aware Speculative Tree, a new dynamic tree approach that explicitly models this trade-off. CAST uses pre-computed cost look-up tables to guide the dynamic construction and reranking of the draft token tree, pruning branches where the computational cost outweighs the expected utility. Experiments across six models and six tasks show that CAST achieves state-of-the-art speedups, outperforming prior methods by 5-20% and autoregressive decoding by up to 5.2x ."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It correctly identifies that SOTA dynamic tree methods ignore critical system costs like batch size and GPU type, which can negate speedups . The proposed cost-utility model, which uses precomputed lookup tables to guide tree construction, is an good solution. \n- A key contribution is the generalization of prior SOTA (EAGLE-2/3), demonstrating they are special cases of this new framework (Theorem 4.1) . The empirical results are strong, showing consistent 5-20% gains over EAGLE-3 and demonstrating superior scalability as batch size increases—a vital metric for production systems."}, "weaknesses": {"value": "- One of the weakness is the reliance on a new set of hyperparameters, specifically the cost thresholds $C_1$, $C_2$, and $C_3$ and the buffer size $R$, whose selection and sensitivity are not discussed or ablated. \n- The method's practicality hinges on pre-computing cost-lookup tables $S_T(B)$ and $S_D(B)$. While practical, the paper does not sufficiently analyze the cost and complexity of this profiling step, which must be run for different hardware and batching configurations. It is unclear how the $select(c)$ approximation for context length impacts the cost model's accuracy."}, "questions": {"value": "- How are the crucial thresholds $C_1, C_2, C_3$ and the FIFO buffer size $R$ determined? Please provide a sensitivity analysis for these new hyperparameters.\n- What is the practical overhead (e.g., in hours) of generating the $S_T(B)$ and $S_D(B)$ lookup tables for a new model and hardware configuration? How sensitive is performance to the accuracy of this precomputed cost model?\n- Algorithm 1 is used for both breadth pruning (Sec 4.1) and reranking (Sec 4.2), but with different cost functions ($c_k^{(i)}$ vs. $c_k$). Can you elaborate on the rationale for using the normalized draft cost for pruning but the normalized target cost for reranking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JI8Q34ErSr", "forum": "iaWyRYthFf", "replyto": "iaWyRYthFf", "signatures": ["ICLR.cc/2026/Conference/Submission18674/Reviewer_9P9F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18674/Reviewer_9P9F"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897257227, "cdate": 1761897257227, "tmdate": 1762928370497, "mdate": 1762928370497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CAST, a dynamic tree-based speculative decoding method that addresses a key limitation in SOTA approaches like EAGLE-3: their failure to account for system-level inference costs. By pre-computing the actual hardware latency for different batch sizes and token counts, CAST reframes tree construction as a cost-utility optimization, dynamically pruning the draft tree's breadth and depth to maximize accepted tokens per unit of time. This cost-aware approach delivers state-of-the-art speedups (up to 5.2x), demonstrating a particularly strong advantage over prior methods in practical, high-throughput batched-inference scenarios where cost-agnostic heuristics fail."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The method replaces prior heuristics with a novel and principled cost-utility framework. This formal optimization of acceptance \"utility\" versus hardware \"cost\" is a more robust and generalizable approach .\n\n2. Comprehensive and Rigorous Experimentation: Claims are exceptionally well-supported by extensive experiments across 6 models, 6 tasks, and 3 different GPU architectures. This thoroughness confirms the method's effectiveness and generality .\n\n3. The authors correctly use \"Speedup Ratio\" as the primary metric and insightfully argue against the misleading \"Average Acceptance Length,\" demonstrating a mature understanding of the evaluation problem. The ablation in Table 3 clearly isolates the individual contributions of the new components (DR, DP, BP), validating the paper's design choices."}, "weaknesses": {"value": "1. Unquantified Profiling Overhead: The method relies on pre-computing cost lookup tables, but the paper never quantifies the one-time profiling cost (e.g., in GPU-hours), which could be a significant practical barrier to adoption.\n\n2. Lack of Hyperparameter Sensitivity Analysis: The new thresholds ($C_1, C_2, C_3$) are critical to the method, but their robustness and the strategy for tuning them are not discussed, leaving a key practical question unanswered.\n\n3. Unclear Intuition for Generalization Claim: The paper claims (Theorem 4.1) that prior work is a \"special case\" of CAST, but the intuition for why a \"cost-agnostic\" method maps to a specific linear cost model is not well-explained."}, "questions": {"value": "1. Cost of Pre-computation: Can the authors quantify the one-time profiling cost (e.g., in GPU-hours) required to generate the cost lookup tables for a single model and hardware setup?\n\n2. Hyperparameter Tuning Strategy: What is the recommended procedure for tuning the thresholds $C_1, C_2,$ and $C_3$, and how sensitive is the method's performance to these values?\n\n3. Cost Model Granularity: What is the performance impact of approximating the context length $c$ using the $select(c)$ function, and how sensitive is the method to this granularity?\n\n4. Intuition for Theorem 4.1: Can the authors provide more intuition for why the cost-agnostic EAGLE-2/3 algorithms are mathematically equivalent to Algorithm 1 with a specific linear cost model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0wE9lzR3N9", "forum": "iaWyRYthFf", "replyto": "iaWyRYthFf", "signatures": ["ICLR.cc/2026/Conference/Submission18674/Reviewer_tm9i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18674/Reviewer_tm9i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940691120, "cdate": 1761940691120, "tmdate": 1762928368828, "mdate": 1762928368828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues existing dynamic tree speculative decoding methods (like EAGLE) are \"cost-agnostic\". They ignore system variables like GPU and batch size, which can paradoxically increase latency. The paper proposes CAST (Cost-Aware Speculative Tree). This method explicitly models these inference costs, often using a lookup table. CAST then dynamically prunes its tree by balancing token utility (acceptance probability) against the actual hardware cost. This cost-aware approach delivers up to 5.2x speedup over standard decoding and outperforms SOTA methods like EAGLE-3 by 5-20% , showing particular strength in batching scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of using utility function to consider resource problem in the speculative decoding setting is novel and motivated. And the paper is well organized.\n\n- The way the paper formulates the utility function based on acceptance rate and how to choose the depth is new.\n\n- The experimental results are comprehensive and convincing. The authors validate their CAST method across a wide array of 6 distinct LLMs and 6 diverse tasks, ranging from multi-turn conversation to code generation."}, "weaknesses": {"value": "- The method introduces precomputation overhead. If the hardware, batching strategy, or even the model (which changes the cost profile) is modified, this entire precomputation step must be redone.\n\n- There are multiple thresholds (at three new cost-utility thresholds) for tuning. What are the overheads?\n\n- The paper considers batch size as a factor and motivation but lacks more comprehensive experiments for that.\n\n- Theorem 4.1: Try to make it self-contained. What does j mean in the formula c_j. And also it is not clear what $\\lambda, \\delta$ means."}, "questions": {"value": "- What is the difference between c and n in Section 4?\n\nSee weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V6FBDcXEhC", "forum": "iaWyRYthFf", "replyto": "iaWyRYthFf", "signatures": ["ICLR.cc/2026/Conference/Submission18674/Reviewer_ZSu1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18674/Reviewer_ZSu1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18674/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978242695, "cdate": 1761978242695, "tmdate": 1762928368390, "mdate": 1762928368390, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}