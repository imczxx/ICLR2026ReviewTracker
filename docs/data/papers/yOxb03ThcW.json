{"id": "yOxb03ThcW", "number": 24718, "cdate": 1758359618469, "mdate": 1759896752840, "content": {"title": "Polyglot-R1: Reinforcement Learning for Multilingual Multi-Perspective Reasoning in LLMs", "abstract": "Multilingual reasoning has recently emerged as a powerful strategy for extending the reach and impact of large language models (LLMs). By enabling models to operate effectively across diverse languages and modalities, it broadens access to advanced reasoning capabilities for a wider range of users and linguistic communities. Yet reliably activating such behaviours through training remains difficult. Existing approaches rely heavily on supervised fine-tuning over synthetic data, which tends to encourage imitation of teacher signals rather than genuine exploration or robust generalisation.\nTo address this gap, we introduce, we propose \\textbf{Polyglot-R1}, the first reinforcement learning framework designed to cultivate multilingual, multi-perspective reasoning behaviours for complex, real-world tasks. Our framework introduces a progressive curriculum that directly tackles the cold-start problem in training with reinforcement learning. We begin with supervised fine-tuning on trajectories from more straightforward multilingual prompts to instil the foundations of this reasoning style. We then transition to reinforcement learning, enabling the model to actively explore and generalise this skill on more challenging multilingual and multimodal problems. Experiments demonstrate that Polyglot-R1 not only improves accuracy but also reshapes the way models reason. At earlier stages of training, multilingual reasoning functions as an exploration strategy, encouraging the model to test diverse lines of thought. At later stages, the same capacity is repurposed as a mechanism for multi-perspective verification, strengthening confidence in the final answer. Most importantly, we validate multilingual reasoning as an intermediate exploration scaffold: a temporary but crucial phase that unlocks more robust, transferable reasoning capabilities across languages.", "tldr": "", "keywords": ["LLMs", "Reasoning", "Multilingual"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ff1f2607129557c395915f83eb859a8463e5984.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Polyglot-R1, a reinforcement learning framework designed to cultivate multilingual, multi-perspective reasoning in LLMs"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tExploring multilingual reasoning + RL."}, "weaknesses": {"value": "1.\tThe models involved are too small (3B and 4B). \n2.\tSince the models involved are all “base version”, it is unclear how inference and evaluation were conducted, zero-shot? few-shot? or with specific prompting strategies? The “base version” models are not good at following instructions, so the standard inference and evaluation are inapplicable. This likely underestimates baseline performance: for instance, Qwen3-4B-Base achieves only 3.2 mean@16 accuracy on MGSM-Symbolic and 7.5 on MSVAMP, which are unreasonably low and raise concerns about evaluation validity.\n3.\tSince this paper needs a cold-start stage, why don’t you just experiment on “Qwen3-4B” instead of choosing the basic version? Is that too difficult to surpass the vanilla model? Cause Qwen3-4B can achieve almost 90 acc on MSVAMP.\n4.\tThe computational cost is not quantitatively discussed, which is important.\n5.\tWhile ablations are present, the separation between “causal” and “structured” variants could be explained more clearly, especially regarding when architectural bias helps or hurts."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4kaxi67lMf", "forum": "yOxb03ThcW", "replyto": "yOxb03ThcW", "signatures": ["ICLR.cc/2026/Conference/Submission24718/Reviewer_iD32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24718/Reviewer_iD32"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761394522825, "cdate": 1761394522825, "tmdate": 1762943173670, "mdate": 1762943173670, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Polyglot-R1, a two-stage curriculum that first cold-starts a model with SFT on structurally simple multilingual parallel traces and then continues training with RL (GRPO-based) to elicit genuinely multilingual, multi-perspective reasoning on harder multilingual/multimodal tasks. The authors argue that multilingual reasoning initially acts as an exploration device and later becomes a verification mechanism, leading to better accuracy on multilingual GSM-style and math benchmarks compared with SFT-only and RL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem setting on how to make RL-style reasoning work in a multilingual, multi-path regime rather than just in English is timely, and the paper gives a coherent training story that directly targets the “no structured multilingual traces for RL” bottleneck.\n2. The empirical analysis of when multilingual parallel blocks appear (early for exploration, later for verification) is interesting and aligns well with the proposed “multilingual reasoning as mid-training scaffold” narrative, giving the method some explanatory depth beyond raw scores."}, "weaknesses": {"value": "1. The novelty claim (“the first RL framework for multilingual multi-perspective reasoning”) is not fully separated from closely contemporary RL-for-parallel-reasoning work (e.g., Parallel-R1), and many of the components—GRPO, alternating structure/accuracy rewards, curriculum from easy to hard—are adaptations rather than clearly new algorithmic contributions. The paper should make clearer what Polyglot-R1 can do that those RL baselines cannot.\n2. The experimental validation is relatively narrow: results are shown only on small backbones (Qwen-3-4B, Llama-3-3B) and on a limited set of multilingual math/reasoning suites (MAIME25, MGSM-Symbolic, MSVAMP), so it is hard to tell whether the gains survive in truly low-resource, code-mixed, or non-math settings, or at larger scales. This weakens the generality of the “transferable capacity” claim.\n3. The cold-start data pipeline is central to the story, but it is described mostly procedurally (zero-shot prompts, format check with <Parallel>/<Path>/<Summary>) without reporting size, language distribution, pass rate, or cost, so readers cannot judge whether the approach is actually “lightweight” compared with existing SFT-heavy multilingual reasoning pipelines. Sections 2.2 and 2.4 are also somewhat repetitive here.\n4. The proposed reward design is quite hand-tuned (accuracy + structure + diversity, alternating schedule, different schedules for causal vs. structured variants), and the results show that the structured variant is sensitive and can even degrade when Stage 1 RL is added; this suggests the method may be brittle and dataset-specific, but the paper does not study failure modes or robustness to other reward mixes."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Wy5YMMJbf", "forum": "yOxb03ThcW", "replyto": "yOxb03ThcW", "signatures": ["ICLR.cc/2026/Conference/Submission24718/Reviewer_skmf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24718/Reviewer_skmf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761869158618, "cdate": 1761869158618, "tmdate": 1762943173460, "mdate": 1762943173460, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents Polyglot-R1, a reinforcement learning framework for developing multilingual, multi-perspective reasoning in large language models (LLMs). The approach follows a progressive curriculum that begins with supervised fine-tuning (SFT) on simple multilingual tasks, then transitions to reinforcement learning (RL) on more complex multilingual and multimodal problems. The authors also design a special reward design balancing accuracy, reasoning structure, and linguistic diversity. Experimental results on multilingual reasoning benchmarks (MAIME25, MGSM-Symbolic, MSVAMP) using the Qwen-3-4B model indicate moderate gains over SFT baselines. The paper further finds that multilingual reasoning evolves from early exploration to a later verification."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes using reinforcement learning (GRPO) to improve LLMs’ multilingual reasoning capabilities through a two-stage training strategy, which consists of a cold-start phase followed by a reinforcement learning phase.\n2. The authors design composite rewards that encourage both parallel reasoning and linguistic diversity, aiming to help improve multilingual reasoning behaviors.\n3. The proposed method show performance improvements over the Qwen3-4B-Base model on several multilingual reasoning benchmarks."}, "weaknesses": {"value": "1. The paper mostly follows existing ideas, i.e., curriculum GRPO + parallel reasoning prompts, as multilingual reasoning scaffolds. There is little insight explaining the design choices and why multilingual reasoning should improve exploration.\n2. The experiments are shallow and limited. Only medium-scale models and a few benchmark datasets are tested, with unclear performance gains. Additional ablation studies on multilinguality are necessary.\n3. The analysis relies solely on quantitative benchmarks, without qualitative or behavioral evidence that the model performs genuine multi-perspective reasoning rather than merely replicating patterned prompts.\n4. The overall presentation quality is not good, with issues in clarity and organization, that require substantial revision."}, "questions": {"value": "- The authors should verify the multilingual reasoning paths to reflect meaningful and diverse reasoning.\n- The paper could provide more justification and analysis of the reward design, including the selection of key hyperparameters.\n- It is better to incorporate human evaluations for the reasoning examination."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7xg0mMIzHE", "forum": "yOxb03ThcW", "replyto": "yOxb03ThcW", "signatures": ["ICLR.cc/2026/Conference/Submission24718/Reviewer_oxY6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24718/Reviewer_oxY6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894478552, "cdate": 1761894478552, "tmdate": 1762943173266, "mdate": 1762943173266, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Polyglot-R1, an SFT+RL framework designed to cultivate multilingual, multi-perspective reasoning capabilities in LLMs. Recognizing that existing SFT approaches often lead to mere imitation of synthetic reasoning data rather than genuine exploration, the authors introduce a progressive training curriculum: first, a cold-start SFT phase on simple multilingual reasoning trajectories to establish foundational reasoning structures; then, a transition to RL on more complex multilingual tasks. The core idea is to guide the model to reason in parallel across multiple languages or perspectives, synthesize insights from these diverse paths, and produce a final, well-justified answer. Experiments on multiple mathematical reasoning benchmarks demonstrate that Polyglot-R1 effectively improves accuracy. Further analysis reveals a evolution in reasoning behavior: during early training, multilingual reasoning serves as an exploratory mechanism that encourages cross-linguistic diversification; in later stages, it shifts toward a verification-oriented strategy, consolidating conclusions through multi-perspective consistency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core strength of this work lies in its novel training framing of multilingualism as a reasoning method. \n2. A intrestiing empirical evidence of reasoning behavior evolution, that multilingual reasoning serves as an exploratory mechanism during early training to verification strategy in later stage."}, "weaknesses": {"value": "1. **Incomplete related work coverage**: The paper overlooks recent relevant studies that also utilize multilingual chain-of-thought reasoning to explore stronger reasoning performance, such as  \n   - *Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning Across Languages*  \n   - *AutoCAP: Towards Automatic Cross-lingual Alignment Planning for Zero-shot Chain-of-Thought*.  \n   These works demonstrate that test-time multilingual prompting alone (without any training) can enhance reasoning, raising questions about the necessity of the proposed training pipeline.\n\n2. **Limited task generality**: All experiments focus on mathematical reasoning benchmarks (e.g., MGSM, AIME). It is unknown whether the benefits of multilingual reasoning method generalize to other domains such as commonsense reasoning.\n\n3. **Training complexity vs. simplicity of alternatives**: The proposed **two-stage training process (SFT + RL)** is computationally intensive and requires careful reward scheduling. In contrast, zero-shot multilingual CoT methods (e.g., mentioned works in Weekness 1) achieve performance gains **without any training**, making them far more practical for real-world deployment.\n\n4. **Potential redundancy of the SFT stage**: Given the strong instruction-following capabilities of modern models like Qwen3-8B base or Instruct model, it is plausible that the model could already generate valid `<Parallel>`-structured outputs without the cold-start SFT phase, e.g multilingual DAPO method. If true, the first training stage—and its associated cost—could be eliminated, significantly improving the method’s flexibility and accessibility."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "66TNGD2xe3", "forum": "yOxb03ThcW", "replyto": "yOxb03ThcW", "signatures": ["ICLR.cc/2026/Conference/Submission24718/Reviewer_MrSd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24718/Reviewer_MrSd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24718/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762088900428, "cdate": 1762088900428, "tmdate": 1762943173094, "mdate": 1762943173094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}