{"id": "ai3LYEFUGn", "number": 13779, "cdate": 1758222422290, "mdate": 1759897413284, "content": {"title": "Unsupervised learning of disentangled representations via diffusion variational autoencoders", "abstract": "We present the diffusion variational autoencoder (DiVA), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, DiVA formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in synthetic datasets, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, DiVA can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.", "tldr": "", "keywords": ["unsupervised learning", "diffusion models", "variational autoencoders", "disentangling", "ELBO"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7f347b6dddf9804d3133ec347dace6b2d54f501.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents DiVA, a diffusion variational autoencoder for disentangled representation learning by combining a diffusion model with a VAE. The training loss is formalized under a unified ELBO objective."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- A novel approach that combines VAEs with diffusion-based models.\n- The new objective formalized of the ELBO.\n- Good results on the selected benchmarks and datasets."}, "weaknesses": {"value": "**Major:**\n- Unorganized results and a lack of detailed reporting in several sections. \n- Lack of comprehensive benchmarks on the suggested datasets.\n- Some experiments are missing important implementation and protocol details.\n\n**Minor:**\n- Limited discussion of related literature on sequential disentanglement for video trajectories."}, "questions": {"value": "1. **In results in Synthetic Disks dataset:** a. Why the authors report MSE for these results? Unconditional generations trending toward noise could artificially lower MSE; please justify this choice or include complementary metrics. \nb. Why not use 2D shapes dataset (e.g., those used in β-VAE) for comparability?\n\n2. **In results CelebA dataset:** a. Lines 321–323: what was the reduction in MSE? b. Why don’t you refer to the DBAE + TC results in Table 3 directly within this section?\n\n3. Why the images are gray in Figure 3?\n\n4. **Feature extraction for pre-trained diffusion models:** a. Can the authors show any additional results for this experiment? Also, please add more details about the fine-tuning protocol.\nb. What are the pros and cons of using pre-trained backbones versus training from scratch?\nc. Does this approach achieve results similar to joint training? By how much does it reduce training time?\n\n5. **In results Encoding of video trajectories:** \na. There is a rich literature on sequential disentanglement that isn’t cited, such as [1] and [2].\nb. Do you have any results demonstrating disentanglement on CelebA-HQ?\nc. What is the motivation for using cosine similarity? How does it support the hypothesis in lines 438–440?\nd. Why do you only compare against DiffAE?\ne. Why the authors do not include benchmarks on a video dataset with labels for static and dynamic factors (e.g., Moving dSprites or related variants)?\n\n**References**\n\n[1] \"Sequential Representation Learning via Static-Dynamic Conditional Disentanglement\" M. Cyrille Simon et al.\n\n[2] \"Sequential Disentanglement by Extracting Static Information From a Single Sequence Element\" N. Berman et al."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J4359PTbtR", "forum": "ai3LYEFUGn", "replyto": "ai3LYEFUGn", "signatures": ["ICLR.cc/2026/Conference/Submission13779/Reviewer_qY7s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13779/Reviewer_qY7s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892718963, "cdate": 1761892718963, "tmdate": 1762924313204, "mdate": 1762924313204, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to condition diffusion models on a learned latent representation $z$. Using this new formulation, the authors derive a novel learning objective where the second term is equivalent to VAEs’ regularisation term. As in $\\beta$-VAE, then add a scalar $\\beta$ to this term to weight the regularisation and induce sparsity. The reconstruction and disentanglement abilities of the model are then evaluated on a synthetic disk dataset and on CelebA."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well written and easy to follow\n- The idea of learning meaningful latents with diffusion models is appealing"}, "weaknesses": {"value": "My main concern is about the disentanglement claim. The authors attempt to replicate $\\beta$-VAE behaviour using a $\\beta$ term in the second term of Eq. 5, but in the experimental section, disentanglement is not measured with any disentanglement metrics (e.g., MIG, DCI or any listed in [1]).  Furthermore, several datasets have been used to benchmark disentanglement (e.g., DSprites, SmallNorb, etc. See [1] or [2] for more examples) but apart from CelebA none of these are used here. We know from [1] that in VAEs, disentanglement capacity varies a lot depending on the dataset, so one would expect a more expansive evaluation when the authors state that their model can \"recover ground truth factors\". Especially given that this is not so clear cut for $\\beta$ VAE or any VAE doing disentanglement. These models tend to induce sparsity with a PCA-like behaviour [3-6], and one can obtain disentangled representations if those PCs correspond to ground truth factors. Overall, I think this paper is interesting, but would need a significant rework of the empirical section to justify the disentanglement statement."}, "questions": {"value": "- The proposed model disentanglement capacity should be evaluated using disentanglement metrics (see [1])\n- The evaluation should be done on several disentanglement dataset (see [1-2])\n-  Could the authors discuss the relationship between the proposed model and diffusion models being a special case of hierarchical markov VAE, as shown in [7]?\n- Would this new formulation allow for other disentanglement techniques than $\\beta$-VAE?\n- I suggest the authors avoid saying that their model \"recovers ground truth generative factor\" as most disentanglement models cannot reliably do this (see [1])\n- The name of the model is quite confusing, given the naming of these two previous works [8-9], and may need to be updated\n\nReferences\n=========\n[1] Locatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\" international conference on machine learning. PMLR, 2019. (limitations of disentanglement)\n\n[2] Gondal, M. W., Wuthrich, M., Miladinovic, D., Locatello, F., Breidt, M., Volchkov, V., ... & Bauer, S. (2019). On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset. Advances in Neural Information Processing Systems, 32.\n\n[3] Dai, B. et al. \"Connections with robust PCA and the role of emergent sparsity in variational autoencoder models.\" Journal of Machine Learning Research 19.41 (2018): 1-42.\n\n[4] Bin Dai, & David Wipf (2019). Diagnosing and Enhancing VAE Models. In International Conference on Learning Representations.\n\n[5] Rolinek, M., Zietlow, D., & Martius, G. (2019). Variational autoencoders pursue pca directions (by accident). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12406-12415).\n\n[6] Bonheme, Lisa, and Marek Grzes. \"Be more active! understanding the differences between mean and sampled representations of variational autoencoders.\" Journal of Machine Learning Research 24.324 (2023): 1-30.\n\n[7] Luo, C. (2022). Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970. (equivalence between diffusion models and HMVAEs)\n\n[8] Ilse, Maximilian, et al. \"Diva: Domain invariant variational autoencoders.\" Medical Imaging with Deep Learning. PMLR, 2020. (DiVA confusing name 1)\n\n[9] Perez R. et al. (2020). Diffusion Variational Autoencoders. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, ĲCAI-20 (pp. 2704–2710). International Joint Conferences on Artificial Intelligence Organization. (DIVA confusing name 2)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cBBlepG1I4", "forum": "ai3LYEFUGn", "replyto": "ai3LYEFUGn", "signatures": ["ICLR.cc/2026/Conference/Submission13779/Reviewer_p7dT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13779/Reviewer_p7dT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918616587, "cdate": 1761918616587, "tmdate": 1762924312644, "mdate": 1762924312644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work basically propose to learn a latent variable model along with the standard diffusion objective. By some algebra, the overall loss can be written in form of a guidance term and can be jointly trained with the unconditional score using denoising score matching plus some KL penalty on gaussian prior. It is demonstrated that the learned latent model can encode disentangled representation of the data distribution."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Combining VAE loss with standard denoising score matching to learn meaningful representation is interesting."}, "weaknesses": {"value": "1. The proposed method is conceptually and methodologically similar to the DiffAE paper. It is unclear why DiVA performs better. It is argued this is because DiVA minimizes the exact ELBO, which is not true. In the algorithm, the weights $\\lambda_t$ is not included, implying DiVA only minimizes ELBO approximately. My question is, what is the unique advantage of DIVA? \n\n2. The trained model has to take a clean image $x_0$ as conditional input, which limits its generation capability. In figure 3 B, I would say when you condition on $x_0$, the generated images seem to be identical to it, with only minor difference. This is a sign of overfitting. Will DiVA generate high quality image when a clean image is not available? How do you calculate the FID in Table 3? Do you generate each image by input a clean image, or are the images generated unconditionally? \n\n3. Can you provide more theoretical analysis on objective (10)? For example, what would be the optimal $q_{\\phi}$ that minimizes this loss? Can you come up with some more in depth characterization of $q_{\\phi}$'s property? Currently, the objective makes sense intuitively, but is kind of superficial in my opinion, as there is no theoretical guarantee that $q_{\\phi}$ can capture disentangled representations. If it indeed does, why? Does it work well consistently on different dataset, or it only works on simple dataset like faces and disks?\n\n4. Experiments are limited to simple dataset such as disks and faces. Please perform experiments on ImageNet to fully demonstrate the strength of your approach. I am not convinced if only experiments on faces and disks are provided, as nowadays, these datasets are considered too simple.\n\n5. What is the current state of the art methods for learning disentangled representations besides the ones based on diffusion models? Does DiVA beat those algorithms? Is it really necessary to learn disentangled representation based on diffusion framework? If so, why? What is the unique advantage of diffusion in this context, from a rigorous theoretical perspective?"}, "questions": {"value": "See my questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1mmL144eY2", "forum": "ai3LYEFUGn", "replyto": "ai3LYEFUGn", "signatures": ["ICLR.cc/2026/Conference/Submission13779/Reviewer_12xU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13779/Reviewer_12xU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942651237, "cdate": 1761942651237, "tmdate": 1762924312022, "mdate": 1762924312022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an unsupervised representation disentanglement model, the Diffusion Variational Autoencoder (DiVA). Specifically, DiVA integrates variational autoencoders and diffusion models to enable unsupervised learning of structured and interpretable latent representations with strong factorization and semantic consistency, while maintaining high-quality generative performance. The proposed model is evaluated on both synthetic and real-world datasets and compared against several baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses the problem of learning disentangled representations for image data, which is an important and long-standing research topic. \n2. The idea of combining the advantages of diffusion models and VAEs is conceptually clear and technically sound. \n3. The paper provides a detailed analysis of the ELBO formulation."}, "weaknesses": {"value": "1. The comparison between the proposed model and baseline methods, particularly diffusion-based disentanglement models, is unclear and difficult to follow. The key results in Appendix A.6–A.7 are important and should be highlighted in the main text. Moreover, no qualitative examples or visual comparisons are provided to illustrate the superiority of the proposed model over recent baselines such as InfoDiff, DisDiff, and DBAE+TC. \n2. The InfoDiffusion model appears highly relevant, as it also includes ELBO analysis and mutual information regularization, but the differences between the two approaches are not systematically discussed. \n3. The evaluation is limited to one synthetic and one real-world dataset. Given that multiple public datasets with ground-truth disentanglement factors (e.g., 3DShapes, dSprites, etc) and commonly used real-world datasets (e.g., CelebA, FFHQ, etc) are available, the experimental validation seems insufficient."}, "questions": {"value": "Please see \"Weaknesses\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BlDMI0v4u8", "forum": "ai3LYEFUGn", "replyto": "ai3LYEFUGn", "signatures": ["ICLR.cc/2026/Conference/Submission13779/Reviewer_2F6a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13779/Reviewer_2F6a"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962266419, "cdate": 1761962266419, "tmdate": 1762924311410, "mdate": 1762924311410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}