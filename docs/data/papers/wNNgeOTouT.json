{"id": "wNNgeOTouT", "number": 1510, "cdate": 1756888494626, "mdate": 1759898205291, "content": {"title": "One-for-All Model Initialization with Frequency-Domain Knowledge", "abstract": "Transferring knowledge by fine-tuning large-scale pre-trained networks has become a standard paradigm, yet the knowledge of pre-trained model is tightly coupled with monolithic architecture, which restricts flexible reuse across models of varying scales.\nIn response to this challenge, recent approaches typically resort to either parameter selection, which fails to capture the interdependent structure of this knowledge, or parameter prediction using generative models that depend on impractical access to large network collections. In this paper, we empirically demonstrate that a model's foundational, task-agnostic knowledge -- its \"learngene\" -- is encoded within the low-frequency components of its weights, and can be inherited efficiently by downstream models.\nBased on this insight, we propose FRONT (FRequency dOmain kNowledge Transfer), a novel framework that uses the Discrete Cosine Transform (DCT) to isolate the low-frequency \"learngene\". This learngene can be seamlessly adapted to initialize models of arbitrary size via simple truncation or padding, a process that is entirely training-free. For enhanced performance, we propose an optional low-cost refinement process that introduces a spectral regularizer to further improve the learngene's transferability. Extensive experiments show that FRONT achieves the state-of-the-art performance, accelerates convergence by up to 15✖ in vision tasks, and reduces training FLOPs by an average of 40.5\\% in language tasks.", "tldr": "", "keywords": ["Model Initialization", "Variable-Sized Models", "Frequency Domain Knowledge Transfer"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c442ab3003737459fb4628e7f94d32ccb043ee3e.pdf", "supplementary_material": "/attachment/b3b331c0aee0d6788c47fd832a75c9adc5690cdb.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes FRONT, a frequency-domain method for initializing neural networks by extracting low-frequency components (via Discrete Cosine Transform, DCT) from pretrained weights. The authors argue that these components—termed “learngenes”—capture task- and architecture-agnostic knowledge, allowing models of various sizes (e.g., different depths or widths) to inherit such knowledge through a training-free process. They also propose FRONT+, which introduces a spectral regularization term to refine these learngenes through a brief fine-tuning process. The authors report experimental performance across: vision models (DeiT, ResNet) and language models (BERT, RoBERTa, GPT2) via various downstream datasets (classification, detection, segmentation, GLUE benchmark). They claim up to 15× faster convergence and 40% less FLOPs compared to training from scratch."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The use of DCT to extract transferable low-frequency components for cross-architecture initialization is interesting. It provides a fresh perspective on model reuse and transfer learning.\n2. The paper offers experiments on both vision and language domains, demonstrating broad applicability and consistent improvements. \n3. The proposed method operationalizes the abstract “learngene” concept in a concrete, reproducible way—turning a theoretical notion into a working initialization strategy."}, "weaknesses": {"value": "1. It lacks rigorous theoretical analysis to support why low-frequency components encode general knowledge. The claim that low-frequency weights correspond to “learngenes” remains speculative.\n2. Unlike images, there is no inherent spatial ordering of weight indices. Applying DCT assumes a kind of smoothness across indices that is not theoretically justified.\n3. The paper doesn't show that low-frequency weights correspond to smoother or more general representations in the activation space.\n4. Although the authors test across multiple datasets, the analysis lacks examination of negative cases—when and why the method fails. There's also little discussion on transfer to fundamentally different architectures."}, "questions": {"value": "1. Why DCT rather than Fourier, Wavelet, PCA, or SVD? The authors only cite DCT’s “energy compaction” property from image compression.\n2. What is the definition of high/low frequency in weight space?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RG0bG3wb9i", "forum": "wNNgeOTouT", "replyto": "wNNgeOTouT", "signatures": ["ICLR.cc/2026/Conference/Submission1510/Reviewer_1adk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1510/Reviewer_1adk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760911768929, "cdate": 1760911768929, "tmdate": 1762915787884, "mdate": 1762915787884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FRONT, a framework that extracts task-agnostic knowledge from pre-trained models by decomposing weights into the frequency domain via DCT and isolating low-frequency components as \"learngenes\" for initializing models of different sizes. The key empirical observation (Figure 1) shows that low-frequency components remain stable across different model scales and downstream tasks, while high-frequency components are volatile and task-specific. Two variants are proposed: FRONT for direct zero-cost extraction, and FRONT+ with frequency regularization for refinement. Extensive experiments on vision and language tasks demonstrate substantial improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The concrete instantiation of learngene as low-frequency components is intuitive and creative, with convincing evidence in Figure 1 demonstrating stability of low-frequency components across models and tasks.\n\n2.  FRONT's zero-cost extraction and flexible padding/truncation mechanism make it substantially more practical than training-based methods like GHN-3 and WAVE.\n\n3. The evaluation spans ViT/ResNet/MLP/CNN architectures, multiple datasets, both vision and language domains, and systematic ablations (Figure 5, Table 11) that strengthen the empirical claims."}, "weaknesses": {"value": "1. The frequency ratio r varies by model size (2.2M/3.2M/13.0M for Ti/S/B in Table 1) without principled justification, suggesting $r$ is model-size dependent. This systematic issue is not explored, and hyperparameters like decay rates $γ_d$ in Eq. 6 lack principled selection guidelines.\n\n2. When comparing with training-based methods (WAVE/TLEG), FRONT+ also requires 150 epochs of training, so these should be evaluated separately from FRONT's direct extraction.\n\n3. In Table 3, FRONT occasionally underperforms LiGO (e.g., Flowers: 92.9 vs 94.2), indicating instability; Tables 4-5 show large improvements on detection/segmentation but lack direct comparison with other initialization methods beyond random initialization.\n\n4. Applying 3D-DCT across layer/input/output dimensions (mixing different semantic meanings) without per-layer processing warrants explanation—why not apply DCT separately to each layer? \n\n5. Evaluation only covers homomorphic scaling (BERT-B→BERT-S) without heteromorphic transfer (e.g., BERT→GPT)"}, "questions": {"value": "1. Why do low-frequency components specifically encode task-agnostic knowledge, and why DCT over other transforms like Fourier or wavelets? Figure 1 provides only empirical observation, not principled justification.\n\n2. Why does LiGO fail (Table 1, \"/\") without explanation, and why is knowledge distillation missing as a baseline in vision tasks despite being used for language tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WElxMf6Hd2", "forum": "wNNgeOTouT", "replyto": "wNNgeOTouT", "signatures": ["ICLR.cc/2026/Conference/Submission1510/Reviewer_nvmv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1510/Reviewer_nvmv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447289865, "cdate": 1761447289865, "tmdate": 1762915787650, "mdate": 1762915787650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FRONT, a training-free initializer that treats a pretrained network’s weights in the frequency domain. It applies a 3D-DCT to each weight tensor, keeps only the low-frequency coefficients as the compact “learngene,” and reconstructs target-size weights for new models by simple zero-padding/truncation and IDCT. An optional FRONT+ step lightly fine-tunes a source model with a spectral regularizer to make those low-frequency components even more transferable. Experiments show faster convergence and substantial compute savings on different tasks and models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method extracts a low-frequency learngene and uses padding or truncating to initialize a variety of models across ViT and CNN. It generalizes well across different depths and width, with minimal computation needed.\n\n- The proposed method speeds up convergence and cuts compute versus scratch or learned-transform baselines."}, "weaknesses": {"value": "- The motivation behind the design is unclear. Why stacking weights across layers and then conduct 3D DCT, what if do this process on 2D weights and then use some selective process to get the learngene?\n\n- The presentation of the experimental results is not that clear, and the experimental settings are concernable. For instance, in table 1, it’s unclear to see what’s the base model in each block is used for initialization? And the results reported in the way of 10-epoch accuracy is not optimal. It should report the final accuracy with the number of epochs of convergence. I would expect a much faster convergence rate of the proposed method versus trivial initialization.\n\n- Results not much improvement over WAVE in Table 1,2,3. Also, why the convergence rate of the proposed initialization method that uses pre-trained knowledge does not show notable advantages over traditional methods?\n\n- Lack ablation studies on deciding the ratio $r$."}, "questions": {"value": "- What if the architecture is different? For example, the transferring standard attention block to the parallel attention block in [1]?\n\n- There are tons of pre-trained models in the model zoo, any principles to select one as the learngene to initialize future trianing?\n\n- What’s the design choice of using DCT, what about DFT, DWT and other basis?\n\n\n\n1.\tDehghani, Mostafa, et al. \"Scaling vision transformers to 22 billion parameters.\" International conference on machine learning. PMLR, 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5N9yjDhglb", "forum": "wNNgeOTouT", "replyto": "wNNgeOTouT", "signatures": ["ICLR.cc/2026/Conference/Submission1510/Reviewer_PMdN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1510/Reviewer_PMdN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886901301, "cdate": 1761886901301, "tmdate": 1762915787533, "mdate": 1762915787533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free learngene paradigm and demonstrates that a model’s fundamental, task-agnostic knowledge is encoded in the low-frequency components of its weights and can be effectively inherited by downstream models. Building on this, it introduces FRONT (Frequency domain Knowledge Transfer), a framework that accelerates model convergence."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The motivation of the paper is clear, and the writing is generally well-structured.\n2.The paper provides evidence that task-agnostic knowledge resides in a model’s low-frequency components—an intuitively plausible and insightful finding. It also instantiates the learngene concept as low-frequency representations that can be readily extracted from the model.\n3.The experiments are generally thorough and demonstrate the effectiveness of the proposed method."}, "weaknesses": {"value": "Please refer to the Questions section below."}, "questions": {"value": "1.The paper only provides empirical evidence for the knowledge-carrying role of low-frequency components; it appears to lack theoretical support for the claim that “low-frequency components encode task-agnostic knowledge.”\n2.FRONT+ enhances low-frequency knowledge by suppressing high-frequency components. But are high-frequency components entirely without transfer value? For example, between similar tasks (e.g., image classification and fine-grained classification), might high-frequency components carry reusable fine-detail information? It would be helpful to further analyze the potential role of high-frequency components.\n3.The adaptation of learngene is implemented solely via “truncation / zero padding,” without considering how architectural differences between the source and target models (e.g., differing numbers of Transformer layers or CNN convolutional kernels) affect knowledge mapping. For instance, when the target model has many more layers than the source model, could zero-padded high-frequency regions introduce invalid information and adversely impact model initialization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MwobiEfn2e", "forum": "wNNgeOTouT", "replyto": "wNNgeOTouT", "signatures": ["ICLR.cc/2026/Conference/Submission1510/Reviewer_DrRR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1510/Reviewer_DrRR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1510/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762328753219, "cdate": 1762328753219, "tmdate": 1762915787384, "mdate": 1762915787384, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}