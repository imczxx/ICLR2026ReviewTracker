{"id": "9z9mgVpXyE", "number": 5753, "cdate": 1757932060140, "mdate": 1759897956493, "content": {"title": "Positional Encoding for Spiking Transformers", "abstract": "Spiking Neural Networks (SNNs) offer superior energy efficiency compared to Artificial Neural Networks (ANNs). Recent Transformer-based SNNs have achieved promising performance by integrating spike-driven computation with Transformer architectures.\nPositional information is essential in sequential tasks. However, existing positional encoding methods designed for ANNs cannot be directly applied to SNNs, as they interfere with the spike-driven computation paradigm, highlighting the need for SNN-specific solutions.\nWe propose Spiking Positional Encoding (SPE), a novel positional encoding specifically designed for Spiking Transformers that captures both absolute and relative positional information. Its key component is the Positional Encoding Leaky Integrate-and-Fire (PE-LIF) neuron layer, which encodes positional information directly into neuron thresholds. Through continuous spike firing and membrane potential reset processes, this positional information is effectively reflected in the emitted spikes while preserving the spike-driven computation paradigm.\nComprehensive experiments across seven datasets, including three time-series forecasting tasks and four natural language processing benchmarks, demonstrate that SPE consistently outperforms existing positional encoding methods and achieves state-of-the-art performance.\nSPE provides a tailored positional encoding solution for Spiking Transformers, bridging the performance gap between ANNs and SNNs, thus advancing neuromorphic computing applications in sequential modeling tasks.", "tldr": "", "keywords": ["Spiking Neural Networks", "Spiking Self-attention", "Spiking Transformers", "Positional Encoding"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5a26f1c8e2e568cabdb8b77b1793ad41665673f2.pdf", "supplementary_material": "/attachment/e54ee15c67ef2693fb7a7c1149bf1ea024d31229.zip"}, "replies": [{"content": {"summary": {"value": "The authors address the lack of absolute and relative positional encoding in Spike Transformers and, drawing inspiration from positional encoding designs in the ANN domain, propose a set of design principles for positional encoding in SNNs. Based on these principles, they design a novel neuron model, PE-LIF, which integrates absolute positional information into its threshold dynamics. Theoretical analysis further demonstrates that this neuron model, while encoding absolute positional information, is also capable of representing relative positional information. Moreover, the authors introduce an MPR loss to maintain the theoretical assumptions. Finally, extensive experimental results are presented to validate the effectiveness of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is clear and well-defined, directly targeting a major pain point in the current Spike Transformer research — the lack of positional encoding, which in turn limits the effectiveness of SNNs in sequential modeling.\n\n2. The paper provides thorough analysis and well-founded theoretical proofs, accompanied by comprehensive experiments that validate the proposed method’s effectiveness across multiple datasets."}, "weaknesses": {"value": "1. The letter notations in the paper are somewhat inconsistent. In Section 3.1, t denotes the time step, N denotes the number of tokens, and D represents the dimension; in Section 4.3, L refers to the number of PE-LIF neuron layers; yet in Table 2, L is again used to represent the input length.\n\n2. The novelty of this paper remains open to question. Although the problem it addresses is indeed an important one in the SNN field, the proposed method is largely based on RoPE — both in terms of the underlying formulation and the way absolute positional encoding is extended to relative positional encoding through the attention mechanism. Moreover, there is no ablation study specifically analyzing the effect of the newly proposed MPR loss; only a comparison of one related metric is provided, and the improvement is not particularly significant. Overall, the work appears to focus primarily on the PE-LIF neuron design derived from RoPE, rather than introducing a fundamentally new mechanism.\n\n3. The remaining details can be found in the Questions section below."}, "questions": {"value": "1. In Proposition 2, the authors state that as the relative distance between tokens increases, their dependency weakens and eventually approaches zero, thereby suggesting that the proposed encoding method enables dependency decay with distance. However, wouldn’t proving monotonicity or approximate monotonicity better demonstrate this characteristic?\n\n2. In Proposition 1, the authors introduce a hypothesis, and in Section 4.2 they incorporate the MPR loss to ensure that this hypothesis is approximately satisfied during training. Could the authors visualize the variation of MPR loss throughout the training process?\n\n3. According to Figure 4, the introduction of MPR loss seems to have little impact on the R² and RSE losses. Have the authors conducted an ablation study where MPR loss is removed and only PE-LIF is considered?\n\n4. In this paper, the variable T used during sequence modeling with SNNs appears to be obtained through repetitive encoding. Although such repeated encoding is widely used in image processing, its role in sequential modeling is not entirely clear. Could the authors further explain the rationale behind adopting this approach for sequences?\n\n5. The authors encode positional information into the thresholds of spiking neurons, resulting in each neuron having a distinct threshold. Since the number of neurons is directly related to the input token length, could the authors discuss the implementation complexity of such independently parameterized thresholds when deploying the model on actual neuromorphic hardware?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c94kof8KXd", "forum": "9z9mgVpXyE", "replyto": "9z9mgVpXyE", "signatures": ["ICLR.cc/2026/Conference/Submission5753/Reviewer_6QkJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5753/Reviewer_6QkJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466005953, "cdate": 1761466005953, "tmdate": 1762918238635, "mdate": 1762918238635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel positional encoding approach for Spiking Transformers; however, the experimental verification is insufficient."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a fundamental limitation in directly applying ANN-based positional encodings to Spiking Neural Networks (SNNs), effectively motivating the need for SNN-specific solutions. and proposes an Integer-firing neuron for positional encoding in spiking transformers."}, "weaknesses": {"value": "1. The explanation of the complexity of SSA in the paper is unsatisfactory(shown in \"A key advantage of SSA lies in its linear attention property, whereby the time complexity of SSA, line 179-184). This paper focuses on \"Positional Encoding for Spiking Transformers\"; however, the spiking self-attention in spiking transformers (or self-attention in general) is primarily designed for large models, where the embedding dimension $ D$ is substantial, and $D^2$ is certainly not negligible.\n2. The model used in the experiments is too small（1-2M） to thoroughly validate the effectiveness of the positional encoding in the Spiking Transformer. Moreover, the experiments are relatively simple — for instance, this paper lacks evaluations on more widely recognized time series benchmarks such as the ETT dataset.\n3. Lack of comparison with the SOTA ANN methods.\n4. Lack of discussion on deployment on neural chips.\n5. Lack of comparison with \"Toward Relative Positional Encoding in Spiking Transformers\""}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SVlfM6cZxg", "forum": "9z9mgVpXyE", "replyto": "9z9mgVpXyE", "signatures": ["ICLR.cc/2026/Conference/Submission5753/Reviewer_SaAm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5753/Reviewer_SaAm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722340004, "cdate": 1761722340004, "tmdate": 1762918237961, "mdate": 1762918237961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the critical issue of incorporating positional information into Spiking Transformers, as conventional methods from Artificial Neural Networks (ANNs) disrupt the essential spike-driven computational paradigm of Spiking Neural Networks (SNNs). The authors introduce a novel method called Spiking Positional Encoding (SPE), which is specifically designed to overcome this limitation. The core of SPE is the Positional Encoding Leaky Integrate-and-Fire (PE-LIF) neuron layer, which ingeniously encodes both absolute and relative positional information directly into the firing thresholds of neurons, thereby preserving the event-driven nature of SNNs. The authors provide a theoretical foundation for their method, proving that SPE can represent relative positions and exhibits a desirable long-term decay property. Through comprehensive experiments on seven datasets spanning time-series forecasting and natural language processing, SPE is shown to consistently outperform existing approaches and achieve state-of-the-art results, effectively bridging a significant performance gap between ANNs and SNNs in sequential modeling tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "This paper presents a compelling and well-executed study, exhibiting significant strengths across originality, quality, clarity, and significance. The originality of this work lies in its novel formulation of positional encoding specifically for Spiking Transformers. Instead of adapting ANN-based methods that disrupt the spike-driven paradigm, the authors introduce Spiking Positional Encoding (SPE), a creative solution that embeds positional information directly into the firing thresholds of a newly proposed PE-LIF neuron layer. This approach elegantly captures both absolute and relative positional information within a unified, spike-native framework. The quality of the research is exceptionally high, substantiated by both rigorous theoretical analysis and comprehensive empirical validation. The authors provide formal proofs (Propositions 1 and 2) to demonstrate SPE's capability to represent relative positions and its possession of a desirable long-term decay property. These theoretical claims are backed by extensive experiments across seven diverse datasets, where SPE consistently achieves state-of-the-art performance. Thorough ablation studies further strengthen the findings by methodically demonstrating the contribution of each component. The paper is presented with outstanding clarity; it logically progresses from a clear problem analysis to a set of well-defined design principles, which then guide the development of the proposed solution. The manuscript is well-written, and the high-quality figures effectively illustrate the core concepts. Finally, the work is highly significant as it addresses a critical bottleneck that has limited the application of SNNs to complex sequential tasks. By providing a principled and effective solution, this research substantially bridges the performance gap between ANNs and SNNs, advancing the field of neuromorphic computing and paving the way for the development of more powerful and energy-efficient, brain-inspired architectures."}, "weaknesses": {"value": "1. The paper's persuasiveness is somewhat weakened by the lack of validation in the computer vision domain.\n2. Hand-crafted, non-learnable positional thresholds. The PE-LIF thresholds adopt fixed sinusoidal formulas (Eq. 8), with even-dimension requirement and a global $\\lambda$. This design may be under-adaptive across modalities, sequence scales, or layers, and the paper does not explore learnable or data-driven variants (e.g., per-layer amplitudes/phases or low-rank adapters)."}, "questions": {"value": "1. I would like to see your performance on static vision datasets as well as neuromorphic vision datasets.\n2. Learnability of PE. Why not make the threshold modulation learnable (per-layer/per-head amplitudes, learnable frequencies/phases) and regularize toward sinusoidal priors? Would a learned variant outperform fixed sin/cos?\n3. Where to place PE-LIF? Beyond the current placements (input/MLP tail and Q/K activations), did you evaluate (i) only Q, (ii) only K, (iii) V, or (iv) alternating layers? A placement study would clarify where positional signals are most beneficial."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "uLGMkaJCyl", "forum": "9z9mgVpXyE", "replyto": "9z9mgVpXyE", "signatures": ["ICLR.cc/2026/Conference/Submission5753/Reviewer_F58v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5753/Reviewer_F58v"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792528933, "cdate": 1761792528933, "tmdate": 1762918237611, "mdate": 1762918237611, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Spiking Positional Encoding (SPE) for spiking Transformers. The core mechanism replaces LIF layers with PE‑LIF layers whose firing thresholds are position-dependent, thereby embedding positional information into the spikes without breaking the spike‑driven computation paradigm. Absolute position is injected by using PE‑LIF in the first spike‑encoding layer and at the end of each MLP, while relative position is encoded by using PE‑LIF for the Q and K activations in Spiking Self‑Attention (SSA). They further introduce a membrane‑potential regularizer (MPR‑Loss) to approximately enforce the required expectation condition. Experiments on time‑series forecasting and text classification show consistent improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. SNN-compatible method. Encoding position via threshold modulation inside PE‑LIF is a clean, spike‑domain design that preserves binary spikes. This approach is simple, yet novel.\n2. The proposed approach is supported by theoretical analysis.\n3. MPR‑Loss operationalizes the expectation‑matching precondition and, in ablations on Solar with L=24, improves R² by about +0.022 and reduces RSE by 0.029. The accompanying histograms also show FR–mean alignment improvements in Q/K layers.\n4. Well-written and organized manuscript."}, "weaknesses": {"value": "1. The “longer‑horizon” advantage is not convincingly substantiated. The paper claims SPE is particularly effective for longer prediction horizons, but the deltas vs. the spike‑driven baseline are mixed. For example on Electricity, the R² gain of SPE over the spiking baseline is +0.027 at L=6 (0.983 vs. 0.956) and +0.021 at L=96 (0.964 vs. 0.943), which does not evidence a stronger effect at longer horizons; trends on other datasets should be similarly quantified, not only averaged.\n2. Limited breadth of baselines on spiking Transformers with relative PE. The paper argues that relative PE usually breaks linear SSA; however, several linear‑attention–compatible relative/bias schemes (e.g., simple phase rotations on Q/K, ALiBi‑style biases) can be adapted without explicitly forming attention maps. A direct comparison (or at least a careful adaptation study) is missing, so it remains hard to isolate the value of encoding position in thresholds versus in Q/K phases or biases.\n3. Where and how absolute PE is injected deviates from conventional approach and needs stronger justification. The method places APE at the very first spike‑encoding layer and at the end of each MLP. The rationale, and the effect of each insertion point are not fully analyzed.\n4. Limited applications. The authors present experimental results applying their method to various tasks, but their application is limited compared to other related papers. How does the method perform on image classification benchmarks commonly used in ViT, such as ImageNet?\n5. Energy analysis is absent. \n6. Limited hyperparameter exploration and robustness anlaysis.\n7. To verify that the proposed method works well as a PE, visualization of the proposed PE is required."}, "questions": {"value": "Please refer to Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RvkqFO4Myh", "forum": "9z9mgVpXyE", "replyto": "9z9mgVpXyE", "signatures": ["ICLR.cc/2026/Conference/Submission5753/Reviewer_QkMm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5753/Reviewer_QkMm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5753/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988459278, "cdate": 1761988459278, "tmdate": 1762918237343, "mdate": 1762918237343, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}