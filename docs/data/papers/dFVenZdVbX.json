{"id": "dFVenZdVbX", "number": 5377, "cdate": 1757905662847, "mdate": 1759897978957, "content": {"title": "Can Speech LLMs Think while Listening?", "abstract": "Recent advances in speech large language models (speech LLMs) have enabled seamless spoken interactions, but these systems still struggle with complex reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning has been shown to significantly improve the reasoning abilities of text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for multi-stream speech LLMs, demonstrating that reasoning in text space improves the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken reasoning tasks. Beyond accuracy, the latency of the spoken response is a crucial factor for interacting with voice-based agents. Inspired by the human behavior of \"thinking while listening,\" we propose methods to reduce the additional latency from reasoning by allowing the model to start reasoning before the user query has ended. To achieve this, we introduce an entropy-based metric, \"question completeness,\" which acts as an indicator to guide the model on the optimal time to start reasoning. This method provides greater control over the accuracy-latency trade-off compared with heuristic-based approaches and, under equivalent latency conditions, yields a 4% accuracy gain on ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference data created using rejection sampling to push the accuracy-latency pareto frontier further, resulting in a 70% reduction in latency without loss in accuracy.", "tldr": "We enhance SpeechLLMs reasoning ability and proposed novel methods to allow concurrent thinking and listening.", "keywords": ["SpeechLLM", "Chain-of-Thought"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/641a508b9da8f5ffdb4d6007feb17b77416f7110.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a method to interleave chain of thought (or any reasoning approaches) into speech language models. The naive way is to just perform CoT in text, before projecting it back to speech. This is a naive baseline that works, but increases latency.\n\nThe paper proposes a method to efficiently reason while the model is still listening to audio tokens. The method first uses something called a question-completeness metric that indicates when there is sufficient information in the question stream for the model to begin answering/reasoning. This metric is calculated using a model's output logits. The authors claim this allows us to generate a training dataset that we can perform SFT on the model.\n\nThen, the authors introduced a preference optimization to further improves the model training."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "I think the paper's overall idea is very relevant and practical for speech-based language models. In its introduction, it gives a clear outline about what the approach is about. I quickly could understand it is trying to interleave reasoning early on while the language model is still listening to the input streams. \n\nThe question-completeness metric also makes perfect sense, allowing us to identify when the LLM has enough information to start thinking about the answer (instead of listening till the end). Hence, I mark the paper's contribution as \"excellent\"\n\nHowever, the paper leaves out the details on how exactly we can use their findings to fine-tune the LLM (e.g., how to use the offline training data created to actually perform SFT, and how that actually helps the LLM to determine the inflection point during runtime). I have some unanswered questions regarding this part, which is arguable the most important part of the paper."}, "weaknesses": {"value": "While the paper does well in explaining the big idea, it has a significant weakness in explaining the details of its approach: it leaves out important details on the actual implementation that makes use of the question-completeness metric to fine-tune it to begin reasoning early. \n\nLet me give an example. In section 3, the authors mentioned that:\n> The first scenario includes questions which can be considered “complete\" before reaching the end. In such cases, the model can start reasoning early and simply ignore the remaining question. In the second scenario, sufficient information may be available to start reasoning before the question ends, but the model still needs the remaining information to provide a correct response. We propose two different methods to enable early thinking.\n\nThis is a great introduction to the motivation behind their approach, and it is very clear. But then the authors proceed by saying:\n> To endow the\nmodel with the ability for early reasoning, we created training examples by using our proposed\nQuestion Completeness metric. This metric is designed to identify the optimal time for the model to\nbegin generating its CoT. Subsequently, we fine-tuned the model on this dataset to teach it to follow\nthe distribution of these early-reasoning examples.\n\nFirst, I understand this metric can identify the optimal time to begin generating its CoT and we can create some kind of training dataset from it. But how does this lead to \"Subsequently, we fine-tuned the model on this dataset to teach it to follow\nthe distribution of these early-reasoning examples.\" I can understand this sentence on its own but I cannot see how the previous metric allows us to \"follow the distribution\" (this phrase is vague on its own). Even when I read on to section 2.2, I cannot clearly see how we can use the created training examples to \"force\" the model to start thinking earlier. My very vague guess is that we need to insert the \"start_COT\" token to the end of the inflection point, which forces the model to start thinking and fine-tune the model from this? In addition to this, we need to append some reasoning traces to the training dataset as well? I might be wrong (and I'm not surprised I am), but that's because the papers leave us to piece together the explanation.\n\nI think there is some logic gap here (arguable at the most important part of the paper) and the authors should elaborate the details better. I'm sure the authors know how the details work but we as readers need to make educated guesses."}, "questions": {"value": "1. Please elaborate how the training dataset can nudge the language model to start reasoning at certain inflection point. if the author can elaborate on this clearly in the rebuttal, I will consider increasing the score.\n2. The authors also claimed that their method is Moshi + CoT (ours)♣ and because of this, they outperformed Moshi by a lot (2-3x). This is kind of misleading - I think simply injecting CoT into speech modeling is not that novel and should only serve as a baselines. The authors should focus on the improvement in latency in the approach (by interleaving reasoning while the inputs are still streaming in).\n3. Third, I think time is something that is indicative of a good speech model. The authors mostly use number of tokens as an indication of latency but what kind of time are we talking about here? Are we able to reply in a small amount of time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "61kIrak5R8", "forum": "dFVenZdVbX", "replyto": "dFVenZdVbX", "signatures": ["ICLR.cc/2026/Conference/Submission5377/Reviewer_d8Do"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5377/Reviewer_d8Do"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761269162605, "cdate": 1761269162605, "tmdate": 1762918030421, "mdate": 1762918030421, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a Speech Language Model (SLM) that enables \"thinking while listening.\" The model is finetuned on Moshi and introduces a question completeness metric to support early reasoning when only partial user queries have been received. Additionally, the model leverages Direct Preference Optimization (DPO) to further enhance its performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of enabling \"thinking while listening\" in SLMs is novel.\n- Experimental results demonstrate strong performance for the proposed model.\n- Comprehensive ablation studies convincingly show the effectiveness of each proposed module."}, "weaknesses": {"value": "- The paper does not evaluate end-to-end latency in terms of seconds (since the streaming ASR and the output of reasoning tokens may introduce further latencies). Latency is a critical factor for achieving natural conversational experiences, especially for full-duplex SLMs such as Moshi.\n- Training relies solely on synthetic audio, limiting the model's ability to handle diverse input voices. Although not a critical weakness, this raises concerns about potential overfitting to the voice characteristics of the training speech. It would be valuable to assess the model's robustness on varied input voices. The authors could consider benchmarks like [VoxEval, 2025] (support both mathematical reasoning and diverse input audio conditions), [VoiceBench, 2024] (support diverse input audio conditions), or similar alternatives."}, "questions": {"value": "- Does the latency discussed in this paper refer to reasoning token length or answer token length? This differs from the \"first package latency\" commonly referenced in SLM systems. The distinction should be clearly explained in the abstract and introduction to avoid confusion.\n- Is the streaming ASR naturally supported by the Moshi model, or is it trained by the authors?\n- How much latency is added compared to the original Moshi model? Is it just the 480ms? Can you count all the latencies, e.g., Codec encoding, 480ms, LLM inference, and Codec decoding?\n- In the speech-based CoT setting, the authors appear to prefix the speech CoT sequence to the spoken response. Why not allow the model to generate the speech CoT reasoning sequence itself? A fairer comparison between Text CoT and Speech CoT would involve the model autonomously generating the Speech CoT."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gwFvX48I34", "forum": "dFVenZdVbX", "replyto": "dFVenZdVbX", "signatures": ["ICLR.cc/2026/Conference/Submission5377/Reviewer_DB5G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5377/Reviewer_DB5G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761442483473, "cdate": 1761442483473, "tmdate": 1762918029977, "mdate": 1762918029977, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study presents the following key contributions:\n\n1. It extends the existing full-duplex SLM, *Moshi*, by integrating a streaming ASR component into its text channel.\n2. It incorporates the reasoning process while introducing a dedicated mechanism to determine the appropriate timing for reasoning during a user’s utterance, thereby aiming to reduce latency.\n3. Furthermore, it employs Direct Preference Optimization (DPO) to guide both response length and answer correctness.\n\nThrough these methods, the authors improve the overall performance of the conversational model and demonstrate that by adjusting the threshold related to reasoning timing, one can effectively balance and trade off between latency and performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written overall and effectively uses figures to support understanding.\n2. The authors successfully demonstrate the effectiveness of the introduced ASR modeling component and the use of DPO.\n3. They also show that latency can be reduced through appropriate adjustment of the threshold."}, "weaknesses": {"value": "I have some fundamental questions regarding the underlying assumption of the scenario proposed in this paper. In typical real-world interactions with voice assistants, I am not sure how frequently situations arise in which it is actually reasonable or necessary for the model to perform reasoning *during* the user’s utterance. \n\nMore specifically, while it seems plausible for the model to reason incrementally based on partial chunks of the user’s speech, I wonder how common it really is for a partial question and the entire question to yield essentially the same reasoning trajectory. In addition, since a 95% threshold would likely result in only a very small difference in word count compared to the full question, I remain somewhat skeptical about whether it is actually possible to reduce latency meaningfully without degrading performance.\n\nAs the authors also mention, lowering the threshold to around 0.65 in order to achieve noticeable latency improvement leads to significant performance degradation. This makes me wonder whether, in most cases, the positions where the model determines that it can safely perform reasoning, say, around the 95% mark of the utterance, are in fact not meaningfully different from having access to the full question.\n\nEven in the figures provided by the authors, except for a few special cases, the reasoning appears to begin near the final 3~5 tokens of the input. Since this portion is part of the prefill stage, I am uncertain whether this actually produces a substantial latency advantage. Consequently, I find myself questioning the necessity and justification of this specific module and setup.\n\nPerhaps I overlooked it, but I would like to know, based on standard speech-related benchmarks (e.g., *VoiceBench*), approximately how many of the final words are typically excluded on average when using a 95% threshold.\n\nTo be clear, my comments are not meant to question the overall necessity of the task itself, but rather to express skepticism about whether the proposed setup reflects a truly *practical* real-world scenario. I hope this perspective is taken in that spirit."}, "questions": {"value": "The only question I raised is addressed in the *Weaknesses* section, and there is a citation typo on line 348 (a missing parenthesis)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yOtWhUjud6", "forum": "dFVenZdVbX", "replyto": "dFVenZdVbX", "signatures": ["ICLR.cc/2026/Conference/Submission5377/Reviewer_8SXS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5377/Reviewer_8SXS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5377/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548831873, "cdate": 1761548831873, "tmdate": 1762918029618, "mdate": 1762918029618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}