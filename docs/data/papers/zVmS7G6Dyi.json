{"id": "zVmS7G6Dyi", "number": 20709, "cdate": 1758309239345, "mdate": 1763536534509, "content": {"title": "Generalization Below the Edge of Stability: The Role of Data Geometry", "abstract": "Understanding generalization in overparameterized neural networks hinges on the interplay between the data geometry, neural architecture, and training dynamics. In this paper, we theoretically explore how data geometry controls this implicit bias. This paper presents theoretical results for overparametrized two-layer ReLU networks trained *below the edge of stability*. First, for data distributions supported on a mixture of low-dimensional balls, we derive generalization bounds that provably adapt to the intrinsic dimension. Second, for a family of isotropic distributions that vary in how strongly probability mass concentrates toward the unit sphere, we derive a spectrum of bounds showing that rates deteriorate as the mass concentrates toward the sphere. These results instantiate a unifying principle: When the data is harder to “shatter” with respect to the activation thresholds of the ReLU neurons, gradient descent tends to learn representations that capture shared patterns and thus finds solutions that generalize well. On the other hand, for data that is easily shattered (e.g., data supported on the sphere) gradient descent favors memorization. Our theoretical results consolidate disparate empirical findings that have appeared in the literature.", "tldr": "", "keywords": ["neural networks", "deep learning theory", "gradient descent", "representation learning", "generalization"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4370fcb7fecf3c83fe93166cf520fc4dace419aa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a novel prospective on how data geometry influences the implicit bias of overparameterized two-layer ReLU networks, with a number of theoretical results demonstrating that solutions below the edge of stability have various generalizability properties that can be desirable or undesirable depending on the setting. They provably show that below the edge of stability solutions can adapt to intrinsic lower dimension subspaces within an ambient space and that a spectrum of generalizability occurs determined by the implicit regularization induced by the \"shatterability\" of the data. Further empirical results are given to validate their theoretical claims and proof techniques."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a novel perspective on how the final solutions of the edge of the stability regime will generalize, while avoiding the dynamics of the rich regime. This formulation allows for intriguing new insights into how the structure of the data affects the implicit bias within the training of these models.\n- This work has interesting implications for a number of different areas as outlined in appendix B and helps foster further work into the emerging direction of data shatterability.\n- The empirical results are presented nicely with subsection 4.2 adding substantive support to practically verifying the effect of theorems on the form of the representations learnt in two-layer ReLU networks.\n- The discussion and further work section is well written and suggests good follow-up directions on the topic."}, "weaknesses": {"value": "- Data shatterability is not concretely defined or explained as a concept in the paper. While it can be roughly gleaned from previous work, a clear and concrete definition or explanation of the concept within the paper would substantially improve it.\n- Definition 2.1 claims to be for \"Isotropic Beta-radial distributions\" but then proceeds to define \"Isotropic alpha-powered distributions\". It is unclear what isotropic beta-radial distributions are in this work as I do not believe they are defined.\n- The theoretical claims are stated, but not much intuition or interpretation is given other than the overall message stated in the abstract and the introduction. A more fleshed out narrative and explanation between the theorems would have helped a deeper understanding of the work presented.\n- The empirical results in subsection 4.1 could be improved by giving a more complete explanation of how to interpret them.\n- For the right panel in figure 3 it is unclear why the correlation coefficient is given when the magnitude of the coefficient is so small. Additionally, the current figure provides no sense of how many of the points are in the bottom left corner.\n- There are some linguistic issues such as: the first sentence in the \"Disclaimers and Limitations\" does not make sense, \"deffered\" on page 4."}, "questions": {"value": "- Is it possible to run experiments on other architectures ideally to hypothesise how one could extend these results beyond the two-layer ReLU networks?\n- Could further experiments be conducted to help elucidate the potential benefit of batch normalization through data shatterability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "99RofwdCId", "forum": "zVmS7G6Dyi", "replyto": "zVmS7G6Dyi", "signatures": ["ICLR.cc/2026/Conference/Submission20709/Reviewer_9RWh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20709/Reviewer_9RWh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760979342903, "cdate": 1760979342903, "tmdate": 1762934100284, "mdate": 1762934100284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response: Major Revision to Formalize Concepts and Unify Theoretical Framework"}, "comment": {"value": "Dear Area Chair and Reviewers,\n\nThank you for your detailed feedback sincerely. We have taken your suggestions seriously and performed a substantial revision of the paper to formalize our core concepts and strengthen the logical structure. \n\nThe revised manuscript has been uploaded, and we believe this revision directly addresses the core concerns raised regarding clarity and formalization. The following are the highlight of the revision.\n\n**Summary of Key Revisions:**\n\n1. **Restructured Narrative for Logical Coherence:**\nWe have reordered the main results (**Isotropic $\\to$ Unified Framework $\\to$ Mixture Models**) to provide a more natural progression from specific intuition to general principles and finally to complex applications.\n2. **Exhibit Proof Techniques and Formalize a Rigorous Proxy for Shatterability (Section 3):**\nTo address the concerns about the definition of \"shatterability\", we have introduced the half-space-depth (Tukey-depth) concentration index ($\\mathsf{S}_{\\text{DQ}}$) as a formal proxy for data shatterability (Section 3.2).\n    - **Isotropic Case (Section 3.1):** We rigorously analyze this proxy for isotropic distributions, showing exactly how the concentration of probability mass (quantified by $\\mathsf{S}_{\\text{DQ}}$) dictates the generalization gap.\n    - **Mixture & Anisotropic Cases (Section 3.3):** We discuss how this principle guides our analysis of mixture models (adaptation to intrinsic dimension) while explicitly acknowledging the fundamental mathematical challenges in defining a scalar metric for general anisotropic distributions.\n3. **Clarifying the Technical Innovation** \nWe have added a \"Technical Novelty\" paragraph  in the Introduction to explicitly articulate the technical challenge and our solution.\n    - **The Challenge:** We clarify that the implicit regularization induced by the EoS condition is highly inhomogeneous over the input domain: it is strong in \"deep\" regions but very weak in \"shallow\" regions, leading to infinite global $L^\\infty$ metric entropy.\n    - **Our Solution:** We explain our **half-space-depth partition technique**, which decouples the analysis based on the strength of regularization. This allows us to enforce strict complexity control in the \"good\" regions while handling the \"bad\" regions via probability mass bounds. We believe this exposition makes the derivation of our bounds transparent and easier to follow. Conceptually, the data shatterability principle is built upon the insights derived from these proof techniques.\n4. **Clarifying the Scope of Analysis:**\nWe wrote  \"Scope of Analysis” paragraph in the Introduction to clarify the applicability of our results. We emphasize that our generalization bounds hold for **any parameter state** along the gradient trajectory that satisfies the BEoS condition. Crucially, our analysis **does not assume stationarity** (i.e., vanishing gradients) or optimality. It characterizes the implicit regularization enforced whenever the training dynamics operate in the stable regime. \n5. **Enhanced Visualization:**\n We added **Figure 1 & Figure 2** to schematically illustrate the concept of data shatterability and how our depth-based proxy captures the geometric difficulty of partitioning data.\n\n**Important Note on Technical Consistency:**\nWe emphasize that **the core technical results (generalization bounds, lower bounds) and experimental results remain unchanged.** This revision is strictly focused on improving the presentation, rigorous formalization of concepts, and narrative flow to better communicate the contributions.\n\nWe will post detailed, point-by-point responses to each reviewer’s specific technical questions in separate comments shortly.\n\nThank you again for helping us improve the quality of this work.\n\nBest regards,\nThe Authors"}}, "id": "Rpzg5jtXpr", "forum": "zVmS7G6Dyi", "replyto": "zVmS7G6Dyi", "signatures": ["ICLR.cc/2026/Conference/Submission20709/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20709/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20709/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763536589822, "cdate": 1763536589822, "tmdate": 1763536589822, "mdate": 1763536589822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how data geometry (especially intrinsic dimension and shatterablity) determines generalization under the edge of stability implicit bias.\nThe paper shows that (1) for features from mixtures of low-dimensional balls, such bias provably drives two-layer ReLU networks's generalization to be controlled by the intrinsic instead of the ambient dimension; (2) for isotropic data, its concentration toward boundary / shatterablity controls the generalization.\nSuch theoretical predictions are verified by experiments. \nIn a finer-grained level, in both proofs and empirical results, the implicit path norm regularization induced by EoS mainly regularizes the harder-to-shatter samples, improving generalization on them, and ignores the easier-to-shatter samples. As a result, data geometry of less shatterablity improves generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proves the adaptation of low-dimensionality and the spectrum of generalization wrt to data concentration under EoS implicit bias. The latter is also equipped with a lower-bound to show tightness.\n- The results are verified by empirical results.\n- The sketch of proof is clearly discussed. Combined with the empirical results, it also clarifies on finer-grained roles of EoS implicit regularization and clarifies why and how data geometry affects controls generalization under EoS. Such results emphasize the data-dependent nature of EoS regularization and provides explanations on the highly data-dependent behaviour of overparameterized neural networks.\n- The two main theoretical results and empirical results point to a promising shatterablity principle."}, "weaknesses": {"value": "- In Sec 4.1, the experiments use label noise instead of I.I.D. sampling to construct training set and measures MSE losses instead of difference between the empirical and population risks. Such choice makes it more difficult to compare with theoretical results. What is the motivation for such choices?\n- Minor:\n  - Line 289: \"g is the population version of the *weighted*.\""}, "questions": {"value": "- In Figure 1a, the slope of theoretical prediction is not marked and compared to the actual slope. They seem to be actual≈-0.9 vs theoretical=-1/6, where a gap is still observed. What is the source of this gap? Does it come from looseness of bounds or that the experimented problem is not the worst case to reach the upperbound (eg, the directions of the $J$ lines are not worst-case)? Or is it beyond the (B)EoS bias, similar to Sec 3.3, and is governed by some bias else? Can this question be answered by some experiments, eg, searching the worst BEoS models and comparing their slopes with the actual and the theoretical? Maybe this question demands too much efforts. But I would greatly appreciate it because it may offer a clear view on the limit as well as relation of EoS biases with other biases.\n- The discussion in Sec 4.3 seems quite generic. Is it possible to develop more general generalization bounds for BEoS weights assuming shatterablity instead of specific assumptions like low-dimensionality that leads shatterablity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dDY6qpP9TU", "forum": "zVmS7G6Dyi", "replyto": "zVmS7G6Dyi", "signatures": ["ICLR.cc/2026/Conference/Submission20709/Reviewer_M57z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20709/Reviewer_M57z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761594784427, "cdate": 1761594784427, "tmdate": 1762952249939, "mdate": 1762952249939, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response: Rigorous Formulation and Unified Framework of “Data Shatterability”"}, "comment": {"value": "Several reviewers asked for a more rigorous and unified explanation of our notion of “data shatterability”. We appreciate this opportunity to clarify the concept, and we summarize the formal definition and its role in our framework below.\n\n**1. Motivation: BEoS-induced, spatially inhomogeneous regularity**\n\nIn the BEoS regime, gradient descent is constrained by a data-dependent weighted path norm\n$$\n\\sum_k |v_k| ||w_k||_2 g(w_k,b_k)\\le C(\\eta),\n$$\nwhere $g$ depends on the training data and the learning rate $\\eta$. When a neuron fires strongly on many samples, $g$ is large and the constraint forces $|v_k|||w_k||_2$ to stay small; neurons that activate only on few points receive almost no penalty and can overfit those regions. Thus BEoS induces **spatially inhomogeneous regularization**: some parts of the input space are effectively regularized, others are nearly unregularized. The question then becomes how to characterize these regions geometrically.\n\n**2. Half-space depth as the geometric bridge**\n\nThis observation leads naturally to half-space (Tukey) depth. A point $x$ has high depth if **every** half-space containing $x$ also contains a significant portion of the data. Any ReLU neuron that activates at such an $x$ must activate on that portion, triggering strong BEoS regularization. Hence deeper points enjoy stronger protection against overfitting:\n$$\n\\operatorname{depth}(x,P_X):=\\inf_{u\\in\\mathbb S^{d-1}}\\mathbb P(u^\\top(X-x)\\ge0).\n$$\nInformally, **the deeper a point is, the better BEoS can be expected to generalize there.**\n\n**3. From pointwise depth to a distribution-level index**\n\nTo capture this effect at the distribution level, we consider the depth-quantile function\n$\\Psi_{P_X}(T):=\\mathbb{P}\\big(\\operatorname{depth}(X,P_X)\\ge T\\big)$ and its area\n$\\int_0^{1/2}\\Psi_{P_X}(T)dT$,\nwhich measures how much mass lies in well-regularized regions. In Definition 3.8 we define the **half-space-depth concentration index**\n$$\nS_{DQ}(P_X):=\\Bigg(\\int_0^{1/2}\\Psi_{P_X}(T)\\,dT\\Bigg)^{-1}\n$$\nas our proxy for shatterability: large area $\\Rightarrow$ strong regularization and **low shatterability**; small area $\\Rightarrow$ most mass is shallow and **high shatterability**. The spherical distribution (depth $=0$ everywhere) yields zero area and divergent $S_{DQ}$, matching the flat interpolation phenomenon (Theorem 3.6). Our isotropic spectrum (Theorem 3.4 & 3.5) and low-dimensional adaptation (Theorem 3.10) are precisely different depth profiles viewed through this lens.\n\n**4. Role in our upper and lower bounds (and the effect of $\\alpha$)**\n\nWe use $S_{DQ}(P_X)$ to quantify how feasible it is to populate many disjoint “shallow” regions (spherical caps). In isotropic Beta$(\\alpha)$-radial distributions, $\\alpha$ controls how much mass lies near the boundary. When $\\alpha$ is small, the boundary shell carries significant mass, enabling many populated caps; this yields larger shatterability and allows the lower-bound construction to create larger generalization gaps. When $\\alpha$ is large, the shallow shell has very small mass: such points may still be easy to fit, but their contribution to population risk is negligible. In our upper-bound decomposition, this corresponds exactly to the shrinking shallow-region term. The visualization can be found in Figure 1.\n\nThis proxy is also instrumental in explaining the tightness of our bounds. As discussed in Remark 3.9,  our upper bound analysis currently relies on selecting an optimal depth threshold $T$, which geometrically amounts to inscribing a single rectangle under the depth-quantile curve. The gap between our upper and lower bounds arises precisely because this \"rectangle approximation\" discards the regularization benefit provided by the remaining area under the curve.\n\n**We sincerely thank the reviewers for engaging deeply with the technical aspects of our work. The topic is inherently subtle, and we truly appreciate the time and effort it takes to evaluate a framework that aims to make these mechanisms precise. We hope that the clarifications above help convey the underlying structure more transparently. We are genuinely grateful for your patience in working through these refinements, and your thoughtful feedback has greatly helped us improve both the clarity and rigor of the presentation.**"}}, "id": "FXycpiKgYj", "forum": "zVmS7G6Dyi", "replyto": "zVmS7G6Dyi", "signatures": ["ICLR.cc/2026/Conference/Submission20709/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20709/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20709/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763620289557, "cdate": 1763620289557, "tmdate": 1763620289557, "mdate": 1763620289557, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of data geometry in shaping the generalization behavior of overparameterized two-layer ReLU networks trained below the edge of stability (BEoS). Building on recent studies on the implicit bias of gradient descent, the authors propose a unifying principle termed data shatterability, which measures how easily data geometry allows ReLU thresholds to separate samples. The main theoretical contributions include:\n(1) A generalization bound for data supported on a mixture of low-dimensional subspaces, showing adaptation to the intrinsic dimension (Theorem 3.2);\n(2) A family of generalization bounds for isotropic distributions parameterized by a concentration parameter α (Theorem 3.5), together with lower bounds (Theorem 3.6) and a constructive example demonstrating perfect interpolation on the sphere (Theorem 3.7).\nEmpirical experiments on synthetic data and MNIST illustrate how data geometry affects generalization and representation structure."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear and well-grounded in contemporary discussions around implicit regularization and the edge-of-stability regime.\n2. The notion of data shatterability offers an elegant conceptual synthesis that connects data geometry, implicit bias, and generalization."}, "weaknesses": {"value": "1. Definition and formalization of “data shatterability.”\nWhile the paper emphasizes shatterability as the central concept, it is not clearly or formally defined in a mathematical sense. The text gives intuitive descriptions (“harder to shatter data generalizes better”), but the precise operational definition is vague. \n\n  (1) Can the authors introduce a rigorous definition or metric of shatterability, perhaps analogous to VC dimension or some geometric measure of separability?\n\n  (2) Why do the authors choose the beta-radial distributions with a parameter $\\alpha$ to characterize this data property?\n\n  (3) There is a gap between the rates in Theorem 3.5 and 3.6. How are they related to the general claim?\n\n  (4) Would a toy mode concretely showing how different data geometries affect generalization make the concept more intuitive?\n\n2. Clarity and logical structure of results.\nThe theoretical results (Theorems 3.2–3.7) are presented in isolation, and their interconnections are not fully clear. The reader may struggle to see how they jointly establish a unified principle.\n\n(1) How do the results for subspace mixtures and isotropic distributions fit into a single theoretical framework?\n\n(2) Is there an overarching theorem or lemma that ties them together through the concept of shatterability?\n\n(3) A high-level diagram or summary of theoretical dependencies would help to improve readability and logical coherence.\n\n3. Lack of dynamics analysis.\nThe paper claims to study generalization “below the edge of stability,” yet the analysis focuses entirely on static properties of stable minima rather than on the gradient descent dynamics that give rise to them.\n\n(1) Without examining the time evolution of GD (e.g., curvature oscillations, stability trajectories), the results seem closer to a stability condition rather than a genuine characterization of the EoS regime.\n\n(2) The current framework could be better described as a stability-based generalization bound rather than an analysis of edge-of-stability generalization.\n\n4. Relation to prior work.\nTheorems 3.2 and 3.5 resemble results in [Wu & Su, 2023] and related stability-based analyses, with the main difference being the explicit dependence on data geometry. However, the paper does not clearly articulate the essential technical innovation over these works.\n\n(1) What are the key mathematical difficulties introduced by considering non-isotropic or low-dimensional data distributions, and how are they overcome here?\n\n(2) A more explicit comparison or ablation (possibly in the appendix) would strengthen the contribution."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0FO6yIMdTr", "forum": "zVmS7G6Dyi", "replyto": "zVmS7G6Dyi", "signatures": ["ICLR.cc/2026/Conference/Submission20709/Reviewer_PDwN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20709/Reviewer_PDwN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933873760, "cdate": 1761933873760, "tmdate": 1762934098003, "mdate": 1762934098003, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the interaction between data geometry and the implicit bias of edge of stability.\nIt shows that EoS bias can drive two-layer ReLU networks to adapt low-dimensionality for mixture of low-dim balls data, and that for isotropic data, its shatterablity determines generalization.\nExperiments verify the theoretical predictions.\nThis work then proposes the principle of shatterablity, where the shatterable data points attract specialized neurons, which are less regularized by the implicit weighted path norm in below the edge of stability."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Based on the algorithmic bias of EoS, this work provides the data dependence aspect of neural network generalization, a valuable problem in modern data- and algorithm-dependent theory for generalization. \n- The work picks two representative examples of interest, where the first one is related to how neural network overcomes curse of dimensionality and the second one novelly reveals the role of shatterablity. The results are verified by empirical results. \n- This work also reveals that under EoS regularization, the network may still overfits, and it is data geometry with low shatterablity that helps resisting overfitting. \n- This work provides principled lens for studying feature learning and data geometry reflected in it, eg, neuron activation rate that impacts regularization strength of EoS bias and affects generalization."}, "weaknesses": {"value": "- The paper supports the shatterablity principle using two proved cases, followed by intuitive interpolation/extrapolation. However, a formal results is missing, leaving shatterablity relying on intuitive definition and restricting its application to more complicated data. Is it possible to derive formal definition of shatterablity and provide more abstract generalization bounds with shatterablity and BEoS as parameters?      \n- In experiments, the training data is constructed by perturbing the label instead of IID sampling. How does this setting fits into the assumption of theories? Under standard setting, what will be low-dimension adaptation like?"}, "questions": {"value": "- Some works have emphasized the surprising importance of (benign) memorization for generalization, especially under long-tailed data distribution. Then is there any connection from lon-tailedness and memorization to shatterablity and neuron specialization? If so, what benign memorization looks like in the framework of shatterablity? At what threshold does memorization becomes harmful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7s2ppc1or8", "forum": "zVmS7G6Dyi", "replyto": "zVmS7G6Dyi", "signatures": ["ICLR.cc/2026/Conference/Submission20709/Reviewer_M2En"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20709/Reviewer_M2En"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976261587, "cdate": 1761976261587, "tmdate": 1762934093547, "mdate": 1762934093547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how the geometry of the training data fundamentally controls the implicit bias of gradient descent (GD) and the resulting generalization performance of overparameterized two-layer ReLU networks trained in the \"Below Edge of Stability\" (BEoS) regime."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The author claims that  \"The less shatterable the data geometry, the stronger the implicit regularization of EoS becomes.\" and illustrates this observation via two specific example.\n\nI thought this is a very interesting result and made a serious try to understand the performance neural network comparing with other \"not even wrong\" work."}, "weaknesses": {"value": "I have not checked the whole proof. The results sound reasonable to me. However, to broad its impacts, it would be more beneficial if the author could make more implications of their theoretical results. e.g., its connection with some exiting theories?  Moreover, the rates stated in theorems are more less to technical,  could the authors  make it more comparable with some existing results?"}, "questions": {"value": "Same to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mRUf0Xaqk5", "forum": "zVmS7G6Dyi", "replyto": "zVmS7G6Dyi", "signatures": ["ICLR.cc/2026/Conference/Submission20709/Reviewer_h9Wg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20709/Reviewer_h9Wg"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission20709/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989270149, "cdate": 1761989270149, "tmdate": 1762934092229, "mdate": 1762934092229, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}