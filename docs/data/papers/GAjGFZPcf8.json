{"id": "GAjGFZPcf8", "number": 24148, "cdate": 1758353326766, "mdate": 1763091183942, "content": {"title": "SegRGB-X: General RGB-X Semantic Segmentation Model", "abstract": "Semantic segmentation across multiple sensor modalities involves leveraging both shared and modality-specific cues. Existing approaches often rely on modality-specific specialist models, which can result in redundancy and suboptimal results. In this work, we propose SegRGB-X, a general model designed to jointly address semantic segmentation across five diverse multi-modal datasets. Our framework incorporates three key components: (1) Modality-Aware CLIP (MA-CLIP), fine-tuned with LoRA to extract modality-specific features; (2) a modality-aligned embedding mechanism that introduces modality-aligned prompts to mitigate the feature gap between input embeddings and control prompts; and (3) a Domain-Specific Refinement Module (DSRM) at the final stage of the backbone to adaptively refine modality-specific features. Extensive experiments on five datasets encompassing event, thermal, depth, polarization, and light field modalities demonstrate the effectiveness of SegRGB-X. Our model achieves an average mIoU of 65.03%, outperforming previous specialist models. The codes will be available.", "tldr": "", "keywords": ["General model", "Semantic segmentation", "multimodal fusion", "vision-language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/821c3dde9b7f52c9cd47158469d7a3334b222676.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed a universal arbitrary-modality semantic segmentation method using modality-aware CLIP, complementary learnable prompts, and a modality-aware selective adapter. The experimental results demonstrate the effectiveness of the proposed on different multimodal segmentation benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed method introduces modality-aware CLIP to provide modality-specific scene understanding.\n- Experiments are conducted on multiple benchmarks, and the results show that the proposed method can serve as a generalist model for advancing multimodal segmentation."}, "weaknesses": {"value": "- In Table 1, the compared methods actually achieved higher results in their respective papers. For example, CMX achieved more than 92% in mIoU on RGB-P segmentation. The results on MFNet are also under-reported in this paper. This is a main concern. Please compare the methods fairly and directly report their original results. \n\n- The proposed DSRM should be directly compared against state-of-the-art relevant modules or adapters to illustrate the superiority of the proposed module.\n\n- Recent generalist models such as OmniSegmentor, MemorySAM, Any2Seg, etc., should be compared in the experiments to demonstrate the superiority of the proposed method."}, "questions": {"value": "- The t-SNE visualization shows the modality embeddings from MA-CLIP, which are well separated. Would you consider showing more results or challenging cases? For example, how about the baseline model and the known CMX model? How about their modality embeddings?\n\n- Would you consider verifying the generalization capacity of your proposed model in modality-missing, modality-degraded, or modality-incomplete scenarios? The generalist models like Any2Seg and AnySeg can also perform well in these scenarios, which could be compared.\n\n- Would you consider presenting the performance scores in different scenarios, such as the OE, UE, MB, and EL scenarios, as in the DeLiVER benchmark? This would help enrich the analysis."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lXWAxfHjbO", "forum": "GAjGFZPcf8", "replyto": "GAjGFZPcf8", "signatures": ["ICLR.cc/2026/Conference/Submission24148/Reviewer_gpma"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24148/Reviewer_gpma"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760583786041, "cdate": 1760583786041, "tmdate": 1762942960854, "mdate": 1762942960854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "RMYX3fVtKr", "forum": "GAjGFZPcf8", "replyto": "GAjGFZPcf8", "signatures": ["ICLR.cc/2026/Conference/Submission24148/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24148/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763091182536, "cdate": 1763091182536, "tmdate": 1763091182536, "mdate": 1763091182536, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Motivated by the goal of achieving state-of-the-art (SOTA) performance across diverse multimodal semantic segmentation datasets, this paper proposes a universal, arbitrary-modality semantic segmentation framework—SegRGB-X—that unifies segmentation across multiple modalities. The method introduces three key components: (1) Modality-Aware CLIP (MA-CLIP), (2) complementary learnable prompts for fine-grained feature capture, and (3) a Modality-Aware Selective Adapter (MASA). The approach is evaluated on a diverse set of datasets encompassing five complementary modalities: event, thermal, depth, polarization, and light field. The authors claim that SegRGB-X outperforms the baselines they selected."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated by the ambition to develop a single model that achieves SOTA performance across various multimodal semantic segmentation benchmarks. The proposed method trains once and performs well across all modalities—a compelling vision. The integration of multimodal CLIP features into the learning pipeline offers a novel perspective on leveraging large pre-trained vision models for downstream tasks."}, "weaknesses": {"value": "1. The paper’s justification for its core premise is insufficiently substantiated. In the introduction, the authors hastily conclude: “Consequently, a separate model must be trained for every modality pair, leading to redundancy in both model design and training.” However, they neither theoretically analyze the commonalities and differences across modalities nor provide empirical evidence to support this claim. For instance, event camera data is extremely sparse, whereas depth provides dense geometric cues—these are fundamentally different signal types. Is a one-size-fits-all unified model truly superior to modality-specific designs? This unsubstantiated assumption undermines the foundational motivation of the work.\n\n2. Questionable Claim of Superiority Over Specialized Models:The paper asserts that its “general-purpose” model outperforms “specialized” models, but this claim does not hold against actual SOTA methods. For example, as shown in Table 1, DFormer-v2 achieves 58.4 mIoU on NYUv2 and 67.1 mIoU on the DELIVER dataset—both significantly higher than the results reported for SegRGB-X. Even a specialized model using only RGB + Event inputs (CMNeXt-RGB-E) achieves 57.48 mIoU. These results suggest that domain-specific models can substantially outperform the proposed “universal” approach, directly challenging the paper’s central thesis.\n\n3. The experimental results are muddled and fail to convincingly validate the proposed contributions. In the ablation studies, removing any one of the three core components—MA-CLIP, modality-aligned prompts, or DSRM—yields only marginal performance drops. This suggests that the modules may not be synergistically integrated, and their individual necessity is unclear. It raises the concern that the authors may have simply stacked components without identifying the essential mechanism driving performance gains."}, "questions": {"value": "1. Why would modality-specific designs inherently be redundant? What fundamental commonalities exist across such diverse modalities (e.g., sparse events vs. dense depth vs. thermal signatures)? What justifies the existence of a single unified framework capable of handling all of them effectively? Given their vastly different data characteristics, what shared structure does your method exploit—and can you provide experimental evidence for this shared representation?\n\n2. How does SegRGB-X compare against the actual SOTA methods in each domain? As noted, DFormer-v2 achieves 58.4 mIoU on NYUv2 and 67.1 mIoU on DELIVER—far surpassing your reported numbers. Doesn’t this demonstrate that specialized models still dominate? If so, what concrete advantages does your “universal” model offer over these domain-optimized approaches?\n\n3. Given that ablating any single component barely affects performance (yet the full model is claimed to achieve SOTA), what is the true essence of your contribution? What is the intrinsic relationship among the three proposed modules? Could some of them be redundant? Please clarify which component(s) are truly responsible for performance gains and why all three are necessary."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qX5rXTYZg8", "forum": "GAjGFZPcf8", "replyto": "GAjGFZPcf8", "signatures": ["ICLR.cc/2026/Conference/Submission24148/Reviewer_Lvta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24148/Reviewer_Lvta"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761140312327, "cdate": 1761140312327, "tmdate": 1762942960583, "mdate": 1762942960583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SegRGB-X, a generalist framework for semantic segmentation that aims to handle arbitrary combinations of RGB and an auxiliary modality. The proposed method features three key technical contributions: Modality-aware CLIP (MA-CLIP), Modality-aligned Embedding, and Domain-Specific Refinement Module (DSRM). The model is trained jointly on five diverse datasets covering event, thermal, depth, polarization, and light field modalities. The results show that SegRGB-X achieves a new state-of-the-art average mIoU of 65.03%, outperforming previous specialist models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The integration of a pre-trained CLIP is interesting.\n2. The authors conduct extensive experiments for validation."}, "weaknesses": {"value": "1. Some components is not clearly introduced, such as how the modality-aligned embedding is designed. Another example is that hot input embeddings, control prompts from MA-CLIP, and modality-aligned prompts are combined is not introduced. Moreover, The text explicitly states there are \"a total of four DSRM modules integrated throughout the network\", yet the main architectural diagram (Figure 2) only shows a single DSRM in the final stage.\n2. The model's performance on the NYUDepthV2 dataset is a significant concern. The authors attribute this to a domain gap, but this explanation undermines the central claim of building a \"generalist\" model.\n3. Comparing a joint trained model with models trained on single dataset is unfair. The authors should compare under the same setting.\n4. The proposed method leads to dramatic increase in computational cost with a relatively marginal improvement in mean mIoU. For many real-world applications, a significant slower model with slight higher performance is not a practical choice."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "thO1vrZq8j", "forum": "GAjGFZPcf8", "replyto": "GAjGFZPcf8", "signatures": ["ICLR.cc/2026/Conference/Submission24148/Reviewer_dHgw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24148/Reviewer_dHgw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761279296053, "cdate": 1761279296053, "tmdate": 1762942960310, "mdate": 1762942960310, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents SegRGB-X, a versatile semantic segmentation model designed to work with any combination of RGB and an additional modality (RGB-X) within a single architecture. The main technical contributions are threefold. First, the authors introduce a Modality-Aware CLIP (MA-CLIP), which adapts CLIP to multiple modalities efficiently using LoRA. Second, they propose a modality-aligned embedding mechanism that employs learnable prompts to reduce the feature gap between CLIP and the backbone network. Third, a Domain-Specific Refinement Module (DSRM) is used to refine features in an adaptive way. SegRGB-X is trained and evaluated across five diverse datasets covering event, thermal, depth, polarization, and light field modalities. The results show that SegRGB-X consistently outperforms both specialist and generalist models in terms of mean IoU and maintains strong robustness under challenging conditions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The framework is clearly illustrated in Figure 2, making the multi-component design (MA-CLIP, prompts, DSRM, etc.) easy to follow.\n2. The feature map visualizations (Figure 8, Figure 10) across backbone stages provide convincing support for claims of modality-specific and hierarchical representation learning."}, "weaknesses": {"value": "1. The model shows a significant drop in performance on NYUDepthV2 (Table 1: 47.77% vs. 56.93% or higher on other datasets). This appears to be more than a practical issue—it reflects a fundamental data imbalance, as most of the training data is outdoor while NYUDepthV2 is indoor (see Appendix A.5 on Generalization Error Analysis). Although the authors acknowledge this, the discussion could be strengthened by suggesting potential mitigation strategies, such as domain adaptation or balanced sampling techniques.\n2. Overreliance on Specific Visual-Language Models: The approach relies solely on CLIP for vision-language alignment. By not evaluating alternative VLMs or more general unsupervised/self-supervised backbones (e.g., BLIP, ALIGN), the work may be limited in generality. While some justification is provided, direct experimental comparisons are lacking.\n3. The label-space unification and remapping function described in Section 3.5 is an important component, particularly since datasets often differ semantically. It would be helpful if the authors clarified how overlapping or ambiguous labels are handled, and whether mismatches in class distribution affect either learning or evaluation.\n4. The paper could clarify whether the joint model is trained using individual paired modalities or batch-wise paired modalities. It is also unclear whether all modalities are required simultaneously during inference, or if the model supports modality dropout. While some implementation details are provided in Section 4.1, the logic behind modality pairing and batch composition remains somewhat ambiguous."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DRWH96fI4p", "forum": "GAjGFZPcf8", "replyto": "GAjGFZPcf8", "signatures": ["ICLR.cc/2026/Conference/Submission24148/Reviewer_M2cX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24148/Reviewer_M2cX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24148/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958473647, "cdate": 1761958473647, "tmdate": 1762942959933, "mdate": 1762942959933, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}