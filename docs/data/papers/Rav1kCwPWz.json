{"id": "Rav1kCwPWz", "number": 8013, "cdate": 1758051791415, "mdate": 1763748890206, "content": {"title": "Programmatic Representation Learning with Language Models", "abstract": "Classical models for supervised machine learning, such as decision trees, are efficient and interpretable predictors, but their quality is highly dependent on the particular choice of input features. Although neural networks can learn useful representations directly from raw data (e.g., images or text), this comes at the expense of interpretability and the need for specialized hardware to run them efficiently. In this paper, we explore a hypothesis class we call Learned Programmatic Representations (LeaPR) models, which stack arbitrary features represented as code (functions from data points to scalars) and decision tree predictors. We synthesize feature functions using Large Language Models (LLMs), which have rich prior knowledge in a wide range of domains and a remarkable ability to write code using existing domain-specific libraries. We propose two algorithms to learn LeaPR models from supervised data. First, we design an adaptation of FunSearch to learn features rather than directly generate predictors. Then, we develop a novel variant of the classical ID3 algorithm for decision tree learning, where new features are generated on demand when splitting leaf nodes. In experiments from chess position evaluation to image and text classification, our methods learn high-quality, neural network-free predictors often competitive with neural networks. Our work suggests a flexible paradigm for learning interpretable representations end-to-end where features and predictions can be readily inspected and understood.", "tldr": "We learn decision trees end-to-end with programmatic representations, where input features are represented as code synthesized by LLMs", "keywords": ["representation learning", "large language models", "program induction"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bb11738e289ea5ddfefe4bfc1818505a49b093ba.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LeaPR (Learned Programmatic Representations) models, which stack arbitrary features represented as code and decision tree predictors. The authors show that their proposed method is competitive with neural networks (without using neural models) on a few set of benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Using Large Language Models for feature generation is an important research directory.\n\n2. Using some evolutionary algorithms like FunSearch to such a certain application is quite interesting."}, "weaknesses": {"value": "1. My biggest concern is that I think authors have to deeply do research on feature generation works. Generating raw features to enhance a prediction model (both for non-neural models and neural models) are widely proposed. For example, OpenFE and AutoFeat use classical learning algorithms to generate raw features. Moreover, CAAFE and OCTree proposes to use LLMs for feature generation. Specifically, OCTree also uses an evolutionary algorithm similar to FunSearch and AlphaEvolve. I suggest authors to carefully look at these works and clarify the difference of LeaPR compared to them. Also, it would be great to be involved as a baseline.\n\n2. Baselines are too weak. Why did the authors only use Transformers as a baseline? There are much more neural-based models powerful than Transformers. For example, for the tabular dataset, TabPFN is known to be a SOTA model.\n\n3. Is the used benchmark useful to show the interpretability of the model? I ask this because, the authors argue that their proposed models show better interpretability than neural-based models. What experiments support this statement? Or is this because LeaPR only uses tree-based models?\n\n4. Number of benchmark is too low and outdated."}, "questions": {"value": "See the above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wWJUQasRgp", "forum": "Rav1kCwPWz", "replyto": "Rav1kCwPWz", "signatures": ["ICLR.cc/2026/Conference/Submission8013/Reviewer_kY8q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8013/Reviewer_kY8q"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463480123, "cdate": 1761463480123, "tmdate": 1762920016822, "mdate": 1762920016822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a method for obtaining interpretable classifiers by using language models to propose a set of feature extractors in the form of Python programs. The feature set is evolved iteratively by training a decision tree on top of the current feature set, and prompting the LLM to propose features to improve the decision tree accuracy. An extension of the method uses an ID3-style algorithm to find features that iteratively split nodes in the decision tree with high classification error. The method is evaluated on three domains: classifying chess positions, images, and text."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- I think this paper presents an interesting solution to the important problem of developing more interpretable machine learning models. Representing features as programs makes it possible to interpret features, edit the features after the fact, and take advantage of the strengths of state-of-the-art (but uninterpretable) language models.\n\n- The method builds on prior work (FunSearch) but represents a meaningful extension: FunSearch searches for a single program, while this method has to search for a set of programs, representing different features, which is a non-trivial extension.\n\n- The paper conducts experiments on several different domains (chess, images, and text), which demonstrates the generality of the approach. The performance is fairly impressive, with LeaPR performing more or less on par with neural network baselines.\n\n- The paper presents a number of interesting examples of programs. These examples show that the method is useful for discovering interesting properties of training distributions---for example, that LM-generated text is more likely to contain a combination of ascii-quotation marks and curly quotation marks. I also appreciated the case study of debugging a spurious feature in the Waterbird dataset.\n\n- I can see this method providing the basis for future work--for example, to learn hierarchical features, or simply to improve the prompting scheme for generating non-hierarchical feature extractor programs."}, "weaknesses": {"value": "- The paper is missing some details about the formal definition of the methods. Figure 2 presents definitions of the two algorithms, but the subroutines in these algorithms are not formally defined (RandomKFeatures, ProposeFeatures, SplitError). Much of the exposition of the algorithms is presented informally in the text. This makes it difficult to understand exactly how the algorithms work, and to reason about the different design choices.\n\n\n- The method is evaluated on very simple problems, where it is possible to obtain good performance with a small number of simple features (e.g. detecting LLM-generated text). It is not clear how well it will scale up to more complex domains. It would have been helpful to see results on a more challenging domain--for example, natural language inference, for text [1]. I don't think the method has to perform well on these datasets to be worthy of publication, but I think showing results on such datasets would help to illustrate the limitations of the method which could be improved in future work.\n\n\n- It is not entirely clear to me why decision trees in particular are used. In my opinion, the paper would be stronger if the method was stated more generally, as a method for iteratively evolving (1) a set of features, and (2) a predictor that predicts outcomes based on the features. This would then make it possible to compare different choices of predictor, such as linear models, in addition to decision trees.\n\n\n- The paper would benefit from more experiments illustrating the computational cost of this approach. For example, LLM-based program synthesis methods are known to benefit from a very high number of samples (see e.g. [2]). Some details about the number of iterations are reported in the appendix, but it would have been useful to see some empirical results about the relationship between number of samples and final performance.\n\n\n- The paper argues that one of the advantages of this method is that it is less data intensive than neural networks. But there is no empirical experiment comparing the neural networks and LeaPR with different data budgets. One could also argue that one of the _benefits_ of neural networks is that they can take advantage of additional training data--if LeaPR does not improve with more data, this is arguable a limitation of the approach. This point is alluded to at the end of section 4.1, but it would be helpful to see some empirical analysis.\n\n\n- In the image domain, LeaPR is trained without access to the underlying image data. The authors acknowledge this limitation in section 4.2, but it could be highlighted more prominently in the introduction.\n\n\nMinor comments\n\n- The terms \"low-level\" and \"high-level\" (as in \"low-level inputs\") in the introduction are not clearly defined.\n\n\nOverall, I think this paper presents a promising idea and represents an exciting direction for future work. However, I think the paper needs some improvements before I could recommend accepting it--especially, to supply the missing formal definitions of the algorithms mentioned above. I would be happy to increase my score if these weaknesses could be addressed.\n\n\n\n[1] Williams et al., 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference.\n\n\n[2] https://blog.redwoodresearch.org/p/getting-50-sota-on-arc-agi-with-gpt"}, "questions": {"value": "- Is there any reason that the feature programs are all given the same name (`def feature(...) -> float:`)?\n\n- Modern frontier LLMs support image inputs. Did you try running the image experiments with training images included in the prompt?\n\n\n- I generally found the related work section to be a good overview of the area, but it could also include related work on iterative library learning, such as DreamCoder [1], and some discussion of what makes a predictor interpretable (e.g. [2]).\n\n\n[1] Ellis et al., 2023. DreamCoder: growing generalizable, interpretable knowledge with wakeâ€“sleep Bayesian program learning.\n\n[2] Lipton, 2018. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y2qW6aaN2Y", "forum": "Rav1kCwPWz", "replyto": "Rav1kCwPWz", "signatures": ["ICLR.cc/2026/Conference/Submission8013/Reviewer_iR4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8013/Reviewer_iR4d"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761662650968, "cdate": 1761662650968, "tmdate": 1762920016227, "mdate": 1762920016227, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present LeaPR, a hypothesis class that combines programmatically generated features (represented as code) with tree-based predictors. The features are synthesized by Large Language Models (LLMs), leveraging their ability to write interpretable, domain-specific code. The paper introduces two algorithms for learning LeaPR models: (1) F2, an adaptation of existing feature-search method FunSearch, where LLMs generate global features based on feature importance, optimized for ensemble predictors such as Random Forests; and (2) D-ID3, a novel algorithm inspired by ID3 decision tree learning, where the model dynamically requests new LLM-generated features during tree construction to improve local splits.\n\nThe authors evaluate LeaPR on three domains, including chess position evaluation, image classification, and text classification. They show that the resulting models (sometimes) achieve comparable predictive performance compared to neural networks while retaining interpretable features. The paper also includes snippets of the learned code-based features and the prompts used for feature generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Leveraging LLMs to generate interpretable, programmatic features for classical models (like Trees) is an appealing approach that combines symbolic interpretability with LLM reasoning capabilities. Also, features represented as code snippets are human-readable and can be inspected or audited, improving model transparency. Authors present the D-ID3 algorithm, which incorporates feature generation dynamically during training, a creative extension of decision tree learning.\n\n2. Authors present experiments on diverse tasks (text, images, chess) show that LeaPR can be used across modalities. They also share the LLM prompts and generated features that enhance reproducibility and interpretability."}, "weaknesses": {"value": "1. Limited novelty: The general idea of using LLMs to generate features, either as code (Python functions) or as text/natural-language, has been explored in prior work since 2023 [1, 2, 3]. Several existing methods already use LLMs to produce interpretable or programmatic features for succinct predictors such as decision trees or linear models. Therefore, while D-ID3 provides an additional algorithmic contribution, the overall framework is not fully novel, and must be compared against existing frameworks.\n\n[1] Singh, C., Morris, J., Rush, A. M., Gao, J., & Deng, Y. (2023, December). Tree prompting: Efficient task adaptation without fine-tuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (pp. 6253-6267).\n[2] Khandelwal, A., Pavlick, E., & Sun, C. (2023). Analyzing modular approaches for visual question decomposition. arXiv preprint arXiv:2311.06411.\n[3] Chan, K. H. R., Chattopadhyay, A., Haeffele, B. D., & Vidal, R. (2023). Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions. arXiv preprint arXiv:2308.12562.\n\n2. Missing baselines: The paper does not compare LeaPR against existing LLM-based feature generation methods, both code-based and text-based, which weakens claims of originality and performance. There is no comparison against LLMs performing direct reasoning or chain-of-thought (CoT) inference, which are strong baselines for structured prediction and may already capture interpretable intermediate reasoning steps without explicit feature synthesis. The study omits traditional feature-engineering baselines with hand-crafted expert features and does not examine whether LLM-generated features complement or improve upon human-designed ones. For predictive performance, the authors should include baselines using direct LLM inference (with the LLM feature-generators: GPT-4o mini and GPT-5 mini) and CoT reasoning to assess the true benefit of learned programmatic representations. For interpretability, results could also be compared against simpler interpretable models such as shallow decision trees or logistic regression with a small number of features.\n\n3. Limited parameter exploration: There is limited analysis of how the number, complexity, or quality of generated features affects model performance and interpretability. The computational cost of using LLMs for feature synthesis is not discussed or compared against traditional feature extraction or neural representation learning.\n\n4. Limited practical applicability: It is not straightforward to see how the chosen tasks could be applied in more practical settings."}, "questions": {"value": "1. Can the LeaPR framework incorporate human-in-the-loop feedback to refine or prune LLM-generated features, thereby improving interpretability and reducing redundancy?\n2. How does LeaPR handle potentially incorrect or low-quality features generated by the LLM, and are there mechanisms to automatically detect or discard unhelpful code-based features during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "mJK5hATtNE", "forum": "Rav1kCwPWz", "replyto": "Rav1kCwPWz", "signatures": ["ICLR.cc/2026/Conference/Submission8013/Reviewer_KrKR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8013/Reviewer_KrKR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761692604275, "cdate": 1761692604275, "tmdate": 1762920015263, "mdate": 1762920015263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is best described as building of FunSearch [1] to introduce a binary tree version of this method \n\nFunSearch proposes using LLMs in an unconventional way. It exploits the fact that LLMs can code very well to use them to create feature extractor by only using prompting and a feature scorer. The LLM is shown examples of best scoring features and their score and instructed to find better ones.\n\n[1] https://www.nature.com/articles/s41586-023-06924-6\n\nthis work applies this principle to constructing decision trees, the main difference being that now we use this method to find new features at each decision point rather than global features. This brings its own set of difficulties that the paper tackles."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The basic premise of FunSearch is daring and interesting and this work proposes logical evolution of it that shares its properties. Its good to see more work in this direction\n\n2. The idea of using decision tress / random forests makes a lot of sense since they are very successful for a number of problems, but are limited by the pool of features the can pick from"}, "weaknesses": {"value": "1. The use of SoTA LLM like GPT-5 in the proposed model can be a big distortion factor against other neural network baselines, since the are very performant. There should be a FunSearch baseline using GPT-5 for comparison and other weaker baselines that may leverage GPT-5 like a plain non-iterative \"instruct GPT-5 to solve problem using code and the same prior knowledge as in the other (i.e. API specs)\".\n\n2. Some claims about the advantages seem inconsistent or not very convincing. The paper states neural networks \"are highly data intensive\" and \"their ability to generalize drops drastically when in-domain data are scarce\", but uses pre-trained NNs in in a way that directly contradicts this i.e. as few-shot generators of features that seem to generalize well. The method is proposed as \"flexible paradigm for learning interpretable representations end-to-end\" but also claims \" our experiments in chess can use up to 50k lines of LLM-generated code\" which seems hardly interpretable."}, "questions": {"value": "How would FunSearch with GPT-5 perform here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CH9TCVvWCJ", "forum": "Rav1kCwPWz", "replyto": "Rav1kCwPWz", "signatures": ["ICLR.cc/2026/Conference/Submission8013/Reviewer_idVN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8013/Reviewer_idVN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8013/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960069231, "cdate": 1761960069231, "tmdate": 1762920014409, "mdate": 1762920014409, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}