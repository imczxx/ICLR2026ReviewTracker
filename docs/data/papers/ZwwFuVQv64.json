{"id": "ZwwFuVQv64", "number": 7558, "cdate": 1758027383340, "mdate": 1763713294043, "content": {"title": "Layerwise Federated Learning for Heterogeneous Quantum Clients using Quorus", "abstract": "Quantum machine learning (QML) holds the promise to solve classically intractable problems, but, as critical data can be fragmented across private clients, there is a need for distributed QML in a quantum federated learning (QFL) format. However, the quantum computers that different clients have access to can be error-prone and have heterogeneous error properties, requiring them to run circuits of different depths. We propose a novel solution to this QFL problem, Quorus, that utilizes a layerwise loss function for effective training of varying-depth quantum models, which allows clients to choose models for high-fidelity output based on their individual capacity. Quorus also presents various model designs based on client needs that optimize for shot budget, qubit count, midcircuit measurement, and optimization space. Our simulation and real-hardware results show the promise of Quorus: it increases the magnitude of gradients of higher depth clients and improves testing accuracy by 12.4% on average over the state-of-the-art.", "tldr": "A novel framework for federated learning of quantum ML models with varying depths", "keywords": ["Federated Learning", "Heterogeneity", "Quantum"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b71aaa8f37c0a443a5a5eee6403d0ea18cdaeab.pdf", "supplementary_material": "/attachment/04bed01ecf3b4935a035cb81bf968965edcd5ee4.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a method to address the problem of different architectures and behaviors of quantum computers in a quantum federated learning scheme. They use layerwise loss to aggregate the parameters of local QNNs with specific approaches, gaining performance improvements. The proposed method is examined on real quantum computers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Using layerwise losses for QFL is a novel approach. \n2. The paper is well-written overall."}, "weaknesses": {"value": "1. The proposed method appears limited to binary classification problems. \n2. Line 291 states \"we design an ansatz where it is possible to obtain the outputs from all layers in a single shot,\" but it's unclear how the ansatz enables single-shot behavior.\n3. The baselines compared are only QML models, no classical baseline is included. It is plausible that classical models could easily outperform QML models on binary classification problems."}, "questions": {"value": "1. In line 265, \"Passing the state unchanged thus requires you to prepare another copy of it, which induces additional shot overhead that is linear in the number of layers and is a nontrivial cost\" → does this mean state tomography is required in general? And how is this linear in the number of layers? \n2. Following W2, how does the ansatz enable the single-shot behavior? Is there any proof? From line 296, \"we evaluate each layer's outputs by computing the marginal distribution on its ancilla\" → how can you calculate the \"distribution\" with a single shot?\n3. Does the parameter aggregation process, which aligns parameters for models with different layers, affect the expressiveness? For example, a quantum state produces output, becoming an intermediate state of another model. And this \"another model,\" after additional layers, will produce the output of the same task. Then why not just use the shallower model if a shallow circuit is enough for such a task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SYJip8ILiM", "forum": "ZwwFuVQv64", "replyto": "ZwwFuVQv64", "signatures": ["ICLR.cc/2026/Conference/Submission7558/Reviewer_hx6a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7558/Reviewer_hx6a"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705490527, "cdate": 1761705490527, "tmdate": 1762919652639, "mdate": 1762919652639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a federated learning framework to train quantum clients with different circuit depths by applying a layerwise loss (pairwise KL coupling and circular aggregation of rotation parameters). The authors claim up to a 12.4 percent accuracy gain over “Q-HeteroFL” on binary classification of MNIST and Fashion-MNIST datasets. The manuscript also presents results across multiple IBM QPUs."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem formulation and framing are clear, focusing on depth heterogeneity in QFL. The concrete FL procedure with circular angle averaging, parameter slicing by depth, and layerwise loss is an impressive methodology.\n\nCircuit engineering is thoughtful. The designs cover orthogonal resource constraints such as shots, ancillae, and mid-circuit measurement, and they are practical on current hardware. The Funnel variant is particularly pragmatic. \n\nValidation on real quantum hardware is a core strength."}, "weaknesses": {"value": "Major Weaknesses: \nThe evaluation scope is narrow. Only binary classification is used, and only on MNIST and Fashion-MNIST (both grayscale, 28×28 toy image datasets). No multiclass tasks, no quantum-native datasets, and missing diversity (especially in a paper that lacks theoretical proofs) weaken the authors’ claims. These datasets are fairly simple and do not require deep quantum circuits to perform well. Under this narrow experimental setting, a 12 percent improvement over a baseline does not establish general scalability, robustness, or utility of the method.\n\nWhen a paper offers only empirical support, it is expected to include evaluation across multiple types of problems, including at least multiclass classification and a different dataset, such as CIFAR-10 or CIFAR-100. None are present.\n\nBecause the experiments are so limited in difficulty and diversity, the work must instead offer compensation with solid theoretical justification that proves why the method should work beyond toy datasets. However, convergence theory is missing, there is no stationary-point guarantee, and no rigorous mathematical explanation of how KL coupling affects the optimization landscape.\n\nMinor (for maximum information transfer) Issues: \nThe data encoding pipeline is under-specified. Feature preprocessing, feature-to-quantum-state mapping, and the reupload schedule (mentioned briefly in the paper) are extremely important. These affect the number of qubits required as well as circuit depth. Upon checking the appendix and supplementary materials, I found that the authors used PCA to reduce the dimensionality. Overall, reproducibility and completeness of the paper are negatively impacted by the lack of inclusion of these details in the main manuscript. I hope the authors include this.\n\nBaseline selection seems to inflate the claimed gains. Vanilla QFL forces all clients to use the shallowest model, giving predictably weak results. Standalone training is a trivial lower bound. Q-HeteroFL is an extension of HeteroFL, implemented by the authors without external validation. It is not clear whether these baselines are suitable enough to be regarded as “state of the art.”\nOn the one hand, the engineering around layerwise circuit execution is very creative, and the hardware study is a solid strength. I would love to score this paper very highly if theoretical guarantees were provided or if the experiments were significantly more diverse."}, "questions": {"value": "1. How does the proposed KL coupling affect the optimization landscape for deeper clients? Can the authors provide a theoretical or empirical justification that this does not restrict the expressive power of deeper PQCs?\n\n2. Do the authors expect the claimed gains to hold on multiclass or quantum-native datasets? It would be very interesting to see."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wnkCvXoEPl", "forum": "ZwwFuVQv64", "replyto": "ZwwFuVQv64", "signatures": ["ICLR.cc/2026/Conference/Submission7558/Reviewer_BzL5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7558/Reviewer_BzL5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789192562, "cdate": 1761789192562, "tmdate": 1762919652314, "mdate": 1762919652314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors propose a structured Federated Learning framework (Quorus) that adaptively selects and synchronizes subsets of model layers within the framework of federated learning optimization. Instead of transmitting the complete parameter set or using uniform layer selection, LFL introduces a layer-wise importance estimator and communication scheduler that determines which layers to aggregate in each round based on gradient variance and update magnitude. \n\nThe experiments on binary classification on the MNIST and Fashion MNIST datasets demonstrate that the proposed approaches consistently outperform other quantum federated learning methods, such as Q-HeteroFL, and achieve better performance on IBM quantum platforms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work firstly proposes the Quorus framework that is tailored for heterogeneous-depth clients, providing an alternative way to realize quantum machine learning on realistic quantum processors without QEC techniques. \n\n2. The proposed layer-importance metric based on normalized gradient variance and the communication schedule can be plugged into existing federated learning systems, demonstrating the advantages of Quorus in this work. \n\n3. The experiments are scalable to 50-200 clients, making the authors' claimed methods reasonable. \n\n4. The appendix includes detailed hyperparameters, dataset information, and pseudo code."}, "weaknesses": {"value": "1. Although the work claims to propose that the Quorus is effective in both simulation and realistic quantum processors, the approach is largely heuristic. In particular, the gradient-variance-based importance metric is intuitive, and it does not involve convergence or bias-variance analysis of partial aggregation. \n\n2. The work exacerbates the fairness gaps across clients if some layers are seldom updated. It looks like the authors mention this in passing, but do not quantify or mitigate it. \n\n3. The paper omits comparison with communication-efficient Federated Learning methods such as FedDrop, Sparse Ternary Compression, or Selective-FedAvg, which are conceptually closest. \n\n4. Ablations on importance metric choices or thresholds are limited. It is unclear how sensitive the system is to hyperparameters (e.g., top-k fraction per round)."}, "questions": {"value": "1. How does your Layerwise Federated Learning (LFL) framework differ conceptually and algorithmically from prior “layerwise parameter sharing” or “selective aggregation” methods?\n\n2. Have you compared with or at least cited existing selective-layer FL methods in the classical setting (e.g., FedPer, FedRep, Selective-FedAvg, FedDrop, FedPAQ)? If not, please justify why those are not directly comparable or include a baseline comparison.\n\n3. Could your algorithm be interpreted as a special case of layer-dropout or structured sparsification techniques? If so, why does it deserve to be treated as a new paradigm rather than an instance of that family?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nwNvOQyVTQ", "forum": "ZwwFuVQv64", "replyto": "ZwwFuVQv64", "signatures": ["ICLR.cc/2026/Conference/Submission7558/Reviewer_LKv7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7558/Reviewer_LKv7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913280773, "cdate": 1761913280773, "tmdate": 1762919651959, "mdate": 1762919651959, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a quantum federated learning algorithm that can account for errors in quantum circuits. The authors use extensive simulations to show how this approach can improve test accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The idea of using varying-depth quantum circuits is both interesting and timely.\n+ The authors provide extensive experiments to validate their approach.\n+ The solution can reduce barren plateau effects.\n+ There are clear relevance to emerging quantum platforms."}, "weaknesses": {"value": "- The gains from the experiments seem very limited.\n- The scalability of this solution is not studied.\n- The results seem limited to basic classification tasks."}, "questions": {"value": "- How does your algorithm scale with the number of layers and number of agents? It would be useful to provide scalability experiments.\n- Can you handle heterogeneous datasets? Can you run experiments with such settings?\n- How do you justify such a complex design for a very small (around 10%) gain?\n- What type of quantum hardware is needed for this solution to work?\n- Can you study the effect of error correction or mitigation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "itcxomfuEc", "forum": "ZwwFuVQv64", "replyto": "ZwwFuVQv64", "signatures": ["ICLR.cc/2026/Conference/Submission7558/Reviewer_6SVZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7558/Reviewer_6SVZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005243894, "cdate": 1762005243894, "tmdate": 1762919651495, "mdate": 1762919651495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}