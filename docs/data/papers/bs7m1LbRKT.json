{"id": "bs7m1LbRKT", "number": 13844, "cdate": 1758223572311, "mdate": 1759897409013, "content": {"title": "Training with Dynamic Sparse Heads as the Key to Effective Ensembling", "abstract": "Model ensembles have long been a cornerstone for improving generalization and robustness in deep learning. However, their effectiveness often comes at the cost of substantial computational overhead. To address this issue, state-of-the-art methods aim to replicate ensemble-class performance without requiring multiple independently trained networks. Unfortunately, these algorithms often still demand considerable compute at inference. In response to these limitations, we introduce _NeuroTrails_, a sparse multi-head architecture with dynamically evolving topology. This unexplored model-agnostic training paradigm improves ensemble performance while reducing the required parameter count. We analyze the underlying reason for its effectiveness and observe that the various neural trails induced by dynamic sparsity attain a _Goldilocks zone_ of prediction diversity. NeuroTrails displays efficacy with convolutional and transformer-based architectures on vision, language, and reinforcement learning tasks. Experiments on ResNet-50/ImageNet, LLaMA-350M/C4, DQN/Atari demonstrate increased performance and stronger robustness in zero-shot generalization, while requiring significantly fewer resources.", "tldr": "We propose NeuroTrails, a sparse multi-head architecture with dynamically evolving topology, which outperforms full ensembles by inducing diverse prediction paths, while using a fraction of the resources.", "keywords": ["deep learning", "ensembles", "sparsity", "dynamic sparse training", "computer vision", "language modeling"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad02e992f6b69efefcdab1eaf6bc86783e0166c7.pdf", "supplementary_material": "/attachment/c40f0b21e37c2cde2f2152e47505321ac4e71ca2.zip"}, "replies": [{"content": {"summary": {"value": "The authors introduce an efficient training and inference methodology, called NeuroTrails. Neurotrails starts by pre-training a base model. It then initializes multiple heads, as in TreeNet. Weights are sparsified. Then, during training, weights can be reactivated using RigL. This results in a highliy performant network with few training and inference FLOPS."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 1}, "strengths": {"value": "* The paper shows strong predictive performance on a large number of tasks. \n* The paper is very clearly written. \n*  Ablations are interesting."}, "weaknesses": {"value": "* The single biggest weakness is a practical one. In Figure 7, the authors show a pareto frontier wrt throughput . But this is just on CPU. In practice, GPU's have become ubiquitous. How is speedup on GPUs with current sparse librarries? I suspect it is very difficult to get competitive walltime speedup, compared to methods which use dense multiplications (like MIMO). I believe this severly limits the utility of this method, and a solution does not appear to be on the horizon. \n\n* The novelty feels weak: It's an combination of two known methods:. TreeNet and RigL. Both methods have existed for a long time. \n\n* The most extensive comparisons are on CIFAR-100 and ImageNet. Of these two, only ImageNet represents a difficult and interesting problem."}, "questions": {"value": "* How do results compare with using lower precision weights instead? [1]\n* How practical are NeuroTrails on GPUs?\n* How do baselines perform on language modelling and RL tasks?\n[1] Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y. Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint arXiv:1606.06160, 2016."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0wIMfSIRbc", "forum": "bs7m1LbRKT", "replyto": "bs7m1LbRKT", "signatures": ["ICLR.cc/2026/Conference/Submission13844/Reviewer_YJp6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13844/Reviewer_YJp6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761620805955, "cdate": 1761620805955, "tmdate": 1762924369014, "mdate": 1762924369014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes NeuroTrails, a model-agnostic ensembling paradigm that shares an early backbone across multiple dynamically sparse heads, trained with dynamic sparse training (DST) and periodically re-wiring masks to explore diverse “neural trails.” At training time, each head receives its own loss and the losses are averaged; at inference time, logits are averaged while the shared backbone is computed once. The key claim is that, by selecting an appropriate split point and maintaining sparsity, NeuroTrails achieves a “Goldilocks” level of prediction diversity that improves accuracy/robustness while cutting inference FLOPs versus full ensembles."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Clear architectural idea. The split-backbone multi-head design is simple and broadly applicable. \n2) Diverse empirical coverage. Results span supervised vision, language pretraining and RL, with gains over single models, TreeNet, MIMO/BatchEnsemble, and full ensembles. \n3) Insight on ensemble diversity. The “PD Goldilocks zone” is a useful observation: the best CIFAR-100 result has lower disagreement than over-separated heads or a full ensemble, suggesting an optimal diversity band rather than “more is better.”"}, "weaknesses": {"value": "1) Novelty relative to TreeNet+DST is incremental. The core ingredients of the proposed method are shared early trunk (TreeNet) plus sparse training (RigL/SET) and multi-head branching, which are known individually. The paper should better isolate what new emerges from their combination beyond “do both”. \n2) Fairness/compute accounting needs tightening. The paper states sparse variants are trained longer (up to a factor ~1/(1−S)) while “keeping training FLOPs below dense counterparts,” but it is unclear whether all comparison baselines are matched on total training FLOPs or wall-clock time. It lacks a single, consolidated compute budget table to confirm matching across methods for each domain. \n3) Practicality on GPUs remains uncertain. Reported speedups rely on CPU(DeepSparse) and discuss emerging hardware. There is no measured GPU latency/throughput for standard frameworks where unstructured sparsity often yields limited wins. Add end-to-end latency on commodity GPUs, or show an engineering pathway to on-GPU speedups at target sparsities. \n4) Diversity measurement is narrow. PD is informative but coarse. Add CKA/representation similarity, error correlation, per-class co-disagreement, and calibration change from ensembling to establish the “Goldilocks” claim more rigorously (and check that higher PD does not merely correlate with miscalibration). \n5) Language experiments are small-scale. LLaMA-130M/350M are instructive but far from modern LLM scales. It is unclear if gains persist at ≥1–7B or with instruction-tuned models. \n6) Outdated comparison baselines (mostly ≤2022). The comparison set skews toward older methods , omitting strong recent contenders from 2023–2025."}, "questions": {"value": "1) For each table, are total training FLOPs and wall-clock matched across all baselines, including dense/full ensembles? \n2) What ∆T, prune fraction p, regrowth rule, and layerwise sparsity targets were used per domain? How sensitive are results to these hyperparameters? \n3) Can you report end-to-end inference latency/throughput on common GPUs for ImageNet and CIFAR-100 models at differend S values and compare with the dense model? \n4) Do accuracy gains persist if PD is held constant (e.g., via temperature/logit scaling or noise)? Can you add CKA/error-correlation analyses to substantiate the Goldilocks hypothesis? \n5) Have you tested ≥1B-parameter LLM backbones? Are attention projections the only dense components? If so, what is the impact of sparsifying them? \n6) Can you add a baseline that is TreeNet with DST but without shared mask, to isolate the specific gain from head-wise DST."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7Sd9U6KSKT", "forum": "bs7m1LbRKT", "replyto": "bs7m1LbRKT", "signatures": ["ICLR.cc/2026/Conference/Submission13844/Reviewer_er7G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13844/Reviewer_er7G"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761736435700, "cdate": 1761736435700, "tmdate": 1762924368579, "mdate": 1762924368579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work applies dynamic sparse training to the TreeNet architecture."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Fundamentally, this work can be viewed as a straightforward combination of DST and TreeNet, which makes it difficult to avoid potential challenges to its novelty claim. Nevertheless, assuming that no prior study has explicitly explored their integration, I would emphasize the strength of this work in providing a thorough analysis of the combination and, moreover, presenting an implementation that achieves practical speed improvements.\n\n- The paper provides a reasonable analysis of the components of TreeNet and DST. Ultimately, what matters is whether ensemble diversity is achieved, and this is well demonstrated through the prediction disagreement analysis."}, "weaknesses": {"value": "- It’s hard to say the presented language experiments as “LLM” experiments (title of Appendix G.1). While it’s understandable that not everyone has access to ample computational resources, the presented zero-shot reasoning results (Table 5) don’t appear particularly meaningful.\n\n- The DST Ensemble should also benefit from DeepSparse, so it would be appropriate to include it in Figure 7. According to Table 2, the DST Ensemble achieves an accuracy of 83.3, equivalent to that of the Full Ensemble, and should therefore appear somewhere to the right of it. It would be important to check where exactly it lies relative to the NeuroTrails boundary."}, "questions": {"value": "- Would it be possible to train the TreeNet architecture with DST in a transfer learning scenario? If so, it seems feasible to leverage a pre-trained model and conduct the language experiments accordingly.\n\n- What about the zero-shot reasoning results corresponding to Table 16?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EWBBQllKMz", "forum": "bs7m1LbRKT", "replyto": "bs7m1LbRKT", "signatures": ["ICLR.cc/2026/Conference/Submission13844/Reviewer_tnJm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13844/Reviewer_tnJm"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937244662, "cdate": 1761937244662, "tmdate": 1762924368183, "mdate": 1762924368183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces NeuroTrails, a novel and model-agnostic training paradigm for creating efficient and high-performing neural network ensembles. The core contribution lies in a unique architecture that combines a shared backbone with multiple, independent heads, where both components are trained using dynamic sparse training (DST). The authors claim this approach fosters a beneficial level of prediction diversity, which they term the \"Goldilocks zone,\" where diversity is neither too high nor too low, leading to optimal performance. This methodology is empirically validated across a wide range of tasks in computer vision, language modeling, and reinforcement learning, demonstrating superior performance over standard ensembles and related methods while requiring significantly fewer theoretical FLOPs for inference."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Simplicity and Generality:** The proposed method is elegant in its simplicity. The architectural split into a shared backbone and multiple heads is intuitive and can be readily applied to various existing architectures, including CNNs (ResNet, DQN) and Transformers (LLaMA), without requiring complex new components like the routers found in Mixture-of-Experts models. The extensive experiments successfully demonstrate this model-agnostic nature.   \n\n- **Novel Conceptual Contribution:** The identification of a \"Goldilocks zone\" for prediction diversity is a valuable and insightful contribution to the field of ensemble learning. It challenges the common assumption that maximizing diversity is always optimal. The paper provides compelling empirical evidence for this phenomenon (Table 7) and proposes the backbone split-point as a simple yet effective hyperparameter to control this diversity, offering a practical tool for practitioners.   \n\n- **Comprehensive Empirical Validation:** The paper's claims are supported by a rigorous and extensive set of experiments across disparate domains. NeuroTrails consistently outperforms strong baselines, including full ensembles and its direct predecessor TreeNet, on standard benchmarks like CIFAR-100, ImageNet, C4, and Atari. The reported improvements in accuracy, perplexity, and out-of-distribution robustness are significant, especially when considering the reduced theoretical inference cost (FLOPs)."}, "weaknesses": {"value": "- **The Unstructured Sparsity Bottleneck (Primary Weakness):** The paper's primary weakness, and a significant threat to its practical impact, is its reliance on unstructured dynamic sparsity (RigL/SET). While this approach effectively reduces theoretical FLOPs, it is well-documented that the irregular memory access patterns of unstructured sparsity do not translate to meaningful wall-clock speedups on modern parallel hardware like GPUs without specialized libraries or hardware. The paper's own speedup analysis (Figure 7) relies on the DeepSparse engine on CPUs, which represents a limited, non-standard deployment scenario for large-scale deep learning. This stands in stark contrast to recent advancements in hardware-aware DST, such as Structured RigL (SRigL), which generates N:M semi-structured sparsity that can be directly accelerated by modern GPUs. This makes the claimed efficiency gains largely theoretical in the most common deployment environments.   \n\n- **Scalability Concerns for Foundation Models:** The NeuroTrails paradigm requires activating all M heads for every forward pass, leading to an inference cost that scales linearly with the number of heads. This \"activate-all\" approach poses a significant scalability challenge for future foundation models that might require hundreds or thousands of specialized components. This scaling property is markedly less efficient than that of Mixture-of-Experts (MoE) architectures, where inference cost remains constant (proportional to a small k) regardless of the total number of experts, making MoE a more viable path for massive model scaling.   \n\n- **Limited Generalizability of the \"Goldilocks\" Hypothesis:** While the evidence for the \"Goldilocks zone\" on CIFAR-100 is compelling, the paper does not sufficiently demonstrate that this is a universal principle. It remains an open question whether this phenomenon holds across different datasets (e.g., ImageNet, C4), model scales, and modalities, or if it is an artifact of the specific experimental setup presented. Further investigation is needed to establish this as a fundamental principle of ensembling."}, "questions": {"value": "- Could you please comment on the practical inference performance (i.e., wall-clock time or throughput) of NeuroTrails on standard GPUs (e.g., NVIDIA A100) without specialized software like DeepSparse? Have you considered integrating your architectural concept with a hardware-aware DST method like Structured RigL to generate N:M sparse heads, which could potentially bridge the gap between theoretical FLOPs and actual GPU speedup?\n\n- The \"Goldilocks zone\" finding is fascinating. Have you performed analyses to see if this phenomenon holds on the other domains you tested, such as ImageNet or the C4 language modeling task? How sensitive is this optimal zone to other hyperparameters, such as the sparsity ratio or the choice of DST algorithm (e.g., SET vs. RigL)?\n\n- Given that the inference cost of NeuroTrails scales linearly with the number of heads M, how do you envision this approach scaling to future foundation models that may contain hundreds or thousands of specialized components? How does it compare to the constant-cost (top-k) scaling of MoE architectures in this large-scale regime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8HTQsf5SR7", "forum": "bs7m1LbRKT", "replyto": "bs7m1LbRKT", "signatures": ["ICLR.cc/2026/Conference/Submission13844/Reviewer_oJ41"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13844/Reviewer_oJ41"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006861612, "cdate": 1762006861612, "tmdate": 1762924365437, "mdate": 1762924365437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}