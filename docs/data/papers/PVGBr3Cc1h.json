{"id": "PVGBr3Cc1h", "number": 13615, "cdate": 1758219828103, "mdate": 1759897424552, "content": {"title": "Theoretical Guarantees for High Order Trajectory Refinement in Generative Flows", "abstract": "Flow matching has emerged as a powerful framework for generative modeling, offering computational advantages over diffusion models by leveraging deterministic Ordinary Differential Equations (ODEs) instead of stochastic dynamics. While prior work established the worst case optimality of standard flow matching under Wasserstein distances, the theoretical guarantees for higher-order flow matching - which incorporates acceleration terms to refine sample trajectories - remain unexplored. In this paper, we bridge this gap by proving that higher-order flow matching preserves worst case optimality as a distribution estimator. We derive upper bounds on the estimation error for second-order flow matching, demonstrating that the convergence rates depend polynomially on the smoothness of the target distribution (quantified via Besov spaces) and key parameters of the ODE dynamics. Our analysis employs neural network approximations with carefully controlled depth, width, and sparsity to bound acceleration errors across both small and large time intervals, ultimately unifying these results into a general worst case optimal bound for all time steps.", "tldr": "", "keywords": ["Flow Matching", "Neural Network Approximation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ce26230ed6bd68b606c391ef5f7335e2991e50e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper builds on the work by Oko et al and Fukumizu et al on the generalization bound of Diffusion process and Flow Matching. The paper brings to the attention that higher order matching methods are emerging, and show that the acceleration matching loss (an approximation risk)  can be bounded from above by the worst case optimal convergence rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper certainly has the credit of"}, "weaknesses": {"value": "I believe that the paper is weak in motivation. Fukumizu et al sets its goal in bounding the wasserstein error between the generated distribution and the target distribution, which is a direct, practical measure of the \"wellness of genenaration\". \n\nIt is very much probable that bounding this risk at the Besov-minimax rate guarantees that any consistent flow trained via this regression (acceleration matching) will yield distributions whose estimation error (in Wasserstein or TV) is of the same order — but the paper never formalizes that link.  Without such support, I am finding it hard to find the value of the results presented in this paper *on this conference venue*. \n\nAlso, it was to be hoped that the theoretical results would support the necessity of higher order matching in some way, but the paper does not seem to provide the connection."}, "questions": {"value": "My greatest question is posed in the weakness section. Can the main results in this paper be directly related to a loss more practical then the the acceleration regression error  $E_{p_t}[ || u_1 - a ||]$ ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Nothing in particular, this is a purely theoretical research."}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "U7v4BwBPOa", "forum": "PVGBr3Cc1h", "replyto": "PVGBr3Cc1h", "signatures": ["ICLR.cc/2026/Conference/Submission13615/Reviewer_pSP1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13615/Reviewer_pSP1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743162282, "cdate": 1761743162282, "tmdate": 1762924195181, "mdate": 1762924195181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an upper bound on the estimation error for second-order flow matching. They show that the error depends polynomially on the smoothness of the target distribution."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "They present a bound for second-order flow matching, unlike previous works that only discuss first-order matching."}, "weaknesses": {"value": "Is the use or training of second-order derivatives common in diffusion or flow-based literature? To my knowledge, their use is niche; most training-based acceleration approaches rely on explicitly learning integral (e.g., consistency models, reflow). Non-training-based approaches (e.g., better solvers) just use (and have to use) the first-order gradient. Clarifying the significance of the results would be helpful.\n\nAdditionally, the definition of  N is unclear. In the paper, N is introduced only in relation to other parameters and never explicitly defined. Because of this coupling, it is difficult to reason about how the bound behaves when, for instance, the neural network is fixed and the sample size increases, or vice versa.\n\n> Furthermore, a multitude of studies have served as supplementary inspirations for our work (Xu et al., 2022a; Dax et al., 2023; Pooladian et al., 2023; Wang et al., 2023c;a; Shen et al., 2025a;b; Wang et al., 2024b; Chen & Lipman, 2024; Klein et al., 2024; Chen et al., 2025c; Cao et al., 2025; Cheng et al., 2024; Wang et al., 2023b; Feng et al., 2024b; Liu et al., 2024; Hu et al., 2024e).\n\nRather than listing a large block of references, it would be more informative to properly discuss these works and explain their connections to this study. Also, discussions of U-ViT, latent diffusion, or multi-scale noise schedules do not seem to directly clarify or contextualize the present work."}, "questions": {"value": "Why not show the distributional bound between the true and generated distributions?\n\nDoes the main theorem provide non-trivial insights/guidelines for practitioners?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "U5s0XHRQar", "forum": "PVGBr3Cc1h", "replyto": "PVGBr3Cc1h", "signatures": ["ICLR.cc/2026/Conference/Submission13615/Reviewer_HDXY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13615/Reviewer_HDXY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761757047506, "cdate": 1761757047506, "tmdate": 1762924194761, "mdate": 1762924194761, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes higher-order flow matching (with an acceleration term) and claims it preserves worst case optimality as a distribution estimator. It proves acceleration-error bounds for small-t (Theorem 4.1) and large-t (Theorem 4.2), with rates tied to Besov smoothness and neural-network complexity."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "(1) To the best of my knowledge, this is the first paper to provide estimates within the higher-order Flow Matching framework, demonstrating nearly minimax-optimal convergence rates.\n(2) Contributions and roadmap are clearly signposted."}, "weaknesses": {"value": "(1) Within the framework of [1,2], the manuscript carries out analogous derivations for higher-order flow matching.\n(2) Assumptions 5.2–5.9 and their role in the theoretical development are important; however, they appear to have already been clarified in [2].\n(3) The essential estimates underlying the main results seem to occur in Lemmas 5.12–5.15. These lemmas primarily develop bounds for the higher-order components (e.g., acceleration terms), which, while technically sound, do not strike me as conceptually deep or revealing new insights beyond [2]. \n\n[1] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. volume 202, pages 26517–26582. PMLR, 4 2023\n[2] Fukumizu K, Suzuki T, Isobe N, et al. Flow matching achieves almost minimax optimal convergence[J]. arXiv preprint arXiv:2405.20879, 2024."}, "questions": {"value": "Could you clarify where the manuscript reveals a substantive, conceptually new contribution that departs from [2]?\nDefinition 3.10 mixes the symbols \\alpha, \\beta with a, b;\nAssumptions 5.7–5.8 use the notation \\kappa and \\overline{\\kappa}, but \\overline{\\kappa} is not defined earlier. Do you intend \\overline{\\kappa} to be the \\tilde{\\kappa} introduced in Assumption 5.4?\nAssumptions 5.5–5.6 bound |\\alpha''_t|+|\\beta''_t|by K_0 N for all t \\in [T_0,1], making these derivatives scale linearly in the sample-size parameter N. Could you justify this dependence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j06FAPbBFJ", "forum": "PVGBr3Cc1h", "replyto": "PVGBr3Cc1h", "signatures": ["ICLR.cc/2026/Conference/Submission13615/Reviewer_qiTf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13615/Reviewer_qiTf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13615/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896416080, "cdate": 1761896416080, "tmdate": 1762924194186, "mdate": 1762924194186, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}