{"id": "BaFPkXHdrg", "number": 7493, "cdate": 1758024523848, "mdate": 1759897849509, "content": {"title": "Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data", "abstract": "Large language models~(LLMs) are expected to be helpful, harmless, and honest.\nIn different alignment scenarios, such as safety, confidence, and general preference alignment,\nbinary preference data collection and reward modeling are resource-intensive but play a central role in transferring human preferences.\nIn this work, we explore using the similarity between sampled generations and reference answers as a supplementary reward function for alignment.\nWhen unary reference answers are available,\nsuch similarity-based rewards can circumvent the need for binary preference data and explicit reward modeling.\nWe introduce \\textit{RefAlign}, a versatile REINFORCE-style alignment algorithm that does not rely on reward or reference models.\nRefAlign utilizes language generation evaluation metrics, such as BERTScore, between sampled generations and reference answers as surrogate rewards.\nBeyond general preference optimization, \nRefAlign can be naturally extended to diverse scenarios, including safety and confidence alignment, by combining similarity-based rewards with task-specific objectives.\nAcross multiple scenarios, RefAlign achieves performance comparable to prior alignment methods while operating without binary preference data or reward models.", "tldr": "", "keywords": ["alignment", "no binary preference data", "safey alignment", "confidence alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c2065c929a993c9e5ae501d87f49e6c449f08255.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces an alignment method that is an alternative to the most common way of performing alignment: using binary preference judgments from humans and training reward models on those judgments for use during RL. Instead of using binary judgments, it uses unary signals. Specifically, it uses the similarity (as measured by traditional similarity metrics such as BERTScore and METEOR) between generations and a reference answer as proxies for reward model signals. The authors find performance similar to existing methods using this approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. To my knowledge, the use of existing text similarity metrics as a proxy reward signal is novel. While some methods like KTO (which the authors cite and compare against) don't use preference judgments, this method seems to be more efficient. \n2. The experiments are pretty thorough and clearly presented, with extensive comparisons with baselines. \n3. The comparisons with different similarity metrics, and especially the analysis with respect to factors such as response length, were very interesting and demonstrate the trade-offs of using different metrics."}, "weaknesses": {"value": "1. My main concern is regarding the scalability of this approach. While it is certainly true that preference annotations are costly to obtain, the same is true for reference answers. High-quality references require much more validation, and that validation itself can be a lot trickier to actually perform. In comparison, relative judgments can be more robust \"out-of-the-box.\" That being said, I think there is certainly an argument for using high-quality references as opposed to binary preferences in certain applications. For instance, I think this may be true for tasks requiring niche domain expertise. I think this work would be more compelling with a more targeted analysis for certain tasks where this is the case, as opposed to the more general helpfulness/harmfulness framework. \n2. There has been a lot of work showing the flaws of automatic metrics (e.g. [1]), especially when references are low-quality. To be clear, the authors do address the trade-offs and particular biases of using certain metrics (such as some metrics causing a bias towards longer responses). However, I think these flaws compound the issue described in the previous weakness. While reward hacking is always a concern even with the usual BT-based setup, I worry that this might be even more prevalent and subtle when using these flawed metrics. Again, with this, I think the paper would be stronger with a more targeted analysis over different domains.\n3. While I do think the analysis of outputs is helpful in the paper, I think it could be expanded to analyze, for instance, the diversity of the resulting outputs after using RefAlign. I also think a small human validation/comparison experiment would be helpful. \n\n[1] Goyal, Tanya, Junyi Jessy Li, and Greg Durrett. \"News summarization and evaluation in the era of gpt-3.\" arXiv preprint arXiv:2209.12356 (2022)."}, "questions": {"value": "1. Why is recall used for BERTScore instead of F1 (or even precision)?\n2. For the ~30% of cases where BERTScore does not correspond to the RM, did you notice any patterns?\n3. Is there a risk of inducing even more mode collapse when optimizing with respect to one reference? Was this something you noticed when analyzing the outputs after using RefAlign?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8QLk9eBnSk", "forum": "BaFPkXHdrg", "replyto": "BaFPkXHdrg", "signatures": ["ICLR.cc/2026/Conference/Submission7493/Reviewer_5LPR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7493/Reviewer_5LPR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761762343434, "cdate": 1761762343434, "tmdate": 1762919606868, "mdate": 1762919606868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RefAlign, a reinforcement learning (RL)-based alignment method for large language models (LLMs) that leverages similarity (BERTScore) between model-generated responses and high-quality reference answers as a surrogate reward function, eliminating the need for costly binary human preference data. RefAlign simplifies the alignment pipeline and can be effectively adapted for general human preference, safety, and confidence alignment scenarios. Empirical experiments demonstrate that RefAlign achieves comparable performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes an approach that leverages similarity between generated responses and high-quality reference answers as a surrogate reward, thereby eliminating the dependency on binary human preference data.\n2. The paper thoroughly validates its approach across various tasks and datasets."}, "weaknesses": {"value": "Although the method proposed in this paper addresses the issue of reducing dependency on reward models, it introduces several limitations. First, the requirement of high-quality reference answers imposes constraints, as collecting these unary reference responses still demands significant human effort—albeit potentially less than binary labeling. More fundamentally, a key advantage of traditional reinforcement learning from human feedback (RLHF) is its capability to optimize models solely based on prompts without requiring pre-existing high-quality responses. In contrast, the algorithm presented here necessitates high-quality reference responses, making practical application somewhat limited. Lastly, while this method replaces the reward model with a text-similarity model, the actual reduction in computational and annotation costs appears minimal, as one form of labeling and model dependency is merely substituted for another.\nWhen the quality of model-generated responses is actually higher than the reference answers, similarity-based scoring metrics can have counterproductive effects."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GdZTwAaiqQ", "forum": "BaFPkXHdrg", "replyto": "BaFPkXHdrg", "signatures": ["ICLR.cc/2026/Conference/Submission7493/Reviewer_Rw8W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7493/Reviewer_Rw8W"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966661404, "cdate": 1761966661404, "tmdate": 1762919606427, "mdate": 1762919606427, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- RefAlign aligns language models to a provided reference answer using semantic similarity as the reward in a simple REINFORCE‑style update.\n- It removes preference pairs and reward models, aiming for a compact pipeline with competitive results on safety alignment, confidence calibration, and general instruction benchmarks.\n- The approach is practical and easy to implement, but performance depends on reference quality and careful length control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Originality: Unifies alignment around reference similarity and removes reward models and preference pairs.\n- Quality: Strong results in safety and confidence alignment; competitive general alignment; reasonable ablations and comparisons.\n- Clarity: Method and objective are easy to follow; training details are practical.\n- Significance: Useful for bootstrapping smaller models from strong references, with lower data and engineering cost."}, "weaknesses": {"value": "- Reference quality dependence: effectiveness hinges on high‑quality, diverse references; noisy or narrow sets can misalign models.\n- Length bias and calibration: outputs tend to be longer; needs explicit length penalties or controls and more consistent calibration reporting (e.g., ECE).\n- Judge consistency: results may vary by judge; add multi‑judge analysis and open‑source judge checks.\n- Baseline breadth: include same‑compute, parallel comparisons to direct optimization baselines (DPO, SimPO, ORPO, KTO) with matched settings.\n- Generalization: broaden evidence to more domains and harder tasks beyond the reported benchmarks."}, "questions": {"value": "- How stable are results across different similarity metrics (e.g., BERTScore, BLEURT, cosine embeddings)?\n- Do multiple references per prompt improve robustness, and how are conflicts resolved?\n- What is the accuracy–length trade‑off under explicit length penalties or constraints, and how does calibration change?\n- How do results differ when references are human‑written or curated versus model‑generated?\n- Can you provide same‑compute, parallel comparisons to direct optimization baselines and report training stability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gUIogP8WaX", "forum": "BaFPkXHdrg", "replyto": "BaFPkXHdrg", "signatures": ["ICLR.cc/2026/Conference/Submission7493/Reviewer_2HJV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7493/Reviewer_2HJV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980222810, "cdate": 1761980222810, "tmdate": 1762919606105, "mdate": 1762919606105, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes RefAlign, a REINFORCE-style algorithm for language model alignment. The method uses the similarity between model generations and unary reference answers, measured by metrics like BERTScore, as a surrogate reward function. This approach aims to achieve alignment in various scenarios without requiring binary preference data or a separately trained reward model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper is well-written and clearly organized, making the proposed method easy to understand.\n+ The experiments and corresponding analysis cover a wide range of tasks. And the baseline methods are thorough.\n+ The proposed framework is versatile and can be adapted to different alignment tasks, including safety and confidence alignment, by modifying the reward function."}, "weaknesses": {"value": "+ The main problem is the issue of unfair comparison in the experimental setup. The authors use external data from Llama-3.3-70B-Instruct to generate reference answers for training RefAlign. It is not clear whether the baseline methods also utilized this external data. My concern is that RefAlign's performance gains may largely benefit from this additional information gain, rather than the algorithmic novelty alone.\n+ The core mechanism of RefAlign appears to be a reinforcement learning version of distillation. Using a BERTScore reward to match outputs from a more powerful model is conceptually very close to distillation, which the paper also compares against. As the reward model optimizes for text similarity, the proposed algorithm may be useful to a part of the community, but the contribution remains limited."}, "questions": {"value": "To my understanding RefAlign is a reinforcement learning algorithm, why not include other RL baselines (GRPO, PPO, DAPO...) ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t1GQoIPsHf", "forum": "BaFPkXHdrg", "replyto": "BaFPkXHdrg", "signatures": ["ICLR.cc/2026/Conference/Submission7493/Reviewer_JyiU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7493/Reviewer_JyiU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7493/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762109838466, "cdate": 1762109838466, "tmdate": 1762919605580, "mdate": 1762919605580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}