{"id": "bb4nYKkmAn", "number": 22265, "cdate": 1758328610456, "mdate": 1759896876229, "content": {"title": "3DSPA: A 3D Semantic Point Autoencoder for Evaluating Video Realism", "abstract": "AI video generation is evolving rapidly.\nFor video generators to be useful for applications ranging from robotics to film-making, they must consistently produce realistic videos.\nHowever, evaluating the realism of generated videos remains a largely manual process -- requiring human annotation or bespoke evaluation datasets which have restricted scope.\nHere we develop an automated evaluation framework for video realism which captures both semantics and coherent 3D structure and which does not require access to a reference video.\nOur method, 3DSPA, is a 3D semantic point autoencoder which integrates 3D point trajectories, depth cues, and DINOv2 semantic features into a unified representation for video evaluation. 3DSPA models how objects move and what is happening in the scene, enabling robust assessments of realism, temporal consistency, and physical plausibility. Experiments show that 3DSPA reliably identifies videos which violate physical laws, is more sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism across multiple datasets. Our results demonstrate that enriching trajectory-based representations with 3D semantics offers a stronger foundation for benchmarking generative video models, and implicitly captures physical rule violations.", "tldr": "", "keywords": ["intuitive physics", "cognition", "point tracking", "autoencoder", "generative video modeling"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a49d04f18260bc587eda4b68abcb66b040db3118.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes using 3D point cloud trajectory reconstruction as a criterion for evaluating the realism of video generation models, representing the first genuine attempt to address physical and motion inaccuracies in generated videos from a 3D perspective, which I believe is the proper way. By integrating both 3D and 2D semantic information, the authors construct a robust input representation. The method is trained on both synthetic and real-world datasets and is validated on TAPVid-3D, EvalCrafter, and VideoPhy2, demonstrating its effectiveness"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper firstly proposes the way to evaluate the realism of generated videos in a 3D perspective, which is a promising way for future works.\n2. The result shows that 3DSPA works accurately both in real and generated videos.\n3. The whole paper is well writen and easy to understand."}, "weaknesses": {"value": "1. The authors did not provide a clear rationale in the paper for the necessity of the supported point track input; its effectiveness is only demonstrated through ablation experiments. It would be better if the paper included more discussion and justification for this component.\n2. The way for selecting the initial point is unclear, if utilizing uniform selection strategy, how to ensure the point will fall into the entity with motion or fast motion (the scene/entity changes several time in a video)\n3. The time costing compared with 2d vlm-based methods, which should be reported as it is important for a evaluation methods. (Note that what I would like to know is the time required to reproduce the results presented in the paper. )"}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OFNHTSoeBH", "forum": "bb4nYKkmAn", "replyto": "bb4nYKkmAn", "signatures": ["ICLR.cc/2026/Conference/Submission22265/Reviewer_QxzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22265/Reviewer_QxzU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760884032292, "cdate": 1760884032292, "tmdate": 1762942142458, "mdate": 1762942142458, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces 3DSPA (3D Semantic Point Autoencoder), an automated framework for evaluating the realism of generated videos, addressing limitations of existing methods in assessing video realism, temporal, and physical plausibility. By integrating 3D point trajectories, depth cues, and DINO semantic features into a unified representation, 3DSPA enables robust assessment of video realism, temporal consistency, and physical plausibility without requiring a reference video. Its primary contribution lies in its ability to reliably identify videos that violate physical laws, its high sensitivity to motion artifacts, and its closer alignment with human judgments of video quality and realism, even outperforming state-of-the-art vision-language models in detecting physical inconsistencies. The research highlights that the combination of semantic and 3D geometric information is crucial for judging physical realism, and that 3DSPA offers a scalable and effective alternative to the labor-intensive human evaluation of video realism."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Novel Framework: 3DSPA integrates 3D point trajectories, depth cues, and DINO semantic features into a unified representation, allowing for robust assessments of video realism, temporal consistency, and physical plausibility without requiring a reference video.\n\nEnhanced Realism Detection: It reliably identifies videos that violate physical laws, is highly sensitive to motion artifacts, and aligns more closely with human judgments of video quality and realism compared to existing methods.\n\nSuperior Performance: Through extensive experiments, 3DSPA demonstrates better performance in 3D point track reconstruction, physical rule violation detection, and matching human annotations of realism, even outperforming state-of-the-art vision-language models in detecting physical inconsistencies. The authors highlight that the combination of semantic and 3D geometric information is crucial, with semantic information being particularly important for judging physical realism.\n\nScalable Alternative: 3DSPA offers a scalable and effective alternative to the labor-intensive human evaluation of video realism."}, "weaknesses": {"value": "1. Representation weakness: Table 1 is out of page, and the paper is less than 9 pages in length.\n2. The difference between 3DSPA and TRAJAN+DINO/+3D need more disscusions.\n3. Lacking of ablations: There is no ablation studies in paper and supp. Only comparison with TRAJAN variants are involved. Ablations of different components in 3DSPA are expected. \n4. More visualization results are expected, including 3D point tracks reconstructed by 3DSPA for more videos, and more unrealistic videos which 3DSPA scores poorly but TRAJAN scores highly."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Ktyj3Vmbcc", "forum": "bb4nYKkmAn", "replyto": "bb4nYKkmAn", "signatures": ["ICLR.cc/2026/Conference/Submission22265/Reviewer_Yrng"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22265/Reviewer_Yrng"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761880753342, "cdate": 1761880753342, "tmdate": 1762942142217, "mdate": 1762942142217, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes 3DSPA, a novel framework for evaluating video realism. It encodes 3D point trajectories coupled with depth cues and DINOv2 semantic embeddings to jointly capture motion dynamics and physical plausibility. Experiments on diverse datasets show that 3DSPA effectively captures motion and physical information and correlates closely with human realism judgments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Paper is well written and organized.\n\n2. Novel 3D semantic-physical representation: Proposes a 3D point autoencoder that jointly encodes geometric motion and semantic features, allowing more comprehensive realism assessment..\n\n3.  Strong empirical validation and interpretability: Experiments valid  3DSPA is capable of reconstructing 3D point tracks and detect physical rule violations. Extensive results show consistent superiority over previous evaluators."}, "weaknesses": {"value": "1. Reliance on additional models: The 3d points are obtained with CoTracker3 and Video Depth Anything. These models are strong in point tracking and metric depth estimation, yet still have limitations, for example, cotracker3 may fail with small objects in the video and VDA may face issues with sharp lines. Also, dramatic motions or camera movements remain problematic in geometric models. Will these possible artifacts or errors of geometric prediction affect the results of 3DSPA? Look forward to authors' analysis or discussion on these cases.\n2. Inference efficiency: With the introduction of multiple geometric and semantic model, comparison of compute cost with existing baselines should be provided.\n3. Failure case analysis: Include some failure case on motion tracking would benefit this paper. I am curious where the failure come from, hallucination from semantic model or geometric model."}, "questions": {"value": "Please refer to weakness.\n\nAdditional questions\n\n4.  generalization: The performance of 3DSPA is closely related to training data. How 3DSPA perform on more diverse videos (e.g. robotics / autonomous driving / 3d scene / games ...) What is the cost and performance if fine-tune 3DSPA to do evaluation on specific domains?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "s5j3bMPUql", "forum": "bb4nYKkmAn", "replyto": "bb4nYKkmAn", "signatures": ["ICLR.cc/2026/Conference/Submission22265/Reviewer_Fjpp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22265/Reviewer_Fjpp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961208401, "cdate": 1761961208401, "tmdate": 1762942141980, "mdate": 1762942141980, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}