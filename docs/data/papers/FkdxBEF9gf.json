{"id": "FkdxBEF9gf", "number": 5080, "cdate": 1757845069262, "mdate": 1763651693264, "content": {"title": "Can-Cap: Calibration-Free and Noise-Resilient Human Motion Capture via LiDAR-Camera Integration", "abstract": "We propose $\\textbf{Can-Cap}$, a $\\underline{\\textbf{Ca}}$libration-Free and $\\underline{\\textbf{N}}$oise-Resilient 3D human motion $\\textbf{Cap}$ture framework that integrates multi-modal data from LiDAR and camera. While multi-modal sensors provide richer information than single-modal sensors, most existing approaches rely on pre-calibration for cross-sensor alignment, which propagates errors, especially when sensors have varying or dynamically changing perspectives. This reliance also requires fixed sensor placement with highly overlapping views, limiting flexibility and diminishing the benefits of diverse viewpoints for handling occlusions. Furthermore, prior methods often degrade under substantial noise or partial sensor failures, conditions common in real-world scenarios. To address these challenges, $\\textbf{Can-Cap}$ introduces a Unified Across-Sensor Motion Estimator that reconstructs local pose and shape in a human-centric space without calibrations between sensors, supporting flexible number of sensors, and a Noise-Resistant Trajectory Tracker that maintains robustness under severe point cloud noise through iterative refinement. These calibration-free and noise-resilient features makes CAN-Cap more practical in real-world deployment. Notable, operating in real time at 25 FPS, $\\textbf{Can-Cap}$ achieves state-of-the-art results on Human-M3 and FreeMotion, as well as strong cross-domain performance on LiDARHuman and RELI11D. This combination of flexibility and robustness opens new opportunities for motion capture in real-world scenarios, e.g. sports analytics, field robotics, and large-scale immersive environments.", "tldr": "", "keywords": ["Motion Capture"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b49984cf8d18b01ecf271c231bf024f012cc1311.pdf", "supplementary_material": "/attachment/06d88b6f19e0504492940c5d89065195913350b9.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a calibration-free and noise-resilient 3D human motion capture framework. A key difference between this work and existing methods is that it can support a flexible number of sensors. Its performance on long-term trajectory estimation also significantly outperforms other methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.The proposed method supports a flexible number of sensors and has been thoroughly evaluated through ablation studies under different sensor configurations.\n\n2.It achieves improvements over existing methods such as WHAM, LiveHPS++, and FreeCap."}, "weaknesses": {"value": "1.The overall contribution is rather limited. I believe this work simply combines the WHAM and LiveHPS++ methods, without significant innovation in the network architecture.\n2.There is a lack of comparative experiments with other multimodal methods; the experiments include only FreeCap as a multimodal method."}, "questions": {"value": "1.Could the authors specifically describe how multiple LiDARs and cameras were set up during data collection?\n2.Could experiments be conducted on other datasets with multiple LiDARs and cameras?\n3.Why does Human-Centric spatial alignment perform better than LiDAR-Centric spatial alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NYQz91KlFb", "forum": "FkdxBEF9gf", "replyto": "FkdxBEF9gf", "signatures": ["ICLR.cc/2026/Conference/Submission5080/Reviewer_wiYn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5080/Reviewer_wiYn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707459840, "cdate": 1761707459840, "tmdate": 1762917862893, "mdate": 1762917862893, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Can-Cap, a framework for 3D human motion capture which integrates multi-modal data from LiDAR and cameras. The primary contributions are that it is “Calibration-Free” and “Noise-Resilient”, which the authors claim to be the key limitations of the existing multi-modal approaches. Can-Cap achieves this by two core components, the Unified Across-Sensor Motion Estimator (which reconstructs the local pose and shape) and the Noise-Resistant Trajectory Tracker (which estimates the global trajectory)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. **Strong Empirical Results**: The method achieves strong results on the provided benchmarks (Human-M3 and FreeMotion), outperforming prior SOTA methods like FreeCap, WHAM, and LiveHPS++ in several key metrics, especially in novel viewpoint scenarios.\n\n2. **Interesting Core Concept**: The \"calibration-free\" approach, enabled by the Unified Across-Sensor Motion Estimator (UAME) that maps inputs to a common human-centric space, is a promising direction that avoids the error propagation common in explicit calibration steps.\n\n3. **Sensor Flexibility**: The paper demonstrates that a single model trained with multiple sensors (e.g., 3 LiDARs + 3 Cameras) can still function effectively when \"degraded sensor configurations\" (e.g., fewer sensors) are used at test time, which is a useful feature for real-world robustness."}, "weaknesses": {"value": "The paper's primary weakness is that its central claims of \"Fault-tolerance\" and \"Noise-resistance\" are \"over-sold\" and not sufficiently supported by the explanations or experiments. The work is difficult to evaluate fully, as it lacks a clear analysis of its limitations and failure cases.\n1. **Unclear \"Fault-Tolerant\" Claim**: The paper claims the UAME's second stage is a \"Fault-Tolerant Sensor Fusion\" module. However, the text does not explain what makes this module \"fault-tolerant.\" It appears to be a standard attention-based sensor fusion mechanism. The motivation for using attention is weak, and the ablation study (Table 4) merely shows it outperforms simple concatenation or addition (\"ours\" vs. \"cat\" and \"add\"). This provides limited insight and does not substantiate the claim of fault tolerance.\n2. **Insufficient Evidence for \"Noise-Resistance\"**: The paper's motivation for the Noise-Resistant Trajectory Tracker (NTT) is sound (i.e., standard normalization fails under noise). However, the description of the NTT itself lacks explanatory depth. It is not clear how the proposed iterative refinement (Algorithm on pg 6) is \"noise-resistant\" in principle, as it appears to be a standard iterative refinement technique. The experimental evidence—while showing good performance on the noisy LiDARHuman26M dataset (Table 2)—is not a direct proof of noise resistance. The evaluation would greatly benefit from a controlled experiment on a clean dataset where increasing levels of synthetic noise are introduced to measure the NTT's performance degradation. Without this, the \"noise-resistant\" claim is an assertion, not a proven property.\n3. **Misleading \"Sensor Scalability\" Claim**: The \"sensor scalability\" claim is misleading. \"Scalability\" suggests the system can scale up to use more sensors during inference than were used at training time. The experiment in Table 3 demonstrates the opposite: the model shows graceful degradation when using fewer sensors than it was trained on. This is a useful feature (robustness to sensor drop-out), but it is not scalability.\n4. **Unhelpful Figure**: Figure 2, while visually appealing, does little to help the reader understand the novelty of the UAME and NTT. It is a high-level data-flow diagram that includes unexplained elements, such as the \"Dynamic Mask\" and the \"Searching Space,\" which are never mentioned or explained in the main text. This is a significant flaw in the paper's presentation.\n5. **Lack of Failure Case Analysis**: The paper is lacking in its analysis of failure cases. The \"Limitations\" section in the appendix only briefly states that performance degrades in \"severe weather conditions\", which is a generic limitation. A more in-depth discussion of when the UAME's alignment fails or when the NTT's tracking breaks would be necessary to understand the method's true boundaries."}, "questions": {"value": "1. What, specifically, makes the Stage 2 \"Fault-Tolerant Sensor Fusion\" module \"fault-tolerant\" beyond what a standard attention-based fusion module provides?\n2. In Stage 2, the paper introduces a \"learnable fusion token\" ($F_{token}$) that \"serves as a query to aggregate multi-modal features\". Is this a single, shared token for the entire model? This token's role and design could be introduced more clearly.\n3. Could you please explain what is architecturally \"noise-resistant\" in the NTT?\n4. Can the authors please clarify if the model can \"scale\" in the other direction? For instance, can the same model trained on (N=3, M=3) sensors accept input from (N=4, M=4) sensors at inference time or is the number of input sensors capped after training?\n5. Could you please explain what the \"Dynamic Mask\" and the \"Searching Space\" components are?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dGaBgxG1So", "forum": "FkdxBEF9gf", "replyto": "FkdxBEF9gf", "signatures": ["ICLR.cc/2026/Conference/Submission5080/Reviewer_9aPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5080/Reviewer_9aPy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848185843, "cdate": 1761848185843, "tmdate": 1762917861979, "mdate": 1762917861979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a framework for utilizing camera and LiDAR sensor data for motion capture/SMPL parameter prediction. The key contribution over existing works is (1) enabling motion capture from any number of uncalibrated sensors (2) noise resiliency (3) improving upon existing methods' use of naive sensor-fusion methods. The authors achieve this by combining modules from existing methods into two new modules, (1) a Unified Across-Sensors Motion Estimator which aligns encoded LiDAR features into the human-centric space and fuses sensors using a cross attention based method (2) a Noise-resistant Trajectory Tracker that predicts point cloud offsets at each timestep to get a trajectory."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The authors have conducted thorough experimentation and ablation studies across many baselines and datasets.\n- The paper introduces a framework and demonstrates its effectiveness using real-world application demos where the system has been deployed. These demos show great results and applicability 'in the wild'.\n- The example visualizations show significant improvements in qualitative results compared to baselines."}, "weaknesses": {"value": "- How is noise introduced in the noisy sensor ablation? Was there experimentation/ablation on different severity of added noise? This ablation will be helpful for corroborating the effectiveness of the NTT module and its resilience to all kinds of noise.\n- (minor) The sentence on line 269 doesn't quite make sense. There appears to be a typo on line 446 in the caption for table 4 (Trajectory)"}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Op0DQy785T", "forum": "FkdxBEF9gf", "replyto": "FkdxBEF9gf", "signatures": ["ICLR.cc/2026/Conference/Submission5080/Reviewer_N9Dp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5080/Reviewer_N9Dp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866146358, "cdate": 1761866146358, "tmdate": 1762917860009, "mdate": 1762917860009, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a multi-modal (LiDAR + RGB) 3D human motion capture solution. At its core, the paper fuses data from all available sensors into a common human-centric space, using LiveHPS++ for point clouds and WHAM for 2D keypoint estimation from images. Doing so results in a calibration-free approach, further refined by a trajectory-tracking algorithm that iteratively corrects for noise and occlusions to improve the localization of human subjects. Through experimental evaluation against recent state-of-the-art approaches, the paper reports better performance and provides a detailed ablation study on the effectiveness of their design choices."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The core idea of mapping the sensor's input into a human-centric frame is sufficiently novel and effective\n- The experimental comparisons with methods relying on LiDAR, RGB Camera, and both are rigorous and highlight the effectiveness of the proposed method\n-  Achieving a calibration-free solution is a plus\n- The paper is well-structured and easy to follow"}, "weaknesses": {"value": "1. The paper contains several writing issues:\n   - There are some inconsistencies in how in-line lists are formatted. For example, in lines 92 and 87, the \"1)\" and \"2).\" formats are used. The dot after the closed parenthesis is not necessary. The same issue is also present in Line 246, Line 308, and Line 314\n   - End of Line 102, \"While the NTT.\" is an incomplete sentence.\n   - L136 - \"Early motion capture methods (that) estimate high-quality human motions rely on wearable sensors\" is missing the \"that\"\n   - Figure 2 - The direction of the arrow going out of the Learnable Query should be reversed according to the paper's descriptions.\n   - L224 - \"algorithm(FPS)\" should be changed to \"algorithm (FPS)\"\n   - L370 - \"In generally\" should be changed to \"In general\" or \"Generally\"\n   - Table 4 - \"Noise-resistant Tra- jectory\" should change to \"Noise-resistant Trajectory\"\n   - Figure 6 and 7 call the method \\$ A^3 \\$Cap\n2. The LiDAR-only variant of the mode performs worse than the LiveHPS++ baseline. This shows that while the hybrid model is more successful than FreeCap, the LiDAR branch is not as good as previous works.\n3. The real-time performance is mostly due to the TensorRT optimization, and not a direct result of scientific contribution. While this is clarified in the supplementary materials, it gives a wrong impression in the main paper. It would be better to compare performance with prior work on an equal basis.\n4. The supplementary video also has some frozen frames around 3, 4, 7, and 8 minute marks"}, "questions": {"value": "1. Please clarify how the multi-person matching strategy works. Section 3.1 mentions that it is adapted from FreeCap, but then the Supplementary materials mention OSNet, which is a ReID model not mentioned in FreeCap.\n2. Please add a discussion on why you think the LiDAR-only variant of CanCap underperforms LiveHPS++."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SWhPLoIKmc", "forum": "FkdxBEF9gf", "replyto": "FkdxBEF9gf", "signatures": ["ICLR.cc/2026/Conference/Submission5080/Reviewer_k6im"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5080/Reviewer_k6im"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5080/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970754303, "cdate": 1761970754303, "tmdate": 1762917859559, "mdate": 1762917859559, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}