{"id": "kMfVTka2WB", "number": 20322, "cdate": 1758304726979, "mdate": 1759896983886, "content": {"title": "An Algorithm to perform Covariance-Adjusted Support Vector Classification in Non-Euclidean Spaces", "abstract": "Traditional Support Vector Machine (SVM) classification is carried out by finding the max-margin classifier for the training data that divides the margin space into two equal sub-spaces. This study demonstrates limitations of performing Support Vector Classification in non-Euclidean spaces by establishing that the underlying principle of max-margin classification and Karush Kuhn Tucker (KKT) boundary conditions are valid only in the Euclidean vector spaces, while in non-Euclidean spaces the principle of maximum margin is a function of intra-class data covariance. The study establishes a methodology to perform Support Vector Classification in Non-Euclidean Spaces by incorporating data covariance into the optimization problem using the transformation matrix obtained from Cholesky Decomposition of respective class covariance matrices, and shows that the resulting classifier obtained separates the margin space in ratio of respective class population covariance. The study proposes an algorithm to iteratively estimate the population covariance-adjusted SVM classifier in non-Euclidean space from sample covariance matrices of the training data. The effectiveness of this SVM classification approach is demonstrated by applying the classifier on multiple datasets and comparing the performance with traditional SVM kernels and whitening algorithms. The Cholesky-SVM model shows marked improvement in the accuracy, precision, F1 scores and ROC performance compared to linear and other kernel SVMs.", "tldr": "We demonstrate limitations of Support Vector Classification (SVC) in Non-Euclidean spaces due to covariance effects, and propose an algorithm to perform population covariance-adjusted SVC using training data.", "keywords": ["Support Vector Machine", "Euclidean Vector Space", "Cholesky Decomposition", "Mahalanobis Distance", "Whitening"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d98d5da5f2d742d4c0d6a673fb1a93133d439f11.pdf", "supplementary_material": "/attachment/3502bb1e6826abc6559f1d9a880e0abfc72d60df.zip"}, "replies": [{"content": {"summary": {"value": "The study highlights limitations of traditional Support Vector Machine (SVM) classification in non-Euclidean spaces. It introduces a method for SVM classification in non-Euclidean spaces by incorporating data covariance through a transformation matrix derived from Cholesky decomposition of class covariance matrices. This approach separates the margin space proportionally to class population covariance. The proposed algorithm iteratively estimates a covariance-adjusted SVM classifier from sample covariance matrices."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The study establishes a model for SVM classification in non-Euclidean spaces.\n\nThe proposed algorithm iteratively estimates the population covariance-adjusted SVM classifier from sample covariance matrices.\n\n Empirical results demonstrate significant improvements over traditional SVM methods."}, "weaknesses": {"value": "The method requires computing covariance matrices for each class and performing Cholesky decomposition.\n\n The performance of Cholesky-SVM depends heavily on the accuracy of the estimated covariance matrices. If sample sizes are small or the data is noisy, covariance estimates may be unreliable.\n\n Computing the inverse of matrices may be time-consuming for high-dimensional data.\n\n The paper does not explicitly mention the computational complexity of the model.\n\nSVMs in the hyperbolic space have been proposed.\n\n The experiments on real data sets are limited."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EAyPXWwSNA", "forum": "kMfVTka2WB", "replyto": "kMfVTka2WB", "signatures": ["ICLR.cc/2026/Conference/Submission20322/Reviewer_ZbFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20322/Reviewer_ZbFi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761445219143, "cdate": 1761445219143, "tmdate": 1762933782715, "mdate": 1762933782715, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel approach to Support Vector Machine (SVM) classification, named Covariance-Adjusted SVM (CSVM), designed specifically for non-Euclidean spaces. The authors' core argument is that traditional SVMs, which are based on maximizing the margin using Euclidean distance, are theoretically flawed when applied to statistical input spaces where data covariance is significant. They posit that in such spaces, the principle of maximum margin is a function of the intra-class covariance.\n\nThe proposed solution involves a two-step process. First, the data for each class is transformed from the non-Euclidean input space to a true Euclidean space. This is achieved using a transformation matrix derived from the Cholesky Decomposition of each class's covariance matrix. Second, a standard SVM is formulated and solved in this transformed space. When mapped back to the original input space, this method results in a classifier that divides the margin in a ratio proportional to the respective class covariances."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1.  **Strong Theoretical Foundation:** The paper is built on a clear and compelling theoretical argument. The distinction between the non-Euclidean statistical space and the Euclidean vector space is well-articulated, providing a strong justification for why traditional SVMs might be suboptimal. Grounding the methodology in the Mahalanobis distance and vector space transformations gives the work significant theoretical weight.\n2.  **Practical Algorithm for an Unseen Data Problem:** The SM algorithm is a clever and practical solution to the real-world problem of unknown population covariance. By iteratively refining the covariance estimate using the test data, it moves beyond a static transformation based solely on training data, which likely contributes to its superior performance."}, "weaknesses": {"value": "1.  **Computational Complexity and Scalability:** The iterative nature of the SM algorithm appears computationally expensive. It requires recalculating covariance matrices and their decompositions, as well as re-labeling the entire test set at each step. The paper acknowledges this but does not provide a formal analysis of the computational complexity or discuss the method's feasibility for very large-scale datasets.\n2.  **Convergence Properties of the SM Algorithm:** The SM algorithm is presented as a heuristic. The paper does not discuss its convergence properties, such as whether convergence is guaranteed or if the algorithm could be sensitive to initialization (e.g., the initial training/test split) and potentially settle in a poor local minimum.\n3.  **Limited Scope of Experimental Comparison:** The evaluation, while thorough within the SVM family, is somewhat insular. The paper does not compare CSVM against other powerful, non-SVM classification paradigms like Multi-Layer Perceptrons (MLPs) or Gradient Boosting. This makes it difficult to assess the method's competitiveness in a broader machine learning context, particularly on more complex datasets where these alternative models are often more suitable. **If the authors could add experiments comparing CSVM to other types of classifiers, I would be inclined to increase my rating of this paper.**"}, "questions": {"value": "1.  Could you elaborate on the convergence properties of the SM algorithm? Is convergence guaranteed, and how sensitive is its performance to the initial state (i.e., the training data)?\n2.  The Cholesky decomposition requires the covariance matrix to be positive definite and invertible. How does your method handle high-dimensional datasets where the number of features exceeds the number of samples, leading to a singular sample covariance matrix?\n3.  The paper makes the interesting claim that an N-class problem would result in N classifiers in the input space. Have you considered extending this framework to multi-class problems, and what challenges do you foresee in adapting the SM algorithm for that scenario?\n4.  **Performance Against Non-SVM Classifiers:** Your experiments convincingly show CSVM's superiority over other SVM variants. However, since SVM is a classifier, a comparison to other state-of-the-art classifiers is essential. How do you expect CSVM to perform against fundamentally different models, such as an MLP, especially on more complex and larger-scale datasets? Including such a baseline would provide valuable context for the performance improvements observed. As mentioned, **adding these experiments would significantly strengthen my overall evaluation of this work.**"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Xlgo3j8c2n", "forum": "kMfVTka2WB", "replyto": "kMfVTka2WB", "signatures": ["ICLR.cc/2026/Conference/Submission20322/Reviewer_EArk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20322/Reviewer_EArk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632528161, "cdate": 1761632528161, "tmdate": 1762933782449, "mdate": 1762933782449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is based on the observation that while the SVM algorithm assumes datapoints live in the usual Euclidean space ($\\mathbb{R}^{n}$ equipped with the Euclidean metric), the Mahalanobis distance is the more natural metric between random vectors in a dataset. The authors note that the Euclidean geometry assumption is necessary for SVM to be valid, and conclude that a different algorithm is needed. They develop one by first using the population covariance matrix to transform data points back into a Euclidean representation, which can be directly subjected to SVM. An algorithm for estimating the population covariance from data covariance is also provided. Comparing their work against several baselines on 5 datasets, the authors report favorable results on 4 of them."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The authors' contribution continues a pre-existing line of research, using the Mahalanobis distance to make SVMs flexible and robust in non-Euclidean spaces.\n\n* The proposed algorithm seems to be intuitive and obvious, and empirically verified.\n\n* The proposed algorithm produces better results compared to traditional SVM with widely used kernels in four out of five datasets."}, "weaknesses": {"value": "* The idea is straightforward and obvious. There is a vast literature on metric learning in machine learning.\n\n* The paper does not discuss or compare with more latest kernel functions, such as data-dependent kernels.\n\nKai Ming Ting, Yue Zhu, and Zhi-Hua Zhou (2018). Isolation Kernel and Its Effect on SVM. In Proceedings of the 24th ACM SIGKDD.\n\nV. V. Malgi, S. Aryal, Z. Rasool, and D Tay (2023). Data-dependent and Scale-Invariant Kernel for Support Vector Machine Classification. In: Proceedings of the PAKDD 2023.\n\n* Experiments are too slim, used only five small datasets.\n\n* None of the stated Lemmas are proven, and, furthermore, their formal statements are a bit vague.\n\n* The presentation is not clear to the reader at several places. For example: (i) In section 2, after transforming both positive and negative instances, the introduction of the margin ratio is a bit hard to follow (recommend reformulating the section starting at equation 8). (ii) What does the acronym SM stand for? (iii) The introduced algorithm is referred to as Covariance-adjusted SVM in the body of the paper and as Cholesky-SVM in the abstract, which is confusing."}, "questions": {"value": "I would appreciate the author's comments on the weaknesses raised above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0fILzKYmbw", "forum": "kMfVTka2WB", "replyto": "kMfVTka2WB", "signatures": ["ICLR.cc/2026/Conference/Submission20322/Reviewer_vm1i"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20322/Reviewer_vm1i"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090489676, "cdate": 1762090489676, "tmdate": 1762933782208, "mdate": 1762933782208, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a covariance-adjusted support vector machine. The main idea is to transform (or whiten) each class of data via Cholesky decompostion of the sample class covariance, which follows a linear SVM in the transformed Euclidean space. The bias term is then adjusted to maintain the ratio of margins in proportion to the class covariance. An interative SM algorithm is also developed to leverage the information from test data to refine the estimation of the class covariance. Across five benchmark binary datasets, the proposed method shows higher accuracy, precision, recall, F1, and ROC‑AUC than baseline methods including linear and kernel SVMs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper studies an interesting and important question, as SVM is a fundamental method in machine learning with strong theoretical guarantees and solid empirical performance on tabular data, where deep learning is often outperformed by SVMs and ensemble methods.\n\n2. The empirical results are presented as strong: the authors claim the proposed method outperforms key baselines."}, "weaknesses": {"value": "1. The main claim is that “SVM is valid in Euclidean space only.” Kernel SVM is well established and operates via kernels in implicit feature (Hilbert) spaces; this claim is therefore not accurate. The proposed method is not essentially different from the kernel trick in the kernel SVM.\n\n2. The numerical studies are not appropriately conducted. After reading the code in the supplemental file, there are several findings. (i) The standardization process includes both training and test data, which potentially leads to data leakage; (ii) There is no tuning of key hyperparameters and default values are used. SVMs are known to perform poorly without careful tuning. (iii) The presented results are based on only a single 80/20 split, which is not stable. (iv) AUC computed from hard class labels rather than probabilities/scores, which is essentially incorrect.\n\n3. Unfair comparison: the method uses information from the test data (semi‑supervised/transductive), while the baselines are purely supervised.\n\n4. The lemmas lack rigorous statements and proofs."}, "questions": {"value": "1. It is commonly observed that kernel SVMs outperform linear SVMs; this is not the case in the reported results. I suspect this is due to the lack of tuning. Can the authors clarify?\n2. The paper repeatedly refers to “N‑class SVM.” What is the performance on multi‑class data, and how is the method extended and evaluated in that setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DNW2D0ys8E", "forum": "kMfVTka2WB", "replyto": "kMfVTka2WB", "signatures": ["ICLR.cc/2026/Conference/Submission20322/Reviewer_xevi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20322/Reviewer_xevi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20322/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154033422, "cdate": 1762154033422, "tmdate": 1762933781953, "mdate": 1762933781953, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}