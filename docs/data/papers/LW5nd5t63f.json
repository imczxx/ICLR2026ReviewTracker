{"id": "LW5nd5t63f", "number": 17284, "cdate": 1758274215330, "mdate": 1759897185097, "content": {"title": "E$^2$GraphRAG: Advancing the Pareto Frontier in Efficiency and Effectiveness for Graph-based RAG", "abstract": "Graph-based RAG methods like GraphRAG demonstrate strong global understanding of the knowledge base by constructing hierarchical entity graphs, but often suffer from inefficiency and rigid, manually defined query modes, limiting practical use. To address these limitations, we present E$^2$GraphRAG, a streamlined graph-based RAG framework that advances the Pareto frontier of Efficiency and  Effectiveness. In the indexing stage, E$^2$GraphRAG utilizes large language models to generate a summary tree, and NLP tools to construct an entity graph from document chunks, with bidirectional indexes linking entities and chunks for efficient lookup. In the retrieval stage, the graph structure filters related entities, while the bidirectional indexes map these entities to their corresponding chunks, supporting an adaptive mechanism that dynamically switches between local and global modes. Experiments show that E$^2$GraphRAG achieves up to $10\\times$ faster indexing than GraphRAG while maintaining comparable QA performance, advancing the Pareto frontier with respect to effectiveness and efficiency. Our code is available at https://anonymous.4open.science/r/E-2GraphRAG-8897.", "tldr": "We propose E^2GraphRAG, a streamlined graph-based RAG framework that advances the Pareto frontier with respect to efficiency and effectiveness.", "keywords": ["Efficient RAG", "Graph-based RAG"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24e89d27a563f31865dc6c6a65b9c32e3570717f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "$E^2GraphRAG$ is a streamlined framework for graph-based Retrieval-Augmented Generation that aims to advance the Pareto frontier of efficiency and effectiveness of the GraphRAG paradigm. The approach enables efficient graph construction and retrieval while maintaining superior performance. Constributions inlcude:\n1. integration of summary trees and entity graphs for lightweight indexing\n2. adaptive retrieval strategy that automatically selects query modes using graph structure"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Efficiency Improvements. By replacing LLM-based entity extraction with standard NLP tools and simplifying extracting specific relation to sentence-level \"co-occurrence\", the method reduces indexing time and computational overhead, making it more practical for large-scale knowledge base.\n2. Adaptive Retrieval. The graph-filtering approach for mode selection is innovative and flexible.\n3. Strong Empirical Validation, as shown by the experiments."}, "weaknesses": {"value": "While the proposed model appears novel and promising, I identified several contradictions and points of confusion that could undermine the paper's overall quality and the replicability of its conclusions.\n\n1.  Usage of SpaCy Sacrifices Generalizability Without Reducing Overall Complexity\n\nIn discussing the motivation for adopting SpaCy-based NER, the authors claim that it reduces computational costs compared to LLM-based alternatives. However, the construction of the summary tree still necessitates processing the entire corpus through LLMs for recursive summarization, rendering the LLM token costs largely unavoidable. Furthermore, the complexity of LLM-based NER would be approximately $O(N \\cdot T)$ (where $N$ is the number of chunks and $T$ is the average token size per chunk). This is comparable to the costs incurred by the summary tree construction. It is not the primary bottleneck relative to those costs. Consequently, employing SpaCy does not alter the overall token complexity of the indexing stage. \n\nIt is also worth noting that SpaCy-based NER can introduce noise and often struggles with domain-specific corpora, potentially compromising the accuracy of entity extraction in specialized contexts (e.g. legal, finance, medical).\n\n2. Limited novelty in indexing stage\n\nThe summary tree construction is similar to the community summarization in GraphRAG[1] and the recursive summarization in Raptor[2], which has been widely used.\n\n3. When building the summary tree, the chunks are grouped by taking the consecutive $g$ chunks following the order in the original passages. How can this be generalize to the domain where this bias is not available? For instance, in a knowledge base where all chunks are independent passages. If you use clustering to group chunks in this scenario, the efficiency may be an issue.\n\n- [1] From Local to Global: A Graph RAG Approach to Query-Focused Summarization, arxiv-2404.16130\n- [2] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval, ICLR, 2024"}, "questions": {"value": "Plz see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YjbsAwjL9u", "forum": "LW5nd5t63f", "replyto": "LW5nd5t63f", "signatures": ["ICLR.cc/2026/Conference/Submission17284/Reviewer_Zxuz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17284/Reviewer_Zxuz"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761637893712, "cdate": 1761637893712, "tmdate": 1762927227800, "mdate": 1762927227800, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response to Concerns Regarding Novelty"}, "comment": {"value": "We sincerely thank all the reviewers for their feedbacks. Regarding the novelty issue, we respectfully disagree with the comment on the lack of novelty of our paper. \n  \nOn the one hand, prior to our work, all existing GraphRAG approaches are too **heavy**, which rely on LLMs for entity and relation extraction, and introduce significant computational and financial costs. Therefore, despite the effectiveness, they are hard to widely use. To address the problem, we propose the first non-llm-based lightwight graph construction method for GraphRAG, which achieves comparable or even better effectiveness as LLM-based models with significant speedup in both indexing and retrieval stages. This is the core contribution of our paper. **In particular, we never claim that the use of lightweight tools is our contribution (as pointed by Reviewers rZe1 and ties), but we are the first to point out the new direction that GraphRAG without LLMs for graph construction and retrieval can still perform well** This paves the way for the wide deployment of GraphRAG and also sheds light on the direction for feasible future study. ``Towards this direction, if small models can be used in GraphRAG to offer practical improvements, why do we need to use costly LLMs?``\n\nOn the other hand, for the retrieval stage, we have our own characteristic. Existing approaches mainly lie in two ways. First, GraphRAG and LightRAG treat the knowledge graph as the source of the information. Second, HippoRAG and HippoRAG2 treat it as an auxiliary component to enhance the performance of embedding based retrieval. Different from them, our method leverages the entity graph as a filter to identify the core entities in the query, which further help locate the relevant chunks. **Note that our constructed graph is an entity graph, but not a knowledge graph**. This also highlights the difference between our method and other baselines in the indexing stage.\n\nIn summary, we provide insights in both indexing stage and retrieval stage. Therefore, we believe our contribution goes beyond an incremental improvement or technical report, marking an initial step toward a new research direction within the GraphRAG framework."}}, "id": "zRVsw6Kcwb", "forum": "LW5nd5t63f", "replyto": "LW5nd5t63f", "signatures": ["ICLR.cc/2026/Conference/Submission17284/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17284/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17284/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763138735573, "cdate": 1763138735573, "tmdate": 1763139891374, "mdate": 1763139891374, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Response to Concerns Regarding Novelty"}, "comment": {"value": "We sincerely thank all the reviewers for their feedbacks. Regarding the novelty issue, we respectfully disagree with the comment on the lack of novelty of our paper. \n  \nOn the one hand, prior to our work, all existing GraphRAG approaches are too **heavy**, which rely on LLMs for entity and relation extraction, and introduce significant computational and financial costs. Therefore, despite the effectiveness, they are hard to widely use. To address the problem, we propose the first non-llm-based lightwight graph construction method for GraphRAG, which achieves comparable or even better effectiveness as LLM-based models with significant speedup in both indexing and retrieval stages. This is the core contribution of our paper. **In particular, we never claim that the use of lightweight tools is our contribution (as misunderstood by Reviewers rZe1 and ties), but we are the first to point out the new direction that GraphRAG without LLMs for graph construction and retrieval can still perform well**. This paves the way for the wide deployment of GraphRAG and also sheds light on the direction for feasible future study. ``Towards this direction, if small models can be used in GraphRAG to offer practical improvements, why do we need to use costly LLMs?``\n\nOn the other hand, for the retrieval stage, we have our own characteristic. Existing approaches mainly lie in two ways. First, GraphRAG and LightRAG treat the knowledge graph as the source of the information. Second, HippoRAG and HippoRAG2 treat it as an auxiliary component to enhance the performance of embedding based retrieval. Different from them, our method leverages the entity graph as a filter to identify the core entities in the query, which further help locate the relevant chunks. **Note that our constructed graph is an entity graph, but not a knowledge graph**. This also highlights the difference between our method and other baselines in the indexing stage.\n\nIn summary, we provide insights in both indexing stage and retrieval stage. Therefore, we believe our contribution goes beyond an incremental improvement or technical report, marking an initial step toward a new research direction within the GraphRAG framework."}}, "id": "zRVsw6Kcwb", "forum": "LW5nd5t63f", "replyto": "LW5nd5t63f", "signatures": ["ICLR.cc/2026/Conference/Submission17284/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17284/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17284/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763138735573, "cdate": 1763138735573, "tmdate": 1763275724618, "mdate": 1763275724618, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes E2GraphRAG, a graph-based retrieval-augmented generation (RAG) framework that improves both efficiency and effectiveness over existing methods such as GraphRAG and LightRAG. The key idea is to combine a hierarchical summary tree (built by LLM summarization) with an entity co-occurrence graph (constructed using lightweight NLP tools instead of LLMs). The model builds bidirectional indexes between entities and chunks to enable adaptive retrieval that automatically switches between local and global query modes. Experiments on long-document QA datasets show that E2GraphRAG achieves up to 10× faster indexing and 100× faster retrieval than GraphRAG, while maintaining comparable QA accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well-motivated improvement over GraphRAG, targeting efficiency–effectiveness trade-offs.\n\nClever use of non-LLM entity extraction tools to reduce computational cost.\n\nComprehensive experiments and ablation studies that clearly demonstrate both speed and accuracy gains."}, "weaknesses": {"value": "The paper’s novelty lies mainly in the integration and efficiency-oriented redesign of existing graph-based RAG components rather than in proposing a fundamentally new retrieval or reasoning paradigm. It offers practical, well-engineered improvements but limited conceptual originality."}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pD4giy3WkA", "forum": "LW5nd5t63f", "replyto": "LW5nd5t63f", "signatures": ["ICLR.cc/2026/Conference/Submission17284/Reviewer_XF2c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17284/Reviewer_XF2c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761706902547, "cdate": 1761706902547, "tmdate": 1762927227580, "mdate": 1762927227580, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduce $E^2$GraphRAG, a GraphRAG method with NLP tools to construct graphs. Specifically, the authors first build a hierarchical chunk summarization tree, and then enrich it by linking chunk nodes to an entity-level knowledge graph extracted using NLP techniques. For each query, the proposed retrieval strategy adaptively selects either local or global retrieval. Experimental results show that the method outperforms existing baselines while maintaining efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The designed retrieval mechanisim can automatically choose local or global retrieve.\n2. The authors compared the number of input and output tokens of different methods.\n3. The authors provided detailed ablation study of the proposed method to demonstrate the effectiveness of different designs."}, "weaknesses": {"value": "1. The novelty of the proposed method is limited. Constructing knowledge graphs using NLP tools such as entity and relation extraction has been widely explored in prior NLP and RAG-related studies, and thus cannot be considered a key contribution of this work. In addition, the hierarchical summarization tree is largely inspired by RAPTOR, which further reduces the originality of the approach.\n\n2. The overall writing quality needs improvement. The paper reads more like a technical report rather than a well-structured academic paper. It lacks clear motivation, and the rationale behind each design choice is not well explained. Many components appear ad hoc, without sufficient theoretical grounding or empirical justification.\n\n3. The experiments are only conducted on three datasets, which is not sufficient to demonstrate the robustness and generalizability of the proposed method. Additional benchmarks, especially multi-hop QA datasets such as MultihopRAG, should be included to more comprehensively validate the effectiveness of the approach."}, "questions": {"value": "1. In the retrieval, \"queries whose entities are densely connected are processed locally, while others fall back to global retrieval.\" What if there is only one entity in the query?\n\n2. What is the reason that the authors summary the consecutive chunks? What if chunks in different documents share the similar semantic meaning?\n\n3. Where did the authors get the heuristic of Graph Filtering, \"truly relevant entities tend to be semantically related and thus connected in the constructed graph\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IX9tUeKFWZ", "forum": "LW5nd5t63f", "replyto": "LW5nd5t63f", "signatures": ["ICLR.cc/2026/Conference/Submission17284/Reviewer_ties"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17284/Reviewer_ties"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761957533987, "cdate": 1761957533987, "tmdate": 1762927227320, "mdate": 1762927227320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces E2GraphRAG, a novel Retrieval-Augmented Generation framework that advances the Pareto frontier of efficiency and effectiveness in graph based RAG systems. Its core innovation lies in combining a hierarchical summary tree built with large language models and an efficient entity co occurrence graph constructed using traditional NLP tools like SpaCy. These two components are interconnected through bidirectional entity to chunk and chunk to entity indexes. This hybrid structure supports adaptive retrieval by allowing queries to dynamically trigger either local entity centric retrieval or global dense embedding retrieval, based on detected query characteristics and graph structure. This approach eliminates the rigidity and computational inefficiency found in prior work such as GraphRAG."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The writing and figures of the paper are clear.\n- The discussion of indexing efficiency in graph-based RAG systems for corpora with long documents is meaningful."}, "weaknesses": {"value": "- The main concern is the novelty in the proposed framework. The core module for constructing the graph combines the \"merge and summarize text chunks to construct a hierarchical tree\" approach (similar to RAPTOR) with building a concise entity graph from dispersed chunks (similar to knowledge graph construction-based methods, such as GraphRAG and HippoRAG2). The use of lightweight tools such as SpaCy and BERT is also not new compared to existing methods. As a result, the framework seems to be an incremental integration of existing baselines, rather than offering strong innovation.\n- In the experiments, the base LLM used in this paper is not consistent with the baselines. Key baselines, such as RAPTOR, HippoRAG2, and LightRAG, are evaluated using stronger generation models, including LLaMA-3.3-70B-Instruct and GPT-4o-mini API, whereas all models are tested solely with a 7B-scale LLM in this paper. This mismatch raises concerns about the credibility of the experimental results, particularly since the proposed method claims to offer better accuracy and efficiency.\n- The experiments only consider datasets with long documents, ignoring classic multi-hop question answering GraphRAG datasets [1] such as MuSiQue, 2WikiMultihopQA, and HotpotQA. This omission may lead to unfair comparisons. While indexing efficiency and accuracy on long documents are important, evaluations on standard multi-hop datasets should not be neglected.\n\n[1] From RAG to Memory: Non-Parametric Continual Learning for Large Language Models"}, "questions": {"value": "As noted in the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "A5Uzr0AVhQ", "forum": "LW5nd5t63f", "replyto": "LW5nd5t63f", "signatures": ["ICLR.cc/2026/Conference/Submission17284/Reviewer_rZe1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17284/Reviewer_rZe1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17284/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972930155, "cdate": 1761972930155, "tmdate": 1762927226869, "mdate": 1762927226869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}