{"id": "aTVollXaaI", "number": 4614, "cdate": 1757727794833, "mdate": 1759898023382, "content": {"title": "SPRINT: Sparse-Dense Residual Fusion for Efficient Diffusion Transformers", "abstract": "Diffusion Transformers (DiTs) deliver state-of-the-art generative performance but their quadratic training cost with sequence length makes large-scale pretraining prohibitively expensive. Token dropping can reduce training cost, yet naïve strategies degrade representations, and existing methods are either parameter-heavy or fail at high drop ratios. We present SPRINT (Sparse--Dense Residual Fusion for Efficient Diffusion Transformers), a simple method that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT leverages the complementary roles of shallow and deep layers: early layers process all tokens to capture local detail, deeper layers operate on a sparse subset to cut computation, and their outputs are fused through residual connections. Training follows a two-stage schedule: long masked pre-training for efficiency followed by short full-token fine-tuning to close the train--inference gap. On ImageNet-1K 256^2, SPRINT achieves 9.8x training savings with comparable FID/FDD, and at inference, its Path-Drop Guidance (PDG) nearly halves FLOPs while improving quality. These results establish SPRINT as a simple, effective, and general solution for efficient DiT training.", "tldr": "Efficient diffusion transformer training", "keywords": ["diffusion models", "generative models", "flow matching", "efficient training", "image generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/78512f1486f5e91d09bddd31171f5121de786b8b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the prohibitive quadratic training cost of Diffusion Transformers (DiTs). The authors propose SPRINT (Sparse-Dense Residual Fusion), a novel training and architecture strategy that enables aggressive token dropping (up to 75%) while preserving quality. SPRINT divides the DiT into three parts: a dense shallow encoder (fθ) that processes all tokens to capture local detail, a sparse deep middle block (gθ) that processes a subset of tokens to model global semantics, and a decoder (hθ) that fuses features from both paths via a residual connection. The method uses a two-stage training schedule (long sparse pre-training followed by short full-token fine-tuning) and a structured group-wise token sampling strategy to ensure local coverage. Additionally, the paper introduces Path-Drop Guidance (PDG), an efficient substitute for Classifier-Free Guidance (CFG) that nearly halves inference FLOPs by bypassing the expensive middle blocks during the unconditional pass. Experiments on ImageNet show SPRINT achieves significant training speedups (e.g., 9.8x fewer TFLOPs to reach comparable quality to SiT-XL) and improved inference efficiency with PDG."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Significant Training Efficiency: The primary strength is the massive reduction in training computation. The paper demonstrates that SPRINT can reach a quality level comparable to the 1400-epoch SiT-XL baseline in only 200 epochs, translating to a 9.8x reduction in TFLOPs. This is a highly practical and valuable contribution. \n 2. Novel and Effective Inference Acceleration (PDG): The proposed Path-Drop Guidance (PDG) is an excellent finding. It cleverly repurposes the SPRINT architecture to create an efficient alternative to CFG, using the shallow path as a \"\"weaker network\"\". This method nearly halves inference FLOPs while also improving generation quality, as shown in Tables 1, 2, 3, and 10, and Figure 7. \n 3. Simplicity and Generality: The SPRINT framework is conceptually simple and not tied to a specific architecture. The authors demonstrate its broad applicability by successfully integrating it with SiT, U-ViT, and the alignment-based REPA, showing consistent and significant improvements in all cases. \n 4. Strong Empirical Validation: The paper is supported by extensive experiments and insightful ablations. The validation of the sparse-dense design (Table 4), the structured sampling strategy (Table 5), and the optimal layer allocations (Tables 6, 8) builds strong confidence in the method's design. The analysis of feature specialization (Fig. 4) provides good intuition for why the method works."}, "weaknesses": {"value": "1. Limited Ablation on Fine-tuning Stage: The paper adopts a fixed two-stage schedule: long sparse pre-training followed by a 100K-iteration full-token fine-tuning stage. While effective, the sensitivity to the duration of this fine-tuning is not analyzed. It is unclear how much fine-tuning is necessary to close the train-inference gap or if more fine-tuning would yield further gains. An ablation on this hyperparameter would strengthen the paper. \n 2. Conceptual Justification for PDG: The paper empirically shows that PDG works well by using the shallow path fθ as the unconditional estimate, inspired by Auto Guidance. This is effective, but a deeper conceptual analysis of why fθ is a suitable unconditional predictor would be beneficial. Does the SPRINT training force fθ to learn an \"\"average\"\" or \"\"blurry\"\" representation that mimics a true unconditional pass? A brief discussion would be insightful."}, "questions": {"value": "1.The performance of PDG is excellent. Does the SPRINT training enable PDG? In other words, could PDG (i.e., using only the first few layers fθ for the unconditional pass) be applied as an inference-time optimization to a standard, densely trained SiT model, or is the sparse-dense training process necessary for fθ to become an effective unconditional predictor?\n2. Could you provide experimental results on text-to-image models (such as PixArt, Flux), similar to the fine-tuning experiments on FLUX, to more convincingly demonstrate the effectiveness of your proposed method?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3nr1zw3Jnl", "forum": "aTVollXaaI", "replyto": "aTVollXaaI", "signatures": ["ICLR.cc/2026/Conference/Submission4614/Reviewer_VDGb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4614/Reviewer_VDGb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826792588, "cdate": 1761826792588, "tmdate": 1762917469861, "mdate": 1762917469861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPRINT, a novel framework for efficient training of Diffusion Transformers (DiTs) by leveraging sparse-dense residual fusion. It enables aggressive token dropping (up to 75%) while preserving representation quality, significantly reducing training costs (up to 9.8×) and inference FLOPs. SPRINT trains DiTs in two stages: sparse pre-training and short full-token fine-tuning to bridge the train-inference gap. It also introduces Path-Drop Guidance (PDG), a more efficient alternative to classifier-free guidance, further improving generation quality and efficiency. The method is simple, architecture-agnostic, and applicable across various resolutions and models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ Good performance\n+ The proposed Dense shallow path  and sparse deep path can effectively accelerate the training speed."}, "weaknesses": {"value": "1. More discussion on Path-Drop Guidance should be included in the Introduction. Currently, the manuscript treats it as merely a supplementary design.\n\n2. The font size in the tables should be consistent."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GZJlzbS89Q", "forum": "aTVollXaaI", "replyto": "aTVollXaaI", "signatures": ["ICLR.cc/2026/Conference/Submission4614/Reviewer_orVZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4614/Reviewer_orVZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761839637262, "cdate": 1761839637262, "tmdate": 1762917469514, "mdate": 1762917469514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPRINT, a training method for Diffusion Transformers (DiTs) that aims to reduce training costs through aggressive token dropping (up to 75%). The core idea is to partition the DiT into encoder-middle-decoder components, where the encoder processes all tokens, the middle blocks operate on sparse tokens, and outputs are fused through residual connections. The authors claim 9.8x training savings with comparable quality on ImageNet-1K $256^2$."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Practical Problem: The paper addresses the important issue of quadratic training costs in DiTs, which is highly relevant for the community.\n2. Strong Empirical Results: The reported 9.8x training speedup with maintained quality is impressive if valid.\n3. Architecture Agnostic: The method appears to work across different architectures (SiT, UViT) and can be combined with other techniques like REPA.\n4. Comprehensive Experiments: The paper includes extensive ablations and analysis across multiple settings."}, "weaknesses": {"value": "Major Concerns\n\n1. Limited Technical Novelty.\nThe core contribution appears to be a modification of MDTv2, essentially replacing the side-interpolator with simple residual connections. The encoder-middle-decoder architecture is questonable, and the paper fails to provide compelling theoretical or empirical justification for why this specific design should outperform existing methods like MDTv2.\n\n2. Insufficient Comparison with Prior Work.\nThe paper does not adequately explain why SPRINT should be superior to MDTv2. The fundamental question remains unanswered: what specific advantages does replacing MDTv2's side-interpolator with residual connections provide? The paper lacks rigorous analysis of this key design choice.\n\n3. Questionable Performance Claims.\nFrom Table 3, SPRINT appears to underperform compared to MDTv2 (in terms of FID). This contradicts the paper's claims of superiority and raises questions about the experimental setup and evaluation fairness.\n\n4. Lack of Theoretical Foundation.\nThe paper provides insufficient theoretical justification for why the proposed encoder-middle-decoder architecture can support such high drop rates (75%). The explanation about shallow vs. deep layer specialization is intuitive but lacks rigorous analysis or proof.\n\n5. Missing Critical Analysis.\nThe paper doesn't adequately address:\n- Why simple residual fusion should be better than more sophisticated fusion mechanisms\n- Why SPRINT can tolerate 75% drop rate\n- How the method compares to MDTv2 in controlled settings with identical experimental conditions\n\nMinor Issues\n- Writing Quality: Some sections lack clarity, particularly the technical description of the fusion mechanism.\n- Experimental Setup: More details needed on fair comparison protocols with baseline methods.\n- Ablation Studies: While extensive, the ablations don't address the core question of why (if really so) this approach works better than MDTv2."}, "questions": {"value": "1. Can you provide a direct, controlled comparison with MDTv2 using identical experimental settings?\n2. What theoretical or empirical evidence supports the advantages of the residual fusion over MDTv2's approach?\n3. Why are the FLOPs for MDTv2 missing in Table 3? It has open-sourced official code, you should check it and measure the costs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oyE12fcBVZ", "forum": "aTVollXaaI", "replyto": "aTVollXaaI", "signatures": ["ICLR.cc/2026/Conference/Submission4614/Reviewer_wnyK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4614/Reviewer_wnyK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923817561, "cdate": 1761923817561, "tmdate": 1762917469148, "mdate": 1762917469148, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SPRINT, a method to accelerate Diffusion Transformer (DiT) training through sparse-dense residual fusion. SPRINT processes all tokens in shallow layers for local details, drops 75% of tokens in deep layers for global semantics, and fuses outputs via residual connections. Training uses sparse pre-training followed by brief full-token fine-tuning. On ImageNet-1K 256×256, SPRINT achieves 9.8× training speedup with comparable quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1)SPRINT achieves 9.8× training speedup on ImageNet-1K while maintaining comparable quality. The method adds only 0.3% parameters and preserves standard DiT blocks, making it easy to integrate. Strong generalization across architectures (SiT, U-ViT, REPA) demonstrates practical value.\n(2)Path-Drop Guidance (PDG) halves inference FLOPs while improving quality. Comprehensive experiments reveal complementary roles of sparse-deep and dense-shallow features, providing valuable insights into DiT representation mechanisms and explaining why the sparse-dense fusion design is effective."}, "weaknesses": {"value": "(1)The paper claims that two-stage training can \"close the train-inference gap,\" but does not quantify how large this gap actually is."}, "questions": {"value": "(1)Why is there no comparative validation using features from different layers for unconditional guidance?\n(2)After pre-training with 75% drop ratio, if full-token inference is performed directly (without fine-tuning), how much would the performance degrade?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "cS1ix4cUbd", "forum": "aTVollXaaI", "replyto": "aTVollXaaI", "signatures": ["ICLR.cc/2026/Conference/Submission4614/Reviewer_pKvE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4614/Reviewer_pKvE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4614/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998608781, "cdate": 1761998608781, "tmdate": 1762917468619, "mdate": 1762917468619, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}