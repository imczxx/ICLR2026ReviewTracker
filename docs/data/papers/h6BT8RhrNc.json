{"id": "h6BT8RhrNc", "number": 20579, "cdate": 1758307830346, "mdate": 1759896969887, "content": {"title": "A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth", "abstract": "Large language models (LLMs) have sparked growing interest in automatic machine learning research agents. Among them, agents capable of autonomously proposing ideas and conducting machine learning experiments are particularly promising, as they maximize research automation and accelerate scientific progress by iteratively refining ideas based on experimental results. However, comprehensively evaluating such agents remains challenging. Existing benchmarks tend to overemphasize engineering aspects while neglecting academic rigor, creating barriers that obscure a clear assessment of an agent’s scientific capabilities of machine learning research. They also suffer from limited task diversity, an overemphasis on application-oriented tasks over fundamental research problems, and limited scalability to realistic research settings. To address these limitations, we introduce FML-bench, a benchmark designed to evaluate automatic machine learning research agents on 8 diverse and fundamental machine learning research problems. It reduces coding burden, emphasizes fundamental problems rather than specific use cases, offers high task diversity, and is extensible to real-world machine learning GitHub repository. Furthermore, we present a unified evaluation framework with five complementary metrics, designed to comprehensively assess agent performance on our benchmark. We evaluate state-of-the-art automatic research agents on FML-bench, and find that agents employing broad research exploration strategies outperform those focusing on narrow but deep exploration. These findings suggest that emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement. Our benchmark is available at anonymous github: https://anonymous.4open.science/r/Anonymous-78B6.", "tldr": "", "keywords": ["Benchmarking and evaluation", "Automatic research agents", "Large language models (LLMs)", "Fundamental ML research tasks"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1bcfa30ed29afc6db5b29b42d7033609579f6524.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces FML-bench, a benchmark designed to evaluate automatic machine learning research agents on fundamental ML problems rather than engineering-style tasks.\nThe benchmark includes eight core tasks (e.g., generalization, continual learning, causality, fairness) and evaluates agents across five dimensions: utility, diversity, academic contribution rate, cost, and step success rate.\nExperiments on three representative agent frameworks demonstrate that agents employing broader exploration strategies tend to outperform those focusing on narrow, deep refinement."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "• The authors correctly observe that existing benchmarks for ML agents primarily assess engineering execution rather than scientific research competence, which is an insightful and valuable perspective.\n\n• Building on this observation, the paper introduces several thoughtful design choices — for instance, providing baseline repositories so that agents can start from existing codebases rather than building projects from scratch, thereby reducing dependence on pure coding ability.\n\n• The authors make a clear effort to ensure benchmark usability and accessibility. I checked the provided code link, and it reflects a well-organized release and attention to potential community impact."}, "weaknesses": {"value": "• As reflected in the title, the main contribution and conclusion “The Importance of Exploration Breadth” may be heavily conditioned on the specific experimental settings. The entire setup is based on combinational innovation of the form A + B = C, which naturally favors “broad exploration” strategies. This could mislead future research directions, encouraging agent systems to maximize benchmark scores by generating superficial combinations instead of pursuing deeper innovation.\n\n• Equation (1) seems to be a conceptual pseudo-formula rather than an operational objective. The five metrics have inconsistent scales, and summing them directly would let high-magnitude terms dominate. The authors do not explain how coefficients such as lambda or beta are chosen, leaving this formulation vague.\n\n• Some metric designs appear rather superficial and arbitrary. For example, for Utility, why did the authors not normalize improvements by percentage? Different tasks have different baseline performance levels. A 20% increase on an unsaturated dataset can be much easier than a 1% gain on ImageNet, even though the latter is more meaningful (I understand this does not change the experiment’s internal conclusion). Diversity relies only on semantic similarity, which may be unstable in large samples. If two pieces of code have identical logic but only differ in variable or function names, would their similarity be high or low? The notion of Academic Contribution Rate raises fundamental questions of subjectivity: How is “academic contribution” actually quantified? For example, ReLU is, in essence, just a simple piecewise linear function. Should that be considered an academic innovation or an engineering refinement? And what about data augmentation? Is it categorized as an engineering technique or as a scientific contribution?\n\n• In Section 5.3.2, beyond correlation coefficients, significance tests should also be conducted, especially given the limited sample size (it is unclear how many samples were actually used for correlation calculation).\n\n• The benchmark examples may not be very discriminative. A benchmark should separate strong methods from weak ones, yet in Tables 2 and 3, the scores are either all high or all low, with little variance across systems. This diminishes its usefulness as a benchmark. Moreover, considering the fast pace of model updates from companies like OpenAI, the benchmark risks becoming obsolete quickly."}, "questions": {"value": "• The authors may consider removing the Academic Contribution Rate metric, which is potentially problematic, especially given that it is derived from LLM-based judgments. Introducing such an automated measure of “scientific contribution” could create conceptual and ethical risks for the community.\n\n• Since the benchmark is described as easily extensible, the authors could consider scaling it up and selecting tasks more carefully so that different systems fall within a reasonable range, rather than the current situation where all perform either very well or very poorly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ns2crkG1dP", "forum": "h6BT8RhrNc", "replyto": "h6BT8RhrNc", "signatures": ["ICLR.cc/2026/Conference/Submission20579/Reviewer_NpRn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20579/Reviewer_NpRn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761483116708, "cdate": 1761483116708, "tmdate": 1762933989363, "mdate": 1762933989363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the FML-benchmark, which can evaluate the automatic machine learning research agents on eight diverse and fundamental machine learning research problems. The proposed benchmark is more general and can extend to real-world GitHub repositories. The paper also provides a unified evaluation framework with five metrics. The paper also evaluates the state-of-the-art automatic research agents on FML-bench,"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Instead of focusing on the engineering perspective, this paper focuses on fundamental ML problems grounded on real-world codebases, including eight categories. All evaluated agents are SOTA.\n2. The paper provides a unified evaluation framework to help formalize the problem.  The paper provides a five-dimensional evaluation protocol covering utility, diversity, academic contribution rate, cost, and step success rate. Apart from pure implementation perspective, the paper also focuses on the diversity, academic contribution, and time. The paper also provides standard deviation for each metric. Those metrics include both quantitative (formula-based) and qualitative metrics (LLM-based).\n3. The paper provides benchmark code, experiment prompts, and configuration files on an anonymous GitHub repo. The visualization of this paper is very good and intuitive."}, "weaknesses": {"value": "1. Some details are missing. For the 8 diverse tasks, it might be better to provide detailed statistics for each of the tasks, such as the number of repositories and task settings. Most of those details are put in the appendix. It would be better to move them (e.g., task definition, metrics definition) into the main text.\n2. The evaluation integrity part is not very rigorous. Some automatic comparison between the evaluation files might be needed. Including a comparison with human evaluation on those corresponding evaluation metrics can further strengthen the paper.\n3. The paper did not contain an error analysis section, which will be very valuable for readers to further improve the existing AI agent frameworks."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MYxA44wo1n", "forum": "h6BT8RhrNc", "replyto": "h6BT8RhrNc", "signatures": ["ICLR.cc/2026/Conference/Submission20579/Reviewer_mYE9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20579/Reviewer_mYE9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979490220, "cdate": 1761979490220, "tmdate": 1762933988941, "mdate": 1762933988941, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FML-bench, a new benchmark for ML engineering. The motivation is to provide a diverse set of real-world codebases and focus the evaluation on the ML rather than writing code from scratch or non-ML skills. It is also noted that the ability of an agent to generate diverse hypotheses can be correlated with utility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I appreciate that the paper is thoughtful about a more careful curation of ML tasks which both covers different aspects of ML as well as focuses more on the ML side rather than general software engineering.\nLooking at diversity and academic contribution rate is interesting and provides a fuller picture of what the agents are doing beyond accuracy.\nThe evaluations include a reasonable set of modern models and scaffolds."}, "weaknesses": {"value": "In section 3, it would be good to separate the evaluation metrics into ones that are objective (utility and cost) and the ones that I take to be more like shaping rewards (e.g., diversity), which don't matter in themselves, but might be helpful for achieving the true objectives.\n\nSection 5.3.3: It's unclear why one wants to encourage academic contribution rates. If you want to improve an ML approach, sometimes it might be better to stick with a simpler method and do better tuning as opposed to aim for a fancier method (an \"academic contribution\").\n\nIt seems like there are two contributions in this paper: (i) introducing the new benchmark (FML-bench) and (ii) studying the diversity and academic contributions of various methods. For example, one could study (ii) with existing benchmarks besides FML-bench.\n\nWhile I appreciate the thoughtfulness of thinking about the composition of FML-bench, the 8 categories are fairly broad, and I worry that having a single task to represent a big topic like \"fairness & bias\" conflates the general category with the specifics of the problem setup, leading to potentially misleading conclusions (e.g., causality is easier than privacy)."}, "questions": {"value": "Questions\n - Was there any prompt optimization and hyperparameter tuning done?  If so, what was the basis for this?\n - Regarding the 8 tasks, I'm looking at Appendix B (I think the citations of the tasks/dtaasets could appear in the main body), but I still have some questions on how the tasks were defined? how was the difficulty determined? How do you ensure lack of train-test contamination? This is important since most of these datasets are old and likely show up in the training dataset."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sSCiDfNja7", "forum": "h6BT8RhrNc", "replyto": "h6BT8RhrNc", "signatures": ["ICLR.cc/2026/Conference/Submission20579/Reviewer_9hah"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20579/Reviewer_9hah"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129600815, "cdate": 1762129600815, "tmdate": 1762933988554, "mdate": 1762933988554, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new benchmark for evaluating AI agents on machine learning research tasks, named FML-bench. Specifically, the authors attempt to make those tasks more fundamental in terms of machine learning research. They pick eight domains (generalization, data efficiency, representation learning, continual learning, causality, robustness and reliability, privacy, and fairness and bias), each with corresponding target tasks, and define five evaluation metrics (Utility, Diversity, Academic Contribution Rate, Step Success Rate, and Cost). For the experiments, the authors employ and compare the AI Scientist, AIDE, and Claude Code. Considering these three AI agents as three prototypes with different exploration structures, based on the evaluation results with those, the authors suggest that broader explorations can be important for AI research agents."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Motivations  \nThe motivation, which is to establish a machine learning research benchmark that encompasses core machine learning directions, sounds reasonable to me given existing work. While there is concurrent work (named MLGym) that aims to provide similar values but from a different angle, having this benchmark and the tasks it provides could be valuable for this domain.\n\n2. Presentation  \nThe submitted manuscript is easy to follow and well-presented. It contains intuitive figures and tables, which can give the readers clear ideas about what the benchmark is, how it is supposed to be different from other existing benchmarks, and what properties the AI agents that are compared in this paper have. The experimental results are also well-organized for presentation.\n\n3. Reproducibility and potential adoption\nPresenting a benchmark paper, the authors clearly reveal the prompts used and release their source codes. This can be important for not only reproducibility but also the adoption of the benchmark."}, "weaknesses": {"value": "1. Some conclusions may need more evidence  \nIt looks to me that some conclusions made in this work may be somewhat early conclusions without strong evidence. The authors propose that \"emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement,\" which is based on the results with the AI Scientist, AIDE, and Claude Code. Given the differences between these methods, I believe there can be other factors that have meaningfully contributed to the results, other than the breadth. Also, the authors suggest that \"CLI-style agents are less suitable for automatic machine learning research than agents specifically designed for it.\" I think drawing this conclusion from the results with Claude Code may not be fully justified, as Claude Code is specifically designed for engineering tasks rather than research tasks.\n\n2. Definition and implication of the evaluation metrics  \nI have concerns regarding how the evaluation metrics are defined and their implications. Regarding the academic contribution rate metric, I am not convinced that this is how the academic contribution of the development of some work should be evaluated. For instance, some agents might make fair academic/research contributions in only a few steps and then polish the codes or experiments by spending many small steps. Depending on the types or complexity of hypotheses or academic contributions, they may require more tuning (or \"engineering\") steps to achieve the desired goal. Also, the diversity metric may be susceptible to benchmark gaming/hacking.\n\n3. Reliability of the LLM-based evaluation of academic contributions  \nThe authors prompt the Qwen3 235b-a22b model for the evaluation of academic contribution, which can have subjectivity in it. While it is understandable that it could be challenging to define systematic or less heuristic evaluation schemes, the current state of the work lacks proper evaluation of the evaluation method. One possible way would be conducting a human study to check how the evaluation by Qwen3 235b-a22b is aligned with human experts' evaluation.\n\n4. Minor issues  \n- I believe \"CodeGraphBERT\" should be \"GraphCodeBERT.\"\n- \"Step To Target\" only appears once in Appendix C."}, "questions": {"value": "Please take a look at the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gdJAYmLYZ1", "forum": "h6BT8RhrNc", "replyto": "h6BT8RhrNc", "signatures": ["ICLR.cc/2026/Conference/Submission20579/Reviewer_s9aw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20579/Reviewer_s9aw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20579/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762161097813, "cdate": 1762161097813, "tmdate": 1762933988005, "mdate": 1762933988005, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}