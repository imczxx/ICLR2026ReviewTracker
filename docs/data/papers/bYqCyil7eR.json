{"id": "bYqCyil7eR", "number": 16290, "cdate": 1758262740724, "mdate": 1763126967229, "content": {"title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder", "abstract": "Image deblurring, removing blurring artifacts from images, is a fundamental task in computational photography and low-level computer vision. Existing approaches focus on specialized solutions tailored to particular blur types, thus, these solutions lack generalization. This limitation in current methods implies requiring multiple models to cover several blur types, which is not practical in many real scenarios. In this paper, we introduce the first all-in-one deblurring method capable of efficiently restoring images affected by diverse blur degradations, including global motion, local motion, blur in low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE) decoding module, which dynamically routes image features based on the recognized blur degradation, enabling precise and efficient restoration in an end-to-end manner. Our unified approach not only achieves performance comparable to dedicated task-specific models, but also shows promising generalization to unseen blur scenarios, particularly when leveraging appropriate expert selection.", "tldr": "The authors propose the unified deblurring method to restore blurry images in diverse scenarios", "keywords": ["image restoration", "inverse problems", "image processing", "computational photography", "deblurring"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/bc2c3b24b775fac8eff0bcf2fde0c79de78c702a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the first all-in-one deblurring method capable of efficiently restoring images affected by diverse blur degradations, including global motion, local motion, blur in low-light conditions, and defocus blur. To achieve this, the authors propose a mixture-of-experts (MoE) decoding module, which dynamically routes image features based on the recognized blur degradation, enabling precise and efficient\nrestoration in an end-to-end manner. The proposed approach not only achieves performance comparable to dedicated task-specific models, but also shows promising generalization to unseen blur scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes the first all-in-one deblurring method that can efficiently restore any blurry image. The motivation and contributions are technically sound, and the design of the mixture-of-experts (MoE) decoding module is novel.\n\n2. The subsection “Deblurring Similarity Analysis” effectively introduces the importance of an all-in-one network for deblurring, enabling readers to easily understand the motivation behind the proposed method.\n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. Although the motivation and idea are technically sound, the performance of the proposed DeMoE seems unsatisfactory. As shown in Table 1, the performance of DeMoE$_{k=1}$ works similar to the baseline NAFNet while with two times number of parameters. \n\n2. As the authors propose a new all-in-one model, it would be better to compare the proposed DeMoE with more recent all-in-one methods in Table 1, such as AdaIR [A] (ICLR 2025), MoCE-IR [B] (CVPR 2025), and DFPIR [C] (CVPR 2025).\n\n3. Although the authors claim to use multiple experts, the experimental results in Tables 1, 2, and 3 only report the performance of a single expert, DeMoE$_{k=1}$, which appears somewhat inconsistent with the proposed multi-expert framework.\n\n[A] AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation. In CVPR 2025.\n\n[B] Complexity Experts are Task-Discriminative Learners for Any Image Restoration: . In CVPR 2025.\n\n[C] Degradation-Aware Feature Perturbation for All-in-One Image Restoration. In CVPR 2025"}, "questions": {"value": "1. In Tables 1, 2, and 3, why do you report only the performance of a single expert, DeMoE$_{k=1}$.\n \n How about experts such as DeMoE$_{k=5}$?\n\n2. Could your compare the proposed method with the latest all-in-one models, such as AdaIR [A] (ICLR 2025), MoCE-IR [B] (CVPR 2025), and DFPIR [C] (CVPR 2025).\n\n3. I am curious about how the proposed method handles images that contain both motion blur and defocus blur. In some cases, one may wish to preserve the defocus blur while removing only the motion blur. However, your method appears to remove both types of blur simultaneously.\n\n[A] AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation. In CVPR 2025.\n\n[B] Complexity Experts are Task-Discriminative Learners for Any Image Restoration: . In CVPR 2025.\n\n[C] Degradation-Aware Feature Perturbation for All-in-One Image Restoration. In CVPR 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "5KoyB2soMo", "forum": "bYqCyil7eR", "replyto": "bYqCyil7eR", "signatures": ["ICLR.cc/2026/Conference/Submission16290/Reviewer_SeTP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16290/Reviewer_SeTP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761366052816, "cdate": 1761366052816, "tmdate": 1762926433926, "mdate": 1762926433926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}, "comment": {"value": "We appreciate the time and effort from the reviewers, AC and PC members, specially Reviewer SeTP. \n\nHowever, most pointed weaknesses were addressed and even explained in the main paper.\nThe main weakness is actually highlighted by us in the paper, Line 409: our all-in-one method **--the first all-in-one method for real-world deblurring--** struggles to surpass (in every benchmark) task specific models designed and trained on particular dataset. \n\n> All-in-One (AIO) restoration methods typically **underperform** compared to task-specific methods. For instance, AIO Restormer achieves a PSNR (dB) of  27.76 in GoPro deblurring, while the task-specific version achieves 32.92 (*+5.2dB*). This is a **well-known practical limitation of AIO methods** Thus, comparison with task-specific methods is only for reference purposes (i.e., upper bound of the solution).\n\nSurprisingly, **our all-in-one restoration method was matching task-specific SOTA on three datasets** (LOLBlur, ReLoBlur, DPDD), *We hope that someone could point out to a single publication where an AIO method outperforms all task-specific methods in 5 tasks.*\n\n\nWe appreciate that the reviewers found interesting our Deblurring Similarity Analysis, and acknowledge the efficiency and practical benefits of DeMoE --- equivalent to 5 task-specific NAFNets of 10M parameters.\n\nHope you understand our point."}}, "id": "sqzOznZQ8O", "forum": "bYqCyil7eR", "replyto": "bYqCyil7eR", "signatures": ["ICLR.cc/2026/Conference/Submission16290/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16290/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763126966442, "cdate": 1763126966442, "tmdate": 1763126966442, "mdate": 1763126966442, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to propose a unified image deblurring model to tackle various blur types in the real world. The paper first investigated the blur type difference of deblurring datasets by the network similarity study.  Then, a mixture-of-experts decoder is introduced to deal with various blur types in a divide-and-conquer way. The performance is evaluated across several deblurring benchmarks, including synthetic and real blur scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Network similarity study provides a novel and insightful analysis of the blur types in different deblurring datasets. The conclusion also contributes to the community.\n- The MoE structure yields an efficient method compared with previous sotas.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- The effect of general deblurring is not satisfactory. In Table 2, DeMoE without manual selection cannot surpass previous methods in both RealDOF and Real-LOLBlur. Meanwhile, the reviewer considers manually selecting experts as a task-specific method, since the type of blur in the input image should be unknown in the unified deblurring scenario.\n- The MoE router is trained with the ground-truth degradation label. Where does the label come from? \n- The allocation of the router in the MoE module needs to be analyzed using various deblurring datasets to verify whether it draws consistent conclusions with the network similarity study section.\n- DeMoE is trained by the proposed AIO-Blur dataset. How about other rivals listed in the experimental results? A fair comparison is supposed to use the same training data."}, "questions": {"value": "Please refer to the weakness part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bFWWUuyrDl", "forum": "bYqCyil7eR", "replyto": "bYqCyil7eR", "signatures": ["ICLR.cc/2026/Conference/Submission16290/Reviewer_1g1q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16290/Reviewer_1g1q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874923314, "cdate": 1761874923314, "tmdate": 1762926433530, "mdate": 1762926433530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified image deblurring framework, DeMoE (Mixture-of-Experts Decoder), which employs a mixture-of-experts mechanism to handle multiple blur types within a single model. Through network similarity analysis, the authors find strong parameter correlations among task-specific deblurring models, motivating a router-controlled MoE decoder that dynamically selects experts for different blur types. Additionally, the paper constructs the AIO-Blur dataset, integrating multiple blur scenarios for unified training and evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive experiments: The study includes AIO-Blur and OOD testing, task-specific comparisons, ablation studies, and efficiency analyses, offering broad coverage and credible conclusions.\n2. Practical significance: DeMoE serves as a unified framework applicable to diverse blur scenarios, reducing the need for multiple specialized models and showing potential for real-world deployment."}, "weaknesses": {"value": "1. This article lacks significant innovation, and its multi expert strategy is very common in all-in-one image restoration, only changing the focus of different restoration tasks in all-in-one image restoration to different scenes under the single task of deblurring.\n\n2. Lack of algorithmic overview: Although Figure 3 shows the network architecture, a concise workflow summarizing differences between training and inference stages is missing. \n\n3. Limited router generalization: The router performs poorly on OOD datasets, leading to incorrect expert selection. While manual expert control mitigates this, it undermines the “dynamically unified model” objective.\n\n4. Unclear expert sharing and independence mechanism : The paper does not clarify whether expert parameters are fully independent or partially shared, nor analyze how this design affects model capacity and generalization.\n\n5. In Table 1, DeMoE fails to reach SOTA on nearly half of the six datasets. On RealBlur, DeMoE is inferior to SFHFormer which has less parameters; on ReLoBlur and DPDD, DeMoE shows no advantage over NAFNet which has less Computational Cost.\n\n6. In Table 3, DeMoE performs near the bottom among compared methods, indicating unsatisfactory results.\n\n7. Labeling error: In Table 3 (ReLoBlur results), two different SSIM values are both marked as SOTA. SSIM(LBAG)=0.9249 and SSIM(DeMoEk=1)=0.925, but the former is marked as 2nd best."}, "questions": {"value": "1. What are the differences between DeMoE’s training and inference stages? Consider adding pseudocode or a system flowchart for clarity.\n\n2. Has the router’s classification accuracy been evaluated on OOD datasets? Would uncertainty-based or entropy-based gating improve its generalization?\n\n3. Since the encoder is shared while each expert in the decoder has independent convolutional modules, please analyze this “partially shared + partially independent” design trade-off. Specifically, does full independence improve performance? Could partial sharing enhance generalization or parameter efficiency? An ablation or comparative study is recommended."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "lXLkoNP0q1", "forum": "bYqCyil7eR", "replyto": "bYqCyil7eR", "signatures": ["ICLR.cc/2026/Conference/Submission16290/Reviewer_uDGq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16290/Reviewer_uDGq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16290/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967609741, "cdate": 1761967609741, "tmdate": 1762926433085, "mdate": 1762926433085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}