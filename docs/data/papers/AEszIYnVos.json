{"id": "AEszIYnVos", "number": 3035, "cdate": 1757319204807, "mdate": 1759898112884, "content": {"title": "NECromancer: Breathing Life into Skeletons via BVH Animation", "abstract": "Motion tokenization is fundamental to the development of generalizable motion models, yet existing approaches remain restricted to species-specific skeletons, such as humans, thereby limiting their applicability across diverse morphologies. We present NECromancer (NEC), a universal motion tokenizer designed to operate on arbitrary BVH skeletons. NEC is built upon three core components: (1) an\nOntology-aWare Skeletal Graph EncOder (OwO), which leverages graph neural networks to encode structural priors extracted from BVH files—including joint-name semantics, rest-pose offsets, and skeletal topology—into robust skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT), which compresses motion sequences into a universal, topology–invariant latent representation, thereby decoupling motion dynamics from morphology; and (3) the Unified BVH Universe (UvU), a large-scale dataset that consolidates BVH motions across heterogeneous skeletons (humans, quadrupeds, and other species), enabling systematic training and evaluation under diverse morphologies. Experimental results demonstrate that NEC achieves high-fidelity motion reconstruction with substantial compression, while effectively disentangling motion from skeletal structure. This capability supports a broad range of downstream tasks, including cross-species motion transfer, motion composition, denoising, generation (plug-and-play with any token-based generator; e.g., MoMask) and motion–text retrieval (via an OwO-based CLIP variant). By grounding motion representation in BVH animation while removing species-specific constraints, NEC establishes a principled framework for universal motion analysis and synthesis across varied morphologies.", "tldr": "", "keywords": ["Motion tokenization，Motion Generation，BVH Animation，Skeletal-Invariant Representation，Cross-Species Motion Transfer，Motion Compression"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/03447ea30b537863ea99aebb8f383cfc5a5f8926.pdf", "supplementary_material": "/attachment/6113d2b2324762ca58082fae08bbb43e9870e3b4.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a novel tokenizer named NECromancer (NEC) to represent the motions of arbitrary BVH skeletons. The tokenizer consists of two key components: Ontology-aWare Skeletal Graph EncOder (OwO) and a Topology-Agnostic Tokenizer (TAT). This work also contributes a new dataset named Unified BVH Universe (UvU). In the experiments, it shows that the proposed NEC outperforms standard VQVAE (Van Den Oord et al. (2017)) and RVQVAE (Guo et al. (2023)) tokenizers."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work proposes a novel motion tokenizer and shows that it performs better than existing standard tokenizers. \n\n2. This work contributes a large-scale BVH benchmark for heterogeneous species and various skeletal topologies."}, "weaknesses": {"value": "1. The dataset contribution is somewhat unclear. \n- The details of dataset construction are largely ignored. It may be partly because the section 3.2 is too short to describe how difficult the dataset construction is. \n- The current description gives the impression that the dataset is simple combination of existing three dataset with some transformations. \n- Also, the use of Truebones Zoo and text annotation is done in the following paper in a more thorough way. \n- W. Lee et al., How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary Objects, ICML 2025.\n\n2. Fig. 2 is hard to see due to too small text with too light color. \n\n3. The empirical evaluation is limited in that the compared baselines are only two - VQVAE and RVQVAE.\n- For example, the following state-of-the-art baselines could be compared. \n- B. Jiang et al., Causal Motion Tokenizer for Streaming Motion Generation, ICCV 22025. \n- J. Zhang et al., Generating Human Motion From Textual Descriptions With Discrete Representations, CVPR 2023.\n- C. Guo et al., TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts, ECCV 2022. \n\n4. In the same vein, the qualitative results are highly limited. \n- No comparison with state-of-the-art models on reconstruction are compared in each dataset.\n- Only a small Table (Table 1) is almost all of empirical evaluation of this work, as Table 2 is a rather straightforward ablation study on the proposed method. \n\n5. Qualitative results are somewhat pointless, as the key message of Fig.3-4 are unclear. \n- Generally, the figures are too small to recognize fine details of comparison. \n- In Fig.3, the NEC results are quite different with GT. With no baseline results, it is hard to know who much the NEC is good. \n- In Fig.4, the success of motion transfer is hard to be convinced.  \n- Also, they could be cherry-picked."}, "questions": {"value": "Please refer to the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uJInUryqzA", "forum": "AEszIYnVos", "replyto": "AEszIYnVos", "signatures": ["ICLR.cc/2026/Conference/Submission3035/Reviewer_gGnx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3035/Reviewer_gGnx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657328240, "cdate": 1761657328240, "tmdate": 1762916512370, "mdate": 1762916512370, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address the problem that existing motion generation models are limited to species-specific skeletons (e.g., humans). To this end, the authors propose a universal motion representation framework named NECROMANCER (NEC). The framework consists of three main contributions: 1) an Ontology-aWare Skeletal Graph EncOder (OwO) to extract skeleton embeddings containing topological and semantic information from BVH files; 2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences of arbitrary skeletons into morphology-agnostic discrete tokens; and 3) a large-scale, multi-species BVH motion dataset named UvU for training and evaluation. Experimental results demonstrate that the framework can achieve high-quality motion reconstruction and cross-species motion transfer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.  **Addresses a significant problem**: This paper directly confronts a core limitation in the field of motion generation—the model's dependency on specific skeleton topologies. The proposed universal tokenizer, capable of handling arbitrary BVH skeletons, greatly expands the applicability of motion models and holds significant research and practical value.\n\n2.  **Systematic contribution**: The contribution is comprehensive and solid. The authors not only propose a new model (NEC) but also build a new, large-scale, and diverse-species dataset (UvU) for it. The dataset itself is a valuable contribution to the community and can facilitate future research in universal motion modeling.\n\n3.  **Solid experimental validation**: The paper thoroughly validates the effectiveness of its method through experiments across multiple tasks (reconstruction, retrieval, motion transfer). The comparison against baselines clearly shows the advantages of NEC in handling heterogeneous skeletons, with particularly impressive performance on non-human skeletons."}, "weaknesses": {"value": "1.  **Strong dependency on data quality**: The `OwO` encoder relies on extracting semantic features from joint names. This means the model's performance is likely highly dependent on the standardization and consistency of joint naming within the BVH dataset. For data from the wild with messy or non-semantic names, the model's generalization capability might be compromised."}, "questions": {"value": "1.  Regarding the `OwO` encoder, to what extent does it rely on canonical joint naming? If the input BVH files use non-semantic names (e.g., 'joint_1', 'bone_23'), how much would the performance degrade? Have any robustness tests been conducted in this regard?\n\n2.  Could the authors further clarify the core novelty of the spatio-temporal module in the Topology-Agnostic Tokenizer (TAT)? Compared to existing spatio-temporal Transformers in motion modeling, what are its key differences and advantages?\n\n3.  In the qualitative demonstrations of cross-species motion transfer, how does the model handle transfers that might be semantically or physically implausible (e.g., transferring a human dance motion to a fish)? Is there a mechanism to evaluate or ensure the plausibility of the transfer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EFtSFKYyiV", "forum": "AEszIYnVos", "replyto": "AEszIYnVos", "signatures": ["ICLR.cc/2026/Conference/Submission3035/Reviewer_rg3f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3035/Reviewer_rg3f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761738402366, "cdate": 1761738402366, "tmdate": 1762916510863, "mdate": 1762916510863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a method to learn skeletal motion tokens by directly consuming the information stored in the BVH format. The ontology-aware skeletal graph encoder is responsible for encoding the skeletal structure. Specifically, each skeletal node (joint) feature is described with the CLIP embedding of its name projected through a fully-connected layer. The edges are described as the concatenation of the connected node features and their offsets encoded with the sinusoidal embedding, projected through a fully-connected layer. A graph attention layer computes the graph joint feature as a weighted sum of the edge-conditioned messages and the attention weight. The skeletal graph encoder is trained with a geometric loss over joint offsets, a topological loss with the least common ancestor prediction, and a semantic loss to encourage node features with semantic consistency. A topology-agnostic tokenizer is responsible for encoding the motion with a repeated sequence of a spatial block followed by a temporal block. The source motions are represented by per-joint translation and rotation (6D representation), which is projected through an MLP and then fused with the graph joint feature. This feature, with a virtual joint feature concatenated, is passed to a spatial block composed of a multi-head attention transformer modeling correlations between joints, then to a temporal block with a 1D convolution and a 1D ResNet. The virtual joint part is discretized with an RVQ to encode the motion token. The decoder is the reverse of the topology-agnostic tokenizer, with the graph joints feature of the target skeleton injected into the non-virtual joint features. The setup is trained with a heterogeneous dataset composed of HumanML3D, Objaverse-XL, and Truebone-Zoo with data filtering and augmentation applied. The paper demonstrates motion reconstruction and motion transfer with the learned features."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The goal to unify the motion tokenization is ambitious.\n* The combined dataset with the curation strategy could help the community"}, "weaknesses": {"value": "* Hard to interpret the provided videos\n  * What do the \"transfer\" examples suppose to mean? All of them have the prefix \"gt\" (ground truth?). Which ones are the source motions, which ones are the transferred motions?\n  * I see only quadrupeds in the transfer folder. Any non-quadruped transfer examples? Humanoid-to-quadruped or quadruped-to-humanoid?\n* Questionable generalizability to different skeletal morphologies\n  * For example, it looks like the joint semantic loss is taken over the same joint indices. This does not make sense as the joint ordering is arbitrary (e.g., children can be in any order), and there can be many intermediate joints (rigs can have a different number of spine joints and neck joints)\n  * The semantic understanding of the joints must be paired with the spatial relations. As far as I can see, there is nothing in the model to learn this\n  * As far as I see, there is nothing in the model and the training strategy to encourage the mapping of the same motion applied to different skeletal features. How would it know to move a fox with the motion token from a humanoid?\n* Not enough ablations on architectural design choices, especially on the effectiveness of RVQ\n* The paper can trim the UvU section. For example, the main method does not care about skinning other than for the visualization. Skinning is important, but since this does not matter for the main paper, why not move UvU to the appendix and add more details on the main architecture?\n* (minor) BVH is just a file format. In theory, there is nothing in the method that hard-couples it with the BVH format. It could be any other 3D formats, such as FBX, USD, and glTF. In fact, BVH is not a suggested format for general rig assets. I would eliminate \"BVH\" from the paper title and most of the main text to minimize confusion. This also enhances the paper's general applicability"}, "questions": {"value": "Please answer questions in Weaknesses, mainly on the generalizability to different skeletons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9KL9TNTudV", "forum": "AEszIYnVos", "replyto": "AEszIYnVos", "signatures": ["ICLR.cc/2026/Conference/Submission3035/Reviewer_14Ko"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3035/Reviewer_14Ko"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889523594, "cdate": 1761889523594, "tmdate": 1762916510727, "mdate": 1762916510727, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to introduce a universal motion tokenizer that can encode and reconstruct motions from arbitrary skeletons formats. This allows unified modeling and transfer of motions across a wide variety of skeletons, in contrast with current methods which are typically limited to skeletons from the same family. Data is gathered from 3 sources: 1) human meshes from HumanML3D, 2) meshes from ObjaVerse, and 3) meshes from Trubones-Zoo. All meshes are put in a unified BVH format. The tokenization consists of two models: a graph encoder that provides a distinguishable encoding to each joint given the skeleton rest pose, and a unified tokenizer that can use the graph embeddings and BVH motion sequences to provide a unified tokenization of different types of skeletons. Experiments show that the proposed tokenizer is provides superior motion reconstruction relative to the naive approach that simply pads joints for skeletons of different lengths. Proof-of-concept experiments for motion transfer between different skeletons and unified skeleton-agnostic motion generation are provided."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* This work tackles an interesting and important problem. Unified skeleton representation has the potential to greatly expand the scope of motion generation models and break free of limitations imposed by scarcity of 4-D data for a variety of skeleton types.\n* The proposed method of learning a unified token representation of motions is a reasonable and potentially useful direction to solve this problem.\n* Experimental results show improved reconstruction over a naive padding-based approach for skeleton reconstruction.\n* Videos in the supplementary material provide evidence that the proposed method can lead to natural reconstruction, transfer, and generation of motions across skeleton types."}, "weaknesses": {"value": "* The motion transfer and generation directions are not explored very thoroughly, and only a few examples are provided.\n* The details of tokenization in the paper are somewhat difficult to follow, especially the relation between the graph embedder and the tokenization model. Perhaps it would help to move Figure 3 and 4 into the supplementary material and provide more mathematical details of training these models in the main text.\n* The training of the graph embedder is based on heuristic objectives and it is not clear whether the training method proposed is the optimal one."}, "questions": {"value": "* Can you provide more examples of motion transfer and unified generation?\n* Is it possible to ablate the importance of the graph embedder to demonstrate its importance within the tokenization model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MGlY23QJE9", "forum": "AEszIYnVos", "replyto": "AEszIYnVos", "signatures": ["ICLR.cc/2026/Conference/Submission3035/Reviewer_ScnN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3035/Reviewer_ScnN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3035/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762211296332, "cdate": 1762211296332, "tmdate": 1762916510572, "mdate": 1762916510572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}