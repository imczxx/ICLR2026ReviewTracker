{"id": "wleUyyqTz2", "number": 14928, "cdate": 1758245653395, "mdate": 1759897341023, "content": {"title": "Direct Preference Optimization for Primitive-Enabled Hierarchical RL: A Bilevel Approach", "abstract": "Hierarchical reinforcement learning (HRL) enables agents to solve complex, long-horizon tasks by decomposing them into manageable sub-tasks. However, HRL methods face two fundamental challenges: (i) non-stationarity caused by the evolving lower-level policy during training, which destabilizes higher-level learning, and (ii) the generation of infeasible subgoals that lower-level policies cannot achieve. To address these challenges, we introduce DIPPER, a novel HRL framework that formulates goal-conditioned HRL as a bi-level optimization problem and leverages direct preference optimization (DPO) to train the higher-level policy. By learning from preference comparisons over subgoal sequences rather than rewards that depend on the evolving lower-level policy, DIPPER mitigates the impact of non-stationarity on higher-level learning. To address infeasible subgoals, DIPPER incorporates lower-level value function regularization that encourages the higher-level policy to propose achievable subgoals. We introduce two novel metrics to quantitatively verify that DIPPER mitigates non-stationarity and infeasible subgoal generation issues in HRL. Empirical evaluation on challenging robotic navigation and manipulation benchmarks shows that DIPPER achieves upto 40% improvements over state-of-the-art baselines on challenging sparse-reward scenarios, highlighting the potential of preference-based learning for addressing longstanding HRL limitations.", "tldr": "We present DIPPER, a preference-based learning approach to hierarchical reinforcement learning that mitigates the issues of non-stationary rewards and infeasible subgoal prediction.", "keywords": ["hierarchical reinforcement learning", "preference based learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22942179dd261e59dd2cf5b7b916551ad7d7276d.pdf", "supplementary_material": "/attachment/4abc8cdf3d1dee86170defafced96839d81d0c04.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose DIPPER, a hierarchical reinforcement learning (HRL) framework that formulates HRL as a bi-level optimization problem. In this formulation, the higher-level “teacher” policy and lower-level “student” policy are jointly optimized while explicitly modeling their interdependence. To mitigate the non-stationarity caused by the evolving lower-level policy, the authors leverage Direct Preference Optimization (DPO) to train the higher-level policy on stationary human preference data rather than environment rewards. Additionally, they introduce a lower-level value function regularization term that encourages the higher-level policy to propose feasible and achievable subgoals."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a rigorous bi-level mathematical formulation of hierarchical reinforcement learning (HRL).\n\nThe use of real human preference data is commendable. Conducting human-in-the-loop experiments introduces significant complexity and time overhead, yet provides stronger evidence for the method’s practical relevance\n\nThe authors empirically evaluate whether DIPPER actually addresses the two challenges they identify, non-stationarity and infeasibility of subgoal, rather than assuming these issues are solved. This careful, hypothesis-driven evaluation is a strong aspect of the paper.\n\nThey explore how varying key hyperparameters (λ, β) affects performance, showing an understanding of what aspects drive DIPPER’s behavior."}, "weaknesses": {"value": "**Stationarity of Human Preferences**\n\nThe paper assumes that human preferences mitigate non-stationarity, yet human preferences themselves are not necessarily stationary.\nPrior work suggests that people’s judgments can change over time [1], particularly when confidence is low. \n\nQuestion: How would DIPPER handle cases where human preferences shift if the same trajectories are presented multiple times? Would the algorithm adapt?\n\n**Missing Related Work**\n\nThe related work section omits relevant approaches in automatic curriculum learning that use bi-level optimization to generate goals for lower-level policies.\nExamples include Narvekar and Stone (2019) [2] and Muslimani et al. (2023) [3]. \nThese should be discussed to contextualize DIPPER within existing curriculum learning frameworks.\n\n**Failure Cases and Environment Suitability**\n\nDIPPER fails in the simple maze task. The paper does not explain why.\n\nQuestion: What types of environments does DIPPER work best in?\n\nIn environments where DIPPER outperforms baselines (Pick & Place and Push), success rates remain low (around 30%).\n\nQuestion: Why does performance plateau at this relatively low level? Would longer training or additional preference data improve results?\n\n\n**Baselines**\n\nThe authors remove the HER component from PIPER “for fairness,” but this substantially alters the original algorithm.\nThis change may make the comparison less meaningful.\n\nQuestion: How would DIPPER perform against the original PIPER implementation (with HER) as described in the source paper?\n\nThe authors mention tuning DIPPER’s hyperparameters using grid search, but it is unclear whether the same effort was made for the baselines.\n This introduces a potential unfair advantage if baselines are not tuned equivalently.\n\n**Evaluation Metrics and Interpretation**\n\nThe “lower Q-function metric” measures Q-values for subgoals proposed by the higher-level policy. However, this could be inflated if the higher policy proposes easy subgoals.\n\nQuestion: Do the proposed metrics actually capture whether DIPPER learns progressively more difficult subgoals over time?\n\n\n**Human Preference Data**\n\nThe paper provides limited details about the human annotators.\n\nQuestions:\n\nWho were the annotators (experts or laypeople)?\n\nWere they trained or provided with guidance?\n\nWhat were their demographic backgrounds?\n\n**Most importantly, was this data collection ethics-approved?**\n\nThese details are crucial to assess the reliability and reproducibility of the preference data.\n\n\n**Sources:**\n[1]Visser et al (2005) https://pubmed.ncbi.nlm.nih.gov/16022057/\n\n[2] Narvekar and Stone (2019) https://arxiv.org/pdf/1812.00285\n\n[3] Muslimani et al (2023) https://arxiv.org/pdf/2204.11897"}, "questions": {"value": "I included my questions in the weaknesses section."}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The authors use human subjects, but I cannot determine whether the data collected was approved by an ethics committee. \nIf the study was ethics approved (which the authors can address during the rebuttal), then I do not believe any further review in this issue is necessary."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EDscZ5GQIz", "forum": "wleUyyqTz2", "replyto": "wleUyyqTz2", "signatures": ["ICLR.cc/2026/Conference/Submission14928/Reviewer_fdjG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14928/Reviewer_fdjG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760903555931, "cdate": 1760903555931, "tmdate": 1762925268312, "mdate": 1762925268312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors present DIPPER, a hierarchical RL method that directly optimizes environment reward and human preferences. Given human preferences, the authors derive a bi-level optimization objective that train the policy to increase the likelihood of the preferred trajectories with the high-level policy while simultaneously updating the low-level policy to try to maintain value improvement for the same objective. The specific objective is derived from DPO, and helps mitigate non-stationarity common to most HRL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Comparisons:** The authors compare against a wide array of relevant baselines, demonstrating great performance against said baselines on manipulation tasks especially. \n\n**Motivation:** Non-stationarity in HRL is a big issue and this paper presents a well-motivated solution for it.\n\n**Experiments:** The experiments are performed on tasks well-suited for HRL, and the analysis on goal distance prediction against HIER and HAC demonstrates that DIPPER’s objective encourages sampling reachable goals for the lower-level policy.\n\n**Clarity:** The paper is overall quite clear and the walkthrough of how to obtain the objective was both interesting and easy to read."}, "weaknesses": {"value": "**Figures**: All of the results figures have small font that make them harder to read, and are also clearly image files put into overleaf instead of vectorized PDFs. The results figures should have thicker lines for each baseline, more spacing between baselines on the legend (in fig 2), and larger font for the x and y axes labels and ticks labels.\n\n**Annotation cost:** As authors admit, there is a high annotation cost to obtaining labels with human prediction. \n\n**Experiments:** Given the fact that the authors have human annotations, it seems that more challenging tasks could’ve been demonstrated in the experiments. This is not a reason to reject the paper, however I will list this as a slight weakness of the paper.\n\n**Minor issues:**\n\n- There’s related work which also mitigates non-stationary and infeasible subgoal generation by modeling *intrinsic* options: e.g., https://arxiv.org/abs/2101.06521, some comparison against this work and any follow-ups in the related works section would be beneficial\n- There’s also related work on unifying low level and high level policy optimization: https://sites.google.com/view/hippo-rl\n- L363: “we maintain empirical consistency across all baselines to ensure fair comparisons” — what does this actually mean? be specific here"}, "questions": {"value": "Instead of a subgoal distance metric (fig 3), why not directly measure the success rate of the lower level policy’s ability to reach goals? This is the actual metric that matters for addressing the subgoal feasability problem, right?\n\nRe: annotation cost, prior work in reward learning has demonstrated that VLMs can be useful for obtaining preferences in place of humans: https://rlvlmf2024.github.io/, have the authors looked into this or tried out similar approaches at a small scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IOLHXKWBUJ", "forum": "wleUyyqTz2", "replyto": "wleUyyqTz2", "signatures": ["ICLR.cc/2026/Conference/Submission14928/Reviewer_DfyA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14928/Reviewer_DfyA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761444129606, "cdate": 1761444129606, "tmdate": 1762925267738, "mdate": 1762925267738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper has proposed a novel hierarchical reinforcement learning framework, which employs preference-based learning in the high-level policy to mitigate the non-stationary issue in hierarchical policy learning. The proposed framework is evaluated in a set of simulated long-horizon tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe research problem of non-stationarity is significantly important in the hierarchical reinforcement learning domain."}, "weaknesses": {"value": "1.\tThe technical contribution of this work is low, which is a direct application of preference-based learning to high-level policy optimization.\n\n2.\tExperiments are limited to simulated environments. Demonstrations in real robots or transfer scenarios would significantly strengthen the empirical validation."}, "questions": {"value": "1.\tHow sensitive is DIPPER to the scale or bias of human preferences? Would synthetic or learned preferences generalize effectively?\n\n2.\tCould DIPPER be extended to fully autonomous preference learning (e.g., self-generated ranking signals) without human input?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AruSETHV07", "forum": "wleUyyqTz2", "replyto": "wleUyyqTz2", "signatures": ["ICLR.cc/2026/Conference/Submission14928/Reviewer_ywxX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14928/Reviewer_ywxX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761899718989, "cdate": 1761899718989, "tmdate": 1762925267288, "mdate": 1762925267288, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This introduces a hierarchical RL framework (DIPPER) aimed to tackles two HRL issues: 1. non-stationarity of the higher-level environment and 2. infeasible subgoal generation. The framework is composed of a high level goal-conditioned HRL with Direct Preference Optimization (DPO) and standard RL at the low level. This work formalize HRL as a constrained bi-level optimization problem. \n\nExperiments on four challenging sparse-reward navigation and manipulation tasks (random mazes, pick-and-place, push, and Franka kitchen) show that DIPPER substantially outperforms both standar and hierarchical baselines, including prior preference-based HRL methods. This work also uses subgoal distance and low-level Q value to empirically support claims about reduced non-stationarity and improved subgoal feasibility."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Originality: The paper gives a clean bi-level optimization formulation of goal-conditioned HRL, rewriting the hierarchy as an upper-level problem with a constraint that the lower-level policy is (locally) optimal. In contrast to PIPER, which learns a reward model and then uses RL/RLHF on top of it, this work directly trains the high-level policy via DPO on a preference dataset, which allows the framework to avoid modeling explicit rewards and avoids having a second RL loop. \n\n2. Significance: This paper provides a preference-based, DPO-driven recipe with a clear regularizer derived from a bi-level formulation that is not present in prior HRL work. The experimental results also suggest an improvement of around 40% in success rates on harder sparse-reward tasks, compared to state-of-the-art baselines that already use preferences and primitive-informed regularization. \n\n3. Clarity: The authors provide pseudo-code for their framework and clearly describe the alternation between preference-trajectory collection and updates of the high-level DPO objective, low-level value function, and low-level SAC policy. The introduction also clearly states the two central issues (non-stationarity and infeasible subgoals). \n\n4. Quality: The derivation from bi-level HRL to a constrained problem, then to a Lagrangian, and finally to a primitive-regularized DPO objective is detailed and internally consistent."}, "weaknesses": {"value": "1. The bi-level formalization in this paper is in some sense rephrasing standard HRL coupling (optimal lower-level policy conditioned on higher-level subgoals) in the language of constrained optimization. The authors could consider adding more analysis of the resulting bi-level problem such as convergence guarantees, regret bounds, and when the relaxed constraint and approximate value function yield near-optimal behavior. \n\n2. This paper mentions that one benefit of DPO over RLHF is computational simplicity and stability. This statement is not quantified in terms of training time, memory etc for DIPPER vs RLHF-based alternatives (such as PIPER or a simple RLHF high-level baseline). ALso, sample efficiency is mainly reflected in success-rate vs timestep plots. There could be a more fine-grained analysis of preference-query complexity or number of environment transitions until convergence.\n\n3. The paper claims their framework works on “long-horizon complex robotic tasks”, however the four environments are standard MuJoCo-style navigation/manipulation tasks (with some challenge enhancements like random mazes and sparse kitchen rewards). The work could consider adding vision-based setting with more complex hierarchical structures. Prior works like CRISP/PEAR do report real-robot experiments with more challenging perception and dynamics."}, "questions": {"value": "1. Are preferences generated from ground-truth reward (e.g., via Bradley–Terry over cumulative environment reward), or from some kind of synthetic labeling procedure? How many preference queries are used per environment, and how does performance scale with the number of preferences?\n\n2. The work approximates V^{L*} by updating V^L_m for m gradient steps between policy updates. How sensitive is DIPPER to m? Intuitively, if V^L_m is poor in the earlier portion of training, the regularizer might be actively pushing the high-level toward bad subgoals. Is there any safeguard (e.g., annealing λ) or empirical evidence that this does not happen?\n\n3. Some suggestions on the presentation: \nThe work could benefit from having a more clear overview diagram that explicitly shows the bi-level view and how it leads to the primitive-regularized DPO block, with arrows from lower-level value function to the high-level DPO loss.\nIn the experiments, it would help to explicitly list the baselines and their key differences in a table, including whether they use preferences, whether they use primitive-informed regularization, and whether they are hierarchical or flat."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "v3AJExzp2d", "forum": "wleUyyqTz2", "replyto": "wleUyyqTz2", "signatures": ["ICLR.cc/2026/Conference/Submission14928/Reviewer_W15h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14928/Reviewer_W15h"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14928/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762895560349, "cdate": 1762895560349, "tmdate": 1762925266958, "mdate": 1762925266958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}