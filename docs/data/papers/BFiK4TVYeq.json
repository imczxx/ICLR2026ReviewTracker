{"id": "BFiK4TVYeq", "number": 19525, "cdate": 1758296965907, "mdate": 1759897034643, "content": {"title": "AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation", "abstract": "Learning generalizable manipulation policies hinges on data, yet robot manipulation data is scarce and often entangled with specific embodiments, making both cross-task and cross-platform transfer difficult. We tackle this challenge with \\textbf{task-agnostic embodiment modeling}, which learns embodiment dynamics directly from \\emph{task-agnostic action} data and decouples them from high-level policy learning. This data-driven perspective bypasses the limitations of traditional dynamics-based modeling and enables scalable reuse of action data across different tasks. \nBuilding on this principle, we introduce \\textbf{AnyPos}, a unified pipeline that integrates large-scale automated exploration with robust inverse dynamics learning. AnyPos generates diverse yet safe trajectories at scale, then learns embodiment representations by \\textit{decoupling arm and end-effector motions} and employing a \\textit{direction-aware decoder} to stabilize predictions under distribution shift, which can be seamlessly coupled with diverse high-level policy models. \nIn comparison to the standard baseline, AnyPos achieves a 51\\% improvement in test accuracy. On manipulation tasks such as operating a microwave, toasting bread, folding clothes, watering plants, and scrubbing plates, AnyPos raises success rates by {30--40\\%} over strong baselines. These results highlight data-driven embodiment modeling as a practical route to overcoming data scarcity and achieving generalization across tasks and platforms in visuomotor control.", "tldr": "", "keywords": ["Robot Learning", "Computer Vision", "Reinforcement Learning", "Embodied AI"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f9c9fb3e8040a62d0c7caff2d0be9851224e84a.pdf", "supplementary_material": "/attachment/b3ded427419d3031ac2dd65e38a04b348025c954.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes AnyPos, a framework for learning robot actions in a task-agnostic way. Its core contribution lies in two connected parts: first, an automated exploration pipeline that uses a reinforcement-trained mapper and safety constraints to sample the robot’s workspace and generate large quantities of collision-free image–action pairs without teleoperation or task labels; second, an inverse-dynamics embodiment model that predicts feasible joint positions from visual inputs using an arm-decoupled design and a direction-aware decoder for sub-degree precision. Together, these components aim to provide a reusable “feasibility prior” that can later be coupled with high-level policy or video-generation models to execute diverse manipulation tasks more efficiently than task-specific datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper’s strengths lie in its clear problem framing and modular design—it separates physical feasibility learning from task semantics through a well-structured pipeline that’s easy to follow. The authors present a fully automated, high-throughput data collection system that efficiently explores the workspace without teleoperation, coupled with a safety-aware exploration scheme that enforces collision avoidance, joint limits, and inter-arm constraints. The precision-oriented inverse dynamics model introduces practical architectural tweaks, such as arm-decoupled estimation and a direction-aware decoder, which demonstrably improve stability and accuracy. Finally, the real-robot replay results show that the learned embodiment prior can be executed reliably, suggesting the overall framework has strong potential as a practical foundation for scalable, modular robot learning."}, "weaknesses": {"value": "The paper’s main weaknesses lie in its limited empirical validation and unclear justification of core design choices. While the motivation is compelling, the work never actually demonstrates that separating semantics from feasibility improves transfer — all experiments are confined to a single robot, with no cross-task, cross-embodiment, or cross-view evaluations. The decision to use reinforcement learning for deterministic action generation is conceptually weak and lacks comparison against straightforward baselines like inverse kinematics or behavior cloning with physical constraints. The proposed Direction-Aware Decoder adds engineering complexity without clear theoretical grounding or fine-grained ablations to prove necessity. Moreover, the probabilistic decomposition introduced early in the method is never operationalized in training, leaving a gap between the stated formulation and the implemented model. Overall, the paper presents a strong motivation but delivers mostly incremental engineering under a broader conceptual narrative that remains unverified."}, "questions": {"value": "1. Why is reinforcement learning needed for deterministic action mapping, and what concrete advantages does it offer over a straightforward inverse-kinematics or behaviour-cloning approach with safety and collision constraints?\n2. Deterministic vs. probabilistic formulation: The paper presents a probabilistic factorization suggesting a distributional treatment of actions, but the actual system predicts a single deterministic joint configuration. How is this formulation relevant if the model never models uncertainty or multiple feasible actions? Since the “world model” is deterministic, how does this help separate “what to do” from “what is physically feasible,” and what additional insight or capability does this probabilistic claim really provide?\n3. How does the model handle camera-view and embodiment differences, given that the state representation is trained purely in the 2D image domain—does it rely on multi-view training, 3D understanding, or re-collection for new viewpoints?\n4. What evidence supports that separating feasibility from task semantics improves transfer, and can the embodiment model generalize across unseen tasks or robot morphologies without retraining?\n5. What is the real contribution of the Direction-Aware Decoder (DAD)? The paper shows marginal experiments but lacks fine-grained ablation; if it is mainly an engineering tweak, why treat it as a core architectural innovation?\n6. What concrete metrics and comparisons demonstrate that the RL-generated dataset is superior (in workspace coverage, safety rate, or diversity) to simpler IK-based or rule-based data generation pipelines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "P1Jp44sTMB", "forum": "BFiK4TVYeq", "replyto": "BFiK4TVYeq", "signatures": ["ICLR.cc/2026/Conference/Submission19525/Reviewer_kAye"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19525/Reviewer_kAye"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761075203299, "cdate": 1761075203299, "tmdate": 1762931415004, "mdate": 1762931415004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a novel method to collect random exploration data to learn good representation for the bi-manual manipulation tasks. THe method use a biased sampling stragtegy for exploration and learns a good representation via inverse dynamics model. Then the model is used to learn downstream imitation learning policy. Experiments show better performance than learning from scratch with pure teleoperation data and previous approach."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The method allows random exploration without direct human supervision, which relax the requirement on human demonstration."}, "weaknesses": {"value": "1. The tasks are not safety critical. The method is hard to generalize to general-purpose tasks, like ones involving safety concerns. Random exploration is not applicable.\n2. The writing is poor. The beginning of the paper involves too much distraction of mathematical formulation. It is more straightforward to describe the method in an intuitive way.\n3. No ablation on the model design. Why all components are needed?"}, "questions": {"value": "1. How's each component contribute to the final performance? How does baselines implemented? Do they similar architecture?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VsvJrIZtDH", "forum": "BFiK4TVYeq", "replyto": "BFiK4TVYeq", "signatures": ["ICLR.cc/2026/Conference/Submission19525/Reviewer_oDT4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19525/Reviewer_oDT4"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761962730469, "cdate": 1761962730469, "tmdate": 1762931414612, "mdate": 1762931414612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces an embodiment modeling framework that learns a vision-to-action mapping directly from large-scale, automatically collected trajectories. It employs an RL-based explorer to sample EEF targets across the 3D workspace and convert them into feasible joint configurations, creating a large dataset. The inverse dynamics model trained on this data maps visual observations to joint positions with good precision, aided by an arm-decoupled architecture and a Direction-Aware Decoder. The system can then pair this embodiment model with high-level policies such as video-generation or VLA models, achieving real-world bimanual manipulation success while requiring no human teleoperation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a large scale dataset by RL-based exploration, and then recording 610k safe, diverse image-action pairs in 10 hours, which is claimed to be 30× faster than human teleoperation.\n\nThe quantitative results for real-world validation looks strong, with high replay success, the demo video also shows performance, though with some failure cases.\n\nThe paper presents detailed implementation and hyperparameter reporting."}, "weaknesses": {"value": "The paper does not provide comparison when replacing learned inverse dynamics model (IDM) with traditional collision-free inverse kinematics (IK). most baselines are vision encoders (ResNet or DINOv2) rather than control pipelines, so the benchmark scope is narrow. Without such an ablation, it is unclear whether the learned model actually outperforms or simply replicates IK performance under noise and multi-arm constraints, since IK is guaranteed to be generalizable but a learned model is not\n\nVariables are introduced abruptly in introduction without clear linkage to the system. The overall abstraction and introduction could better define these terms and explicitly describe the model’s input and output, integrating this explanation with Figure 1 to clarify the pipeline flow.\n\nThe cross embodiment experiment is limited given the paper claims embodiment-agnostic modeling\n\nArms overlap with objects or arms or move out of frame -- conditions that are common in cluttered manipulation scenarios, which the model seems not be able to address."}, "questions": {"value": "It seems AnyPos depends only on URDF/kinematics and can be “replayed” for new viewpoints. How robust is the IDM to camera shifts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4z3jR1mjFF", "forum": "BFiK4TVYeq", "replyto": "BFiK4TVYeq", "signatures": ["ICLR.cc/2026/Conference/Submission19525/Reviewer_dmk8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19525/Reviewer_dmk8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974974413, "cdate": 1761974974413, "tmdate": 1762931414232, "mdate": 1762931414232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AnyPos, a framework for task-agnostic embodiment modeling in robotic manipulation. It addresses the critical challenges of data scarcity, task-specificity, and poor cross-platform generalization in robot learning. The core innovation lies in decoupling the learning of physically feasible actions from high-level task semantics. The paper achieve this through a two-stage pipeline: First, an automated, safety-aware exploration process collects a dataset of diverse, feasible robot trajectories without human teleoperation. Second, an inverse dynamics model is trained on this data, employing arm-decoupled estimation and a direction-aware decoder to achieve high-precision action prediction robust to distribution shifts. The resulting embodiment model serves as a reusable \"motion prior\" that can be seamlessly coupled with various high-level policy models. Experiments show a good improvement in action prediction accuracy and real-world task success rates over strong baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1) The paper is well written, the proposed pipeline is simple and easy to follow.\n\nS2) The automated data collection framework demonstrates a highly effective strategy for generating large-scale robotic datasets.\n\nS3) The design of arm-decoupled estimation and the direction-aware decoder directly addresses the challenges of high-dimensional action spaces and visual ambiguity in task-agnostic data. These components are empirically shown to be critical for achieving the high-precision action prediction required for real-world deployment."}, "weaknesses": {"value": "W1) I concerns the practical deployment and generalization of the automated data collection framework. My primary question revolves around the necessity and process of training the RL-based projection policy (`f_RL`). To clarify its scope and limitations: when encountering a new physical scene with different objects and layouts, must a new RL policy be trained from scratch in a simulation that explicitly reconstructs that specific bounded workspace volume? Furthermore, was a single, universal RL policy used to collect all the task-agnostic data for the diverse tasks in the paper, or were multiple scene-specific policies required? Ultimately, I seek to understand the inherent limitations of this approach, specifically, how a policy trained on one scene is expected to perform when deployed in a novel, unseen environment without retraining, particularly regarding its ability to avoid collisions and maintain feasibility.\n\nW2) While the paper's core contribution lies in using improved data to train a more robust inverse dynamics model, the proposed \"task-gnostic embodiment modeling\" framework ultimately relies on a pipeline combining a video generation model (VGM) with this learned model. This architecture appears to have limited practical utility due to its inherent susceptibility to error propagation. The system's performance is heavily contingent on the quality of the generated videos, where any inaccuracies in the predicted future frames are directly translated into action errors by the inverse dynamics model. A shortcoming of the work is the lack of a detailed analysis quantifying this compounding error and visualizing the pipeline's robustness."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QW1az7vUPR", "forum": "BFiK4TVYeq", "replyto": "BFiK4TVYeq", "signatures": ["ICLR.cc/2026/Conference/Submission19525/Reviewer_EEKr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19525/Reviewer_EEKr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19525/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999343849, "cdate": 1761999343849, "tmdate": 1762931413752, "mdate": 1762931413752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}