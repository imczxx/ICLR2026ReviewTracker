{"id": "9nxtfO6gOu", "number": 3972, "cdate": 1757577184075, "mdate": 1763750753346, "content": {"title": "Disentangling Multimodal Knowledge Preservation and Editing via Low-rank Adaptation", "abstract": "Knowledge editing facilitates precise and targeted updates in Large Language Models (LLMs) and Multimodal Models (LMMs) without the need for full retraining. Although existing editing methods achieve remarkable performance in textual modality, they still struggle to simultaneously preserve pre-trained knowledge and generalize new knowledge in intricate multimodal contexts. To address this challenge, we propose ELoRA, a novel solution that disentangles the conflicting editing objectives. Specifically, ELoRA decomposes the standard Low-Rank Adaptation (LoRA) update into two complementary subspaces: (1) a null space aligned with preserved knowledge, constructed via multimodal initialization to maintain the model's general capabilities, and (2) a knowledge space extracted from the model's internal states to multimodal perturbations, capturing invariant semantics of updates. Extensive experiments on various LMMs, including LLaVA-v1.5-7B, Qwen2.5-VL-7B, and Phi-4-multimodal, show that ELoRA outperforms most LoRA-based methods by an average of 14.2% accuracy across three metrics: reliability, generality, and locality under rigorous LLM-as-a-Judge evaluation, which demonstrates that ELoRA can achieve superior real-world editing quality.", "tldr": "This work disentangles knowledge preservation and editing in parameter-efficient fine-tuning for multimodal large language models.", "keywords": ["Knowledge editing", "Multimodal large language models", "Parameter-efficient fine-tuning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85222d37753a22dfa16c51734a209d878950ff3a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ELoRA, a LoRA-based framework that decomposes the parameter update space into a null space for preserving existing knowledge and a knowledge space for injecting new information, aiming to balance locality and generality in multimodal editing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clearly written and well-organized, with fluent language that make the presentation easy to follow."}, "weaknesses": {"value": "1.\tThe authors claim that directly leveraging LoRA for multimodal KE faces challenges, including low locality and low generality. However, these two challenges do not appear to be specific to multimodal KE, and the paper does not clearly differentiate them from those already existing in unimodal KE.\n2.\tThe title emphasizes mitigating the challenges of multimodal knowledge editing, yet the proposed method does not seem to provide further insights or methodological innovations from a multimodal perspective.\n3.\tThe proposed null space idea is highly related to AlphaEdit [1], and the initialization strategy also resembles that of CorDA [2], which substantially reduces the method’s novelty. The authors should clarify the differences and additional contributions compared to these two methods.\n4.\tThe authors introduce Gaussian noise and other perturbations to construct the knowledge space, but the robustness of this approach is questionable. Could such perturbations risk losing important information or even induce additional hallucinations?\n5.\tIn Fig. 1 (a), the colors of the different lines are too similar, making them difficult to distinguish.\n\n[1] AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models\n\n[2] CorDA: Context-Oriented Decomposition Adaptation of Large Language Models for Task-Aware Parameter-Efficient Fine-tuning"}, "questions": {"value": "1.\tAs noted in the weaknesses, could the authors elaborate on the differences and unique contributions of their method compared with AlphaEdit, LoRA-Null, and CorDA?\n2.\tWhy does AlphaEdit perform significantly worse on the MMKE-Bench dataset? Could the authors provide an analysis or explanation for this phenomenon?\n3.\tSince the paper emphasizes multimodal knowledge editing, could the authors clarify which parts of the proposed method are explicitly designed to handle multimodal data or interactions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CPyMYRSwuY", "forum": "9nxtfO6gOu", "replyto": "9nxtfO6gOu", "signatures": ["ICLR.cc/2026/Conference/Submission3972/Reviewer_92Un"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3972/Reviewer_92Un"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413955166, "cdate": 1761413955166, "tmdate": 1762917119077, "mdate": 1762917119077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ELoRA, which achieves precise and targeted updates for multimodal models. The method projects the A matrix in LoRA to a null space aligned with preserved knowledge, and projects the B matrix to a knowledge space extracted from perturbed internal states, in order to maintain pre-trained knowledge and generalize new knowledge during editing."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a new editing method ELoRA for multimodal models, addressing the challenge of simultaneously preserving pre-trained knowledge and generalizing new knowledge during editing.\n- The authors considered the problem that using a single edit sample can easily cause overfitting, and solved this issue through data augmentation by adding Gaussian noise to the samples.\n- The authors effectively combined the advantages of editing methods and LoRA methods."}, "weaknesses": {"value": "- ELoRA successfully transferred the application of null space from editing methods to LoRA, but I think this still seems like a combination of two methods, with somewhat limited innovation.\n- The paper lacks introduction to some experimental settings, such as what r is used in ELoRA and which module in the 7th layer is the object being edited.\n- The paper lacks discussion on the editing targets. As a LoRA-like method, ELoRA should be applicable to every module in the model (FFN or Attention), but the paper does not discuss whether there would be differences when various modules serve as editing targets."}, "questions": {"value": "- What r is used in ELoRA?\n- Which module is the object being edited?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "13sxiSnqqd", "forum": "9nxtfO6gOu", "replyto": "9nxtfO6gOu", "signatures": ["ICLR.cc/2026/Conference/Submission3972/Reviewer_og9B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3972/Reviewer_og9B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761650850338, "cdate": 1761650850338, "tmdate": 1762917118851, "mdate": 1762917118851, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ELoRA, a novel knowledge editing framework that enables precise and generalizable updates. The method addresses a key challenge that existing approaches struggle to balance knowledge preservation and new knowledge generalization in multimodal contexts. Build upon LoRA, ELoRA decomposes LoRA updates into two subspaces: (1) a null space for preserving knowledge, and (2) a knowledge space for promoting generalizable updates. ELoRA achieves both high locality and high generality with this subspace decomposition. Experiments demonstrate consistent gains over standard LoRA-based methods across reliability, generality, and locality. Overall, this paper makes a technically sound and empirically validated contribution by providing a framework that balances knowledge preservation and semantic generalization in multimodal knowledge editing."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "+ The combination of subspace decomposition and LoRA fine-tuning is creative in multimodal knowledge editing. \n+ The theoretical analysis of null space and knowledge space constructions are well-motivated, supported by clear mathematical formulations. The experiments are comprehensive, covering both triplet-structured knowledge and free-form real-world multimodal knowledge. The performance gains are consistent across multiple models.\n+ The paper is well-organized and clearly written. The figures are well-designed.\n+ The work is impactful for both the knowledge editing and parameter-efficient fine-tuning. It offers a general framework to update multimodal models."}, "weaknesses": {"value": "+ While the authors claim that the target knowledge is encoded within a low-dimensional, invariant manifold in the model's internal representations, this assumption appears to be empirically motivated rather than theoretically or experimentally substantiated. This hypothesis is conceptually appealing but lacks rigorous analytical or empirical verification.\n+ The paper acknowledges that constructing the projection matrix for the knowledge space requires approximately 45 seconds per edit when using 511 perturbed samples, which is a nontrivial cost for real-time or batch editing applications. Although the authors claim that this cost can be linearly reduced by decreasing the number of perturbed samples, the paper does not analyze how the number of perturbed samples affects editing performance, making the efficiency-performance trade-off unclear.\n+ The authors highlight the necessity of applying separate perturbations for visual and textual modalities; however, the paper does not clearly specify how the covariance matrix from the two modalities are fused or weighted when constructing the knowledge space. Is the overall covariance matrix obtained through direct concatenation or another aggregation strategy? If so, how does the method ensure that the fusion process balances different modalities to prevent one modality from dominating the overall subspace representation?\n+ The paper does not clarify how the text-image pairs used to construct the null space are selected. Since the null space is intended to represent preserved knowledge, the sampling strategy could affect its coverage. Without specifying selection criteria, or sensitivity analysis, it is difficult to assess the stability of the null space.\n+ The paper reports locality scores but lacks an analysis of the edited models' downstream task performance, which would help quantify the broader impact of editing on the models' general capabilities."}, "questions": {"value": "Please see the above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kKW1FPsnRZ", "forum": "9nxtfO6gOu", "replyto": "9nxtfO6gOu", "signatures": ["ICLR.cc/2026/Conference/Submission3972/Reviewer_QMCg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3972/Reviewer_QMCg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877583741, "cdate": 1761877583741, "tmdate": 1762917118599, "mdate": 1762917118599, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose ELoRA, a novel solution that disentangles the conflicting editing objectives. Specifically, ELoRA decomposes the standard Low-Rank Adaptation (LoRA) update into two complementary subspaces: (1) a null space aligned with preserved knowledge, constructed via multimodal initialization to maintain the model’s general capabilities, and (2) a knowledge space extracted from the model’s internal states to multimodal perturbations, capturing invariant semantics of updates."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose ELoRA, a novel solution that disentangles the conflicting editing objectives. Specifically, ELoRA decomposes the standard Low-Rank Adaptation (LoRA) update into two complementary subspaces: (1) a null space aligned with preserved knowledge, constructed via multimodal initialization to maintain the model’s general capabilities, and (2) a knowledge space extracted from the model’s internal states to multimodal perturbations, capturing invariant semantics of updates.\n\n2.  The authors conduct extensive experiments on various LMMs, including LLaVAv1.5-7B, Qwen2.5-VL-7B, and Phi-4-multimodal, show that ELoRA outperforms most LoRA-based methods by an average of 14.2% accuracy across three metrics:"}, "weaknesses": {"value": "1. The construction of the knowledge space is contingent on several key hyperparameters: the window length for visual masking, the noise variance (σ) for text, the similarity threshold, and the number of perturbed samples (B). Figure 3 and 4a show that performance is sensitive to these choices. However, the process for selecting the final values used in the main experiments (e.g., why a specific θ was chosen) is not adequately justified. A more systematic analysis or a principled strategy for setting these parameters is needed to ensure reproducibility and robustness.\n\n2.​​ The perturbation strategies seem somewhat arbitrary. The paper would benefit from a deeper discussion on whythese specific perturbations are suitable for extracting \"invariant semantics.\" Are there more principled, modality-specific augmentation techniques that could be more effective?\n\n3. Some widely-used baselines, such as ROME、MEMIT, should be compared"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "x8jwMfrK1N", "forum": "9nxtfO6gOu", "replyto": "9nxtfO6gOu", "signatures": ["ICLR.cc/2026/Conference/Submission3972/Reviewer_56no"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3972/Reviewer_56no"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3972/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965389900, "cdate": 1761965389900, "tmdate": 1762917118372, "mdate": 1762917118372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}