{"id": "ppQWp8yrm7", "number": 2456, "cdate": 1757094372455, "mdate": 1759898146977, "content": {"title": "Reconstruction Alignment Improves Unified Multimodal Models", "abstract": "Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture.\nHowever, conventional training relies on image–text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details, even when they use hundreds of words to describe a simple image. We introduce **Reconstruction Alignment (RecA)**, a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense “text prompts,” providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73 → 0.90) and DPGBench (80.93 → 88.15), while also boosting editing benchmarks (ImgEdit 3.38 → 3.75, GEdit 6.94 → 7.27). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs.", "tldr": "We present Reconstruction Alignment (RecA), a resource-efficient post-training method that improves unified multimodal models by leveraging visual understanding encoder embeddings as dense “text prompts”.", "keywords": ["Unified Multimodal Models; Image Generation; Image Editing; Visual Understanding"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1239087c361925657ea54bca75b607881d495ad1.pdf", "supplementary_material": "/attachment/1cb8fa741fb49f2bfc83d689c60f81bef51f709f.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Reconstruction Alignment (RecA), a lightweight post-training framework designed to improve semantic consistency between the visual understanding and visual generation modules of unified multimodal models (UMMs). The key idea is to use the embeddings from the vision encoder as dense semantic prompts for self-supervised image reconstruction. By enforcing a reconstruction loss, the model explicitly learns to map semantic embeddings to visual outputs, thereby refining the alignment between understanding and generation. RecA is model-agnostic and can be applied to multiple architectures. Extensive experiments on GenEval, DPGBench, and multiple image editing benchmarks demonstrate consistent improvements with minimal computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed reconstruction alignment is conceptually simple yet effective. By reusing visual embeddings as dense prompts, it provides a self-supervised approach to enhance multimodal coherence without relying on additional labeled or distilled data. Across multiple UMM architectures (e.g., Harmon and Show-o), RecA consistently improves the fidelity of text-to-image generation and editing. Furthermore, RecA demonstrates practical efficiency, requiring only 27 A100 GPU hours for deployment."}, "weaknesses": {"value": "- Unified multimodal models (UMMs) aim to integrate both visual understanding and generation within a single architecture. However, RecA is based on a reconstruction task, where the training (I2I) and testing (T2I or I2T) processes differ. It is expected to further clarify how RecA benefits T2I generation tasks as well as visual understanding tasks, especially considering that visual understanding requires both detailed and high-level comprehension.\n\n- Although the work claims to enhance unified multimodal alignment, the quantitative results primarily focus on generation tasks (T2I and editing). Direct evaluation on understanding tasks (e.g., VQA) is needed to substantiate the claim of improving the \"unified\" aspect of UMMs, as the current results do not fully address this dimension.\n\n- The mechanism by which semantic embeddings improve alignment is primarily intuitive. A deeper representation analysis (e.g., embedding similarity before/after RecA) would strengthen the causal understanding of why RecA works.\n\n- RecA primarily operates as a post-training technique, which inherently limits its scalability.\n\n- Some important technical details are missing. For instance, the paper does not clearly specify where and how the visual embedding is integrated into the model. Additionally, it remains unclear whether hyperparameter tuning is required for each model, which limits the reproducibility of the results."}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m11GHagrRH", "forum": "ppQWp8yrm7", "replyto": "ppQWp8yrm7", "signatures": ["ICLR.cc/2026/Conference/Submission2456/Reviewer_uKdX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2456/Reviewer_uKdX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761196020445, "cdate": 1761196020445, "tmdate": 1762916244103, "mdate": 1762916244103, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reconstruction Alignment (RecA), a simple yet effective post-training method for Unified Multimodal Models (UMMs) that unifies understanding and generation. The key idea is to condition a UMM on its own visual understanding encoder embeddings and optimize it to reconstruct the input image using a self-supervised reconstruction loss."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Simple yet effective. The key idea is very simple, but surprisingly works well. It yields consistent improvements in image generation and editing performance while being computationally efficient (27 GPU hours). \n\n2. Broad generality: RecA is architecture-agnostic and demonstrated across various UMM families (autoregressive, masked-autoregressive, and diffusion-based). Works across AR, MAR, and AR+Diff UMMs with consistent gains.\n\n3. Good experimental results. Experiments show that a 1.5B-parameter UMM post-trained with RecA outperforms much larger models, achieving GenEval 0.90 and DPGBench 88.15, and improves editing benchmarks such as ImgEdit (3.38→3.75) and GEdit (6.94→7.27).\n\n4. Clarity and reproducibility: Strong visualizations, clear methodology, and detailed appendices."}, "weaknesses": {"value": "1. Limited theoretical grounding: The paper would benefit from more analysis on why reconstruction alignment so effectively bridges understanding–generation gaps (e.g., mutual information or feature alignment metrics).\n\n2. What are the failure cases—does RecA ever degrade diversity or overfit to reconstruction-style artifacts?"}, "questions": {"value": "1. Could RecA be extended to cross-modal reconstruction (e.g., text→image→text) for deeper alignment?\n\n2. Could combining RecA with RL-based alignment (e.g., DPO/GRPO) further enhance reasoning alignment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FR6TWv179M", "forum": "ppQWp8yrm7", "replyto": "ppQWp8yrm7", "signatures": ["ICLR.cc/2026/Conference/Submission2456/Reviewer_U3Cq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2456/Reviewer_U3Cq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837525197, "cdate": 1761837525197, "tmdate": 1762916243960, "mdate": 1762916243960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Reconstruction Alignment (RecA), a simple yet remarkably effective post-training technique for unified multimodal models (UMMs). RecA addresses the limitation of sparse text-caption supervision by leveraging dense supervision from visual encoder embeddings, training the model to reconstruct the original image. This process realigns the model's understanding and generation capabilities without requiring any additional labeled data. Extensive experiments across diverse UMM architectures (autoregressive, hybrid, diffusion-based) demonstrate consistent performance gains on both generation (GenEval, DPGBench) and editing (ImgEdit, GEdit) benchmarks. Notably, the method is computationally lightweight."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed method is characterized by its simplicity and broad applicability, ensuring easy implementation with low computational overhead. It demonstrates strong empirical performance, evinced by consistent improvements across four distinct model families. The study maintains rigorous experimental standards, including fair comparisons and thorough ablation studies (e.g., SFT vs. RecA, encoder choice, resolution). The approach has high practical relevance as a plug-and-play component for future UMM pipelines, and its benefits are clearly illustrated through intuitive figures and examples"}, "weaknesses": {"value": "1. The heavy reliance on simple benchmarks (GenEval and DPGBench) raises questions about true generalization. The absence of tests for real-world prompt fidelity and compositional reasoning"}, "questions": {"value": "1. Could the authors provide an analysis, perhaps using feature attribution methods, to identify where RecA contributes most significantly, such as spatial layout, geometry, or fine-grained attributes?\n2. Could applying RecA iteratively—through multiple cycles of reconstruction—lead to further improvements in the output?\n3. How does RecA perform when given open-ended or abstract prompts for which no concrete visual reference exists?\n4. Is there a quantifiable trade-off between the fidelity of the reconstructions and the diversity of the generated samples?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tqo7Aqj0Gq", "forum": "ppQWp8yrm7", "replyto": "ppQWp8yrm7", "signatures": ["ICLR.cc/2026/Conference/Submission2456/Reviewer_xRHK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2456/Reviewer_xRHK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995118106, "cdate": 1761995118106, "tmdate": 1762916243754, "mdate": 1762916243754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identified an important misalignment and information gap during the training of image generation (text to image): text input provides only sparse visual information while the model learns to fine-grained visual details. The sparsity holds even when the prompt is completed with hundreds of words. In the context of a unified multimodal model (UMM), the mapping from visual features to language space gives rise to a new source of dense supervision for image generation, i.e., the semantic embeddings of the target image itself. Built on this insight, the authors proposed a post-training technique named Reconstruction Alignment (RecA), where images are abstracted by a visual encoder and mapped to the input space of LLM, then the generation part of the UMM reconstructs the input image. This enables the learning of structural and low-level details of images that may be rare in image captions. RecA was experimented on various types of UMM and brought consistent gains for both image generation and editing."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The core idea of the paper, i.e, dense conditioning for image generation, is intuitive and well-motivated. It has been a long-standing issue in multimodal learning that generating visual details is much easier than perceiving them. The authors' perspective on the information density of condition signals is insightful.\n\n2. And the solution, a simple reconstructive supervision, is widely applicable to various UMM paradigms and is effective across different benchmarks. Particularly, the results on GenEval are exceptional, without relying on gpt-distilled data samples.\n\n3. The paper is well written and easy to follow."}, "weaknesses": {"value": "1. The proposed method seems limited to the post-training stage. It is unclear whether ReCA will also benefit the pretraining of image generation models or unified models.\n\n2. More reconstruction results are expected for all the models studied. I believe the reviewers and readers would be interested in how well the images can be reconstructed.\n\n3. It is unclear how the performance would be affected by the training images, e.g., the scale of the data, visual quality, real data v.s., synthetic data.\n\n4. A factual error: Show-o is not an AR model for image generation. Instead, it is a mix of AR for understanding and discrete diffusion for generation."}, "questions": {"value": "1. The improvement on Harmon is exceptionally larger than on other models. Can the authors provide more insight? Would it be due to the shared visual encoder or because Harmon mainly uses short captions for training, thus leaving a large space for improvement with additional dense supervision?\n\n2. The performance gains on GenEval are crystal clear. However, GenEval prompts are all short, and the evaluation mainly focuses on high-level image understanding like object detection and counting. Why would the dense supervision be so effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0mxMkRZybz", "forum": "ppQWp8yrm7", "replyto": "ppQWp8yrm7", "signatures": ["ICLR.cc/2026/Conference/Submission2456/Reviewer_ccV6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2456/Reviewer_ccV6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173393579, "cdate": 1762173393579, "tmdate": 1762916243576, "mdate": 1762916243576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}