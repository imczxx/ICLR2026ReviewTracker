{"id": "JB7QNQ1a9U", "number": 5128, "cdate": 1757852266939, "mdate": 1759897992945, "content": {"title": "MSEarth: A Multimodal Scientific Dataset and Benchmark for Phenomena Uncovering in Earth Science", "abstract": "The rapid advancement of multimodal large language models (MLLMs) has unlocked new opportunities to tackle complex scientific challenges. Despite this progress, their application in addressing earth science problems, especially at the graduate level, remains underexplored. A significant barrier is the absence of benchmarks that capture the depth and contextual complexity of geoscientific reasoning. Current benchmarks often rely on synthetic datasets or simplistic figure-caption pairs, which do not adequately reflect the intricate reasoning and domain-specific insights required for real-world scientific applications. To address these gaps, we introduce MSEarth, a multimodal scientific benchmark curated from high-quality, open-access scientific publications. MSEarth encompasses the five major spheres of Earth science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere, featuring over 289K figures with refined captions. These captions are crafted from the original figure captions and enriched with discussions and reasoning from the papers, ensuring the benchmark captures the nuanced reasoning and knowledge-intensive content essential for advanced scientific tasks. MSEarth supports a variety of tasks, including scientific figure captioning, multiple choice questions, and open-ended reasoning challenges. By bridging the gap in graduate-level benchmarks, MSEarth provides a scalable and high-fidelity resource to enhance the development and evaluation of MLLMs in scientific reasoning. The benchmark is publicly available to foster further research and innovation in this field.", "tldr": "", "keywords": ["Earth science", "Multimodal Large Language Models", "Dataset", "Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d51d05a6789074aa396f52afb2c18217b9e7ac5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces MSEarth, a large-scale, multimodal dataset and benchmark designed to evaluate graduate-level reasoning in Earth science. Curated from over 64,000 open-access scientific publications, it features over 289K figures across the five major spheres of Earth science. The central contribution is the concept of \"refined captions,\" which programmatically enrich the original, brief figure captions with deep contextual information, analysis, and reasoning extracted from the main body of the source paper. Based on this enriched data, the authors developed a semi-automated pipeline using a multi-agent voting system to generate and filter a vast collection of VQA tasks, including scientific figure captioning, multiple-choice questions, and open-ended questions. Extensive evaluations on a wide range of state-of-the-art Multimodal Large Language Models (MLLMs) reveal a significant performance gap between tasks requiring simple perception and those demanding complex, domain-specific reasoning, demonstrating the benchmark's difficulty and relevance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The \"refined caption\" methodology is a major strength. It moves beyond the limitations of simplistic figure-caption pairs by integrating deep contextual information. \n\n2. The paper introduces a robust and scalable pipeline for benchmark creation. The use of a multi-agent voting system combined with a phased filtering process effectively automates the generation of high-quality, challenging questions, striking an excellent balance between scale and rigor."}, "weaknesses": {"value": "1. While the authors commendably introduce LLM-based metrics like Cap-Eval and OE-Eval, the evaluation tables for captioning and open-ended QA (Tables 3 and 4) still heavily feature traditional lexical metrics like ROUGE, BLEU, and METEOR. As the paper's own results show, these n-gram-based metrics exhibit minimal variance across models and fail to capture the significant performance differences observed in the accuracy-based MCQ task. They are fundamentally ill-suited for evaluating the factual correctness and scientific nuance required by this benchmark, potentially masking the true capabilities of the models and reducing the conclusiveness of these specific results.\n\n2.  Given the rapid pace of MLLM development, the inclusion of some of the most recent and powerful open-source models (e.g., Mimo-VL, GLM-4.5V) would make the comparative analysis more conclusive and compelling.\n\n3. The manuscript would benefit from a final round of careful proofreading to address minor inconsistencies in formatting and capitalization (e.g., \"gemini-2.5-pro\" in line 1538)."}, "questions": {"value": "1. The pipeline for generating MSEarth seems highly generalizable. What challenges do you foresee in applying this framework to other scientific domains, such as molecular biology or particle physics, where figures and data representations follow vastly different conventions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "This paper introduces MSEarth, a large-scale, multimodal dataset and benchmark designed to evaluate graduate-level reasoning in Earth science. Curated from over 64,000 open-access scientific publications, it features over 289K figures across the five major spheres of Earth science. The central contribution is the concept of \"refined captions,\" which programmatically enrich the original, brief figure captions with deep contextual information, analysis, and reasoning extracted from the main body of the source paper. Based on this enriched data, the authors developed a semi-automated pipeline using a multi-agent voting system to generate and filter a vast collection of VQA tasks, including scientific figure captioning, multiple-choice questions, and open-ended questions. Extensive evaluations on a wide range of state-of-the-art Multimodal Large Language Models (MLLMs) reveal a significant performance gap between tasks requiring simple perception and those demanding complex, domain-specific reasoning, demonstrating the benchmark's difficulty and relevance."}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "cIEwGUKQ86", "forum": "JB7QNQ1a9U", "replyto": "JB7QNQ1a9U", "signatures": ["ICLR.cc/2026/Conference/Submission5128/Reviewer_ZUa8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5128/Reviewer_ZUa8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643088745, "cdate": 1761643088745, "tmdate": 1762917898819, "mdate": 1762917898819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper contributes a large-scale human-reviewed test & llm-generated training set for Multimodal understanding of Earth Science. The authors first parse the Multimodal content from Earth Science and then leverage various LLMs through many stages to build a large-scale dataset comprising QA, captioning, and open-ended generation question-answer pairs. Then they leverage human experts to label ~1k of them as a golden test set. They proved that models trained on their training set have better results compared with the original models. They also show that the test set is even challenging for the current most advanced commercial LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The dataset for earth science is potentially useful to this community, as well as improving the general capabilities of MLLMs.\n\n2. The constructed q-a pairs was proved to help improve the capabilities of MLLMs.\n\n3. The human-expert labeled test set was challenging for current models and serves as a potential testbed for current AI's understanding of Earth science."}, "weaknesses": {"value": "1. The authors mentioned that \"All papers used were obtained from OpenDataLab\" so this paper actually does not provide any new data sources. And the contribution lies in post-processing those data using the combination of various LLMs to ensure quality, though the quality is questionable.\n\n2. In my opinion, the most valuable part of this dataset is the human-labeled Open-Ended QA test set. However, the current results show that the majority of the open-sourced and Proprietary LLMs demonstrate similar results, despite their sizes or providers. This makes me question the real quality of this test set since it's expected to see significant differences. For example, the BERTSCORE is almost always 82~83, which explains nothing.\n\n3. There have already been many Captioning tasks in previous work, though the authors emphasize that their Captioning requires additional context from the paper; however, this is not new."}, "questions": {"value": "1. In Table 2&3&4, why do the results from Proprietary Models even underperform (like gpt4o) compared with open-sourced models (e.g. Qwen2.5-VL-32B)? Does this mean the open-sourced model is really better? Or does this mean there is some quality issue within the dataset?\n\n2. In Table 2&3&4, there is almost little difference between results on various model sizes, e.g. DeepSeek-VL2, Qwen2.5-VL-32B & Qwen2.5-VL-72B. This is unintuitive since the results have almost no change with increasing model size, which might imply inherent flaws in the datasets.\n\n3. Do you have better evals for open-ended QA? ROUGEL or BLEU, etc seem not very suitable if all the results are very close to each other."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qyI84gSzG6", "forum": "JB7QNQ1a9U", "replyto": "JB7QNQ1a9U", "signatures": ["ICLR.cc/2026/Conference/Submission5128/Reviewer_A2sD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5128/Reviewer_A2sD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965824926, "cdate": 1761965824926, "tmdate": 1762917898200, "mdate": 1762917898200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MSEarth, a large-scale multimodal scientific benchmark for evaluating and developing multimodal large language models (MLLMs) in the field of Earth sciences. The authors argue that current benchmarks fail to capture the depth, complexity, and reasoning required at the graduate academic level. MSEarth includes over 289K scientific figures across all five Earth system spheres (atmosphere, cryosphere, hydrosphere, lithosphere, biosphere), paired with carefully refined and context-enriched captions. The benchmark supports several tasks, including figure captioning, multiple-choice questions, and open-ended scientific reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors propose first benchmark that targets graduate-level geoscientific multimodal reasoning.\n\n2. This benchmark is curated from high-quality, open-access scientific publications, and covers all five Earth science spheres (atmosphere, cryosphere, hydrosphere, lithosphere, biosphere)."}, "weaknesses": {"value": "1. The authors does not clearly explain how the ``ground truth'' for refined captions, QA pairs, or reasoning tasks is constructed. Although it claims that captions are enriched using discussions from the original papers, it remains ambiguous whether these are (1) directly extracted from the text, (2) rewritten by humans, or (3) generated using LLMs.\n\n\n2. The benchmark does not include human or expert performance, making it difficult to assess its true difficulty or validate the claim of “graduate-level” reasoning. Low model performance alone is not sufficient evidence of task difficulty—it may instead indicate a lack of domain-specific pretraining. Without human baselines, it remains unclear whether the benchmark genuinely requires advanced reasoning or is simply out-of-distribution for current MLLMs.\n\n\n3. Although MSEarth demonstrates a clear gap between perception and scientific reasoning, the paper does not provide a deeper analysis of why models fail or which types of reasoning are most challenging. There is no taxonomy of reasoning skills (e.g., causal inference, trend interpretation, quantitative estimation), no breakdown across Earth system domains, and no systematic error analysis. As a result, the benchmark highlights a problem but offers limited insight into the underlying mechanisms or how future models should be improved."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "egTJmhJL19", "forum": "JB7QNQ1a9U", "replyto": "JB7QNQ1a9U", "signatures": ["ICLR.cc/2026/Conference/Submission5128/Reviewer_nNLz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5128/Reviewer_nNLz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5128/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762057122099, "cdate": 1762057122099, "tmdate": 1762917897443, "mdate": 1762917897443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}