{"id": "5mMa7mC8Pl", "number": 16280, "cdate": 1758262642773, "mdate": 1759897250384, "content": {"title": "Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs", "abstract": "Accurate uncertainty quantification in large language models (LLMs) is essential for providing credible confidence estimates over their outputs. However, fine-tuned LLMs often exhibit overconfidence in uncertain predictions, which stems from their limited ability to generalize with sparse data. Existing parameter efficient fine-tuning (PEFT) uncertainty quantification methods for LLMs focus on post fine-tuning stage, and thus fail to address the core issue: limited specialization of PEFT adapters to accurately capture task-specific input-output relationships. To address these limitations, we propose Functional-Level Uncertainty Quantification for Calibrated Fine-Tuning (UQ4CT), which captures and calibrates uncertainty over the space of functions that map input prompts to outputs. We implement UQ4CT during the fine-tuning stage via a mixture-of-experts framework that hierarchically decomposes the functional space. Empirically, UQ4CT achieves over 25% reduction in Expected Calibration Error (ECE) while preserving high accuracy across five benchmarks. Even under distribution shift, UQ4CT maintains superior ECE performance with high accuracy, showcasing improved generalizability.", "tldr": "A method to quantify and calibrate uncertainty in LLMs with Mixture of Experts.", "keywords": ["Uncertainty Quantification", "Large Language Models", "Mixture of Experts", "Parameter Efficient Fine Tuning"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12c7344b14d943d02d0f2cc530a273106fbbc266.pdf", "supplementary_material": "/attachment/4c11fa3dd218fda4e8d030d75544ed042a695282.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a new method for quantifying uncertainty in large language models. The proposed method focuses on quantifying functional uncertainty using a mixture of LoRA experts to all parameter matrices during finetuning. The model is deemed to be more confident for a particular generation given an input if it assigns high probability to a single expert. The overall functional uncertainty score computed across all model trainable components is incorporated as an auxiliary loss function during fine-tuning. Overall experimental results show that the proposed approach offers better uncertainty estimation and high performance retention using a Llama-3.1 8B model across several standard datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Really nice and creative approach for efficiently calibrating LLMs during fine-tuning."}, "weaknesses": {"value": "- While I really liked the method, the experiments are limited. Only a single model size from a single family is tested. Furthermore, fine-tuning on the datasets you used in Table 2 or OBQA is not realistic (see my suggestions below). The current experimental setting does not convincingly show if the proposed method generalizes."}, "questions": {"value": "1. You should report results using at least one more model family (e.g., Qwen3, OLMO 2 or similar) and model size (e.g. 3B or larger than 8B if you have enough compute).\n\n2. You should use standard instruction tuning data (Alpaca or similar) to SFT your models instead of your current setting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YJg27ad1G9", "forum": "5mMa7mC8Pl", "replyto": "5mMa7mC8Pl", "signatures": ["ICLR.cc/2026/Conference/Submission16280/Reviewer_yiFV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16280/Reviewer_yiFV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761412057722, "cdate": 1761412057722, "tmdate": 1762926428555, "mdate": 1762926428555, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a fine-tuning approach for calibrating MixLoRA-adapted LLMs. The authors define functional uncertainty (FLU) as the predictive difference between the adapted model and its vanilla LLM counterpart. The paper then shows that this FLU can be linearly expressed by the MoE weights in the MixLoRA architecture. To calibrate this FLU score, the paper introduces a Brier-score-like calibration loss as the auxiliary loss."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The analysis presented in Fact 3.1 is insightful. The resulting equation, which linearly approximates the perturbation using $\\Delta \\alpha(h)$ and $g(h)$ (omitting super- and subscripts for simplicity), is surprising.\n2. The experiments are basically comprehensive, including both in-distribution and out-of-distribution evaluations, and the reported calibration performance appears strong.\n3. The methodology does not require modifications to the model's architecture, making it agnostic to the underlying vanilla LLM."}, "weaknesses": {"value": "1. The derivation of uncertainty in Fact 3.1, defined as the difference between the fine-tuned (MixLoRA-adapted) LLM and the vanilla LLM, is counter-intuitive. The authors should clarify which model is the subject of the uncertainty analysis. If, as understood, it is the fine-tuned LLM, a more logical definition of uncertainty would seem to involve a comparison between the MixLoRA LLM and a perturbed version of itself, rather than a comparison against the vanilla LLM.\n\n2. In Fact 3.1, $\\Delta f(x)$ is expressed using both $\\Delta \\alpha(h)$ and $g(h)$. However, $g(h)$ is omitted from the subsequent definition of FLU. The authors should justify this omission.\n\n3. The definition of FLU in Eq. (12), the calibration loss in Eq. (13), and Proposition (3.2) are highly counter-intuitive. Eq. (12) appears to model the confidence that \"Expert $i$ should be involved in the computation,\" and the calibration loss (Eq. 13) reinforces this interpretation. This, however, represents confidence, not uncertainty. This contradiction is evident from Proposition (3.2): a perfect (100% accurate) fine-tuned LLM would have optimal MoE selection, implying zero uncertainty. In contrast, the proposed formulation would result in an FLU of 1.\n\n4. The computational budget comparison with BLOB (Lines 399-402) is not straightforward. BLOB requires multiple forward passes, whereas the proposed method utilizes an MoE structure.\nGiven these fundamental architectural differences, a more rigorous and detailed complexity analysis is needed to make the comparison solid.\n\n5. Figure 2 requires polishing, and its caption is misleading. Furthermore, when referenced in the text (e.g., Lines 273-274), there is a lack of proper guidance for the reader to interpret it.\n\n6. The Introduction and Related Work sections could be strengthened by discussing other lines of LLM calibration, such as evidential methods [1], to provide a more complete background.\n\n[1] Li, Yawei, et al. \"Calibrating LLMs with Information-Theoretic Evidential Deep Learning.\" ICLR 2025."}, "questions": {"value": "Please see the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0u0NkzLXW7", "forum": "5mMa7mC8Pl", "replyto": "5mMa7mC8Pl", "signatures": ["ICLR.cc/2026/Conference/Submission16280/Reviewer_d28n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16280/Reviewer_d28n"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663689124, "cdate": 1761663689124, "tmdate": 1762926428171, "mdate": 1762926428171, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new approach to uncertainty estimation and calibration in large language models during fine-tuning. Instead of modeling uncertainty at the parameter level or requiring multiple stochastic forward passes, the authors introduce functional-level uncertainty , which quantifies uncertainty directly from the Mixture-of-Experts routing probabilities in the model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a timely and interesting topic.\n2. The proposed functional-level uncertainty formulation, leveraging MoE routing probabilities, is conceptually novel and computationally efficient."}, "weaknesses": {"value": "1. Limited application scope. The method only applies to Mixture-of-Experts (MoE) architectures and to discriminative tasks with ground-truth labels. It is not directly applicable to generative or open-ended tasks, which limits its broader relevance in LLM research.\n2. Theoretical justification is oversimplified. A more rigorous or relaxed analysis would strengthen the paper.\n3. Comparable performance to BLoB (N=10). Although the proposed method is more efficient (single-pass), its calibration and accuracy improvements over BLoB(N=10) are modest, suggesting a trade-off between novelty and practical gains."}, "questions": {"value": "1. Unclear token-level aggregation. In Eq. (12), the aggregation of routing probabilities across tokens is not well defined — it should specify whether it averages over all tokens, the [CLS] token, or the last token.\n2. Missing ablation results in main text. The ablation study on the calibration loss weight (Lines 454–458) should be moved from the appendix to the main body for better visibility, since it demonstrates the effectiveness of the calibration component."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "77NABc15Pm", "forum": "5mMa7mC8Pl", "replyto": "5mMa7mC8Pl", "signatures": ["ICLR.cc/2026/Conference/Submission16280/Reviewer_6Wcn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16280/Reviewer_6Wcn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832222848, "cdate": 1761832222848, "tmdate": 1762926427697, "mdate": 1762926427697, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes UQ4CT, which quantifies uncertainty at the functional level during fine-tuning using a Mixture-of-Experts (MoE) architecture with LoRA adapters. The method computes functional-level uncertainty (FLU) from router weights and uses a calibration loss to align FLU with predictive correctness. Experiments on five multiple-choice QA benchmarks show >25% ECE reduction while maintaining accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a novel calibration mechanism where router weights naturally encode functional-level uncertainty.\n2. Achieves calibration during training with only one forward pass at inference, unlike ensemble methods (8× slower) or BLoB(N=10) (10× slower)."}, "weaknesses": {"value": "1. All evaluated benchmarks (OBQA, ARC, BOOLQ, ClimateQA, MMLU) are classification tasks. This may limit the method's applicability, as multiple-choice QA is inherently more suitable for calibration compared to open-ended short-form or long-form QA. Can this approach be extended to open-ended QA datasets such as TriviaQA[1]?\n2. There is a lack of comparison with alternative uncertainty estimation methods, including self-consistency[2] and training-based approaches[3].\n3. Proposition 3.2 only holds at global optimum over true distribution. No analysis of: (a) convergence with non-convex MoE optimization, (b) finite sample effects, (c) conflicts between CE, Lb, and Lcal in Eq. 14. Need empirical verification that learned FLU ≈ P(correct).\n\n[1]TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\n[2]Self-Consistency Improves Chain of Thought Reasoning in Language Models\n[3]Can AI Assistants Know What They Don't Know?"}, "questions": {"value": "1. Line 108: The model name appears inconsistently as \"Llama3.1-8B\" and \"LLaMA3.1-8B.\" Please standardize the naming convention.\n2. What is the performance of the untuned LLaMA3.1-8B on these tasks? This information is necessary to distinguish whether improvements are due to better task learning or improved calibration.\n3. In Table~1, it would be clearer to present ECE and ACC on the same row under each dataset for better visual alignment and comparison."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "06TamW6uTo", "forum": "5mMa7mC8Pl", "replyto": "5mMa7mC8Pl", "signatures": ["ICLR.cc/2026/Conference/Submission16280/Reviewer_Qqiu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16280/Reviewer_Qqiu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16280/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900304645, "cdate": 1761900304645, "tmdate": 1762926427342, "mdate": 1762926427342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}