{"id": "jV4JMmh2I2", "number": 3791, "cdate": 1757523895410, "mdate": 1759898069596, "content": {"title": "Bridging Human Vision and Deep Perception with a Saccade-Fixation ROI Prior for Medical Image Segmentation", "abstract": "Automatic medical image segmentation converts subjective visual interpretation into objective, pixel-level quantitative indicators with high precision and repeatability, providing essential morphological evidence for early disease detection and surgical planning. However, current segmentation networks universally follow an \"equal-pixel\" paradigm: every spatial location consumes the same amount of parameters regardless of its semantic saliency. Consequently, a large portion of computational resources are expended on lesion-free regions, leading to unnecessary GPU and memory overhead, and increasing the risk of overlooking tiny pathological areas. Human vision solves this problem through an active saccade-fixation strategy by first performing a rapid, low-resolution saccade to localize suspicious regions, then applying high-resolution fixation only where necessary. Inspired by this mechanism, we propose SaccadeFixationNet (SF-Net), a medical image segmentation framework that integrates biologically motivated gaze behaviors into an end-to-end trainable U-shaped architecture. SF-Net consists of a Saccade–Fixation Encoder (SFE) that combines global saccadic scanning with fixation-driven feature refinement, a Fixation Connectivity Module (FCM) that generates a Gaze ROI Map by modeling inter-fixation relations, and a Gaze-MoE Decoder (GMD) that adaptively routes fixation-relevant tokens to high-capacity experts while assigning peripheral regions to lightweight experts. This design enables ROI-guided selective computation, closely mimicking the allocation of neural resources in human vision. Extensive experiments on four heterogeneous medical datasets demonstrate that our model achieves significant performance gains and substantially outperforms baselines.", "tldr": "", "keywords": ["Mechine Learning", "Medical Image Segmentation", "Saccade-Fixation Strategy"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0406852de481a4fdd8758df27421cb2a08519fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces SF-Net, a novel gaze-inspired segmentation model that uses adaptive expert allocation and uncertainty-driven attention to simulate the human saccade–fixation process. The work would benefit from stronger ablations, more precise methodological specifications, and a thorough efficiency analysis to fully validate the claims made in the paper."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed work presents a segmentation framework with biological inspiration that bridges the gap between computational design and neuroscience by imaginatively incorporating the human saccade-fixation mechanism into a U-shaped architecture. The method employs different modules which consists of the Saccade–Fixation Encoder, Fixation Connectivity Module, and Gaze-MoE Decoder. Numerous tests on 2D and 3D datasets show steady improvements over baselines."}, "weaknesses": {"value": "Major:\n1. A primary weakness of the paper is the lack of methodological clarity and correctness of equations.\n2. Lack of baseline comparisons like nnU-Net, Swin-UNETR, etc. are not done. These comparisons would benefit the paper greatly.\n3. Eq. (5) combines an aggregated relational score $\\sigma(\\Gamma(.))$ with an average over attention rows; however, dimensions and reduction axes are not given; it is not clear how $\\Gamma$ maps $\\mathbf{R}_l$ and $\\mathbf{T}_l$ to a per-token scalar.\n4. The ablations are not extensive and not clear. On 2D tasks only, the w/o DINO, w/o FCM, and Top-k routing variants are displayed. The decoupling contributions of MoE but w/o G-Map bias and w/o MoE (dense decoder) are absent.\n5. The BraTS 2025 (Pre+Post) is not clearly explained. How the BraTS Pre and Post are combined?\n6. Evidence of small lesions and boundaries is primarily anecdotal. There is no experimental evidence to support claims of improved tiny/diffuse lesion capture.  Visuals help but aren’t sufficient.\n7. The following crucial training hyperparameters are absent: $\\lambda$ for ROI loss, optimizer/schedule, precise augmentations (ranges/parameters), etc.\n\nMinor:\n1. Some parts of  Section 3.3 and 3.4 are difficult to follow because of notation errors (such as indices l, i, j, and mixing tokens vs. spatial maps).\n2. For better presentation, the visualizations would benefit from identical windowing, blinded side-by-side comparisons, and consistent color bars."}, "questions": {"value": "See weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q9D1KsOiOE", "forum": "jV4JMmh2I2", "replyto": "jV4JMmh2I2", "signatures": ["ICLR.cc/2026/Conference/Submission3791/Reviewer_NsgX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3791/Reviewer_NsgX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973167925, "cdate": 1761973167925, "tmdate": 1762917030126, "mdate": 1762917030126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors focus on reducing computation allocated for lesion-free regions. To this end, they adopt two input encoders: a Dinov3 to get global semantics, and a Tok-Kan, to get high-frequency semantics. Their segmentation results are fused into the final segmentation map."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The reported performance is better than baselines."}, "weaknesses": {"value": "1. Fusing global and local features is a common technique used for many years. There's little novelty here, except for the implemetation details (replacing CNN/ViT with KAN, etc.) In addition, Saccade and Fixation are quite unusual terms. Why not simply call them global and local? You cannot invent novelty by simply naming old concepts with new terms.\n2. It is weird that although the authors stress from the beginning that their aim is to reduce compute, no compute related metrics are reported, including FLOPs, RAM, or wall-clock time. \n3. The comparison with baselines don't consider number of params? This makes readers unable to evaluate the results fairly, since more params usually lead to better performance; we can keep adding experts to make it perform better.\n4. Minor issues: 1) line 89, the citation to tok-kan used the wrong format."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "13IrkrfBx7", "forum": "jV4JMmh2I2", "replyto": "jV4JMmh2I2", "signatures": ["ICLR.cc/2026/Conference/Submission3791/Reviewer_P9H6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3791/Reviewer_P9H6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762014944693, "cdate": 1762014944693, "tmdate": 1762917029794, "mdate": 1762917029794, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is presenting a mechanism called SaccadeFixation, to do medical image segmentation with gaze points. \nThe paper re-invents the idea of \"recognition\" and present this in conjunction with gaze location and then delineation. To be honest, I am very surprised how authors are unaware of the entire gaze-based image analysis methods out there in the literature, and how ignorant authors are to neglect them and not recognize. Beside,"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "-writing is smooth\n-figures are nice, cosmetically appealing."}, "weaknesses": {"value": "-- I would not sound be harsh however, the paper has many major drawbacks and mistakes. At best, the paper is ignoring a very large body of related works on this topic, and presenting the gaze based segmentation as a new tool, which is totally wrong and unfair to all the scientists who worked on this topic. \n\n-- First of all, \"focus on important region\" is not an innovative idea. It is a known topic in segmentation literature. Starting with A. Rozenfeld, the segmentation is considered as two connected tasks \"recognition\" and \"delineation\". Later, recognition was used for some other purpose but it means whereabout the object is. In other words, segmentation considers the object of interest location first, and then do the delineation. This can be done by mouse, gaze, or automatically. There is nothing new here in this concept. 1980s are the ones this was revealed. If you look at the first medical segmentation paper with gaze, without deep learning but image processing, I assume it should be N. Khosravan's MICCAI 2016 paper, where gaze was used to segment tissues and organs, and 3D and real-time or realistic. Later a few years ago there was GazeSAM work, and many more can be said. None of them are mentioned and authors are unaware of many other similar works.\n\n-- there is an analogy missing, and original ideas from the past are recycled with fancy names. Course to fine pyramid --> Saccade Fixation encoder, fixation connectivity module --> attention map/saliency refinement., Gaze-MoE Decoder --> token pruning.\n\n--The paper claims biological inspiration. But, they forget that saccades are rapid and ballistic eye movements, and fixations re Hugh acuity poses. SF-Net does not do any of these: gaze shifts, no temporal dynamics, no uncertainty driven exploration, etc. The presented network is static, single pass U net with multi scale features. SFE looks like a dilated CNN with downsampling. \n\n-- MoE with two experts? That is perhaps not called MoE. This looks like gated skip connection similar to ResNet EfficientNet and etc. Real MoE architectures uses more experts (8 to 128....sparse). \n\n--eye tracking related literature is coming from 1990. Not extensive. Especially last years, there are many good / strong papers in the field.\n\n--claim about \"being the first in segmentation with saccade/ fixation\" is wrong. See previous comments.\n\n--FCM is visual attention maps, authors should show how they truly separate it into two regions.\n\n--authors are missing the point that not only FLOPS but also model accuracy is increased if the object territory is known. Which is called Attention. If you are in the close vicinity of object, then the delineation is simpler. Authors failed to connect efficiency and attention operations to improved segmentation. \n\n--authors never disclose where the gaze information come from truly. It appears fully synthetic. Then, it would be necessary to do human study with gaze and see if the assumptions hold. Perhaps no, especially in radiology settings, gaze behaviors are from experts people who know how to search pathology. The text-book definition of gaze or loose attention can be learned with DL architectures, but they are not truly biologically inspired. What Fixation tokens authors ay is truly a k-means clustering on feature maps. top-k activation are projected into 2D. Gaze ROI is nothing but learned attention mask. Sacade fixation cycle is nothing but single forward pass with musicale features.\n\n--In training, there is no aux-gaze information incorporated, the entire system is dice + CE loss based optimization.\n\n-- no discussion in failure modes."}, "questions": {"value": "weaknesses section is self-contained and all the comments include questions to authors. Please treat them as they are.\n\napart from that\n-- no code, no data, no model are shared."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0Znq8pHtIG", "forum": "jV4JMmh2I2", "replyto": "jV4JMmh2I2", "signatures": ["ICLR.cc/2026/Conference/Submission3791/Reviewer_i9mF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3791/Reviewer_i9mF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3791/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762015256465, "cdate": 1762015256465, "tmdate": 1762917029007, "mdate": 1762917029007, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}