{"id": "9WtgapSaCv", "number": 5348, "cdate": 1757903196188, "mdate": 1759897980367, "content": {"title": "Beyond Reward Margin: Rethinking and Resolving Likelihood Displacement in Diffusion Models via Video Generation", "abstract": "Direct Preference Optimization (DPO) has shown promising results in aligning generative outputs with human preferences by distinguishing between chosen and rejected samples. However, a critical limitation of DPO is likelihood displacement, where the probabilities of chosen samples paradoxically decrease during training, undermining the quality of generation. Although this issue has been investigated in autoregressive models, its impact within diffusion-based models remains largely unexplored. This gap leads to suboptimal performance in tasks involving video generation. To address this, we conduct a formal analysis of DPO loss through updating policy within the diffusion framework, which describes how the updating of specific training samples influences the model's predictions on other samples. Using this tool, we identify two main failure modes: (1) Optimization Conflict, which arises from small reward margins between chosen and rejected samples, and (2) Suboptimal Maximization, caused by large reward margins. Informed by these insights, we introduce a novel solution named Policy-Guided DPO (PG-DPO), combining Adaptive Rejection Scaling (ARS) and Implicit Preference Regularization (IPR) to effectively mitigate likelihood displacement. Experiments show that PG-DPO outperforms existing methods in both quantitative metrics and qualitative evaluations, offering a robust solution for improving preference alignment in video generation tasks.", "tldr": "", "keywords": ["Video Generation; Reinforcement Learning; Direct Preference Optimization; Diffusion Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9bd86e6b088abf97ea6e76339e62ad81027e4e05.pdf", "supplementary_material": "/attachment/0038fae92bdadfbb7777e9d56d6d007988331fe0.zip"}, "replies": [{"content": {"summary": {"value": "This paper analyzes likelihood displacement in DPO for diffusion-based video generation, identifying two failure modes and proposing PG-DPO with Adaptive Rejection Scaling and Implicit Preference Regularization to resolve them. The method is empirically validated, and shows consistent improvements, though hyperparameter sensitivity and limited human evaluation leave room for improvement."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**Clarity:**\n\n1. The paper is well-structured: problem → analysis → solution → experiments.\n\n2. Visualizations (likelihood trajectories, qualitative generations) clearly support the claims."}, "weaknesses": {"value": "**Hyperparameter sensitivity:**\nPG-DPO introduces new hyperparameters. The paper admits these require careful tuning, but no adaptive scheme or systematic guideline is provided. Ablation study is encouraged.\n\n**Generality claims:** \nThe framework is said to be extensible to various fine-tuning methods, but no empirical validation outside DPO is provided."}, "questions": {"value": "1. Hyperparameter sensitivity. See W1.\n\n2. Does the regularization have similar effect with MaPPO [1]? Some discussion of similar approaches are encouraged.\n\n[1] MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge. arXiv:2507.21183, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DeAKq3qgRY", "forum": "9WtgapSaCv", "replyto": "9WtgapSaCv", "signatures": ["ICLR.cc/2026/Conference/Submission5348/Reviewer_VfvG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5348/Reviewer_VfvG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760588920340, "cdate": 1760588920340, "tmdate": 1762918018355, "mdate": 1762918018355, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents PG-DPO, by modifying DPO objective with adaptive rejection scaling and implicit reference regularization. The initiative is based on two failure modes of DPO: optimization conflict and suboptimal maximization. The proposed method is applied to preference alignment of video generation, specifically VideoCrafter2 and Wanx2.1. The PG-DPO shows improved metrics compared with SFT and VideoDPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Modify DPO objective based on identified failure modes."}, "weaknesses": {"value": "- The modification to DPO is considered as incremental. \n- Insufficient experiments:\n  - The proposed method is not validated on image generation.\n  - The proposed method is only compared with VideoDPO in experiments."}, "questions": {"value": "- How's this method work on text-to-image generation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VW2uzTOlb2", "forum": "9WtgapSaCv", "replyto": "9WtgapSaCv", "signatures": ["ICLR.cc/2026/Conference/Submission5348/Reviewer_c9mr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5348/Reviewer_c9mr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799281934, "cdate": 1761799281934, "tmdate": 1762918018077, "mdate": 1762918018077, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper provides the analysis of likelihood displacement in diffusion models and introduces Policy-Guided DPO (PG-DPO) — a theoretically grounded, empirically validated method that stabilizes preference alignment for video generation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a novel formal decomposition of DPO's updating dynamics in diffusion models, revealing actionable failure modes.\n\n2. The proposed approach PG-DPO effectively combines ARS and IPR to address both small- and large-margin issues, with empirical evidence (e.g., Fig. 2) showing consistent probability increases for chosen samples.\n\n3. Also the framework extends to other fine-tuning algorithms (e.g., SFT, KTO) and high-dimensional tasks like video generation."}, "weaknesses": {"value": "1. Hyperparameters (e.g., K1, K2 in ARS/IPR) introduce tuning complexity without clear ablation studies.\n\n2. Experimental details (e.g., datasets, baselines, quantitative metrics) are referenced but not fully provided in the visible pages, making it hard to assess reproducibility or superiority claims. for example, SFT is proven to be the most effective way for post-training. The paper lacks the pipeline of choosing post-training data. Is the DPO done only on pretraining or also on SFT as well? it is not very clear."}, "questions": {"value": "1. How was the preference dataset collected (e.g., human annotators, synthetic pairs), and what scale was used for training/evaluation?\n\n2. Can you provide more details on the experimental setup, including baselines (e.g., VideoDPO, Diffusion-DPO), metrics (e.g., FVD, human evaluations), and computational resources?\n\n3. Does PG-DPO generalize to non-video diffusion tasks, such as text-to-image, and were any such experiments conducted?\n\n4. Were ablation studies performed to isolate the impacts of ARS and IPR, and how sensitive is performance to hyperparameters like K1, K2, and β?\n\n5. How does PG-DPO compare to recent DPO variants (e.g., SPIN-Diffusion) in terms of stability and preference data efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ts4dU7AvDU", "forum": "9WtgapSaCv", "replyto": "9WtgapSaCv", "signatures": ["ICLR.cc/2026/Conference/Submission5348/Reviewer_Tc72"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5348/Reviewer_Tc72"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5348/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762724544442, "cdate": 1762724544442, "tmdate": 1762918017876, "mdate": 1762918017876, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}