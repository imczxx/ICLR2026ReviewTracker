{"id": "3rO59y6pgO", "number": 13381, "cdate": 1758217192740, "mdate": 1759897441388, "content": {"title": "Revealing the Impacts of In-Context Learning on Gender Bias in Large Vision-Language Models", "abstract": "In-context learning (ICL) has emerged as a flexible paradigm, enabling large vision-language models (LVLMs) to perform tasks by following patterns demonstrated in context exemplars. While prior work has focused on improving ICL performance across multimodal tasks, little attention has been paid to its potential to amplify societal stereotypes. This study aims to fill this gap by systematically investigating how ICL influences societal biases, with a focus on gender bias, in LVLMs. To this end, we propose a comprehensive evaluation framework comprising six ICL settings and evaluate four LVLMs across two tasks. Our findings indicate that ICL could amplify gender bias, while female-presenting in-context examples generally do not exacerbate bias and may even mitigate it. In contrast, similarity-based retrieval methods, originally designed to improve ICL performance, fail to consistently reduce gender bias in LVLMs. To mitigate gender bias through ICL, we propose a provisional approach that replaces natural in-context images with synthetic ones. This method achieves lower gender bias while maintaining stable performance on standard quality metrics. We advocate for the pre-deployment assessment of gender bias in the context for LVLMs and call for the advancement of ICL strategies to promote fairness on downstream applications.", "tldr": "Revealing how ICL impacts LVLMs and Debiasing LVLMs through ICL.", "keywords": ["Large Vision-Language Models", "In-context Learning", "Gender Bias", "Social Bias"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/179aa385e52cffc0de4eedfe4c4d64226c0f453b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the ability of in-context examples to affect the gender bias in vision-language model predictions. The authors focus on two tasks: image captioning with the COCOBIAS dataset and pronoun prediction with the VISOGENDER dataset. The study focuses on manipulating the construction of the examples between random, male-only, female-only, balanced, image-image similarity-based, and image-text similarity based. The results with QwenVL, MiniCPM-o 2.6, Qwen2.5-VL-7B, and Idefics-3-8B show that careful construction of the example set can mitigate gender bias, but the error analysis shows that very few of the examples are incorrectly predicted, raising concerns about the nature of this problem in modern VLMs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: The paper is clearly written, with a clear structure. The experiments are well-motivated and described. This is an important and interesting problem to study."}, "weaknesses": {"value": "W1: The analyses and supporting figures are too difficult to understand. It takes too much effort for the reader to figure out exactly how \"MR_m - MR_f are more favorable under the FS setting than under the MS setting\" (L304). Figures 2 and 3 need urgent attention to the y-axis labels, and the captions, reminding the reader about the meaning of RS, MS, FS, Bx, SIIR, and SITR.\n\nW2: The analyses in this paper focus on rather simple next-token completion tasks that are exactly how the models would have been adapted for multimodal ability. The analysis could be improved by considering VQA-style tasks, for which binary gender data splits exist [1].\n\n[1] Cabello et al. EMNLP 2023. Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models."}, "questions": {"value": "Q1: What is the relationship netween the Bias Evaluation Metrics used in this paper and Bias Amplification?\n\nQ2: Does mutlimodal ICL actually work in these models? The lack of variation in the classic measures in Table 1 suggests that QwenVL might be ignoring the in-context examples. \n\n[2] Wang and Russakovsky. ICML 2021. Directional bias amplification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KtH41cBNqf", "forum": "3rO59y6pgO", "replyto": "3rO59y6pgO", "signatures": ["ICLR.cc/2026/Conference/Submission13381/Reviewer_FZqq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13381/Reviewer_FZqq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761606518901, "cdate": 1761606518901, "tmdate": 1762924023075, "mdate": 1762924023075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies whether and how in-context learning (ICL) alters gender bias in large vision-language models (LVLMs). The authors design a six-setting ICL evaluation framework, measure four state-of-the-art LVLMs on image-captioning and VQA, and observe that (i) ICL can indeed amplify gender bias, (ii) placing female-presenting exemplars in the prompt often mitigates bias, (iii) similarity-based example-retrieval does not consistently help, and (iv) synthetic in-context images provide a practical mitigation knob. The work concludes with a recommendation that LVLM deployments should examine the bias of their prompts and that more fairness-aware ICL strategies are needed."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Important and timely question: while ICL is now the de-facto way to steer LVLMs, its influence on social bias has been largely unexplored.\nComprehensive evaluation design: six prompt settings, two tasks, four public LVLMs, and both amplification and mitigation analyses.\nInteresting empirical findings: (a) ICL occasionally increases gender bias even when overall utility improves; (b) female exemplars are systematically more helpful than male ones; (c) retrieval-based exemplar selection, though popular for accuracy, does not cure bias.\nPractical mitigation recipe: swapping natural contextual images for synthetic renders is simple yet shows measurable bias reduction without hurting standard metrics.\nClear call-to-action: the paper argues convincingly that prompt designers should audit bias and that future ICL research must consider fairness."}, "weaknesses": {"value": "Limited definition of “gender bias”: the study focuses on binary male/female stereotypes; it ignores non-binary identities and intersectional dimensions (race, age, etc.).\nCausal attribution is still weak: while correlations are shown, the mechanisms of amplification (e.g., attention patterns, token probabilities) are not deeply analysed.\nOnly two downstream tasks: conclusions may not generalise to other vision-language tasks such as visual reasoning or grounded dialogue.\nDependence on proprietary models: two of the four LVLMs have opaque training data and safety filters, making some observations hard to reproduce or explain.\nSynthetic-image mitigation is evaluated with a small set of GAN-generated faces; effectiveness on more complex scenes or in real applications remains unclear."}, "questions": {"value": "Did you control for content drift when replacing natural images with synthetic ones? Could changes in low-level image statistics, not gender attributes, drive the mitigation?\nHow sensitive are results to the number of in-context exemplars? Does bias amplification scale linearly with more biased examples?\nCan your synthetic-image strategy be combined with similarity-based retrieval (i.e., retrieve similar images, then substitute them with synthetic gender-balanced versions)?\nDid you examine other social biases (race, age, profession)? If so, are the trends similar; if not, do you foresee any obstacles to extending your framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GjJEiQrO86", "forum": "3rO59y6pgO", "replyto": "3rO59y6pgO", "signatures": ["ICLR.cc/2026/Conference/Submission13381/Reviewer_JEjY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13381/Reviewer_JEjY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644840382, "cdate": 1761644840382, "tmdate": 1762924022686, "mdate": 1762924022686, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper systematically investigates the impact of in-context learning (ICL) on gender bias in Large Vision-Language Models (LVLMs). The authors propose a comprehensive evaluation framework comprising six ICL settings (Random, Male-only, Female-only, Balanced, and two similarity-based retrieval methods). This framework is used to evaluate four different LVLMs across two tasks: image captioning (on COCOBIAS) and pronoun prediction (on VISOGENDER). The study finds that ICL can amplify existing gender biases, but this effect can be masked by model safety filters or low \"reveal rates\". A key finding is that female-presenting in-context examples tend to mitigate bias, whereas common similarity-based retrieval methods (designed for performance) fail to consistently reduce it and may even amplify it. As a provisional mitigation strategy, the paper proposes replacing natural in-context images with synthetic ones, which is shown to lower gender bias while maintaining stable task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Important and Timely Problem: The paper tackles a critical and under-explored area. While ICL for LVLMs is a popular research topic, most work focuses on performance improvements, neglecting the potential for bias amplification. This study provides a much-needed analysis of the fairness implications.\n2. Comprehensive Methodology: The proposed framework of six distinct example-selection strategies is a key strength. It allows for a systematic and controlled study of how the composition of in-context examples influences model behavior, moving from random baselines to attribute-specific (MS, FS) and performance-oriented (SIIR, SITR) settings.\n3. Thorough Experimentation: The findings are supported by experiments on four modern LVLMs (QwenVL, MiniCPM, Qwen2.5VL, Idefics3) and two distinct tasks (generation and prediction). This demonstrates the robustness and generalizability of the conclusions."}, "weaknesses": {"value": "1. Contradictory Claims on Prior Bias: The paper's explanation for why introducing female examples helps reduce bias relies on the claim that they \"help counterbalance the LVLMs’ biased priors against females\". This is directly contradicted by the paper's own zero-shot results on COCOBIAS, which found that three of the four models already \"demonstrate a consistent bias toward the female category\". This inconsistency undermines the core explanation for the paper's main finding.\n2. Lack of Mechanistic Explanation: Related to the point above, the paper does a good job of showing what happens (FS mitigates bias) but struggles to provide a deep explanation of why it happens.\n3. Limited Scope of ICL Factors: The study focuses exclusively on the composition of the demo examples. However, a large body of work on ICL has shown it to be highly sensitive to other factors, such as the order of the examples (Yang et.al). The \"Balanced Sample\" setting, for instance, interleaves examples, but it's unknown if the effect would hold if the order were changed (e.g., order the examples with the same label as the query in the beginning or closest to the query).\n4. Visual vs. Textual Cues: By claiming ICL can amplify existing gender biases but not so much, this paper assumes that the models are reacting to the visual content of the in-context examples. However, recent research has suggested that multimodal models often pay little attention to visual context in ICL, relying heavily on textual cues instead (Chen et.al). Is it possible that the model is not using the images, and that's why the results show ICL has limited influence?\n5. Practicality of Mitigation: The proposed mitigation (using synthetic images)  is interesting but its practical application is unclear. This approach adds a significant computational overhead (running a text-to-image model like SDM) for every set of in-context examples, which may not be feasible for real-time applications. The paper doesn't discuss this trade-off.\n\nReference\n\nYang, X., Peng, Y., Ma, H., Xu, S., Zhang, C., Han, Y., & Zhang, H. (2024). Lever LM: configuring in-context sequence to lever large vision language models. Advances in Neural Information Processing Systems, 37, 100341-100368.\n\nChen, S., Liu, J., Han, Z., Xia, Y., Cremers, D., Torr, P., ... & Gu, J. True Multimodal In-Context Learning Needs Attention to the Visual Context. In Second Conference on Language Modeling."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zt7kP34U92", "forum": "3rO59y6pgO", "replyto": "3rO59y6pgO", "signatures": ["ICLR.cc/2026/Conference/Submission13381/Reviewer_3tRC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13381/Reviewer_3tRC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13381/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734124960, "cdate": 1761734124960, "tmdate": 1762924022389, "mdate": 1762924022389, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}