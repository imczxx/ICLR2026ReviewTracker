{"id": "M2AXTAKXbz", "number": 22979, "cdate": 1758337780858, "mdate": 1759896837827, "content": {"title": "PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases", "abstract": "Tool-augmented language agents frequently fail in real-world deployment due to tool malfunctions—timeouts, API exceptions, or inconsistent outputs—triggering cascading reasoning errors and task abandonment. Existing agent training pipelines optimize only for success trajectories, failing to expose models to the tool failures that dominate real-world usage.\nWe propose \\textbf{PALADIN}, a generalizable framework for equipping language agents with robust failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories constructed via systematic failure injection and expert demonstrations on an enhanced ToolBench dataset. Training uses LoRA-based fine-tuning to retain base capabilities while injecting recovery competence. At inference, PALADIN detects execution-time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars aligned with ToolScan's taxonomy, then executes the corresponding recovery action.\nThis approach generalizes to novel failures beyond the training distribution, retaining 95.2\\% recovery performance on unseen tool APIs. Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR), and Efficiency Score (ES). PALADIN improves RR from 32.76\\% to 89.68\\% (+57\\% relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34\\%) by +13.3\\%. Against vanilla agents, PALADIN achieves 89.86\\% RR (+66\\% relative improvement from 23.75\\%). These results establish PALADIN as an effective method for building fault-tolerant agents capable of robust recovery in real-world tool environments.", "tldr": "", "keywords": ["Language model agents", "Tool failure recovery", "Self-correction", "Agent robustness", "LoRA fine-tuning", "Failure injection training", "Tool-augmented LLMs"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1e40c6dda824612ff79e2cdcc4b91ffe0ac75ad.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces PALADIN, a framework that trains LLM agents to detect, diagnose, and recover from real-world tool failures, such as API timeouts or malformed outputs, using over 50,000 recovery-annotated trajectories generated through systematic failure injection and expert supervision. PALADIN combines LoRA-based fine-tuning for recovery-aware learning with an inference-time retrieval mechanism that selects corrective strategies from a curated library of over 55 exemplar failures. Experimental results across multiple LLM backbones show that PALADIN boosts recovery rates and outperforms baselines like CRITIC and ToolReflect, proving that execution-level robustness is a learnable and generalizable behavior."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The whole inference time error fixing framework is new and interesting, even though it seems very complex and cannot be modularized for convenient adaptation and using.\n\n- The problem the paper seeks to solve is important and timely, as robustness in tool call is seldomly directly considered as an optimization goal in the agent system. Therefore, the paper’s topic may be of interest to many people in the agent community.\n\n- The metrics introduced in the paper is interesting and novel. The benchmarks tested are relevant and comprehensive to reflect the method’s effectiveness."}, "weaknesses": {"value": "- The paper does not give a clear definition of what level of tool calling failure is mainly investigated. Tool parameter filling? Turn level? Or trajectory level? Also I think it’s better to distinguish “wrong” and “failure”, which may be respectively rooted in model itself’s inability or environment’s failure.\n\n- Some part of the method is unclear and seems arbitrary, for example, why we have exactly 55 exemplar failure cases to choose from? Why more cases won’t benefit, or how can you justify these examples are not cherry picked for the test cases? I some parts like this in the methods needs further clarification and justifications. (Especially ablation shows these 55 examples are very important, this makes readers more curious how they are picked, and what is the root cause for performance raise)\n\n- Current result presentation is like repeating the numbers in the tables and graphs but lacks insights on what is the causing this phenomenon or what these phenomenon indicates. Generally repeating the numbers in the papers are not appreciated.\n\n- The result part is also unclear on the error types and does not define what is “generalization”. Currently readers can only read numbers but does not really understand what these numbers stand for. The paper will benefit from giving more examples like what is generalization in error fixing."}, "questions": {"value": "- From the motivation, the previous works are pointed out to be “reactive” and only perform “call level fixes”, so how is your method jumping out of this paradigm? I don’t think current design is “proactive”?\n\n- Further of the last question, what’s current raw model performance in fixing tool calls if they are prompted through the best strategy? How can you properly and empirically justify the training is necessary? If PALADIN’s data is derived from GPT-5, why cannot we just use GPT-5 for tool call fixing on the fly?\n\n- Run time failure is a very broad concept, what are some specific error patterns and how you empirically deal with them (as in the training data) concretely? If the error is caused due to environment side, why any fixing method would be useful if environment itself is not robust?\n\n- Can the method repair “trajectory level wrongness” or just “turn level failure”. If latter, there’s a subsequent StableToolBench which gives a stable backend to avoid the instability in the environment. Will using this benchmark undermines the effectiveness of your method as environment side instability is eliminated?\n\n- What’s the error that your current method still cannot fix or solve properly? A statistics on your method’s error analysis will further benefits later on research to build upon your method and continue better fixing methods."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "wDDdG3vm6w", "forum": "M2AXTAKXbz", "replyto": "M2AXTAKXbz", "signatures": ["ICLR.cc/2026/Conference/Submission22979/Reviewer_PUXc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22979/Reviewer_PUXc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771057427, "cdate": 1761771057427, "tmdate": 1762942461726, "mdate": 1762942461726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework named PALADIN, designed to enhance the robustness of tool-augmented LLM agents in handling tool invocation failures. Its core idea is to train the model—through systematic failure injection and recovery-annotated trajectories—to detect, diagnose, and recover from runtime errors. The authors construct a dataset containing over 50K recovery trajectories, fine-tune the model using LoRA, and integrate a retrieval-based recovery strategy during inference. Experiments conducted across multiple base models and benchmarks (e.g., ToolBench, ToolReflectEval) demonstrate that PALADIN significantly outperforms baselines (such as CRITIC and ToolReflect) in Recovery Rate, Task Success Rate, and Catastrophic Success Rate."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The research addresses a problem of practical significance: tool invocation failures represent a critical bottleneck for language agents in real-world scenarios. By focusing on execution-level robustness rather than conventional task accuracy, the work demonstrates notable innovation.\n\n2. The methodology of \"failure injection + recovery\" for generating large-scale recovery-annotated trajectory data is clearly articulated and offers good reproducibility.\n\n3. The introduction of new metrics—such as Recovery Rate (RR), Catastrophic Success Rate (CSR), and Error Sensitivity (ES)—provides a complementary evaluation framework for assessing robustness."}, "weaknesses": {"value": "1. The overall approach primarily integrates existing components—failure injection, LoRA fine-tuning, and retrieval-based recovery—in a linear pipeline, without introducing novel algorithmic contributions or optimization strategies. PALADIN appears more as a systematic engineering effort rather than a research breakthrough in theory or algorithm design.\n\n2. The reliance on offline training with synthetically generated data may limit the agent's ability to autonomously learn and generalize in dynamic environments.\n\n3. Several conceptual definitions lack rigor:\n\n- The metric \"Catastrophic Success Rate (CSR)\" is ambiguously defined, and its formulation as \"1 - hallucinated success\" lacks methodological precision.\n\n- The \"Efficiency Score\" merely uses the reciprocal of average step count, which does not accurately reflect real computational cost, time consumption, or API call overhead.\n\n- Vague phrases such as \"robustness is learnable\" and \"Pareto frontier\" are repeatedly used without sufficient empirical or theoretical justification.\n\n4. While the writing is fluent, the paper suffers from excessive verbosity and structural redundancy (e.g., Sections 2.1–2.5 largely reiterate existing tool systems). The appendix is over-extended (e.g., error catalogs, recovery strategy tables), diluting the focus on core methodology and experimental insights in the main text."}, "questions": {"value": "1. How does the model during inference determine whether to trigger recovery retrieval? Is there an error detection threshold or confidence-based mechanism governing this decision?\n\n2. Given the extensive use of GPT-4 as an evaluator in the experiments, what measures were taken to mitigate potential evaluation bias? Was human consistency validation performed to verify the automated scoring?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wJpwrJEuvC", "forum": "M2AXTAKXbz", "replyto": "M2AXTAKXbz", "signatures": ["ICLR.cc/2026/Conference/Submission22979/Reviewer_bfHL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22979/Reviewer_bfHL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762003110983, "cdate": 1762003110983, "tmdate": 1762942461326, "mdate": 1762942461326, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a framework for fine-tuning language agents to add failure recovery capabilities. PALADIN trains on 50,000+ recovery-annotated trajectories on an enhanced ToolBench dataset. At inference, PALADIN detects execution time errors and retrieves the most similar case from a curated bank of 55+ failure exemplars to executes the corresponding recovery action.  Evaluation across PaladinEval and ToolReflectEval demonstrates consistent improvements in Recovery Rate and Task Success Rate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Significant: The paper focuses on an important problem which is teaching LLMs to deal with Failures. LLM based systems can benefit from a permanent solution to common failures such as tool or network related issues.\n- Error-recovery Dataset: The Dataset created using GPT-5 is useful for future failure recovery experiments."}, "weaknesses": {"value": "- Cost of Maintenance: Since PALADIN needs fine-tuning LLMs, it has a high maintenance cost as the failure types change and new LLMs are released. Also, PALADIN leverages a knowledge-base at inference time, it is not clear why a knowledge base still needs to be maintained and used when one pays the cost of fine-tuning on failures.\n- Soundness: The paper uses LLM-As-A-Judge to evaluate the correctness of the tasks and the same model used to generate the data is used to evaluate the results which can result in inaccuracies.\n- Experiment: The paper only compares 4 smaller size LLMs. It's not clear whether the frontier models such as latest Claude, Gemini or GPT would be able to perform better than PALADIN."}, "questions": {"value": "- Why is CSR a good metric and what does it capture?\n- Have you done any experiments on latest Claude, Gemini or GPT models to compare PALADIN's performance against the vanilla model or existing agents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sknp30C8Ev", "forum": "M2AXTAKXbz", "replyto": "M2AXTAKXbz", "signatures": ["ICLR.cc/2026/Conference/Submission22979/Reviewer_149B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22979/Reviewer_149B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145743726, "cdate": 1762145743726, "tmdate": 1762942460948, "mdate": 1762942460948, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the PALADIN framework for training tool-call agents, along with new metrics and benchmarks for quantifying the execution-level robustness of tool-call agents. This addresses the limitations of existing methods that only optimize success trajectories, thus tackling the complex challenges of real-world tool-call scenarios.\nThe main contributions of this paper include: (1) proposing the PALADIN framework for tool-call agents, which possesses execution-level robustness and can detect and recover from failures; (2) using LLMs for failure injection and recovery annotation, constructing an enhanced tool-call dataset for fine-tuning PALADIN; and (3) proposing three new metrics—Recovery Rate, Catastrophe Success Rate, and Efficiency Score—to measure the agent's failure detection and recovery capabilities, and establishing the PaladinEval benchmark for system robustness evaluation."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "(1) This paper addresses the execution-level robustness problem of tool-invoking agents, focusing on endowing agents with the ability to detect and recover from faults. It proposes the PALADIN framework to address complex challenges in real-world scenarios, overcoming the limitations of previous research.\n(2) This paper constructs a fine-tuning dataset using LLMs, proposes new metrics to measure the agent's fault detection and recovery capabilities, and establishes the PaladinEval benchmark for system robustness evaluation.\n(3) A recovery dictionary is constructed, and retrieval augment are added during inference, thereby improving the agent's ability to detect and recover from faults."}, "weaknesses": {"value": "(1) Although this paper raises an important problem—the detection and recovery capability of tool-augmented agents under failure conditions—the proposed solution lacks sufficient innovation. The core techniques, including data augmentation via large language models, dataset-based fine-tuning, and similarity-based retrieval during inference, are relatively common practices. As such, the methodological novelty of the paper is somewhat limited.\n(2) The experimental evaluation is not sufficiently comprehensive. It mainly includes the main comparison (Table 1), generalization test (Figure 6), and ablation study (Figure 7). In the main experiments, more baseline methods should be included for a stronger comparative analysis. In the generalization experiments, testing on only one out-of-domain dataset is insufficient to convincingly demonstrate the generalization capability of the proposed approach. The ablation study only removes the retrieval component; more ablations are needed to verify the contribution of each module in the framework. Furthermore, the paper does not clearly specify which dataset the ablation experiments were conducted on.\n(3) According to Table 1, PALADIN achieves inferior Efficiency Score (ES) compared to the best baselines in most cases, yet when using Qwen-2.5-14B as the base model, PALADIN attains the highest ES. The authors neither analyze this exception nor explain why PALADIN generally performs worse on ES. This inconsistency should be explored, possibly through additional experiments or case studies. Moreover, the experimental analysis appears verbose at times—for example, in the ablation section, similar percentage drops for each model and metric are repeatedly described, which could be summarized more concisely.\n(4) Although Table 1 shows that PALADIN outperforms all baselines on most metrics except ES, Figure 7 reveals that, once inference-time retrieval is removed, PALADIN performs worse than baselines such as CRITIC in RR, TSR, and CSR across all model architectures. This indicates that fine-tuning on the constructed dataset alone does not yield better performance than existing methods, and the performance gains mainly stem from the retrieval component. Additional experiments are needed to demonstrate that the combination of fine-tuning and retrieval is indeed superior to using either one alone.\n(5) The paper does not clearly describe how similarity is computed in the retrieval module. The similarity metric should be mathematically defined in greater detail to ensure reproducibility."}, "questions": {"value": "(1) Design experiments to validate the effectiveness of the constructed tool-augmented dataset and fine-tuning process. Additional experiments should be conducted to demonstrate that the performance improvements of PALADIN do not solely rely on inference-time retrieval. Specifically, experiments comparing fine-tuning only, retrieval only, and fine-tuning + retrieval configurations are necessary to show that the combined approach achieves the best overall performance.\n\n(2) Expand the experimental scope. The experimental section should be enriched in several aspects: Include more state-of-the-art baselines in the main comparison to provide a fairer and more comprehensive evaluation. Incorporate additional out-of-domain datasets in the generalization study to more convincingly demonstrate cross-domain robustness. Extend the ablation study to cover other critical components of the framework, rather than focusing solely on the retrieval module.\n\n(3) Introduce additional analyses. Further experiments should be designed to evaluate inference latency and computational cost, providing a deeper investigation into the trade-off between robustness and efficiency. Such analysis would enhance the paper’s completeness and practical relevance for real-world deployment."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "md4BYcva47", "forum": "M2AXTAKXbz", "replyto": "M2AXTAKXbz", "signatures": ["ICLR.cc/2026/Conference/Submission22979/Reviewer_rXTV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22979/Reviewer_rXTV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22979/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762173937723, "cdate": 1762173937723, "tmdate": 1762942460514, "mdate": 1762942460514, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}