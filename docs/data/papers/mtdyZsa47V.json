{"id": "mtdyZsa47V", "number": 19443, "cdate": 1758296305521, "mdate": 1759897038569, "content": {"title": "Fast Language Generation through Discrete Diffusion Divergence Instruct", "abstract": "The fast generation of language texts is the holy grail that people pursue in the AI era. In this work, we introduced **Di**screte **Di**ffusion Divergence **Instruct** (**DiDiInstruct**), a training-based method that leads to fast language generation models by initializing from a pre-trained (masked) discrete diffusion language model (dLLM). The resulting DiDi-Instruct model outperforms the dLLM counterparts and the GPT-2 baseline with 64$\\times$ acceleration. In the theoretical part of the paper, we build the foundation of DiDi-Instruct in a framework of integral KL divergence minimization, with practical training algorithms. We also introduce techniques like grouped reward normalization, intermediate-state matching, and the reward-guided ancestral sampler (RGAS) that significantly improve the training stability, the model coverage, and the inference performances. On OpenWebText, DiDi-Instruct outperforms all accelerated language generation models as well as the GPT-2 baseline and the standard dLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128 NFEs). These performance gains are accomplished with a negligible entropy loss of about $1$\\% and $20\\times$ less additional training wall-clock time. We further validate the robustness and effectiveness of DiDi-Instruct through extensive ablation studies, model scaling, and the generation of discrete protein sequences. In conclusion, DiDi-Instruct is an efficient yet effective distillation method, enabling language generation in the blink of an eye. We will release our code and models along with the paper", "tldr": "We distill efficient few-step discrete diffusion language model, achieving fast inference and lower perplexity than state-of-the-art baselines.", "keywords": ["discrete diffusion models", "masked diffusion models", "distillation", "integral KL divergence", "large language models", "generative modeling"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/defc7d08423afa0d387e57baf01a38120430916d.pdf", "supplementary_material": "/attachment/7672c72f52cb7493855cf369c1d65726f006eee6.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Discrete Diffusion Divergence Instruct (DiDi-Instruct), a distillation framework that trains a few-step discrete diffusion language model (student) to match a pre-trained masked diffusion language model (teacher) by minimizing an integral KL divergence (IKL) across noise levels. The core gradient is rewritten as a score-function estimator weighted by a log density-ratio reward between student and teacher; the reward is estimated via an auxiliary discriminator. Training is stabilized by grouped reward normalization and a score decomposition that exposes the student to intermediate corruption states. At inference, a Reward-Guided Ancestral Sampler (RGAS) uses discriminator signals for gradient tilting in early steps and re-ranking in later steps. On OpenWebText, the model achieves perplexity 62.2- 18.4 at 8 - 128 NFEs, with small entropy loss and lower distillation cost, and it scales to larger models and a protein-sequence task."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper adapts IKL-based distribution matching to masked discrete diffusion and derives a policy-gradient form that bypasses non-differentiable paths, with a discriminator estimating the log density ratio used as reward. This method is different from time-matching distillations and provides a route to few-step discrete generation.\n2. The derivation gives the IKL objective and yields a score-function identity in which the reward is the marginal log-density ratio between student and teacher; the proof sketch and assumptions are stated, and the link to a tractable discriminator estimator is explicit.\n3. On OpenWebText the method reports consistently lower PPL across 8–128 steps, with faster distillation and small entropy loss. The paper includes cumulative and leave-one-out ablations indicating gains from the proposed components, and shows transfer to protein sequence generation.\n4. The paper is clearly written and well structured."}, "weaknesses": {"value": "1. The comparison to the baselines are is too abbreviated with only one figure of perplexity. It would be better to have a table presenting more metrics besides the ppl.\n2. DiMO is discussed in the related work in appendix but is not include as an experiment baseline."}, "questions": {"value": "1. Can you report tokens/sec (or sequences/sec) and latency per 1k tokens at matched perplexity versus a similarly sized AR model with and without speculative decoding, on the same hardware and kv-cache settings? This would clarify the practical speed–quality trade-off beyond NFEs.\n2. Can you include downstream tasks? A single perplexity metric is not enough to establish practical advantage of the new method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hdhFk53Q81", "forum": "mtdyZsa47V", "replyto": "mtdyZsa47V", "signatures": ["ICLR.cc/2026/Conference/Submission19443/Reviewer_n4tg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19443/Reviewer_n4tg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524101734, "cdate": 1761524101734, "tmdate": 1762931364301, "mdate": 1762931364301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a training-based distillation framework DiDi-Instruct to accelerate generation for discrete diffusion language models (dLLMs). The core idea of DiDi-Instruct is to distill a pre-trained teacher dLLM into a student model that can generate high-quality text in very few steps. The method is centered on minimizing the Integral KL (IKL) divergence between the teacher and student models, which forces the student to match the teacher's data distribution. The author proposes comprehensive solutions to make the training on discrete data tractable, concluding: 1) inspired by policy gradient, decomposing the gradient of the objective as a score function weighted by a log-density ratio to avoid non-differentiable issue. 2) using an auxiliary discriminator to approximate the log-density ratio because it is intractable to compute directly. 3) leveraging grouped reward normalization to stabilize reward. The training is end-to-end and jointly for discriminator and student model. During inference, the author uses the trained discriminator to guide the sampling process and improve output quality. Experiments show the effectiveness of the proposed method that DiDi-Instruct significantly outperforms existing accelerated models and the GPT-2 baseline in perplexity, and the distilled model surpasses the 1024-step teacher's quality with only 16 steps."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a mathematically rigorous solution to the difficult problem of distillation in a discrete state space. Instead of relying on heuristics, it adapts the IKL divergence objective from continuous models and  reformulates it as a tractable policy gradient problem.\n2. The method consistently outperforms other distillation methods and the GPT-2 baseline in perplexity across all sampling budgets.\n3. The proposed method demonstrates high efficiency. The training only needs one GPU, compared with 8 GPU for training teacher model. For generation, it is able to surpass the text quality of its 1024-step teacher model with only 16 inference steps, representing a significant acceleration.\n4. The proposed method avoids the mode collapse problem, which is a common problem in distillation. Experimental results on text generation demonstrate negligible entropy loss."}, "weaknesses": {"value": "1. The author performs distillation on a self-trained teacher model. This raises questions about the method's transferability and effectiveness when applied to other pre-trained, open-source dLLMs, such as LLaDA [1] or Dream [2].\n2. In the text generation experiments, the evaluation is limited to perplexity and entropy. These metrics alone cannot accurately reflect the true text generation capabilities. The authors should conduct a more comprehensive evaluation on standard text generation benchmarks (e.g., MMLU, GSM8K, Humaneval) to demonstrate that the student model can indeed achieve performance comparable to the teacher model while using fewer steps.\n3. In the protein sequence experiments, the pLDDT score increases significantly after distillation. However, excessively high pLDDT scores might indicate a potential risk of mode collapse (e.g., collapsing to a few sequences with the highest pLDDT, thereby failing to sample other valid sequences), which will lead to a reduction in generation diversity. The authors need to clarify the generation results from the perspective of protein sequence diversity (e.g., by using MMseqs clustering) to address this concern.\n\n[1] Dream 7B: Diffusion Large Language Models\n\n[2] Large Language Diffusion Models"}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cKatyPVvd4", "forum": "mtdyZsa47V", "replyto": "mtdyZsa47V", "signatures": ["ICLR.cc/2026/Conference/Submission19443/Reviewer_HAtL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19443/Reviewer_HAtL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761786261961, "cdate": 1761786261961, "tmdate": 1762931363797, "mdate": 1762931363797, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Discrete Diffusion Divergence Instruct (DiDi-Instruct), a training-based distillation method that initializes from a pre-trained dLLM to create a fast, few-step student generator. The framework is founded on integral KL-divergence minimization, which is reformulated from a policy gradient perspective to derive a tractable update rule for the discrete state space of language."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper proposes a principled training method by reformulating the IKL distillation objective from a policy gradient perspective, which provides a tractable update rule for discrete spaces.   \n- DiDi-Instruct achieves new state-of-the-art results on the OpenWebText benchmark, consistently delivering lower PPL across 8-128 NFEs with over 20x faster distillation time than competing methods."}, "weaknesses": {"value": "- The proposed Reward-Guided Ancestral Sampler (RGAS) introduces new hyperparameters ($h$, $M$), but there is no sensitivity analysis.\n- It is unclear how the baseline results like DUO or SDTT are obtained."}, "questions": {"value": "- Could you clarify the experimental setup for the SDTT and DUO baselines?   \n- Could you provide a sensitivity analysis for the RGAS hyperparameters ($h$, $M$) and elaborate on the stability of the adversarial training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5yQPbw7jtH", "forum": "mtdyZsa47V", "replyto": "mtdyZsa47V", "signatures": ["ICLR.cc/2026/Conference/Submission19443/Reviewer_Qgt2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19443/Reviewer_Qgt2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19443/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876382575, "cdate": 1761876382575, "tmdate": 1762931363209, "mdate": 1762931363209, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}