{"id": "wAb8vtEZfM", "number": 11955, "cdate": 1758204871212, "mdate": 1759897542622, "content": {"title": "Size Doesn't Matter: Data Efficient Deep Learning Beyond the Big Data Paradigm", "abstract": "Since the emergence of deep learning, machine learning scientists have focused\non improving algorithms to achieve higher classification accuracies. This work\nhas consisted of connecting encoders and decoders through attention mechanisms\nor having multi-layer perceptrons. However, in many data analysis fields, collecting\nlarge sets of data is not possible. Therefore, we challenge the “bigger is\nbetter” paradigm. We propose a new simple method, known as selective embedding,\nbased entirely on how data is loaded. Existing experiments are highlighted,\nand new experiments are conducted in four different areas: heavy machinery, railway,\nmanufacturing, and medical imaging. A medical dataset is used as a baseline,\ncontaining 65,000 patient data points for various diseases, by reducing the number\nof patients to demonstrate that dataset size does not matter. In addition, for\neach area, a dataset was selected to show high performance accuracy using a deep\nlearning algorithm. Our method achieves 87%+ accuracy in all areas when using\nsmaller sets of data for classification tasks without overfitting.", "tldr": "", "keywords": ["Selective Embedding", "Data-Efficient Deep Learning", "Medical Imaging", "Generalization under Data Scarcity", "Cross-Modality Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/49ea4719f69baa7d89e57ef5615a326b16c7eafe.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "A study that reads as if most of it was LLM generated. \nThe main idea - if any - seems to be a new way to compose mini-batches.\nHowever, unfortunately the presentation is so obscure that it is difficult to say what the contribution is."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "The authors admit that they used an LLM in the writing process."}, "weaknesses": {"value": "This is a clear reject. I am not a native speaker myself, so I am sorry to write that communication is also an important part of research.\nThis is a submission that reads as if it was in large parts LLM generated.\n\nThe text is an incoherent concatenation of unclear statements, some of which are wrong. \nThe mathematical statements lack support, they are not proven. The notation is inconsistent.\nEven the main algorithm is not clearly explained.\n \nJust some examples:\n\n- Abstract: „Since the emergence of deep learning, machine learning scientists have focused on improving algorithms to achieve higher classification accuracies.“ No, I would say ML researchers always have been doing this.\n- „Conventional wisdom in deep learning asserts that high performance necessitates massive datasets.“ I disagree, in particular if one does not consider generative AI.\n- Background: The background gives the impression as if small data sets would be something special. They aren’t. The UCI benchmark repository is full of - by today’s standards - small data sets.\n- 4.5.4: No, standard PAC-Bayes analysis does not consider „gradient variance“.\n- 4.6.1 Proof?\n- ... ... ..."}, "questions": {"value": "Where does PAC-Bayesian analysis consider „gradient variance\"?\nCan you provide a rigorous proof of the statement in 4.6.1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SvBC2ahxI1", "forum": "wAb8vtEZfM", "replyto": "wAb8vtEZfM", "signatures": ["ICLR.cc/2026/Conference/Submission11955/Reviewer_7GxM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11955/Reviewer_7GxM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760886948070, "cdate": 1760886948070, "tmdate": 1762922956356, "mdate": 1762922956356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper follows up on an earlier paper named \"Selective embedding for deep learning\", or SE for the purpose of this review. SE is a proposed 'data loading' strategy that could be added as a wrapper to any model with no change to it. SE alternates short segments of data from multiple sources (and data types or modalities) into a single input stream prior to any processing. The idea decorrelates inputs from either interleaved stream to improve the entropy rate of data by reducing the variance (svd style). The intention is apparently to intended to challenge the \"bigger is better\" paradigm, in terms of the amount of data. Experiments are diverse and on a smaller number of examples for a comparable   statistical strength."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": ". The method sppears original, and I have not otherwise read literature in the domain generalization theme where such multi-source data  are interleaved.\n\n. The 11x  speedup reported"}, "weaknesses": {"value": ". The idea that data are 'mixable' at anything above the byte level is  a stretch to belief. In disregard to data type, their  information density relative to that type, their representational nature (e.g. 3D vs 1D), it does no appear sound to just mix them up.\n\nI would have  understood if data were being projected  to a unified embedding  space before such interleaving were performed. But it is as crude as the concatenation it derides.\n\nI use the term embedding in a more general manner, to stricke a contrast with what the paper's quite non standard usage of the term.\n\n. Other weaknesses include small scale experiments e.g. on a 800/200 example split. The \"size\" emphasis is weakened. If these are all images, load-time mixing is moot. Here, CT and MRI is being mixed, to the horror of anybody who understands how different these modalities are. \nThe paper doesn't divulge that in Table 1 the traditional ' parallel' loading went through  an   equivalent pipeline, or at least the architecture changed.\n\n. Experiments aren't extensive or on private data"}, "questions": {"value": ". How does the paper differ from 'se for dl' in its core idea? I understand that the writing is due to an LM and therefore a lot improved."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1RDFUxR0PJ", "forum": "wAb8vtEZfM", "replyto": "wAb8vtEZfM", "signatures": ["ICLR.cc/2026/Conference/Submission11955/Reviewer_ikD8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11955/Reviewer_ikD8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581766861, "cdate": 1761581766861, "tmdate": 1762922955974, "mdate": 1762922955974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper lacks significant novel contributions. The proposed method, Selective Embedding (SE), is not introduced here but originates from prior work (Sehri et al., 2025). The industrial experiments on heavy machinery, railway, and manufacturing domains are directly borrowed from that earlier publication, without new analysis or ablations. The only new experiment is on the CheXpert medical imaging dataset. However, they use an extremely small training set and achieve implausibly perfect validation metrics, raising strong concerns about overfitting and lack of generalization. Hence, the central  “size doesn’t matter” claim is not convincingly supported by any means. Overall, while the paper presents an interesting perspective on data efficiency, it does not offer a clear methodological advance or rigorous empirical validation to justify its main conclusions."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Poorly written paper. The clarity is fair, and it’s clearly not delivering any evidence to support their claims."}, "weaknesses": {"value": "The core claim, “size doesn’t matter”, is not supported in any way in this paper. The borrowed experiment results like heavy machine and railroad cases don’t support the size claim, but reveal the order of data loading and variety in data sample impact efficiency. There’s no mention of a subset of the data used in the training, which would have provided some evidence that size can indeed be reduced. So the result doesn’t support the “size doesn’t matter” claim. The only new experiment with the CheXpert dataset was conducted on a smaller subset, but without testing if the result can be generalized to a bigger dataset. The fact that they got AUROC of 1.0 strongly suggests overfitting. Hence, this experiment doesn’t support their claim either. In the paper they did mention that smaller datasets can be used for customization, but that doesn’t justify that larger datasets are not needed, either."}, "questions": {"value": "Please check my reviews on weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ydJFoHZrlw", "forum": "wAb8vtEZfM", "replyto": "wAb8vtEZfM", "signatures": ["ICLR.cc/2026/Conference/Submission11955/Reviewer_Cvbb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11955/Reviewer_Cvbb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693234608, "cdate": 1761693234608, "tmdate": 1762922955470, "mdate": 1762922955470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how interleaving data from multiple modalities using an existing method called \"selective embedding\" into short segments helps with learning when compared to using continuous streams of data. Experiments on different datasets seem to show this is helpful. There are theoretical results that discuss some of the features of the SE method."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* The motivation to question scale of datasets is interesting. \n* SE method is simple enough. It can be useful when used in appropriate tasks."}, "weaknesses": {"value": "* **Unclear hypothesis and motivation:** The authors set out to address two main questions: \"data efficient deep learning\" and to show how \"redundancy hurts, and diversity helps\". However, it is unclear how the rest of the paper -- particularly the empirical evidence is proving this? Where are the results for showing data efficiency? The authors just mention \"reducing computation time by factors of 2x to 11x\". Where are the experiments for this? \n\n* **Double claiming novelty:** The authors say  they propose a new method \"selective embedding\" which is based on existing work referenced [1]. Are the authors claiming this current work does a variation of SE in [1]? It is very unclear, and ethically problematic to claim novelty over existing work.\n\n* **Theoretical claims are confusing:** I could not understand the point of Sec. 4 with theory that is very confusing. The results in Sec. 4 are disconnected, to say the least. It is also unclear how these theoretical results inform the experiments reported later in the paper. How are these theoretical insights evaluated in the specific experiments performed?  \n\n* **Experiments do not support the claims:** Results in Table 1 and 2 are not very informative. It is unclear as to what the datasets are in Table 1. And in Table 2, are the authors comparing with the original Chexpert dataset? Or just the train-val-test using 1000 images. If so, how can they claim that SE helps to achieve similar performance when they are not comparing the same data splits?\n\n* **Missing baselines/ improper experimental design:** There are simple baselines like random sampling or other coreset methods that have been studied extensively to improve data efficiency. There are no other baseline methods reported. I would be curious to see how SE fares compared to simple methods. \n\n### References \n\n[1] Mert Sehri, Z. Hua, Felipe de A. Boldt, and Patrick Dumond. Selective embedding for deep learning. 2025.\n\n## Other comments\n\n* What is the point of Sec 4.6.1? How are these assumptions relevant here? What is $\\beta$ mixing with sub-Gaussian loss in this problem setting?\n* Dataset is not clearly explained. What are the \"heavy machinery\". \"Railway\", and \"manufacturing\" datasets? \n\n\n* What do the authors mean by the second sentence in the Abstract\n> This work has consisted of connecting encoders and decoders through attention mechanisms\nor having multi-layer perceptrons. \n\nAnd how is this related to the claims about \"bigger is better\" paradigm.  \n\n* Sec 4 is trivially split into subsections. Hampers readability"}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oXzYQR7y1W", "forum": "wAb8vtEZfM", "replyto": "wAb8vtEZfM", "signatures": ["ICLR.cc/2026/Conference/Submission11955/Reviewer_j8jt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11955/Reviewer_j8jt"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761821723753, "cdate": 1761821723753, "tmdate": 1762922955164, "mdate": 1762922955164, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a detailed theoretical introduction and experimental verification for selective embedding (SE). Both theory and experiment are comprehensive and thorough. Specifically, the manuscript claims that the method of data loading is crucial. By replacing the simple random shuffle strategy with strategies such as modality alternation and oversampling, the accuracy of the model can be improved. In particular, the manuscript provides several theoretical explanations for the effectiveness of the SE method, including enhancing diversity, increasing the effective sample size, and reducing the covariance between samples. Overall, although the method is not novel, the theory is detailed, the experiments are thorough, and the manuscript is well-organized and easy to follow."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The manuscript is well-organized and is easy to follow and understand\n- The theory is simple yet sound and comprehensive.\n- The experimental results cover a wide range of fields including medicine and heavy industry, with a significant span."}, "weaknesses": {"value": "- The issue that concerns me the most is that the work claims that the size of the dataset is not important, but this seems inconsistent with the content of the work. The focus of this work appears to be on improving the efficiency and accuracy of optimization through an optimized data loader. This does not seem to conflict with the size of the dataset itself. In other words, I think the work should prove that using the SE strategy can improve task accuracy and data efficiency, rather than reducing the size of the used dataset.\n- Although the author may be concerned that the excessive introduction of the SE method might affect the innovation of the paper, I believe that the main focus of this work should be on theoretical explanations and extensive experimental verification. Therefore, the specific strategies of SE should be presented in detail. This aspect is notably lacking in the current manuscript; Chapter 3 is too short and it is difficult to follow.\n- Considering that most ICLR readers may not be familiar with the fields of heavy industry and medicine, I suggest that the author provide more detailed information about the datasets. From the current perspective, at least the validation set for medical images is very small, consisting of only a few hundred images. This significantly reduces the credibility of the experimental results.\n- I approve of the theoretical explanation of this work, although it is simple, it is comprehensive and sound. However, theoretical results should be verified through experiments. For instance, if the author claims that SE has improved the optimization efficiency, then a training curve should be drawn to demonstrate this.\n- Although the current experimental results cover heavy industry and medicine. However, the result is singular, with only the precise result. The author should provide more validations based on the theoretical results. For instance, the changes in gradients (variances), individual ablations for SE method components (such as oversampling and data source alternation), and validations of generalization.\n- Despite all the aforementioned flaws, I still appreciate the concise and effective work. I observed a similar approach in JMP (a molecular and materials-based model from Meta), so I am confident in the effectiveness of this work. However, from the perspective of the ICLR conference, I believe that this work is lacking in considerations of the main claimed points, the richness of the visual content, the verification of various theoretical results, and the completeness of the experimental setup. Even though it might be difficult, if the author can fully address the issues mentioned, I will increase my score."}, "questions": {"value": "1. I need to see more validation of the theoretical results, such as the entropy, gradient covariance, and effective sample size mentioned in the text. Just having high accuracy is not enough for me to believe the multiple theoretical results presented in this paper, which is one of the most significant contributions of this paper.\n2. The selection of the data set for the paper seems rather unreasonable. For instance, the validation set for medical images appears to be very small, and it seems that the test set has not been mentioned? In such a small dataset, if possible, cross-validation or increasing the size of the validation set should be adopted. Considering that the paper claims a significant improvement in accuracy, I believe it is crucial to illustrate this point in order to enhance the credibility of the experiment.\n3. I suggest that the author provide a more detailed explanation of the methods used in the paper. The current Section 3 is too short. Good work should openly acknowledge the limitations and innovations of the research.\n4. As a suggestion, the author seems to deliberately aim to illustrate the universality of the SE method. If so, I suggest that there should be at least three application scenarios, and they should not be scenarios like heavy industry which are relatively rare. Even so, I think the author should focus more on providing more thorough experimental evidence to support his theoretical conclusions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TXvEP8WtFR", "forum": "wAb8vtEZfM", "replyto": "wAb8vtEZfM", "signatures": ["ICLR.cc/2026/Conference/Submission11955/Reviewer_vPPQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11955/Reviewer_vPPQ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762262598822, "cdate": 1762262598822, "tmdate": 1762922954506, "mdate": 1762922954506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}