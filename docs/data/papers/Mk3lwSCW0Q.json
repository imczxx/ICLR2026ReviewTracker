{"id": "Mk3lwSCW0Q", "number": 13101, "cdate": 1758213598280, "mdate": 1759897464925, "content": {"title": "Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration", "abstract": "As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length.", "tldr": "", "keywords": ["Multi-agent systems", "long context modeling", "context window", "multi-agent collaboration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b3721d277a11b58112f05c5a5963b4a9888cc753.pdf", "supplementary_material": "/attachment/e93b85851762e1d931aa6bb3d74e9de2b9429600.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Graph-of-Agents (GoA), a method that begins by semantically clustering text chunks. For each cluster, it then constructs a path graph by assessing the mutual information between the chunks and the specific input question. An agent component sequentially summarizes the documents along this path graph. Finally, the aggregated summary is passed to a Large Language Model to generate the answer."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The core strength of this work lies in its input-dependent graph construction. From a compression perspective, this approach dynamically measures the relevance between the question and the incrementally built summary, aiming to achieve efficient information compression and long-context question answering."}, "weaknesses": {"value": "1. Despite the \"Graph\" nomenclature, the resulting structure is a \"path graph\"‚Äîa linear list. This architecture is structurally similar to a \"Chain of Agents,\" which significantly diminishes the novelty and the claimed advantages over simpler, chain-based methods.\n\n2. The path graph construction process appears computationally intensive. Even with a greedy algorithm, adding each new node incurs a computational complexity of o(N), where N is the total number of chunks. This overhead could be very high for long contexts.\n\n3. The algorithm's objective function is based on the similarity between the LLM's summary and the question. However, the underlying assumptions seem overly idealized, casting doubt on the method's practical feasibility. For instance, LLM may inevitably omit critical, question-relevant details during its summarization pass. Furthermore, GoA also does not account for cases requiring the synthesis of information from multiple, non-sequential chunks.\n\n4. The entire graph is question-dependent and offers no reusability. A new graph must be constructed de novo for every single query, which severely limits the method's practicality and scalability in real-world applications."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T7zTPSyfNo", "forum": "Mk3lwSCW0Q", "replyto": "Mk3lwSCW0Q", "signatures": ["ICLR.cc/2026/Conference/Submission13101/Reviewer_ZUtv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13101/Reviewer_ZUtv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489986657, "cdate": 1761489986657, "tmdate": 1762923828281, "mdate": 1762923828281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes model-agnostic long-context modeling as a compression problem and derives a principled objective via mutual information, then instantiates it with GoA‚Äîa dynamic, input-dependent linear forest of collaborating worker LLMs plus a manager. The graph is built by clustering semantically related chunks and greedily ordering within each cluster to maximize semantic alignment between the evolving summary and the query, motivated by InfoNCE/PMI connections. Empirically, on six LongBench QA tasks with Llama-3.1-8B and Qwen3-8B, GoA improves averaged F1 over strong RAG and Chain-of-Agents baselines and, notably, with a 2K window surpasses Llama-3.1-8B with a 128K window. Figures and Table 1 support these gains and ablations show the importance of semantic, contextual next-chunk selection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* This paper is clear writing and easy to follow.\n\n* Across two backbones and six datasets, GoA consistently matches or beats RAG/CoA at the same context length; with 2K context it even edges the 128K vanilla model"}, "weaknesses": {"value": "* Experiments focus on LongBench QA; claims about ‚Äúlong-context modeling‚Äù could be tested on repo-level code understanding, or agentic tasks to assess generality of the compressor.\n\n* While linear forests allow parallelism, the paper does not quantify total token usage, wall-clock latency, or budget vs. RAG/CoA under equal compute constraints. (Algorithm suggests multiple LLM calls.)\n\n* Appendix shows k-medoids underperforms k-means/spectral; the main results fix ùëò=4 and a particular clustering, leaving open how robust performance is under automatic selection of ùëò/method."}, "questions": {"value": "* Did you try simple look-ahead (beam) or submodularity analyses to bound the greedy gap? Even small beams could test whether myopia matters in multi-hop cases.\n\n* Under a fixed token/latency budget, how do GoA, RAG (with reranking), and CoA compare in end-to-end wall-clock and dollar cost, given multiple worker calls + manager? A cost-efficiency plot would be valuable."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "V7sUqdOdor", "forum": "Mk3lwSCW0Q", "replyto": "Mk3lwSCW0Q", "signatures": ["ICLR.cc/2026/Conference/Submission13101/Reviewer_D7yF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13101/Reviewer_D7yF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917442073, "cdate": 1761917442073, "tmdate": 1762923827979, "mdate": 1762923827979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose Graph-of-Agents (GoA), a framework for long context question answer tasks. In GoA, the input text is first split into chunks, then clustered according to their embedding. Next, each worker agent working on each cluster is given these chunks and is asked to summarize them in order of similarity of the chunk and the current summary and query. Finally, a manager agent reads the results and generates the answer. Results on 6 datasets with 2 models show that the proposed method surpasses RAG and CoA baselines on average."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Long context task is an important and urgent issue in LLMs.\n2. The results show that the average performance is better than baseline."}, "weaknesses": {"value": "1. The biggest concern is novelty and contribution. Compared with the baseline, the new component is basically clustering the chunks of document into different groups and assigning them to multiple Worker agents. However, such a method is widely used in various papers (document clustering works). Besides, cluster-then-process only changes the reading order of the baseline method, which it difficult to recognize it as a significant contribution of the method.\n\n2. The improvement is unjustified. Since many results are lower than the baseline, it is important to carefully conduct an analysis of why it is lower. On the other hand, some case studies on why the proposed method is better than the baseline are also needed. Without these analyses, it is not very convincing to believe the proposed method works well.\n\n3. The experiments are not solid. Some baselines can be added to enhance the results. Such as LongAgents, which is also a tree structure. Also, it is not very clear about the effectiveness of each component in the framework, such as an ablation study for random order GoA or random clustered GoA. Furthermore, the motivation for using clustering of documents can be tested, for instance, whether clustering can break the natural order of the document, and it is not certain if there really exist some topics for each cluster, or just a mixture of things in each chunk."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lNi5Z6z4nR", "forum": "Mk3lwSCW0Q", "replyto": "Mk3lwSCW0Q", "signatures": ["ICLR.cc/2026/Conference/Submission13101/Reviewer_EDBe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13101/Reviewer_EDBe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979051521, "cdate": 1761979051521, "tmdate": 1762923827578, "mdate": 1762923827578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Graph of Agents (GoA), a multi-agent framework for long-context reasoning in LLMs. Instead of a fixed Chain of Agents (CoA)‚Äîa serial pipeline where each agent summarizes one chunk‚ÄîGoA dynamically constructs a graph (specifically, a linear forest) of agents based on semantic similarity among chunks. Each subgraph processes a semantically coherent subset of text, and all run in parallel; their outputs are later combined by a manager agent. Theoretical grounding is provided: for a Bayes-optimal predictor, maximizing expected log-likelihood equals maximizing mutual information between compressed input and answer, motivating semantic information preservation as the key compression criterion. Empirically, GoA outperforms baselines (vanilla, RAG, CoA) on LongBench QA tasks using small-context LLMs, suggesting improved effective context length. However, dynamic graph construction introduces non-trivial overhead not measured in latency terms, leaving open the practical speed-accuracy trade-off."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ principled design: it derives the agent architecture from an information-theoretic formulation rather than heuristics. The proposed GoA generalizes prior CoA structures, introducing dynamic, input-dependent collaboration among agents. This enables better modeling of non-linear information dependencies in long documents. \n+ Empirical results demonstrate consistent accuracy improvements across six QA datasets, even outperforming models with much larger context windows. The authors also provide meaningful ablations (embedding models, clustering strategies, contextual vs. lexical search) that validate design choices. The method‚Äôs modularity makes it model-agnostic‚Äîapplicable to any base LLM without fine-tuning‚Äîand it is theoretically elegant in unifying mutual-information maximization with graph-structured collaboration."}, "weaknesses": {"value": "- GoA‚Äôs performance depends heavily on the semantic embedding and clustering quality; if embeddings fail to capture query relevance, summarization paths degrade. \n\n- The dynamic graph construction‚Äîcomputing embeddings, clustering, building subgraphs, and launching multiple agent calls‚Äîintroduces computational overhead. Now in section 4.1, the authors claim that Parallelization \"significantly\nreducing latency\". I find this hard to believe for the reason mentioned. For this claim, I would encourage the authors to to report latency or time-to-first-token measurements."}, "questions": {"value": "Proposition 1 Motivation: The Bayes-optimal predictor result equating log-likelihood and mutual information seems mainly illustrative. Is there a deeper analogy to be interpreted?\n\nClarification: Section 5.1 datasets mentions \"The datasets range from 3.6K to 18K tokens,..\", but table-1 references 128-K?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sS9Kx7KaVq", "forum": "Mk3lwSCW0Q", "replyto": "Mk3lwSCW0Q", "signatures": ["ICLR.cc/2026/Conference/Submission13101/Reviewer_8Ngo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13101/Reviewer_8Ngo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13101/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996200160, "cdate": 1761996200160, "tmdate": 1762923827284, "mdate": 1762923827284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}