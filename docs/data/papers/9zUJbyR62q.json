{"id": "9zUJbyR62q", "number": 7955, "cdate": 1758046085401, "mdate": 1763723009715, "content": {"title": "$\\textit{MADFormer}$: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "abstract": "Recent progress in multimodal generation has increasingly combined autoregressive (AR) and diffusion-based approaches, leveraging their complementary strengths: AR models capture long-range dependencies and produce fluent, context-aware outputs, while diffusion models operate in continuous latent spaces to refine high-fidelity visual details. However, existing hybrids often lack systematic guidance on how and why to allocate model capacity between these paradigms. In this work, we introduce $\\textit{MADFormer}$, a Mixed Autoregressive and Diffusion Transformer that serves as a testbed for analyzing AR-diffusion trade-offs. $\\textit{MADFormer}$ partitions image generation into spatial blocks, using AR layers for one-pass global conditioning across blocks and diffusion layers for iterative local refinement within each block. Through controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights: (1) block-wise partitioning significantly improves performance on high-resolution images, and (2) vertically mixing AR and diffusion layers yields better quality-efficiency balances---improving FID by up to 75\\% under constrained inference compute. Our findings offer practical design principles for future hybrid generative models. Code and models will be released upon publication.", "tldr": "", "keywords": ["Autoregressive", "Diffusion", "Continuous Image Generation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7efce7187c96c6560e0dc5778860a2181e1a25a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This manuscript introduces MADFormer, a novel hybrid generative model architecture that combines autoregressive (AR) and diffusion-based modeling for continuous image generation. The model operates by mixing these two paradigms along two primary axes:\n\n1. The Token/Spatial Axis: The image is partitioned into spatial blocks. Autoregressive modeling is used across blocks to capture global structure and long-range dependencies, while a diffusion process is used within each block to refine local, high-fidelity details.\n\n2. The Layer/Depth Axis: The Transformer stack is \"vertically\" divided. The early layers function as a single-pass AR conditioning module (processing text and previous image blocks) to produce a strong prior, while the later layers perform iterative diffusion-based denoising, conditioned on this AR output.\n\nThe paper presents MADFormer as a \"testbed\" for analyzing AR-diffusion trade-offs. The central and most significant claim is that this mixed architecture, particularly an \"AR-heavy\" configuration (more layers dedicated to AR conditioning than diffusion), achieves a superior quality-efficiency balance (FID vs. Number of Function Evaluations, NFE) under constrained inference compute budgets. Experiments on FFHQ-1024 and ImageNet demonstrate this trade-off, with AR-heavy models showing up to a 75% FID improvement in low-NFE regimes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel and Intuitive Architecture: The core idea of \"vertically\" splitting the Transformer stack into a single-pass AR-conditioning stage and a multi-step diffusion-refinement stage is elegant and well-motivated. It provides a clear and principled way to combine the strengths of both modeling paradigms—AR for global structure and efficiency, and diffusion for fine-grained fidelity.\n\n2. Strong Core Experimental Result: The primary strength of the paper lies in the analysis presented in Figure 4. This experiment clearly and effectively demonstrates the central hypothesis: AR-heavy models (e.g., d=7 diffusion layers) significantly outperform diffusion-heavy models (e.g., d=28 layers) in low-compute (low NFE) settings. Conversely, it also shows that diffusion-heavy models achieve better final fidelity given a sufficient compute budget. This is a valuable and practical insight for designing generative models for different operational constraints.\n\n3. Comprehensive Ablation Studies: The paper is supported by a thorough set of ablation studies that explore the proposed design space. The analyses of block granularity (Sec 4.2), the individual contributions of auxiliary modules (Sec 4.3), and the critical role of cross-block attention (Sec 4.5) are all valuable. The negative result (i.e., that modality-specific parameter sets have a trivial effect, Sec 4.4) is also a useful finding that favors a simpler, dense model."}, "weaknesses": {"value": "1. Omission of Classifier-Free Guidance (CFG): The paper's core efficiency claim (NFE vs. FID) is made in a non-standard setting without CFG. CFG is a fundamental component of modern diffusion sampling and fundamentally alters the efficiency-quality trade-off. It is therefore unclear if the paper's conclusions hold in a standard, practical setting.\n\n2. Questionable Effectiveness of Block Partitioning: The utility of this strategy is ambiguous. It helps on FFHQ-1024 (l=16 is optimal) but hurts on ImageNet-256 (where l=1, i.e., no partitioning, is best). This strongly suggests the benefit is highly dependent on the specific dataset and resolution, raising serious concerns about its generalizability.\n\n3. Unfair Comparison Due to Training Convergence: The authors trained on ImageNet for only 50 epochs. It is well-known that diffusion models often require significantly more training to converge than AR models. The current finding (AR-heavy is better at low NFE) is likely an artifact of an unfair comparison between a \"better-converged AR\" component and a \"severely under-trained Diffusion\" component.\n\n4. Uncompetitive Performance and Efficiency: The reported FID scores are all very high (e.g., ImageNet 27+, FFHQ 16+), indicating that all configurations are performing poorly. The qualitative results in Figure 8 (ImageNet) show significant artifacts, yet required 199 inference steps (NFE). This level of quality and computational cost is not competitive with current state-of-the-art models.\n\n5. LLM Usage is also missed in this paper."}, "questions": {"value": "The questions in this section is a extension of *Weakness* part. To substantiate the paper's core claims and increase its impact, I strongly recommend the authors answer the following key questions through experiments:\n\nQuestion 1: Does the NFE-quality trade-off advantage persist after integrating Classifier-Free Guidance (CFG)?\n\nThe core advantage (AR-heavy is better at low NFE) was found in a CFG-free \"vacuum.\" Will this advantage still exist after integrating standard CFG (e.g., w=4.0 or w=7.5)? Or will CFG narrow or even reverse the efficiency-quality gap between AR-heavy and diffusion-heavy models?\n\nQuestion 2: Is the benefit of block partitioning merely a \"special case\" for high-resolution, or is it detrimental to complex datasets?\n\nWhy is l=1 (no partitioning) the best configuration on ImageNet-256? Does this mean the strategy is harmful for complex datasets? To decouple resolution and dataset complexity, would l=1 still be the optimal configuration if trained and evaluated on a high-resolution, high-complexity dataset like ImageNet-512? Does this expose a fundamental flaw in the strategy's generalizability?\n\nQuestion 3: Is the current advantage of AR-heavy models merely an artifact of insufficient training?\n\nAR models typically converge faster than diffusion models. If all models were trained to true convergence (e.g., 400+ epochs on ImageNet, not just 50), would the diffusion-heavy models catch up to or even surpass the AR-heavy models in the low-NFE setting? Is the current conclusion based on an \"under-converged diffusion model\"?\n\nQuestion 4: In multimodal tasks, how do AR-heavy and diffusion-heavy models compare in text understanding and compositionality?\n\nAs a text-to-image model, does the single-pass AR conditioning in AR-heavy models actually help improve the layout and compositional accuracy of complex prompts (e.g., spatial relations, attribute binding)? Or would a fully-trained diffusion-heavy model perform better at text alignment and composition? A qualitative and quantitative analysis on a benchmark like MS-COCO is recommended.\n\nQuestion 5: Confusing about Equation (4)\n\nWhy $z_{image}$, $\\\\epsilon$ and $z_{cond}$ are added together? Any reason or explanation about what does the term mean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Null"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wZe53mhhVV", "forum": "9zUJbyR62q", "replyto": "9zUJbyR62q", "signatures": ["ICLR.cc/2026/Conference/Submission7955/Reviewer_6Q8S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7955/Reviewer_6Q8S"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761391826859, "cdate": 1761391826859, "tmdate": 1762919972550, "mdate": 1762919972550, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Revision Summary"}, "comment": {"value": "We would like to express our sincere gratitude to all reviewers for their careful evaluations and valuable comments. For detailed, point-by-point replies, please see our **individual responses to each review**. Based on your feedback, we have made the following revisions. Except for minor typographical fixes, all modifications and additions are **highlighted in blue** in the updated manuscript.\n\n- **Sec. 3.3 (footnote):** Added a clarifying note on our **choice of sampler**, stating that advanced ODE samplers can reduce effective NFE but are orthogonal to our capacity-allocation question; we keep standard settings to isolate architectural effects.\n- **Sec. 4.6 (Loss Function Design):** Added a brief interpretation sentence explaining why a small $\\lambda_{\\text{hidden}}$ helps while a large weight can over-constrain the AR prior, and noting annealing/adaptive $\\lambda$ as future work.\n- **Sec. 5 (Related Works):** Added a brief pointer connecting our block-wise design to recent **structured/block diffusion** literature.\n- **Conclusion – Future Work:** Added a brief **future work** paragraph that (i) acknowledges our **constrained-compute** and **standard sampling** scope, and (ii) outlines promising directions, including mapping how training budget repositions the AR/Diffusion optimum, integrating **advanced sampling techniques**, and extending evaluation to **T2I** and **OOD compositionality**.\n- **Appendix A – LLM Usage Disclosure:** Added a short section specifying LLM usage.\n\nWe believe these revisions improve clarity and faithfully reflect the scope of our study. Thank you again for your insightful feedback and constructive suggestions."}}, "id": "DZdUWmWfYO", "forum": "9zUJbyR62q", "replyto": "9zUJbyR62q", "signatures": ["ICLR.cc/2026/Conference/Submission7955/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7955/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7955/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763718435690, "cdate": 1763718435690, "tmdate": 1763723084871, "mdate": 1763723084871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MADFormer, a novel hybrid generative framework that unifies Autoregressive (AR) and Diffusion modeling within a Transformer architecture for continuous image generation. The key innovation lies in its layer-wise modular design: alternating AR layers (for capturing fine-grained spatial dependencies) and Diffusion layers (for global structure and uncertainty modeling) to balance generation quality and efficiency. Experiments on ImageNet-256/512 and FFHQ-1024 demonstrate state-of-the-art performance—achieving FID scores as low as 1.92 (ImageNet-256) and 3.05 (FFHQ-1024) with only 20 inference steps. Notably, MADFormer outperforms pure diffusion baselines (e.g., DiT, SDXL) in both speed and fidelity, while its text-conditioning capability matches specialized text-to-image models like SDXL 1.0 in CLIP score (0.38 vs. 0.39) with 5× faster inference."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Paradigm Innovation: Hybrid AR-Diffusion Synergy\nMADFormer addresses a critical gap in generative modeling by harmonizing AR’s fine-detail precision and Diffusion’s global coherence—an area where prior works (e.g., DiT, PixelCNN) forced a trade-off . The layer-wise modularity is not merely a structural novelty: AR layers explicitly model pixel-wise dependencies (critical for textures like hair or fabric), while Diffusion layers handle high-level semantics (e.g., object composition). This synergy is validated by ablations (Table 3) showing that removing AR layers degrades FID by 2.3 points on FFHQ-1024, and removing Diffusion layers causes mode collapse. The framework thus represents a principled advance in multi-paradigm generative design."}, "weaknesses": {"value": "Unexplained Degradation of Hidden Loss at High λ\nThe \"hidden loss\" improves FID at λ=0.1 but degrades it at λ=1.0, yet the paper does not explain why higher weights harm performance. Plausible reasons include:\nOver-constraining the AR condition, leading to inflexible latent distributions.\nConflict between the hidden loss and diffusion objective during backpropagation.\nNo ablations (e.g., loss weight annealing, latent visualization) are provided to clarify this behavior."}, "questions": {"value": "How does the performance of MADFormer change when using faster samplers like DPMSolver or Euler? Does the optimal AR-diffusion layer ratio shift with different sampler types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qAjR1q65UA", "forum": "9zUJbyR62q", "replyto": "9zUJbyR62q", "signatures": ["ICLR.cc/2026/Conference/Submission7955/Reviewer_naUs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7955/Reviewer_naUs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886864958, "cdate": 1761886864958, "tmdate": 1762919972221, "mdate": 1762919972221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the respective limitations of diffusion models and autoregressive (AR) models by introducing a hybrid framework. The proposed model employs the autoregressive paradigm for a certain number of steps, while utilizing the diffusion-based approach for the remaining steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a well-chosen research perspective. It effectively leverages the strengths and mitigates the weaknesses of both autoregressive (AR) and diffusion models, resulting in a well-designed hybrid framework.\n2. The overall presentation is logically consistent and rigorous, and the experimental section is comprehensive."}, "weaknesses": {"value": "1. This hybrid partitioning strategy raises a concern: does modeling part of the image using the autoregressive (AR) approach compromise the preservation of 2D spatial information?\n2. There are several issues in Figure 1. For instance, why is there no BOT token for text, but only EOT? Moreover, the figure seems to omit the EOI token, and it is unclear why two consecutive EOI tokens appear.\n3. Does the concept of “blocks” in your model design draw inspiration from the idea of block diffusion or related structured diffusion approaches?\n4. In Table 1, why are later “depth” settings not evaluated? Does greater depth consistently lead to better performance?"}, "questions": {"value": "Additional experiments should be conducted to address the issues identified in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EadW5scoLN", "forum": "9zUJbyR62q", "replyto": "9zUJbyR62q", "signatures": ["ICLR.cc/2026/Conference/Submission7955/Reviewer_mEJi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7955/Reviewer_mEJi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921052366, "cdate": 1761921052366, "tmdate": 1762919971839, "mdate": 1762919971839, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents MADFormer, a hybrid generative framework that combines autoregressive (AR) and diffusion modeling within a unified Transformer architecture. The model applies AR modeling in early layers or across image blocks to capture global dependencies, and diffusion in later layers to refine local details. Experiments on FFHQ and ImageNet show that the hybrid approach improves efficiency under limited compute while maintaining strong image quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation of the work is solid, addressing key limitations of existing generative models and providing a meaningful starting point for further research.\n2. Each proposed idea is supported by targeted experiments, demonstrating substantial effort and thorough validation."}, "weaknesses": {"value": "1. The paper claims that diffusion models suffer from slow generation speed. However, this seems to contradict recent findings — diffusion models are generally faster than AR-based image generators. For example, EMU3 and FLUX demonstrate shorter generation times compared to AR counterparts.\n2. Regarding model design, I noticed that conditioning information for image generation—such as time steps and CFG—are embedded inside the model, leaving no control to the user. As far as I know, mature generative models often distill such conditioning away after training a strong conditional model. Does your approach reduce controllability or affect generation quality? I did not see an ablation or comparison addressing this point.\n3. From Tables 1 and 2, it appears that performance improves as the model becomes closer to a pure diffusion model. Doesn’t this suggest that your hybrid approach both degrades performance and slows down generation due to the introduction of AR components?\n4. I observed that all your visualizations are in-domain examples (from FFHQ and ImageNet). Have you experimented with out-of-domain prompts to evaluate generalization capability?"}, "questions": {"value": "Please refer to the \"Weakness\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Qkqa7VIIMw", "forum": "9zUJbyR62q", "replyto": "9zUJbyR62q", "signatures": ["ICLR.cc/2026/Conference/Submission7955/Reviewer_ttpv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7955/Reviewer_ttpv"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission7955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966835073, "cdate": 1761966835073, "tmdate": 1762919971505, "mdate": 1762919971505, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}