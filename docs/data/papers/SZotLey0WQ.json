{"id": "SZotLey0WQ", "number": 14884, "cdate": 1758245031586, "mdate": 1759897343604, "content": {"title": "$S^{2}$-FracMix: Self-Saliency Fractal Mixup", "abstract": "Data augmentation methods have shown impressive performance in learning training data distributions to minimize the generalization gap. Recently, these approaches have been replaced by adversarial mixup methods to produce online mixed samples to improve robustness and generalization of deep neural networks. In addition, previous saliency-based methods simply extract the salient region from the source image and paste it into target image. Although these approaches improve performance, they may introduce unreliable samples during training in addition to substantial computational overhead. In this paper, we introduce a Self-Saliency ($S^2$) mixup  method that creates challenging samples by extracting only salient patches at varying scales and places back into the non-salient regions of the same image. The aim is to learn scale-invariant features to improve generalization with less computational overhead. Also, to improve resilience against adversarial perturbations, we propose a new approach \\textit{FracMix} which only mixes self-similarity pattern into salient patches with different mixing ratios. Our proposed $S^{2}$-FracMix enables the model to learn from both fractal and non-fractal structures simultaneously within a single training image, offering a more targeted and label-consistent form of augmentation. The proposed $S^{2}$-FracMix demonstrates state-of-the-art performance on seven datasets including coarse and fine-grained classification, robustness against corruption, calibration, contrastive learning, object detection, data scarcity ($5$, $10$, and $100$ shots), and transfer learning compared to the existing state-of-the-art methods.", "tldr": "", "keywords": ["data augmentation", "mixup", "image classification", "transfer learning", "object detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f57423e59cb775a81f0a5d60eca65aaca8c1cf40.pdf", "supplementary_material": "/attachment/e230fac50ad0240666cebc11ff13c1a3cc5e3a96.pdf"}, "replies": [{"content": {"summary": {"value": "S2-FracMix introduces a novel data augmentation framework that integrates self-saliency mixing with fractal texture injection to enhance model robustness against local deformation and texture perturbation. The method is structurally simple and highly generalizable—it requires no additional data, complex optimization, or architecture modification, and can be easily applied to both CNNs and Vision Transformers. By mixing self-salient patches and injecting fractal structures, S2-FracMix preserves semantic consistency while significantly enriching feature diversity and improving generalization. Extensive experiments across multiple datasets and tasks demonstrate consistent and competitive performance, highlighting its strong practical value."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tS2-FracMix is lightweight and easy to implement. It does not rely on additional data, external generators, or complex optimization procedures. The method can be seamlessly integrated into CNN or Transformer training pipelines with minimal computational overhead, making it highly practical and broadly applicable.\n2.\tThe use of self-saliency ensures that the augmentation process preserves the main semantic content of an image. Meanwhile, the self-mixing of salient patches and fractal injection enrich local variations in scale, shape, and texture. Combined with multi-mode mixing, this produces more diverse and semantically stable samples, significantly improving model generalization and robustness.\n3.\tThe authors evaluate S2-FracMix across multiple benchmark datasets and compare it with various state-of-the-art augmentation methods. It consistently achieves better classification accuracy, robustness, and calibration, and performs well in supervised, semi-supervised, and transfer learning settings — demonstrating stable and reliable performance across tasks."}, "weaknesses": {"value": "1.\tThe effectiveness of S2-FracMix heavily relies on the accuracy of the saliency map. If salient regions are detected incorrectly, the mixing process may disrupt semantics or produce suboptimal augmentations, leading to unstable performance in noisy or complex data scenarios.\n2.\tThe fractal injection strength, controlled by the parameter λ, must be carefully tuned. Excessive injection may distort local textures or corrupt fine-grained semantics, particularly in detailed classification tasks. Proper parameter selection is essential to balance augmentation strength and semantic fidelity.\n3.\tA main limitation of the paper is that it does not clearly verify the true source of its performance gains. Although results are strong, the authors attribute improvements mainly to fractal injection without isolating its effect or comparing alternative designs, such as using salient shapes from other images with the current image’s textures. Without such ablations, it remains unclear whether the advantage comes from the fractal patterns themselves or simply from increased structural diversity."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tCOtg4O7Gl", "forum": "SZotLey0WQ", "replyto": "SZotLey0WQ", "signatures": ["ICLR.cc/2026/Conference/Submission14884/Reviewer_njcS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14884/Reviewer_njcS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760506740217, "cdate": 1760506740217, "tmdate": 1762925232706, "mdate": 1762925232706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new data augmentation method combining saliency-based mixing and fractal mixing. The effectiveness of the proposed method is verified in 7 datasets with different tasks showing higher performance compared to the exisiting methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper firstly incorporates the saliency mixing and fractal mixing to improve DA performance.\n2. Intensive experiemts are performed in various datasets and tasks, making the proposed method convincing.\n3. The proposed method is very fast while showing the best performance in all the tasks in the paper."}, "weaknesses": {"value": "1. Although the proposed method reveals better performance, the novelty seems limited. That is, saliency, fractal mixing methods already exisit and this method combined them sequentially. \n\n2. Augmented samples are not visualized. Especially, factal mixing results for salient patches are not shown in the paper, which limits the possibility of analysis on augmented representations and the generalization power.\n\n3. The motivation is unclear. That is, the fractal mixing was devised for adversarial robustness, and this paper applied it to salient regions. Why do this combination results in better perforamce to clean images (including adversarial robustness)? What is the underlying mechanism of fractal mixing for salient patches in terms of enhancing generalization performance? One can think that data augmentation should consder the trade off between diversity and fidelity for the augmented data. Such analysis is missing which makes this work seem heuristic.\n4. As far as I know. saliency mixing is very fast since it compute the saliency map only for the first image in a mini batch, decide the position to be cropped and attached. This position is applied to all the remaining samples in the batch. How did you measure the computational complexity of it? It should be checked carefully."}, "questions": {"value": "Please see the Weakness and answer the all concerns in it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jEHZTd2oes", "forum": "SZotLey0WQ", "replyto": "SZotLey0WQ", "signatures": ["ICLR.cc/2026/Conference/Submission14884/Reviewer_N3st"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14884/Reviewer_N3st"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761698408730, "cdate": 1761698408730, "tmdate": 1762925232249, "mdate": 1762925232249, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes S2-FracMix, a novel data augmentation method that combines two key ideas: (1) Self-Saliency (S2) mixing, which extracts multi-scale salient patches from an image and reinserts them into non-salient regions of the same image after applying transformations (e.g., rotation, blur); and (2) FracMix, which injects self-similar fractal patterns only into those salient patches to enhance structural diversity while preserving semantic consistency. Additionally, the authors adopt a high-level mixing strategy that randomly selects among several augmentation modes (including Mixup, CutMix, ResizeMix, and S2-FracMix) during training to increase regularization diversity. The method is evaluated across a range of tasks including general/fine-grained classification, robustness to corruption, calibration, few-shot learning, transfer learning, and self-supervised learning on datasets such as CIFAR-100, Tiny-ImageNet, ImageNet-1K, CUB-200, and Stanford Cars. The results consistently show S2-FracMix outperforming prior state-of-the-art mixup methods with lower computational overhead."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Innovative design: The idea of intra-image saliency-guided mixing (S2) is conceptually distinct from prior inter-image saliency methods (e.g., PuzzleMix, Co-Mixup), reducing computational cost while maintaining semantic fidelity. \n* Targeted fractal augmentation: Restricting fractal blending to salient regions (FracMix) avoids the distribution shift caused by global fractal mixing (e.g., in PixMix or DiffuseMix), which is a thoughtful improvement.\n* Strong empirical performance: The method demonstrates consistent gains across diverse tasks and architectures (CNNs and ViTs), including robustness, calibration, and transfer learning, which suggesting broad applicability.\n* Efficiency: The paper convincingly shows lower training time compared to heavy saliency-based methods, making it more practical for real-world use."}, "weaknesses": {"value": "* Limited scale of experiments: While the paper claims generalizability, all experiments are conducted on relatively small to medium-scale datasets (e.g., CIFAR-100, Tiny-ImageNet) and architectures (up to ResNet-50, ViT-B). There is no evaluation on truly large-scale settings, such as full ImageNet-21K pretraining, billion-parameter models, or modern large vision-language models, which are increasingly standard in top-tier vision and learning venues like ICLR.\n* Absence of large model evaluation: The largest backbone tested is ViT-Base. Given the growing importance of scaling in modern ML, the omission of experiments with larger transformers (e.g., ViT-L, ViT-H) or foundation models weakens the claim of broad applicability.\n* Lack of theoretical analysis: The method is presented purely from an empirical and heuristic perspective. There is no theoretical justification for why self-saliency mixing or localized fractal injection should improve generalization or robustness—e.g., no connection to invariance principles, information theory, or optimization dynamics. This limits insight into why the method works and under what conditions it might fail."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MuweluyQ6r", "forum": "SZotLey0WQ", "replyto": "SZotLey0WQ", "signatures": ["ICLR.cc/2026/Conference/Submission14884/Reviewer_Rnnd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14884/Reviewer_Rnnd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761789107055, "cdate": 1761789107055, "tmdate": 1762925231917, "mdate": 1762925231917, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces $S^2$-FracMix, a new data-augmentation framework combining Self-Saliency ($S^2$) and FracMix components to  improve classification, object detection, and few-shot performance, as well as increased calibration and robustness.\n\n$S^2$ extracts multi-scale salient patches from an image and reinserts them into non-salient regions of the same image after simple transformations (rotation, blur), fostering scale-invariant feature learning. FracMix further injects self-similar fractal textures into the salient patches to increase structural diversity and adversarial robustness while preserving semantics.\n\nThe final training pipeline incorporates a mix of multiple mixing schemes, randomly alternating between them during training in order to enrich diversity. The authors find that this improves final performance.\n\nExperiments on seven datasets and multiple tasks show consistent improvements over prior methods such as AdAutoMix and PuzzleMix with a lower computational cost."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates the effectiveness of the proposed method, $S^2$-FracMix, through evaluations across a wide range of tasks, achieving half the training time of similar-performing previous methods. The results show consistent and significant improvements in top-1 accuracy over prior methods.\n\nThe improved robustness and calibration gains resulting from the augmented training are also noteworthy.\n\nThe GradCAM visualization, particularly those in Appendix E with occlusions, is informative and qualitatively shows the framework's effectiveness."}, "weaknesses": {"value": "The paper misses an important citation (Choi et al., 2021) while claiming novelty in mixup using the same training sample (lines 155-157). Choi at. al. (2021) in the previous work SalfMix, quoting their abstract, \"produce a self-mixed image based on a saliency map\". \n\nThe experiments demonstrate only the performance of a transformer-based model (ViT-B) in a single experimental setup: transfer learning on CUB and Stanford Cars. Given that transformers are currently the most common architecture for these use cases, it is important to demonstrate that the results generalize to them as well.\nMoreover, the paper notes that a ViT-B model was trained on ImageNet-1K before being transferred to CUB and Stanford-Cars. This would make the evaluation on the ImageNet-1K validation set, i.e., a ViT-B column in Table 1, trivial. However, these results have not been presented.\n\nUnclear methodology:\nIn my understanding, saliency maps are per-pixel based. Lines 190-192 \"Patches are extracted from the salient region of the input image Ii at np scales P\", what is meant by \"salient regions\" here? How are they computed? How is the number of scales $n_p$ chosen? \n\nHow robust is the performance of the proposed method to the saliency algorithm used (Zhang et al., 2020)? Is the performance still superior when using the same saliency methods as those compared? According to Appendix E.3, this is the primary reason for the improvement in computational efficiency. If so, will the other methods also show the same improvement when using the saliency algorithm in this work?\n\nThe model architecture used in the self-supervised learning experiments is not mentioned.\n\nWriting style:\nSection 3.2 is extremely hard to follow with no supporting methodology figures. I did not understand what $s_k$ in Equation 2 is. In lines 191-192, I also recommend using commas and/or framing the sentence better for clarity.\nIn Algorithm 1, it is unclear what $P_m$ represents.\n\n\nChoi, J., Lee, C., Lee, D., & Jung, H. (2021). SalfMix: a novel single image-based data augmentation technique using a saliency map. Sensors, 21(24), 8444."}, "questions": {"value": "I am not an expert in MixUp-style data augmentation. However, given the omission of SalfMix (Choi et al. 2021) as a citation, the proposed work appears to be a combination of existing methods. Furthermore, SalfMix also incorporates other mixing techniques, such as CutMix, into its pipeline (same as the proposed $S^2$-FracMix), albeit in a different way. Could you please explain whether there is a high-level difference between SalfMix and the proposed $S^2$-FracMix, particularly with respect to mixup within a single training sample?\n\nDo the findings generalize to transform-based architectures? At the very least, a performance comparison of ViT-B on ImageNet1-K should be presented (as the authors should have already trained that model, according to my understanding of the transfer setup).\n\nPlease also clarify how the choice of the saliency algorithm, which is not part of the proposed methodology, affects performance and computational efficiency.\n\nPlease provide clarification on my concerns listed in weaknesses on the $S^2$ self-saliency mixup algorithm.\n\nIn this paper, the authors have, through a well-crafted pipeline and mixup design, demonstrated strong gains in performance across multiple tasks, primarily using ConvNet architectures, which is a positive. However, same-image mixing, incorporating multiple mixup methodologies into a single pipeline, or fractal-based mixing are not novel ideas; and there are missing citations and important experiments, which is the reason for my rating. \nI am open to further discussion and reevaluation based on the authors' responses to my queries."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LjtFo5Up1Z", "forum": "SZotLey0WQ", "replyto": "SZotLey0WQ", "signatures": ["ICLR.cc/2026/Conference/Submission14884/Reviewer_tebB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14884/Reviewer_tebB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14884/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005493262, "cdate": 1762005493262, "tmdate": 1762925231430, "mdate": 1762925231430, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}