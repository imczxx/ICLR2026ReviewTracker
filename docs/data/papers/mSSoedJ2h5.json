{"id": "mSSoedJ2h5", "number": 18484, "cdate": 1758288177546, "mdate": 1759897100532, "content": {"title": "Prover Agent: An Agent-Based Framework for Formal Mathematical Proofs", "abstract": "We present Prover Agent, a novel AI agent for automated theorem proving that integrates large language models (LLMs) with a formal proof assistant, Lean. Prover Agent coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while also generating auxiliary lemmas to assist in discovering the overall proof strategy. It achieves an 88.1% success rate on the MiniF2F benchmark, establishing a new state-of-the-art among methods using small language models (SLMs) with a much lower sample budget than previous approaches. We also present theoretical analyses and case studies that illustrate how these generated lemmas contribute to solving challenging problems.", "tldr": "We present Prover Agent, an AI agent for automated theorem proving that integrates LLMs with Lean and auxiliary lemma generation, achieving 88.1% on MiniF2F, the new SOTA among methods using small language models.", "keywords": ["Agent", "Formal Theorem Proving", "Automated Theorem Proving", "Small Language Model"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b38c46fb3e81bf0d83b3768acb57a86d77ef166b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies a reasoning pipeline where one LLM performs informal reasoning to prove theorems and the result of informal reasoning is then used for write a formal proof for theorems in the miniF2F dataset. The paper reports some accuracy gains using its pipeline. It also provides some analysis about about the performance its framework for ATP."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The methodology is interesting and possibly a useful contribution for the community if the code is released. The main idea of proving a formal theorem using a combination of informal reasoning and a formal theorem prover has been present in the literature, but an open source implementation that can improve the SoTA accuracy will be useful for the community, in my view.\n\n\nPaper reports improving the accuracies although it does not report its computational cost and it remains unclear at what cost better accuracies are gained."}, "weaknesses": {"value": "Goedel Prover V2 8B model reports accuracy of 84.6% pass 32 in their own paper. Prover Agent, however, reports a lower accuracy of 84.4% for the higher sample budget of 50. This indicates either bugs in the implementation and/or other issues in the proposed pipeline. Same model with self correction achieves accuracy of 86.7% pass 32, again higher than the 85.7% that paper reports for its ensemble of two models (DSP V2 + GDP V2) pass 50. Based on these, it appears to me that despite the additional LLM that prover agent utilizes, it does not achieve better accuracies.\n\n\nThe paper has deployed other LLMs for informal reasoning in addition to the original LLM used for theorem proving, and it does not report the token budget, i.e., the additional computational cost, for the extra LLM. Hence, all the claims about computational efficiency and accuracy gains remain in a fog when computational costs are not reported.\n\nReporting only the sample budget is not insightful, in my view, given that paper utilizes an additional LLM. The authors should report token budgets over their entire pipeline and for all their experiments. Without such information, the advantage of the proposed method will be unclear.\n\nUnfortunately, in Table 1, pass 32 accuracies are not reported for all the models. Moreover, for the paper's pipeline accuracies are reported for unconventional number of attempts such as 50, 100, and 260. These inconsistencies make the interpretation of results difficult.\n\nOther benchmarks such as Putnam Bench are not studied.\n\nI find the list of contributions on page 2 unclear, especially when the paper claims: \"The 88.1% success rate was achieved using only SLMs with a much smaller sample budget than previous high-performing approaches.\" The paper is using Goedel Prover V2 and they have improved its accuracy by some margin. The paper should quantify its sample/token budget for that performance and then report what accuracy other models achieve using that same sample/token budgets. This way the claim will be more clear.\n\nThere are inaccuracies in Table 1 and possibly elsewhere in the paper. For example, the small version of Goedel Prover V2 has 8B parameters while paper states 7B.\n\nThe writing of the paper is not entirely smooth and it can be improved."}, "questions": {"value": "Please see weaknesses. I'd be happy to revise my score based on the rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "nZXVfFYvfa", "forum": "mSSoedJ2h5", "replyto": "mSSoedJ2h5", "signatures": ["ICLR.cc/2026/Conference/Submission18484/Reviewer_XyUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18484/Reviewer_XyUv"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889099065, "cdate": 1761889099065, "tmdate": 1762928177192, "mdate": 1762928177192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Prover Agent, an AI agent framework for automated theorem proving that combines large language models with the Lean proof assistant. The system coordinates an informal reasoning LLM, a formal prover model, and feedback from Lean while generating auxiliary lemmas to aid proof discovery. The authors report an 88.1% success rate on the MiniF2F benchmark, claiming state-of-the-art performance among small language model approaches with reduced sample budgets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Addresses an important problem in automated theorem proving by bridging informal and formal reasoning\n- Achieves strong empirical results (88.1% on MiniF2F) with reportedly lower sample complexity than prior work\n- The approach of generating auxiliary lemmas to decompose complex proofs is intuitive and potentially valuable"}, "weaknesses": {"value": "- Theoretical analysis lacks practical relevance (Section 4): The theoretical framework appears disconnected from the empirical contributions. The assumptions made seem uncheckable and are introduced primarily to enable mathematical tractability rather than to provide meaningful insights. The analysis does not quantify the effectiveness of the approach in a way that connects to the experimental results. Unless this analysis can be meaningfully tied to the empirical performance, it should be removed or significantly revised.\n\n- Weak improvement from refinement: Table 1 shows that refinement provides only a marginal improvement of 2-3% over pass@50 without refinement, raising questions about the value added by this component of the system."}, "questions": {"value": "Clarification needed on informal-to-formal guidance: How exactly does the informal natural language proof guide the formal prover models (DeepSeek-Prover, Gödel-Prover)? Section 3.1 mentions using informal proofs \"as contextual guidance,\" but these models operate on formal Lean statements and produce formal proofs. The mechanism for incorporating informal reasoning into the formal proving process needs to be explained more clearly."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zJVvHfGyS9", "forum": "mSSoedJ2h5", "replyto": "mSSoedJ2h5", "signatures": ["ICLR.cc/2026/Conference/Submission18484/Reviewer_QVUy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18484/Reviewer_QVUy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948898952, "cdate": 1761948898952, "tmdate": 1762928176604, "mdate": 1762928176604, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Prover Agent, an agentic LLM-based pipeline for formal theorem proving. Prover Agent first attempts a direct proof with whole-proof generation. When that fails, it first generates a sequence of candidate lemmas, either with sub-goals or special/related cases of the original theorem. These are attempted to be proved with an iterative process using feedback from Lean. Finally, all the successfully generated lemmas feed into a final step of attempting to prove the original theorem again, iterating with Lean feedback. Experiments show improvements on minif2f on top of both DeepSeekProver-V2 and Goedel-Prover-V2, with a final boost gained by ensembling both models. The paper also includes a theoretical analysis that justifies why this decomposition pipeline works under certain assumptions."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a well-motivated pipeline which uses auxiliary lemmas in a flexible and interesting way. Prior work use auxiliary lemmas primarily for subgoal decomposition (e.g., POETRY), whereas Prover Agent can use lemmas to explore special or related cases to the current theorem. While those might not be directly invoked in the final proof, they can help the agent to get signal on what proof strategies seem to work for the given problem. They can also, of course, end up serving as useful lemmas to be invoked in the final proof.\n\nThe paper attempts to formalize the key assumptions for this kind of technique to be effective. While I still find the analysis here a bit vague (see below), I do appreciate this effort, and I think this might motivate future work to propose more principled approaches that come from plausible models of the process of proving mathematical theorems.\n\nThe results on minif2f show a consistent improvement at the smaller model scale (7-8B models) on top of two distinct base models, showing that the approach generalizes across models. The per-step ablation shows that each step in the pipeline is contributing to the final result."}, "weaknesses": {"value": "While the empirical result is generally positive, it is (1) currently limited to minif2f, and (2) small in absolute terms. I think minif2f served well as a benchmark for several years, but the baseline results at all scales have improved significantly, so it might not represent the biggest outstanding challenges in formal theorem proving anymore. For instance, Goedel Prover V2 already achieves 85.2% of success rate in Pass@256. With Prover Agent, this improves to 86.5% (or 88.1% with ensembling with DeepSeekProver, but this should be compared to an ensembling baseline, e.g. 128 attempts with each model). 85.2% --> 86.5% only translates to a handful of minif2f problems (I assume this is 208 --> 211 theorems proved). Thus, this is positive but a bit marginal, given the complexity of the pipeline (whereas the baseline is very simple). I encourage the authors to find other more challenging benchmarks (like PutnamBench, ProofNet, LeanWorkbook, etc) where the improvements from the ideas here might be more significant.\n\nThe paper doesn't provide much insight into the typical structure of a Prover Agent trace in practice. Auxiliary evaluations, such as how often are the proposed lemmas actually used as lemmas in the final proof vs just as illustrative solutions to related problems, would help us get a more concrete sense of how Prover Agent differs in practice from prior methods (like POETRY, or DSP variants) where lemmas are generated to prove subgoals. The example in Appendix E is helpful, but since it's just one example it serves more as an illustrative case right now: it is difficult to know if that's typical or representative. It would have been better if some interpretable behavior (e.g., lemma reuse, or some relationship between successful proofs of lemmas and the final complete proof) could be quantified here. Even just knowing how often the theorem is proved in step 1 would already help.\n\nFinally, the theoretical analysis has a bit of a vague setup. For instance, Assumption 1 is \"For a certain class of theorems, it is necessary to satisfy m essential intermediate facts F1, . . . , Fm\". It's unclear what a \"fact\" is more precisely here, since it's not a lemma (but it's implied that a lemma can either contain a fact or not). Same goes later for a \"proof strategy\" (these look like can be seen as partitions of the set of proofs, though that definition can have issues, since not all informal strategies are mutually exclusive). I understand the gist of the argument for Theroems 4.4 through 4.6 -- if I understand, they basically follow from the gap that is assumed to exist in Assumption 4.3 between proving the complete theorem with and without conditioning on some intermediate facts, and this \"fact covering\" model of proving theorems). But since the idea here is to provide a more formal analysis, it would be better to have more precise definitions to start from, and useful to ground them in one or two examples to show exactly what you mean in practice (e.g., can you spell out plausible underlying facts in the example in Appendix E?)"}, "questions": {"value": "- The base results even with budget 1 (i.e., ¨\"Direct proving w/o iterative refinement\", just the initial proof attempts) are slightly better for Prover Agent compared to the base model results - for instance, 58.6% -> 61.5% with DeepSeek-Prover-V2. Why do you think that is the case? Just better prompts?\n- If you just ensemble the whole proof generation results from DeepSeek-Prover and GoedelProver, does that improve on top of those baselines? That would be a more fair baseline to compare to ensembled Prover Agent.\n- Although you attempted to match the sampling budget in terms of calls to the LLM, how did typical runtimes (or number of output tokens) compare between one run of Prover Agent vs the baseline models? If the whole-proof generation baselines are just sampling N times from the LLM + verifying proofs with Lean, they might be more efficient even if the number of calls to the prover LLM is the same.\n- How exactly does the recursive breakdown of lemmas happen? The paper mentions that lemmas can be further decomposed \"up to a depth limit D\" (L188), but that part does not show up in the pseudo-code. How do you allocate the budget when deciding how far to try to decompose a particular lemma instead of keep refining?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "842t4JND99", "forum": "mSSoedJ2h5", "replyto": "mSSoedJ2h5", "signatures": ["ICLR.cc/2026/Conference/Submission18484/Reviewer_W19d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18484/Reviewer_W19d"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143358483, "cdate": 1762143358483, "tmdate": 1762928176121, "mdate": 1762928176121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The goal of this paper is to use LLMs that are good at natural language proofs in order to generate formal proofs that pass the Lean verifier - LLMs that excel at natural language cannot directly transfer to formal proofs. The natural language model generates auxiliary lemmas, which can be “subgoals”, but can also be special cases or more open-ended intermediate facts.\n\nIn Prover Agent, the natural language agent generates auxiliary lemmas when necessary, if more direct proofs do not work. The technique is as follows.\n1. First, the informal model generates a full natural language proof. Then, the formal model uses that to generate a formal proof. This is repeated a set number of times, and if this does not work, then move on to the next step.\n2. Then, iterative refinement is performed. Beginning with the formal proof from the previous round that has the fewest errors in Lean, refine the proof iteratively. The prover model, in each round, takes the previous proof and the corresponding Lean feedback, and generates a new proof.\n3. If the previous approaches fail, then the natural language model generates some auxiliary lemmas, which can be relatively open-ended, rather than being based on the proof sketch so far. These are then converted into formal statements, and the prover model tries directly proving them.\n4. Then, using the successful auxiliary lemmas as assumptions, try proving the original statement again.\n\nThis work also provides some simplified theoretical analysis, of how using lemmas as assumptions will boost the success rate of the formal proof. Theorem 4.4 essentially says that the difficulty of proving a theorem grows exponentially in the number of basic facts that it depends on (provided that successfully proving each of the basic facts is an independent event). Thus it is advantageous to prove the basic facts in a hierarchical way. Additionally, Section 4.2 models the LM’s response as a distribution over problem solving strategies. Each lemma gives an “observation” of whether a strategy might be effective. The probability of succeeding increases exponentially in the mutual information between the lemmas’ observations and the prior distribution over strategies.\n\nThe results are relatively strong. They evaluate on Mini-F2F with Prover Agent - the informal reasoning model is Deepseek-R1-0528-Qwen3-8B and the formal model is Deepseek-Prover-V2-7B. The success rate on MiniF2F is 88.1%, which is the best result so far among other works with similar model sizes. Additionally, this work presents ablations on various aspects of the Prover Agent technique. Having a large number of initial drafts, in order to select a good initial draft, seems important. If there are only 1 or 10 initial drafts, then later refinements don’t end up helping that much. (In other words, the amount of Lean errors is a useful proxy for selecting the draft.) Iterative refinement is also important, compared to simply increasing the budget with iterative sampling.\n\nThe result in Table 2 is also impressive. Prover Agent can surpass Deepseek Prover V2 671B on the IMO problems in MiniF2F-test, using a much smaller sampling budget, and outperform Goedel Prover V2 using roughly half the sampling budget, albeit using an additional natural language model."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper has very strong results, getting SOTA on MiniF2F among small language models.\n2. The paper is very well-written."}, "weaknesses": {"value": "1. It would be nice to compare more in-depth to other works that also generate auxiliary lemmas in formal theorem proving?\n    1. For example, Seed Prover also performs a somewhat open-ended exploration of potentially useful lemmas.\n    2. In particular the “heavy” inference setting they use has some breadth in terms of the conjectures it generates."}, "questions": {"value": "1. How is the informal proof presented to Deepseek Prover V2, in the direct proving scenario? Does Deepseek Prover V2 accepts informal proofs as inputs in a straightforward way?\n    1. Could you share the prompt used for Deepseek Prover V2 here?\n2. Could you explain why your method is superior to Deepseek Prover V2’s strategy, intuitively?\n    1. Is it because the natural language proof is not visible from the beginning, in your setup? How true is that in your setting - your informal LLM also generates a full proof, right?\n3. Is there any criteria for which lemmas you choose to include in the context for the overall proof? Or do you include all lemmas? (For example, Seed Prover includes lemmas that have a low successful proof rate, which is a proxy for the usefulness of the lemmas.)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qGZAScgChL", "forum": "mSSoedJ2h5", "replyto": "mSSoedJ2h5", "signatures": ["ICLR.cc/2026/Conference/Submission18484/Reviewer_rWUY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18484/Reviewer_rWUY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18484/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762235845997, "cdate": 1762235845997, "tmdate": 1762928175383, "mdate": 1762928175383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}