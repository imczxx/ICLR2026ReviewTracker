{"id": "gpf2dYfVwg", "number": 17699, "cdate": 1758279451447, "mdate": 1763737323398, "content": {"title": "Self-Supervised Learning with Spatiality Preserving Representation for EEG Signals", "abstract": "Self-supervised learning (SSL) has revolutionized the field of deep learning with EEG signals, yet current approaches face a critical limitation: the loss of crucial spatial information due to architectures that fail to adequately preserve the one-to-one electrode relationship between the input and representation. To address this, we introduce Spatiality Preserving Representation (SPR) Learning. Unlike existing methods relying on reconstruction or temporal prediction with separate encoders, SPR learns spatial relationships through an innovative coherence pseudolabel prediction task, teaching models to understand the intricate topographical organization of brain signals that conventional approaches overlook. Through comprehensive evaluation, SPR demonstrates superior performance over state-of-the-art methods (4.7%, 9.7%, 1.6%, and 15.6% fine-tuning improvements over different datasets), learning meaningful spatial representations that capture the complex spatial-temporal dynamics inherent in EEG data. Our work opens new avenues for interpreting the relationships of different brain regions by prioritizing spatial awareness, and thus bridge the gap between functional connectivity analysis and self-supervised EEG representation learning.", "tldr": "We introduce a novel SSL method with a coherence pseudolabel prediction task, teaching models to understand the intricate topographical organization of EEG signals and achieving SOTA results.", "keywords": ["EEG", "brain-computer interface", "self-supervised learning", "representation learning"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/965095ae21e282dbfdfe70b28a3c31034cf0625a.pdf", "supplementary_material": "/attachment/5866aeca2724310deaadfd2b46ea2cc101f6fa15.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes Spatiality Preserving Representation (SPR) Learning, a self-supervised EEG framework that maintains electrode-wise spatial relationships through a novel coherence pseudolabel task. It shows promising performance improvements across multiple EEG datasets, emphasizing the role of spatial awareness in representation learning."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "See the questions"}, "weaknesses": {"value": "See the questions"}, "questions": {"value": "1. While the proposed SPR framework shows strong performance in end-to-end evaluation, it remains unclear how transferable the learned representations are to other downstream decoders. To better demonstrate the utility of the learned embeddings, the authors are encouraged to verify whether SPR representations can improve classification performance when integrated with alternative decoding architectures, validating via mainstream decoding backbones.\n2. The authors include several visualizations of dataset characteristics in the Appendix. However, the paper lacks interpretability analysis on the model’s internal behavior and how it aligns with key assumptions of the framework, such as whether spatially structured representations truly drive the improvements. A more thorough examination of attention maps, coherence heatmaps, or feature attribution could help validate the claimed spatial awareness and further support the design rationale.\n3. The method relies on spectral domain features for pretext target generation, where frequency resolution is tied to the temporal length of EEG segments. All datasets used in this study contain relatively long segments (5–10 seconds), which favor stable frequency estimates. It remains unclear whether SPR is applicable to shorter EEG epochs, where frequency resolution may be insufficient to reliably compute coherence features in meaningful bands. Since many EEG task involve short-duration segments, the method’s effectiveness under limited temporal length should be explicitly evaluated.\n4. The use of cross-power spectral density (CPSD) to characterize inter-channel dependencies is an interesting choice. However, it would be helpful for the authors to justify this choice more explicitly. Given that temporal correlation is a more direct and computationally simpler measure of channel dependency, why was it not considered or compared? Clarifying the trade-offs between CPSD and other spatial metrics would strengthen the paper’s methodological rigor.\n5. The proposed use of a band-mixture approach (α, β, γ) to generate coherence pseudolabels is novel and well-motivated. However, its necessity remains unverified in the main paper. An ablation study isolating the performance of full-band vs. band-mixture pseudolabels would help determine whether mixing bands provides a meaningful benefit or introduces unnecessary complexity.\n6. The authors are strongly encouraged to validate the proposed method on additional datasets with diverse tasks and recording conditions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "9R03rQgoL2", "forum": "gpf2dYfVwg", "replyto": "gpf2dYfVwg", "signatures": ["ICLR.cc/2026/Conference/Submission17699/Reviewer_jRWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17699/Reviewer_jRWD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761542222362, "cdate": 1761542222362, "tmdate": 1762927542666, "mdate": 1762927542666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Spatiality Preserving Representation (SPR) Learning, a self-supervised learning framework for EEG signal analysis that addresses a limitation in most existing methods: the loss of spatial information about electrode relationships. SPR introduces a coherence pseudo-label prediction task that teaches models to understand the topographical organization of brain signals. The method computes magnitude-squared coherence across channel pairs in specific frequency bands (alpha, beta, gamma), excluding lower frequencies that lack spatial specificity. These coherence patterns serve as pseudo-labels for the self-supervised pretext task, forcing the model to learn meaningful inter-channel spatial relationships. The authors report performance improves between 1.6 to 15.6 % across four EEG data sets (STEW, TUEV, TUAB, CHB-MIT). The idea of using spatial coherence as a pretext label is interesting and well-motivated. However, the overall novelty appears incremental, as the contribution primarily consists of introducing a new pretext task rather than a substantial methodological innovation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Using spatial coherence patterns as prediction targets rather than reconstruction is conceptually interesting as it directly targets brain connectivity patterns.\n- The specific combination of alpha, beta and gamma bands while excluding the other bands based on spatial specificity is well-motivated.\n- Strong empiral results across several EEG data sets with consistent improvements\n- Compact model achieving competitive performance against much larger foundation models\n- Ablations for several components (temperature, masking ratio, coherence label, model component) show quite reasonable robustness"}, "weaknesses": {"value": "- While the coherence pseudo-label seems to be novel, the overall architecture (masked autoencoding with transformers) is fairly standard. \n- While justified, the exclusion of lower frequency bands and equal weighting of selected bands seems somewhat arbitrary. Adaptive or learned band selection might be more principled. Also ablation studies for a different choice of frequency bands is missing. \n- While t-SNE plots show better separation, there is limited investigation into what spatial patterns the model actually learns and whether they align with known neurophysiology.\n- The authors briefly mention but don't deeply address how volume conduction affects their coherence measures, which is a significant confound in EEG connectivity analysis.\n- Some baseline comparisons use different preprocessing or don't use the same in-domain pre-training setup, making it harder to isolate the contribution of the coherence objective."}, "questions": {"value": "- How do you address the fact that coherence measures in EEG are significantly affected by volume conduction, which can create spurious correlations between nearby electrodes?\n- Why use equal weighting for alpha, beta and gamma bands when they have different functional significance and SNR characteristics? Have you experimented with learnable band weights or attention mechanisms over frequency bands? What happens if you include low frequency bands but with lower weights rather than excluding them entirely? Could you show ablations with different band combinations to justify your specific choice?\n- Why did you use specifically coherence over other connectivity measures (e.g., phase-locking value, mutual information, Granger causality)? How would results change?\n- Could you ensure all baselines use the same in-domain pre-training setup to isolate the contribution of your coherence objective?\n- What spatial patterns are learned? Can you provide interpretability analyses showing which channel pairs the model focuses on?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QKShALz9gr", "forum": "gpf2dYfVwg", "replyto": "gpf2dYfVwg", "signatures": ["ICLR.cc/2026/Conference/Submission17699/Reviewer_uzH8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17699/Reviewer_uzH8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761743629254, "cdate": 1761743629254, "tmdate": 1762927542189, "mdate": 1762927542189, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a self-supervised learning framework for EEG called Spatiality Preserving Representation (SPR) learning. The authors contribute a novel pretext task in which the model aims to predict a band-mixture coherence matrix, which encodes the connectivity between electrode pairs. The authors provide evaluations of their methodology on multiple datasets and it's claimed to achieve new state-of-the-art performance on multiple EEG benchmarks with in some cases showing massive gains (e.g. CHB-MIT)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The pretraining task explores a novel and promising direction, which is welcome to the field, as it does not focus on input-based reconstruction.\n2. The paper is generally easy to read and the methodology is clearly explained.\n3. The authors perform evaluation on multiple datasets and provide ablation results."}, "weaknesses": {"value": "1. **Fundamental concerns with experimental validation**: The paper's central claim is that its pretext task leads to superior representations. However, the results appear to contradict this. Specifically, the SPR (**Random**) model, which is just the paper's transformer model trained from scratch without pretraining, achieves higher performance than the baseline methods on nearly every metric. For example, on TUEV, SPR (Random) scores 58.1% compared to the their best pretrained comparison method EEG2Rep at 53%. SPR (Random) is furthermore 12-17% better than all other methods.\nThis implies that the model architecture is the dominant driver of performance, not the novel pretext task. However, no ablations or explanations are provided for this. Given that there doesn't appear to be a lot of novelty in the encoder (a single-channel encoder isn't new in and of itself, for example Mohsenvand et al. (2020) used one (which the authors cite but do not compare against), I am concerned that either A) the models are not evaluated under strict comparable conditions (such as dataset versions and splits) or B) the baseline models were poorly evaluated.\n\n2. **Implausibility of the pretext task**: The pretext task itself seems too simple to be the source of such large performance gains. The model is only asked to predict a single scalar value per channel pair. I don't understand how learning this extremely low-resolution spatial map could produce such powerful, state-of-the-art representations, especially when compared to the complexity of reconstruction or contrastive tasks. The fact that the SPR(Random) model performs so well I think reinforces this skepticism: the novel pretext task does not appear to be the critical ingredient. These concerns I think are quite self-evident from the paper in its current state I would think and should be addressed.\n\n3. **Inconsistent comparisons**:\nIn the main results, SPR is compared against BIOT, which is presented by the original authors as a cross-domain model. However, more recent methods such as Labram and Cbramod, from a similar cross-domain paradigm but which sometimes outperform SPR, are placed in the appendix instead. \n\n4. **Strange results**: The single most impressive result to me, which is a 15.6% balanced accuracy improvement on CHB-MIT (89.6% versus 74.0% for the next best model) is only presented in the appendix. A performance gain of this size should be a central finding of the paper, I would think. The fact its not included in the main paper I find puzzling and concerning. Naturally, here too I remain unconvinced whether a strictly identical evaluation protocol was used for SPR and comparison methods. If there was, it would be simple to run ablation experiments that would be extremely valuable to the EEG community to explain where these massive performance gains come from (is it here too just the architecture?).\n\n5.  **Lack of justification for methodological choices**: \nThe authors surprisingly exclude delta and theta bands. Their argumentation is plausible, but these bands are widely deemed *critical* for many EEG tasks, including seizure detection (!). Their strong claim that these bands should be omitted requires empirical validation, which is lacking.\n\n6. The ablation in Table 5 shows that a single wide band (8-100Hz) results only in a drop of 1.3% performance, suggesting that the author's complex band-mixture approach provides only a marginal benefit. This raises the question how the model is able to learn the highly useful features during pretraining that give it such strong downstream performance.\n\n\n\nMinor: The comparisons lack some earlier EEG-specific methods which do not rely on transformer architectures or reconstruction. I realize one cannot compare exhaustively, but a lot of the initial EEG literature seems missing. Just as an example, I think it would be valuable if methods like Banville et al. (2021; 10.1088/1741-2552/abca18) would be included."}, "questions": {"value": "1. The SPR(Random) model outperforms nearly all SOTA SSL baselines. Can you please explain this? What specific architectural component is responsible?\n\n2. Given that the pretext task is to predict a single (or 3-avg) scalar per channel pair, how do you explain this extremely low-resolution task leading to such powerful representations? This seems implausible, especially when your random baseline is already so strong.\n\n3. Why are comparisons to major foundation models (LaBraM, CBraMod) and the CHB-MIT results (Table 6) placed in the appendix , while the (less relevant) cross-domain BIOT model is in the main results?\n\n4. Given the 15.6% accuracy improvement on CHB-MIT, why was this not a central result? Can you provide more analysis, including a comparison to your SPR(Random) architecture trained from scratch on this dataset?\n\n5. Can you provide an empirical ablation showing the performance impact of including the δ (0.5-4 Hz) and θ (4-8 Hz) bands in your pretext task?\n\n6. The performance drop for using a single wide band (8-100 Hz) vs. your \"band mixture\" is only 1.3% (Table 5). This suggests the band-mixture component offers minimal benefit, but more concerningly, it indicates that a extremely low-resolution pretraining task is almost as good as your complete method. Could you comment on this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OPecgFPQJr", "forum": "gpf2dYfVwg", "replyto": "gpf2dYfVwg", "signatures": ["ICLR.cc/2026/Conference/Submission17699/Reviewer_ePRy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17699/Reviewer_ePRy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922327361, "cdate": 1761922327361, "tmdate": 1762927541834, "mdate": 1762927541834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a self-supervised Spatiality Preserving Representation (SPR) framework that pre-trains by predicting band-limited channel–channel coherence to capture spatial relationships. It achieves consistent state-of-the-art performance on four benchmark datasets under both linear probing and fine-tuning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. SPR introduces a novel SSL framework that preserves spatial electrode topology, bridging functional connectivity analysis with EEG representation learning.\n\nS2. The band-mixture coherence pretext task efficiently models spatial structures.\n\nS3. The method consistently outperforms prior EEG SSL approaches across multiple datasets, demonstrating strong generalization and robustness."}, "weaknesses": {"value": "W1. The coherence-based pseudolabel measures only the average linear synchronization between channels in the frequency domain. While simple, it is essentially a low-order statistic that cannot capture directional, nonlinear, or dynamic interactions. Moreover, the spatial patterns may be partially biased by volume conduction, as also noted in Appendix B.\n\nW2. Temporal modeling appears relatively weak. Table 5 suggests that the framework relies heavily on positional encoding, indicating that it primarily learns static spatial structures rather than time-evolving EEG dynamics.\n\nW3. The band-mixture strategy may obscure band-specific physiological characteristics, particularly in cognitive or disease-related tasks where frequency-band specificity is crucial.\n\nW4. Although the experiments cover four datasets (STEW, TUAB, TUEV, CHB-MIT), they share similar setups, including low-density, short-segment EEG with comparable sampling rates and channel counts. Such homogeneity limits the generalizability of claims about broad applicability. It would be valuable to test the method on more diverse paradigms such as MI, SSVEP, or neurodegenerative-disease datasets to better demonstrate the advantages of spatial coherence modeling."}, "questions": {"value": "Q1. Could the authors elaborate on how the learned spatial representations correspond to specific brain regions or electrode clusters in each task, and whether consistent or task-dependent patterns emerge across datasets? The paper provides such analysis only for two datasets in the appendix, whereas this should be a central focus of the experimental discussion.\n\nQ2. The introduction mentions spatial relationships, functional connectivity, and disease localization as motivations, yet the concrete application scenarios remain vague. In what practical contexts does spatial preservation provide the most tangible benefits? Given the lack of a clear link between the spatial learning mechanism and downstream functional or clinical outcomes, the motivation currently appears more algorithm-driven than application-driven.\n\nQ3. What do the authors see as the greatest potential real-world value of spatially preserving EEG representations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "K3mlCNRFyk", "forum": "gpf2dYfVwg", "replyto": "gpf2dYfVwg", "signatures": ["ICLR.cc/2026/Conference/Submission17699/Reviewer_4ps2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17699/Reviewer_4ps2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17699/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986706171, "cdate": 1761986706171, "tmdate": 1762927541530, "mdate": 1762927541530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}