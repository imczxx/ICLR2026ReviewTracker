{"id": "F8o9gzJFxr", "number": 19635, "cdate": 1758297865530, "mdate": 1763734576797, "content": {"title": "A Generative Likelihood Framework for High-Resolution Climate Model Evaluation", "abstract": "Next-generation high-resolution (km-scale) climate models promise unprecedented accuracy in climate projections, but realising their potential requires robust methods to quantify how well simulations align with real-world observations. Average-based metrics conventionally used for climate model evaluation ignore the physics encoded in the finescale structures of km-scale simulations. To overcome this limitation, we propose a novel, statistically principled evaluation methodology based on the likelihood function of a generative image model. Our method provides a continuous similarity metric derived from the likelihood distribution of observation and simulation snapshots, which can redefine the evaluation, intercomparison, and parameter tuning of high-resolution climate models. We demonstrate the applicability and interpretability of this method by evaluating convective clouds simulated by two state-of-the-art global km-scale models, using their outgoing infrared radiation fields. This work establishes a scalable pathway toward observation-based evaluation of next-generation climate simulations.", "tldr": "We introduce a novel approach to quantitatively evaluate high-resolution climate simulations against satellite observations, using the distance between generative likelihood distributions to quantify similarity.", "keywords": ["High-resolution climate models", "climate model evaluation", "normalising flows", "deep generative models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a576141bb28742b0f08fc1be1e2a61a25e7cc255.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a new statistical framework to evaluate next-generation kilometre-scale (km-scale) climate models using likelihoods from generative models in comparison with satellite observations. The authors address the problems of Conventional evaluation metrics (e.g., mean-square error) which rely on spatial or temporal averages by introducing a generative likelihood-based similarity measure that compares distributions of simulated and observed climate fields rather than their mean values. The authors introduce a dataset-agnostic, likelihood-based metric that quantifies how realistic a model’s outputs are relative to observations. They propose a uniform preprocessing pipeline to make observational and model datasets directly comparable. A normalizing-flow model (specifically a Neural Spline Flow) is trained on satellite data to learn the distribution of observed fields. The log-likelihood distributions of both simulated and observed snapshots are compared using symmetrized KL divergence to yield a continuous realism score. Overall, the paper addresses an important gap in evaluation of climate modelling at high resolution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Reframing climate model evaluation as a distribution-matching problem rather than simple temporal and spatial means is statically principled.\n2. The combination of generative likelihoods, HEALPix remapping, and histogram matching, forms a statistically principled method.\n3. Comprehensive experiments using real satellite data (GOES-16) and two leading km-scale models (IFS, ICON), including quantitative and qualitative analysis of spatial and temporal biases.\n4. The proposed evaluation framework represents a meaningful and timely contribution at the intersection of deep learning and climate science."}, "weaknesses": {"value": "While the framework is conceptually strong, its empirical validation is restricted to a single observable variable (outgoing longwave radiation, OLR) and one satellite platform (GOES-16). Extending the experiments beyond that would test whether the likelihood-based approach consistently identifies biases across different atmospheric processes and instruments.\nAlthough the paper compares its metric with MAE and multifractal analysis, additional distance metrics like Wasserstein Distance could strengthen the paper"}, "questions": {"value": "1. Can authors extend the analysis to one or more additional observables (e.g., shortwave radiation, cloud optical depth, or precipitation) and cross-validate using multiple satellite sensors (e.g., Himawari-8, MODIS). \n2. Can authors address the metric comparison by adding Wasserstein Distance in analyses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TOBJRuD8mI", "forum": "F8o9gzJFxr", "replyto": "F8o9gzJFxr", "signatures": ["ICLR.cc/2026/Conference/Submission19635/Reviewer_dp4d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19635/Reviewer_dp4d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761130879340, "cdate": 1761130879340, "tmdate": 1762931486099, "mdate": 1762931486099, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework for evaluating high resolution (km-scale) climate models, and more precisely for evaluating how well deep convection is represented in these climate models. \n\nIt works as follows: \n - project data into a HEALPIX grid \n - train a normalizing flow to learn a distribution representing observation data \n - input both observation and climate model data at time t to the trained normalizing flow, to get two distributions\n - compute KL divergence between the two distributions\n\nThis metric can be decomposed into spatial and temporal components to assess spatial and temporal biases, and outline the strengths and weaknesses of the different climate models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Mapping the different data sources to healpix grids is great, since it doesn't suffer from the distortions of the regular latitude-longitude grids. \n\nUsing a normalizing flow to map from image-like input data to a distribution is also a good idea. \n\nDeveloping a metric that separates spatial and temporal component is very important. \n\nThe experiment with GOES, IFS and ICON data are interesting."}, "weaknesses": {"value": "The main concernI have with the proposed metric is that, even though it is novel, it is not compared to appropriate baselines and, in my opinion, seems very complicated for something that can be done much more easily and very similarly with the Continuous Ranked Probability Score (CRPS).\n\nThe metric first finds probability distribution scores, before comparing the distributions. It seems very similar to (CRPS), which is also a continuous similarity metric derived from the likelihood distribution of observation. CRPS seems much more natural and easier to compute, and also less prone to errors in training the normalizing flows. It would be good to compare to CRPS on top of MAE and multifractal analysis. \n\nThe analysis done in 4.4 could be done with CRPS as well. More generaslly, the way that the metric \"asseses spatial and temporal biases\" is not novel, in the sense that it does either a temporal or a spatial averaging, which can be done with other metrics as well.\n\nMoreover, the distribution learned by the normalizing flow is not really evaluated. How do you know if the distribution is faithful to the data that it is trained on (GOES in your case)? Reporting some metrics here would be useful. Also, if you train on the climate model distribution and evaluate GOES, do you find similar (or \"symmetrical\") results?\n\nThe presentation of the paper makes it hard to understand what the framework is. I would suggest describing the normalizign flows a bit more, detailing how you obtain the log likelihood for each image (healpix grid point), and specifying more clearly that the KL difference is computed on the distributions of log-likelihood. Also, you can give less details on the healpix remapping (which is common).\nI would also suggest replacing \"weather variables\" by \"climate variables\". It's a bit confusing since you're trying to evaluate climate models and not weather models."}, "questions": {"value": "Why are you using the symmetrical KL divergence and not the regular KL divergence since you treat the GOES likelihood as the \"ground truth\"? \n\nI am surprised by the values of the log-likelihood. Is there an explanation why the values are so high?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "L7oUxqZKIX", "forum": "F8o9gzJFxr", "replyto": "F8o9gzJFxr", "signatures": ["ICLR.cc/2026/Conference/Submission19635/Reviewer_dAwz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19635/Reviewer_dAwz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761693945988, "cdate": 1761693945988, "tmdate": 1762931485311, "mdate": 1762931485311, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel framework for evaluating (high-resolution) climate models. The core idea is to move beyond traditional, average-based metrics, which fail to capture the fine-scale physical structures that high-res models now resolve.\nThe authors propose training a likelihood-based generative model (a normalising flow) on high-resolution observational data. This model is then used to compute the log-likelihood for every data patch from both the observational test set and the climate model simulations. The final similarity metric is the symmetrised Kullback-Leibler divergence between these two 1D likelihood distributions.\nThe authors demonstrate this framework by evaluating Outgoing Longwave Radiation (OLR) fields from two leading km-scale models, ICON and IFS. The results show that ICON's OLR statistics are significantly closer to the observations than IFS's."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Well motivated, important application, and clearly explained.\n- Design is reasonable (e.g. Healpix projection, comparing likelihoods) and well-justified.\n- The comparison versus some baseline metrics shows that the proposed distance captures certain details better.  \n- Very well written* \n\n*I'm rating the presentation component as only 2 due to the lack of contextualization."}, "weaknesses": {"value": "1. Insufficient contextualization and missing related works. \n- The authors claim to be addressing a \"gap\"  that is not as wide as suggested, due to the omission of highly relevant, recent work. The paper completely fails to cite or compare against the entire line of work using Wasserstein distances for climate model evaluation. Most notably, the Spherical Convolutional Wasserstein Distance (SCWD) [1] is a direct competitor. This distance satisfies many, if not most/all requirements that are claimed to be unaddressed in prior work. The paper's claims of novelty/gap are significantly weakened by this omission, and a discussion and comparison of the trade-offs between a likelihood-based approach and a Wasserstein-based one is critically needed.\n- There are various ML for Earth papers using the Healpix grid too, which should be discussed appropriately, e.g. [2]\n- There's a new generation of ML-based climate model (emulators) which is not mentioned in the paper [3-6]. Optimally, it would be really nice to see the proposed evaluation be used on those models/emulators, but at least some discussion should be added.\n2. Evaluation is limited to OLR. Something like surface temperature or precip. would be interesting as those are the most relevant variables for climate projections.\n3. Sensitivity of distance to the trained model is unclear.\n- How to tell that model has been trained \"well enough\" on the observations to be able to serve for evaluation?\n- How sensitive is the distance (and esp. the ranking and drawn insights) to the choice of architecture/optimization etc.?\n4. Runtime complexity is not discussed. This seems like a potential limitation given the introduced need of forwarding through a neural net for each patch individually (and there are *many* of them in this climate modeling context of high-res data over multiple years).\n\nMinor:\n- Line 76: *\"to a square format better\"*... Do you mean to a Healpix grid?\n- Some sentences are a bit inaccesible for non domain experts. E.g. *\"Geostationary sensors sample the Earth on a grid that is regular in satellite viewing geometry but projects to a curvilinear latitude-longitude grid, with highest resolution near the sub-satellite point and coarser resolution toward the limb.\"* Would be nice to improve the clarity of these (and note its implications explicitly)\n- $x$ is introduced as the frame/global map at a specific timestep in Section 3.1, but used to denote a *patch* of it in line 279 (and before?). This is confusing.\n\n[1] Validating Climate Models with Spherical Convolutional Wasserstein Distance; Garrett et al. (NeurIPS 2024; https://proceedings.neurips.cc/paper_files/paper/2024/file/6cac74e7bb50d1f21626800f5b49a869-Paper-Conference.pdf)\n\n[2] Advancing Parsimonious Deep Learning Weather Prediction Using the HEALPix Mesh; Karlbauer et al. (JAMES 2024; https://doi.org/10.1029/2023MS004021)\n\n[3] ACE: A fast, skillful learned global atmospheric model for climate prediction; Watt-Meyer et al. (NeurIPS CCAI workshop 2023; https://arxiv.org/abs/2310.02074)\n\n[4] Probabilistic Emulation of a Global Climate Model with Spherical DYffusion; Ruhling Cachay et al. (NeurIPS 2024; https://arxiv.org/abs/2406.14798)\n\n[5] Neural general circulation models for weather and climate; Kochkov et al. (Nature 2024; https://www.nature.com/articles/s41586-024-07744-y)\n\n[6] A Deep Learning Earth System Model for Efficient Simulation of the Observed Climate; Cresswell-Clay et al. (AGU Advances 2025; https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2025AV001706)"}, "questions": {"value": "- The authors note that regular latitude–longitude grids bias evaluation metrics. Can't this be fixed by latitude-weighted aggregating? Or why is that not enough?\n- Since the likelihood is estimated for each timestep independently, would that be detrimental to evaluate temporal fidelity (e.g. phenomena at weekly/monthly timescales)?\n- Why is the data split by day of the month? Why not splitting chronologgically, which is more common in time series problems?\n- Will code be released?\n- How do you intend to convince the climate modeling community to adopt ML-based evaluations like the proposed one?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9vzjbaFvwy", "forum": "F8o9gzJFxr", "replyto": "F8o9gzJFxr", "signatures": ["ICLR.cc/2026/Conference/Submission19635/Reviewer_8c32"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19635/Reviewer_8c32"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891692060, "cdate": 1761891692060, "tmdate": 1762931484450, "mdate": 1762931484450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a likelihood-based framework for evaluating climate models against observational (satellite) datasets by comparing their respective distributions. All datasets are first standardized to an equal-area HEALPix grid using first-order conservative remapping to avoid area-related artifacts, and the models are remapped to the observed marginal distribution via histogram/quantile matching so that the downstream metric focuses on finescale structure rather than mean bias. The authors then train a normalizing-flow model only on observations, use it to score both observations and models patch-wise, and then compare the two resulting log-likelihood distributions with a symmetrized KL divergence ($D_{SKL}$); lower $D_{SKL}$ indicates closer agreement. The method is applied to ICON and IFS simulations of outgoing longwave radiation (OLR) and GOES-16 observations, with spatial and diurnal stratifications to diagnose when and where models disagree with the observations. Compared to a simple MAE and a multifractal scaling approach, $D_{SKL}$ highlights distinct biases and often separates ICON and IFS differently, illustrating how likelihood-based evaluation captures aspects that pointwise errors miss (see Table 2 and associated maps)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Training a generative model on observational data, then scoring observations and models under its likelihood, is a novel and rigorous approach to validating climate model output. This idea is well motivated, theoretically sound, and naturally allows the results to be stratified over time and space. This is quite useful for assessing behavior at km scales. \n\n2. Understanding the biases of climate models and validating them against observations is a timely topic of significant importance. Climate change projections rely on having well-validated models that accurately represent the underlying physical processes.\n\n3. The empirical results show that their likelihood based approach represents fine-scale structure and is able to capture differences that raw MAE based evaluations miss.\n\n4. Writing is generally clear and well organized. I did not have any difficulty following the manuscript."}, "weaknesses": {"value": "1. Mismatched framing. The paper is positioned as climate model evaluation, but all numerical experiments use weather prediction systems (ICON/IFS). Climate models (e.g., GCMs/ESMs) differ from NWP models in design goals and timescales (days vs decades). To support the climate framing, please add GCM/ESM experiments (e.g., CMIP variables such as near-surface temperature, precipitation, geopotential height) evaluated over multi-decadal periods ($\\approx$ 20–30 years) with distributional summaries. Otherwise, consider reframing the contribution around assessing weather-forecast distributions.\n\n2. Insufficient baselines / engagement with prior work. The paper compares primarily against MAE and a few internal baselines, but there is substantial prior literature on distributional and structural climate model evaluation that should be discussed and (where feasible) quantitatively compared. Examples include Wasserstein-based validation (e.g., Garrett et al., 2024), moment-based comparisons (Lund & Li, 2009; Li & Smerdon, 2012), functional-data approaches (Staicu et al., 2014; Zhang & Shao, 2015; Harris et al., 2021), and hypothesis-testing frameworks for statistical significance (Li et al., 2016; Yun et al., 2022). Without engagement with these lines of work, it is difficult to judge the added value of this approach.\n\n3. Limited treatment of extremes. While assessing bulk distribution behavior is important, climate extremes have become increasingly important for climate model evaluation. This work should discuss the sensitivity/applicability of the method to tails and engage with relevant prior work (e.g., Perkins et al., 2009; Cooley & Sain, 2010; Cao & Li, 2018). In addition, histogram/quantile mapping is known to compress tails and underestimate variability; consider adding a sensitivity analysis showing whether your conclusions are robust with and without quantile mapping"}, "questions": {"value": "1. Could you clarify the intended application domain: is this primarily a weather-model evaluation method (short lead times, synoptic variability) or a climate-model evaluation method (long-run distributions)? If the latter, why are there no GCM/ESM experiments (e.g., CMIP5/CMIP6 variables such as near-surface temperature, precipitation, geopotential height)? Evaluating one or two CMIP variables over a standard evaluation period (30 years or more), with summary statistics focused on distributional fidelity (seasonal cycles, extremes) rather than pointwise forecast error could be helpful here. If the intent is weather-focused, please reframe accordingly and spell out what carries over (or doesn’t) to climate evaluation.\n\n2. Are likelihood scores computed per patch and then aggregated, or on full-sphere fields? If patch-wise, please detail: (i) any tapering/windowing used to mitigate boundary effects; (ii) how aggregation is done (mean/median, area-weighted); and (iii) whether patch scoring can miss large-scale dependencies (teleconnections, Rossby waves). It would help to add a brief analysis showing that conclusions are stable across patch sizes and can detect large-scale dependency mismatches.\n\n3. Beyond MAE, could you provide quantitative comparisons against existing distributional metrics such as (Garrett et al. 2024) or standard Continuous Ranked Probability Score (CRPS) methods? This could really help demonstrate what the proposed method offers over existing works and it would be quite interesting to see where the methods disagree and why.\n\n4. Since conclusions hinge on the accuracy of the flow-based likelihoods, could you include some basic calibration/stability diagnostics: train/val NLL (bits-per-dim) curves or PIT histograms to demonstrate how well the likelihood fits the data? A few comparisons to simple parametric baselines (e.g. Gaussian for temperature or Gamma for precipitation) could help show how this method is able to accurately capture the distribution of climatic variables and that this approach is well posed for high-dimensional fields."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "G4sewgq10C", "forum": "F8o9gzJFxr", "replyto": "F8o9gzJFxr", "signatures": ["ICLR.cc/2026/Conference/Submission19635/Reviewer_MrjL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19635/Reviewer_MrjL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19635/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761955659419, "cdate": 1761955659419, "tmdate": 1762931483752, "mdate": 1762931483752, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to Reviewers"}, "comment": {"value": "We appreciate the insightful reviews and thoughtful suggestions from the reviewers. \n\nWe are pleased that the following aspects of our work were appreciated:\n* We developed a novel evaluation framework for high-resolution global climate models. Reviewers called this work an “important”[MrjL, 8c32, dAwz], “well-motivated”[MrjL, 8c32], “meaningful and timely contribution at the intersection of deep learning and climate science” [dp4d].\n* Our likelihood-based evaluation framework was appreciated to be “statistically principled” [dp4d] and “theoretically sound” [MrjL].\n\nAdditional Feedback:\n* The reviewers found our design to be “well-justified” and our manuscript to be “clear and well organized” [MrjL] and \"very well written\" [8c32].\n\nRevisions and Additions:\n\nBased on reviewer feedback, we have conducted several novel analyses and revised the manuscript. We provide an outline of key additions and changes here, with more details in individual answers to reviewers:\n\n* **Included additional related works:** edited Section 2 to introduce the spherical convolutional Wasserstein distance (SCWD) and other relevant statistical approaches to climate model evaluation and discussed how they motivate our work. [MrjL, 8c32, dAwz, dp4d]\n* **Baseline comparison to SCWD:** We included the Spherical Convolutional Wasserstein Distance (SCWD, Garrett et al. 2024) as an additional baseline (Figure 6) and summarised our findings in Section 4.5, with further explanation and results in Appendix E. [MrjL, 8c32, dAwz, dp4d]\n* **Evaluation of trained NSF model:** We added training and validation log-likelihoods in Section 4.2, included generated samples from the trained model in Figure 8 in the Appendix, and added Figure 9 which shows that the distribution of generated OLR samples closely matches that of the GOES-16 training data. [MrjL, 8c32, dAwz]\n* **Additional sensitivity tests:** We added sensitivity tests to model architecture, histogram matching and patch size and showed that likelihood scores are robust to hyperparameter choices in Section 4.6. [MrjL , 8c32]\n\nWe believe these revisions have addressed the reviewers' concerns and enhanced our work. Thank you for your valuable feedback and recognition.\n\n*To facilitate review, all changes have been highlighted in red in the revised version of our manuscript.*"}}, "id": "VbUyKPGjvm", "forum": "F8o9gzJFxr", "replyto": "F8o9gzJFxr", "signatures": ["ICLR.cc/2026/Conference/Submission19635/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19635/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission19635/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763739371817, "cdate": 1763739371817, "tmdate": 1763739471569, "mdate": 1763739471569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}