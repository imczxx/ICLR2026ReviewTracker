{"id": "2eQK1j1idl", "number": 4289, "cdate": 1757655964297, "mdate": 1759898041437, "content": {"title": "Dive into the Scene: Autonomous Focus Plan Generation in Vision-Language Decision-Making", "abstract": "Vision-Language Models (VLMs) are promising in decision-making tasks, whereas the visual hallucination issue limits their performance in complex visual scenes. In such scenes, a number of visual objects exist, while the essential ones related to actions require focus in each step, avoiding the interference of unrelated objects. In this work, we propose SceneDiver, a coarse-to-fine, two-stage focus plan generation pipeline, to tackle the key technical challenge of identifying the essential objects from scenes with complicated visual and semantic structures. First, the VLM executes a virtual, coarse-grained plan over the scene graph. Then, it zooms into local neighborhoods around each graph node to perform fine-grained focusing. The resulting focus map controls the attentions of VLM during decision making, steering the model toward task-critical objects and alleviating the perceptual hallucination of VLMs. Experimental results under robotic manipulation and room navigation benchmarks demonstrate that our approach successfully overcomes the perceptual limitation of VLMs, meanwhile significantly enhances their decision-making performance and generalization ability. Our code will be open-released upon acceptance.", "tldr": "", "keywords": ["vision-language model", "decision-making", "multimodal reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72bdcc036d22dc28e66a7b080b254dcde37dc44e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce SceneDiver, a pipeline to generate a focus plan over visual scenes in order to aid decision making of VLMs. The VLM is tasked with constructing a coarse-to-fine focus plan which is used to modify the image by cropping or zooming into parts of the image. This is done to reduce the influence of distracting objects and other noise that could potentially (negatively) influence the decision making. The approach is evaluated on a robotic manipulation task and room navigation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- important problem of reducing hallucination in visual decision making\n- extensive evaluation across tasks and models"}, "weaknesses": {"value": "- no clear distinction from previous focus-plan methods. There is no benchmark comparison to Huang et al., 2024 and Zhu et al., 2025 which tackle the same problem. It should also be made clear, what the conceptual differences to those methods are."}, "questions": {"value": "- Line 57: tasks-relevant -> task-relevant\n- Line 194: missing brackets for citation"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "N0Y32fmyxz", "forum": "2eQK1j1idl", "replyto": "2eQK1j1idl", "signatures": ["ICLR.cc/2026/Conference/Submission4289/Reviewer_H5pP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4289/Reviewer_H5pP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761232680601, "cdate": 1761232680601, "tmdate": 1762917278152, "mdate": 1762917278152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SceneDiver, a novel method to mitigate visual hallucination in Vision-Language Models (VLMs) for decision-making tasks. SceneDiver generates a coarse-to-fine focus plan by having the VLM first reason over a scene graph and then zoom into local neighborhoods, creating an attention map that highlights task-critical objects. Experimental results in robotic manipulation and room navigation demonstrate that this approach significantly enhances decision-making performance and generalization ability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The overall auto-focusing framework is well designed and makes sense.\n\n2. The experiments cover robotic manipulation and navigation tasks."}, "weaknesses": {"value": "1. Limited Experimental Scope and Baselines:\nWhile the experiments in robotic manipulation and room navigation demonstrate the potential of SceneDiver, the experimental settings appear somewhat simplified and lack comparison against established, strong baselines. To convincingly validate the method's effectiveness and impact, it is crucial to benchmark it on widely-adopted and challenging benchmarks in the community, such as RLBench for manipulation or Habitat-Matterport3D for navigation. The absence of comparisons against state-of-the-art task-specific models (e.g., end-to-end IL/RL policies) or other VLM-based reasoning frameworks on these benchmarks makes it difficult to gauge the true performance leap offered by the proposed approach. Demonstrating superiority or competitive performance in these complex, standardized environments would significantly strengthen the paper's claims.\n\n2. Architectural Disconnect with Mainstream Embodied Model:\nThe proposed method is presented as a \"paradigm\" for enhancing VLM-based decision-making, but it operates primarily as a pre-processing or intermediate reasoning step that modifies the visual input for a VLM. However, a dominant and successful trend in embodied AI is the use of end-to-end Vision-Language-Action (VLA) models, which directly map observations to actions. It is unclear how SceneDiver's two-stage, graph-based reasoning process can be integrated into such end-to-end architectures, which are often trained jointly for optimal performance. The authors should discuss the compatibility and potential integration strategies (e.g., using the focus plan as an intermediate tokenized representation) with these SOTA VLA models to address its applicability in the current research landscape."}, "questions": {"value": "Here are some questions / minor weakness:\n\n1. What is the computational and latency overhead introduced by the coarse-to-fine focus planning pipeline? Please provide an analysis of the inference time and computational footprint (e.g., FLOPs) of SceneDiver compared to the base VLM without focusing. A discussion on whether this overhead is justifiable given the performance gains is essential.\n\n2. Can SceneDiver be integrated into existing navigation and manipulation frameworks (Both end-to-end VLA methods or hierarchical VLM-based methods are acceptable) in a general and plug-and-play manner?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "c7HtEsjKVP", "forum": "2eQK1j1idl", "replyto": "2eQK1j1idl", "signatures": ["ICLR.cc/2026/Conference/Submission4289/Reviewer_Q8qL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4289/Reviewer_Q8qL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761455410336, "cdate": 1761455410336, "tmdate": 1762917277765, "mdate": 1762917277765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SceneDiver, a coarse-to-fine, two-stage focus plan generation pipeline aimed at mitigating object hallucination in Vision-Language Models (VLMs) during decision-making tasks. The method first constructs a scene graph and executes a virtual, coarse-grained plan to identify essential objects, followed by fine-grained, localized focusing around each anchor node. The resulting focus map is used to modify input images, thereby steering model attention to task-critical objects and suppressing distractors. Extensive experiments on robotic manipulation and room navigation benchmarks demonstrate improved decision-making performance and generalization. Ablation studies are performed to analyze the benefits of step-wise focusing and the robustness of the approach to scene graph quality."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear Problem Motivation:** The work targets the highly relevant issue of object hallucination in vision-language decision-making, specifically addressing scenarios where VLMs are used out-of-the-box without task-specific retraining.\n- **Methodological Innovation:** SceneDiver’s two-stage, coarse-to-fine approach for generating a focus plan—comprising global scene graph planning followed by local fine-grained focusing—offers a structured strategy to mitigate distractor interference. This is visually clarified in Figure 1, which presents an end-to-end pipeline spanning instruction input to image modification.\n- **Strong Empirical Support:** The paper provides comprehensive experiments (Tables 1 & 2) across multiple VLMs and tasks, showing consistent performance gains over baselines. Ablation analysis (Section 4.2, Table 2) convincingly dissects the contribution of each system component.\n- **Thorough Visualizations:** The qualitative trajectories in Figures 3 and 4 and additional examples in the appendix (Figures 6–13) provide interpretability into the method’s effect on attention and decision steps.\n- **Practical Robustness:** The paper demonstrates that the approach remains beneficial—even with untrained or partially trained scene graph generation—suggesting real-world applicability.\n- **Reproducibility:** The methods and experimental setups are described in detail, including a stated intention to release code."}, "weaknesses": {"value": "1. **Incomplete Positioning against Closest Prior Work:** Several recent and highly relevant works addressing hallucination in VLMs (see 'Potentially Missing Related Work') are not cited or discussed. This is a significant oversight given their strong alignment in motivation and technical foundation. Examples include works on visual inference chains, multi-view reasoning, and systematic hallucination benchmarking. This omission directly affects the paper’s contextualization and could impact the novelty claim.\n2. **Math and Clarity Issues in Focus Map Construction (Section 3.4):** The mathematical exposition around score map construction and image editing suffers from a lack of clarity and rigor:\n   - The definition of the coefficient $c(d)$ in Section 3.4.1 uses a Gaussian decay, but it is unclear how $\\sigma$ is chosen, how sensitivity to hyperparameters is controlled, or how “distance to the image center” generalizes to complex scenes with off-center targets.\n   - The blending equations for image emphasis/de-emphasis ($I' = I \\odot (\\beta + (1-\\beta) s)$ and $I'' = (1-a) \\odot I' + a \\odot \\mathcal{B}_{\\sigma_b}(I)$) are correct in isolation, but the implementation details—such as the handling of per-object overlaps and thresholds—are not fully described. It is unclear how the method ensures that background suppression does not accidentally eliminate critical context in complex, occluded scenes. Furthermore, the method implicitly assumes $s$ is well-calibrated, yet there is no explicit calibration or adaptive thresholding, which can reduce robustness in cluttered or unfamiliar visual settings.\n3. **Missing and/or Weak Baselines:** While ablation studies are included, some direct baselines from recent literature on hallucination reduction (such as multi-view and multi-hop reasoning or inference-chain frameworks) are missing. The lack of direct comparison to these approaches in Table 2 or the main results is a methodological gap that makes it difficult to situate SceneDiver’s relative merits and shortcomings.\n4. **Over-reliance on Scene Graph Quality:** Despite some sensitivity analysis, the method fundamentally depends on the quality of object and relationship detection in the scene graph. While the results (discussed around Figure 5) show some robustness, there is insufficient quantitative exploration of failure modes—e.g., what happens if critical task-relevant objects are misdetected or missed in the initial coarse stage. The qualitative figures (e.g., Figure 4) focus on clean success cases rather than highlighting or dissecting difficult failures.\n5. **Lack of Theoretical Guarantees or Deeper Analysis:** The paper motivates its pipeline with conceptual intuition but does not provide theoretical guarantees or analysis (e.g., convergence, error propagation, or attention calibration bounds). For example, in Section 3.3, the three focusing strategies are described, but no formalism is provided regarding their completeness or worst-case behavior. The stepwise approach may fail to recover in situations of cascading errors, which is not examined.\n6. **Some Ambiguity in Experimental Protocols:** While the experimental setup is well detailed, it is not always clear how statistical significance is established in performance comparisons (Table 2)—variance is reported, but there’s no mention of hypothesis testing or statistical thresholds used to claim “significant enhancement.” Additionally, the supplementary visualizations (Figures 6–13) add interpretability but do not systematically evaluate failure cases.\n7. **Potential Issues with Generalizability:** The method is tested on robotic manipulation and room navigation but may not generalize to visual domains with different object spatial relationships or more abstract task definitions (e.g., medical imaging or non-spatial language grounding). This limitation is briefly discussed but not empirically explored.\n8. **Limited Discussion of Limitations in Main Text:** Most of the limitations and future work are condensed at the end, without being integrated into the problem discussion or experimental interpretation.\n\nPotentially Missing Related Work\n\n1. Zheng, H., Xu, T., Sun, H. (2024): *Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination* — Proposes a Visual Inference Chain framework highly aligned in spirit with the focus on mitigating hallucinations in complex visual scenes. Should be cited and compared in Section 2 (Related Work, Object Hallucination) and included as a baseline/alternative strategy in comparisons (Tables 1 or 2).\n2. Gu, Z., Chen, J., Liu, F. (2025): *MedVH: Toward Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context* — Establishes systematic benchmarks for hallucination evaluation; relevant for broader claims and should be referenced in Section 4 (Experiments) when discussing generalizability and benchmarking.\n3. Anonymous (2025): *See, Think, Hallucinate: Interpreting Reasoning and Hallucinations Beyond the First Hop in Vision-Language Models* — Investigates hallucinations beyond first-step reasoning, providing deep insights into multi-step or cascading errors similar to those in this work. Should be discussed in Sections 2 and 4, and ideally referenced alongside SceneDiver's multi-stage planning in Section 3.\n4. Anonymous (2025): *Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning* — Introduces a training-free framework for hallucination reduction using multi-view, multi-path reasoning. Highly relevant as a direct baseline or discussed alternative; should be cited in Section 2 and quantitatively compared in Section 4/Table 2.\n5. Anonymous (2025): *VIDHALLUC: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding* — Develops benchmarks for evaluating hallucinations in temporal/multimodal tasks. Should be referenced when situating SceneDiver’s applicability and generalization to temporally-evolving scenes."}, "questions": {"value": "1. **Clarification on Focus Score Calibration:** How is the $\\sigma$ parameter (Section 3.4.1) chosen for the attention map, and how sensitive are the results to its value? Is there an adaptive mechanism or heuristic for this, especially in cluttered or off-center tasks?\n2. **Handling of Overlapping/Adjacent Candidates:** When multiple candidate objects overlap or are spatially adjacent, how is the focus map $A$ constructed to avoid unintended attention diffusion or masking of critical context?\n3. **Failure Cases in Scene Graph Detection:** Can the authors provide additional quantitative analysis of how failures in scene graph extraction (missed objects/relations) propagate through the pipeline? What is the worst-case impact on downstream decision accuracy?\n4. **Baselines Against Recent Hallucination-Mitigation Methods:** Are there technical reasons for not including Visual Inference Chain, Multi-View Reasoning, or related methods as direct baselines? If so, please clarify. If not, would the authors be able to add these for a fairer comparison?\n5. **Statistical Significance:** What specific statistical tests or thresholds are used to determine whether the improvements in Table 2 are significant?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "abhWLVahTw", "forum": "2eQK1j1idl", "replyto": "2eQK1j1idl", "signatures": ["ICLR.cc/2026/Conference/Submission4289/Reviewer_X5bq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4289/Reviewer_X5bq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984988471, "cdate": 1761984988471, "tmdate": 1762917277138, "mdate": 1762917277138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "### Summary: Dive into the Scene\n\n#### Research Problem\nThe paper addresses **visual hallucination** in Vision-Language Models (VLMs) when applied to complex decision-making tasks (like robot manipulation and navigation). In cluttered scenes, VLMs struggle to identify and focus on **task-relevant objects**, leading to poor decision accuracy.\n\n#### Methodology (SceneDiver)\nThe authors propose **SceneDiver**, a novel, training-free, **coarse-to-fine, two-stage focus plan generation pipeline** to guide VLM attention:\n\n1.  **Stage 1: Coarse-Grained Plan:** The VLM constructs a **scene graph** (object nodes and relationships) from the input image and executes a **virtual plan** over this graph. This process identifies coarse **anchor nodes** (critical objects) and minimizes the search space, validating consistency and mitigating multi-object hallucination.\n2.  **Stage 2: Fine-Grained Focusing:** The VLM autonomously explores local neighborhoods around each anchor node, using simple strategies (Graph-Consistent Focus, Semantic-Guided Local Zoom, Stochastic Outward Search) to refine the focus and correct initial scene graph inaccuracies.\n3.  **Result:** The generated focus plan is converted into a **Focus Score Map**, which is used to **edit (modify/dim)** the input image. This modification suppresses irrelevant background content and steers the VLM's attention toward essential targets, improving its grounded perception and decision-making.\n\n#### Key Experiments\nThe approach is validated on challenging benchmarks:\n* **Robotic Manipulation** (assembling pieces in cluttered scenes).\n* **Room Navigation** (using complex scenes and hard-to-identify targets).\n\n**Results** show that SceneDiver significantly **enhances the decision-making performance** and generalization ability of various off-the-shelf VLMs (e.g., Qwen, GPT-4o-mini, Gemini-2.5-Flash) by overcoming their perceptual limitations. Ablation studies confirm the superiority and robustness of the step-by-step focusing strategy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "### Strengths of the Paper\n\n1.  **Effective Solution for VLM Visual Hallucination:** The paper directly addresses a critical and common failure mode of VLMs in complex real-world decision-making: **visual hallucination** and poor grounding due to cluttered scenes. By introducing an autonomous, training-free mechanism (SceneDiver) to generate a focus plan and edit the input image, the method successfully steers the VLM's attention to task-relevant objects, significantly improving decision accuracy.\n\n2.  **Training-Free and Model-Agnostic Approach:** **SceneDiver** is implemented as a *training-free* pipeline using existing VLM reasoning capabilities (scene graph construction, virtual planning). This makes the approach highly practical, as it does not require collecting new paired data for training. Furthermore, it is **model-agnostic**, demonstrating performance gains when integrated with various off-the-shelf VLMs (e.g., Qwen, GPT-4o-mini, Gemini-2.5-Flash)."}, "weaknesses": {"value": "### Potential Weaknesses of the Paper\n\n1.  **Dependency on Accurate Scene Graph Construction:** The Stage 1 coarse-grained plan hinges on the VLM's ability to accurately construct a **scene graph** (objects and relationships) from the initial image. In highly ambiguous, heavily occluded, or extremely cluttered scenes, an inaccurate initial scene graph will lead to the selection of incorrect **anchor nodes**, which may cause the subsequent fine-grained focusing stage to search the wrong area, resulting in failure.\n\n2.  **Increased Computational Latency:** While the method is *training-free*, the **coarse-to-fine two-stage planning** (scene graph generation, virtual plan execution, and multiple local zooming/refinement steps) adds significant computational overhead. This sequential reasoning process increases the **inference latency** of the overall decision-making system, potentially limiting its deployment in time-critical, real-time applications.\n\n3.  **Ambiguity in Image Editing (Focus Map):** The final step involves generating a **Focus Score Map** to **edit (modify/dim)** the input image. The effectiveness of this technique relies on the assumption that VLM attention mechanisms are reliably influenced by these simple visual edits. There is a risk that highly capable VLMs might \"see through\" the dimming or that the focus map might inadvertently obscure necessary contextual information.\n\n4.  **Limited Scope of Visual Hallucination Addressed:** The method primarily tackles **multi-object hallucination** (distraction from irrelevant objects). It may not fully resolve other common types of visual hallucinations, such as **non-existent object hallucination** (the VLM imagining an object not present) or **attribute hallucination** (misidentifying an object's color or state), which are related to the VLM's internal generative biases rather than just focus."}, "questions": {"value": "1. Although the method is training-free, could the author report the number of reasoning or action steps and the reasoning time taken by the relevant model when completing the task? This is beneficial for us to have a clear understanding of the computational complexity of the method.\n\n2. Since the method tells us that this pipeline can overcome the visual illusion phenomenon, can the author further implement this method in the benchmarks of related manipulation and navigation tasks? For example, libero and habitat, calculate the completion rate of related tasks and provide relevant indicators?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YruinEUsTm", "forum": "2eQK1j1idl", "replyto": "2eQK1j1idl", "signatures": ["ICLR.cc/2026/Conference/Submission4289/Reviewer_qNoN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4289/Reviewer_qNoN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4289/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761986486572, "cdate": 1761986486572, "tmdate": 1762917276867, "mdate": 1762917276867, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}