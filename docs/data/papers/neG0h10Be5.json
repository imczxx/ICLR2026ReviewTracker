{"id": "neG0h10Be5", "number": 24890, "cdate": 1758361604811, "mdate": 1759896743729, "content": {"title": "Few-Shot Adversarial Low-Rank Fine-Tuning of Vision-Language Models", "abstract": "Vision-Language Models (VLMs) such as CLIP have shown remarkable performance in cross-modal tasks through large-scale contrastive pre-training. To adapt these large transformer-based models efficiently for downstream tasks, Parameter-Efficient Fine-Tuning (PEFT) techniques like (Low-Rank Adaptation) LoRA have emerged as scalable alternatives to full fine-tuning, especially in few-shot scenarios. However, like traditional deep neural networks, VLMs are highly vulnerable to adversarial attacks, where imperceptible perturbations can significantly degrade model performance. Adversarial training remains the most effective strategy for improving model robustness in PEFT.  In this work, we propose AdvCLIP-LoRA, to our knowledge the first method designed to enhance the adversarial robustness of CLIP models fine-tuned with LoRA in few-shot settings. Our method formulates training as a minimax optimization over low-rank adapters and adversarial perturbations, enabling robust adaptation with a small trainable footprint. Across eight datasets and two backbones (ViT-B/16 and ViT-B/32), AdvCLIP-LoRA achieves state-of-the-art performance in few-shot classification, adversarial base-to-new generalization, and cross-dataset transfer, delivering higher adversarial robustness than prompt tuning baselines without sacrificing much clean accuracy. These findings highlight AdvCLIP-LoRA as a practical approach for robust adaptation of VLMs in resource-constrained settings.", "tldr": "Adversarial training via low-rank adaptation for VLMs in few-shot settings.", "keywords": ["Adversarial Training", "Minimax Optimization", "Low-Rank Adaptation", "Vision Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/745a82e1778fc90cd9cbaab05f9cc2d664957393.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper specifically focuses on adapting Parameter-Efficient Fine-Tuning (PEFT) techniques (e.g., LoRA) to adversarial VLM finetuning to enhance robustness in an efficient scheme. The paper also introduces convergence analyses of the proposed AdvCLIP-LoRA method from a theoretical perspective. Extensive experiments across diverse datasets and backbones demonstrate the efficacy of the proposed method. In addition to adversarially robust few-shot learning, the paper also investigated zero-shot robustness transfer to other unforeseen datasets. Ablation analyses across diverse setups/design choices demonstrate the efficacy of the proposed method."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written and organized. The storyline is straightforward and intuitive. The methodology part is easy to follow.\n2. The paper introduces theoretical analyses regarding the convergence of the proposed PEFT-based adversarial finetuning method.\n3. According to experimental results (especially Tables 1&2), the proposed method shows a significant improvement beyond previous approaches."}, "weaknesses": {"value": "1. The proposed method seems to be a simple combination of LoRA and standard adversarial fine-tuning (TeCoA) [a]. I find it hard to get some novel insights from the paper. Can authors explicitly show the difficulty of merging these two techniques?\n\n2. In addition, the paper lacks experimental comparisons with previous adversarial fine-tuning approaches [b,c].\n\n3. In addition to LoRA, there also exist a lot of PEFT strategies; the author can also discuss and compare some of them, e.g., vision prompt tuning.\n\n4. The paper primarily focuses on robustness evaluation of PGD attacks. However, a lot of adaptive and composite adversarial attack methods are missing, e.g., AutoAttack. The paper should also explore diverse downstream visual-language task extensions.\n\n5. Training time cost comparisons and complexity analyses are missing.\n\n[a] Understanding zero-shot adversarial robustness for large-scale models (ICLR 2023)\n[b] Pre-trained model guided fine-tuning for zero-shot adversarial robustness (CVPR 2024)\n[c] Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models (ICML 2024)"}, "questions": {"value": "1. The paper should include additional comparisons with other PEFT strategies. \n2. Can the authors also test hallucination scenarios in comparison with adversarial perturbations?\n3. Would there be any cross-domain cases for robust few-shot learning?\n4. In addition to the vanilla LoRA extension, is it possible to test some recently improved LoRA pipelines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bCMn2ct76v", "forum": "neG0h10Be5", "replyto": "neG0h10Be5", "signatures": ["ICLR.cc/2026/Conference/Submission24890/Reviewer_jvcV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24890/Reviewer_jvcV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897183382, "cdate": 1761897183382, "tmdate": 1762943233528, "mdate": 1762943233528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AdvCLIP-LoRA, a parameter-efficient adversarial fine-tuning framework that integrates LoRA into CLIP for few-shot learning scenarios. By formulating the problem as a minimax optimization between adversarial perturbations and low-rank adapter parameters, the method aims to improve both robustness and efficiency. Experimental results across multiple datasets demonstrate that AdvCLIP-LoRA achieves strong clean and adversarial accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a systematic exploration of adversarial robustness in few-shot LoRA-based CLIP adaptation, offering a parameter-efficient alternative to adversarial prompt tuning that achieves superior results on both clean and robust accuracy.\n- The paper provides a nontrivial convergence guarantee for the minimax optimization, enhancing the methodological soundness and theoretical completeness of the work."}, "weaknesses": {"value": "- The paper lacks depth. It mainly demonstrates the numerical advantages of LoRA-based adversarial fine-tuning without providing a clear explanation of why it performs better than prompt-tuning-based methods. As an attempt to apply adversarial fine-tuning to a new adaptation paradigm, the paper should comprehensively compare and discuss various fine-tuning frameworks (e.g., prompt tuning, adapter tuning, full fine-tuning) to strengthen its contribution.\n- The experiments are not sufficiently comprehensive, as they only evaluate performance under PGD attacks without including other attack types (e.g., AutoAttack, CW, or black-box attacks).\n- The paper lacks runtime and memory cost analysis, as well as a direct comparison of computational efficiency between LoRA-based and prompt tuning-based adversarial fine-tuning methods."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cGDcv2Aks9", "forum": "neG0h10Be5", "replyto": "neG0h10Be5", "signatures": ["ICLR.cc/2026/Conference/Submission24890/Reviewer_MkaV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24890/Reviewer_MkaV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902458013, "cdate": 1761902458013, "tmdate": 1762943233336, "mdate": 1762943233336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AdvCLIP-LoRA, a method for enhancing the adversarial robustness of Vision-Language Models (CLIP) in the few-shot learning setting. The authors claim to be the first to explore the LoRA (Low-Rank Adaptation) and Adversarial Training for few-shot VLM adversarial robustness, in contrast to the prompt-tuning based adversarial methods. The paper also provides a theoretical convergence analysis that guarantees convergence to a stationary point."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe application of combining LoRA with adversarial training for adversarial robustness in few-shot VLMs.\n2.\tExcellent experimental performance, especially compared to prompt-tuning methods in the 1-shot setting."}, "weaknesses": {"value": "1.\tLimited novelty. Related methods have already been studied in [1, 2, 3], e.g., [1] investigated a similar method, though not specifically in the few-shot setting.\n2.\tLimited theoretical contribution. The convergence analysis relies on several strong assumptions, such as the guarantee that the low-rank matrices A and B remain bounded in each iteration (i.e., not exceeding constants cA and cB in line 286). However, the optimization process lacks explicit constraint on the matrix norms. Therefore, the convergence proof is merely an application of standard non-convex optimization problems and may not provide valuable insights for practice.\n3.\tFew-shot VLMs emphasize data-scarce, resource-constrained settings, however, adversarial training is computationally expensive. The paper lacks discussion on the trade-off between training cost and performance, such as a comparison of training time with [4].\n4.\tExperimental Issues: \nA. There are significant discrepancies between Table 1 and the original paper [3], particularly in the experimental data for 5 methods (AdvVP, APT, AdvMaPLe, AdvVLP, FAP) on OxfordPets, Food101, SUN397. \nB. Related papers and follow-up works often use 11 datasets. Why were EuroSAT, FGVCAircraft, and StanfordCars not included in the experiments?\nC. In Figure 4 (Top-right), the ablation on the attack budget ϵ is incomplete. Would using ϵ=1/255 or 3/255 yield better results? \nD. The performance of the text-only is missing in Table 4. \nE. Some experimental data matches the original paper exactly, while others show minor discrepancies (e.g., AdvVLP in the 1-shot setting on ImageNet-1K in Table 1). Were these methods reproduced? Please provide clarification.\n\n[1] Ji, Yuheng, et al. \"Advlora: Adversarial low-rank adaptation of vision-language models.\" (2024).\n\n[2] Zanella, Maxime, and Ismail Ben Ayed. \"Low-rank few-shot adaptation of vision-language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.\n\n[3] Yuan Z, Zhang J, Shan S, et al. FullLoRA: Efficiently Boosting the Robustness of Pretrained Vision Transformers[J]. IEEE Transactions on Image Processing, 2025.\n\n[4] Zhou, Yiwei, et al. \"Few-shot adversarial prompt learning on vision-language models.\" Advances in Neural Information Processing Systems 37 (2024): 3122-3156."}, "questions": {"value": "My major concerns lie in the limiations on novelty and experiments. See weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GTcloqNCNq", "forum": "neG0h10Be5", "replyto": "neG0h10Be5", "signatures": ["ICLR.cc/2026/Conference/Submission24890/Reviewer_r64Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24890/Reviewer_r64Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979253889, "cdate": 1761979253889, "tmdate": 1762943233141, "mdate": 1762943233141, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a parameter-efficient adversarial fine-tuning scheme (AdvCLIP-LoRA) that applies LoRA to both CLIP encoders and jointly optimizes low-rank adapters with inner-loop PGD on images (generating adversarial images) under a minimax objective. Experiments show the consistent gains in adversarial robustness with minial natural performance drop across different datasets and backbones. Furthermore, the paper introduces a convergence analysis from a theoretical perspective."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper has a clear problem framing about robust few-shot adaptation of CLIP with PEFT, filling a gap left by prompt-only approaches.\n\nThe method is simple to implement and explained with theoretical analyses.\n\nThe proposed method obtains strong gains in robustnss at low shots, with competitive clean accuracy.\n\nTheoretical analysis gives convergence to a stationary point."}, "weaknesses": {"value": "Robustness is evaluated mainly against PGD in L-inifinty norm. There are also some other cases like L2-robustness and AutoAttack Robustness Evaluation.\n\nIt seems that at higher shots, the robustness advantage under PGD narrows and sometimes trails the best prompt baseline on clean accuracy.\n\nExperiments focus on ViT-B/16 and ViT-B/32; larger or newer CLIP/SigLIP backbones are not explored.\n\nThe convergence guarantee relies on smoothness/boundedness assumptions."}, "questions": {"value": "What is the exact trainable-parameter footprint versus full adversarial fine-tuning, and how does computational cost scale with the hyperparameter?\n\nCan adversarially updating prompts or text-encoder inputs further improve robustness given that both encoders receive LoRA adapters?\n\nHow sensitive are results to the PGD step schedule and budget?\n\nWould stronger inner maximizers (adversary generation) (e.g., MI-FGSM/AutoAttack variants) change conclusions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i5EReK9sES", "forum": "neG0h10Be5", "replyto": "neG0h10Be5", "signatures": ["ICLR.cc/2026/Conference/Submission24890/Reviewer_Sya2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24890/Reviewer_Sya2"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24890/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991876283, "cdate": 1761991876283, "tmdate": 1762943232966, "mdate": 1762943232966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}