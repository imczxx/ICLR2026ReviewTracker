{"id": "kT8ZJzf6Jb", "number": 1167, "cdate": 1756856567917, "mdate": 1759898224248, "content": {"title": "Judging with Confidence: Calibrating Autoraters to Preference Distributions", "abstract": "The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or \"autoraters\". However, their reliability is limited by a foundational issue: they are trained on discrete preference labels, forcing a single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that a reliable autorater must learn to model the full distribution of preferences defined by a target population. In this paper, we propose a general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with a distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks.", "tldr": "We propose a general framework for calibrating probabilistic autoraters to any given preference distribution.", "keywords": ["autorator", "preference alignment", "calibration"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8d4ec873fae9de7c1cdfca8e0283844fcec49928.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper argues that automated LLM judges (\"autoraters\") should be trained to output a probability distribution over preferences, rather than a single discrete label. This is motivated by the fact that many evaluation tasks are subjective, ambiguous, or nuanced, and collective human judgment is better represented by a distribution than a single \"ground truth\".\n\nThe authors formalize the target as a population-level Bernoulli parameter $p^*(x) = \\Pr(B \\succ A \\mid x)$ for a given pair of responses $(A, B)$. They propose two general methods to finetune an autorater to verbalize this probability:\n1.  **Supervised Finetuning (SFT):** Used when dense, probabilistic labels (e.g., from multiple annotators) are available.\n2.  **Reinforcement Learning (RL):** Used when only sparse, binary labels are available. This approach uses proper scoring rules (specifically, Brier or Log rewards) to optimize the policy.\n\nThe authors create a synthetic \"ground truth\" distribution by prompting Gemini-2.5-Flash with various personas. Empirically, their finetuned models (especially RL with a Brier reward) show improved alignment (lower MSE) and calibration (lower ECE) against this target distribution, a significant reduction in positional bias, and strong, competitive performance on objective evaluation benchmarks like JudgeBench."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  **Clear and Important Problem Framing:** The paper provides a clear motivation for moving beyond discrete labels in LLM evaluation. It correctly identifies that collapsing subjective, population-level disagreement into a single \"majority vote\" label discards crucial information about uncertainty and minority viewpoints. The proposal to model the full preference distribution is a well-motivated and important direction.\n2.  **Principled Training Methods:** The paper proposes two distinct and practical training strategies tailored to different, realistic data availability scenarios (dense probabilistic vs. sparse binary labels) . The use of proper scoring rules (Brier, Log) for the RL objective is theoretically sound, and the authors provide a proof of Fisher consistency (Proposition 1).\n3.  **Strong Results on Positional Bias:** A key strength is the rigorous analysis of positional bias. The paper uses clear metrics (consistency and symmetry deviation)  and demonstrates that the distribution-matching objective, particularly with the RL-Brier reward, nearly eliminates this bias, achieving near-perfect swap-symmetry (Table 2).\n4.  **Data Efficiency Insight:** The finding that RL with many sparse binary labels is more data-efficient than SFT with fewer dense probabilistic labels (for a fixed annotation budget) is a valuable practical insight, highlighting the importance of prompt diversity."}, "weaknesses": {"value": "1.  **Arbitrary \"Ground Truth\" Distribution:** The entire framework's success is benchmarked against a \"ground truth\" preference distribution $p^*(x)$. However, this distribution is synthetically constructed using a single teacher model (Gemini-2.5-Flash) prompted with a specific, ad-hoc set of 7 personas and hard-coded weights (e.g., \"Everyday User (4x Weight)\" ) at a high temperature ($T=1.0$). This target distribution seems highly design-dependent and arbitrary. It is unclear whether this target represents a meaningful, general-purpose population preference or merely a specific artifact of this particular prompt-engineering process.\n2.  **Ignores Preference Structure and Transitivity:** The paper models each response pair $(A, B)$ as an independent Bernoulli trial. While the authors present this as an advantage that avoids the Bradley-Terry (BT) assumption, it also discards a massive amount of structural information. This formulation does not enforce or model transitivity (i.e., if $A \\succ B$ and $B \\succ C$, then $A \\succ C$). This structure is fundamental to preference learning and is exploited by standard reward models to improve sample efficiency and logical consistency. *The paper provides no analysis of its model's transitivity violations nor any comparison against standard BT-style reward model baselines.*\n3.  **Training and Evaluation Data Overlap:** The main results for alignment (MSE) and calibration (ECE) in Table 1 are reported on a test set where the labels were generated by the *exact same* Gemini-2.5-Flash + persona methodology used to create the training data. While OOD results are provided later, these headline results may primarily demonstrate successful *imitation* of the teacher's specific (and arbitrary) label distribution, rather than a more general improvement in judgment or calibration.\n4.  **No Downstream Task Validation:** The paper strongly motivates the need for probabilistic outputs for \"effective risk management\"  and \"cost-sensitive decision-making\". However, this practical benefit is never demonstrated. There are no downstream experiments (e.g., using the probabilistic rewards in an RLAIF pipeline) to show that these calibrated probabilities provide a tangible advantage over simpler discrete judges, which could potentially be calibrated post-hoc.\n5.  **Missing Robustness Details:** The RL method relies on a parser $g$ to extract the numeric probability from the model's text output. The paper states that the reward function drives the parsability rate $s_{\\theta}(x)$ \"toward 1\"  and that the optimal policy is \"parsable everywhere\", but it never reports the *empirical* parsability rate. This makes it difficult to assess the robustness of this method during training and inference."}, "questions": {"value": "The main concerns are presented in the weaknesses. Moreover, I am very curious about the exact output distribution of the calibrated probabilities. From my experience, even with calibration, the model would still output some pseudo-probabilities - they do not have mathematical meaning and are basically some sparsed, discontinuous scores like 0.7, 0.4, or something alike.  I wonder what your model's outputs look like, and if it still looks like that, I highly doubt whether the model is just imitating the behavior of outputting scores or it really knows how to predict meaningful probabilities as claimed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "02agbFQClz", "forum": "kT8ZJzf6Jb", "replyto": "kT8ZJzf6Jb", "signatures": ["ICLR.cc/2026/Conference/Submission1167/Reviewer_ZXxy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1167/Reviewer_ZXxy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761108802271, "cdate": 1761108802271, "tmdate": 1762915695729, "mdate": 1762915695729, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work 1/ argues for the superiority of a probabilistic autorater (LLM used a rating judge), ie a model able to model a whole distribution instead of a single label, to better represent the views of a population ; 2/ suggests two routes to build such distributional autoraters, SFT in the dense case, when multiple annotations are available, and RL in the sparse case, with only one label ; 3/ elects to train models to better verbalize probability, building on the ability of the model to generate natural language rationales ; 4/ shows good results on alignment, calibration as well as resilience to positional biases, with the \"RL‑Brier\" variant showing the best overall trade‑off ; and 5/ finds their approach to adapt well OOD, to human annotations and to an objective benchmark."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the problem formulation is clear and indeed compelling: estimating probabilities by a model-judge is a nice way to retain the inherent uncertainty from a population ;\n- using a verbalized probability is appropriate, principled and well implemented ;\n- good experimental results, alignment and calibration, for the two models (Gemma-2-9B and Qwen-2.5-7B) ;\n- the distribution-matching fine-tuning appears to eliminate position bias (by looking at what happens when swapping the answers) ;\n- the results are confirmed OOD, either with human-annotations or for objective tasks ;\n- the extra study on the budget is interesting: it's better to do RL on many prompts annotated once than SFT on less prompts with less annotations."}, "weaknesses": {"value": "- the metrics used for the evaluation rely on the majority label when the whole point is to better model the distribution of views ;\n- the distribution for the main experiment comes from Gemini not a human population. That there is a confirmation of the results on the PandaLM dataset is nice but in that case the majority vote is treated as the ground truth ;\n- relying on natural language is relevant but it comes with potential issues when parsing to extract the probabilities ;\n- the budget study, admittedly not core to this work (but it is presented as \"key finding\"), feels a bit rushed: one would like to see what happens when using the same prompts for example ;"}, "questions": {"value": "- have you looked carefully at the influence of the prompts, and their diversity, on the quality of the modeling?\n- similarly, one has to wonder about the magic number of 50K annotations for the training (50x10 or 50x1): have you experimented with less/more annotations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N72oDI1vin", "forum": "kT8ZJzf6Jb", "replyto": "kT8ZJzf6Jb", "signatures": ["ICLR.cc/2026/Conference/Submission1167/Reviewer_eNX3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1167/Reviewer_eNX3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761312977489, "cdate": 1761312977489, "tmdate": 1762915695588, "mdate": 1762915695588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a methodology for training large language models (LLMs) to serve as automated judges. Specifically, the model is trained to predict dense, probabilistic labels through chain-of-thought (CoT) reasoning by using supervised fine tuning (SFT), and subsequently refined using reinforcement learning based on binary labels to further enhance its judgment capability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper suggests a new reward modeling approach for RL training in LLM. Brier Reward, which differs from the conventional logarithmic reward commonly used for training within the [0, 1] reward range. Which demonstrates high performance compare with previous approach.\n2. The paper show that the model trained using this method performs well not only on difference test datasets but also on different objective test set too. This result suggests that the proposed approach generalizes effectively and does not overfit to a specific dataset.\n3. The paper is well-written and easy to follow, clearly explaining both the experimental design and the intended objectives behind each component of the study."}, "weaknesses": {"value": "1. Since the proposed method assumes that distributional labels are already provided at the annotation stage, the paper’s contribution is  limited to training a model to accurately predict the given scalar label values. The learning approach extends conventional LLM-as-a-judge tasks, which predict scalar scores, into a continuous (float-level) prediction, which represents only a marginal conceptual advancement.\n2. The model is trained through labeled data, yet the primary baselines used for comparison are zero-shot and calibration-based methods that do not involve any training. This makes the comparison inappropriate. It would be more valid to compare against other fine tuning approaches using similar training setups.\n3. The paper compares its model against other fine-tuned models as baselines (JudgeLM-7B, PandaLM-7B, etc); however, this comparison is still unfair. A valid evaluation should involve models that share the same training data and base model, differing only in the fine-tuning method. As it stands, the reported results measure absolute performance levels rather than truly assessing the validity or effectiveness of the proposed method."}, "questions": {"value": "1. The paper argues that predicting probabilistic labels rather than binary labels is important for improving model performance. However, generating probabilistic labels inherently requires more data than binary annotations. The paper does not address this issue. It would be interesting to see an experiment where the total number of preference samples is fixed, directly comparing the performance between models trained with binary labels and those trained with probabilistic labels.\n\n2. Does the paper's approach have any explicit advantages compared to \"generative reward models\" paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DmPoZeBSl7", "forum": "kT8ZJzf6Jb", "replyto": "kT8ZJzf6Jb", "signatures": ["ICLR.cc/2026/Conference/Submission1167/Reviewer_RzUv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1167/Reviewer_RzUv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761816313169, "cdate": 1761816313169, "tmdate": 1762915695456, "mdate": 1762915695456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a method for calibrating LLM-as-a-judge (that is, autoraters) for preference distributions. More specifically, the authors go beyond the binary label and model the whole preference distribution as a Bernoulli distribution of which the output is the original binary label. The discussions are made within verbalized probabilities. The authors suggest that the Bernoulli's $p$ can be learned and calibrated by probabilistic labels, including either averaging $m$ binary labels (and then do direct SFT) or adopting a loss between binary labels and probabilistic outputs (and then do RL). The methods show improvements over alignment MSE, preference accuracies, and calibration performance when compared to methods from zero-shot baselines and calibration baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is clearly written. Going beyond binary labels is an active research trend. This paper motivates why single binary labels are insufficient. The paper is also well-organized and easy to follow.\n\n2. The model is simple but effective. The authors model the learning problem as learning the Bernoulli's $p$, which is one step beyond binary labels. Although I think $p$ itself might not be anough (for example, if $p = 0.5$, there are differences between what's controversial (e.g., two groups of people favoring strongly for different answers) and what's moderately vague (e.g., all people do not show strong preference between different answers)), I do think $p$ is good point to start.\n\n3. The empirical results are valid. I think the authors have included the necessary details of empirical results."}, "weaknesses": {"value": "1. The source of label itself. The label comes from a \"teacher\" model (also known as the oracle). I agree that it's hard to collect real human-annotated data under probabilistic labels so LLM-as-a-judge can work as a proxy. But the problem is, the performance is also evaluated with that \"teacher\" model's probabilistic outputs. The effectiveness of the method is kind of a circular argument: we can improve LLM-as-a-judge by calibrating it with another perfectly calibrated LLM-as-a-judge (the teacher model). I'm not criticizing using LLM-as-a-judge as a proxy for human-annotated data, but assuming the access to oracle (no matter human or LLM) label distribution is a bit unrealistic.\n\n2. The OOD performance is not satisfying. As I have discussed in the previous point, the main results are made under the assumption that the training data and the evaluation come from the same source. It means that the learner has access to (a noisy version of) the oracel $p^\\ast$; Using that additional information in the training unsurprisingly increases the performance. But for other cases where the OOD issue is possible, the performance is not satisfying (see Table 3 and Table 4)."}, "questions": {"value": "1. Please identify my concerns about the source of label. What are the outcomes if you adopt real human-annotated data as both the training and the evaluation data, instead of LLM-as-a-judge? I think that's important. Maybe the real human-annotated data is too limited to do such training, but you can provide results when training data is from \"LLM-as-a-judge trained from human data\" and evaluation is conducted on real human data (no OOD).\n\n2. Please identify my concerns about the OOD issue. I think it's very unsurprising that you get improvement with access to the oracle $p^\\ast$ and train your model on that. But what if you only get one LLM-as-a-judge as the training data and another as the evaluation data? I would appreciate it if you capture your calibration error/performance drop within the scope of OOD severity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BzqVjFzEdh", "forum": "kT8ZJzf6Jb", "replyto": "kT8ZJzf6Jb", "signatures": ["ICLR.cc/2026/Conference/Submission1167/Reviewer_9YcT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1167/Reviewer_9YcT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1167/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029935216, "cdate": 1762029935216, "tmdate": 1762915695265, "mdate": 1762915695265, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}