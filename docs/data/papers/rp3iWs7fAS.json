{"id": "rp3iWs7fAS", "number": 2371, "cdate": 1757065161530, "mdate": 1759898152738, "content": {"title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "abstract": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code processing capabilities, evaluating their performance on engineering-level code remains challenging. Existing repository-level benchmarks primarily focus on single scenarios, such as code generation or bug fixing, without adequately capturing the diversity and complexity of real-world software or project engineering workflows. Furthermore, these benchmarks suffer from limited controllability in question positioning and reliability issues in their generated test cases. To address these limitations, we present CorePipe, a fully automated pipeline that converts repositories into comprehensive test cases, and introduce CoreCodeBench, a configurable multi-scenario repository-level benchmark. To simulate real engineering scenarios, CorePipe generates three types of atomic questions (Development, BugFix, and Test-Driven Development) specifically targeting core code segments. These atomic questions are further combined into three types of composite questions, with difficulty levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides a comprehensive and extensive repository-level benchmark to investigate the applicability of LLMs in real-world engineering projects. Experiments with 16 LLMs across diverse scenarios reveal varying capabilities and offer multi-dimensional insights into LLM performance in engineering contexts. Code of CorePipe and data of CoreCodeBench are available.", "tldr": "", "keywords": ["Large Language Models", "Benchmark", "Code Generation"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e514430117423b3eacdcfb6ef65005dcb8760320.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces CoreCodeBench, a new repository-level benchmark designed to evaluate the coding capabilities of Large Language Models (LLMs). The authors argue that existing benchmarks are challenging to use for engineering-level code because they typically focus on single scenarios (like only bug fixing or code generation), lack diversity, and suffer from poor controllability and reliability in their test cases.\n\nTo solve this, the paper presents CorePipe, a \"fully automated pipeline\" that processes GitHub repositories to generate comprehensive, high-quality test cases. CorePipe is designed to simulate real-world engineering workflows by generating six distinct types of programming tasks. These are divided into three Atomic (Single-Function) tasks, including Development, BugFix, and Test-Driven Development. CorePipe can combine multiple atomic problems to test an LLM's ability to handle complex interactions, cross-file dependencies, and implementation planning.\n\nThe pipeline works by first preprocessing repositories (selecting active, complex projects with good test coverage), generating function call trees, and then identifying \"core code segments\" to create problems. The difficulty of these problems is configurable by adjusting hyperparameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper directly tackles two critical challenges of existing benchmarks: their \"Single Scenario\" focus and their \"Lack of Controllability and Reliability\". \n2. Unlike benchmarks focused on one task, CoreCodeBench provides six task types across single- and multi-function scenarios. This provides a \"multi-dimensional\" and more realistic assessment of an LLM's practical engineering skills.\n3. The CorePipe pipeline is \"fully automated\" and can generate all six problem types in a single run without any human intervention. This makes the benchmark highly scalable and continuously evolvable.  The benchmark's difficulty is configurable, adjusting hyperparameters that control the complexity of multi-function problems (e.g., call depth $d$ and number of functions $n$).\n4. The paper uses both the standard AC@1 (pass/fail) metric and a complementary AC Rate. AC Rate provides a finer-grained assessment by measuring the relative improvement over the retest baseline, allowing it to capture partial correctness."}, "weaknesses": {"value": "1. The data statistics in Table 2 show a significant imbalance in problem generation. While there are 511 Development problems, there are only 10 Multi-BugFix problems. The authors acknowledge this, noting that a smaller number of available problems in this category makes it difficult to draw strong conclusions. In fact, I am rather skeptical about the authenticity of the Multi-BugFix scenario. In a real scenario, even though there are multiple bugs, the fixing process is done one by one in sequence rather than fixing all bugs at once. Therefore, I think the value of this task is questionable.\n2. For BugFix problems, the pipeline focuses more on constructing code snippets that contain logical errors. It does this by using an LLM to generate erroneous logic descriptions and a smaller LLM to write the buggy code. This simulation process may not fully capture the nuance and diversity of complex, human-introduced bugs found in real-world development.\n3. While the manual inspection is a strength, it was not comprehensive. The 78.55% qualification rate was derived from a sample of 30 problems per repository. The authors' release of a separate \"CoreCodeBench-Dev-Verified\" subset implies that the main benchmark still contains unverified or potentially flawed problems. This also means that throughout the entire dataset construction process, the role of humans cannot be completely separated."}, "questions": {"value": "Refer to weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gXg1aFnQto", "forum": "rp3iWs7fAS", "replyto": "rp3iWs7fAS", "signatures": ["ICLR.cc/2026/Conference/Submission2371/Reviewer_tyYJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2371/Reviewer_tyYJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827853898, "cdate": 1761827853898, "tmdate": 1762916212485, "mdate": 1762916212485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents an automation pipeline to convert repositories into comprehensive teset cases, besides, this paper introduces a benchmark corecodebench to configure multi-scenario repository-level benchmark. Their experiments present results with 16 LLMs across diverse scenarios reveal varying capabilities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a useful automatic pipeline to construct high-quality test cases from repositories, which is very promising to accelerate the code agent development.\n2. besides, they proposes a large-scaled dataset besides the framework. This dataset contains many cases over the average. line 3414.\n3. They present a detailed experiments comparing with other dataset and over 16 LLMs."}, "weaknesses": {"value": "1. human quality control over the new benchmark is not sufficient. A new automatic framework along with a new benchmark requires sufficient human quality control over each process. However, this paper does not illustrate sufficient human quality control process. only 30 samples from each repository are selected for inspection."}, "questions": {"value": "1. how to demonstrate the new framework can address the challenge 2: lack of controllability and reliability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TUsA2CkBDl", "forum": "rp3iWs7fAS", "replyto": "rp3iWs7fAS", "signatures": ["ICLR.cc/2026/Conference/Submission2371/Reviewer_2pU5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2371/Reviewer_2pU5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973401661, "cdate": 1761973401661, "tmdate": 1762916212126, "mdate": 1762916212126, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CorePipe, which is an automated pipeline that converts real repositories into testable instances. Using this pipeline, the authors presents CORECODEBENCH, a configurable benchmark for evaluating repository-level code generation ability with three single-function tasks and three multi-function tasks. Experiments on 16 LLMs suggest a gap in solving multi-function tasks and BugFix tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper benchmarks 16 major LLMs across six diverse programming tasks, providing a broad and comprehensive analysis. The results clearly reveal performance differences among models, offering valuable insights into current model limitations in multi-function and BugFix tasks.\n\n2. The proposed CorePipe design, which scales from single-function to multi-function tasks, provides a systematic framework to study how model performance evolves with increasing code module complexity and effectively simulates repository-level code development in a controlled setting.\n\n3. The inclusion of manual verification, the qualification rate, supports dataset reliability and enhances the credibility of the automatically generated benchmark."}, "weaknesses": {"value": "1. The authors stated in the introduction (Challenge 2) that SWE-bench has relatively fixed testing locations tied to historical PRs, and that CorePipe overcomes this limitation. However, CorePipe itself relies on existing unit tests in the repository and can only generate test cases for code snippets covered by these unit tests. Consequently, it cannot ensure flexible control over test position either. In this sense, CorePipe does not fully improve positional controllability compared to SWE-bench. It would be better if the authors can clarify the intended meaning of \"flexible position\" further to avoid confusion.\n\n2. The current design of the Multi-BugFix setting appears limited in its realism and discriminability. It simply combines several single function problems with call relationships and requires LLMs to correct multiple known buggy snippets simultaneously. However, real repository-level debugging usually involves locating the root cause of a single observed failure that propagates across modules, not fixing a predefined set of bugs with unit tests. As the authors also mentioned, the correlation coefficient between Single-BugFix and Multi-BugFix results is 0.85, which is quite similar but different from the real-world software debugging.\n\n3. The benchmark defines three single-function problem types to represent different aspects of software engineering, but these categories remain largely independent in the multi-function setting. The authors don't consider cross-type interactions. Although it is reasonable to keep BugFix tasks separate, integrating Development and TDD subtasks could more realistically simulate the scenarios where new implementations and test-based development co-exist within the same module."}, "questions": {"value": "1. The paper describes an LLM-based process for identifying “core code” functions but does not specify any supervised validation or manual verification to ensure the correctness of this identification. Could the authors clarify how they assess or ensure the reliability of the LLM’s chosen core segments?\n\n2. The paper generates BugFix tasks by using LLM to to produce an erroneous logic description and then asking a small LLM to implement the buggy code accordingly. Could the authors clarify whether any validation or analysis was conducted to ensure the quality of the generated erroneous logic - specifically, that it is incorrect yet reasonable? Additionally, how do the authors ensure that the smaller LLM faithfully reflects the intended error rather than introducing unintended or multiple inconsistencies?\n\n3. The paper uses Pass@1 as the percentage of problems whose first generated solution passes all tests, and AC Rate as the proportion of previously failing tests that are newly passed by the model’s code. Could the authors clarify the motivation for including tests that already pass without any modification in the Pass@1? Would it be more reasonable to exclude these tests from evaluation entirely because these tests can still be passed even without any code modification?\n\n4. In the quality inspection stage, the paper mentions retaining problems that none of the evaluated models could solve. Could the authors clarify why they decide to keep these problems? How do they verify that such cases represent actually challenging but valid tasks rather than instances where the LLM-generated descriptions or explanations are misleading or inconsistent with the target code and tests?\n\n5. The paper reports a manual inspection with a 78.55% qualification rate to support dataset reliability, but the definition of “qualified” is unclear. Could the authors clarify what specific criteria were used here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "joc3uo8SDM", "forum": "rp3iWs7fAS", "replyto": "rp3iWs7fAS", "signatures": ["ICLR.cc/2026/Conference/Submission2371/Reviewer_5qeC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2371/Reviewer_5qeC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997690613, "cdate": 1761997690613, "tmdate": 1762916211255, "mdate": 1762916211255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper ntroduces a new benchmark designed to evaluate large language models (LLMs) on complex, real-world software engineering tasks. The authors argue that existing benchmarks—like SWE-Bench and BigCodeBench—mainly test narrow tasks (e.g., code generation or bug fixing) and fail to represent the variety and complexity of real engineering workflows. To address this, they propose CorePipe, an automated pipeline that transforms real code repositories into diverse problem types, and build CoreCodeBench, a repository-level benchmark that spans six scenarios including Development, BugFix, and Test-Driven Development (TDD), across both single- and multi-function levels.\n\nCorePipe automatically identifies core code segments within repositories and generates problem sets by masking code, injecting logical bugs, or linking test cases. It supports fine-grained control over task difficulty using parameters that determine how functions interact, effectively simulating real-world coding challenges. CoreCodeBench is created from 12 repositories, producing over 1,500 problems that range in complexity and realism. Evaluation is done using two main metrics—AC@1, which checks if the first generated solution passes all tests, and AC Rate, which measures incremental correctness improvements. The authors also apply quality filtering, including an Information Gain (IG) check and human annotation, to ensure data reliability, reporting a 78.5% qualification rate for Development problems \n\nExperiments on 16 LLMs (including GPT-5, Claude 3.7, Gemini 2.5, and Qwen3-Coder) reveal that while advanced models perform well on single-function tasks, their accuracy drops sharply in multi-function scenarios. This shows that current LLMs struggle with planning and reasoning across dependent code segments. Overall, CoreCodeBench provides a structured, configurable framework for assessing coding models in realistic engineering contexts, highlighting both their strengths and persistent limitations in handling multi-file, interdependent programming problems"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "One strength of this paper is that it tackles a real gap in evaluating coding models. Most existing benchmarks only test narrow, isolated tasks like writing or fixing short pieces of code. This work instead focuses on repository-level evaluation — meaning it looks at full projects with real dependencies and interactions between functions. By creating six different types of coding tasks (such as Development, BugFix, and TDD), it captures a much wider range of what real developers do. This broader scope makes the benchmark more realistic and useful for understanding how LLMs might perform in actual software engineering situations.\n\nAnother strong point is the CorePipe system itself. It automatically converts real GitHub repositories into structured test cases with controllable difficulty, reducing the need for human curation. The pipeline ensures that problems come from important parts of the code and maintains reliability through several quality checks, including automated filters and manual review. This automation and configurability make the benchmark scalable — future researchers can easily generate new or harder tasks as LLMs get better. It’s a well-engineered framework rather than just a static dataset.\n\nFinally, the paper provides a detailed and fair evaluation of many current models, both open-source and proprietary. The authors use clear metrics (AC@1 and AC Rate) and analyze results across multiple problem types to show where models succeed and fail. Their findings — such as the drop in performance on multi-function problems — give practical insights into current LLM limitations. This makes the paper not only a dataset proposal but also a valuable diagnostic tool for future model development."}, "weaknesses": {"value": "A key weakness of this paper is that, despite its technical sophistication, the overall scientific contribution feels somewhat incremental. It mainly extends existing repository-level benchmarks like SWE-Bench or RepoExec by adding more task types and an automated data generation pipeline. While the design of CorePipe is thorough, it largely combines and refines ideas that have already appeared in earlier works — such as masking code segments, using LLMs for test generation, or leveraging repository structures. There’s no fundamentally new paradigm introduced for how to evaluate or model code reasoning, which limits its novelty from a research standpoint.\n\nAnother issue is the heavy dependence on LLMs for generating and validating data. Although the paper includes an Information Gain filter and manual checks, using LLMs to create “ground truth” problems can still introduce subtle biases or logical inconsistencies. The fact that only about 78% of the problems passed human verification suggests quality control remains a concern. Without stronger validation or independent baselines, it’s hard to fully trust the benchmark’s objectivity, especially when evaluating models that are similar to the ones used to build it.\n\nLastly, while the benchmark aims to mimic real engineering environments, it still oversimplifies many aspects of real-world software development. The tasks are limited to small-scale, function-level or short multi-function snippets and don’t cover more complex activities like refactoring, dependency management, or long-term version control reasoning. The multi-function problems, though larger, are still synthetic rather than derived from genuine project evolution. In short, the paper makes progress on scale and diversity, but it doesn’t yet capture the full realism or workflow dynamics of professional software engineering."}, "questions": {"value": "1) Since CorePipe uses LLMs to generate and validate problems, how do the authors ensure that the “correct” solutions are truly correct, and not just aligned with the model’s generation bias? Could the benchmark unfairly favor models similar to the ones used for data generation (e.g., GPT-4o, Claude 3.5)? If so, how might one mitigate this leakage?\n\n2) The benchmark focuses on single- and multi-function snippets. How well do these settings represent actual repository-level workflows where developers need to reason across many files, modules, and dependencies over long histories?\n\n3) How consistent are the evaluation results when the same model is prompted differently or when context size changes? Does CoreCodeBench measure robustness to prompt variation?\n\n4) How does CoreCodeBench generalize across programming languages beyond Python? Could the same pipeline scale to mixed-language repositories or low-resource ecosystems?\n\n5) Could the authors clarify what new scientific questions CoreCodeBench enables that previous datasets could not address?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GHTSlnR9c1", "forum": "rp3iWs7fAS", "replyto": "rp3iWs7fAS", "signatures": ["ICLR.cc/2026/Conference/Submission2371/Reviewer_AZuj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2371/Reviewer_AZuj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2371/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762136036924, "cdate": 1762136036924, "tmdate": 1762916211039, "mdate": 1762916211039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}