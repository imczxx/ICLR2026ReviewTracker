{"id": "5GVfneFvhq", "number": 17514, "cdate": 1758277037607, "mdate": 1759897169967, "content": {"title": "WaterDrum: Watermark-based Data-centric Unlearning Metric", "abstract": "Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. Existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when the forget and retain sets have semantically similar content and/or retraining the model from scratch on the retain set is impractical. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking to overcome these limitations. We introduce new benchmark datasets (with different levels of data similarity) for LLM unlearning that can be used to rigorously evaluate unlearning algorithms via WaterDrum.", "tldr": "We propose the first data-centric LLM unlearning metric based on watermarking that is effective and practical.", "keywords": ["machine unlearning", "watermarking", "metric", "LLM"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3607407edb505ab97a08932e46c9b8e8df3685e.pdf", "supplementary_material": "/attachment/278d6ad9262948acfc6b47ef34bb5fde12cea2d8.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes WaterDrum, the first data-centric unlearning metric for LLMs, which leverages robust text watermarking to overcome the limitations of existing utility-centric metrics that fail to accurately measure the degree of unlearning in real-world scenarios."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The idea of evaluating unlearning from the watermarking perspective is novel and insightful.\n2. The introduction of the new benchmark WaterDrum-Ax provides a useful foundation for future research in this area."}, "weaknesses": {"value": "1. The proposed method has an inherent limitation, it requires the training data to be watermarked prior to model training.\n2. The methodology section is somewhat verbose and repetitive, which makes the experimental part appear less substantial in comparison."}, "questions": {"value": "1. Apart from Waterfall, are there other text watermarking methods that could potentially meet WaterDrum’s watermarking requirements? Have you considered conducting compatibility tests with alternative watermarking schemes?\n2. For datasets that have already been released without watermarks, are there any remedial or alternative approaches that would allow WaterDrum to still be applied for unlearning evaluation?\n3. Can WaterDrum be extended to assess unlearning during the pre-training stage, rather than only in fine-tuning? If so, would it require adjustments to the watermarking strategy or validation process?\n4. Beyond the LLaMA2-7B model, have you evaluated WaterDrum on other model families or larger-scale models? If so, how do the experimental results differ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rcdtzi1aJG", "forum": "5GVfneFvhq", "replyto": "5GVfneFvhq", "signatures": ["ICLR.cc/2026/Conference/Submission17514/Reviewer_ftSk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17514/Reviewer_ftSk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760619871998, "cdate": 1760619871998, "tmdate": 1762927395271, "mdate": 1762927395271, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper first points out that practical unlearning is not really well tested by existing unlearning benchmarks. Existing evaluation protocols 1) don’t test on semantically similar data, 2) need to train a “retrained LLM” (I like to call this the “ideal LLM”) and 3) need better ways to elicit the inclusion of a data point.\n\nThe main contribution of this work is a new metric for unlearning, that measures the model’s ability to unlearn similar data (?) and a new dataset based off of arxiv, which forms a new unlearning benchmark. Results show that the WaterDrum metric is superior and matches the desiderata laid out in the paper."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "S1. This paper raises both interesting and important point that are neglected in unlearning benchmarks: \n\n1) existing benchmarks (like TOFU) evaluate degradation of general model capabilities after unlearning, which does not truly evaluate whether the data has been forgotten and retain the desired data.\n\n2) unlearning benchmarks need to think closely about how to elicit whether a model has forgotten or retained a datapoint.\n\n3) practically, the forget set and retain sets may be semantically close to each other. if you’re just evaluating model capabilities, you would miss a more fine grained notion of whether the model has forgotten.\n\nBecause the focus is interesting, I wouldn’t mind if this paper was accepted."}, "weaknesses": {"value": "W1. The presentation of this paper is not ideal, in my opinion. While I can see the authors obviously spent a lot of time in clarifying the writing, I think the notation is extremely dense for a paper that is not theoretical in nature.\n\nW2. I think the focus on “desiderata” and how WaterDrum achieves them is not the best way to present this topic. While I appreciate the sincere thought about what an unlearning benchmark should be, I feel like there’s a more straightforward presentation of your findings and contributions by **highlighting gaps with current benchmarks and showing how your benchmark provides more sensitivity.** I feel this is a more sensible approach, given that some of the desiderata are plainly intuitive e.g. separability — I don’t really need a whole mathematical definition of what is basically captured by AUC ROC.\n\nThe paper focuses on interesting points, but I think the paper’s impact would be much better if it is presented in a more narrative way. In fact, I don’t really understand what is going on with most of the results as they point to all different aspects of the authors’ framework which I think is overtheorized.\n\nW3. Finally, the last weakness is regarding the watermarking aspect. From what I gather, there needs to be a way to elicit whether an unlearned model forgets or retains data points. Watermarking could be a nice way to trace that, however, it seems that watermarking and detecting those watermarks would only focus on membership information. It does not seem to me to necessarily capture the higher level concepts within the forget set that is actually intended to be forgotten."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UBB4QlQpzs", "forum": "5GVfneFvhq", "replyto": "5GVfneFvhq", "signatures": ["ICLR.cc/2026/Conference/Submission17514/Reviewer_mg5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17514/Reviewer_mg5c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900590670, "cdate": 1761900590670, "tmdate": 1762927394596, "mdate": 1762927394596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework to evaluate how well a dataset has been unlearned from a LLM. The paper focuses on a setting where one has only a black-box  access to the proposed \"unlearned\" LLM, and no access to a baseline retrained LLM. As such, I consider this to be first and foremost a paper about auditing black box model.\n\nThe authors formulate four main properties their auditing system: \n1. separability, i.e, the metric is able to beat a random guesser on whether an llm answer was influenced by data belonging to the unlearned set or not)\n2. calibration, to take into account imperfect unlearning in practice\n3. practicality, i.e non-reliance on a retrained model\n4. robustness to similar data between the unlearned dataset and the retained dataset\n\nFrom these desired properties, the authors conclude that the use of a watermarking system is well-suited to design a metric fulfilling these properties.\n\nThey further go on to specify the desired properties of a watermarking scheme useful for the auditing task -- which I won't describe since they are pretty straightforward properties of most watermarking systems contrary to what the authors claim. \n\nFinally, the auditing system is described as follow: content from each owner is watermarked using a specific key, unique to the owner but fixed across datapoints of this owner. The verification system then detects the presence of the watermark from a the proposed LLM output using a query formed using data points belonging in the unlearned dataset.\n\nFinally, the authors instantiate their auditing system using the WaterFall watermarking scheme and validate the performance of their method against other metric such as ROUGE, Truth Ratio, KnowMem and some membership inference attack.\n\nI also note that the authors propose a new dataset for their benchmark based on the collection of arXiv abstract."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Clarity**: This might be the greatest strength of the paper. Every desired property is extremely well-defined, with a clear formalism and pedagogical explanations. The idea is ell motivated and simple in its implementation. Similarly, the experimental metrics look well chosen and motivated to my non-expert view (with one exception that I will touch in the questions). Consequently, the experiments were convincing to me. \n\n**Originality**: I am not qualified to judge on this point as I am not an expert on unlearning evaluation. However, the use of watermarking for tracking the use of data across LLM training sets is definitely not new, for example in the landmark paper [1] (strangely not being cited !).\n\n**Significance**: I admit to be impressed by the elegant solution this paper propose to black-box auditing. The idea of **not** relying on data for the auditing part allows some very impressive results and flexibility not available without watermarking and I such I find this work quite significant when watermarking is actually available in practice."}, "weaknesses": {"value": "**Some claims are not substantiated**: To be precise, the necessity of watermarking for unlearning metrics, as claimed in Remark 1, is not obvious to me from the desiderata. I would be hard pressed to find a better solution fulfilling both the robustness and separability aspects, but the author do not provide a proof either. Since the onus is on them, I would either retract from such claims or provide actual proof. \n\n**Over-reliance on a specific, watermarking scheme**: Another important unsubstantiated claim is the uniqueness of the WaterFall algorithm in meeting the desiderata. The paper feels heavily skewed towards promoting this specific framework, which appears to be a very recent paper from 2024, possibly from the same research group.  I found it quite surprising that from the classical text watermarking schemes such as [2,3,4] only KGW was (quickly) considered in the appendix (I). I don't understand the reason for disregarding them, especially since the \"radioactivity\" [1] demonstrated that one can use them and recover a watermark signal in text generated by LLM trained on such schemes. I looked at the Waterfall paper and could not find such a discussion the main body either.  To be precise, the paper dismisses several schemes in the appendix by claiming they fail W5, but this seems like a post-hoc justification to select Waterfall. It's not clear why other schemes couldn't be adapted. \n\n**Lacking references**: This is somewhat of a follow up to the previous weakness. I cannot assess this for the unlearning part, but the choice of references for watermarking is somewhat bizarre. Kirchenbauer is incorrectly referred to as \"model watermarking\": it is not post-hoc indeed, but does not watermark the model. It watermarks the generated text by modifying the sampling distribution. Aaronson [3] is not cited, despite being the first distortion-free scheme. The authors seems to have focused on post-hoc schemes, but I don't understand why. Furthermore, I don't see any reference to the use of watermark as a tracking technology for training data despite -- e.g. [1] using it exactly for this.  The paper feels, at times, more like an application paper for Waterfall than a comprehensive exploration of watermarking for unlearning evaluation."}, "questions": {"value": "**Questions**:\n- Most importantly, I would advise the author to refrain from promoting Waterfall this much without providing better arguments as to why it is necessary or unique within other text watermarking in achieving the desiderata. Either the author should provide a better discussion/experimental proofs that other schemes do not meet the desiderata or tone down the claims towards Waterfall.\n\n- I don't understand the possibility of forgetting data when **exact** duplicates can be found between the retain and forged set. Maybe I am misunderstanding something in the experimental design. I understand the paraphrased data will be different thanks to the use of different keys. But since the **original** data is still the same, what is the point ? Is it only for illustration purposes of the failure of classic unlearning algorithms. I would appreciate the authors make this point clearer in the paper.\n\n- I feel that the paper lacks a calibration study for other watermarking scheme such as KGW and Aaronson. I don't ask for a whole benchmark, but given that these schemes have been shown to contaminate output data for LLM in [1], I believe such a study would at least make the choice of Waterfall a bit more meaningful.\n\n**Recommendation**: I am somewhat torn about this paper. On one hand, I really like the idea of leveraging watermarking capabilities to allow black-box auditing. Furthermore, the paper is very meticulous in describing what it wants to achieve and I found it to be a joy to read in that regard. On the the other hand, the lack of engagement with rest of the field and the huge promotion of Waterfall makes me somewhat suspicious. I am ready to give a *borderline accept* thanks to the attractive idea but would gladly increase my score if the authors can engage with the rest of the art and **really** demonstrate both theoretically and empirically the superiority of Waterfall compared to other schemes. Note that, although I can be considered an expert on **watermarking**, I am definitely not an expert in **unlearning** and I might be missing some insights in the current literature on the well-foundedness of the approach.\n\n**References**:\n\n- [1] Sander, Tom, et al. \"Watermarking makes language models radioactive.\" Advances in Neural Information Processing Systems 37 (2024): 21079-21113.\n- [2] Kirchenbauer, John, et al. \"A watermark for large language models.\" International Conference on Machine Learning. PMLR, 2023.\n- [3] Aaronson, S. My AI Safety Lecture for UT Effective Altruism, November 2022. URL https://scottaaronson.blog/?p=6823.\n- [4] Dathathri, Sumanth, et al. \"Scalable watermarking for identifying large language model outputs.\" Nature 634.8035 (2024): 818-823."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dTkHUMkp34", "forum": "5GVfneFvhq", "replyto": "5GVfneFvhq", "signatures": ["ICLR.cc/2026/Conference/Submission17514/Reviewer_mahr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17514/Reviewer_mahr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17514/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932429861, "cdate": 1761932429861, "tmdate": 1762927393902, "mdate": 1762927393902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}