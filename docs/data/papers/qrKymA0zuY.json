{"id": "qrKymA0zuY", "number": 22000, "cdate": 1758324634919, "mdate": 1763352883680, "content": {"title": "Explaining Multimodal LLMs via Intra-Modal Token Interactions", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet their internal decision-making mechanisms remain insufficiently understood. Existing interpretability research has primarily focused on cross-modal attribution, identifying which image regions the model attends to during output generation. However, these approaches often overlook intra-modal dependencies. In the visual modality, attributing importance to isolated image patches ignores spatial context due to limited receptive fields, resulting in fragmented and noisy explanations. In the textual modality, reliance on preceding tokens introduces spurious activations. Failing to effectively mitigate these interference compromises attribution fidelity. To address these limitations, we propose enhancing interpretability by leveraging intra-modal interaction. For the visual branch, we introduce Multi-Scale Explanation Aggregation (MSEA), which aggregates attributions over multi-scale inputs to dynamically adjust receptive fields, producing more holistic and spatially coherent visual explanations. For the textual branch, we propose Activation Ranking Correlation (ARC), which measures the relevance of contextual tokens to the current token via alignment of their top-$k$ prediction rankings. ARC leverages this relevance to suppress spurious activations from irrelevant contexts while preserving semantically coherent ones. Extensive experiments across state-of-the-art MLLMs and benchmark datasets demonstrate that our approach consistently outperforms existing interpretability methods, yielding more faithful and fine-grained explanations of model behavior.", "tldr": "", "keywords": ["XAI", "Multimodal LLM"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/9e6f1704fcc645cfa3ccf0296824a73986954e01.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MSEA (Multi-Scale Explanation Aggregation) and ARC (Activation Ranking Correlation) to produce token-level visual attributions for multimodal LLMs. MSEA averages logit-lens maps computed from multiple input resolutions to reduce spatial fragmentation; ARC down-weights interference from preceding text tokens using rank-based similarity between their next-token distributions and that of the target token. Evaluations on captioning/scene datasets report IoU-style improvements over baselines (e.g., TAM)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. Clear, post-hoc, model-agnostic recipe (no retraining); easy to plug into popular MLLMs.\n\n2. Empirical gains on localization-type metrics across several models/datasets."}, "weaknesses": {"value": "1. Overstated novelty about “intra-modal interactions.” \nFull-graph methods (gradients, LRP/AttnLRP, attention rollout, TAM/LLaVA-CAM) already propagate relevance through all cross- and intra-modal paths; interaction effects are not “ignored” by default.\n\n2. Receptive-field misconception. \nIn ViTs, global self-attention makes a token’s effective receptive field image-wide after early layers. Fragmented maps arise from attribution noise/aggregation choices, not from an inherently local RF. MSEA’s value is ensembling across tokenizations, not “expanding RF.”\n\n3. Baselines lack crucial details. \nWhich layer/heads for CAM/Grad-CAM? How was AttnLRP configured? Without this, fairness and reproducibility are uncertain. The authors should write details at supplementary material.\n\n4. No faithfulness evaluation. \nResults rely on human-grounded IoU and “cleanliness.” There are no causal tests (insertion/deletion curves, AOPC/ROAR, evidence-erasure, logit drop when masking top-k regions). MSEA/ARC may yield prettier maps yet be less causal.\n\n5. Qualitative analysis is thin. \nFew examples; captions are terse; comparisons focus mainly on TAM. Lacks side-by-sides against other listed baselines (Grad-CAM/++, Rollout, LRP/AttnLRP, IG-style).\n\n6. Cost/latency not discussed. \nmulti-scale forward passes + ARC scoring add overhead; no wall-clock or memory analysis vs TAM/AttnLRP/perturbation methods.\n\n7. There is no detailed captions at every figure."}, "questions": {"value": "See above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hPyDGtjFEu", "forum": "qrKymA0zuY", "replyto": "qrKymA0zuY", "signatures": ["ICLR.cc/2026/Conference/Submission22000/Reviewer_3Q2o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22000/Reviewer_3Q2o"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761540345743, "cdate": 1761540345743, "tmdate": 1762942012886, "mdate": 1762942012886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "CN5gGL5EDR", "forum": "qrKymA0zuY", "replyto": "qrKymA0zuY", "signatures": ["ICLR.cc/2026/Conference/Submission22000/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22000/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763352882539, "cdate": 1763352882539, "tmdate": 1763352882539, "mdate": 1763352882539, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework to enhance the explainability (attribution) of Multimodal Large Language Models (MLLMs), focusing specifically on Intra-Modal Interactions. The framework aims to overcome the issues of fragmented, incoherent, and noise-contaminated explanations prevalent in existing methods, striving to generate more Faithful and Fine-grained attributions. The core contributions include:\n\n1. Multi-Scale Explanation Aggregation (MSEA): For the visual modality, this aggregates attribution results from different input scales, dynamically adjusting the receptive fields of visual tokens to capture spatial context, leading to more holistic visual explanations.\n\n2. Activation Rank Correlation (ARC): For the text modality, this method quantifies semantic relevance by comparing the Top-k prediction rank consistency (using the RBO metric) between the current token and context tokens. This mechanism is used to suppress spurious activations and noise originating from irrelevant contexts.\n\nThe approach is post-hoc and training-free. It is validated across state-of-the-art MLLMs like LLaVA-1.5 and Qwen2-VL, showing consistent superiority over existing baselines (e.g., TAM) in metrics like Obj-IoU, Func-IoU, and F1-IoU."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Strong Solution to Technical Problems: The mechanistic design of MSEA and ARC effectively addresses known limitations in Logit Lens attribution. The introduction of Top-k Prediction Rank Correlation (RBO) in the text modality is a clever and effective mechanism for filtering semantic noise.\n\n2. Extensive Experimental Validation: Comprehensive testing across representative MLLM architectures (LLaVA-1.5, Qwen2-VL, and InternVL2.5) increases the credibility of the results.\n\n3. Practicality: As a post-hoc method, it requires no retraining or fine-tuning of the MLLM, making it easy to deploy and apply."}, "weaknesses": {"value": "1. Causal Logic Risk (MSEA): Multi-Scale Explanation Aggregation (MSEA) obtains attribution by altering the input scale, which may introduce a causal logic inconsistency. The attribution result may not strictly reflect the model's decision under the original input, and this lacks rigorous theoretical or empirical support.\n\n2. Missing Diagnostic Application/Value: Although the method achieves high fidelity, the paper lacks diagnostic case studies focusing on real model errors like MLLM Hallucination. Analyzing such error cases is crucial to demonstrate the method's diagnostic scope and application value, enabling attribution to truly fulfill its role in model analysis.\n\n3. Computational Burden: Since both MSEA (involving multiple forward passes for different scales) and ARC (involving Top-k rank consistency calculation, which can be resource-intensive with long contexts/outputs) are added on top of the base Logit Lens method, the computational overhead may be high, especially when applied to very long contexts or complex MLLMs."}, "questions": {"value": "1. Causal Consistency (MSEA): Given that MSEA aggregates attribution results by altering the input scale, we are concerned about potential causal logic inconsistencies. Could the authors provide deeper theoretical arguments or additional experimental evidence (e.g., ensuring the original decision remains unchanged) to guarantee the fidelity of the MSEA attribution results with respect to the model's decision mechanism under the original input?\n\n2. Practical Application (Hallucination Diagnosis): Since the method aims to improve attribution fidelity, we suggest the authors provide qualitative case analyses demonstrating how MSEA and ARC can effectively diagnose the root causes of MLLM Hallucinations (whether due to visual signal misinterpretation or spurious activations from text context). A comparison with baseline methods would be highly beneficial.\n\n3. RBO Sensitivity: Regarding the ARC module, we encourage the authors provide ablation studies or sensitivity analysis concerning the choice of the Top-k value (e.g., the default $k=50$), to prove the stability and reasonableness of this parameter selection.\n\n4. Computational Complexity: Considering the overhead of MSEA (multiple forward passes) and ARC's RBO calculation, could the authors provide a practical time breakdown? Specifically, what is the required wall-clock time to analyze a single case for different context lengths?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bkmgxpEKQR", "forum": "qrKymA0zuY", "replyto": "qrKymA0zuY", "signatures": ["ICLR.cc/2026/Conference/Submission22000/Reviewer_SxV5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22000/Reviewer_SxV5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761572558381, "cdate": 1761572558381, "tmdate": 1762942012661, "mdate": 1762942012661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of current interpretability studies on Multimodal Large Language Models (MLLMs), which mainly focus on cross-modal attribution while neglecting intra-modal dependencies. The authors point out that, within the visual modality, existing methods often divide images into independent patches and thereby overlook the importance of spatial contextual relationships. To address this issue, the paper proposes a Multi-Scale Explanation Aggregation (MSEA) approach, which aggregates attributions across multiple image resolutions to dynamically adjust the receptive field, resulting in more coherent visual explanations.\nWithin the textual modality, to mitigate spurious activations caused by preceding tokens, the paper introduces an Activation Ranking Correlation (ARC) method. By evaluating the ranking consistency among the Top-k predicted tokens, ARC quantifies the relevance between each contextual token and the current token, and further suppresses spurious activations from irrelevant contexts based on this correlation.\nExperiments conducted on multiple MLLMs and datasets demonstrate that the proposed method achieves superior interpretability quality compared with existing mainstream approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper innovatively focuses on the intra-modal dependencies in the interpretability of Multimodal Large Language Models (MLLMs). In the visual modality, it captures more contextual visual information by adjusting the visual receptive field. In the textual modality, it suppresses irrelevant spurious activations by quantifying the ranking consistency among Top-k predicted tokens. Based on the experimental results and visualizations provided by the authors, the proposed method demonstrates superior performance compared to existing interpretability approaches."}, "weaknesses": {"value": "The presentation and explanation of the proposed method are overly abstract, with some steps omitted and theoretical aspects insufficiently detailed, which may make it difficult for readers to fully understand. Furthermore, some of the visualized results and analyses are limited.\n\n(1) The explanation of “logits” and the “Logit Lens interpretability mechanism” in Section 3.1 is somewhat abstract, which may make it difficult for readers to clearly understand the related background.\n(2) The motivation and interpretation of the proposed input–output level operations in Section 3.2 are also rather abstract, which may affect readers’ comprehension.\n(3) It is recommended to clarify in Section 3.2 which variable remains constant when the image scale is changed, and how this affects the variation of the receptive field.\n(4) As shown in Figure 2, it appears that one patch is divided into four sub-patches after tokenization. It is suggested to provide a clearer explanation of this process or include a formula to describe it.\n(5) For Equation (5), it is recommended to provide a more detailed explanation of how the results are rescaled to the original image size after the aggregation in Equation (4).\n(6) It is recommended to provide a more detailed explanation of the use of Rank-Biased Overlap (RBO) in Equation (7) and the settings of its parameters.\n(7) In Section 3.3, the definition of “base attribution” is not clearly described, particularly regarding its computation for the token with index k. Moreover, the way in which the defined “base attribution” is utilized is not clearly explained.\n(8) Figure 2 does not clearly illustrate the proposed method, especially the depiction of ARC, which appears misaligned with the textual description. It is recommended to refine both the figure and the corresponding explanation.\n(9) It is recommended to provide a detailed explanation of each evaluation metric, including its computation method and meaning, to help readers better understand these metrics.\n(10) It is suggested to provide additional visualizations of non-semantic tokens."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Aa1atoRzIi", "forum": "qrKymA0zuY", "replyto": "qrKymA0zuY", "signatures": ["ICLR.cc/2026/Conference/Submission22000/Reviewer_hFsC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22000/Reviewer_hFsC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761656427817, "cdate": 1761656427817, "tmdate": 1762942012411, "mdate": 1762942012411, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework to enhance the interpretability of Multimodal Large Language Models (MLLMs) by intra-modal interactions in vision-language modalities. The two core contributions are: 1. Multi-Scale Explanation Aggregation (MSEA): Aggregates visual token attributions across multiple image resolutions; 2. Activation Ranking Correlation (ARC): Identifies and suppresses spurious activations from irrelevant preceding tokens in the textual modality, based on alignment of top-k token ranking predictions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MSEA and ARC complementary work together to improve both visual and textual interpretability. SEA captures spatial context across visual tokens, while ARC reduces noise in textual attributions caused by irrelevant context tokens.\n\n2. Quantitative improvements and qualitative visualization results support the effectiveness of the approach.\n\n3. Good writing and presentation."}, "weaknesses": {"value": "1. High Overlap with TAM and Incremental Innovations:\n\nThe paper’s methodology is built upon TAM[1]. MSEA is a straightforward extension of multi-scale input techniques commonly used in computer vision tasks, offering limited innovation. SEA is also a technique reuse on LLM inference.\n\n2. Computational Complexity:\n\nMSEA introduces significant computational overhead due to multi-resolution processing and attribution aggregation, a clear trade-off on significant increased complexity and performance.\n\nARC share a similar case, that requires token-ranking alignment computations for every preceding token, further compounding the computational cost.\n\nThe paper does not provide a detailed analysis of runtime performance or scalability trade-offs, which raises concerns about practical applicability."}, "questions": {"value": "1. What is the runtime cost of MSEA and ARC compared to TAM or other baseline methods?\n\n2. How does the method scale with larger models?\n\n3. Robustness: How does the method handle challenging scenarios, such as: Images with multiple similar objects (e.g., distinguishing specific birds or cars)? Occluded objects where parts of the image are missing? Interaction-heavy scenes involving relational reasoning (e.g., a person interacting with tools)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ys70HRJ3G3", "forum": "qrKymA0zuY", "replyto": "qrKymA0zuY", "signatures": ["ICLR.cc/2026/Conference/Submission22000/Reviewer_UEeF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22000/Reviewer_UEeF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22000/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979913312, "cdate": 1761979913312, "tmdate": 1762942012214, "mdate": 1762942012214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}