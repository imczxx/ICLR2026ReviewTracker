{"id": "EXtPGwYdWe", "number": 19414, "cdate": 1758296038073, "mdate": 1759897040121, "content": {"title": "Attention to Mamba: A Recipe for Cross-Architecture Distillation", "abstract": "State Space Models (SSMs) such as Mamba have become a popular alternative to Transformer models, due to their reduced memory consumption and higher throughput at generation compared to their Attention-based counterparts. On the other hand, the community has built up a considerable body of knowledge on how to train Transformers, and many pretrained Transformer models are readily available. \nTo facilitate the adoption of SSMs while leveraging existing pretrained Transformers, we aim to identify an effective recipe to distill an Attention-based model into a Mamba-like architecture. In prior work on cross-architecture distillation, however, it has been shown that a naive distillation procedure from Transformers to Mamba fails to preserve the original teacher performance, a limitation often overcome with hybrid solutions combining Attention and SSM blocks.\nThe key argument from our work is that, by equipping Mamba with a principled initialization, we can recover an overall better recipe for cross-architectural distillation. To this end, we propose a principled two-stage approach: first, we distill knowledge from a traditional Transformer into a linearized version of Attention, using an adaptation of the _kernel trick_. Then, we distill the linearized version into an adapted Mamba model that does not use any Attention block.\nOverall, the distilled Mamba model is able to preserve the original Pythia-1B Transformer performance in downstream tasks, maintaining a perplexity of 14.11 close to the teacher's 13.86. To show the efficacy of our recipe, we conduct thorough ablations at 1B scale with 10B tokens varying sequence mixer architecture, scaling analysis on model sizes and total distillation tokens, and a sensitivity analysis on tokens allocation between stages.", "tldr": "We propose a novel method for cross-architecture distillation from a Transformer model to a State-Space Model", "keywords": ["Distillation", "Mamba", "Transformers", "Linear Attention", "SSM"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ebc83b2b76691471b32e825bc66daf09e6371d95.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to present a better strategy for distilling Transformers to Mamba. The major concept of this paper is introducing an intermediate step that transfers knowledge from softmax based Transformers to the linear attention variant, which is basically proposed by Hedgehog. Then, reusing the QKV weights to their corresponding CBX components in Mamba, and distilling it again, the authors show that it leads to better performance than previous works."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is very straightforward -- discovering the importance of the intermediate Hedgehog transfer. As the idea is very simple, it's easy to understand, probably also for people who are not familiar with distillation or SSM literature."}, "weaknesses": {"value": "As the idea is very simple, the novelty is limited. Most of the contents in the paper are about preliminary papers; for example, Section 1.1, Section 2 are only about previous works, and about 80% of Section 3 are introducing the ideas from Hedgehog or Mamba2."}, "questions": {"value": "One critical ablation is missing: how important are \"Parameter initialization\" and \"Attention scores normalization\" in Section 3.2? Also, I am not convinced with the necessity of attention scores normalization. Mamba architecture does not have a separate normalization of such scores, but why did the authors choose to use the trick?\n\nThe paper only compares to Hedgehog. How is it compared to MOHAWK?\n\nIn Table 1, Hedgehog's PPL is 14.89. But in Table 3, the 100/0 row shows very high PPL. Where does this significant difference come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NNjW0E3xNa", "forum": "EXtPGwYdWe", "replyto": "EXtPGwYdWe", "signatures": ["ICLR.cc/2026/Conference/Submission19414/Reviewer_jYaM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19414/Reviewer_jYaM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761447404882, "cdate": 1761447404882, "tmdate": 1762931329405, "mdate": 1762931329405, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the challenge of distilling a pretrained Transformer (Pythia) into Mamba to achieve faster inference and lower memory usage while retaining performance. Specifically, it proposes a two-stage distillation recipe: 1. Distilling the Transformer’s softmax Attention into a Linear Attention based on the Hedgehog approach. 2. Using this linearized Attention to initialize and fine-tune a Mamba-based architecture (HedgeMamba). The method is evaluated on the OpenWebText dataset, and the proposed HedgeMamba achieves superior performance compared to the simple direct distillation into Mamba and Hedgehog distillation into Linear Attention."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "- The two-stage distillation process is novel and reasonable. It works much better than the simple direct distillation from Transformer to (Hedge)Mamba. Also, thanks to the superior expressiveness of Mamba over that of Linear Attention, the distilled HedgeMamba works better than Linear Attention models.  \n- The ablation studies effectively show the importance of the two-stage recipe.  \n- The paper is well written and provides enough details about their implementations, including the limitations."}, "weaknesses": {"value": "#### Major Weaknesses  \n- The important information about the efficiency of HedgeMamba architecture is missing. Because the motivation of the distillation in this paper is to create an efficient model, the author should compare the inference speed or FLOPs. I doubt the efficiency of the HedgeMamba because the hidden stage dimension is huge (2048) compared to Mamba (16 to 64), in addition to the newly introduced layers.  \n- Why is the large SSM hidden state used? Although it is related to the previous weakness point, the inference speed should be much slower compared to the original Mamba (I guess 2~5 times slower) due to the large hidden state size. Did you use Mamba2 instead of Mamba? (Mamba2 should be relatively faster even if the hidden state size is large.)  \n- A gap exists between the motivation and the evaluation. Although the motivation is to borrow the strong performance of large Transformers, the experiments are conducted with the 1B size model. The Pythia-1B is inferior to Mamba-790M, and, to this end, HedgeMamba is inferior to Mamba-790M. In addition, the distillation cost (12 days with 8xA100 GPUs) seems not so small compared with the scratch training cost of Mamba.  \n- The lack of comparison against other methods proposing the Transformer to Mamba distillation [1, 2]. Although the authors mentioned that the experimental setups are different, comparisons can be possible by trying previous methods with this paper’s setup. Different architectures can be compared with accuracy-efficiency trade-off.  \n\n#### Minor Weakness\n- Typo  \n    - L037) tokens representations -> token representations  \n\n[1] Wang, J., Paliotta, D., May, A., Rush, A., & Dao, T. (2024). The mamba in the llama: Distilling and accelerating hybrid models. Advances in Neural Information Processing Systems, 37, 62432-62457.  \n[2] Bick, Aviv, et al. \"Transformers to ssms: Distilling quadratic knowledge to subquadratic models.\" Advances in Neural Information Processing Systems 37 (2024): 31788-31812."}, "questions": {"value": "Please see major weakness. As to the third point, if it is difficult to evaluate with large models, it can be interesting if you can show some evaluations that the distillation improves the weak points of the vanilla Mamba described in some papers such as [3, 4]. In addition, if HedgeMamba is efficient, I want to raise the rating.\n\n[3] Park, J., Park, J., Xiong, Z., Lee, N., Cho, J., Oymak, S., ... & Papailiopoulos, D. (2024). Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248. \n[4] You, W., Tang, Z., Li, J., Yao, L., & Zhang, M. (2024). Revealing and Mitigating the Local Pattern Shortcuts of Mamba. arXiv preprint arXiv:2410.15678."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZPX6co2XU2", "forum": "EXtPGwYdWe", "replyto": "EXtPGwYdWe", "signatures": ["ICLR.cc/2026/Conference/Submission19414/Reviewer_g5YJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19414/Reviewer_g5YJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657014871, "cdate": 1761657014871, "tmdate": 1762931329004, "mdate": 1762931329004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tries to fine-tune full attention into linear attention, first by using the Hedgehog distillation, and then fine-tune into a Mamba variant with parameters reused in the wake of state space duality. Empirical experiments and ablation studies verify the effectiveness and necessity of the two-stage recipe."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of conversion to Mamba by reusing weights based on state-space duality is interesting.  \nEmpirical results are good."}, "weaknesses": {"value": "Stage 1 is identical to Hedgehog, which reduces the novelty of the approach."}, "questions": {"value": "From my understanding, Stage 1 is identical to the Hedgehog work. Please clarify if there is any difference with their work. The identical part should be ideally presented in a more concise way, and details (e.g., the reference of kernelization/Mercer's theorem) should be put into the appendix, leaving more space for the novel part.\n\nThe difference between HedgeMamba and Mamba is important. It would be easier to follow if the author could prompt the reader earlier (e.g. in Fig 2) to check Fig 4 for it.\n\nHave the authors considered the applicability to other linear attention approaches, e.g., DeltaNet?\n\nPlease also report the average accuracy in Table 1-3 for easier comparison.\n\nThe method becomes suspicious when the largest improvement comes from gated attention; Is the Mamba part really necessary? How about the case if you add gating only?\n\nL69,350, ...: Should use \\citep  \nL138,353,354, ...: Should use \\citet \nL249-253, ...: Should use \"by \\citet{...}\" instead of \"in \\citet{...}\"  \nSome numbers are out-of-margin in Table 1"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fNdQ7XhdG2", "forum": "EXtPGwYdWe", "replyto": "EXtPGwYdWe", "signatures": ["ICLR.cc/2026/Conference/Submission19414/Reviewer_ET8B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19414/Reviewer_ET8B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19414/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840057823, "cdate": 1761840057823, "tmdate": 1762931328496, "mdate": 1762931328496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}