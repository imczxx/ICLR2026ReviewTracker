{"id": "Y9TgNFsNyP", "number": 4821, "cdate": 1757771962909, "mdate": 1759898011202, "content": {"title": "FF-Erase : Machine Unlearning and Verification for Forward-Forward Models", "abstract": "The Forward-Forward (FF) algorithms present promising and biologically plausible alternatives to backpropagation (BP), enabling efficient model training through layer-wise greedy optimization. However, the critical task of machine unlearning for FF models, which involves efficiently removing specific training data's influence without full retraining, remains a foundational yet unexplored problem. The inherent characteristics of FF models, such as their sensitivity to parameter tuning and layer-wise independent training, pose unique challenges, often causing catastrophic model collapse when applying conventional unlearning methods. To fill this gap, we introduce a novel unlearning framework specifically for FF models, which employs a goodness-guided strategy. This method proposes a stable guidance model to generate target goodness distributions, steering the original model to unlearn forgetting data by shifting its layer-wise goodness scores, thereby effectively adapting gradient-based unlearning for the FF architecture. To enable robust verification on unlearning performance, we also propose a novel goodness-based membership inference attack (G-MIA), a powerful and lightweight black-box attack that leverages the unique properties of FF models' goodness scores. Our experiments demonstrate that our proposed method effectively removes the influence of target forgetting data on FF models while preserving model utility on the remaining data. Critically, our approach accomplishes 1.9 to 3.1$\\times$ faster than retraining from scratch, establishing an efficient foundation for FF unlearning.", "tldr": "We propose the first machine unlearning algorithm for forward-forward models, and we also propose a MIA specific to FF models for accurate and practical unlearning verification.", "keywords": ["forward-forward", "machine unlearning", "machine unlearning verification", "backpropagation-free"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/57cc8a684fcf485ff18ad4256feaa9ad89af1c35.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents the first unlearning method for forward-forward models as well as a membership inference attack for the FF architecture."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Rethinking machine unlearning for models that do not use backpropagation is highly interesting, as no prior work exists and discoveries might lead to advances in both areas (BP and FF).\n\nThe new method is intuitive (similar to the popular SCRUB method in BP unlearning) and shows good performance in the experiments.\n\nThe G-MIA also makes sense, adapting traditional MIA to the layer level of FF models.\n\nBoth the new method and G-MIA are adapting existing working approaches to the FF setting, providing a valuable contribution for future research.\n\nTo preempt this point from other reviewers: Both FF-Erase and G-MIA are adaptations to proven approaches in BP unlearning. While some might call this a lack of novelty, I see the adaptation to FF as non-trivial and valuable."}, "weaknesses": {"value": "The literature is missing BP unlearning methods that also utilize student-teacher approaches (SCRUB, Bad Teacher, …). Adding this literature and highlighting why it fails (ideally with experiments) would improve the paper."}, "questions": {"value": "Why not use the original model as the guidance model and perform unlearning like SCRUB? I may have missed it, but the rationale behind training a new guidance model instead of using the existing starting model eludes me (it has been shown to work in BP unlearning)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VT7eAlg3U8", "forum": "Y9TgNFsNyP", "replyto": "Y9TgNFsNyP", "signatures": ["ICLR.cc/2026/Conference/Submission4821/Reviewer_y2EG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4821/Reviewer_y2EG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761664283993, "cdate": 1761664283993, "tmdate": 1762917594612, "mdate": 1762917594612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an unlearning algorithm that is tailored for Forward-Forward network where traditional unlearning methods may cause model collapse. The proposed method follows the spirit of the common unlearning strategy that preserves retain datasets performance (or goodness here) and reduce the forget set's performance. The loss function for reducing forget set's goodness in this paper is by reducing its difference to a guidance model with KL divergence. The results show the proposed method works in terms of against MIA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem addressed in this paper is quite unique. The Forward-Forward (FF) network is not yet widely adopted in practice, and it is unclear why one would need to consider data unlearning for such models at this stage. Nevertheless, the paper takes an interesting forward-looking perspective by identifying and tackling a potential issue that others have not yet considered."}, "weaknesses": {"value": "1. The paper is not well polished and contains several errors. For example, $\\mathbf{g}^l$ is defined as a vector with a dimensionality equal to the number of classes, yet the paper also states that $\\mathbf{g}^l = ||\\mathbf{h}^l||_1$. The $L_1$ norm of a vector is a scalar, not another vector, which raises confusion about how this formulation is meant to work. What is the correct equation here?\n\n2. The method rely on a guidance model to learn how to behave on forget set. The funny thing is that if one can have a guidance model that is ignorant of the forgetting data, why do we need to do the unlearning? In addition, the training of guidance model through equation 7-8 shows it is not a trivial task. It makes me wander the whole purpose of making up this problem. One should retrain the FF model from scratch, which might be cheaper?\n\n3. The paper claims that conventional unlearning methods can cause model collapse in Forward-Forward networks, but no empirical evidence or citation is provided to support this claim. It remains unclear whether this is based on actual observation or merely intuition. Providing experimental or theoretical justification would strengthen the paper’s motivation."}, "questions": {"value": "My questions are listed in the weakness section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lEmJbdQ1TB", "forum": "Y9TgNFsNyP", "replyto": "Y9TgNFsNyP", "signatures": ["ICLR.cc/2026/Conference/Submission4821/Reviewer_4v3L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4821/Reviewer_4v3L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761670464562, "cdate": 1761670464562, "tmdate": 1762917594354, "mdate": 1762917594354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Forward-forward (FF) models provide an alternative to backpropagation algorithms. Machine unlearning for FF models, which aims to remove specific data knowledge from a model, has not been explored. Using classic unlearning algorithms can cause model collapse on these models. To fill this gap, the paper introduces FF-Erase, a novel machine learning algorithm tailored to FF models. The method uses a guidance model (obtained via mini-retrain or fast distillation) to provide goodness scores and steer the original model. The authors also propose G-MIA, a goodness-based membership inference attack for evaluation. Experiments on CIFAR/MNIST variants show that FF-Erase achieves similar unlearning effectiveness to retraining while being faster."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n* The motivations and presentation are clear. The authors provides a novel approach for machine unlearning in FF models, which has not been explored in the literature.\n\n* The authors also introduce a novel goodness-based MIA metric, and show its effectiveness."}, "weaknesses": {"value": "* The proposed method is mainly compared with GA and retraining as baselines. Can the performance of other unlearning methods be assessed?\n\n* The paper states that applying existing unlearning updates causes model collapse in FF models, but the explanation is mostly high level. However, more thorough analysis as to why this is the case in FF models would be helpful. \n\n* The method requires obtaining a guidance model, which causes an initial overhead. It also requires maintaining both the original and guide model, and forward passes through both models. \n\n* The guidance model is distilled from the original model, and is then used to guide the original model through a KL divergence loss, which could be inefficient. \n\nMinor issues:\n\n* Some colors in the figure 5(a) and (b) can be hard to distinguish (lambda=1, 0.1)\n* What the dashed lines represent in figure 4(a) and (b) is not clear."}, "questions": {"value": "* What is the performance of  the guidance model on the forget and remain data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YOaq2xyPGZ", "forum": "Y9TgNFsNyP", "replyto": "Y9TgNFsNyP", "signatures": ["ICLR.cc/2026/Conference/Submission4821/Reviewer_r4jW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4821/Reviewer_r4jW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984114491, "cdate": 1761984114491, "tmdate": 1762917594039, "mdate": 1762917594039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Here, the authors explore the critical challenge of machine unlearning with the Forward-Forward networks. The authors propose FF-ERASE to efficiently remove the influence of specific training data from FF models. The method tackles the unique difficulties posed by FF architectures, such as their sensitivity to parameter changes and layer-wise greedy optimization, which can easily trigger catastrophic model collapse during the unlearning process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- It is the first formalized solution for machine unlearning in the increasingly relevant and research-intensive FF network paradigm, filling a critical gap.\n- The method is tailored to the layer-wise optimization of FF models, suggesting better unlearning method than adapting a BP-based unlearning technique.\n\n- Novel method which directly tackles the significant practical challenge of catastrophic model collapse that FF models face when their parameters are forcefully adjusted, making the unlearning process feasible."}, "weaknesses": {"value": "- The core principle of FF is layer-wise, greedy optimization, which means there is no global loss signal tying all layers together. When unlearning, removing knowledge from one layer (e.g., a shallow layer's feature representation) doesn't guarantee that the influence is fully pruned in deeper layers. This lack of global coherence means residual knowledge traces of the forgotten data might persist in high-level feature spaces, making the unlearning reversible, even if the final classification decision is erased. Use Centered Kernel Alignment (CKA) or a similar representation similarity metric to compare the feature spaces generated by $D_{\\text{forget}}$ samples in three different layers (shallow, middle, deep) before and after FF-ERASE.\n\n- Computation efficiency and complexity of unlearning in FF models are not compared to the BP based unlearning methods.\nDirectly compare the total computational time required for FF-ERASE to unlearn a specified set $D_{\\text{forget}}$ against the most efficient BP-based unlearning methods (e.g Gradient Ascent ) on a CNN model."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "xeGInuYJLf", "forum": "Y9TgNFsNyP", "replyto": "Y9TgNFsNyP", "signatures": ["ICLR.cc/2026/Conference/Submission4821/Reviewer_CRuv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4821/Reviewer_CRuv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4821/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004062127, "cdate": 1762004062127, "tmdate": 1762917593738, "mdate": 1762917593738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}