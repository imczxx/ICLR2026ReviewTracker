{"id": "GbNQyQEJOo", "number": 16602, "cdate": 1758266630797, "mdate": 1759897230230, "content": {"title": "Combating Data Laundering in LLM Training", "abstract": "Data rights owners can detect unauthorized data use in large language model (LLM) training by querying with proprietary samples. \nOften, superior performance (e.g., higher confidence or lower loss) on a sample relative to the untrained data implies it was part of the training corpus, as LLMs tend to perform better on data they have seen during training.\nHowever, this detection becomes fragile under data laundering, a practice of transforming the stylistic form of proprietary data, while preserving critical information to obfuscate data provenance.\nWhen an LLM is trained exclusively on such laundered variants, it no longer performs better on originals, erasing the signals that standard detections rely on.\nWe counter this by inferring the unknown laundering transformation from black-box access to the target LLM and, via an auxiliary LLM, synthesizing queries that mimic the laundered data, even if rights owners have only the originals.\nAs the search space of finding true laundering transformations is infinite, we abstract such a process into a high-level transformation goal (e.g., \"lyrical rewriting\") and concrete details (e.g., \"with vivid imagery\"), and introduce synthesis data reversion (SDR) that instantiates this abstraction. \nSDR first identifies the most probable goal that synthesis should step into to narrow the search; it then iteratively refines details, such that synthesized queries gradually elicit stronger detection signals from target LLM.\nEvaluated on the MIMIR benchmark against diverse laundering practices and target LLM families (Pythia, Llama2, and Falcon), SDR consistently strengthens data misuse detection, providing a practical countermeasure to data laundering.", "tldr": "When unauthorized LLM training data gets laundered through synthetic transformation to wash away privacy breach, data right owners can fight back by reverse-engineering the laundering practice.", "keywords": ["Unauthorized data usage detection", "data laundering"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c703d752a94cde1244efff83a7debf7ec9f0dfc3.pdf", "supplementary_material": "/attachment/5394254098158624672fb800e48e928d8e8756ee.zip"}, "replies": [{"content": {"summary": {"value": "The paper addresses the challenge of detecting unauthorized data use in large language model (LLM) training when proprietary data has been \"laundered\" through semantic-preserving transformations (e.g., stylistic rewriting) to evade detection. The authors propose Synthesis Data Reversion (SDR), a two-stage method that infers the unknown laundering transformation from black-box access to the target LLM using an auxiliary LLM. SDR abstracts transformations into a high-level goal (e.g., \"lyrical rewriting\") from a taxonomy of 23 linguistic registers and refines it with iterative details to synthesize \"training-like\" queries. Evaluations on the MIMIR benchmark across LLM families (Pythia, Llama-2, Falcon) show SDR consistently improves detection metrics under various laundering scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is timely and important to investigate the data laundering in LLM training\n2. Strong detection performance compared with baseline methods."}, "weaknesses": {"value": "1. The proposed method Involves multiple iterative queries to both target and auxiliary LLMs, potentially costly for large datasets or real-world audits, with no detailed cost analysis provided.\n2. It would be better if the authors could report more results via TPR@1%, considering such an audit task requires much confidence.\n3. There is another concern about the assumption, the authors assumes laundering follows a promptable goal-details structure executable by an auxiliary LLM, however, real-world laundering might be more opaque or non-prompt-based (e.g., human-edited)."}, "questions": {"value": "1. What is the specific computational cost of the proposed method?\n2. Can the proposed method effectively address real-world laundering (e.g., human-edited)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "K56MFxUd4l", "forum": "GbNQyQEJOo", "replyto": "GbNQyQEJOo", "signatures": ["ICLR.cc/2026/Conference/Submission16602/Reviewer_GhSm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16602/Reviewer_GhSm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760932474671, "cdate": 1760932474671, "tmdate": 1762926675567, "mdate": 1762926675567, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles data laundering in LLM training, where copyrighted texts are stylistically transformed to evade provenance checks. It introduces Synthesis Data Reversion (SDR), a black box method that first infers a high level laundering goal (e.g., lyrical rewrite) and then refines concrete stylistic details, using an auxiliary LLM to generate probes that match the laundered style. These probes restore detection gaps so the target model again reveals training exposure. On the MIMIR benchmark across diverse laundering practices and model families, SDR consistently boosts misuse detection."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clearly shows that standard post hoc provenance tests collapse when models are trained on laundered surrogates rather than originals, making detection with losses or calibrated confidence ineffective. The setup and failure case are well motivated and demonstrated.\n2. Reframe detection as reversing the unknown laundering transform, then reuse off-the-shelf detectors. The two-stage SDR pipeline uses a goal, then details abstraction over registers to search a compact prompt space with only black box access. Algorithms 1 and 2 are clear and practical.\n3. In the experiment section, SDR consistently boosts several detectors across prompts, datasets, and model families and works with different auxiliary LLMs."}, "weaknesses": {"value": "1. Most results rely on the laundering produced by GPT-style rewriting under predefined prompts. And the GPT may introduce a new bias. How about adding a third-party laundering pipeline, which could strengthen the whole paper.\n2. SDR needs repeated calls to an auxiliary model to build templates and iterate prompts. The experiment section only includes a limited introduction of query budgets, latency, and sensitivity to n, m, l, and K. A more comprehensive sensitivity study would help strengthen the paper.\n3. Results compare SDR plus standard detectors to the detectors alone. Since the contribution is a laundering-aware search, comparisons to other data-centric countermeasures or prompt search strategies would help to improve.\n4. The goal identification stage assumes the laundering transformation belongs to one of the 23 predefined registers. Though this is noted in Appendix I, many realistic transformations, like pseudo-translation or hybrid creative styles, etc, may limit SDR’s recall and generalization. Moreover, the method is not tested on unseen or mixed registers, leaving its robustness to out-of-distribution transformations uncertain."}, "questions": {"value": "Please see the weakness section. I will raise the score if the author addresses the questions clearly"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fN5xBp5JEz", "forum": "GbNQyQEJOo", "replyto": "GbNQyQEJOo", "signatures": ["ICLR.cc/2026/Conference/Submission16602/Reviewer_zZxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16602/Reviewer_zZxJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967877141, "cdate": 1761967877141, "tmdate": 1762926675046, "mdate": 1762926675046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the task of data contamination (or membership inference) detection, particularly in scenarios where the training data have been laundered (e.g., undergone some register transfer) prior to model training. Previous work on data contamination or membership inference has primarily focused on detecting unauthorized data use on the exact same data, without considering potential laundering transformations.\n\nAs I understand it, the paper proposes to “reverse the laundering process”, i.e., first by identifying the most likely laundering register (i.e., the stylistic or structural form into which the original data may have been transformed), and then by recovering finer-grained data details through iterative prompting of an auxiliary LLM. Experimental results show that existing data contamination detection methods experience a substantial performance drop on laundered/synthetic samples, while the proposed reverse process helps recover several detection metrics."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper investigates an important and interesting topic that has not been extensively explored in the existing literature.\n\n- The proposed method demonstrates several intriguing empirical results."}, "weaknesses": {"value": "- The threat model (and the corresponding protocol) is somewhat unclear. Several entities (such as the target model $M_t$, the auxiliary model $M_a$, and the datasets $D_{pro}$ and $D_{held}$) seem to have implicit assumptions, but these are not clearly stated. For example, is $D_{held}$ guaranteed to contain only non-member samples (i.e., data that were never used in training $M_t$)? Is the proprietary data $D_{pro}$ fully or partially assumed to have been used during training? These are important clarifications, especially for a work claiming to detect “unauthorized data usage”, which typically requires strict, verifiable integrity assumptions to be meaningful in practice. In addition, the data setup for evaluation remains somewhat vague (See detailed questions below). \n\n- The method description is somewhat confusing. Stronger intuition and clearer formulation of the design ideas would help. For example, in Algorithm 2, the pseudo-code suggests that a single “system prompt” is always returned, whereas the text description implies that it may instead correspond to sample-level prompts. It might be clearer to use explicit sample indices or a more formalized notation to distinguish between them."}, "questions": {"value": "- How are member versus non-member samples defined when computing AUC, ACC, or TPR in Tables 2–5? Are they randomly split from the same MIMIR dataset? If so, how do you ensure that the non-member data were indeed not used during the pretraining of the target model (given that many large models are trained on massive web-scale corpora that may overlap with MIMIR?) Some explicit discussion of data provenance control and efforts made to ensure disjoint membership would be very helpful. Also, what exactly does the detector observe, the laundered data samples, original samples, or both?\n\n- Related to the above concern about the unclear threat model, it remains unclear what exactly is meant by “unauthorized training data detection on $M_t$” in the construction of $Perf_r$ (see Algorithm 1, line 20, and Algorithm 2, line 11). Why is it considered a reasonable setting to compute this directly on the target model that is itself under test? Would this not constitute a form of data leakage or evaluation exposure?  In this sense, a clear and explicit description of the different data subsets (e.g., the member data, non-member data, held-out data, and potentially shadow data) and their respective roles in training and evaluation is essential, but currently missing. This clarification is particularly critical for a submission claiming to address the detection of unauthorized training data.\n\n- Algorithm 1, Line 22: variable naming inconsistency — $perf_r$ should be in uppercase ($Perf_r$) for consistency.\n\n- Algorithm 1 explicitly specifies the auxiliary model as GPT-5, but Algorithm 2 does not mention which model is used."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1FyQPVOXP6", "forum": "GbNQyQEJOo", "replyto": "GbNQyQEJOo", "signatures": ["ICLR.cc/2026/Conference/Submission16602/Reviewer_W3wo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16602/Reviewer_W3wo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16602/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762032428848, "cdate": 1762032428848, "tmdate": 1762926674659, "mdate": 1762926674659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}