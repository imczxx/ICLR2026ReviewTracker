{"id": "81RoHr6g8f", "number": 11392, "cdate": 1758198039781, "mdate": 1759897578285, "content": {"title": "ExpertSteer: Intervening in LLMs through Expert Knowledge", "abstract": "Large Language Models (LLMs) exhibit remarkable capabilities across various tasks, yet guiding them to follow desired behaviours during inference remains a significant challenge. Activation steering offers a promising method to control the generation process of LLMs by modifying their internal activations. However, existing methods commonly intervene in the model's behaviour using steering vectors generated by the model itself, which constrains their effectiveness to that specific model and excludes the possibility of leveraging powerful external expert models for steering. To address these limitations, we propose ExpertSteer, a novel approach that leverages arbitrary specialized expert models to generate steering vectors, enabling intervention in any LLMs. ExpertSteer transfers the knowledge from an expert model to a target LLM through a cohesive four-step process: first aligning representation dimensions with auto-encoders to enable cross-model transfer, then identifying intervention layer pairs based on mutual information analysis, next generating steering vectors from the expert model using Recursive Feature Machines, and finally applying these vectors on the identified layers during inference to selectively guide the target LLM without updating model parameters. We conduct comprehensive experiments using three LLMs on 15 popular benchmarks across four distinct domains. Experiments demonstrate that ExpertSteer significantly outperforms established baselines across diverse tasks at minimal cost.", "tldr": "", "keywords": ["Large Language Models", "Activation Steering", "Inference Time Intervention"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4cdab05b85d6609d51d520b0c67cc350a12902bd.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces EXPERTSTEER, an activation steering framework for guiding Large Language Models (LLMs) at inference time. Unlike prior activation-steering methods that derive steering vectors from the same model, EXPERTSTEER enables cross-model steering by leveraging external expert models to generate more general and effective control signals. Comprehensive experiments demonstrate that EXPERTSTEER achieves significant performance gains over existing baselines, improving controllability and transferability with minimal computational cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper introduces an innovative framework that enables activation steering across different models, breaking the limitation of self-steering seen in prior work.\n\n2. The experiments cover multiple LLM architectures and a wide range of benchmarks (15 datasets across 4 domains), demonstrating robustness and generality."}, "weaknesses": {"value": "1. Although the paper successfully demonstrates cross-model activation steering, each expert–target model pair still requires specific fine-tuning and configuration, which limits the generality and scalability of the proposed framework.\n\n2. During inference, both the expert model and the target model must be executed simultaneously, resulting in significant computational overhead. Moreover, the final steered outputs do not surpass the expert model’s standalone performance, making the overall motivation and practical value somewhat unclear.\n\n3. The readability of the paper could be improved. Several tables occupy half a page, causing many hyphenated words to break across lines and reducing overall clarity and presentation quality.\n\n4. The base models used in the experiments appear somewhat outdated compared to current state-of-the-art LLMs, which may limit the practical relevance and generalizability of the reported results."}, "questions": {"value": "1. Does the proposed EXPERTSTEER framework still require the expert model to be executed during inference? If so, how does this approach compare to directly using the expert model for generation in terms of efficiency and effectiveness? What practical advantage does EXPERTSTEER offer in this case?\n\n2. Definition of “same domain” in Table 3: The paper mentions that “both the expert and target models belong to the same domain.” Could the authors clarify this definition? For instance, the expert model Qwen2.5-14B-Instruct does not appear to be tied to any specific domain—how is the domain categorization determined in this case?\n\n3. In Figure 4, the accuracy still appears to be increasing after 2,000 training examples. Could the authors provide results with more training examples to demonstrate that performance saturates and to better justify the claim that “2,000 training examples are sufficient for generating effective steering vectors”?\n\n4. Could the authors present more detailed experimental results showing how the hyperparameters $P$ and $\\epsilon$ affect performance? \n\n5. Could the authors include results on more recent or stronger LLMs to validate the general applicability and relevance of EXPERTSTEER?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yGqcpbVhgG", "forum": "81RoHr6g8f", "replyto": "81RoHr6g8f", "signatures": ["ICLR.cc/2026/Conference/Submission11392/Reviewer_oDcb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11392/Reviewer_oDcb"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761705042345, "cdate": 1761705042345, "tmdate": 1762922513196, "mdate": 1762922513196, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes **EXPERTSTEER**, an activation steering framework that transfers domain-specific knowledge from an *expert model* to any *target LLM* during inference without fine-tuning. The method (1) aligns hidden representations via auto-encoders, (2) selects intervention layer pairs using *mutual information*, (3) extracts non-linear steering vectors via *Recursive Feature Machines (RFMs)*, and (4) injects these vectors into the target model’s activations. Experiments on multiple domains (medical, financial, mathematical, and general) and models (Llama, Qwen, Gemma) show consistent improvements over fine-tuning and prior steering baselines. The paper claims EXPERTSTEER enables efficient, parameter-free cross-model knowledge transfer."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Originality:**  \nThe paper introduces an original perspective on *activation steering* by leveraging external expert models instead of relying solely on the target model’s internal activations. This “cross-model steering” concept is novel within the activation control literature and provides a new direction for knowledge transfer between LLMs.\n\n**Quality:**  \nThe experimental evaluation is extensive, covering multiple model families (Llama, Qwen, Gemma) and domains (medical, financial, mathematical, general). The comparisons against both fine-tuning (SFT, KD) and steering-based baselines (ITI, CAA, SADI, SAS) are thorough. The empirical results are consistent and generally favorable to the proposed method.\n\n**Clarity:**  \nThe paper is overall well-structured and presents its four-step method clearly with supporting diagrams and ablation studies. The inclusion of implementation details, such as hyperparameter ranges and data usage, improves reproducibility and readability.\n\n**Significance:**  \nThe study extends the potential of activation steering beyond single-model manipulation, showing that external expert knowledge can effectively guide other models during inference. This could inspire future research in efficient, parameter-free cross-model transfer and domain adaptation for LLMs."}, "weaknesses": {"value": "1. **The motivation lacks conceptual clarity and fails to align with experiments.**  \n   Although the paper claims to “guide LLMs toward human-desired reasoning behaviors,” its experiments only evaluate domain-specific benchmarks (e.g., MedQA, MMLU, FPB) rather than any human-alignment or controllability tasks. There is no human preference modeling, interpretability evaluation, or reasoning-alignment metric. As such, the stated motivation of aligning model reasoning with human expectations remains untested and conceptually detached from the empirical evidence. The work effectively focuses on cross-domain knowledge transfer rather than human-centered reasoning control.\n\n2. **Layer correspondence via mutual information is poorly justified and not robust.**  \n   The method does not perform layer-wise one-to-one alignment between the expert and target models; instead, it selects layer pairs with *maximal mutual information*. However, this heuristic is questionable—mutual information can be dominated by noise or superficial correlations, especially across architectures with different depth or width. There is no analysis showing that high-MI pairs yield semantically meaningful alignment or better transfer. Moreover, the MI computation is performed on limited samples, raising concerns about statistical reliability. Without theoretical or empirical validation, this alignment strategy feels ad hoc and undermines the credibility of the proposed steering mechanism.\n\n3. **The approach still requires model-specific adaptation, contradicting its claimed generalizability.**  \n   EXPERTSTEER is described as a general framework, yet it requires training a **separate auto-encoder for each expert–target model pair**. This introduces significant overhead when scaling to multiple models or domains. The paper does not test whether a single alignment model can transfer across different LLM families or tasks. Consequently, the claim of broad generalization is overstated and unsupported by empirical evidence.\n\n4. **Architectural mismatch between expert and target models is unaddressed.**  \n   The method implicitly assumes that aligned layer pairs share compatible dimensionalities, but models like Qwen2.5-7B and Gemma-2-2B have different depths and intermediate dimensions. The paper does not explain how such discrepancies are handled—through interpolation, pooling, or projection. Without this clarification, the reproducibility and scalability of the approach are questionable, especially in cross-family scenarios.\n\n5. **Lack of theoretical or mechanistic justification for core components.**  \n   The use of *mutual information* for layer selection and *Recursive Feature Machines (RFMs)* for extracting steering vectors lacks principled reasoning. No ablation or theoretical discussion is provided to justify why these specific methods are necessary or superior to simpler alternatives (e.g., cosine similarity or attention-map correlation). The method remains largely empirical and lacks a clear mechanistic interpretation of how the steering vector alters hidden representations.\n\n6. **Limited scale and weak empirical validation.**  \n   The experiments are confined to mid-sized models (≤13B parameters), without any evaluation on large-scale models such as 70B or beyond. It is unclear whether the proposed method scales effectively to models with more complex representational hierarchies. The absence of large-scale experiments significantly limits the paper’s generality and practical relevance.\n\n7. **Modest improvements without causal interpretation.**  \n   The reported gains (≈+4–6 points on average) are small and lack causal validation. The paper provides no visualization or probing to confirm that the steering vectors genuinely encode transferable expert knowledge. It remains unclear whether performance improvements stem from meaningful knowledge transfer or incidental activation shifts.\n\n8. **Evaluation scope is narrow and lacks strong baselines.**  \n   Key comparisons are missing, such as parameter-efficient adaptation methods (LoRA, prompt-tuning) and MoE-based expert aggregation. Furthermore, there is no zero-shot or few-shot evaluation to demonstrate true transferability. As a result, the empirical section feels incomplete and insufficient to substantiate the general claims.\n\n9. **Efficiency and scalability claims are overstated.**  \n   While the paper highlights short fine-tuning times (~17 minutes on 2k samples), it omits the substantial preprocessing cost of extracting hidden states from large experts and training separate auto-encoders for each layer pair. This hidden cost undermines the “lightweight” and “scalable” claims made in the abstract.\n\n10. **Cross-lingual results are superficial and lack analysis.**  \n    The Chinese experiments (XCOPA-zh, XNLI-zh, Flores) show marginal gains on small datasets. The authors provide no linguistic or typological analysis explaining transfer behavior, and the expert model choice (Llama3.1-Chinese-Chat) is not justified. The multilingual extension feels like an afterthought rather than a core contribution.\n\n---\n\n**Typos and Formatting Issues**\n- **Extra period at Line 107** and multiple sentence fragments reduce overall readability.  \n- Some figure references (e.g., “Figure 2”, “Figure 3”) are missing or lack clear correspondence with the main text.  \n- Minor grammatical inconsistencies suggest insufficient proofreading before submission."}, "questions": {"value": "> I encourage the authors to thoroughly address the weaknesses and questions raised in this review. If the authors can provide detailed explanations and in-depth clarifications during the rebuttal, and if the revised version demonstrates substantial progress in both clarity and improvement, I will be willing to reassess the manuscript and **adjust my overall rating** accordingly, based on the quality and depth of the revision.\n\n---\n\n1. **Conceptual motivation and alignment with stated objectives.**  \n   The paper claims to steer LLMs toward “human-desired reasoning,” yet all experiments evaluate domain transfer tasks (e.g., medical, financial, mathematical) without explicit behavioral or preference-based validation.  \n   - Could the authors define what constitutes “human-desired reasoning” in measurable terms?  \n   - How do domain benchmarks operationalize this concept?  \n   - Would incorporating preference alignment, trustworthiness, or interpretability metrics better ground the claimed motivation?\n\n2. **Layer correspondence under architectural heterogeneity.**  \n   EXPERTSTEER aligns expert and target layers via *mutual information (MI)*, rather than strict layer-to-layer mapping. However, this MI-based selection may produce unstable or semantically shallow correspondences when the two models differ in depth or dimensionality (e.g., Qwen2.5-7B vs. Gemma-2B).  \n   - How is MI normalized across architectures where $ N_E \\neq N_T $?  \n   - What happens when the two models differ by more than 30% in layer count—does the alignment remain reliable?  \n   - Have the authors examined whether MI-selected pairs correspond to functionally similar layers, or are they empirically chosen without semantic validation?\n\n3. **Scalability and computational feasibility of per-layer auto-encoder training.**  \n   The method appears to require one auto-encoder per aligned layer pair for every expert–target combination, which could quickly become infeasible for large models (e.g., 70B-scale).  \n   - How many auto-encoders are trained per experiment, and what are their parameter counts or GPU-hour costs?  \n   - Have the authors considered hierarchical parameter sharing or a unified latent projection space to mitigate scalability issues?  \n   - Without such mechanisms, how can the framework be considered “lightweight” or “model-agnostic”?\n\n4. **Theoretical justification for mutual information as the pairing criterion.**  \n   MI estimation in high-dimensional hidden states is non-trivial and prone to bias.  \n   - Why was MI chosen instead of more interpretable measures like CCA or CKA for layer correspondence?  \n   - What MI estimator was used, and how was its stability validated given only ~500 samples?  \n   - Could noise in MI estimation lead to spurious or misleading layer matches?\n\n5. **Rationale and necessity of Recursive Feature Machines (RFMs).**  \n   The paper introduces RFMs to extract steering vectors, but their theoretical advantage over simpler methods (e.g., PCA, kernel PCA, or mean-difference vectors) remains unproven.  \n   - What specific kernel and regularization are used in $ K_t(\\cdot, \\cdot) $?  \n   - How much variance or representational gain does the non-linear RFM mapping contribute compared to PCA?  \n   - Could the authors provide ablation evidence confirming that RFMs are necessary for the observed improvements?\n\n6. **Interpretability and causal validation of steering interventions.**  \n   While the reported accuracy gains are consistent, there is no causal or interpretability analysis demonstrating *how* steering vectors alter model behavior.  \n   - Can the authors visualize activation shifts before and after steering, e.g., via cosine distance or neuron activation heatmaps?  \n   - Are these shifts correlated with domain-relevant token patterns or attention focus changes?  \n   - Without such analysis, how can we confirm that improvements stem from meaningful knowledge transfer rather than random activation drift?\n\n7. **Comparison with stronger and more recent baselines.**  \n   The baselines mainly cover activation-based steering (ITI, CAA, SADI), but exclude more representative *parameter-efficient transfer* approaches (e.g., LoRA fusion, AdapterFusion, or MoE-based domain routing).  \n   - How does EXPERTSTEER conceptually differ from LoRA fusion, which can also perform targeted domain transfer without full fine-tuning?  \n   - Could the authors include or discuss such baselines to better position their contribution within the broader parameter-efficient adaptation landscape?\n\n8. **Sensitivity and robustness to key hyperparameters.**  \n   The method depends critically on the number of selected layer pairs $ P $ and steering magnitude $ \\varepsilon $.  \n   - Have the authors conducted systematic sensitivity tests across $ P \\in \\{2, 4, 8, 12\\} $?  \n   - Does performance degrade sharply when $ \\varepsilon $ increases, suggesting instability or over-steering?  \n   - Such results would help substantiate claims of robustness and generality.\n\n9. **Scale generalization and missing evaluation on large models.**  \n   All reported results are based on models ≤13B parameters.  \n   - Have the authors tested EXPERTSTEER on larger-scale models (e.g., Llama3-70B or Qwen2.5-72B) to validate scalability?  \n   - Are the MI computations and per-layer auto-encoders still tractable at that scale?  \n   - Without such evidence, the claim of “scalable steering for arbitrary models” seems overstated.\n\n10. **Cross-lingual and typological generalization.**  \n   The Chinese experiments (XCOPA-zh, XNLI-zh, Flores) show small gains and limited linguistic analysis.  \n   - Were the auto-encoders retrained for Chinese data or reused from English alignment?  \n   - Given differences in tokenization and syntax, how robust is the steering effect across typologically distinct languages?  \n   - Could the authors provide a quantitative breakdown of cross-lingual steering efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vu8XDNBonB", "forum": "81RoHr6g8f", "replyto": "81RoHr6g8f", "signatures": ["ICLR.cc/2026/Conference/Submission11392/Reviewer_14cR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11392/Reviewer_14cR"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807075872, "cdate": 1761807075872, "tmdate": 1762922512858, "mdate": 1762922512858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The work introduces a cross-model activation steering method named ExpertSteer that employs external expert model to generate steering vectors capable of guiding or modifying the behavior of target LLMs during inference. It aims to overcome the limitation of prior steering methods that rely solely on the internal activations of a single model."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Breaks model dependency in activation steering, enables cross-model knowledge transfer. ExpertSteer introduces external expert models to generate steering vectors. It not only injects domain-specific knowledge absent in the target model, but also aligns feature dimensions via auto-encoders and matches intervention layers using mutual information analysis. \n\n2. Accurately captures expert knowledge, enhances steering vector effectiveness. Traditional methods often rely on linear feature extraction techniques such as Mean Difference (MD) or PCA. ExpertSteer employs recursive feature machines (RFM) to extract nonlinear features. Experiments show that compared to MD and PCA, steering vectors extracted by RFMs improve target model accuracy by 3–7% on MedQA and 5–6% on MMLU-Medical.\n\n3. ExpertSteer improved interpretable capability in activation steering task. By using mutual information analysis to select intervention layers, ExpertSteer precisely identifies layers where the knowledge gap between the target and expert models is largest, avoiding noise from random interventions."}, "weaknesses": {"value": "1. The effectiveness of EXPERTSTEER heavily depends on the expertise of the expert model and the quality of training data. If the expert model lacks domain knowledge (e.g., using a general-purpose model instead of a domain expert) or the training data contains mislabeled samples (e.g., incorrect medical classifications), the steering vector’s effectiveness may significantly reduced. Since the experiments show that using a general model (e.g., Llama-3-8B) to generate steering vectors only yields a 1–2% performance gain, far below the 4–7% achieved with domain expert models. Besides, RFMs require 2,000 positive and 2,000 negative samples, limiting its applicability in low-resource domains.\n\n2. Hyperparameter tuning relies on validation sets, lacks automation. EXPERTSTEER involves two critical hyperparameters: the number of intervention layers (P) and the intervention strength (ε). The optimal values varies across models (e.g., Llama-3.1-8B-Instruct prefers $P = 4, ε = 8$, while Qwen2.5-7B-Instruct prefers $P = 6, ε = 12$). The paper notes that a small validation set is required for hyperparameter search, and no automated tuning mechanism is provided. This increases deployment cost and complexity in large-scale multi-model scenarios.\n\n3. No comparison with cross-model steering methods. The authors didn't include the comparisons with cross-model steering methods. \n\n4. Lack of robustness evaluation in extreme scenarios. Although the paper validates ExpertSteer in common domains such as medicine and finance, it does not evaluate its performance in extreme scenarios such as low-resource languages, high-risk tasks, or adversarial examples."}, "questions": {"value": "Q1. Can the single expert model be replaced with multi-expert? Would this improve the steering performance?\n\nQ2. If the output layer overlap between the expert and target models is large, does learning at earlier layers still provide meaningful benefits?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ys6hhq8Zhh", "forum": "81RoHr6g8f", "replyto": "81RoHr6g8f", "signatures": ["ICLR.cc/2026/Conference/Submission11392/Reviewer_46Sn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11392/Reviewer_46Sn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926236661, "cdate": 1761926236661, "tmdate": 1762922512467, "mdate": 1762922512467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper bridge the gap between activation steering and knowledge transferring by introducing a robust four-step pipeline, coined ExpertSteer, which transferring the steering direction of a specific domain calculated from an expert model trained on related data to the target model of interest. By only training an auto-encoder-like adapter to account for the difference in model architecture of the expert and the target model, this method can benefit from a wide range of already open-sourced fine-tuned model to calculate steering direction for a specific domain, only requiring to much lower trainable parameters compared to fine-tuning an expert model for the domain of choice."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method provides a new way to choose layers for steering using Mutual Information criterion and the idea of matching layers between expert and target models is very interesting.\n2. The authors introduce a novel idea of calculating steering direction as the direction capturing most variation in the discriminative feature space.\n3. Diversing empirical experiments as well as detailed ablation studies."}, "weaknesses": {"value": "1. The way the Auto-encoder is trained on expert model's feature space is only through reconstruction loss, which lack the regularization for the hidden state space of the auto-encoder to align with the feature space of the target model. Taking what is written in the paper, the authors only use an affine layer for the encoder/decoder; this allows for spurious hidden feature spaces that is may not align with the target model at all: if the expert model's feature space dimension ($d_E$) is smaller than that of the target model ($d_T$), then the hidden feature space of the auto-encoder with dimension of $d_T$ can be learnt to only utilize the first $d_E$ dimension to perfectly reconstruct the expert model hidden state. With the other case, then the auto-encoder hidden feature space is learnt as the span of the eigenvectors correspond to $d_T$ largest eigenvalues of the expert feature space.\n2. The way the intervened layer pairs are chosen current does not account for the order of the layers, which could dampened the performance of the method.\n3. While the idea is sounded and novel, there is no theoretical evidence to back the method.\n4. As there are some evidence that the representation between models that belong to the same family architecture are wildly difference, how would the result be at higher scale LM (I would love to see some result on 32B version of LLama and Qwen)."}, "questions": {"value": "Refer to the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GypTx6Z5bs", "forum": "81RoHr6g8f", "replyto": "81RoHr6g8f", "signatures": ["ICLR.cc/2026/Conference/Submission11392/Reviewer_1afh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11392/Reviewer_1afh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982557578, "cdate": 1761982557578, "tmdate": 1762922511751, "mdate": 1762922511751, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents EXPERTSTEER, an activation steering framework that enables cross-model knowledge transfer by using external expert models to guide target LLMs during inference. The method aligns representations via auto-encoders, selects intervention layers using mutual information, and derives steering vectors through Recursive Feature Machines (RFMs) to influence activations without fine-tuning. Experiments on three LLMs across multiple domains show consistent gains over existing steering and fine-tuning methods, highlighting its efficiency and generalizability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Introduces a generalizable and model-agnostic steering framework that integrates external expert knowledge.\n\n- Comprehensive experiments across multiple domains and model families support the proposed method’s effectiveness.\n\n- Demonstrates both same-family and cross-family transfer, showing robustness.\n\n- Computationally efficient with negligible inference overhead.\n\n- Clear methodology and detailed ablation studies on design choices (RFMs, alignment order, expert selection)."}, "weaknesses": {"value": "- Limited dataset diversity. Several benchmarks such as MMLU variants are overused, which may overstate generalization.\n\n- Scalability concerns remain, as only small-to-medium LLMs are tested. It is unclear how EXPERTSTEER performs for much larger models.\n\n- The mutual information-based layer pairing and RFM feature extraction steps, while intuitive, would benefit from stronger theoretical or empirical justification.\n\n- The paper does not analyze potential risks of applying expert interventions (e.g., bias transfer, domain drift)."}, "questions": {"value": "- How sensitive is EXPERTSTEER to the choice of the expert model? Could an inappropriate expert degrade performance?\n\n- How would the framework scale to models with hundreds of layers or significantly higher hidden dimensions?\n\n- Can the approach be adapted for multimodal models or instruction-following alignment tasks?\n\n- Does applying external steering vectors risk transferring unintended biases from the expert model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8tImfsgudg", "forum": "81RoHr6g8f", "replyto": "81RoHr6g8f", "signatures": ["ICLR.cc/2026/Conference/Submission11392/Reviewer_i1wX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11392/Reviewer_i1wX"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission11392/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993773845, "cdate": 1761993773845, "tmdate": 1762922511318, "mdate": 1762922511318, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}