{"id": "HV5FVzTAtB", "number": 13820, "cdate": 1758223166213, "mdate": 1759897410441, "content": {"title": "SOLAR: Communication-Efficient Model Adaptation via Subspace-Oriented Reparametrization", "abstract": "Parameter-efficient fine-tuning (PEFT) methods, such as LoRA,  enable scalable adaptation of foundation models by injecting low-rank adapters. However, their communication and storage costs remain a major bottleneck in resource-constrained settings.\nWe propose SOLAR (Subspace-Oriented Latent Adapter Reparameterization), a post-training compression framework that substantially reduces the communication cost (i.e., the number of parameters to transmit or store) of PEFT adapters.\nSOLAR expresses each PEFT update as a linear combination of basis vectors formed from the foundation model’s singular vectors with controlled random perturbations.\nBy exploiting the subspace similarity (the alignment of principal directions) between the foundation model and task-specific fine-tuned updates, SOLAR decouples the  adapter size from PEFT structure and ensures compact yet expressive representations. It is model-agnostic and compatible with existing PEFT methods, including LoRA and other adapter modules.\nWe theoretically establish a bound on the reconstruction error.\nExperiments on language and vision tasks using LLaMA, GPT, and ViT models demonstrate that SOLAR preserves task performance while significantly reducing model representation sizes, offering an effective and communication-efficient solution for deployment in distributed systems and edge devices.", "tldr": "We propose SOLAR, a post-training compression framework that reparameterizes PEFT adapters as sparse combinations of model-informed subspace bases, reducing communication and storage costs without sacrificing performance.", "keywords": ["Parameter-efficient fine-tuning", "Adapter compression", "LoRA", "Sparse approximation", "Model adaptation", "Communication-efficient training", "Foundation models", "Post-training compression"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d96f89520932154530df8d203492458487d8a941.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes the SOLAR method, which constrains parameter updates to the subspace spanned by singular vectors through subspace-oriented reparameterization and combines this with a sparse composition mechanism, achieving significant reductions in communication and storage costs. The paper also provides a theoretical upper bound on the reconstruction error and demonstrates the method’s effectiveness on NLP and CV tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a subspace-oriented reparameterization approach that differs from mainstream low-rank methods.  By constraining parameter updates to the subspace spanned by singular vectors, it offers a novel perspective for PEFT research.\n2. The combination of the reparameterization strategy with a sparse composition mechanism is well-motivated and supported by solid theoretical grounding.\n3. The paper is reasonably well-structured, with the method presented in a mathematically rigorous way and the experimental tables and results clearly organized."}, "weaknesses": {"value": "1. While the paper introduces a theoretical upper bound on the reconstruction error, the connection between this bound and the observed empirical performance could be discussed in more depth.\n2. The evaluation might be further strengthened by including comparisons with some of the most recent efficient PEFT methods."}, "questions": {"value": "The paper presents a theoretical upper bound on the reconstruction error, but the discussion of its tightness and its relation to experimental results is not sufficiently developed. Including experiments that compare the theoretical bound with the observed reconstruction error would help strengthen the theoretical contribution.  In addition, the empirical evaluation could be further improved by incorporating comparisons with more recent efficient PEFT methods to enhance the overall persuasiveness of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6n8kiuB5Rp", "forum": "HV5FVzTAtB", "replyto": "HV5FVzTAtB", "signatures": ["ICLR.cc/2026/Conference/Submission13820/Reviewer_qfT9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13820/Reviewer_qfT9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761653798990, "cdate": 1761653798990, "tmdate": 1762924348201, "mdate": 1762924348201, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SOLAR, a parameter-efficient finetuning (PEFT) method that aims to reduce communication costs, i.e. the number of tunable parameters. The key idea is to represent the weight update as a linear combination of multiple basis matrices, where each basis is generated by randomly permuting the original pretrained weights. The only learnable parameters are the coefficients of this linear combination, effectively restricting finetuning to a low-dimensional subspace. SOLAR is evaluated on a range of model architectures, including LLaMA, GPT, and ViT."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The idea of reparameterizing finetuning in a low-dimensional subspace is conceptually interesting and has potential implications for lightweight adaptation.\n\nThe approach is simple and easy to understand, which makes it appealing for practical deployment scenarios."}, "weaknesses": {"value": "1. Although the number of learnable parameters is small, the subspace matrices $M_A$ and $M_B$ themselves occupy non-trivial memory. This undermines the claim of efficiency, as these auxiliary tensors could impose significant additional storage and communication overhead.\n\n2. The method implicitly assumes that the task-specific update $\\Delta W$ lies close to a subspace spanned by random permutations of the pretrained weights $W$. This is a strong and non-obvious assumption. The paper should provide empirical evidence or theoretical justification to support the existence of such subspace similarity.\n\n3. Unrepresentative experimental setup:\n- The experimental design raises concerns about the generality of the results. Using only 10 samples for finetuning is not representative of typical adaptation scenarios.\n- The reliance on outdated models (e.g., GPT-2) and small-scale datasets (CIFAR-10/100) limits the relevance and impact of the findings.\n\n4. The reported gains are significant — as shown in Table 1, SOLAR does NOT outperform baselines such as LoRA. In addition, ablation studies on the construction of the subspace (e.g., number of bases, permutation strategy) are missing, leaving the effectiveness of key design choices unclear.\n\n5. The writing is not well structured and organized. It takes extra efforts to understand some unexplained concepts, like subspace similarity, the alignment of principal directions, PEFT structure, etc."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "siCOXZvnUd", "forum": "HV5FVzTAtB", "replyto": "HV5FVzTAtB", "signatures": ["ICLR.cc/2026/Conference/Submission13820/Reviewer_BHU1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13820/Reviewer_BHU1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803967136, "cdate": 1761803967136, "tmdate": 1762924347608, "mdate": 1762924347608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In my understanding, SOLAR is a post-training compression method designed to make parameter-efficient fine-tuning (PEFT) adapters like LoRA much lighter for communication and storage. It uses the sparsity of LoRA to its advantage. Instead of storing full adapter matrices, SOLAR converts them into sparse combinations of basis vectors derived from the foundation model’s singular vectors, each with a slight random perturbation. This approach leverages the natural alignment between fine-tuned updates and the model’s subspace, allowing adapters to be reconstructed using only a few coefficients and a random seed. As a result, SOLAR can shrink adapter size by a large margin while preserving accuracy, making it ideal for distributed or resource-constrained environments without changing the original training process."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Well-Motivated Idea: The paper addresses a critical bottleneck in PEFT, which is the storage overhead. This can even be discussed more in the paper, as for mobile applications which might leverage lots of LoRA on-device, it is super useful to save space. The authors leverage LoRAs sparsity and subspace alignment for compression. \n\nTheoretical Backing: The paper is built on a strong theoretical foundation, from the subspace analysis(Amplification factor) literature on LoRA. The authors show formal reconstruction error bounds and explain why subspace-oriented bases preserve task performance.\n\nEmpirical: The method impressive results across vision and language tasks, achieving up to 98 to 99% adapter size reduction with negligible accuracy loss. Robustness under quantization and scalability to large models further validate the approach.\n\nSignificance: Offers a practical solution for storing and transmitting large numbers of LoRA adapters on-device or in federated learning setups, making PEFT more viable in real-world deployments."}, "weaknesses": {"value": "[W1] Not sure if I missed it, but I did not find a comparison of  subspace-based compression v/s a simple post-hoc compression approach such as [1] (Simply dropping singular values)\n\n[W2] There is a table on effect of quantization of SOLAR, but quantization itself if a method of saving memory footprint. There needs to be a comparison of how SOLAR performs v/s quantization (LoRA+different quantization levels v/s SOLAR+different quantization levels)\n\n[W3] I understand a case when SOLAR performs well in compressing LoRA weights when the base model itself can represent what the LoRA has learnt (there is a method to extract this information from the subspace of the model). However, many adapters are highly orthogonal (and are trained to be highly orthogonal) to the model weights. This is also analogous to the information present in LoRA being part of nullspace of the model (discussed in [2]). The authors must analyze the effect of SOLAR compression on tasks with varying range of orthogonality (increasing amp factor to the model weights).  \n\n[1] https://arxiv.org/pdf/2509.10971\n[2] https://arxiv.org/abs/2506.04244"}, "questions": {"value": "Q1. How does the method compress adaptive-rank selective PEFT methods (AdaLoRA, SORA, FouRA)? Will the memory saved be lesser?\nQ2. Methods such as FouRA and Wavelet-based LoRAs try to demonstrate that singular values are not widely spread out in the low rank subspace. How would the subspace-based compression affect these models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IbvZqzQkT6", "forum": "HV5FVzTAtB", "replyto": "HV5FVzTAtB", "signatures": ["ICLR.cc/2026/Conference/Submission13820/Reviewer_Z2Yv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13820/Reviewer_Z2Yv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973423008, "cdate": 1761973423008, "tmdate": 1762924347079, "mdate": 1762924347079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The author introduces the methods SOLAR (Subspace-Oriented Latent Adapter Reparameterization), a post-training compression method, which aims to target the communication and storage bottlenecks of PEFT method. It re-expresses the PEFT update in a more compact way, formulates each PEFT update as a linear combination of basis vectors formed from the foundation model’s singular vectors with controlled random perturbations. By exploiting subspace similarity, SOLAR remains compact and expressive with bounded reconstruction error. SOLRA offers interesting applications for deploying PEFT in distributed systems and edge devices."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- the results in Table 5 and Table 6 are quite impressive, especially because language models are the most scaled models, this huge reduction in parameter count can have significant implications for PEFT methods's communication and storage when applied to large language models\n- interesting results provided in the subspace analysis, I think it greatly justifies the different alignments used in SOLAR compared to NOLA\n- theoretical analysis of the SOLAR reconstruction error bound"}, "weaknesses": {"value": "- the author mentions in the abstract that it's method is model-agnositc and compatible with existing PEFT methods other than LoRA, while the authors only applies SOLAR to low-rank based methods. I think it is required to see whether I will be interested into seeing the results of SOLAR applied to PEFT methods with other examples, for example, the authors mentioned orthogonal finetuning methods in Section 2.0, I would interested to see if it also works there, because different than low-rank based methods, orthogonal matrices are full-rank, is the mentioned improvement of paramter-efficiency still present, also under lower model precision? I would be interested in additional results in general, at least results of Table 5 to add the results of OFT and quantized OFT (to the best of my knowledge, I think orthogonal finetuning also supports low-precision base model) and the reduction in parameters in llama and gpt are most significant. I think this will definitely strengthen the paper and as it shows SOLAR is generic and applicable to different sorts of PEFT methods"}, "questions": {"value": "- I am wondering will SOLAR also works with fine-tuning diffusion models? LoRA has often been applied into finetuning diffusion models, will the reparameterization affect the image generation quality? \n- The reductions in parameters in ViT not seem be as significant as Llama and GPT,  what is the implication for that? \n- I might miss something, but I am a bit confused with what SOLAR stores and how to calculate the trainable parameters of SOLAR: with a_iM_A and b_jM_B? (like the equation 5?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v5T9rCUIgF", "forum": "HV5FVzTAtB", "replyto": "HV5FVzTAtB", "signatures": ["ICLR.cc/2026/Conference/Submission13820/Reviewer_jmkV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13820/Reviewer_jmkV"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994235301, "cdate": 1761994235301, "tmdate": 1762924346569, "mdate": 1762924346569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}