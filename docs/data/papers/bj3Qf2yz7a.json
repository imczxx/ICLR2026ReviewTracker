{"id": "bj3Qf2yz7a", "number": 13138, "cdate": 1758213983863, "mdate": 1759897461567, "content": {"title": "MEAL: A Multi-dimensional Evaluation of Alignment Techniques for LLMs", "abstract": "As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values, organizational norms, and safety standards has become a central pursuit in machine learning. The field has developed diverse alignment approaches including traditional fine-tuning methods (e.g., RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these techniques to guide implementation and deployment decisions. This paper introduces MEAL: A Multi-dimensional Evaluation of ALignment Techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across these major alignment techniques. This framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. To demonstrate the utility of this framework, we run a series of experiments across diverse base models and alignment techniques. This paper describes these experiments and their results and concludes by identifying the strengths and limitations of current state-of-the-art models and providing valuable insights as to the trade-offs among these alignment techniques.", "tldr": "This paper introduces MEAL, a framework for systematically evaluating and comparing different alignment techniques for Large Language Models across multiple dimensions like awareness, efficiency, quality, and robustness.", "keywords": ["Large Language Models", "alignment", "human values", "safety standards", "robustness", "model evaluation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1eb1b513d891bf69165c45ac19e83289c5b26fb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses the critical challenge of evaluating and comparing alignment techniques for Large Language Models (LLMs) to ensure their outputs adhere to human values and safety standards. To tackle this issue, the authors introduce MEAL (Multi-dimensional Evaluation of ALignment Techniques), a unified framework that systematically assesses alignment methods across four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through extensive experiments on various base models and alignment techniques, the paper demonstrates MEAL’s utility by identifying the strengths, limitations, and trade-offs of current state-of-the-art alignment approaches, providing valuable guidance for practitioners in selecting and deploying these techniques."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a novel and underexplored challenge in the LLM alignment field: how to unify evaluation across multiple alignment dimensions.\n2. The work is supported by extensive and well-designed experiments that validate the proposed framework’s effectiveness across diverse settings.\n3. The introduction of a unified evaluation framework for LLM alignment is valuable, as it provides essential guidance for researchers and practitioners in selecting and comparing alignment techniques."}, "weaknesses": {"value": "1. The authors lack a detailed description and an overview figure of the key design of the proposed framework in Section 3. The current presentation merely offers a naive explanation of the four evaluation dimensions, which reads more like a group meeting discussion than a rigorous academic framework. A comprehensive framework design should include specifics such as the data design, definitions and extensibility of alignment dimensions, whether new metrics are introduced or existing ones are integrated, and the relationship between the framework’s design and its usage. Additionally, it remains unclear whether the framework supports user customization. An overview figure and a restructured Section 3 are essential to clearly convey the novelty of the work.\n2. The relationship between the evaluation protocol and the framework is ambiguous. It is unclear whether the protocols are customizable or fixed, which directly affects the stability and generalizability of the framework. In the current draft, protocols appear to be flexible experimental settings. However, without a clear justification, it is uncertain how changes in protocols might influence the results and, more importantly, the credibility of those results.\n3. The experimental section lacks ablation studies on the framework itself. For instance, can additional evaluation dimensions be incorporated based on specific alignment needs? How do different metrics affect the evaluation outcomes? What are the impacts of modifying key design modules within the framework? These questions remain unaddressed, limiting a deeper understanding of the framework’s robustness and adaptability."}, "questions": {"value": "Please check the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2wvVxVYfDy", "forum": "bj3Qf2yz7a", "replyto": "bj3Qf2yz7a", "signatures": ["ICLR.cc/2026/Conference/Submission13138/Reviewer_YAnR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13138/Reviewer_YAnR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761725757703, "cdate": 1761725757703, "tmdate": 1762923857946, "mdate": 1762923857946, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the MEAL framework, which conducts a multi-dimensional evaluation of various alignment strategies for large language models (LLMs), including alignment detection capability, alignment quality, efficiency, robustness, and safety. Through extensive experiments, the paper validates the performance of different strategies (e.g., instruct, aligner, base, few-shot) across multiple benchmarks, emphasizing that a single metric is insufficient to assess overall alignment effectiveness. It further introduces a multi-dimensional framework for comprehensive comparative evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper systematically integrates multiple dimensions of alignment evaluation, considering not only alignment quality but also efficiency and safety, making it more comprehensive than existing alignment assessments.\n\n2. The experiments are extensive, covering mainstream benchmark datasets and a wide range of models."}, "weaknesses": {"value": "1. The framework primarily focuses on integrating multi-dimensional metrics and conducting experimental comparisons, lacking innovation in new evaluation methods, theoretical foundations, or alignment assessment techniques. It also does not provide clear, actionable guidance for new algorithms, theories, or alignment approaches. While the comprehensive evaluation has value, it is relatively intuitive and lacks profound new insights or theoretical explanations.\n\n2. Although MEAL offers multi-dimensional evaluation, the framework’s quantifiable metrics and normalization methods are not clearly defined, particularly regarding how to integrate different dimensions and handle trade-offs between them. Its guidance for practical scenarios is limited, and it lacks explicit decision-making procedures."}, "questions": {"value": "Please refer to the relevant points in the Weaknesses section. If the authors can provide clarification and improvements, I would be very happy to raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iZ3n2tRVFq", "forum": "bj3Qf2yz7a", "replyto": "bj3Qf2yz7a", "signatures": ["ICLR.cc/2026/Conference/Submission13138/Reviewer_hfSG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13138/Reviewer_hfSG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845995919, "cdate": 1761845995919, "tmdate": 1762923857712, "mdate": 1762923857712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MEAL, a multi-dimensional evaluation framework for alignment techniques applied to large language models. MEAL formalizes four evaluation axes: alignment detection, alignment quality, computational efficiency, and robustness and safety. The authors implement MEAL across a diverse set of open-source base models, instruct-tuned variants, and lightweight aligner modules, evaluate on multiple benchmarks including BeaverTails, SafeRLHF, XSTEST-response, TruthfulQA, HarmfulQA and Reward-bench 2, and measure detection metrics, pairwise correction win rates under LLM and reward-model judges, end-to-end latency and peak memory, and resistance to adversarial attack using the StrongREJECT suite. The experimental results highlight trade-offs between approaches, with a small specialized aligner called granite-aligner showing strong detection and quality while remaining efficient, ethical-aligner excelling on some robustness measures, and instruct-tuned models often achieving high quality at higher resource cost. The paper argues for choosing alignment strategies by weighing these multiple dimensions rather than a single metric."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an important and timely problem, namely the lack of a unified, praxis-oriented evaluation protocol for alignment techniques, and does so in a way that is both broad and practical. The four-dimension decomposition is intuitive and useful because it mirrors real deployment constraints, explicitly bringing together detection, quality, efficiency and robustness into one decision framework. The experimental scope is substantial for a single paper: multiple model families, a diverse set of benchmarks that cover different harm modalities, two separate judging pipelines including reward-model ensembles, and adversarial tests from StrongREJECT. The use of both judge-model panels and reward-model panels to compare aligned versus original responses is a good design choice because it surfaces evaluator variance and highlights reliability issues. The paper is clear in its motivation and structure, presents results in tables that make cross-dimension comparisons possible, and offers thoughtful discussion about how alignment choices produce trade-offs in practice."}, "weaknesses": {"value": "1. The aggregation into a single MEAL score is described but not formalized. The paper should make explicit the normalization and weighting scheme, motivate it, and include ablations showing how overall rankings change under alternative normalizations and weights.\n2. The similarity-based detection rule for some aligners is under-specified and potentially brittle. The paper uses BLEU and ROUGE thresholds to decide safety, yet these metrics are known to poorly correlate with semantic equivalence in many cases. Authors can report the exact threshold values, their selection procedure, and sensitivity analyses showing how results change across reasonable thresholds."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SG0ZuR7qum", "forum": "bj3Qf2yz7a", "replyto": "bj3Qf2yz7a", "signatures": ["ICLR.cc/2026/Conference/Submission13138/Reviewer_73Ev"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13138/Reviewer_73Ev"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762111191543, "cdate": 1762111191543, "tmdate": 1762923857408, "mdate": 1762923857408, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MEAL, a “multi-dimensional” framework to compare LLM alignment techniques across four axes—detection, quality, efficiency, and robustness—then reports results over several datasets and model families."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposed an unified evaluation of diverse alignment strategies which is important and timely.\n2. Clear structure and released code link.\n3. The paper provides the broad metric coverage including detection/quality/efficiency/robustness with reasonably transparent protocols."}, "weaknesses": {"value": "1. The four dimensions largely mirror standard axes used across recent evaluation work (safety/robustness, win-rate judging, latency/memory). The contribution is primarily an aggregation and specific choice of tools, with little methodological innovation (no new metrics, calibration procedures, or principled aggregation). The “overall score” is a simple normalization/averaging that the paper itself questions. \n2. For aligners that don’t emit labels, the paper infers “safe vs. harmful” by string similarity thresholds between input and output BLEU/ROUGE with tau=0.5) This is not a validated proxy for safety classification and can spuriously reward minor paraphrases or penalize faithful neutralization. It also prevents reporting AUC/AUROC, making cross-method comparison uneven.\n3. The paper optimizes prompts for base/instruct/ICL models but uses fixed training templates for aligners, which likely advantages aligners on some tasks and disadvantages others. There’s no ablation showing sensitivity to prompt choices or threshold calibration, and no standardized prompt budget across methods.\n4. The authors acknowledge a “relatively small number of open-source models,” limited families, and non-exhaustive strategies. This undermines the claimed generality of MEAL and makes the overall scoreboard shown in the Table 5 fragile."}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "APykGMY5am", "forum": "bj3Qf2yz7a", "replyto": "bj3Qf2yz7a", "signatures": ["ICLR.cc/2026/Conference/Submission13138/Reviewer_xx4g"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13138/Reviewer_xx4g"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13138/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762785612844, "cdate": 1762785612844, "tmdate": 1762923857117, "mdate": 1762923857117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}