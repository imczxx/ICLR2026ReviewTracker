{"id": "rb5eTktqbc", "number": 526, "cdate": 1756744202329, "mdate": 1759898255414, "content": {"title": "Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step Returns", "abstract": "We introduce a sequence-conditioned critic for Soft Actor--Critic (SAC) that models trajectory context with a lightweight Transformer and trains on aggregated $N$-step targets. Unlike prior approaches that (i) score state--action pairs in isolation or (ii) rely on actor-side action chunking to handle long horizons, our method strengthens the critic itself by conditioning on short trajectory segments and integrating multi-step returns---without importance sampling (IS). The resulting sequence-aware value estimates capture temporal structure critical for extended-horizon and sparse-reward problems. On local-motion benchmarks, we further show that freezing critic parameters for several steps makes our update compatible with CrossQ's core idea, enabling stable training without a target network. Despite its simplicity---a 2-layer Transformer with 128--256 hidden units and a maximum update-to-data ratio (UTD) of $1$---the approach consistently outperforms standard SAC and strong off-policy baselines, with particularly large gains on long-trajectory control. These results highlight the value of sequence modeling and $N$-step bootstrapping on the critic side for long-horizon reinforcement learning.", "tldr": "T-SAC trains a transformer-based SAC critic on windowed sequence chunks with N-step TD returns and critic alignment, improving temporal credit assignment, stability, and sample efficiency on long-horizon, multi-phase, and sparse-reward tasks.", "keywords": ["Soft Actor-Critic (SAC)", "Transformer-based Critic", "Sequence Chunking", "N-step Returns", "Critic Alignment", "Double Q-Learning", "Deep Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/de8c13829be8e40757eabca974b989a0dcc2c02d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This article extends the work in the literature [1] by introducing a sequence-conditioned critic for SAC that models trajectory context with a light-weight Transformer and trains on aggregated N-step returns. Empirical results on standard online RL benchmarks show that the proposed method sometimes outperforms other algorithms.\n\n\n[1]Qiyang Li, Zhiyuan Zhou, and Sergey Levine.Reinforcement learning with action chunking.arXiv  preprintarXiv:2507.07969,2025."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is overall well-organized and easy to follow. \n2. The proposed transformer critic is intuitive and is supported by several theoretical results."}, "weaknesses": {"value": "1. The improvement of the algorithm in the experiments is not significant for a basic benchmark, Mujoco.\nThe authors claim that an action mask is needed for T–SAC and may degrade performance with high trajectory variability.\nT-SAC is better than other methods in Meta-World tasks, however, there is higher trajectory variability in Meta-World tasks than that in Mujoco.\n2. The authors of literature [1] claim that we might desire a final optimal Markovian policy, the exploration problem can be better tackled with non-Markovian and temporally extended skills, and that action chunking offers a very simple and convenient recipe for obtaining this. \n*However,* they did not provide any theoretical explanation for the aforementioned phenomenon. They also did not offer any intuitive examples to demonstrate the effectiveness of action chunking in MDP tasks. The same problems are shown in this paper.\n3. I think the major contribution of this work is the proposed causal Transformer. Thus, in addition to the performance gain by inserting the causal Transformer into SAC, it would be good to discuss other metrics that can directly evaluate the \"quality\" or \"informativeness\" of representations learned in the causal Transformer. Such metrics might include Centered Kernel Alignment and Mutual Information Neural Estimation.\n4. The contribution is ambiguous. An alternative interpretation of the benefit from action chunking and T-SAC is that the model is simply given a richer input feature set, which allows a powerful function approximator (like a deep neural network) to better fit a complex value or policy function. \n5. The new method should be compared with QC, which is proposed in [1].\n\n\n*If my concerns and questions are all addressed, I will raise the score.*"}, "questions": {"value": "1. What is the subset of MDPs that have the property that the Transformer-based action chunking can be provably useful?\nThis is an important question. As you can see, the proposed method does not perform well in Mujoco.\n2. Could you discuss the computational cost and scalability of the proposed method in detail, especially as the sequence length increases? \n3. Could you provide a more detailed analysis of how your Transformer-based action chunk gain was observed in your experiments? In particular, in addition to the comparison of rewards in the paper, is there an experiment that more intuitively demonstrates the significant improvement in sample efficiency?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fxyQ1Uc3AU", "forum": "rb5eTktqbc", "replyto": "rb5eTktqbc", "signatures": ["ICLR.cc/2026/Conference/Submission526/Reviewer_9zeX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission526/Reviewer_9zeX"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761060465466, "cdate": 1761060465466, "tmdate": 1762915538882, "mdate": 1762915538882, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes T-SAC (Transformer-based Soft Actor-Critic), which enhances standard SAC by introducing a sequence-conditioned critic that models short trajectory segments via a lightweight Transformer and trains with aggregated N-step returns—without importance sampling. The approach enables long-horizon credit assignment while keeping the actor update one-step. A simple critic-freezing schedule further removes the need for target networks. Experiments on various benchmarks show consistent improvements over SAC, CrossQ, and other baselines, demonstrating that sequence modeling on the critic side can improve sample efficiency and stability in long-horizon control."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea of chunking the critic rather than the actor is novel and conceptually clean, offering a new perspective on temporal abstraction in off-policy RL.\n\n- The removal of importance sampling through prefix-conditioned targets is a practical and effective simplification.\n\n- The paper combines Transformer critics, multi-horizon learning, and critic freezing in a coherent design that improves stability without target networks.\n\n- Experiments are broad and rigorous, with thorough ablations verifying each design component.\n\n- Writing is clear, structured, and technically sound."}, "weaknesses": {"value": "- The novelty is largely architectural; theoretical justification is limited, and the benefits of critic-side chunking lack deeper analysis.\n\n- All results use low-dimensional state inputs; no experiments on visual or partially observable tasks.\n\n- Statistical significance of gains and computational cost comparisons are not deeply analyzed."}, "questions": {"value": "1. How sensitive is T-SAC to the choice of N-step horizon and freezing interval?\n\n2. Would the approach extend to visual or partially observable environments?\n\n3. Could actor and critic share a Transformer backbone for further efficiency?\n\n4. How does T-SAC compare computationally to CrossQ and TOP-ERL at scale?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1xVfRLBtQV", "forum": "rb5eTktqbc", "replyto": "rb5eTktqbc", "signatures": ["ICLR.cc/2026/Conference/Submission526/Reviewer_2mBX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission526/Reviewer_2mBX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739993910, "cdate": 1761739993910, "tmdate": 1762915538759, "mdate": 1762915538759, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of long-horizon credit assignment in standard step-based reinforcement learning. It introduces T-SAC, an algorithm that replaces the standard MLP critic in Soft Actor-Critic (SAC) with a lightweight Transformer. This new critic conditions on short sequences of actions from the replay buffer and is trained on N-step returns across multiple horizons without requiring importance sampling, using a gradient averaging scheme to ensure stability. On long-horizon and sparse-reward control benchmarks like Meta-World ML1 and Box-Pushing, T-SAC is shown to achieve superior sample efficiency and final performance compared to strong off-policy baselines."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1, T-SAC significantly outperforms strong off-policy baselines such as SAC and CrossQ on the Meta-World ML1 benchmark and the FANCYGYM Box-Pushing tasks. Its 58% success rate on the sparse-reward Box-Pushing variant is particularly notable, clearly demonstrating the effectiveness of its long-horizon reasoning capabilities.\n\n2, While N-step returns are known to suffer from high variance as n increases, the proposed gradient averaging technique enables stable training for horizons as long as n=16. This challenges the conventional wisdom of using small n and provides a robust method for learning long-term dependencies.\n\n3, The paper provides a clear justification for its architectural choices through rigorous ablation studies. It demonstrates the critical importance of the self-attention and causal mask components in the Transformer critic and shows its superiority over recurrent backbones like GRU and LSTM."}, "weaknesses": {"value": "1, The performance of T-SAC is not uniform across all benchmarks. On Gymnasium MuJoCo, it performs worse than the standard SAC baseline on Hopper and Walker2d.The authors attribute this to the need for an \"action mask\" on tasks with \"high trajectory variability\", which points to a limitation in the method's generality.\n\n2, The critic conditions on multi-step action sequences (at, ..., at+n-1) generated by the current, often random, policy. Early in training, these sequences are noisy and suboptimal, potentially introducing significant variance into the critic's learning target. This may slow down the initial convergence speed compared to standard critics that condition only on a single (s, a) pair. This effect could be particularly pronounced in dense-reward settings like mujoco, where the benefit of a clean, immediate one-step TD target is high.\n\n3, The proposed critic-parameter freezing schedule (Sec 4.4), which enables training without a target network, is only demonstrated on locomotion tasks. The authors admit it is unstable in sparse-reward settings, where the paper's main results are achieved using conventional Polyak averaging. This makes the target-free contribution a secondary point with limited applicability."}, "questions": {"value": "1, Could you please elaborate on the specific role and necessity of the \"action mask\" mentioned as the cause for performance degradation on \"high trajectory variability\" tasks like Hopper and Walker2d ? Experimental result(e.g. with action mask vs without action mask) would be helpful to understand your claim.\n\n2, The paper's core mechanism is averaging gradients rather than targets. The result shows that naive target averaging fails in sparse-reward settings (Fig. 10b, App. E). Could you provide more intuition as to why averaging gradients is more effective at preserving the sparse reward signal?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4WGmdU1kzC", "forum": "rb5eTktqbc", "replyto": "rb5eTktqbc", "signatures": ["ICLR.cc/2026/Conference/Submission526/Reviewer_m1ZH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission526/Reviewer_m1ZH"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907652867, "cdate": 1761907652867, "tmdate": 1762915538583, "mdate": 1762915538583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes T-SAC (Transformer-based Soft Actor-Critic), an off-policy RL method whose critic is a lightweight causal Transformer trained on short trajectory segments with aggregated (N)-step returns. Unlike standard SAC, which evaluates single state–action pairs, T-SAC conditions the critic on action prefixes $(s_t, a_t,\\ldots,a_{t+n-1})$ and predicts prefix-conditioned values for multiple horizons without importance sampling. The method averages gradients across N-step targets and introduces a parameter-freezing schedule for the critic in locomotion tasks. Experiments on Meta-World ML1, Box-Pushing, and Gymnasium MuJoCo show improved sample efficiency, especially on long-horizon and sparse-reward tasks; e.g., 96.8% success on Box-Pushing (dense) and solving most ML1 tasks within ~5M interactions at UTD=1."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The causal Transformer predicts prefix-conditioned values aligned with realized action prefixes, which plausibly improves temporal credit assignment. Using a “non-soft” critic with entropy regularization on the policy side preserves the standard SAC actor objective and simplifies the critic target, improving implementability.\n2. Gradient-level averaging is supported by Lemma 3 and Theorem 5, establishing $\\mathrm{Var}[\\nabla_\\psi \\bar L] < \\sigma_w^2$ under equicorrelation assumptions. The paper analyzes target-side variance reduction for both reward and bootstrap components, providing bounds $1 \\le R_\\gamma(N) < 4$ and conditions for reducing bootstrap variance. The IS-free formulation avoids high-variance importance ratios by conditioning on realized prefixes and is clearly derived from the N-step return objective.\n3. The evaluation spans 57 tasks across Meta-World ML1, Gymnasium MuJoCo, and Box-Pushing. Gains are pronounced on difficult tasks (e.g., 96.8% on Box-Pushing (dense), outperforming step-based baselines ≤85%) and on multi-phase ML1 tasks such as Assembly, Disassemble, Hammer, and Stick-Pull."}, "weaknesses": {"value": "1. The core idea—Transformer critic trained with N-step returns—resembles TOP-ERL; the manuscript acknowledges this but does not sufficiently sharpen the technical distinctions. The claim of “bridging step-based and episodic regimes” appears overstated: the policy remains step-based while sequence conditioning is confined to the critic. Several components (causal Transformer, parameter freezing, N-step averaging) draw on prior art, but the paper does not isolate which design choices constitute the principal contribution.\n2. Comparisons with traditional off-policy N-step methods using IS are empirical only; formal connections to prior analyses are not developed.\n3. Reported instabilities on Box-Pushing-Sparse lead to a fallback on soft targets, limiting generality. The action-mask requirement and performance variability on Ant/Hopper/Walker2d suggest fragility and sensitivity to dynamics/constraints.\n4. T-SAC trails CrossQ on Hopper and Walker2d; the explanation “action mask needed may degrade performance” lacks systematic investigation. On Box-Pushing-Sparse, success is 58%, below TOP-ERL’s 70%; the claim of being “competitive under sparse feedback” does not reconcile this gap.\n5. No ablations on freeze length $K$ or reuse factor $N_c$ are provided to delineate stability regions.\n6. Equation (1) defines $G^{(n)}$ with $V_\\phi(s_{t+n})$ but does not clarify whether this is a soft or standard value; later (Sec. 4.3) it states the critic estimates the standard (non-soft) action-value. The notation should reflect this distinction.\n7. All benchmarks are simulated continuous-control tasks; the paper does not evaluate discrete action spaces, stronger partial observability (beyond action history), or real-robot settings that motivate the approach."}, "questions": {"value": "1. Can T-SAC be evaluated on discrete-action domains (e.g., Atari) or more strongly partially observable settings?\n2. Do the benefits of sequence-conditioned critics carry over to image-based ML1 or partially observed Box-Pushing variants?\n3. How does T-SAC compare with Transformer-based policies and offline RL approaches (e.g., Decision Transformer)?\n4. For GRU/LSTM critics, how extensive was the hyperparameter and architecture search (depth, hidden size, normalization, teacher forcing, sequence length)?\n5. Have you tested robustness under observation/action noise, stochastic terminations, or mixed-policy replay buffers? This seems particularly relevant given the “off-policy without IS” choice.\n6. What aspects of the design (e.g., window sampling, bootstrap lag, prefix length) most limit performance in sparse-reward settings, and could targeted modifications (e.g., auxiliary returns, adaptive horizons) mitigate the observed instability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "43HWg2dfRy", "forum": "rb5eTktqbc", "replyto": "rb5eTktqbc", "signatures": ["ICLR.cc/2026/Conference/Submission526/Reviewer_qLQQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission526/Reviewer_qLQQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission526/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762181756820, "cdate": 1762181756820, "tmdate": 1762915538450, "mdate": 1762915538450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}