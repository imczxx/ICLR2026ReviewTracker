{"id": "URPXhnWdBF", "number": 1963, "cdate": 1756972101788, "mdate": 1759898176336, "content": {"title": "AC-Foley: Reference-Audio-Guided Video-to-Audio Synthesis with Acoustic Transfer", "abstract": "Existing video-to-audio (V2A) generation methods predominantly rely on text prompts alongside visual information to synthesize audio. However, two critical bottlenecks persist: semantic granularity gaps in training data (e.g., conflating acoustically distinct sounds like different dog barks under coarse labels), and textual ambiguity in describing microacoustic features (e.g., \"metallic clang\" failing to distinguish impact transients and resonance decay). These bottlenecks make it difficult to perform fine-grained sound synthesis using text-controlled modes. To address these limitations, we propose **AC-Foley**, an audio-conditioned V2A model that directly leverages reference audio to achieve precise and fine-grained control over generated sounds. This approach enables: fine-grained sound synthesis (e.g., footsteps with distinct timbres on wood, marble, or gravel), timbre transfer (e.g., transforming a violin’s melody into the bright, piercing tone of a suona), zero-shot generation of sounds (e.g., creating unique weapon sound effects without training on firearm datasets) and better audio quality. By directly conditioning on audio signals, our approach bypasses the semantic ambiguities of text descriptions while enabling precise manipulation of acoustic attributes. Empirically, AC-Foley achieves state-of-the-art performance for Foley generation when conditioned on reference audio, while remaining competitive with SOTA video-to-audio methods even without audio conditioning.", "tldr": "", "keywords": ["Video-to-Audio Generation; Audio Generation;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/246c253e082fe8b0281fd1c3c9f7a81618b0c2d5.pdf", "supplementary_material": "/attachment/cca5a64941a4d6734b41175e6c53de50c7b689d2.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose AC-Foley, which directly addresses the expressive limitations that arise when using only text or audio semantic information, enabling fine-grained audio control. During training, the model adopts both approaches: conditioning on parts of the target audio and conditioning on segments that do not overlap with the target audio. By structuring this in a two-stage process, the model is explicitly trained to learn both aspects. This approach ensures that the generated audio is well-synchronized while avoiding overfitting to the conditioned audio, allowing for better generalization. Trained in this manner, the resulting multimodal transformer, AC-Foley, can generate long audio sequences of up to 8 seconds and demonstrates superior performance compared to other video-to-audio models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experiments are diverse. They convincingly demonstrate the validity of AC-Foley through a variety of baseline methods and a wide range of metrics, and the ablation study shows that the methodology in the paper is rigorously designed.\n\n2. The authors clearly pointed out the limitations of existing audio synthesis methods that rely solely on text or semantic features, and their idea of directly conditioning on acoustic features is particularly commendable.\n\n3. The two-stage curation in training to ensure that the model effectively learns the newly introduced method of “direct audio conditioning” is meaningful. Based on the experimental results, it appears to be an efficient training strategy that allows the multimodal transformer to adequately understand this new form of conditioning. With the authors’ approach, the model becomes both generalizable and aligned with the conditioning audio."}, "weaknesses": {"value": "1. It is not sufficiently explained whether using overlapping or non-overlapping audio from the same video during training. It seems unlikely that the model would learn the ability to appropriately modify the conditioned audio to match the video. The paper does not clearly explain the basis for generating semantically aligned audio when conditioned on audio from a different video. In particular, for the timbre transfer shown in Figure 1, while the output is temporally aligned, it is difficult to judge whether it is semantically aligned.\n\n2. Reduced complexity of generated audio is another limitation of this work. Since the model is trained using conditioned audio, as the authors themselves note in the limitations, the complexity of the audio the model can generate is likely constrained and cannot easily exceed that of the conditioned audio."}, "questions": {"value": "Please explain how the model can generate semantically well-aligned audio even when conditioned on a random audio input. (Related to Weaknesses No.1)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "aKfR6cClY3", "forum": "URPXhnWdBF", "replyto": "URPXhnWdBF", "signatures": ["ICLR.cc/2026/Conference/Submission1963/Reviewer_jtQU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1963/Reviewer_jtQU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761476662385, "cdate": 1761476662385, "tmdate": 1762915977494, "mdate": 1762915977494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AC-Foley, a framework for video-to-audio synthesis. The model introduces direct reference audio conditioning to bypass the semantic ambiguity of language and the lack of acoustic granularity in training datasets, aiming to provide fine-grained control over the characteristics of the synthesized sound. The framework is built upon a multimodal transformer and utilizes a two-stage training strategy to ensure that the generated sounds maintain acoustic fidelity to the reference while accurately adapting to the temporal structure and context of the video. When conditioning on reference audio, AC-Foley achieves state-of-the-art performance across metrics measuring distribution matching, semantic alignment, and spectral fidelity. Furthermore, the framework remains highly competitive with other advanced video-to-audio methods even when the audio condition is removed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper unlocks some interesting applications, such as fine-grained sound synthesis (e.g., varying footsteps based on surface material), timbre transfer (applying one sound's tone to a different visual event), and at the same time the framework remains competitive, matching or closely approaching existing state-of-the-art performance in standard video-to-audio tasks.\n- The method supports variable-length audio conditioning and does not require the reference audio and generated audio to have identical durations, unlike prior approaches.\n- Overall, the paper is written well and easy to follow."}, "weaknesses": {"value": "- Overall the task itself is not too challenging and therefore the technical contribution is relatively limited. Most components are reused from previous works, and adding additional audio control seems straightforward with the help of all state-of-the-art modules. The method relies on a multimodal conditioning vector that integrates information from video, text, and audio. This vector modulates the transformer input using adaLN layers, which is a common conditional technique and utilized in other generative models. The novelty lies in what is fed into the conditioning vector (i.e., acoustic features via VAE) rather than the conditioning mechanism itself.\n- As the authors mentioned, the proposed method may have trouble when dealing with complex sound environments. When the input videos and conditional audio contain multiple concurrent sound sources (such as overlapping dialogue, ambient noise, and object interactions), AC-Foley may struggle to precisely align specific sound elements with their corresponding visual triggers. Moreover, it does not seem straightforward to me that this challenge can be easily addressed by the proposed framework."}, "questions": {"value": "- What architectural or training modifications are being considered to improve the model's ability to isolate and precisely align specific sound elements to their corresponding visual triggers in complex auditory environments?\n- Can the authors detail which specific acoustic features (beyond general timbre, such as transient impacts, decay characteristics, or harmonic content) are most successfully encoded by the VAE latents, enabling the fine-grained control that text and semantic encoders like CLAP fail to capture?\n- The investigation into the ground truth DeSync mismatch (0.558s) suggested that both AC-Foley and baselines might be over-optimizing for the Synchformer metric, and that the metric's 4.8-second context window is inadequate for the 8-second video segments used. Have the authors explored alternative or complementary synchronization metrics that can accurately capture long-term temporal alignment across the full 8-second duration of the generated output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "j9b6KE9Oob", "forum": "URPXhnWdBF", "replyto": "URPXhnWdBF", "signatures": ["ICLR.cc/2026/Conference/Submission1963/Reviewer_nvKW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1963/Reviewer_nvKW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713156969, "cdate": 1761713156969, "tmdate": 1762915977304, "mdate": 1762915977304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed AC-Foley, a novel conditioned Foley audio generation dataset that supports both text and audio conditioning. The proposed method uses a two-stage training paradigm to force the flow matching model to learn the multi-modal condition fused with text, video, and conditional audio. The proposed method surpassed existing SoTA on multiple datasets under different settings. In the Human evaluation, AC-Foley shows an over 70% winning rate against the SoTA baseline."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The idea of two-stage training is very interesting and has proven to be effective. AC-Foley separates the learning of audio features and audio-visual alignment into two stages. Starting from an easier setting with conditions from the same clip, and then extending to a harder case with unseen conditioning. This paradigm is quite inspiring.\n\n2. This paper provides a rigorous evaluation against multiple baselines and different datasets. It not only outperforms in the audio-conditioned case, but also performs well on text-to-audio generation.\n\n3. The provided example video includes some very nice results; the quality of the generated audio is very impressive."}, "weaknesses": {"value": "1. The proposed method uses a multi-modal condition embedding in the flow-matching transformer. However, this multi-modal condition is fused from 3 different modalities (and one more time embedding), which naturally raises a concern about the quality of this conditional embedding. While the final results show that the model works well, the reviewer still wants to know what the intuition is behind this design, and what if we just use part of the conditional signals? \n\n2. While the human study shows a much better preference against MMAudio-L-V2 in terms of acoustic fidelity, this human study only uses 16 selected results from VGGSound. This is a very small evaluation set, and it may lead to some bias and weaken these results.\n\n3. During the evaluation, the paper only uses the last 2 seconds of the 10-second-long video, which could also introduce some bias. For example, the last few seconds may contain fewer actions than the previous ones, leading to a simplified task setting.\n\n4. While the quantitative results look fine overall, the last example on the Greatest Hits is not very good. The proposed method generates a scratching sound while the action in the video is actually hitting. This may indicate that the model may fail when the conditioning sample is dramatically different from its silent input video."}, "questions": {"value": "1. The paper uses Synchformer to measure the quality of audio-action alignment, which may not be the best choice (as mentioned in the paper, the Synchformer is biased by itself). Is it possible to use some other metrics to measure this alignment? For example, the onset comparison used in CondFoley, where one just needs to use the traditional onset detection method and compare against the GT audio.\n\n2. It would be great if there could be some more examples of failure cases, as this can help better understand the limitations of the proposed method."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uRSUvEg7Sa", "forum": "URPXhnWdBF", "replyto": "URPXhnWdBF", "signatures": ["ICLR.cc/2026/Conference/Submission1963/Reviewer_k8xt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1963/Reviewer_k8xt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878387330, "cdate": 1761878387330, "tmdate": 1762915977076, "mdate": 1762915977076, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes reference-audio–guided video-to-audio (V2A) generator that conditions generation on a short audio clip to produce synchronized Foley for a silent video. To achieve this, the authors use a shared conditioning vector built from (i) CLIP text/video embeddings, (ii) Synchformer sync features, (iii) and VAE latents of the reference audio. The model is trained in two stages: first stage uses an overlapping reference segment while uses a non-overlapping tail segment to force temporal adaptation."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The problem setup is novel and interesting. use reference audio indeed provides more precise control over the generated audio.\n- The qualitative results are solid and impressive and show well the advantage of the proposed method. \n- The paper is well-written\n- The method is simple"}, "weaknesses": {"value": "- The method is a bit less intuitive for me. The conditioning modalities use average pooling which collapses time but the temporal information seems to be well-preserved. Can the authors explain the reason for this? Have the author tried temporal representation for the temporal modalities (such as the video)\n- Missing discussion with multiple related work on V2A (e.g Rhythmic Foley, SSV2A, AV-LINK)"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jV7TbhIpyc", "forum": "URPXhnWdBF", "replyto": "URPXhnWdBF", "signatures": ["ICLR.cc/2026/Conference/Submission1963/Reviewer_8Hbv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1963/Reviewer_8Hbv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1963/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993642041, "cdate": 1761993642041, "tmdate": 1762915976894, "mdate": 1762915976894, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}