{"id": "35KC8ozdIO", "number": 24736, "cdate": 1758359839245, "mdate": 1759896751879, "content": {"title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws", "abstract": "Transformers deliver outstanding performance across a wide range of tasks and are now a dominant backbone architecture for large language models (LLMs). Their task-solving performance is improved by increasing parameter size, as shown in the recent studies on the Scaling Law for parameters. Although recent mechanistic-interpretability studies have deepened our understanding of internal behavior by analyzing the residual stream, the relationship between these internal mechanisms and the Scaling Law for parameters remains unclear. That hinders the efficient performance improvement of Transformers. To bridge this gap, we propose a theoretical framework that formulates the contribution of each layer, composed of modules such as the embedding layer, Multi-head Attention (MHA), and Multi-layer Perceptron (MLP) within the residual stream through a bias–diversity decomposition. The decomposition separates (i) bias, the error of each layer's output from the ground truth, and (ii) diversity, which indicates how much the outputs of each layer differ from each other. Analyzing Transformers under this framework reveals that performance improves when individual layers make predictions close to the correct answer and remain mutually diverse. We show that diversity becomes especially critical when individual layers' outputs are far from the ground truth. Furthermore, we introduce an information-theoretic treatment of bias and diversity and indicate that adding layers enhances performance only when those layers behave differently, i.e., are diverse. Moreover, we also reveal the performance gains from increasing the number of layers exhibit submodularity: marginal improvements diminish as additional layers increase, mirroring the logarithmic convergence predicted by the Scaling Law for parameters. Moreover, experiments on multiple semantic-understanding tasks with various LLMs empirically confirm the theoretical properties derived in this study. Our code will be available at https://github.com/[Anonymous].", "tldr": "We theoretically reveal the importance of diveristy in transformer layers for task-solving performance", "keywords": ["Transformer", "Residual Stream", "Logit Lens", "Scaling Law"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e9b06c63ae9632acd435ae3169b6beb89a0f9182.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work introduces a bias-diversity decomposition framework to theoretically analyze the contributions of individual layers (e.g., embedding, attention, MLP), aiming to explain some phenomena and outcomes of layer scaling."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": ">s1: Proposes a bias-diversity decomposition for analyzing layer-wise contributions in Transformer residual streams.\n\n>s2: The work integrates the bias-diversity decomposition with an information-theoretic lens.\n\n>s3: It has a theoretical and empirical analysis."}, "weaknesses": {"value": ">w1: **There is a critical point of confusion and error in the technical details. Equation (1) implies that the outputs of the embedding layer, all MHA layers, and all MLP layers are summed together directly. This is incorrect.** For the correct sequential structure, please refer to [1].\n>\n> [1] On layer normalization in the transformer architecture. ICML 2020.\n\n>w2: For Section 3.1. Omitting the softmax operation, while understandable for analytical convenience, requires explicit justification to ensure the theoretical rigor of the analysis.\n\n>w3: Line 135-137. **Theorem 1, derived from Equation 1, requires further validation (See w1).** The authors should provide additional supporting assumptions and evidence to substantiate its theoretical soundness.\n\n>w4: The formulation of Theorem 2 requires refinement for clarity. Please state its central claim and underlying rationale more explicitly.\n\n>w5: I have concerns regarding Theorem 4. The application of the information-theoretic chain rule appears incomplete, as the dependencies among the variables {$u_i$} are not adequately accounted for in the derivation, especially as manifested in Equation (13).\n\n>w6: **Line 250-252 \"These results seem to indicate that\". There are some gaps between the theory and the conclusions, requiring more details of the actual contributions. Instead of simply using expressions like \"seem\" to attempt an explanation.**\n\n>w7: Section 5. The authors used relatively simple tasks such as SST-2 and RTE for models like llama3-8B, and more complex tasks are needed to validate their conclusions. For instance, in Lora+ [2], Roberta-base can already achieve over 0.9 accuracy on the SST-2 task.\n\n>[2] LoRA+: Efficient Low Rank Adaptation of Large Models. ICML 2024."}, "questions": {"value": "See Weaknesses. The authors provided an invalid anonymous link."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "3oPcKbKuAm", "forum": "35KC8ozdIO", "replyto": "35KC8ozdIO", "signatures": ["ICLR.cc/2026/Conference/Submission24736/Reviewer_Ufo2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24736/Reviewer_Ufo2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760939338515, "cdate": 1760939338515, "tmdate": 1762943180587, "mdate": 1762943180587, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the role of layer-wise diversity in Transformers and its connection to parameter scaling laws. The authors propose a bias-diversity decomposition framework for the residual stream to theoretically and empirically analyze how the interplay between diversity and bias across layers and modules (e.g., attention, MLP) influences model performance. By introducing an information-theoretic perspective, they explain why increasing depth yields diminishing returns, aligning with the submodularity observed in scaling laws. Experiments on various LLMs and NLP benchmarks substantiate these theoretical findings."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. A key strength of this work is its novel theoretical framework for bias-diversity decomposition, which it tailors specifically to the Transformer's residual stream. While grounded in ensemble learning, the formulation (Theorem 1, Eq. 8) provides an original and principled lens to quantify the contributions of individual layers and modules (e.g., attention, MLP) in granular, interpretable terms.\n2. A particularly deep theoretical contribution is the reframing of diversity via information-theoretic measures (Eq. 13). This formalism allows the authors to establish a direct, causal link between layer stacking and submodularity, theoretically deriving—rather than just observing—the diminishing returns that characterize LLM scaling laws.\n3. The experiments robustly substantiate the theory across a wide range of models and tasks. The evidence is compelling and multi-level: heatmaps (Fig. 3) establish macro-level correlations, module graphs (Fig. 4) offer microscopic insights into component behavior, and ablation figures (Fig. 5-7) meticulously track the interplay of key variables, collectively demonstrating the theory's robustness and generality."}, "weaknesses": {"value": "1. The notation used throughout the paper is inconsistent and poorly standardized. Many symbols are not clearly defined upon their first appearance. The authors should provide a unified and clear definition for all symbols used in the theorems and mathematical formulae.\n2. The results in Figures 3 and 5 do not strongly support the authors' viewpoint. In Figure 3, there is no clear correlation shown between bias, diversity, and accuracy, while the strong correlation between MSE and accuracy is not surprising. As for Figure 5, the changes in model performance across multiple tasks do not show a clear pattern.\n3. Across all tasks shown in Figure 5, regardless of how performance changes, diversity consistently demonstrates a similar trend: it first remains unchanged and then suddenly increases. Moreover, in the few effective task groups, the rise in diversity clearly lags behind the decrease in bias. This phenomenon indicates that the authors' view on the trade-off between bias and diversity does not necessarily hold true."}, "questions": {"value": "see weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HZHsBHArNi", "forum": "35KC8ozdIO", "replyto": "35KC8ozdIO", "signatures": ["ICLR.cc/2026/Conference/Submission24736/Reviewer_2UE7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24736/Reviewer_2UE7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760952874234, "cdate": 1760952874234, "tmdate": 1762943180324, "mdate": 1762943180324, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how the diversity among Transformer layers influences model performance and connects it to parameter scaling laws. Using a bias–diversity decomposition framework, the authors show that optimal performance arises when layers produce outputs that are both accurate (low bias) and mutually different (high diversity). The study also demonstrates that adding layers improves performance only when they contribute diverse behaviors, and that such gains diminish submodularly, consistent with scaling laws. Experiments on various LLMs and NLP tasks empirically confirm that diversity is a key factor driving performance improvements."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces an original bias–diversity decomposition framework to connect internal Transformer dynamics with scaling laws, offering theoretical insight.\n\nThe work presents some theorems to formalize key relationships: \n\n(1) the decomposition of prediction error into bias and diversity; \n\n(2) the trade-off between bias and diversity; \n\n(3) the submodularity of performance gains from adding layers \n\nComprehensive experiments across multiple LLMs (e.g., LLaMA, Phi, Mistral) and NLP benchmarks confirm the theoretical predictions"}, "weaknesses": {"value": "1. Theorems 5 and 6 indicate that increasing the number of model layers does not necessarily improve performance. This paper does not provide relevant experimental examples to illustrate this phenomenon. Both Conditional Redundancy and Redundancy increase monotonically, but their difference may still increase or decrease monotonically.\n\n2. Theorem 7 assumes that when U_i are independent, the improvement of model performance is submodular. This paper doesn’t explain the rationality of the independence assumption for U_i. In practice, U_i should be correlated.\n\n3.What conclusions can be drawn from Figure 3? The correlations between Diversity, Bias and accuracy vary significantly across tasks, which is inconsistent with the previous theoretical analysis (decomposition of Diversity and Bias).\n\n4.The trade-off between Diversity and Bias means that reducing bias leads to a decrease in Diversity. In Figure 5, does \"no performance improvement\" refer to the ARC dataset, while \"performance improvement\" refers to the AG News dataset? The paper needs to provide more detailed explanations of this. Additionally, Section 5.4 mentions that bias does not tend to zero in practice, and this point also requires more detailed elaboration. Furthermore, Section 5.4 doesn’t explain when the Trade-off will fail and the reasons for such failure.\n\n\n5.The experiments in Figure 6 are overly simple. Specifically, as the number of layers increases, performance first increases and then decreases, instead of ” the effectiveness of increasing layers actually decreases”. Additionally, more elaboration is required on why the conclusion that Conditional Redundancy is greater than 0 can be drawn."}, "questions": {"value": "See the weakness above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IPKLj5UqJZ", "forum": "35KC8ozdIO", "replyto": "35KC8ozdIO", "signatures": ["ICLR.cc/2026/Conference/Submission24736/Reviewer_Joen"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24736/Reviewer_Joen"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761790550744, "cdate": 1761790550744, "tmdate": 1762943180014, "mdate": 1762943180014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper bridges mechanistic interpretability and parameter scaling laws in pre-layer normalization Transformers by decomposing prediction error in the residual stream via a bias-diversity framework, where bias measures per-layer deviation from ground-truth logits and diversity captures inter-layer output differences, revealing that low bias + high diversity minimizes MSE"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a bias–diversity decomposition framework to analyze Transformer layers, offering a novel theoretical lens linking internal mechanisms to scaling laws.\n2. Bridges mechanistic interpretability and empirical scaling-law studies, a connection rarely explored.\n3. Strong theoretical grounding with multiple theorems supported by rigorous proofs.\n4. Empirical validation across diverse LLM families and NLP benchmarks strengthens claims.\n5. The paper is clearly structured, with smooth transitions between theoretical and empirical sections."}, "weaknesses": {"value": "1. While the theory links diversity to performance, concrete architectural or training recommendations (e.g., diversity regularization or pruning criteria) are missing. Demonstrating such interventions could make the work more actionable.\n2. Experiments focus only on NLP benchmarks. Testing on multimodal or vision Transformers could demonstrate broader applicability and confirm whether the diversity–scaling relationship generalizes beyond text models.\n3. Although the work aims to connect interpretability and scaling laws, the link remains largely conceptual. More explicit layer-wise interpretability analyses (e.g., probing or attribution) could better ground the theoretical findings in observable model behavior.\n4. the link of the code is invalid."}, "questions": {"value": "1. Could the authors elaborate on how their information-theoretic diversity measure relates to previously used diversity notions in ensemble learning or representation learning (e.g., orthogonality, mutual information between features)?\n2. The theory assumes that the number of layers is proportional to parameter count and that layers are independent given the output label. How sensitive are the results to violations of these assumptions, for example, in shared-layer or MoE architectures?\n3. Could the authors discuss potential architectural or training interventions (e.g., promoting inter-layer diversity or penalizing redundancy) that might emerge from this framework? Would such interventions alter scaling behavior in practice?\n4. is the same bias–diversity scaling relationship expected to hold in vision or multimodal Transformers, where layer specialization and redundancy differ? Any preliminary evidence or reasoning would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fp9Rz7ObDt", "forum": "35KC8ozdIO", "replyto": "35KC8ozdIO", "signatures": ["ICLR.cc/2026/Conference/Submission24736/Reviewer_Q2CT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24736/Reviewer_Q2CT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24736/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808714414, "cdate": 1761808714414, "tmdate": 1762943179719, "mdate": 1762943179719, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}