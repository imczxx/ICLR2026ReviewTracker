{"id": "xCatVTpI4K", "number": 5679, "cdate": 1757927038138, "mdate": 1763750371155, "content": {"title": "Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization", "abstract": "Learning conditional distributions $\\pi^\\star(\\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \\sim \\pi^\\star$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development of *semi-supervised* models that utilize both limited paired data and additional unpaired i.i.d. samples $x \\sim \\pi^\\star_x$ and $y \\sim \\pi^\\star_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired data **seamlessly** using data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish an **end-to-end** learning algorithm to get $\\pi^\\star(\\cdot|x)$. In addition, we derive the universal approximation property, demonstrating that our approach can theoretically recover true conditional distributions with arbitrarily small error. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously.", "tldr": "", "keywords": ["semi-supervised domain translation", "likelihood maximization", "inverse entropic optimal transport"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf256acad91393b0ff66dae59e7443a271e53210.pdf", "supplementary_material": "/attachment/74233800408e2982a82870104a3e6e06bd1efd0d.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a new method, based on optimal transport, for semi-supervised learning using inverse entropic optimal transport. In a nutshell, the authors consider the problem of learning a conditional distribution $\\pi(\\cdot|x)$ given a paired set $\\set{x_i, y_i}$ and two unpaired sets $\\set{y_j}$ and $\\set{x_k}$. This corresponds to the semi-supervised learning case, where one has a smaller set of paired, supervised set, and two sets of unsupervised unpaired data. The authors then draw a relationship between the problem of learning $\\pi(\\cdot|x)$ and inverse OT. Inverse OT, on its own, is a sub-problem within OT where, instead of finding the OT plan, one finds the ground-cost. The authors demonstrate that their method beat other baselines in an toy example and a real case scenario of weather prediction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "__S1.__ I think the authors do a good job in linking the inverse EOT problem with the semi-supervised domain translation objective.\n\n__S2.__ The use of energy based models is also insightful, and nicely decouples the learning terms involving paired and unpaired data.\n\n__S3.__ I also think the authors do a nice job in devising a practical algorithm for optimizing equation 13."}, "weaknesses": {"value": "__Weakness 1 (Incremental Novelty).__ While the paper is well motivated, its main contribution seems incremental over __(Mokrov et al., 2024)__. For instance, looking at Algorithm 1 in the main paper, the only difference with respect Algorithm 1 of __(Mokrov et al., 2024)__ is the loss function. Other aspects of this submission, such as,\n\n1. The usage of the Gibbs-Boltzmann parametrization, and,\n2. The energy function $E(\\cdot|x)$\n\nare the same as in the aforementioned paper. __As a consequence, this submission does not meet the novelty criterion for publication at ICLR__.\n\n__Weakness 2 (Limited Experiments).__ The main paper only contains 2 experiments: a toy example, and a weather prediction task. In terms of scale, the second problem is very limited, as it contains only 692 samples. __As a consequence of this remark, this submission does not meet the significance bar for publication at ICLR.__\n\n- __Side Note.__ Given the similarity with __(Mokrov et al., 2024)__ I think a comparison with their method is warranted.\n\n__Weakness 3 (Too general title, too restrictive setting).__ The title of this paper claims that inverse EOT solves semi-supervised learning. However, upon reading the paper, the semi-supervised setting the authors are referring to is actually semi-supervised domain translation. I think this is an important distinction that must be made in the title of the paper if the authors don't actually experiment with general semi-supervised learning.\n\n# References\n\n__(Mokrov et al., 2024)__ \"Energy-guided entropic neural optimal transport.\" arXiv preprint arXiv:2304.06094 (2023)."}, "questions": {"value": "From the overall discussion of the authors method, I feel it could be applied to semi-supervised learning in general (e.g., in the classificaiton/regression settings). Is that the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eMT8ZQu3Hv", "forum": "xCatVTpI4K", "replyto": "xCatVTpI4K", "signatures": ["ICLR.cc/2026/Conference/Submission5679/Reviewer_RGyB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5679/Reviewer_RGyB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761060195179, "cdate": 1761060195179, "tmdate": 1762918190092, "mdate": 1762918190092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The aim of the paper is to learn an unknown conditional distribution in a semi-supervised manner,\nwhere both paired and unpaired training samples from the joint and marginal distributions are available.\nThis is a well-studied problem in the literature,\nfor which there exist several numerical algorithms.\nThe goal of the paper is to address this problem in a novel manner\nby establishing a connection to the so-called inverse entropic optimal transport.\nThis is done using specific models and parametrizations\nof the unknown conditional distribution, which finally is modeled as Gaussian mixture.\nThe derived algorithm exploits the connection to optimal transport and employs efficient methods from this field.\nThe presented approach is compared to other methods in two numerical experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Overall,\n  the theoretical part of the paper is well-developed and nicely written.\n  The problem is well explained and motivated.\n  The related literature and algorithms are comprehensively reviewed,\n  embedding the paper and its approach in the broader fields of machine learning and optimal transport.\n The employed model of the unknown conditional distribution\n  and the relation to inverse entropic optimal transport,\n  which is one of the main contributions,\n  is well presented.\n  Besides the brief calculations in the main text,\n  the detailed rearrangements are worked out in the appendix,\n  making the paper also accessible for non-expert readers.\n The presented relation between semi-supervised domain translation\n  and inverse entropic optimal transport is\n   interesting."}, "weaknesses": {"value": "- Without Appendix C.3 and D.1,   the experimental illustrations in §5 are extremely hard to follow.\n  Since the information in these appendices is essential,\n  they should be briefly included in the main text\n  to make §5 self-contained. \n- The first example (§5.1) deals with the approximation\n  of an synthetic conditional distribution.\n  At first glance,   it seems that the goal is to estimate optimal transport plan,\n  which in fact is not entirely true.\n  The construction of the *ground truth* should be more highlighted,\n  especially why the conditional distribution spread out\n  and that the plan $\\pi^*$ is not an optimal one.\n  This would ease the interpretation of the results.\n- The second example (§5.2) deals with real-world weather data.\n  Although based on real-world data,\n  the experiment seems to be highly synthetic.\n  More details regarding the dataset,\n  like the considered measurement locations (local or world-wide),\n  as well as a motivation of the preprocessing step are missing.\n  This information however could be helpful to understand\n  the aim of the example and the relation to actual applications.\n- Some references may be added concerning \nconditional generative models like Hagemann et al. ICLR 2024 or\nArdizzone et al., 2019 ...\nand\nconcerning inverse entropic OT and related metric learning like \nHuizing, Cantini and Peyr\\'e, ICLR 2022,\nAuffenberg et al., Unsupervised Ground Metric Learning, 2025.\n- My main concern is the improvement of the numerical presentation."}, "questions": {"value": "- How exactly the log-likelihood values in Table 1 can be interpreted?\nMore precisely,\nhow are these values are used\nto evaluate the quality of the estimated conditional distribution\nand why are they interesting?\nHow does Table 1 looks like if the CFD,\nwhich is easier to interpret,\nis used instead?\n- To improve the presentation of the first numerical illustration: could\n  the ground truth and the employed data be moved to\n  a separate figure that\n  is presented before the results?\n  At present   data and results are mixed\n  which reduces readability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "none"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dh2VjC9VP0", "forum": "xCatVTpI4K", "replyto": "xCatVTpI4K", "signatures": ["ICLR.cc/2026/Conference/Submission5679/Reviewer_pfyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5679/Reviewer_pfyh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761504608989, "cdate": 1761504608989, "tmdate": 1762918189732, "mdate": 1762918189732, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a semi-supervised algorithm using inversed OT. The author showed that the formulation is related to the likelihood maximization of an energy-based model. The universal approximation property is derived and some numerical experiments are conducted."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper showed that the entropy-regularized inverse OT problem can be formulated as a likelihood maximization problem of an energy-based model. This is an interesting result. The universal approximation property is derived, showing the soundness of the method."}, "weaknesses": {"value": "1. One limitation is that this formulation requires that the marginal of the paired data also follows $\\pi_x$ and $\\pi_y$. For example, if the paired data is artificially selected, i.e., they do not follow $\\pi_x$ and $\\pi_y$, then the method no longer works: the first term in Eqn (18) is no longer an approximation of the first term in Eqn (13). I suggest making this clearer in the paper.\n\n2. Clearness: There are too many bold, italic, underlined words throughout the paper, even in the abstract. Many unimportant words like \"sequence of\", \"proofs\", \"seamlessly\" and \"extended discussion\" are underlined. Such emphasis adds no novelty to the paper and makes it rather difficult to read. My suggestion is to reduce the use of them.  I also suggest refraining from using the objective words like \"fancy\".\n\n3. The author discussed the use of neural networks, which can be more useful in practice. However, this is hidden in the appendix. I suggest moving it to the main paper, and moving the discussion of Gaussian mixture models to the appendix.\n\n\n4. All experiments are in small scale. Some larger scale experiments like those in Gu et al., 2022 should be considered."}, "questions": {"value": "I do not have further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "khAXk6G1hI", "forum": "xCatVTpI4K", "replyto": "xCatVTpI4K", "signatures": ["ICLR.cc/2026/Conference/Submission5679/Reviewer_kC9e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5679/Reviewer_kC9e"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5679/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567875062, "cdate": 1761567875062, "tmdate": 1762918189468, "mdate": 1762918189468, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response and Revision"}, "comment": {"value": "We sincerely thank the reviewers for their valuable feedback and for recognizing several strengths of our work: the clarity and thoroughness of the manuscript **(pfyh, RGyB)**, our demonstration that the entropy-regularized inverse OT problem can be formulated as a likelihood maximization of an energy-based model for semi-supervised learning **(kC9e, pfyh, RGyB)**, and our universal approximation theorem **(kC9e)**.\n\nIn response to the reviewers’ suggestions, we made the following changes, highlighted in orange in the revised manuscript:\n- **(kC9e)** Removed excessive use of bold, italic, and underlined words throughout the paper and abstract.\n- **(kC9e)** In Appendix B.3, added the case where the paired samples are artificially constrained.\n- **(pfyh)** In Appendix B.5, extended the discussion of related work on metric learning and updated the descriptions of the two main experiments.   \n- **(kC9e, pfyh)** Moved the experiment on the Colored MNIST dataset to the main text. Due to space constraints, it was not previously possible.  \n\nWe believe these revisions have strengthened the manuscript and addressed the reviewers’ concerns.\n\nThank you once again for your constructive feedback. Detailed responses to each individual comment are provided below."}}, "id": "lNdpGDJawk", "forum": "xCatVTpI4K", "replyto": "xCatVTpI4K", "signatures": ["ICLR.cc/2026/Conference/Submission5679/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5679/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5679/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763750602377, "cdate": 1763750602377, "tmdate": 1763750602377, "mdate": 1763750602377, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}