{"id": "TfpcJt3LBV", "number": 17891, "cdate": 1758281660060, "mdate": 1759897147607, "content": {"title": "Quantifying Cross-Domain Knowledge Distillation in the Presence of Domain shift", "abstract": "Cross-domain knowledge distillation often suffers from domain shift. Although domain adaptation methods have shown strong empirical success in addressing this issue, their theoretical foundations remain underdeveloped. In this paper, we study knowledge distillation in a teacher–student framework for regularized linear regression and derive high-dimensional asymptotic excess risk for the student estimator, accounting for both covariate shift and model shift. This asymptotic analysis enables a precise characterization of the performance gain in cross-domain knowledge distillation. Our results demonstrate that, even under substantial shifts between the source and target domains, it remains feasible to identify an imitation parameter for which the student model outperforms the student-only baseline. Moreover, we show that the student's generalization performance exhibits the double descent phenomenon.", "tldr": "We theoretically quantify the potential performance gain from cross-domain knowledge distillation.", "keywords": ["knowledge distillation;domain adaption; generalization error; random matrix"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/283457b299496b77952160cff2ac3e536b7f5244.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper builds upon high-dimensional random matrix theory to analytically study cross-domain knowledge distillation (KD) in teacher–student linear regression settings under both covariate and model shifts. It derives asymptotic expressions for the excess risk through bias–variance decomposition, covering both regularized (ridge) and unregularized (ridgeless) regimes with deterministic and random parameters. The analysis includes an exploration of the imitation parameter $\\xi$, showing that the optimal $\\xi$ may lie outside the interval [0,1] (even negative), and discusses the conditions under which double descent arises."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear, testable theoretical claims: The paper presents three main results that express the excess risk of cross-domain KD as explicit matrix functions involving Stieltjes transforms. The derivations are mathematically sound and well-grounded in the theory of random matrices.\n2. Beyond single-domain baselines: The work demonstrates the existence of an imitation coefficient $\\xi$ that allows the student model to outperform pure supervised learning (Proposition 1).\n3. Unified explanation of observed phenomena: The analysis connects double descent behavior in KD with the interplay among $\\xi$, teacher/student regularization, and the dimensional ratio, offering a coherent theoretical interpretation."}, "weaknesses": {"value": "1. Limited practical applicability beyond linear models: All results are derived for linear regression; implications for nonlinear or deep KD remain speculative. A discussion or small-scale experiment validating the theoretical predictions in nonlinear settings would strengthen the paper.\n2. Empirical validation is light: The experiments are limited to synthetic data, verifying asymptotic formulas.\n3. Strong assumptions and limited robustness analysis: Theoretical results rely on independence, bounded moments, and concentration assumptions."}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "euhJDXpIMb", "forum": "TfpcJt3LBV", "replyto": "TfpcJt3LBV", "signatures": ["ICLR.cc/2026/Conference/Submission17891/Reviewer_rJMR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17891/Reviewer_rJMR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795557367, "cdate": 1761795557367, "tmdate": 1762927714650, "mdate": 1762927714650, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper analyzes cross-domain KD for linear regression with ridge (and ridgeless) estimators. A source-domain teacher (\\beta_t) is trained on ((X_1,y_1)) and used to supervise a target-domain student on ((X_2,y_2)) via the imitation parameter (\\xi) in the objective (L(\\xi)=\\xi \\ell(y_2^t,y_2^s)+(1-\\xi)\\ell(y_2,y_2^s)). Closed-form resolvent-based expressions yield high-dimensional (HD) bias–variance formulas for the excess target risk (ER(\\beta_s)) under covariate shift ((\\Sigma_1\\neq\\Sigma_2)) and model shift ((\\beta_1\\neq \\beta_2)). The paper also treats random-(\\beta) and an under-parameterized ridgeless regime. Key theorems (1–3) give deterministic equivalents that expose dependence on (\\Sigma_1,\\Sigma_2,\\beta_1,\\beta_2), and on (\\xi), and show conditions where the student beats the student-only baseline; (\\xi) can even be negative (“anti-learning”)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. In the under-parametrized, ridgeless setting you show the student is a convex combination of the two OLS estimators and give a closed-form (ER) with optimal (\\xi\\in(0,1)). This is intuitive and useful. \n **Allowing (\\xi\\in\\mathbb{R}).** You justify that negative (\\xi) can be optimal (isotropic corollary), and you prove existence of a strictly better (\\xi) than the student-only baseline under natural conditions.  \n\n2. The paper formalizes (ER=\\text{Bias}+\\text{Var}) and then supplies deterministic equivalents for both parts under deterministic ((\\beta_1,\\beta_2)) and random (\\beta) models (Theorems 1–2)."}, "weaknesses": {"value": "1. Because both squared losses share the same quadratic term in (\\beta), the Hessian of (L(\\xi)) is (N_2^{-1}X_2X_2^\\top+\\lambda_s I), **independent of (\\xi)**; hence the problem is convex and well-posed for any real (\\xi). It would help readers to add a short lemma right after (1) and your closed form for (\\beta_s), making the “negative (\\xi) is still safe” point explicit. \n\n2. Expressions like Theorem 1’s Bias/Var have (o_{\\text{a.s.}}(1)) terms while the main terms scale with traces ((\\Theta(M)) in isotropic cases). Please specify whether your (o_{\\text{a.s.}}(1)) is *absolute* or *per-dimension*, and consider normalizing (ER/M) in statements to make asymptotic orders unambiguous. \n\n3.  Assumption 1(a) currently asks for all moments; you note it can be relaxed. Give a concrete bound ((4+\\epsilon) or (8) moments) sufficient for Lemma 6 / local laws used later, so readers know what’s truly required. \n\n4. Theorem 1 already shows dependence on (\\Sigma_1,\\Sigma_2) via (\\Pi_1,\\Pi_2,S_i(\\cdot)). It would help to rewrite one or two key trace terms (e.g., ( \\mathrm{Tr},[\\Pi_1 \\Pi_2 \\Sigma_2])) in the eigen-bases of (\\Sigma_1,\\Sigma_2), or to use an overlap matrix to highlight principal-angle effects. You partly do this in App. B.6 (eq. (28)); elevating a compact “eigenvector-overlap” corollary to the main text would greatly aid intuition.  \n\n5. You note (and Proposition 1 leverages) that (ER(\\beta_s)) is a convex quadratic in (\\xi). Please collect coefficients (A,B,C) in closed form (from Theorems 1–2) and give a short table for common regimes (isotropic; shared (\\beta); pure model shift). This immediately yields (\\xi^\\star!=!-B/2A) and clarifies when (\\xi^\\star<0) or (\\xi^\\star>1) without case-by-case reasoning.   \n\n6. Your analysis assumes/advocates Bayes-consistent teacher probabilities and studies how teacher quality controls the SGD variance term. Ye et al.[1] propose BCDE, a teacher-training objective based on conditional mutual information explicitly aimed at estimating the Bayes conditional distribution for KD. It’s a natural methodological precursor/neighbor to your “Bayesian teacher” prescription and directly relevant to your noise model and guidelines.\n\n[1] Ye, Linfeng, et al. “Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information.” ICLR 2024 (Twelfth)."}, "questions": {"value": "see the weakness above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RqgbBHAVuo", "forum": "TfpcJt3LBV", "replyto": "TfpcJt3LBV", "signatures": ["ICLR.cc/2026/Conference/Submission17891/Reviewer_aTNu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17891/Reviewer_aTNu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972066271, "cdate": 1761972066271, "tmdate": 1762927714015, "mdate": 1762927714015, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies cross-domain knowledge distillation with domain shift through a teacher-student framework. Authors leverage Random Matrix Theory and present a theoretical analysis in the context of linear regression, comprising a deterministic-parameter setting where the teacher and student parameters are non-random and a random-parameter setting where a shared parameter vector is drawn from prior distributions. This paper discovers that the knowledge distillation still works even under substantial domain discrepancies. Authors also observe a double-descent phenomenon in the knowledge distillation process."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The overall theoretical analysis is clear and well-organized. Authors use tools from Random Matrix Theory and derive precise, high-dimensional asymptotic expressions for the excess risk. The analysis is mathematically sound.\n- Interesting phenomenon that knowledge distillation is still possible when the source domain and target domain share substantial domain discrepancies. The Anti-Learning discovery is also insightful, which points out that the best imitation parameter $\\xi$ is not limited to the [0, 1] range."}, "weaknesses": {"value": "- All the derivations in this paper only work for linear regression. In the real world, complex, non-linear models like deep neural networks are much more popular. Therefore, it's unknown whether these insights remain applicable in practice and how much they can provide guidance for Knowledge Distillation.\n- Lack of Quantification for the extent of the Substantial Shift. The paper claims efficacy even under substantial domain discrepancies, but the degree of \"substantial\" is not well quantified. The analysis shows that an optimal $\\xi$ exists such that $\\text{ER}(\\beta_s) < \\text{ER}_0$, but how does this performance gain degrade as a function of the domain shift? A potential weakness is that the paper does not introduce an explicit metric to quantify domain shift. The discrepancy between domains is only implicitly reflected through covariance geometry.\n- The finding that $\\xi < 0$ (anti-learning) can be optimal (Corollary 2) is intriguing but seems rather uncommon in practical scenarios. Does this imply that the teacher performs so poorly that the student benefits from learning the opposite of the guidance? A more in-depth discussion on the underlying reasons for this phenomenon and its practical implications would further strengthen the paper."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "68Qm3K8bOH", "forum": "TfpcJt3LBV", "replyto": "TfpcJt3LBV", "signatures": ["ICLR.cc/2026/Conference/Submission17891/Reviewer_AThT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17891/Reviewer_AThT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762479358166, "cdate": 1762479358166, "tmdate": 1762927713658, "mdate": 1762927713658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a theoretical framework for \\textbf{cross-domain knowledge distillation (KD)} under both \\emph{covariate} and \\emph{model shifts} in a teacher--student ridge regression setting. \nUsing tools from \\textbf{random matrix theory}, it derives high-dimensional asymptotic expressions for the student's excess risk via bias--variance decomposition. \nThe analysis shows that even under substantial domain shift, there exists an optimal imitation parameter $\\xi$ such that the student model outperforms the student-only baseline, and the generalization risk exhibits a clear \\textbf{double descent} behavior."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a rigorous theoretical analysis of cross-domain knowledge distillation (KD) using random matrix theory, deriving precise high-dimensional asymptotic characterizations that extend previous student-only or fixed-ξ formulations. It analytically demonstrates that an appropriately chosen imitation parameter ξenables the student model to outperform the baseline even under significant domain shifts. Moreover, simulation results closely align with the theoretical predictions, validating the framework and revealing a clear double-descent behavior in the student’s excess risk."}, "weaknesses": {"value": "1. The theoretical analysis is limited to linear ridge regression.   \n2. Relies on bounded spectral norms, independence between domains, and high-moment conditions, potentially unrealistic for real-world KD settings.  \n3. The ridgeless regression analysis only covers the under-parameterized case (M<N_1,N_2).  \n4. The dependence of the optimal imitation parameter ξ and the interaction between λ_tand λ_s lacks intuitive or empirical guidance.   \n5. Experiments are entirely synthetic, with no demonstrations on real KD applications (e.g., vision or language models).  \n\nMinors:\n1. Theorem 1 and related derivations are notation-heavy; adding a concise notation summary table would improve readability."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xiYnEUY48c", "forum": "TfpcJt3LBV", "replyto": "TfpcJt3LBV", "signatures": ["ICLR.cc/2026/Conference/Submission17891/Reviewer_VYeN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17891/Reviewer_VYeN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17891/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762791167561, "cdate": 1762791167561, "tmdate": 1762927713356, "mdate": 1762927713356, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}