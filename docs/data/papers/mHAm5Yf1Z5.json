{"id": "mHAm5Yf1Z5", "number": 6077, "cdate": 1757952283118, "mdate": 1763015144287, "content": {"title": "Trust the Process? Backdoor Attack against Vision–Language Models with Chain-of-Thought Reasoning", "abstract": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in multimodal understanding, with the integration of Chain-of-Thought (CoT) further enhancing their reasoning abilities. By generating a step-by-step thought process, CoT significantly enhances user trust in the model's outputs. However, we contend that CoT also poses serious security risks as it can be exploited by attackers to execute far more covert backdoor attacks, a threat that remains unexplored by prior work. In this paper, we present the first systematic investigation into the vulnerability of the CoT process in VLMs to backdoor attacks. We introduce **ReWire**, a novel and stealthy backdoor attack that leverages data poisoning to hijack the model's reasoning process. Unlike typical label attacks, ReWire initially generates a correct and plausible reasoning chain consistent with the visual input. Subsequently, it injects a predefined ``pivot statement\" that stealthily redirects the reasoning path toward a malicious, attacker-specified conclusion. We conduct extensive experiments on several mainstream open-source VLMs across four distinct datasets, demonstrating that ReWire uniformly achieves an attack success rate of over 97\\%. Furthermore, the attack stealth has been fully validated, as the malicious CoT it generates accurately reflects the image's visual content (fidelity), is presented in fluent, natural language (coherence), and forms a logically sound, albeit manipulated, progression to the final malicious answer (consistency). Our findings uncover a critical new security risk in VLM reasoning systems and underscore the urgent need to develop more robust defense mechanisms.", "tldr": "We introduce ReWire, the first backdoor attack specifically designed to hijack the reasoning process (Chain-of-Thought) in Vision Language Models (VLMs).", "keywords": ["backdoor attack", "vision-language model", "chain-of-thought"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/dd78dc073c98fe2014122607e17e3a310732e845.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies backdoor attack against vision-language models with chain-of-thought reasoning, a thread unexplored by prior work. The paper presents the first systematic investigation into the vulnerability of the CoT process in VLMs to backdoor attacks. Then, a novel and stealthy backdoor attack method ReWire is introduced that injects 'pivot statement' that redirects the logic towards the malicious conclusion. Experiments conducted on four tasks and multiple SOTA VLMs demonstrate its effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper studies the important topic of backdoor attack on VLMs. The safeness of models have drawn increasing attention due to wide usage of VLMs.\n- Different from previous works, this paper studies the vulnerability of the CoT reasoning process, which is a new perspective and could inspire further research.\n- The proposed ReWire method shows effective yet stealthy attack. The authors conducted comprehensive experiments on several VLMs and different datasets. Overall, the results are extensive and presentations are clear. Many ablation studies are provided to validate the effectiveness of different components.\n- In addition to attack method, defenses are also studied. The proposed method shows robustness on complex tasks."}, "weaknesses": {"value": "- The proposed attack uses a fixed set of CoT trigger generated with GPT-5. They are not connected to the previous reasoning, thus could be less stealthy if defensen methods can identify such common patterns."}, "questions": {"value": "- For the tasks evaluated in the paper, the answer seems to be a single value. I wonder how the proposed attack method works if the answer has a more complex structure, e.g., a sequence of actions to finish some task. Is there a flexibility that the CoT trigger can lead to a partial failure of the answer as well as a complete failure? It may not be results on particular datasets, but some discussions could be fine.\n- For triggers listed in A.3.4, they seem to directly specify the misleading target answer. Will the attack still be effective with other format of triggers if we do not allow to specify such answers in the trigger? This pattern may be used for identify poison data with defense methods, thus reducing its stealthy."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v7KmrzAQq6", "forum": "mHAm5Yf1Z5", "replyto": "mHAm5Yf1Z5", "signatures": ["ICLR.cc/2026/Conference/Submission6077/Reviewer_bXJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6077/Reviewer_bXJ9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761013126133, "cdate": 1761013126133, "tmdate": 1762918449806, "mdate": 1762918449806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "UGjxi8vCMS", "forum": "mHAm5Yf1Z5", "replyto": "mHAm5Yf1Z5", "signatures": ["ICLR.cc/2026/Conference/Submission6077/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6077/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763015143391, "cdate": 1763015143391, "tmdate": 1763015143391, "mdate": 1763015143391, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the security vulnerabilities of Chain-of-Thought (CoT) reasoning in Vision-Language Models (VLMs). The authors propose ReWire, a novel data-poisoning backdoor attack that hijacks the CoT reasoning path. Unlike conventional label-based attacks, ReWire first generates a plausible and visually faithful reasoning process before inserting a short “pivot statement” that redirects logic to an attacker-specified conclusion. Extensive experiments across four datasets (captcha, CLEVR, A-OKVQA, ScienceQA) and multiple VLMs (Qwen2.5-VL, LLaVA-OneVision, Janus-Pro, InternVL2.5) demonstrate high attack success (>97%) with minimal degradation in clean accuracy. The paper highlights that reasoning-level backdoors are both potent and stealthy, warranting urgent attention to defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The paper formulate a novel problem, CoT-targeted backdoor attacks in VLMs."}, "weaknesses": {"value": "1. While the paper explores backdoor attacks on the CoT reasoning process in VLMs, it does not sufficiently situate itself within the existing literature. Recent work such as Badchain[1] has already examined CoT-targeted attacks in large language models, and methodologies like Chain-of-Scrutiny[2] have been developed for detecting such attacks. The authors do not discuss or compare against these efforts, which limits the perceived novelty of their contribution. Given the presence of existing attack mechanisms and detection frameworks focused on CoT, simply introducing another attack may be insufficient unless compelling new insights or empirical advancements are demonstrated.\n\n[1] Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, and Bo Li. 2024.Badchain: Backdoor chain-of-thought prompting for large language models. CoRR, abs/2401.12242.\n\n[2] Li, X., Mao, R., Zhang, Y., Lou, R., Wu, C., & Wang, J. (2025). Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models. Findings of the Association for Computational Linguistics: ACL 2025.\n\n2. Necessity. While the paper presents a fresh perspective by focusing on attacking the CoT reasoning process, a direction that distinguishes it from prior work. It does not provide sufficient evidence to establish the necessity of specifically targeting CoT. My concern is that the authors primarily support their claims of enhanced stealth through human evaluation, arguing that CoT-based attacks are harder to detect by annotators. However, detecting traditional output-based backdoor attacks is already known to be highly challenging for humans, especially given the volume and complexity of contemporary vision–language model outputs.  **A crucial question remains unaddressed: Is attacking the CoT reasoning chain inherently harder to model or detect using existing automated backdoor detection methods, compared to other attacks?** The paper does not present empirical results on how current detection algorithms perform against ReWire or similar CoT-based attacks. According to Table 2, both label attacks and naive CoT interventions display even higher and more consistent attack success rates (ASR) than the proposed method, which raises questions about the necessity and advantage of targeting the reasoning chain specifically."}, "questions": {"value": "To strengthen the argument, I recommend that the authors incorporate comparative analyses utilizing established automated backdoor detection techniques, such as Chain-of-Scrutiny[2]. Demonstrating whether CoT-based attacks present fundamentally different or greater modeling challenges compared to existing attack paradigms would provide valuable empirical support. Without such evidence, the practical significance and urgency of the proposed threat vector remain difficult to assess."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6lQ4SDKRIj", "forum": "mHAm5Yf1Z5", "replyto": "mHAm5Yf1Z5", "signatures": ["ICLR.cc/2026/Conference/Submission6077/Reviewer_PQid"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6077/Reviewer_PQid"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761601507394, "cdate": 1761601507394, "tmdate": 1762918448855, "mdate": 1762918448855, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a stealthy backdoor attack that targets the Chain-of-Thought (CoT) reasoning process in vision-language models. It works by poisoning training data so that the model first produces a correct reasoning chain before inserting a hidden pivot statement that subtly redirects the reasoning toward a malicious conclusion. The attack achieves high success rate while remaining highly stealthy, revealing a security risk in multimodal reasoning systems."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method works in black box setting, whereas prior work assumes white or grey box access to the vision language models.\n2. The paper is well written, and the method is clearly explained."}, "weaknesses": {"value": "1. It is unclear whether the setting is realistic. This method requires constructing specific texts and requires the user (model training) to use the same texts for training. However, in practice, the model trainer will not probably use the texts online, and will only scrape the images. Maybe the authors can clarify how realistic the attack is.\n2. The attack is not really stealthy. The paper claims that the users will stop reading because CoT is too long, overlooking the pivot. However, in the example in figure 1, the problematic part is actually right before the answer, where the users can obvious notice."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "10OlwRMiA7", "forum": "mHAm5Yf1Z5", "replyto": "mHAm5Yf1Z5", "signatures": ["ICLR.cc/2026/Conference/Submission6077/Reviewer_GS4s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6077/Reviewer_GS4s"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761714671192, "cdate": 1761714671192, "tmdate": 1762918447616, "mdate": 1762918447616, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies backdoor attacks under the process supervision setting, where LLMs are trained not only on final answers but also intermediate reasoning traces. The authors show that backdoors can be injected into the reasoning process itself, enabling malicious outputs when a specific trigger appears while preserving benign behavior otherwise. The attack leverages trigger-dependent “detour reasoning paths” and masked process-labeling to make the backdoor latent during clean inference. Experiments on math reasoning datasets (e.g., GSM8K) using process-supervised LLMs (e.g., Qwen-7B/14B and Llama-2-Chat) demonstrate high attack success rates with low accuracy drop on clean prompts. The work highlights a previously unstudied threat where backdoors target chain-of-thought and policy traces, not only outcomes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel threat model: exploring process-level backdoors beyond standard output-label poisoning is timely given adoption of chain-of-thought supervised training.\n2.The method design—trigger-conditioned reasoning detour paths, masking for stealth—is intuitive and grounded in process learning frameworks.\n3.Experiments on multiple LLMs and reasoning datasets show comparable performance (not best though) and modest clean-accuracy degradation."}, "weaknesses": {"value": "1. The impact of rational r is unclear. In clean model, if we add a contradict rational r, the final output may also be wrong. This can be a normal hallucination. The author should provide evidence that the attack is not because of r by comparing with adding contradict rational r under clean settings. \n2. Evaluation focuses solely on math reasoning- output a number; assessing tasks requiring symbolic reasoning or text-based tasks would strengthen generality claims.\n3. Only simple finetune defense is provided. what about the input-based defenses (e.g., blur or smooth the image trigger), or other recent defenses."}, "questions": {"value": "1. The impact of rational r is unclear. In clean model, if we add a contradict rational r, the final output may also be wrong. This can be a normal hallucination. The author should provide evidence that the attack is not because of r by comparing with adding rational r under clean settings. \n2. What is the impact of reasoning step M?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zkc5DCQmIK", "forum": "mHAm5Yf1Z5", "replyto": "mHAm5Yf1Z5", "signatures": ["ICLR.cc/2026/Conference/Submission6077/Reviewer_9AEJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6077/Reviewer_9AEJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6077/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965924205, "cdate": 1761965924205, "tmdate": 1762918446621, "mdate": 1762918446621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}