{"id": "5E2iFvO6bH", "number": 11110, "cdate": 1758189465310, "mdate": 1759897607902, "content": {"title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for Long-Context LLMs Inference", "abstract": "Recent advances in large language models (LLMs) have showcased exceptional performance in long-context tasks, while facing significant inference efficiency challenges with limited GPU memory. Existing solutions first proposed the sliding-window approach to accumulate a set of historical \\textbf{key-value} (KV) pairs for reuse, then further improvements selectively retain its subsets at each step. However, due to the sparse attention distribution across a long context, it is hard to identify and recall relevant KV pairs, as the attention is distracted by massive candidate pairs. Additionally, we found it promising to select representative tokens as \\pq in each sliding window to accurately represent the entire context, an approach that has been overlooked in the pursuit of effective KV cache eviction. Thus, we propose \\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that dynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the relevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a token-level indicator, Activation Bias, within each context window, enabling the proper construction of \\pq for retrieval at pre-filling stage. \nTo accurately recall the relevant KV pairs and minimize the irrelevant ones, we design a dynamic KV cut-off mechanism guided by information density across layers at the decoding stage. Experiments on the Long-Bench and $\\infty$ Benchmarks demonstrate its state-of-the-art performance with competitive inference quality and resource efficiency. \nOur source code is available at \\href{https://anonymous.4open.science/r/ActQKV-DDE1}{\\textnormal{https://anonymous.4open.science/r/ActQKV-DDE1}}.", "tldr": "", "keywords": ["dense retrieval", "KV retrieval", "KV cache eviction", "long context reasoning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0ae3438fe3fd726abe52a870e2a225020124a361.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ActQKV, a training-free framework for efficient key-value (KV) retrieval in long-context LLM inference. It proposes an activation-aware probe-query (APQ) that emphasizes highly activated “anchor” tokens to better represent window-level context, and a dynamic KV cut-off mechanism (DCM) that adaptively allocates KV budgets across layers based on information density. Experiments on LongBench and ∞ Bench show consistent improvements over InfLLM, QLLM, and TokenSelect, achieving up to 10% accuracy gain with a 16× KV reduction. While slightly increasing latency, ActQKV offers a simple and effective way to enhance long-context reasoning and retrieval efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces an activation-aware probe-query that leverages token-level activation bias to better capture semantic anchors within each window. The KV cut-off mechanism also adaptively allocates KV budgets across layers using information entropy, taking the sensitivity of different layers into consideration.\n2. The proposed method is training-free and generalizable, easy to apply to multiple LLMs.\n3. The method outperforms InfLLM, QLLM, and TokenSelect on existing long-context accuracy benchmarks under only 2K KV budget. The paper also includes detailed ablation and visualization showing how activation-aware selection better preserves models' performance."}, "weaknesses": {"value": "1. The activation-bias computation and dynamic entropy-based cut-off introduce significant latency (1.6~1.9× slower than InfLLM), limiting real-time applicability.\n2. The paper does not include comparisons with more recent retrieval-augmented or memory-optimized models (e.g., Quest, H2O). \n3. While the paper introduces an “activation-aware probe-query,” the overall framework (importance-based KV retrieval) is conceptually close to existing attention sparsification and adaptive KV compression works (e.g., Ada-KV, PyramidKV, etc.)."}, "questions": {"value": "1. Is there a way to amortize or cache the activation bias computation across windows to reduce latency?\n2. How is the overall latency (Table 10) distributed between different parts of ActQKV -- such as probe construction and dynamic KV recall?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "661mFJyCCc", "forum": "5E2iFvO6bH", "replyto": "5E2iFvO6bH", "signatures": ["ICLR.cc/2026/Conference/Submission11110/Reviewer_8isy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11110/Reviewer_8isy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761180114959, "cdate": 1761180114959, "tmdate": 1762922286768, "mdate": 1762922286768, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ActQKV, a training-free KV-cache retrieval method for long-context LLMs. It builds an activation-aware probe-Query to identify salient tokens and uses per-layer entropy to adaptively allocate KV budgets during decoding. On LongBench and infinite-Bench, ActQKV achieves up to 16x KV reduction and improves accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Weighting Query vectors based on activation deviation is an interesting, training-free mechanism to surface semantically salient tokens within the current sliding window.\n2. Adaptive KV allocation across layers, provide a more principled alternative to fixed top-k selection.\n3. The inspection of cosine similarity distributions and perplexity behavior gives helpful insights into how retrieval behavior changes."}, "weaknesses": {"value": "1. Computational overhead analysis in decoding, although the paper claims negligible overhead, no throughput or latency results are reported. This is critical in production inference.\n2. Adaptive KV cache selection per layer leads to non-uniform attention shapes, which can: degrade kernel efficiency, complicate padding/masking, and reduce batching utilization. The paper does not evaluate real-world inference throughput under batched conditions.\n3. When KV caching interacts with CPU/DRAM paging, retrieval cost would dominate, the scheduler should be discussed."}, "questions": {"value": "1. Does activation bias correlate with attention distributions? Is there redundancy? Most training-free long context inference methods are based on attention sparsity. \n2. How much average KV budget is actually allocated across layers during decoding? Whether the selected k is related the the depth of layer? Such as, early layers prefer larger k?\n3. Can the authors report batch inference efficiency under typical serving batch sizes? Are attention kernels padded to the max? What is the GPU efficiency impact?\n4. Does probe-Query computed in head-level or layer-level? For head-level, whether all the heads show the ability to detect important KV cache."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wIHyFEtO05", "forum": "5E2iFvO6bH", "replyto": "5E2iFvO6bH", "signatures": ["ICLR.cc/2026/Conference/Submission11110/Reviewer_GGF3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11110/Reviewer_GGF3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925343284, "cdate": 1761925343284, "tmdate": 1762922286194, "mdate": 1762922286194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The idea of detecting “anchor” tokens via activation bias is interesting and addresses an important gap: many retrieval-methods treat all tokens uniformly. \nFocusing the Probe-Query on tokens that deviate from the window mean gives a sharper context representation.\n\nHowever, the paper doesn’t fully explore how retrieval behaves when the output is very long rather than short. Their experiments use long input / short output tasks, so applicability to long generation needs more validation.\nAlso, their notion of “sliding window” is loosely framed: in practice the windows appear to be blocks rather than fine-grained sliding strides, which may reduce continuity between windows and affect anchor detection near boundaries.\n\nThe proposed method relies on similarity matching between probe and key vectors, but don’t deeply discuss vector normalization vs dot-product subtleties, since Q×K in attention isn’t exactly cosine similarity, assumptions here may affect retrieval quality in practice.\n\nIn summary, the paper provides an easy to follow step in long context inference, especially in the matching stage of KV cache retrieval. However, it would be more convincing if the method were evaluated with reasoning models on long reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Focuses on finding truly important tokens in long context, which gives a sharper and more useful probe for retrieval.\n2. Training-free and easy to plug into existing long-context systems.\n3. Shows notable accuracy gains under tight KV budgets.\n4. Simple mechanism (activation bias) yet practical and effective.\n5. Dynamic layer-wise KV budgeting is thoughtful and avoids wasting cache where it matters less."}, "weaknesses": {"value": "1. Evaluations focus on long-input/short-output cases, so behavior in long-generation settings is unclear.\n2. Windowing feels more like block windows rather than true sliding, which may lose continuity near boundaries.\n3. Assumes dot-product similarity behaves like cosine without deeply addressing normalization or magnitude effects.\n4. No latency/throughput reporting, so real deployment efficiency remains uncertain."}, "questions": {"value": "1. Do you truly observe anchor tokens staying stable when outputs are long, or does the signal weaken in long-generation cases?\n2. Have you compared token-level activation signals versus chunk-level aggregation? In real systems chunk-level often aligns better with KV grouping.\n3. Since Q·K is not cosine, did you explicitly normalize vectors for retrieval, or rely on raw dot-product similarity?\n4. Does activation bias sometimes highlight stylistic or formatting spikes rather than semantic anchors?\n5. How does the method behave during very long decoding sequences where the active context evolves continuously?\n6. What is the latency and memory overhead for computing activation bias across windows at scale?\n7. Could head-specific signals improve anchor selection compared to layer-averaged bias?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WKJQGGQRzL", "forum": "5E2iFvO6bH", "replyto": "5E2iFvO6bH", "signatures": ["ICLR.cc/2026/Conference/Submission11110/Reviewer_z8d7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11110/Reviewer_z8d7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998433438, "cdate": 1761998433438, "tmdate": 1762922284490, "mdate": 1762922284490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ActQKV, a training-free method to improve KV cache retrieval for long-context LLMs. It claims two main contributions: (1) An 'Activation-aware Probe-Query' (APQ) that weights query vectors based on an 'Activation Bias' to (in theory) better represent the context window, and (2) A 'Dynamic KV Cut-off Mechanism' (DCM) that allocates KV budgets per layer based on information density during decoding. Experiments on Long-Bench and $\\infty$-Bench are presented to support these claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper addresses an important and practical problem: KV cache efficiency for long-context inference.\n* The authors are transparent about the method's latency overhead, which is reported in the appendix (Appendix D.5).\n* The writing is clear and the proposed mechanism is easy to follow."}, "weaknesses": {"value": "1.  **Missing Evaluation on Long-Decoding Scenarios:** The paper's entire evaluation is on benchmarks like Long-Bench, which are primarily \"long-prompt, short-answer\" tasks (e.g., document QA). This setup completely misses the main bottleneck of current LLMs: **long-decoding scenarios**. These are tasks like long-form chain-of-thought reasoning, where the prompt might be short, but the *generated content* becomes extremely long. As the model generates thousands of tokens, its *own* KV cache (from its *own* output) becomes the bottleneck. This method is never tested on this critical \"short-prompt, long-generation\" workload, making its practical utility highly questionable.\n2.  **Lack of Reasoning Task Evaluation:** Related to the first point, the method's impact on complex, multi-step reasoning is completely unknown. The authors should have added experiments on strong reasoning models (like DeepSeek-R1 or Qwen3) on reasoning-specific tasks to see if this retrieval method actually helps or hurts the quality of a long, generated chain of thoughts.\n3.  **Incremental Contribution:** The core ideas are not very original. Using an importance-weighted sum to create a query vector, or dynamically adjusting a budget based on layer-wise statistics, are both fairly straightforward extensions of existing work. The performance gains do not seem to justify the added complexity.\n4.  **Significant Latency Overhead:** The method, by the authors' own admission in Table 10, **slows things down considerably** (1.6-1.9x slower than the InfLLM baseline). This is a massive practical disadvantage. A method that nearly doubles inference time is a non-starter for most real-world applications, and the paper does not provide a strong enough justification for this trade-off."}, "questions": {"value": "1.  Why did you choose to only evaluate on long-prompt, short-answer tasks? The main bottleneck for many LLM applications is long-form *generation* (long decoding). Can you provide any data on how your method performs in a long chain-of-thought reasoning task, where the generated content far exceeds the prompt length?\n2.  Can you add experiments on reasoning-focused models, such as DeepSeek-R1, to demonstrate that your retrieval mechanism does not harm (and ideally, helps) the complex, step-by-step generation required for these tasks?\n3.  Given the 1.6-1.9x latency overhead, how do you justify the practical applicability of this method? This severe slowdown seems to hinder, not help, its application."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n6B58mfsiO", "forum": "5E2iFvO6bH", "replyto": "5E2iFvO6bH", "signatures": ["ICLR.cc/2026/Conference/Submission11110/Reviewer_HQVx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11110/Reviewer_HQVx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11110/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762143736011, "cdate": 1762143736011, "tmdate": 1762922284159, "mdate": 1762922284159, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}