{"id": "oIfTyGmks3", "number": 15904, "cdate": 1758256894414, "mdate": 1759897274123, "content": {"title": "ADAPTIVE PARAMETER TUNING FOR ROBUST CLUSTERING: A MULTI-CRITERIA OUTLIER DETECTION APPROACH", "abstract": "Robust clustering requires effective outlier detection mechanisms that can adapt\nto cluster-specific characteristics. We introduce an adaptive parameter tuning approach\nthat enhances traditional clustering with multi-criteria outlier detection\nand intelligent restart-based clustering quality optimization. Our method develops\ncluster-specific threshold models using adaptive scaling factors (α = 3.5σ), enabling\nautomatic parameter selection based on real-time performance monitoring.\nWe propose a multi-criteria validation framework requiring satisfaction of at least\n3 out of 4 criteria, potentially reducing false positive rates compared to singlecriteria\napproaches. The framework integrates Statistical Process Control (SPC)\nfor adaptive parameter optimization and intelligent restart-triggered silhouettebased\nk-refinement that automatically searches competitive k values (k-2 to k+3)\nwhen clustering quality is poor (silhouette < 0.25), enabling dynamic adjustment\nof outlier detection sensitivity while ensuring competitive clustering structure.\nExperimental evaluation on diverse datasets demonstrates that our approach\nachieves ultra-conservative outlier detection (0.36-0.43% outlier rates) with competitive\nprecision (17.9%) among tested outlier detection algorithms and low false\npositive rate (1.8%), while maintaining competitive clustering quality (silhouette\nscores 0.573-0.781) and computational efficiency (18.6 seconds for 3,700 points),\nmaking it suitable for practical clustering applications.", "tldr": "Adaptive clustering with smart outlier detection that auto-tunes, self-corrects, and delivers robust clusters with minimal false alarms.", "keywords": ["outlier detection", "clustering algorithms", "statistical process control", "multi-criteria validation", "robust clustering", "adaptive parameter tuning", "K-means clustering", "representation learning", "anomaly detection"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3887986434bececcbe376f44b87a55b6014b65a6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an adaptive parameter tuning framework for robust clustering that integrates multi-criteria outlier detection with real-time optimization via SPC. The method introduces cluster-specific distance thresholds scaled by an adaptive factor, which is dynamically adjusted based on monitored outlier rates using SPC control charts."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "•\tThe paper addresses a well-known challenge in clustering—sensitivity to outliers—and proposes a system that combines statistical rigor (via SPC) with heuristic robustness, which could be useful in real-world applications requiring conservative anomaly handling.  \n•\tThe integration of multiple components—cluster-specific thresholds, multi-criteria validation, SPC-based adaptation, and k-refinement—demonstrates thoughtful system design. The use of silhouette score as a trigger for automatic re-clustering is a pragmatic heuristic.  \n•\tThe authors evaluate their method on diverse datasets and report metrics including precision, false positive rate, silhouette scores, and runtime, offering a reasonably comprehensive empirical profile."}, "weaknesses": {"value": "•\tWhile the system integrates several existing ideas, the core contributions appear incremental. The use of SPC for parameter adaptation in unsupervised learning is not unprecedented (e.g., SPC has been applied in online learning and monitoring contexts), and the multi-criteria outlier rule resembles ensemble or voting-based anomaly detection schemes. The paper does not sufficiently differentiate itself from prior work in adaptive clustering or robust k-means variants (e.g., k-means-- or outlier-aware k-means).  \n•\tThe evaluation is restricted to small-scale, low-dimensional datasets (max 3,700 points, 22 features). There is no comparison against modern robust clustering baselines (e.g., robust k-means with trimming, deep clustering with outlier rejection, or recent adaptive DBSCAN variants).   \n•\tAlso, the reported precision (~18%) is low, and the paper acknowledges low recall—yet it does not discuss the practical implications of missing >85% of true outliers in safety-critical settings.  \n•\tThe paper uses terms like “ultra-conservative” without formal definition. The pseudocode omits critical details such as how centroids are merged or how the global threshold τ_global is computed."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ucd7ug3nU9", "forum": "oIfTyGmks3", "replyto": "oIfTyGmks3", "signatures": ["ICLR.cc/2026/Conference/Submission15904/Reviewer_ihi2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15904/Reviewer_ihi2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716855016, "cdate": 1761716855016, "tmdate": 1762926122918, "mdate": 1762926122918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel method for robust clustering that integrates adaptive parameter tuning, multi-criteria outlier detection, and Statistical Process Control (SPC) for real-time optimization. The goal is to improve clustering quality by reducing false positives in the outlier detection subroutine without heavy manual parameter tuning."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "Robust clustering and outlier detection are both problems of practical interests."}, "weaknesses": {"value": "1. The paper reads more like a technical report than a research paper. The problem statement is not clearly introduced, there is no explicit motivation for the proposed approach, and the paper provides limited conceptual insight. Although the introduction begins by discussing the challenge of robust clustering, the methodology and experiments primarily focus on outlier detection. This shift in focus, without clear justification, makes the paper’s main objective confusing and inconsistent. Moreover, while the method is formally presented as Adaptive Multi-Criteria Outlier Detection in Algorithm 1, it is later referred to as Ultra-Conservative Outlier Detection (UCOD) in the experiments, adding further inconsistency and ambiguity.\n\n2. The motivation for emphasizing “conservative” outlier detection—specifically minimizing false positives—is unclear and poorly justified. The paper does not explain why this property is important, nor does it provide concrete examples or practical applications where such conservatism would be desirable. Including real-world scenarios or use cases could greatly strengthen the motivation.\nThe experimental evaluation is inadequate. Only three datasets are used, two of which are employed solely for ablation studies of the proposed approach, while only one dataset is used for comparison against baseline methods. Moreover, the baselines themselves are outdated, limited to four outlier detection algorithms published before 2008. If the paper’s focus is indeed on outlier detection, it should include comparisons with more recent, state-of-the-art methods (see, e.g., a compilation of the state-of-the-art OD methods [1]). Alternatively, if the focus is robust clustering, the paper should clearly define what “robust” means and compare against the state-of-the-art clustering algorithms (see, e.g., a compilation in [2]).\n\n3. Furthermore, the proposed approach appears highly heuristic and relies on numerous hard-coded hyperparameters with little or no justification. Many thresholds and constants—such as defining “small clusters” as those with fewer than 10 samples, doubling global thresholds for small clusters (Eq. 6), applying a 1.5× safety margin (Eq. 7), setting initial $\\alpha=3.5$ (Eq. 5), updating it every five iterations, or fixing arbitrary ratios such as 4.0 and 1.8 in Eqs. (11) and (12)—are introduced without explanation. Similarly, a series of parameters from Eq. (17) to Eq. (25) are defined with opaque numerical choices. Without theoretical reasoning, empirical tuning details, or sensitivity analysis, these ad hoc parameter settings undermine the method’s soundness and reproducibility.\n\n----\n\n[1] https://github.com/yzhao062/pyod\n\n[2] https://github.com/collinleiber/ClustPy\n\n----\n\n4. In addition to the points above, the presentation of the paper is confusing and poorly structured in several respects. \n\n* The introduction is extremely brief, comprising only two short paragraphs, and provides insufficient background for readers to understand the problem context, the key challenges, or how the proposed approach is intended to address them. For example, it is unclear what “single-criteria validation results” in the first paragraph means. Similarly, the statement about “cluster-specific threshold models using adaptive scaling factors” in the list of contributions lacks explanation or motivation, making it difficult to grasp what the proposed approach actually entails.\n\n* Beyond the lack of conceptual framing, the notation and definitions are inconsistent and sometimes incorrect. Several variables and terms appear without prior definition—for instance, $\\mu_j$ in Eq. (2) and the global threshold $\\tau_{global}$ in Eq. (6) are introduced without explanation. The notation $C$ is used inconsistently: it represents clusters in Eq. (2) but is later reused to denote criteria in Eqs. (9) -- (12), causing unnecessary confusion. Moreover, the silhouette score mentioned in Section 2.4 is never formally defined, which makes it difficult to understand how the algorithm determines or refines the number of clusters based on that metric.\n\n* The presentation of experimental results also suffers from similar ambiguity. In Table 1, the column labeled “Decision” is not explained, making it hard to understand the results in section 3.2."}, "questions": {"value": "At a high level, what specific problem or challenge is the proposed algorithm designed to solve? What is the key intuition or underlying rationale behind its design? Furthermore, what concrete advantages does the proposed algorithm offer over existing state-of-the-art methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vZ44MJBCen", "forum": "oIfTyGmks3", "replyto": "oIfTyGmks3", "signatures": ["ICLR.cc/2026/Conference/Submission15904/Reviewer_nZ9M"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15904/Reviewer_nZ9M"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964020068, "cdate": 1761964020068, "tmdate": 1762926121884, "mdate": 1762926121884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper assumes a data environment containing outliers and proposes an iterative process in which an outlier score is computed for each instance. Four outlier criteria are defined, and any instance satisfying at least three of them is regarded as an outlier and removed. To prevent excessive elimination, the method incorporates the concept of Statistical Process Control (SPC) by adjusting a conservative scaling factor, alpha. In addition, the algorithm automatically determines the number of clusters based on the Silhouette score. In summary, this study proposes a clustering method that simultaneously detects outliers and automatically determines the optimal number of clusters."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Automatic adjustment of parameters."}, "weaknesses": {"value": "- The literature review is completely missing, so it is unclear how far existing research has progressed, what limitations prior studies have, and why this particular study is necessary.\n- The claimed contributions in the Introduction cannot be verified as genuine contributions due to the lack of prior research context.\n- The proposed ideas are not novel, determining outliers per cluster, using multiple outlier criteria, refining centroids after excluding outliers, and selecting the number of clusters based on internal validation indices are all well-established techniques.\n- The only slightly new aspect, the SPC concept for parameter tuning, is not particularly interesting.\n- The writing quality is poor, with awkward citation formatting (missing parentheses). In the abstract, alpha and sigma appear without prior definition, making them difficult to understand. The term “false positive rate” appears abruptly in the Introduction, and only later is it clear that it refers to that for outlier detection.\n- The proposed method is filled with ad-hoc and heuristic components lacking any theoretical justification. The method relies entirely on empirical settings and parameters, with no clear theoretical foundation.\n- The experimental evaluation is based on only a few datasets, which is insufficient to demonstrate the generalizability or robustness of the proposed method."}, "questions": {"value": "- Why are alpha=3.5 and gamma=0.03 used as default values?\n- Why was the Silhouette score chosen as the internal validation index among many other possible indices?\n- Why are three outlier criteria required for satisfaction, not two, and not all four?\n- Why is a cluster with 10 instances considered sufficient?\n- Why were the following numerical values selected without justification: 2.0 in Eq. (6), 1.5 in Eq. (7), Q99 in Eq. (10), 4.0 in Eq. (11), 1.8 in Eq. (12), w=30 in Eq. (17), alpha_spc=0.05 in Eq. (18), and target_rate=0.002 in Eq. (19)? There are several such constants whose rationale is unclear.\n- What are the meanings of the horizontal and vertical axes in Figure 3?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "m8P0XWr2kA", "forum": "oIfTyGmks3", "replyto": "oIfTyGmks3", "signatures": ["ICLR.cc/2026/Conference/Submission15904/Reviewer_7Q2f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15904/Reviewer_7Q2f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989980690, "cdate": 1761989980690, "tmdate": 1762926121275, "mdate": 1762926121275, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an \\textit{Adaptive Parameter Tuning approach for Robust Clustering} integrated with multi-criteria outlier detection, aiming to address limitations of traditional clustering (e.g., global thresholds ignoring cluster-specific characteristics, high false positive rates from single-criteria validation, and heavy manual tuning). \n\n### Core Contributions:\n\n1. **Cluster-Specific Threshold Models**: Derive thresholds via adaptive scaling factors (\\(\\tau_j = \\bar{d}_j + \\alpha\\sigma_j\\), default \\(\\alpha=3.5\\)) tuned by Statistical Process Control (SPC).\n\n2. **Multi-Criteria Validation Framework**: Requires satisfying at least 3 out of 4 criteria (e.g., distance exceeding cluster-specific threshold, 99th percentile of cluster) to reduce false positives.\n\n3. **SPC-Based Real-Time Adaptation**: Monitors outlier detection performance (rolling average of outlier rates) and adjusts \\(\\alpha\\) dynamically (e.g., \\(\\alpha_{\\text{new}} = \\alpha_{\\text{old}} - 0.2\\) if outlier rate exceeds UCL).\n\n4. **Outlier-Aware Centroid Refinement**: Excludes confirmed outliers when updating centroids to avoid distortion.\n\n5. **Silhouette-Based K-Refinement**: Triggers k-search (from \\(\\max(2, k-2)\\) to \\(k+3\\)) if silhouette score < 0.25, selecting the optimal k with the highest score.\n\n### Experimental Results:\n- Achieves ultra-conservative outlier detection  with 1.8\\% false positive rate (FPR) and 17.9\\% precision on the Protein Function Hierarchy dataset.\n- Maintains competitive clustering quality (silhouette scores 0.573–0.781) and efficiency (18.6s for 3,700 points on the Varying Density Clusters dataset).\n- Outperforms baselines (DBSCAN, Isolation Forest, LOF, One-Class SVM) in FPR while maintaining moderate precision."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "### 1. Innovative Integration of Cross-Disciplinary Ideas\nThe paper bridges **Statistical Process Control (SPC)** (from industrial engineering) with clustering and outlier detection, a non-trivial combination that addresses the long-standing issue of \"static parameters\" in traditional methods. For example, SPC dynamically adjusts \\(\\alpha\\) based on real-time outlier rates (Equation 21: \\(rollingavg_t = \\frac{1}{\\min(t,w)}\\sum_{i=\\max(1,t-w+1)}^t outlier\\_rate_i\\)), ensuring parameters adapt to data distribution changes—an improvement over static thresholds in DBSCAN [Ester et al., 1996] and LOF [Breunig et al., 2000].\n\n### 2. Rigorous Mathematical Formulation and Reproducibility\nAll core mechanisms are supported by detailed math:\n- Cluster-specific threshold: \\(\\tau_j = \\max(\\bar{d}_j + \\alpha\\sigma_j, 1.5\\times\\tau_{\\text{global}})\\) (Equation 7) ensures conservatism for large clusters.\n- Multi-criteria outlier definition: \\(Outlier(x_i) = \\text{True if } \\sum_{c=1}^4 \\mathbb{I}(Criterion_c(x_i)) \\geq 3\\) (Equation 43) explicitly quantifies outlier classification.\n- Experimental settings (e.g., \\(\\alpha_{\\text{min}}=0.1\\), \\(\\alpha_{\\text{max}}=20.0\\), window size \\(w=30\\)) are fully reported, enabling reproducibility.\n\n### 3.  Well-Structured Methodology and Experimental Narrative\nThe pseudo-code (Algorithm 1) clearly outlines the end-to-end workflow (initialization → clustering → outlier detection → SPC tuning → k-refinement). Experimental results are presented in a logical progression: SPC evolution (Figure 1) → k-refinement (Table 1, Figure 3) → baseline comparison (Table 2), making it easy to follow how each component contributes to overall performance.\n\n### 4.  Practical Relevance for Ultra-Conservative Scenarios\nThe ultra-conservative design addresses a critical unmet need in domains where false positives are costly (e.g., protein function annotation, where misclassifying normal proteins as outliers could mislead biological research). Compared to Isolation Forest  and One-Class SVM (Table 2), the method’s lower FPR makes it more suitable for such high-stakes applications."}, "weaknesses": {"value": "### 1. Limited Dataset Diversity and Lack of High-Dimensional Validation\n All experiments use low-dimensional data (2D for Varying Density Clusters, 8D for Protein Function Hierarchy, 22D for UCI Mushroom), while real-world clustering often involves high-dimensional data (e.g., 100+ features in image embeddings or tabular data). The method’s performance on high-dimensional data (where distance metrics degrade, \"curse of dimensionality\") is untested.\n\nEvaluate on high-dimensional public datasets (e.g., MNIST embeddings (784D), KDD Cup 99 (41D)) and report metrics like clustering quality (silhouette score) and computational efficiency. Additionally, discuss how the cluster-specific threshold model mitigates the curse of dimensionality (e.g., whether \\(\\alpha\\) needs larger values in high dimensions).\n\n### 2. Inadequate Ablation Studies for Key Parameters\n\n Critical parameters (e.g., \\(\\alpha_{\\text{init}}=3.5\\), SPC window size \\(w=30\\), small cluster threshold \\(\\tau_{\\text{small}}=2\\times\\tau_{\\text{global}}\\)) are set without justification. For example, why is the small cluster threshold 2× the global threshold instead of 1.8× or 2.2×? No ablation is provided to show how these parameters impact performance.\nConduct ablation studies:\n  - Vary \\(\\alpha_{\\text{init}}\\) (2.5, 3.5, 4.5) and report FPR, outlier rate, and silhouette score.\n  - Test \\(w=15, 30, 45\\) to show how window size affects SPC’s responsiveness to outlier rate changes.\n  - Compare \\(\\tau_{\\text{small}}=1.5\\times\\tau_{\\text{global}}, 2.0\\times\\tau_{\\text{global}}, 2.5\\times\\tau_{\\text{global}}\\) to validate the 2× choice.\n\n### 3. Lack of Comparison with State-of-the-Art (SOTA) Post-2020 Methods\nBaselines only include classic methods (DBSCAN, Isolation Forest, LOF, One-Class SVM) but omit recent SOTA adaptive clustering methods (e.g., Deep SVDD [Pang et al., 2019], Adaptive Density-Based Clustering [Zhang et al., 2023]). This makes it unclear how the proposed method performs against modern alternatives.\nAdd comparisons with SOTA methods:\n  - Deep SVDD (a popular deep anomaly detection method) on the Protein Function Hierarchy dataset.\n  - Adaptive DBSCAN (Zhang et al., 2023) on the Varying Density Clusters dataset (to highlight advantages of SPC over other adaptive mechanisms).\n  - Cite these works and explain performance differences (e.g., \"Our method achieves 1.8% FPR vs. 2.5% FPR of Adaptive DBSCAN due to multi-criteria validation\")."}, "questions": {"value": "### 1. Questions\n1. The initial value of \\(\\alpha\\) is set to 3.5 (Section 2.1). What empirical or theoretical basis supports this choice? Have you tested \\(\\alpha_{\\text{init}}=2.5\\) or 4.5, and if so, how did they affect outlier detection accuracy and FPR?\n2. The SPC window size \\(w=30\\) (Equation 17) is used for rolling average calculation. How does varying \\(w\\) (e.g., 15, 45) impact the responsiveness of parameter tuning? For example, does a smaller \\(w\\) lead to over-adaptation (frequent \\(\\alpha\\) changes) or a larger \\(w\\) lead to delayed responses to sudden outlier rate spikes?\n3. The method is tested on datasets with up to 22 features (UCI Mushroom). How does it perform on high-dimensional data (e.g., 100+ features)? Does the cluster-specific threshold model (\\(\\tau_j = \\bar{d}_j + \\alpha\\sigma_j\\)) need modification to handle the \"curse of dimensionality\" (where all pairwise distances become similar)?\n\n### 2. Suggestions\n1. Add a \"Related Work\" section to explicitly position the method against recent adaptive clustering and outlier detection works (e.g., Deep SVDD [Pang et al., 2019], Adaptive DBSCAN [Zhang et al., 2023]). This will clarify the paper’s novelty relative to SOTA.\n2. Include a runtime comparison with baselines on larger datasets (e.g., 10,000+ points). The current experiment uses 3,700–5,644 points; showing scalability will strengthen the method’s practical relevance.\n3. Provide a case study (e.g., on the Protein Function Hierarchy dataset) to visualize false positives/negatives of the proposed method vs. baselines. This will make the FPR advantage (1.8% vs. 2.1–2.3% in Table 2) more intuitive."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MRUGN7gF0M", "forum": "oIfTyGmks3", "replyto": "oIfTyGmks3", "signatures": ["ICLR.cc/2026/Conference/Submission15904/Reviewer_k4xf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15904/Reviewer_k4xf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994661472, "cdate": 1761994661472, "tmdate": 1762926117444, "mdate": 1762926117444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an adaptive outlier-aware clustering framework with automatic parameter tuning and restart-based optimization.  The paper addresses an interesting and relevant problem; however, the novelty of the proposed approach is not clearly demonstrated. The manuscript does not convincingly articulate what new ideas or methodological innovations it contributes beyond existing work. In addition, the experimental section lacks sufficient detail and rigor to convincingly support the claimed improvements. The dataset selection appears limited, and the experimental analysis does not adequately demonstrate generalization or provide meaningful insights into the performance behaviors of the method.\n\nFurthermore, the overall presentation requires improvement. The organization, clarity of explanation, and discussion of results need significant refinement. Several parts of the paper are difficult to follow, and the narrative does not clearly connect the proposed approach to the empirical findings. The experimental results and analysis are not sufficiently comprehensive or precise to meet ICLR standards.\n\nIn summary, while the topic has potential, the current version of the manuscript falls short in terms of novelty, presentation quality, experimental thoroughness, and clarity of contribution. Substantial revisions and strengthening of the method, experiments, and writing would be necessary before the paper can be considered for acceptance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The research problem and methodology are appealing and have the potential to contribute meaningfully to the field."}, "weaknesses": {"value": "1. Related work is insufficient: The literature review does not adequately cover prior studies, and the connection to existing research is limited.\n\n2. Clarity of presentation needs improvement: The problem formulation, key ideas, and methodology are not explained clearly, making it difficult to follow the technical contributions.\n\n3. Experimental setup is unclear: Details regarding datasets, experiment design, evaluation metrics, and implementation settings are not sufficiently described, affecting reproducibility.\n\n4. Results lack strong evidence: The experimental findings and conclusions are not fully convincing; additional empirical validation and deeper analysis are needed to demonstrate the method’s effectiveness."}, "questions": {"value": "The paper evaluates the proposed method exclusively on synthetic data but does not provide a clear justification for this choice. While synthetic datasets are valuable for controlled experimentation and isolating specific properties, relying solely on them limits the practical credibility and generalizability of the results. It is possible that I am unfamiliar with the dataset rationale; could the authors clarify why synthetic data was chosen instead of (or in addition to) real-world data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kcdfYWP1xK", "forum": "oIfTyGmks3", "replyto": "oIfTyGmks3", "signatures": ["ICLR.cc/2026/Conference/Submission15904/Reviewer_X9CX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15904/Reviewer_X9CX"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission15904/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042157784, "cdate": 1762042157784, "tmdate": 1762926117034, "mdate": 1762926117034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}