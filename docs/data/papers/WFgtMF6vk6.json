{"id": "WFgtMF6vk6", "number": 5772, "cdate": 1757934076553, "mdate": 1759897954885, "content": {"title": "TFMAudio: High-Fidelity Long-Form Text-to-Audio via Mamba-based Flow Matching", "abstract": "Recent advancements in audio generation have been dominated by transformer-based diffusion models, which face challenges in extrapolating positional encodings and exhibit quadratic complexity in self-attention, limiting their consistency and efficiency for long-form generation.\nTo address these limitations, we propose TFMAudio, a novel latent audio generation model that integrates the strengths of Flow Matching and a custom-designed TFMamba backbone.\nTFMamba employs a dual-scan mechanism: TimeMamba captures long-range causal dependencies with linear complexity, while FrequencyMamba models spectral correlations such as harmonic structures. To enhance stability, we further introduce Energy-Aware Guidance (EAG), which mitigates state drift by adaptively regularizing classifier-free guidance. Experiments demonstrate that TFMAudio achieves state-of-the-art performance on text-to-audio benchmarks and exhibits robust extrapolation to ultra-long sequences. Remarkably, our model generates 30-minute high-fidelity audio while preserving temporal consistency and semantic alignment, significantly advancing the scalability and usability of text-to-audio models.\n  Demo:https://huggingface.co/spaces/tfmaudio/TFMAudio", "tldr": "", "keywords": ["Text-to-Audio", "Flow-Matching", "Mamba", "SSM", "Diffusion"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/be70f7eba89c14fce4d09027c37eb308d3292d6a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes TFMAudio, a latent text-to-audio generator that replaces Transformer backbones with a Time–Frequency Mamba (TFMamba) block trained with flow matching, plus an Energy-Aware Adaptive Guidance (EAG) scheme to stabilize long generations."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Support ultra long audio generation. To my best knowledge, this is currently the TTA model that generates the longest duration."}, "weaknesses": {"value": "- Lacking too many well-known baseline models for 10s audio generation. For example, Make-An-Audio 2, EzAudio, AudioGen and Tango series (Tango, Tango-AF, Tango2). I understand the authors only want to compare with 44.1kHz or 48kHz models, but since they claim state-of-the-art, comparing more existing models will be more convincing.\n\n- AudioCap’s testing set has fewer than 1000 audio samples, each paired with 5 prompts. In the description of Section 5.1 for datasets, the authors mentioned that they evaluated their model on the AudioCaps testing set of 1,811 prompt-audio pairs. Thus, I think there are some audio ground truth samples reused more than once, paired with different prompts. This is not the standard way of evaluating on AudioCaps. Mostly, TTA models follow the evaluation of the AudioLDM series, which have predefined prompt-audio pairs. Some papers, like Ezaudio, randomly sample one prompt for each audio by themselves. However, it is unclear how this work derived a testing set with 1,811 prompt-audio pairs.\n\n- In the original paper of TangoFlux, they achieve an IS score higher than 11. However, this work reports an IS score of 8.85 for TangoFlux. This shows that their configuration for the AudioCaps testing set severely affects the objective performance of TTA models.\n\n- Please also report the FD score using PANNS, as this is the most common metric for evaluating fidelity.\n\n- This work does not have human listening tests, MOS, or subjective user studies. Providing audio samples for demo is not enough.\n\n- For the demo in the link provided in the abstract, there is no reference clip for comparison. Prompts used to generate different durations of audio are also not the same. It is unable to compare the difference between short-form and long-form audio generation using the same prompt. It feels like they are just cherry-picking the good examples for different audio durations.\n\n- Up to this point, it cannot be confirmed whether this work is state-of-the-art or not.\n\n- If it is just competing on the AudioCaps evaluation set, we do not need 30-minute audio clips, as most of the audio in AudioCaps is just 10 seconds long. However, if the aim is to generate long-form audio, I don’t think this work has done long-form audio evaluations in a proper way. In section 5.2 for “Ultra Long Audio Generation”, they mention that the generated 30-minute audio is segmented into non-\noverlapping 30-second clips, with each clip evaluated with objective metrics. It is not clear if they computed the FAD metric with the same audio ground truth clip for each segment. Assuming they did, this still does not really justify the internal coherence of a long audio. It mostly tests how much each segment resembles that single clip’s distribution (or fidelity). For calculating KL, if the reference is one clip’s logit distribution, the metric then rewards segments that mimic that clip’s class distribution, again saying nothing about cross-minute structure. You can have \n$D_{\\mathrm{KL}}(p_i \\| q)$ small for all $i$, yet $D_{\\mathrm{KL}}(p_i \\| p_j)$ large for some $i \\ne j$. \nHere is an example: \n$ q=(0.7,0.2,0.1), p_1 =(0.80,0.19,0.01), p_2 =(0.60,0.21,0.19)$, where\n$D_{\\mathrm{KL}}(p_1 \\| q)  = 0.074, D_{\\mathrm{KL}}(p_2 \\| q)  = 0.039, D_{\\mathrm{KL}}(p_2 \\| p_1) = 0.407, D_{\\mathrm{KL}}(p_1 \\| p_2) = 0.181$\n\n- For IS, since it does not require a reference, there is no reference and no between-segment relation.\n- For CLAP, consider the prompt “Birds chirping, dogs barking, and a duck quacking”. What does uniformly high CLAP per-segment score mean? Consider an audio clip that only has birds chirping in the first 10-minute segment, only dogs barking in the second 10-minute segment, and only a duck quacking in the last 10-minute segment. Should this audio clip have similar high CLAP scores for each segment? Even if it does, does it mean this whole audio clip is actually following the text prompt? Could an audio clip with all birds chirping still give you similar results by measuring CLAP score against the text prompt “Birds chirping, dogs barking, and a duck quacking”? I think it still yields similar CLAP scores for each segment in this case, as the audio is all birds throughout, each segment’s audio embedding will align strongly with the “birds” component of that text vector. As a consequence, uniform scores in this setup do not prove the audio covers dogs or ducks, nor any long-form structure, just that each segment sounds related to the composite prompt in aggregate.\n\n- Again, no user study or evaluation on long-form audio generation.\n\n- Latency should also be compared against other baseline TTA models, not just between TFMamba and the transformer-based version.\n\n- The idea of EAG itself is novel, but the performance improvement compared to CFG is very minor as shown in Table 5. For 30 s generation: CLAP +0.0011, IS +0.014, FAD −0.0052, KL −0.0059 (vs. CFG).\n\n- In conclusion, unfortunately, this paper is far from ready to meet the standards of ICLR at this stage."}, "questions": {"value": "- In Table 1, for reference-dependent metrics like FAD, do you use the same ground-truth (≈10-second) clip as the reference regardless of the generated audio’s duration?\n\n- What’s the rationale for generating a 30-minute clip from a very short prompt like \"Birds chirping, then a dog barking, then a duck quacking?\" Wouldn’t a long, detailed prompt describing the full 30 minutes be a more realistic test?\n\n- If we want to generate a dog barking sound, are you sure your model can generate the same dog barking for 30 minutes?\n\n- Lack of references: Line 50 is talking about autoregressive (AR) models, but the citations are mostly non-AR models.\nWorth citing UniAudio, MusicGen, and AudioMNTP, which are also autoregressive text-to-audio or text-to-music generation models."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NLpfjoLd1A", "forum": "WFgtMF6vk6", "replyto": "WFgtMF6vk6", "signatures": ["ICLR.cc/2026/Conference/Submission5772/Reviewer_hKMd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5772/Reviewer_hKMd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760776709663, "cdate": 1760776709663, "tmdate": 1762918253473, "mdate": 1762918253473, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TFMAudio, a text-to-audio generation framework that integrates Time-Frequency Mamba (TFM) and Energy-Aware Guidance (EAG). The model achieves linear-time long-form generation and maintains temporal and spectral consistency, producing up to 30-minute high-fidelity 44.1 kHz audio with strong semantic alignment."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "**1. Effective Dual-Axis Modeling via Time-Frequency Mamba**\n\nThe proposed Time-Frequency Mamba performs 1D scans along both temporal and frequency axes, allowing the model to jointly capture temporal causality and spectral correlation — a capability that conventional Transformers struggle to achieve.\n\n**2. Linear-Time Complexity for Long Sequences**\n\nThe Mamba-based recurrent formulation enables linear computational complexity O(L) with respect to sequence length, offering a far more efficient alternative to the quadratic O(L²) cost of Transformers while maintaining long-range dependencies.\n\n**3. Stable and Scalable Audio Generation with Energy-Aware Guidance**\n\nThe introduced Energy-Aware Guidance (EAG) mitigates state drift by decomposing the flow-matching velocity field and adaptively damping unstable components, enabling reliable ultra-long (30-minute) 44.1 kHz audio generation with temporal consistency."}, "weaknesses": {"value": "**1. Marginal Impact of Energy-Aware Guidance (EAG)**\n\nAccording to the ablation results, the performance improvement from EAG is minimal, with only slight differences in objective metrics.\n\n**2. Lack of Flexible Length Control in Generation**\n\nWhile the paper demonstrates 10s and 30s generations, it is unclear whether the model allows arbitrary-length synthesis (e.g., 13s or 27s) or only supports pre-defined durations tied to training configurations."}, "questions": {"value": "**1. On the Effectiveness of Energy-Aware Guidance (EAG)**\n\nThe ablation results suggest that EAG provides only marginal gains across objective metrics.\nCould the authors elaborate on specific scenarios or qualitative aspects where EAG meaningfully contributes to stability or audio fidelity?\n\n\n**2. On Length Controllability During Generation**\n\nCan TFMAudio support arbitrary-length generation (e.g., 13s or 27s) beyond fixed training configurations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "v2ko9AABB5", "forum": "WFgtMF6vk6", "replyto": "WFgtMF6vk6", "signatures": ["ICLR.cc/2026/Conference/Submission5772/Reviewer_xDov"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5772/Reviewer_xDov"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552129837, "cdate": 1761552129837, "tmdate": 1762918253226, "mdate": 1762918253226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TFMAudio, a Mamba-based text-to-audio (T2A) model that integrates a Time-Frequency Mamba (TFMamba) backbone and Energy-Aware Guidance (EAG) for long-form generation. The model combines linear-complexity state-space modeling with flow matching to improve efficiency and stability over transformer-based diffusion models. The authors claim that TFMAudio achieves state-of-the-art performance on AudioCaps and WavCaps benchmarks and can generate up to 30 minutes of 44.1kHz audio with temporal consistency.\n\nWhile the motivation—to overcome the quadratic complexity of transformers for long-form generation—is reasonable, the experimental evidence and claims are weak and overinterpreted. The model is only trained on 10-second clips, and therefore the claimed ability to generate consistent 30-minute audio is not supported by data. The long-form generation experiment effectively tests extrapolation far outside the training distribution and yields repetitive, semantically meaningless output. Overall, the paper reads more as a well-written engineering demonstration than a solid scientific contribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and technically clear. Mathematical derivations (flow matching, EAG) are properly explained, and the figures are informative.\n\n2. Using Mamba for efficient long-range modeling is a timely and interesting idea. The dual-scan mechanism (TimeMamba + FrequencyMamba) provides a coherent architecture for time–frequency modeling."}, "weaknesses": {"value": "1. The entire training uses 10-second AudioCaps/WavCaps clips, yet the main claim of the paper is about ultra-long (30-minute) generation. This makes the core contribution unverifiable—the model has never seen long-form data, so the “30-minute consistency” claim lacks credibility. The long audio is almost certainly repetitive or degenerate, as suggested by the flat metric curves and absence of qualitative human evaluation.\n\n\n2. Only four metrics (CLAP, IS, FAD, KL) are reported—no subjective MOS tests, no human preference studies. The improvement margins over baselines are modest and not statistically validated. \n\n3."}, "questions": {"value": "1. why the demo pages donot show the full 30 mins audio?\n\n2. The Mamba structure can bring better performance than transformer?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Qx6w1sXwzv", "forum": "WFgtMF6vk6", "replyto": "WFgtMF6vk6", "signatures": ["ICLR.cc/2026/Conference/Submission5772/Reviewer_ipzw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5772/Reviewer_ipzw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761841187224, "cdate": 1761841187224, "tmdate": 1762918252908, "mdate": 1762918252908, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed TFMAudio, a Mamba-based flow-matching model capable of long-form audio generation.\nThe authors proposed to use a combination of TimeMamba and FrequencyMamba to improve results.\nIn addition, the authors proposed an energy-aware adaptive guidance (EAG) mechanism which adaptively adjusts guidance weight, further boosting performance.\nPutting everything together, TFMAudio can generate 30 minutes of high-quality audio."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The generation results are strong. Linear complexity in sequence length is verified theoretically and empirically. Overall, the technical aspect of this work is quite solid.\n\nIf the weaknesses can be sufficiently addressed, I will consider increase the rating."}, "weaknesses": {"value": "The writing of the paper can be improved:\n\n- There are plenty of places where math symbols are not in a math environment. For example, O(L) in line 178, \"d is the feature dimension\" in line 268, etc.\n\n- Figure organization is inconsistent. Some figures appear at the top (e.g., Figure 2), while others are in-line (e.g., Figures 3 and 4).\n\n- Some claims in the background sections may not be accurate. For example, the authors claim that\n  > When applying transformers to audio latents $x \\\\in \\\\mathbb{R}^{L \\times C}$, conventional patchification treats the representation as an image grid and breaks the native channel-wise coupling and causal temporal structure.\n\n  While some transformer-based models may treat audio signal in this manner, many do not, and instead model the audio latent embeddings as a 1-D sequence.\n\nAdditionally, while the authors compared TFMAudio to several strong baseline models, some recent high-performance methods, such as IMPACT [1], are missing. I suggest including these methods in Table 1.\n\nI would also invite the authors to make some clarifications regarding the questions in the section below, and add the discussions to the appropriate sections of the paper.\n\n[1] Huang et al. IMPACT: Iterative Mask-based Parallel Decoding for Text-to-Audio Generation with Diffusion Modeling."}, "questions": {"value": "- If I understood it correctly, Mamba is causal. While it is understandable for TimeMamba to process the time-domain signal in a causal manner, it is unclear why FrequencyMamba should scan from the highest to the lowest channel, and not the other way around. Do you think a bi-directional FrequencyMamba can further improve the results?\n\n- Do you think the effectiveness of FrequencyMamba is tied to the Stable Audio Open VAE used in this work? If a different continuous VAE is used, will FrequencyMamba still help? What if a discrete tokenize is employed instead?\n\n- Is energy-aware adaptive guidance \"bundled\" with TFMAudio, or is it useful for general diffusion models? Is EAG necessitated by TFMAudio's properties? Since this paper is mostly about TFMAudio, I think it is important to clarify its relationship with EAG."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nt9jawwHx1", "forum": "WFgtMF6vk6", "replyto": "WFgtMF6vk6", "signatures": ["ICLR.cc/2026/Conference/Submission5772/Reviewer_zQ3d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5772/Reviewer_zQ3d"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5772/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979251255, "cdate": 1761979251255, "tmdate": 1762918252621, "mdate": 1762918252621, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}