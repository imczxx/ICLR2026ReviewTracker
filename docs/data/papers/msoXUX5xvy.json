{"id": "msoXUX5xvy", "number": 4662, "cdate": 1757738480997, "mdate": 1759898021124, "content": {"title": "Modeling the language cortex with form-independent and enriched representations of sentence meaning reveals remarkable semantic abstractness", "abstract": "The human language system represents both linguistic forms and meanings, but the abstractness of the meaning representations remains debated. Here, we searched for abstract representations of meaning in the language cortex by modeling neural responses to sentences  using representations from vision and language models. \nWhen we generate images corresponding to sentences and extract vision model embeddings, we find that aggregating across multiple generated images yields increasingly accurate predictions of language cortex responses, sometimes rivaling large language models. \nSimilarly, averaging embeddings across multiple paraphrases of a sentence improves prediction accuracy compared to any single paraphrase. Enriching paraphrases with contextual details that may be implicit (e.g., augmenting \"I had a pancake\" to include details like \"maple syrup\") further increases prediction accuracy, even surpassing predictions based on the embedding of the original sentence, suggesting that the language system maintains richer and broader semantic representations than language models. Together, these results demonstrate the existence of highly abstract, form-independent meaning representations within the language cortex.", "tldr": "Information encoded in the human language cortex can be modeled from diverse representational sources that differ substantially from the original linguistic stimulus in both form and modality.", "keywords": ["Language cortex modeling", "NeuroAI", "LLMs", "fMRI", "Abstraction", "Semantics", "Vision models"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/10896bd6fddc7b45adf218eb5c50b0ed6e39bbb0.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how abstract meaning is represented in the human language cortex by training ridge regression encoding models that predict fMRI responses from embeddings derived from large language models and vision models. The authors introduce a form-independent modeling framework that evaluates whether diverse inputs, such as paraphrases, enriched contextual sentences, and generated images, can predict neural activity in the language cortex. The study shows that averaging across multiple paraphrases or image embeddings enhances encoding accuracy and that semantically enriched sentences can even surpass the original text representations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The work investigates a fundamental question in cognitive neuroscience and AI — whether the brain encodes semantic meaning in a form-independent and modality-independent manner.\n\n* Using representational augmentations across different forms and modalities (such as multiple paraphrases and generated images) provides a novel approach to probing semantic abstraction in brain representations."}, "weaknesses": {"value": "* The correlation-based encoding models cannot establish whether the brain’s “abstract” representation is genuinely modality-independent or form-independent.\n\n* The paper reports that representational diversity (such as enriching sentences) substantially improves encoding accuracy. How do the authors ensure that these gains arise from adding meaningful, semantically relevant information rather than simply increasing input length or redundancy? The paraphrases and images generated by large models may include noise or implausible details; quality control and human validation are insufficiently described.\n\n* In several experiments, purely visual models (e.g., Swin Transformers) outperform CLIP in predicting language-cortex responses, even though CLIP is explicitly trained for vision–language alignment. Could the authors clarify why this occurs?\n\n* The paper reports many comparisons and trends (e.g., improvement with more paraphrases or images) but lacks formal statistical testing of key effects.\n\n* The paper is dense, and main findings could be summarized more clearly and concisely."}, "questions": {"value": "* How consistent are the findings across individual subjects and brain regions (beyond aggregate “language cortex” analyses)?\n\n* While the paper provides a comprehensive list of the visual models (such as ResNet, DINO, and Swin Transformer) in Appendix A.4, it does not specify the source or configuration of their pretrained checkpoints. For instance, it remains unclear whether the Swin Transformers were initialized from ImageNet-1K or ImageNet-22K.\n\n* How were the enriched paraphrases validated to ensure they add plausible commonsense information rather than arbitrary details?\n\n* Given the low temporal resolution of fMRI, how confident can we be that the observed improvements truly reflect more abstract or enriched semantic representations rather than temporal averaging effects or hemodynamic smoothing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jt8R5QHx5E", "forum": "msoXUX5xvy", "replyto": "msoXUX5xvy", "signatures": ["ICLR.cc/2026/Conference/Submission4662/Reviewer_Hsss"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4662/Reviewer_Hsss"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761366319225, "cdate": 1761366319225, "tmdate": 1762917499811, "mdate": 1762917499811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that fMRI response to sentence stimuli in the brain’s language network can be modeled by not only LLM embeddings of the original sentences, but also by vision model embeddings of images generated based on sentence prompts, and LLM embeddings of paraphrased sentence. The predictivity of language and vision models is also comparable when considering only content or header words in the original sentence. Finally, they find that brain predictivity can be improved when considering “enriched paraphrases” beyond the original sentence."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper uses state-of-the-art AI models to answer important questions about the human language network\n\nThe paper compares a wide range of multimodal model inputs to compare different sources of semantic information\n\nThe “enriched paraphrase” results are interesting and novel and suggest ways the language network extracts meaning beyond the linguistic input."}, "weaknesses": {"value": "A growing body of work has now shown that large language and vision models learn overlapping representations of iamges and their captions (e.g., Platonic Representation Hypothesis Huh et al 2024). This has been leveraged in similar cognitive neuroscience studies to show that LM embeddings of sentence captions can predict the brain’s visual cortex responses (Doerig et al., 2025, Conwell et al. 2025). The authors do not cite this large body of relevant prior work. They also do not report the correlation / overlap between LM and VM embeddings in any of hte experiments, but given that LM and VM model representations are likely overlapping, the bulk of the results in this paper are not particularly surprising. Much of the discussion (and title) describing these as “independent” are incorrect/misleading. For each experiment, the authors should quantify the overlap between the “independent” sources of information and adjust their intepretation accordingly. \n\nWhile based on prior work, the authors should provide more context on the fMRI experiments so the paper is more easily interpretable / can be read on its own. In particular, details/differences between the different datasets and the discussion of “header words” in the Pereira dataset were hard to follow.\n\nThroughout, the authors show only mean predictivity across the entire language network, a large swath of cortex averaging over hundreds (or thousands?) of voxels. It would help to analyze the data in a more fine-grained way to see if there are particular regions that are sensitive to different types of information.\n\nThe figures throughout could be made more readable. Eg the CLIP dots/lines are very difficult to see in most figures.\n\nThe paper could use some more technical details / motivation for the different model types. For example, a brief overview of the different model architectures and why they are included (just for variety?)"}, "questions": {"value": "As detailed above, the most important question to address in each experiment is the extent of overlap between the vision and language model embeddings, without this it is impossible to assess the novelty/significance of the paper.\nHow was image quality assessed/quantified in figure 4?\nThe results of the “enriched paraphrase” are very interesting, but how does this reconcile with the large body of prior work suggesting that the human language network does not represent extralinguistic knowledge?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lkVAl91o3W", "forum": "msoXUX5xvy", "replyto": "msoXUX5xvy", "signatures": ["ICLR.cc/2026/Conference/Submission4662/Reviewer_tWEL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4662/Reviewer_tWEL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857584483, "cdate": 1761857584483, "tmdate": 1762917499144, "mdate": 1762917499144, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This manuscript asks if fMRI activity produced by sentences, as measured in the language network (here, several frontal and temporal regions) can be encoded from representations that differ from the original linguistic input. Specifically, they evaluate ability to predict language driven activity from embeddings from vision-only models (for matching pictures) and text models (operating over paraphrases and enriched text). The authors show that embeddings from vision models, generated from sentence-derived images, can predict sentence-evoked fMRI in language-selective cortex. They argue this demonstrates \"remarkable abstractness\" (see title) in the language system."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* The encoding pipeline is solid from an ML perspective\n* The finding that averaging of sentence paraphrases improves predictivity is interesting (but see below) \n* Contributes to an existing literature on abstract encoding of semantics in the brain"}, "weaknesses": {"value": "There is overstated novelty regarding documenting/discovering \"abstractness\" in language encoding . The title (“**remarkable abstractness**”) and Discussion (“**highly abstract**”) frame the main contribution as discovering abstract, cross-modal semantics. However, semantic abstractness across modalities has already been demonstrated, most directly in decoding studies showing cross-modal identification: Nikolaus et al. (not cited; https://arxiv.org/abs/2403.11771): show cross-modal fMRI-to-feature decoding between images and captions also when using unimodal vision features. Other, less recent cross-modal decoding results (all also not cited): Simanova et al. 2014; Shinkareva et al. 2011; Fairhall & Caramazza 2013 all show that category information generalizes over words/pictures/sounds in multiple regions. Importantly, many of those regions are also outside the language network as defined here. \n\nI therefore argue that the central claim of discovering abstractness is not novel; what is novel is the encoding approach for sentence-level activity.\n\n2. There is therefore a scholarship gap: the above literature is central to the paper’s framing, constrains the predictions, can guide the brain areas to be analyzed, but is currently missing. The manuscript should cite and relate to these studies. While one might argue that the focus on the language network as encoding abstractness is a contribution or focus of the paper, this is only valid if prior work is cited as prior related work, and furthermore, the prior work importantly suggests that in order to understand how the brain codes for modality-general information we would do well to look outside the language network in the first place. \n \n3. I think there is an over-interpretation of the results relating to the effects of averaging paraphrases of the target sentence. The paper treats accuracy gains from averaging sentence paraphrases as evidence for \"meaning amplification\". A more parsimonious account is, instead, that it indicates variance reduction in the model space: paraphrase-specific idiosyncrasies (lexico-syntactic/style factors) would cancel under averaging, improving reliability of the embeddings rather than enriching semantics. As written, the manuscript does not try to distinguish enrichment from denoising. This can be done, for example, by adding the variance of paraphrases as an additional predictor.  If enrichment drives the result, increased variance should improve prediction, beyond the mean.\n\n4. Re' methods: there is a potential circularity introduced by multimodal-trained encoders. Some tested models (nominally referred to as vision or text models) are trained with multimodal objectives (e.g., CLIP), which already align image–text embeddings. Using them to argue for brain abstractness is circular; the authors should use only unimodal encoders."}, "questions": {"value": "none at this point"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "PrbQwncAkT", "forum": "msoXUX5xvy", "replyto": "msoXUX5xvy", "signatures": ["ICLR.cc/2026/Conference/Submission4662/Reviewer_DsH1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4662/Reviewer_DsH1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761919028709, "cdate": 1761919028709, "tmdate": 1762917498388, "mdate": 1762917498388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper uses internal representations extracted from LLMs and well as from visual models to reconstruct activation patterns (fMRI) in the language-related brain regions of participants reading English/Chinese sentences. The comprehensive set of experiments investigate the impact of the using the representations based on the original texts, paraphrases thereof, images generated from the texts, as well as elaborations of the texts with additional, inferreble details. The findings suggest that the brain represents linguistic data in a largely modality independent way, while at the same time incorporating aspects of semantics which are not explicitly present in the input."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- The paper builds on a growing body of work using deep learning models to investigate the human brain. It uses this toolkit to address the question of how modality dependent and abstract the brain's representations are, in comprehensive detail.\n- The methodology seems solid.\n- The paper is written with admirable clarity and very easy to follow.\n- The results are of broad interest to cognitive science."}, "weaknesses": {"value": "I didn't indentify any substantive weaknesses. Some details of the experimental setup were missing, but can be easily added in a revision (see questions)."}, "questions": {"value": "- What exactly do you mean by abstract? Would a more precise terms be modality-independent or form-independent, as appropriate?\n- How are sentence-level embeddings produced for the different models? For LLMs, does this involve aggretation token-level embeddings?\n- Why were the embeddings extracted from the penultimate layer (in most or all cases as far as I can see). Was this based on a preliminary experiment?\n- Lines 698-701: Can you elaborate on how combining two losses and using backprop for training is related to the presence of NaN values in the data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "25vjki7Bl8", "forum": "msoXUX5xvy", "replyto": "msoXUX5xvy", "signatures": ["ICLR.cc/2026/Conference/Submission4662/Reviewer_QaNP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4662/Reviewer_QaNP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4662/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930830859, "cdate": 1761930830859, "tmdate": 1762917497840, "mdate": 1762917497840, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}