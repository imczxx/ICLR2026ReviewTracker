{"id": "nXJmbeMHDQ", "number": 7742, "cdate": 1758034330956, "mdate": 1759897835905, "content": {"title": "Star-Corrector: A Multi-Turn Interactive Reinforcement Learning Framework for Lean4 Theorem Correction", "abstract": "Formal mathematical reasoning requires models to generate verifiably correct proofs, a process where existing single-turn generation paradigms may fail to utilize the critical feedback from proof checkers. To bridge this gap, we present Star-Corrector (State-Thinking-Answer-Reward Corrector), a multi-turn, feedback-driven framework that explicitly learns from verification signals to refine faulty proofs. Our approach models proof generation as an iterative refinement process: starting from an initial flawed attempt, the model interacts with the Lean verifier at each turn to identify errors and progressively revises the proof. This multi-turn interaction allows the model to internalize corrective signals and learn precise repair strategies. The core contributions of this work are threefold. First, we introduce a multi-turn interaction model that formally defines the proof generation as a process of iterative correction based on verifier feedback, moving beyond single-turn generation. Second, we effectively optimize the policy within this interactive setting by applying GRPO to leverage the sequential verification outcomes. Third, we develop a sampling strategy that dynamically balances problem difficulty during training by leveraging pre-defined difficulty levels and the model's evolving success rate. On the MiniF2F benchmark, STAR-Corrector elevates the pass rate from 64.34\\% (base model with 32 samples) to 96.72\\% under a 32+32 sampling budget, marking an absolute gain of +32.37\\%. The results demonstrate that our approach, particularly the adaptive sampler, effectively enhances generalization on medium and hard problems, validating the importance of closed-loop, data-efficient training for formal reasoning. All code and data are available at an anonymous repository: \\href{https://anonymous.4open.science/r/ICLR-Star-E096/}{https://anonymous.4open.science/r/ICLR-Star-E096/}.", "tldr": "A Multi-Turn Reinforcement Learning Framework for Lean4 Theorem Proving via Environment Interaction", "keywords": ["multi-turn interaction", "reinforcement learning", "Multiagent", "Formal Proof"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c058817c68a205f322cc70e967566c91e80dc5d3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work presents a model trained as an iterative corrector for automated theorem proving. The model operates in a multi-turn process, interacting with the Lean verifier to identify and progressively fix errors in a proof. The experimental results are impressive: when combined with Deepseek-prover-V2, the corrector achieves a very high pass rate on the miniF2F benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Regarding originality, although several concurrent works (e.g., Goedel-Prover-v2, Seed-Prover) are also exploring the idea of multi-turn revision for ATP, the concept in this paper is still valuable (since previous works like Deepseek-Prover-v2 and Goedel-Prover did not explore this direction). Regarding significance, the reported performance is truly impressive, demonstrating the effectiveness of their method."}, "weaknesses": {"value": "The first weakness is that the reported results seem questionably high, perhaps \"too good to be true.\" This work claims 96% on miniF2F with a \"32 + 32\" budget (which is not clearly defined as an equivalent pass rate), a result significantly stronger (over 30%) than Deepseek-prover-v2. Moreover, this performance is reportedly achieved with an extremely small training dataset. Section 3.2 states the high-quality corpus consists of only 918 problem-correction pairs. This seems implausibly small, as previous or concurrent works like Deepseek-prover and Goedel-prover use datasets on the order of 100K to 1M for the SFT phase and over 10K for the RL phase.\n\nThe second issue is that many experimental descriptions are very unclear. It is not explained what a \"32 + 32\" budget means or what its equivalent pass@k configuration is. Also there are contradictory gains reported: line 328 claims a 32.37% gain, but line 345 claims a 6.97% gain. It is not clear where these numbers come from or what they are being compared against.\n\nThe third point is that the experimental setup at line 309 appears to contradict the results from the original Deepseek-Prover-V2 paper. This work states the baseline pass@32 performance is only 64% under their \"unified experimental setup,\" whereas the original Deepseek-Prover-V2 paper reported a performance of over 80%."}, "questions": {"value": "1. Could you please clarify the \"32+32\" experimental setup? What does this configuration specifically entail (e.g., number of samples, correction rounds), and what is its equivalent pass@k metric?\n\n2. We noted a discrepancy in the Deepseek-prover-v2 baseline results: the performance cited in line 310 is significantly lower than that reported in the original paper. What specific factors in your \"unified experimental setup\" cause this mismatch? Were you able to successfully reproduce the original paper's reported results using your infrastructure?\n\n3. Could you please specify which base model (e.g., architecture and pre-trained weights) was used to train your corrector?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3UOPYVEDSq", "forum": "nXJmbeMHDQ", "replyto": "nXJmbeMHDQ", "signatures": ["ICLR.cc/2026/Conference/Submission7742/Reviewer_39LF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7742/Reviewer_39LF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761522275016, "cdate": 1761522275016, "tmdate": 1762919793478, "mdate": 1762919793478, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to define the task of ATP as \"an iterative refinement process: starting from an initial flawed attempt, the model interacts with the Lean verifier at each turn to identify errors and progressively revises the proof.\" The paper experiments on miniF2F dataset using DeepSeek Prover V2."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "I can't think of any strengths for the current version of the paper."}, "weaknesses": {"value": "Token budgets are not reported for the experiments. With no information about the token budget, it is difficult to evaluate the contribution of the paper.\n\n\nThe method is not described clearly enough. Very little information is provided about the method and the experiments while page 4 is mostly dedicated to prompt templates and the paper concludes on the 7th page.\n\nIt is not clear to me what 32 + 32 means when the paper reports it for it s sampling budget. How should one compare the 32 budget to a 32 + 32 budget?\n\nPaper talks about curating a training set but it remains unclear to me how the training set is used for training purposes.\n\nResults in Table 1 are very brief and the jump in accuracy comes with almost no explanation or additional details about how it has been achieved.\n\nGiven the paper's view of proof trajectories, I would have guessed that it will use a step prover model and also utilize the tree search methods. However, the paper uses whole proof generation models such as DSP V2. There is also no discussion of step provers such BFS Prover, InternLM, etc. I'm not sure if this is because of lack of familiarity with the existing models or not.\n\n\nCitations mostly use wrong formatting. This is more troublesome in the tables."}, "questions": {"value": "Any clarifications may be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "LnwcUt2y5C", "forum": "nXJmbeMHDQ", "replyto": "nXJmbeMHDQ", "signatures": ["ICLR.cc/2026/Conference/Submission7742/Reviewer_zBW5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7742/Reviewer_zBW5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761897259131, "cdate": 1761897259131, "tmdate": 1762919793138, "mdate": 1762919793138, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose reframing automatic theorem proving as a iterative refinement process and using trajectory-level RL to optimize on these longer horizons.\n\nContributions include:\n- multi-turn formulation that incorporates lean verifier feedback\n- trajectory level GRPO RL\n- curriculum sampling: dynamically adjusts sampling weights for problem difficulty strata\n\nThe authors show improved pass rates on miniF2F test split."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- formalization of proof repair as MDP\n- strong results (improvement) on miniF2F, a standard benchmark for automatic theorem proving work\n- curriculum learning approach in RL loop"}, "weaknesses": {"value": "- iterative self repair for proof generation is not entirely novel. [Zhou et al 2025](https://arxiv.org/abs/2507.15225)'s \"Solving Formal Math Problems by Decomposition and Iterative Reflection\" have propose something similar and additionally add complexity with problem decomposition, achieving similar miniF2F score (95) without training (not cited)\n  - I think Goedel Prover also uses proof repair as a training objective?\n- lack of a training-compute equalized baseline-- would be instructive to compare this training approach to single turn proof generation\n- little analysis of the impact of the curriculum sampling"}, "questions": {"value": "- What exactly is meant by the \"x+y\" sampling budget?\n- Were there any ablation studies on the curriculum sampling component?\n- Are other benchmarks evaluated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "srkdjo2Zz6", "forum": "nXJmbeMHDQ", "replyto": "nXJmbeMHDQ", "signatures": ["ICLR.cc/2026/Conference/Submission7742/Reviewer_Q195"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7742/Reviewer_Q195"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959437211, "cdate": 1761959437211, "tmdate": 1762919792788, "mdate": 1762919792788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces STAR-Corrector, a multi-turn reinforcement learning framework for Lean4 theorem correction. Unlike single-turn proof generation, STAR-Corrector models theorem proving as an iterative dialogue between the model and the Lean verifier. Each turn involves (1) analyzing verifier feedback, (2) generating a reflection on errors, and (3) proposing a corrected proof attempt. The agent is trained using GRPO with a composite reward that combines verification success, efficiency (fewer attempts), code validity, and similarity to reference proofs. A dynamic difficulty-aware sampler adjusts the problem distribution based on model progress."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed method is intuitive and reasonable."}, "weaknesses": {"value": "The paper is poorly written, with several significant issues, including:\n\n- Numerous formatting errors. For example, references in Table 1 are not cited in the correct format, and the authors misuse \\citep and \\citet throughout the paper. Additionally, the appendix appears immediately after the main text instead of following the references.\n\n- Inconsistencies in reported results. In Table 1, the authors report an absolute gain in accuracy of 32.37%, whereas line 345 states, “our method achieves an absolute gain of +6.97% in pass rate.”\n\n- Missing technical details. For instance:\n    - In line 259, the authors write, “sampling weights for these 5 levels are then dynamically adjusted based on the smoothed global pass rate over recent steps,” but the procedure is not clearly explained.\n    - In line 478, the authors state, “For reproducibility, the default hyperparameters used in the experiments are listed in Table A.5.” However, Table A.5 does not exist.\n    - A link to an anonymous repository is provided, but the repository is empty.\n    - DeepSeek-Prover-V2 (Unified Experimental Setup) is listed as a main baseline, but the paper does not explain what this setup is.\n    - Given that the main content ends halfway through page 7, the authors should have sufficient space to include these missing details.\n\nDue to the numerous writing issues and missing information, I am unable to assess the quality of the reported results. I believe this paper requires substantial revision."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["Yes, Research integrity issues (e.g., plagiarism, dual submission)", "Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "Given the way this paper is written, I am skeptical of the reported results' authenticity."}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kGRFS1DbvR", "forum": "nXJmbeMHDQ", "replyto": "nXJmbeMHDQ", "signatures": ["ICLR.cc/2026/Conference/Submission7742/Reviewer_kmCC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7742/Reviewer_kmCC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7742/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762149365909, "cdate": 1762149365909, "tmdate": 1762919792474, "mdate": 1762919792474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}