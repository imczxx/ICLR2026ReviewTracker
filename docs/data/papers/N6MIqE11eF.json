{"id": "N6MIqE11eF", "number": 23977, "cdate": 1758351291899, "mdate": 1763689402768, "content": {"title": "Is Bidirectionality Necessary in Mamba for Time Series Forecasting?", "abstract": "Mamba is a sequential model that has recently emerged as a promising alternative to Transformers, offering near-linear complexity.\nHowever, although channels in time series (TS) data generally lack a sequential order, recent studies have adopted Mamba to capture channel dependencies (CD) in TS, introducing a sequential order bias. To address this, prior works have adopted bidirectional Mamba to scan channels in both forward and reverse orders. In this paper, we show that unidirectional Mamba can effectively replace the bidirectional Mamba with simple strategies. To this end, we propose FSMamba, a TS forecasting method employing a unidirectional Mamba that incorporates a regularization strategy to minimize the discrepancy between two embedding vectors generated from data with reversed channel orders, thereby enhancing robustness to channel order. Furthermore, we introduce channel similarity modeling, a pretraining task to preserve similarities between channels from the data space to the latent space to enhance the ability to capture CD. Extensive experiments demonstrate the efficacy of our method, achieving state-of-the-art performance on diverse datasets.", "tldr": "We propose FSMamba, a time series forecasting method that uses Mamba with regularization strategy to address the sequential order bias in capturing channel dependencies.", "keywords": ["Time Series Forecasting", "Mamba"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf428f2b7b338e6732b17e2ed317a213028a6cfe.pdf", "supplementary_material": "/attachment/d0aef3ef8995051541105dd0e448b12055805dee.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies whether bidirectionality is necessary for Mamba in multivariate time series forecasting. Since channels lack natural order, applying bidirectional Mamba for channel dependencies (CD) adds complexity. The authors propose FSMamba, a unidirectional model with a regularization term that minimizes the distance between embeddings of reversed channel orders and a channel similarity modeling (CSM) pretraining step. Experiments on multiple datasets show FSMamba achieves comparable or better performance than bidirectional models with fewer parameters, proving that bidirectionality is unnecessary for effective CD modeling."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear and easy-to-understand writing with clean and visually appealing figures.\n2. Detailed experimental analysis"}, "weaknesses": {"value": "1. Since Mamba itself is designed for TD, the motivation for applying it to CD is not well justified.\n2. The evaluation of Mamba’s modeling capability on CD is insufficient."}, "questions": {"value": "1. The paper proposes a regularization method that minimizes the distance between two embedding vectors generated with reversed channel orders. However, this seems to prevent Mamba, as a sequence modeling method, from capturing sequential information. I do not understand the rationale behind this design. The final architecture appears equivalent to using Selective SSM to capture CD. Please explain why this design is necessary and why it works.\n\n2. Since the existing benchmarks contain too few dimensions, to further validate FSMamba’s capability in modeling CD, it would be helpful to include experiments on some datasets from Time-HD benchmark [1] and Wike2000 from TFB [2].\n\n[1] Ni, J., Wang, S., Liu, Z., Shi, X., Zhong, X., Ye, Z., & Jin, W. (2025). U-Cast: Learning Hierarchical Structures for High-Dimensional Time Series Forecasting. arXiv preprint arXiv:2507.15119.\n[2] Qiu, X., Hu, J., Zhou, L., Wu, X., Du, J., Zhang, B., ... & Yang, B. (2024). Tfb: Towards comprehensive and fair benchmarking of time series forecasting methods. arXiv preprint arXiv:2403.20150"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8ht4NKW9aq", "forum": "N6MIqE11eF", "replyto": "N6MIqE11eF", "signatures": ["ICLR.cc/2026/Conference/Submission23977/Reviewer_P4ub"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23977/Reviewer_P4ub"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842511874, "cdate": 1761842511874, "tmdate": 1762942879968, "mdate": 1762942879968, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FSMamba, a unidirectional Mamba model for time series forecasting that addresses the \"sequential order bias\" found in bidirectional models. It uses a \"Flipped Siamese\" regularization strategy to enforce robustness to channel order and removes the 1D-convolution layer from the Mamba block, arguing it is unsuited for non-sequential channel data. The authors also introduce Channel Similarity Modeling (CSM), a pretraining task designed to preserve channel correlations from the data space to the latent space."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Principled Bias Correction: The paper clearly identifies the sequential order bias from applying Mamba to non-sequential channels. The proposed \"Flipped Siamese\" regularization is an intuitive and effective method to enforce order robustness using a single, shared-weight unidirectional model, avoiding the inefficiency of two-model bidirectional approaches. \n2. Computational Efficiency: FSMamba achieves state-of-the-art performance while being significantly more efficient than bidirectional baselines like S-Mamba. It uses 37.6%–38.1% fewer parameters, consumes less GPU memory, and demonstrates faster training and inference times.\n3. Thorough Validation: The method is validated on 13 diverse datasets against numerous strong baselines. The paper includes extensive ablation studies (Section 6) that confirm the benefits of the regularization, 1D-conv removal, and CSM pretraining."}, "weaknesses": {"value": "1. Limited Permutation Robustness: The regularization strategy only enforces robustness against a single permutation (the reversed channel order) not general permutation invariance. The paper dismisses using random permutations as \"unstable\" without a deep investigation, meaning the model is order-robust only to a single, specific flip.\n2. Disconnected Pretraining Task: The Channel Similarity Modeling (CSM) pretraining task, which aims to preserve channel correlations, feels disconnected from the paper's main thesis on sequential order bias and its novelty is debatable.\n3. Contradictory 1D-Convolution Results: The paper justifies removing the 1D-convolution by stating channels lack sequential order. However, it also notes PEMS datasets do have a meaningful geographical order. The results in Table 9 show removing the 1D-conv does not harm performance on PEMS, a counter-intuitive finding that is not adequately explained and weakens the motivation for the removal."}, "questions": {"value": "1. Why random permutation result in unstable training? Why can this strategy not lead to better robustness compared to the reverse flip?\n2. Does the conclusion in Fig. 7  and the robustness generalize to even higher-dimensional data[1]?\n3. For the CSM pretraining task, why only preserving the linear correlations? Will preserving non-linear correlations also help?\n4. Can the two loss strategies (L_reg and L_CSM) be applied to other CD models like iTransformer[2] and Duet[3]?\n\n\nReferences:\n[1] Ni J, Wang S, Liu Z, Shi X, Zhong X, Ye Z, Jin W. U-Cast: Learning Hierarchical Structures for High-Dimensional Time Series Forecasting. arXiv preprint arXiv:2507.15119. 2025 Jul 20.\n[2] Liu Y, Hu T, Zhang H, Wu H, Wang S, Ma L, Long M. itransformer: Inverted transformers are effective for time series forecasting. arXiv preprint arXiv:2310.06625. 2023 Oct 10.\n[3]Qiu X, Wu X, Lin Y, Guo C, Hu J, Yang B. Duet: Dual clustering enhanced multivariate time series forecasting. InProceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 1 2025 Jul 20 (pp. 1185-1196)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2XRXpzFDPY", "forum": "N6MIqE11eF", "replyto": "N6MIqE11eF", "signatures": ["ICLR.cc/2026/Conference/Submission23977/Reviewer_7wxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23977/Reviewer_7wxw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937991996, "cdate": 1761937991996, "tmdate": 1762942879300, "mdate": 1762942879300, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether bidirectionality is essential in applying Mamba for modeling channel dependencies (CD) in multivariate time series forecasting. The authors propose FSMamba, a lightweight alternative that removes bidirectionality and introduces a regularization term to align embeddings generated from original and reversed channel orders. Experiments on thirteen benchmark datasets (including ETT, PEMS, Exchange, Weather, ECL, Solar, and Traffic) demonstrate that FSMamba achieves state-of-the-art accuracy with around 37% fewer parameters than prior Mamba-based models and exhibits enhanced robustness to channel-order permutations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper asserts that minimizing embedding distance improves robustness to channel order but does not formally analyze why this suffices to approximate bidirectional behavior. A stronger theoretical connection between the regularizer and bidirectionality could strengthen the contribution.\n2. The paper only reports λ-sensitivity results on the ETTh1 dataset (Table 14), showing stable performance within [0.01, 0.1], but it does not provide quantitative evidence across other datasets such as Weather, ECL, or PEMS. As the regularization term is central to FSMamba’s robustness design, a more systematic analysis of λ’s influence across datasets would strengthen the reliability and generality of the proposed approach."}, "weaknesses": {"value": "1. The paper asserts that minimizing embedding distance improves robustness to channel order but does not formally analyze why this suffices to approximate bidirectional behavior. A stronger theoretical connection between the regularizer and bidirectionality could strengthen the contribution.\n2. The paper only reports λ-sensitivity results on the ETTh1 dataset (Table 14), showing stable performance within [0.01, 0.1], but it does not provide quantitative evidence across other datasets such as Weather, ECL, or PEMS. As the regularization term is central to FSMamba’s robustness design, a more systematic analysis of λ’s influence across datasets would strengthen the reliability and generality of the proposed approach."}, "questions": {"value": "1.Could you provide a more formal justification of how the proposed regularization approximates bidirectional scanning?\n2.How sensitive is FSMamba to the regularization weight λ across datasets beyond ETTh1?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ajS6RS1fK9", "forum": "N6MIqE11eF", "replyto": "N6MIqE11eF", "signatures": ["ICLR.cc/2026/Conference/Submission23977/Reviewer_6NWD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23977/Reviewer_6NWD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23977/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762071438739, "cdate": 1762071438739, "tmdate": 1762942878822, "mdate": 1762942878822, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}