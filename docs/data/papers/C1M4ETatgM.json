{"id": "C1M4ETatgM", "number": 15456, "cdate": 1758251563421, "mdate": 1759897305881, "content": {"title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition and Multi-Reward Policy Optimization", "abstract": "Vision-Language Models (VLMs) often suffer from visual hallucinations – generating things that are not consistent with visual inputs – and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning and Multi-Reward Policy Optimization. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. In addition, Multi-Reward Policy Optimization separately computes advantages and log probabilities for both the visual reasoning reward and the answer accuracy reward. The method then calculates KL divergence regularization and Actor Loss using the combined sum of these two reward components. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.", "tldr": "We present multi reward and multi loss objective reinforcement learning training method to improve visual understanding and reduce hallucination.", "keywords": ["machine learning", "vision-language models", "deep learning", "reinforceme"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c19863df31031769ed6b49d8a87e05d7cde29d8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Vision-SR1, which decomposes VLM reasoning into visual perception and language reasoning, then self-verifies whether the perception alone is sufficient to answer the question, providing a self-visual reward without external supervision to reduce hallucinations and language shortcuts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear and well-written.\n- Method is effective across many tasks with thorough experimental validation.\n- Provides intuitive theoretical analysis supporting the design choices.\n- Raises several valuable, thought-provoking questions for the community."}, "weaknesses": {"value": "- Visual perceptions that are sufficient to answer the question without referring back the input image: is it possible (some visual information is hard to express in language)? Is it necessary (visual reasoning might be understood as a carrier of latent space reasoning, used to aggregate and evolve visual information and implicit thinking via attention)?\n- “If the correct answer is derived, a self-visual reward is assigned.” Which part of generation is this visual reward used to encourage in the paper? I am a bit confused. Can this visual reward be assigned solely to the visual perception part in the first rollout? What would the effect be?\n- Only Qwen-2.5-VL is trained; how about other model series? Adding them would be more convincing.\n- Is the Language Shortcut Rate metric reasonable? First, “If the evaluator can reproduce the correct ground-truth answer using only this information, the generated visual reasoning is deemed self-contained.” How many cases are there where the evaluator reproduces the wrong answer but the generated visual reasoning is actually self-contained? It would be better to have a more detailed human analysis (on a small amount of data is fine).\n- Curious about the method’s performance on spatial reasoning tasks, because I think many perceptions and reasoning in 3D space are hard to express in natural language.\n- Typo: an extra comma at line 067."}, "questions": {"value": "- Additional discussion: Whether the generated visual reasoning contains all information needed to answer the question—Is it possible, and is it necessary?\n\n- Additional experiments:\n  - If possible, try assigning the visual reward solely to the visual perception part in the first rollout.\n  - If time permits, train models beyond the Qwen-2.5-VL series.\n  - If time permits, evaluate Vision-SR1 on spatial reasoning tasks such as VSI-Bench [1], MMSI-Bench [2], and OmniSpatial [3].\n\n- Additional human analysis: In the Language Shortcut Rate evaluation, how many cases are there where the evaluator reproduces the wrong answer while the generated visual reasoning is actually self-contained? It would be better to include a more detailed human analysis (a small amount of data is sufficient).\n\n\n[1] Yang, Jihan, et al. \"Thinking in space: How multimodal large language models see, remember, and recall spaces.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[2] Yang, Sihan, et al. \"MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence.\" arXiv preprint arXiv:2505.23764 (2025).\n\n[3] Jia, Mengdi, et al. \"OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models.\" arXiv preprint arXiv:2506.03135 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "SMZej0W8dg", "forum": "C1M4ETatgM", "replyto": "C1M4ETatgM", "signatures": ["ICLR.cc/2026/Conference/Submission15456/Reviewer_mWSw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15456/Reviewer_mWSw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760760351050, "cdate": 1760760351050, "tmdate": 1762925742121, "mdate": 1762925742121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Vision-SR1, a self-reward reinforcement learning (Self-RL) framework designed to address visual hallucinations and language shortcuts in VLMs. It decomposes reasoning into two stages: visual perception and language reasoning. The model first generates a self-contained perceptual description (c), then re-prompts itself and relies solely on c to answer the question, thereby generating a self-derived visual reward. The method employs a multi-reward strategy for optimization, computing decoupled losses for both visual and answer rewards, effectively enhancing visual reasoning capability while reducing the LSR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "(1) Vision-SR1 allows VLMs to self-verify whether their visual perception is self-contained and sufficient to answer a given question. This effectively provides dense and adaptive reward signals for intermediate visual reasoning steps, avoiding the high costs, biases, and reward hacking risks associated with external human annotations or large model distillation.\n(2) Advantage functions and loss terms are computed separately for visual and answer rewards, effectively decoupling the training signals. This ensures balanced reinforcement of both visual perception and language reasoning, while also avoiding the signal sparsity and entanglement caused by the traditional approach of summing rewards."}, "weaknesses": {"value": "(1) Vision-SR1 uses the model itself as a verifier, evaluating the quality of visual perception by re-prompting the same-policy VLM. This may lead to circular dependencies and inherent biases: the model could learn to generate perceptions that \"convince itself\" rather than perceptions that truly align with the image.\n(2) By re-prompting the same-policy VLM to assess visual perception quality, the method relies on two rollouts during training, which may effectively double the training cost."}, "questions": {"value": "The authors mention that self-rewarding may lead to reward hacking. Could the authors quantitatively analyze the intrinsic bias of the reward model in Vision-SR1? Furthermore, would it be possible to introduce lightweight external verification (e.g., a frozen CLIP or BLIP module) to cross-check the true visual consistency of the self-reward, thereby mitigating the circular dependency issue inherent in self-rewarding?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iTjKTsx3Wt", "forum": "C1M4ETatgM", "replyto": "C1M4ETatgM", "signatures": ["ICLR.cc/2026/Conference/Submission15456/Reviewer_ExG7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15456/Reviewer_ExG7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761871411221, "cdate": 1761871411221, "tmdate": 1762925740354, "mdate": 1762925740354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose an improvement to vision-R1, decoupling the visual perception description generation part and the answer generation part for reward, to avoid LVLMs overly relying on language reasoning shortcuts. The method is clear, the experiments are solid, and the comparison with vision-R1 is clear."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The writing is clear, the method description is clear and specific, very straightforward, and the experiments are solid."}, "weaknesses": {"value": "1. The only concern is novelty, as the idea of first generating image description and then generating answer based on the description has been explored before. [1]\n2. Requires inference twice, which would be time-consuming and not infrastructure-friendly.\n\n[1] Multimodal Chain-of-Thought Reasoning in Language Models"}, "questions": {"value": "1. Have the authors analyzed how the model's attention changes after this training, and whether it affects its distribution on visual tokens?\n\n2. How do the authors ensure that the generated image description is self-contained? Since the model can see both the question and the image, it could potentially guess the answer and then generate an image description where the description already contains answer-related information.\n\n3. How do the authors guarantee that decoupling the visual perception stage and the answer generation stage can avoid language shortcuts? From a training perspective, in the second stage, after removing the image input, wouldn't the model be more prone to relying on language shortcuts? So why does using this two-stage decoupling method mitigate language shortcuts and visual hallucination? Why can't a single-stage approach achieve this? Is there a theoretical explanation? For example, changes in attention patterns over visual tokens?\n\n4. How do the authors address the infrastructure inefficiency issue for this two-stage training approach?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Dn0OHcxN5Q", "forum": "C1M4ETatgM", "replyto": "C1M4ETatgM", "signatures": ["ICLR.cc/2026/Conference/Submission15456/Reviewer_f7j6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15456/Reviewer_f7j6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996857497, "cdate": 1761996857497, "tmdate": 1762925739720, "mdate": 1762925739720, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Vision-SR1, a self-rewarding reinforcement learning framework that decomposes vision-language reasoning into visual perception (i.e., visual descriptions) and language reasoning stages. The method introduces Multi-Reward Policy Optimization, which separately computes and combines visual perception rewards and answer accuracy rewards, avoiding entangled learning signals. Besides, Vision-SR1 generates self-contained visual perceptions and verifies them using the model itself, improving grounding and reducing hallucination. Experiments on 47K examples show gains across visual reasoning, math, and hallucination benchmarks, outperforming Vision-R1 and related baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "I believe the studied direction is important -- overcoming the VLMs overly relying on learned priors, particularly text priors. Also, the math equation is descried clearly in the paper. \n\nEmpirical results on a couple of comprehensive benchmarks are good, demonstrating improvements. It mostly contains general visual understanding and multimodal mathematical reasoning. Some ablation studies are included as well. It also curated Vision-SR1-47K from some open source VLM benchmarks (table 1), while I am unsure if data may pollute the test set or not, since the performance of finetuning over on this data improve the performance quite a lot --- Vision-R1 47K data (fair comparisons)."}, "weaknesses": {"value": "From my perspective, the proposed two-stage pipeline is a bit ad-hoc due to it explicitly enforces there could be two stages, where the first stage generate some visual descriptions, and then the second stage is doing the reasoning. Such pipeline is expectedly not the ultimate goal for VLMs. It is basically a captioning models + a LLM --> this is not something we want to achieve for VLMs. Such behavior should emerge in VLM inference process, not enforced by constructing a small scale datasets with specific setting ⟨visual reasoning⟩ c ⟨/visual reasoning⟩.\n\nFor evaluation,I would recommend to include some benchmarks directly motivated by language shortcuts and text priors, such as Probing Visual Language Priors in VLMs and Winoground: Probing vision and language models for visio-linguistic compositionality. \n\nAlso, extend to larger scale of models, such as 72B, will make the whole evaluation solid. Extending the experimental results to other base-model beyond Qwen-2.5-VL is also helpful."}, "questions": {"value": "Could the author consider the experimental questions shown above, particularly I curious how the finetuned model perform in the datasets which directly motivated by VLM overly relying on language priors? I will consider the response and other reviewers' comments to adjust my final score, but plz feel free to skip any experiments you believe is unreasonable. \n\nRegarding the two stage things, I will definitely discuss with other reviewers and AC."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yVrJmGPKyb", "forum": "C1M4ETatgM", "replyto": "C1M4ETatgM", "signatures": ["ICLR.cc/2026/Conference/Submission15456/Reviewer_BBYi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15456/Reviewer_BBYi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15456/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762060291444, "cdate": 1762060291444, "tmdate": 1762925738666, "mdate": 1762925738666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}