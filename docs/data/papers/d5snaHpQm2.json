{"id": "d5snaHpQm2", "number": 955, "cdate": 1756825132115, "mdate": 1758806378136, "content": {"title": "Incentivizing Visual Thinking Cues via Reinforcement Learning for Complex Scene Reasoning, Planning and Understanding", "abstract": "Modern vision-language models struggle with long-horizon, compositional tasks because they lack explicit intermediate structure and often rely on post-hoc, unverifiable rationales. We propose Reinforcement Learning with Visual Thinking Cues (RL-VTC), a training framework that incentivizes models to generate compact, verifiable intermediate artifacts—visual thinking cues—such as spatial relation graphs, coarse scene sketches, affordance heatmaps, and temporal action sketches. RL-VTC couples a cue policy (that produces VTCs) with a reasoner/planner (that solves the downstream task conditioned on the cues). A task-aware verifier computes rewards that combine (i) final task success, (ii) counterfactual utility—the causal improvement when the same policy acts with vs. without the cues, (iii) temporal consistency, and (iv) parsimony to discourage gratuitous markup. We further introduce retrospective credit assignment with advantage estimates from cue ablations and off-policy relabeling to stabilize training. Across complex-scene reasoning, embodied planning, and multi-hop understanding benchmarks, RL-VTC consistently outperforms strong supervised and chain-of-thought baselines while reducing hallucinated relations and improving sample efficiency. Human and automatic fidelity checks show that learned cues are faithful (predictive under perturbations), succinct, and actionable for planning. Ablations confirm the necessity of counterfactual utility and consistency terms. Our results demonstrate that learning to think visually—by rewarding intermediate, testable structure—yields more reliable reasoning and planning in complex scenes, offering a principled path toward RL-driven interpretability in multimodal agents.", "tldr": "", "keywords": ["Thinking", "Reinforcement", "Reasoning", "Planning", "Understanding"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "", "supplementary_material": ""}, "replies": [], "withdrawn": true}