{"id": "LgeMkjbasf", "number": 24488, "cdate": 1758357339747, "mdate": 1759896763201, "content": {"title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective post-training method for improving the reasoning abilities of Large Language Models (LLMs), mainly by shaping higher-order behaviors such as reflection and planning. However, previous RLVR algorithms often apply uniform training signals to all tokens, without considering the different roles of low-entropy \\textbf{knowledge-related tokens} and high-entropy \\textbf{reasoning-related tokens}. Some recent methods try to separate these token types by gradient masking or asynchronous updates, but these approaches may break semantic dependencies in the model output and hinder effective learning. In this work, we propose \\textbf{Archer}, an entropy-aware RLVR approach with \\textbf{dual-token constraints} and synchronous updates. Specifically, our method applies weaker KL regularization and higher clipping thresholds to reasoning tokens to encourage exploration, while using stronger constraints on knowledge tokens to maintain factual knowledge. Experimental results on several mathematical reasoning and code generation benchmarks show that our approach significantly  outperforms previous RLVR methods, reaching or exceeding state-of-the-art performance among models of comparable size.", "tldr": "", "keywords": ["Large Language Model", "Reinforcement Learning", "Token Entropy"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ee326398473daf76d49b49cda4dea9d699fbf61b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes an entropy-aware RLVR method that splits tokens within each response into low-entropy knowledge tokens and high-entropy reasoning tokens using a response-level quantile. It updates both categories synchronously but applies different constraints: tighter clipping and stronger KL regularization for knowledge tokens to preserve factual style, and looser clipping with weaker KL for reasoning tokens to encourage exploration. The method integrates into GRPO or DAPO by making clipping ranges and KL weights token wise. On math and code benchmarks the approach improves accuracy and training stability over DAPO with single stage training and modest compute."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear motivation that probability mass concentration and uniform signals across heterogeneous tokens can either destabilize knowledge or suppress reasoning.\n2. Response-level quantile avoids cross-prompt entropy mismatch and provides an adaptive way to select salient tokens without global thresholds.\n3. Simple to implement in existing RLVR pipelines by turning clipping and KL into token wise functions.\n4. Competitive results on math and code"}, "weaknesses": {"value": "1. Missing head-to-head baselines against other token balancing methods, especially High Entropy Minority Tokens and Low Probability Dominance with Advantage Reweighting or Lopti. Indirect ablations that freeze low-entropy tokens are not a substitute for these established recipes.\n2. Limited analysis of sensitivity to the response-level quantile and to the pair of clipping and KL hyperparameters. The stability gains may depend on narrow settings."}, "questions": {"value": "1. Have the authors considered providing a direct comparison to (or against) token balancing methods, such as those proposed in *Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs* or *Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning*？\n2. Would a continuous weighting over tokens based on entropy or confidence outperform the binary split, and have the authors tried more than two levels within a response？"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "62jOcw3Lww", "forum": "LgeMkjbasf", "replyto": "LgeMkjbasf", "signatures": ["ICLR.cc/2026/Conference/Submission24488/Reviewer_XjH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24488/Reviewer_XjH3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937705309, "cdate": 1761937705309, "tmdate": 1762943101085, "mdate": 1762943101085, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "As proposed in the article, tokens in RLVR can be divided into two categories based on their entropy values: low-entropy knowledge-related tokens and high-entropy reasoning-related tokens. The adoption of varied CLIP ranges and KL weights for these two types of tokens yields enhanced experimental results, effectively reconciling the relationship between stability and exploration."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is simple and easy to apply and optimize within existing frameworks.\n2. The paper is well-written and easy to follow, with plenty of visualizations to help convey the key findings."}, "weaknesses": {"value": "1. Please refer to the \"Questions\" section for details."}, "questions": {"value": "1. Experimental evidence and concise discussions are provided, yet a theoretical justification for the two proposed improvements remains absent; it would be valuable to understand why they are effective.\n2. The authors systematically vary the hyper-parameters associated with each improvement, but a rigorous ablation study that entirely ablates each component appears to be missing.\n3. Is the method compatible with PPO, and has its applicability been analyzed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1P9n6KhhHJ", "forum": "LgeMkjbasf", "replyto": "LgeMkjbasf", "signatures": ["ICLR.cc/2026/Conference/Submission24488/Reviewer_Hrnn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24488/Reviewer_Hrnn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988598326, "cdate": 1761988598326, "tmdate": 1762943100861, "mdate": 1762943100861, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper primarily propsed a method to balance the stability of factual knowledge with the enhancement of reasoning abilities when using Reinforcement Learning with Verifiable Rewards (RLVR) to train Large Language Models (LLMs). The research finds that traditional RLVR algorithms apply a uniform training signal to all tokens, which ignores their different roles: low-entropy tokens are often associated with factual knowledge and should remain stable, while high-entropy tokens are typically linked to logical reasoning and require more exploration. To solve this, the paper proposes the Archer framework to use synchronous updates combined with dual-token constraints.\nExperimental results show that Archer significantly outperforms baselines like DAPO on mathematical reasoning and code generation benchmarks, achieving state-of-the-art (SOTA) performance among models of comparable size."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Significant Performance Gains. The method achieves outstanding results on several challenging math and code benchmarks. Compared to the baseline DAPO algorithm, Archer demonstrates significant gains, such as +6.6 Pass@1 on AIME24, and achieves SOTA performance among similarly sized models.\n- Higher Training Efficiency: The paper reports that unlike other SOTA models that rely on complex multi-stage or multi-round training, Archer achieves its best average accuracy with only single-stage training and fewer GPU hours.\n- Thorough Empirical Analysis on a specific model. The paper strongly supports its core claims with detailed ablation studies."}, "weaknesses": {"value": "- Introduction of Hyperparameters. The method introduces several hyperparameters that require careful tuning, including the clipping ranges ($\\epsilon^{k}$, $\\epsilon^{r}$) and KL weights ($\\beta^{k}$, $\\beta^{r}$) for both token types. The ablation studies show that model performance is quite sensitive to these values (especially $\\beta^{k}$), which may increase the difficulty of reproducing the best results on different tasks or models.\n- Limited Generalizability of Model and Task. All experiments are conducted on a specific 1.5B parameter base model (DeepSeek-R1-Distill-Qwen-1.5B). Furthermore, the evaluations are focused on math and code, which are highly structured reasoning domains. It is unclear if this method would be equally effective on much larger models (e.g., 100B+) or different types of tasks (e.g., common-sense reasoning, creative writing).\n- Oversimplified Token Classification. Archer relies on a binary classification based on entropy to distinguish \"knowledge\" from \"reasoning\" tokens. This binary split might be too simplistic, as a token's function in reality could be complex, multidimensional, or fall on a spectrum, rather than always being clearly defined by the single metric of entropy."}, "questions": {"value": "The paper chose the 80th percentile of response-level entropy as the threshold for distinguishing high- and low-entropy tokens. How was this specific threshold determined, and how sensitive is the model's performance to this quantile value?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gjqcJoKjTu", "forum": "LgeMkjbasf", "replyto": "LgeMkjbasf", "signatures": ["ICLR.cc/2026/Conference/Submission24488/Reviewer_uydx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24488/Reviewer_uydx"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996443987, "cdate": 1761996443987, "tmdate": 1762943100654, "mdate": 1762943100654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The proposed method, Archer, introduces a simple training recipe for RL with verifiable rewards that treats tokens differently based on response‑level entropy while keeping synchronous updates: high‑entropy “reasoning” tokens receive looser clipping and weaker KL to encourage exploration, and low‑entropy “knowledge” tokens receive tighter clipping and stronger KL to preserve factual recall. The method drops cleanly into GRPO/DAPO‑style trainers and shows consistent gains on both math (AIME24/25, AMC23, MATH‑500, Minerva, OlympiadBench) and code (LiveCodeBench v5/v6), with ablations that illuminate why keeping some KL on low‑entropy tokens and carefully setting clip ranges matter for stability and final quality."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This proposed method adds a clear, implementation‑friendly mechanism—token‑typed constraints from response‑level entropy—that integrates seamlessly with existing GRPO/DAPO setups. The decision to keep synchronous updates while relaxing constraints on reasoning tokens is well motivated, and the visual analysis of optimization regions and token interleaving makes the intuition concrete.\n\n2. The paper demonstrates consistent improvements across math and code benchmarks, suggesting the approach benefits reasoning structure rather than a single task or dataset. Ablations on KL weights and clip ranges are thoughtfully designed, revealing collapse‑vs‑stability trade‑offs and guiding practitioners toward robust settings"}, "weaknesses": {"value": "1. All results rely on a single 1.5B base (DeepSeek‑R1‑Distill‑Qwen‑1.5B); adding a second backbone or a larger‑scale model would strengthen generality. Further, The entropy quantile (ρ) is fixed without a sensitivity study, and the work would benefit from either a small sweep or an adaptive rule of thumb.\n\n2. No direct head‑to‑head with token masking/asynchronous baselines. The paper critiques these strategies but does not supply a controlled replication (same data/compute) of a recent masking method (e.g., high‑entropy minority emphasis) to empirically validate “synchronous > masking/asynchrony.”"}, "questions": {"value": "1. How are token entropies computed and cached in practice—under the rollout policy or the updating policy—and how stable is token typing across epochs?\n\n2. Do the same ε and β settings transfer to other backbones or to larger models, or is retuning required at new scales/architectures?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "egXdDYa4WA", "forum": "LgeMkjbasf", "replyto": "LgeMkjbasf", "signatures": ["ICLR.cc/2026/Conference/Submission24488/Reviewer_2PyW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24488/Reviewer_2PyW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762154829007, "cdate": 1762154829007, "tmdate": 1762943100371, "mdate": 1762943100371, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Archer, an RL training scheme that treats tokens with high entropy as “reasoning” and tokens with low entropy as “knowledge.” It keeps updates synchronous but applies looser constraints (clip, KL) on high-entropy tokens and tighter constraints on low-entropy tokens. Experiments show gains on math (AIME24/25) and code (LiveCodeBench) with a 1.5B Qwen backbone."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The method is simple. Step-wise entropy is easy to compute and does highlight where models tend to struggle. \n2. The paper gives ablations showing that removing KL on low-entropy tokens causes collapse."}, "weaknesses": {"value": "1. The core assumption is that entropy reliably separates reasoning from knowledge, which is questionable. Entropy shifts with sampling, style, and prompt form, and generation length. \n2. The paper uses some heuristics which needs more justification to claim effectiveness, such as the fixed empirical quantile threshold for entropy. \n3. The evaluation is narrow (math, code) and does not test whether the method generalizes to less structured reasoning. I feel them not convicing since we all know the noise w.r.t randomness and, more importantly, fair comparison, in these settings. \n4. Data hygiene is hand-waved; contamination remains a real issue in math tasks. \n4. The KL story is incomplete: the paper shows collapse when $\\beta=0$, but does not study adaptive KL or schedules that could match Archer’s behavior without entropy gating."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "stWnweTT7I", "forum": "LgeMkjbasf", "replyto": "LgeMkjbasf", "signatures": ["ICLR.cc/2026/Conference/Submission24488/Reviewer_PKQW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24488/Reviewer_PKQW"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24488/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762815314735, "cdate": 1762815314735, "tmdate": 1762943100219, "mdate": 1762943100219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}