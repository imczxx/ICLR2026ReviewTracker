{"id": "Gq7cBZC04L", "number": 25424, "cdate": 1758367936295, "mdate": 1759896721465, "content": {"title": "Steering Language Models for Theorem Proving", "abstract": "Recent progress in automated theorem proving leverages Large Language Models (LLMs) for their capacity to comprehend informal mathematical statements and generate corresponding formal proofs. Even though these techniques perform well, very little exploration has been done to understand how language models interpret and utilize these informal mathematical cues to generate formal proofs more effectively. To address this, we explore activation steering, a lightweight, inference-time mechanism that identifies linear directions in a model’s residual activations corresponding to informal “thought” traces, and nudges those activations to improve proof construction entirely without finetuning. Unlike previous approaches, activation engineering offers valuable insights into language models’ internal reasoning dynamics encoded in their activation space. We evaluated these activation vectors on two distinct tasks: formal proof generation from formal theorems and formal proof generation from informal problem descriptions. Our contributions are twofold: (1) we propose an activation-based intervention technique to guide proof synthesis in LLMs; and (2) improve performance across two different decoding strategies without additional training.", "tldr": "", "keywords": ["Theorem proving", "activation steering"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2387c2996333c2671934a348f83f77f88b91180f.pdf", "supplementary_material": "/attachment/2b3be24122b5e297732737020636f7a8fb930635.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an activation-steering approach for neural theorem proving by computing steering vectors as differences of mean residual activations between prompt pairs that do v.s. do not include informal natural-language reasoning, then add these vectors at selected layers during inference. \nThey claim this both sheds mechanistic light on how informal reasoning is encoded and improves proof search without training. Experiments are reported on Llemma‑7B, InternLM2‑7B, and InternLM2.5‑StepProver, with results on MiniF2F and PutnamBench."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Compute‑efficient knob: The method is a parameter‑free inference‑time intervention. The paper describes a simple pipeline (difference of means; residual addition; layer selection) that is easy to reproduce conceptually. Algorithm 1 is clear."}, "weaknesses": {"value": "1. Minimal novelty beyond known activation‑steering: the paper does not introduce new objectives, diagnostics, or provably better layer/scale selection. CAA [2] already articulate the core technique and its caveats. \nThe core mechanism of the paper is a direct application of contrastive activation addition (difference-of-means steering) and residual injection, but the paper neither validates linearity assumptions in this domain nor provides rigorous sensitivity analyses for layer choice and scaling factor. The “valley” heuristic is only qualitatively motivated by a cosine-similarity plot; no statistical tests or robustness checks are provided.\n\n2. Baselines appear misconfigured/outdated. The paper’s InternLM2.5‑StepProver baseline (48.2% on MiniF2F) is substantially below InternLM2.5‑StepProver’s own paper [1], which reports 65.9% on MiniF2F‑test (and significantly stronger results elsewhere). Without reconciling search budgets and evaluation protocol, the claimed 18‑point gain may largely reflect a weak baseline rather than a strong method.\n\n3. Key claims rely on extremely fragile evaluations. Reported improvements on MiniF2F use one specific sampling/search setting, with no seeds, CIs, or significance tests. On PutnamBench, the improvement is 6→7 solved (Lean; 0.9%→1.1%), which is within run-to-run variance for theorem provers and is not accompanied by error bars or per-category breakdowns.\n\n4. Comparison to current SOTA is missing. As of 2025 July (the contemporaneous cutoff of iclr 2026), DeepSeek‑Prover‑V2 reports 88.9% on MiniF2F‑test and solves 49/658 PutnamBench problems; Seed‑Prover reports 100% MiniF2F and strong Putnam/IMO performance. The paper neither compares against nor discusses these systems, making it hard to judge practical relevance. \n\n5. Typos: \"Roc1\" on p. 9 => \"Rocq\", \"dataset(Lin et al.\" => \"dataset (Lin et al.\". Also, the spelling \"Lean‑STaR\" and \"LeanSTaR\" are inconsistent. Also wrong model name: \"Lemma (Azerbayev et al., 2024)\" => \"Llemma (Azerbayev et al., 2024)\" \n\n[1] Wu, Zijian, et al. \"InternLM2. 5-stepprover: Advancing automated theorem proving via critic-guided search.\" 2nd AI for Math Workshop@ ICML 2025. 2025.\n\n[2] Panickssery, Nina, et al. \"Steering llama 2 via contrastive activation addition.\" arXiv preprint arXiv:2312.06681 (2023)."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "M7vsq7Kl6v", "forum": "Gq7cBZC04L", "replyto": "Gq7cBZC04L", "signatures": ["ICLR.cc/2026/Conference/Submission25424/Reviewer_JcFZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25424/Reviewer_JcFZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761419851867, "cdate": 1761419851867, "tmdate": 1762943428605, "mdate": 1762943428605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a simple method of activation steering to inject informal reasoning into an LLM for formal theorem proving. The authors also demonstrate this improves performance in downstream tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The proposed activation steering method requires only forward passes on a trained model, which is very computationally lightweight. The method of constructing the steering vector (difference of means) is lightweight but effective. The authors demonstrate that steering increases performance on downstream tasks in three models."}, "weaknesses": {"value": "In the last year there has been an abundance of work in long-CoT provers, such as DeepSeek-Prover-V1.5/V2, Goedel-Prover, Self-play Theorem Prover, Kimina-Prover. In fact all major formal theorem-proving LLMs since 2025 are long-CoT. They explicitly answer questions that the authors seek to answer: “how do informal reasoning patterns inform formal proving within a model’s internal representations?” (L85) and “how a model processes and integrates informal guidance with formal reasoning” (L118), by performing informal chain-of-thought reasoning before generating the formal proof. Other scaffolds such as DSP+ and Hilbert also follow an informal planning stage followed by a formal proof stage.\n\nThe authors have not mentioned or compared their perspective to such recent work. Instead, the models the authors tested (Llemma, InternLM2, etc) are from early 2024 and all predate long-CoT models. Since virtually all recent prover models use the chain-of-thought format, this seems to limit the significance of this work to practitioners in LLM-based theorem proving. There are some questions to be answered before this work can be actually used, such as if activation steering applies to any recent long-CoT prover model, and what the conceptual difference is between long-CoT prover models and activation steering (or is the difference only in computational cost?). For this reason I am hesitant to recommend this paper for ICLR."}, "questions": {"value": "On L297–298, the authors mention that “we additionally examine proof characteristics including average length, tactic distribution, and the frequency of intermediate lemma usage (via the `have` tactic)”. Where is the analysis of tactic distribution and frequency of `have` tactics?\n\nMinor suggestions:\n\n- L42: Lemma -> Llemma"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fw6fT7vmZK", "forum": "Gq7cBZC04L", "replyto": "Gq7cBZC04L", "signatures": ["ICLR.cc/2026/Conference/Submission25424/Reviewer_PYvo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25424/Reviewer_PYvo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965981017, "cdate": 1761965981017, "tmdate": 1762943428291, "mdate": 1762943428291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an activation steering method for automated theorem proving with LLMs: it computes contrastive “informal-reasoning” vectors from paired prompts (with/without natural-language sketches) and injects these vectors into the residual stream at selected layers during inference. The authors motivate the approach with the hypothesis that reasoning features are captured by approximately linear directions, and present a layer-selection procedure based on cosine similarity \"valleys.\" On MiniF2F and PutnamBench, steering improves proof success rates for several 7B math-tuned models under both sampling and best-first decoding, without parameter updates. The work also analyzes where steering is most effective (later layers), how it changes proof length distributions, and discusses limitations and transfer to Rocq."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novelly frames informal NL guidance itself as a steerable linear direction in the model’s residual stream and applies it to theorem proving, distinct from prior fine-tuning or retrieval approaches.\n\n2. Provides a simple, contrastive difference-of-means construction for steering vectors from paired prompts, adapted to proof settings. Demonstrates non-trivial gains on MiniF2F and improvements on PutnamBench Lean.\n\n3. Evaluates across three math LLMs and two benchmarks, reporting pass rates and ablations (layer sensitivity, search budgets, proof-length effects). Provides a LoRA comparison, showing favorable parameter-efficiency of steering (competitive without training).\n\n4. The paper is well structured, with intuitive figures and concrete hyperparameters. It clearly states assumptions (approximate linearity).\n\n5. Shows scaling with search budget and evidence that benefits concentrate in later layers and in shorter proofs, which is a useful guidance for future theorem-proving pipelines. The early sign of cross-system transfer (Lean-trained vectors helping Rocq in at least one case) also hints at portability of reasoning directions. All of those make this work valuable for future works to reference."}, "weaknesses": {"value": "1. The paired prompts come from Lean-STaR-style data and an internal filtering step; it’s unclear how sensitive results are to the exact pairing scheme, dataset domain, and prompt formatting. For example, authors can provide robustness checks: different pairing heuristics, smaller data subsets, and cross-domain steering vectors (e.g., algebra vs. geometry only).\n\n2. PutnamBench gains are modest, and Rocq discussion rests on a single highlighted success. It would be good to include some more analyses to establish reliability beyond MiniF2F.\n\n3. While vectors are derived from NL-augmented prompts, the mechanism could also capture style/format or search-friendly biases rather than genuine reasoning. To fix this, consider control experiments against, for exmaple, (a) steering vectors from semantically shuffled NL, (b) from synthetic boilerplate, and/or (c) from unrelated NL text, to isolate causal factors.\n\n4. Claims about \"more structured reasoning\" would benefit from automatic metrics. Table 3 is a good start but not sufficient for mechanism claims: example evaluations can include rates of have/calc usage, lemma reuse, etc."}, "questions": {"value": "1. LoRA is the only training baseline. Missing are previous baselines that use inference-time alternatives or retrieval-augmented proving. Can consider to add head-to-head comparisons with matched compute.\n\n2. The Rocq success is intriguing. Can you report some more results in addition to a single anecdote? Also, does the system transfer happen for other proof assistants as well?\n\n3. Slightly inconsistent terminologies: e.g., \"miniF2F\" and \"MiniF2F\" both appear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mCwFHVwyh6", "forum": "Gq7cBZC04L", "replyto": "Gq7cBZC04L", "signatures": ["ICLR.cc/2026/Conference/Submission25424/Reviewer_hgza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25424/Reviewer_hgza"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998978402, "cdate": 1761998978402, "tmdate": 1762943428030, "mdate": 1762943428030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper explores activation steering, a fine-tuning-free, inference-time intervention, to improve LLMs' ability to generate formal mathematical proofs. The authors observe that informal mathematical reasoning expressed in natural language can provide important structural guidance for proof construction, but existing models rarely use it effectively. The authors hypothesize that informal reasoning induces distinct activation patterns in the model’s residual stream, and these can be linearly isolated and reapplied to improve formal proof generation. Concretely, they construct steering vectors that capture these patterns by contrasting model activations between prompts with and without informal reasoning. Injecting these vectors into transformer layers during inference steers the model toward reasoning-rich proof trajectories without changing model weights. The authors evaluate on MiniF2F and PutnamBench and show the proposed method improves theorem-proving success rates across multiple models such as Llemma-7B, InternLM-2, and InternLM2.5-StepProver."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The concept of steering model activation to improve theorem proving is novel and interesting. \n2. The proposed activation steering method is lightweight, fine-tuning-free, requires only forward passes to compute steering vectors, making it readily pluggable into existing LLMs.\n3. The authors demonstrate strong gain brought by activation steering on MiniF2F, +18.2%."}, "weaknesses": {"value": "1. With activation steering, while short proofs improve significantly, long or highly compositional proofs show limited benefit and sometimes even degraded performance due to noisy reasoning insertions.\n2. The gain is noticeable for InternLM2.5 but modest for smaller models.\n3. The robustness of activation steering is unclear with respect to prompts and hyperparameters.\n4. It would nice to evaluate the effect of activation steering on more recent state-of-the-art theorem-proving models such as Goedel Prover.\n5. The paper lacks comparisons with frontier theorem-proving frameworks such as Seed-Prover [1], Goedel Prover [2], and LLM-based provers such as GPT-5, Qwen-235B, Claude Sonnet, and Grok.\n\n\n\n[1] Chen, Luoxin et al. “Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving.” ArXiv abs/2507.23726 (2025): n. pag.\n\n[2] Lin, Yong et al. “Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction.” ArXiv abs/2508.03613 (2025): n. pag."}, "questions": {"value": "Do the authors have any insights on why activation steering brings significant gains on MiniF2F but only minimal improvements on PutnamBench? Similarly, why is the gain more noticeable for InternLM2.5 compared to smaller models? Under what conditions does activation steering tend to work well, and when does it fail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hWQb6Oez09", "forum": "Gq7cBZC04L", "replyto": "Gq7cBZC04L", "signatures": ["ICLR.cc/2026/Conference/Submission25424/Reviewer_YYwB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25424/Reviewer_YYwB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25424/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044160070, "cdate": 1762044160070, "tmdate": 1762943427726, "mdate": 1762943427726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}