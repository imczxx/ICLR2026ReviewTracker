{"id": "qE2FyvRvuF", "number": 7118, "cdate": 1758008471378, "mdate": 1759897871832, "content": {"title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models", "abstract": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. \nReinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots.\nWe introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment.\nIn contrast to widely used latent world models, \nWMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images.\nCrucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods.\nExtensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.", "tldr": "WMPO is a world-model-based framework that conducts reinforcement learning for Vision-Language-Action policies entirely in imagination.", "keywords": ["World Models; Vision-Language-Action Models; Reinforcement Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/87dc3486f87611d63bb2b4c4072d851dce941436.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors propose a system to train a VLA via RL on trajectories from a world model. A video diffusion model is transformed into a world model by additionally conditioning upon actions. For the reward model, a videoMAE model is finetuned on binary classification on successful/failed trajectories, and the model output probability is used as a reward. The used RL algorithm is GRPO, where the initial frame comes from a real dataset, and the trajectories in the cohort are different sampled videos starting from the same frame. For baseVLA, openvla-OFT is chosen. Across both sim and real, this method beats baselines, and the authors also show that the method improves generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The idea is simple and clearly explained\n- The authors show solid improvement on sim and real"}, "weaknesses": {"value": "- only a single base-VLA is used (openvla-OFT)\n- More real-world experiments would be good\n- missing comparison to DreamGen\n- the training of the world model lack details"}, "questions": {"value": "1. What is meant by 50/1000 steps?\n2. Can you compare with PPO as well?\n3. Can you try multiple different VLAs?\n4. Can you do more real-world experiments?\n5. Can you add more details regarding how the world model was trained? Hyperparameters, datasets and so on."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "g15qTAOpsw", "forum": "qE2FyvRvuF", "replyto": "qE2FyvRvuF", "signatures": ["ICLR.cc/2026/Conference/Submission7118/Reviewer_eaSU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7118/Reviewer_eaSU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761338601987, "cdate": 1761338601987, "tmdate": 1762919288402, "mdate": 1762919288402, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes World Model-based Policy Optimization (WMPO), a framework for improving Vision-Language-Action (VLA) models via reinforcement learning inside a learned pixel-level video world model. The core idea is to avoid costly real-world robot interactions by performing on-policy RL (specifically GRPO) entirely within an autoregressive video diffusion model pretrained on large-scale robotic data and fine-tuned on downstream policy rollouts. The framework includes: A pixel-space world model for visual fidelity; Policy Behavior Alignment to adapt the world model to policy-induced state/action distributions; A learned binary reward model for task success; Noisy-frame conditioning and frame-level action control to mitigate long-horizon prediction drift; Dynamic Sampling during GRPO to avoid vanishing gradients in sparse-reward settings. Experiments in MimicGen simulation and real-world tasks (e.g., “insert square into stick”) show consistent improvement over imitation learning baselines and offline RL methods such as DPO and limited online GRPO."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses a critical bottleneck in VLA RL: sample inefficiency and brittleness of imitation learning. While prior works (e.g., RT-2, OpenVLA) have shown impressive generalization, they remain confined to IL and struggle to recover from failures. WMPO’s goal — learning to self-correct via on-policy RL in a world model — is both ambitious and well-justified.\n\n2.  The authors’ fine-tuning of the world model on policy-generated trajectories is a principled way to close the distribution shift between expert demonstrations and actual policy rollouts.\n\n3. The qualitative results showing self-correction are compelling. This is not merely improvement in success rate but evidence of learning novel recovery strategies absent in demonstrations."}, "weaknesses": {"value": "1. Overclaiming of “On-Policy” Scalability Without Real Costs: While WMPO avoids real-world rollouts during optimization, it still requires 128–1280 real trajectories to fine-tune the world model and initialize policy behavior alignment. This is not zero-shot or low-data RL — it is offline world-model RL with modest real data.\nRecent works like **IRASim** [1] and **World4RL** [2] also use diffusion world models but start from far fewer real trajectories (e.g., 50–100).\nThe paper does not compare directly to these methods, making it unclear whether the gains come from GRPO or from the specific world-model design.\n\n2. Reward Model is a Black Box with No Robustness Checks. The reward model is trained on binary success labels from real trajectories. But: What if the world model generates semantically correct but visually shifted outcomes (e.g., object slightly displaced)?\nThere is no evaluation of reward hacking — e.g., does the policy learn to “fool” the reward model by generating plausible but incorrect motions?\n\n3. Comparison to Offline RL is Weak. The DPO baseline is implemented naively: it uses trajectory-level preferences but does not leverage recent advances. Why not compare to IQL + diffusion policy? Also, DPO’s poor performance on background disruption (Tab 2) may reflect overfitting to visual cues, not the inherent weakness of offline RL.\n\n4. The quality of experiments: One task, 30 trials for the real robot task, underpowered for statistical significance. Missing key ablations (e.g., world model w/o policy alignment, latent vs. pixel space, reward model threshold sensitivity).\n\n[1] IRASim: A Fine-Grained World Model for Robot Manipulation \n\n[2] World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation"}, "questions": {"value": "Please address the aforementioned weakness as thoroughly as possible."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "BTuHk3d7dR", "forum": "qE2FyvRvuF", "replyto": "qE2FyvRvuF", "signatures": ["ICLR.cc/2026/Conference/Submission7118/Reviewer_vH24"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7118/Reviewer_vH24"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536376333, "cdate": 1761536376333, "tmdate": 1762919286853, "mdate": 1762919286853, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes WMPO (World-Model-based Policy Optimization), an on-policy RL framework for VLA models that replaces costly real-robot rollouts with trajectories “imagined” by a pixel-space video world model. Key ingredients are: (1) a diffusion video world model with noisy-frame conditioning for robustness and frame-level action control for precise action–frame alignment; (2) policy behavior alignment, i.e., fine-tuning the world model on the policy’s own rollouts to match failure as well as success states; (3) a lightweight clip-based reward model for outcome (0/1) labeling; and (4) on-policy GRPO training with dynamic sampling and no KL term (no reference model). The method aims to keep the visual state space aligned with pretrained VLA encoders by decoding back to pixels. Experiments on Mimicgen show consistent gains over GRPO and DPO baselines at two real-rollout budgets (e.g., mean SR 47.1% vs. 37.3% at P=128 and 57.6% vs. 42.4% at P=1280). WMPO also reports emergent self-correction, improved generalization under spatial/background/texture shifts, lifelong learning improvements via alternating policy/world-model updates, and a real-robot result on “Insert the square into the stick”"}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Clear, modular recipe: world-model rollouts + outcome classifier + GRPO, with practical choices (pixel-space decoding to match VLA features; noisy-frame conditioning; frame-level action injection). The write-up is concrete and reproducible. \n\n\nConsistent empirical gains over strong baselines across four Mimicgen tasks and two rollout budgets; improvements grow with budget (data-efficiency + scaling). \n\n\nBehavioral insights: convincing qualitative evidence of self-correction and reduced “getting stuck,” with trajectory-length analysis. \n\n\nGeneralization: better robustness under spatial/background/texture disruptions than baselines."}, "weaknesses": {"value": "Heavy compute / practicality: training uses 32× H100 for world-model/WMPO phases (plus 8× H100 for SFT). The paper would benefit from wall-clock, throughput, and ablations on smaller budgets/hardware. \n\nModel-world fidelity & safety: while qualitative results are strong, there’s limited quantitative assessment of rollout fidelity (e.g., per-step action-conditioned metrics), failure taxonomy, or safety constraints—especially since outcome-only rewards can reward shortcuts."}, "questions": {"value": "1. please try to handle these weaknesses\n\n2. Do you try to use some PeFT methods for low-resource environments? Maybe these discussions will help"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ffCLMQTSmA", "forum": "qE2FyvRvuF", "replyto": "qE2FyvRvuF", "signatures": ["ICLR.cc/2026/Conference/Submission7118/Reviewer_Wgig"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7118/Reviewer_Wgig"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761765509438, "cdate": 1761765509438, "tmdate": 1762919286417, "mdate": 1762919286417, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "WMPO proposes a novel policy finetuning method without using online interaction by using a world model on the pixel space, which allows robust finetuning of vision-language-action models on imagined trajectories. There are a few desirable properties of this method compared to traditional world modeling and finetuning objectives:\n1. The world model can directly be trained in pixel space, which allows a pixel based policy to interface with the world model without extra decoders.\n2. The method can be completely open sourced, and can be adapted into other distributions. \n3. This method allows on policy finetuning of VLAs using GRPO without any real-world demonstrations or preexisting datasets.\nThe authors then used GRPO to perform finetuning of a base OpenVLA-OFT policy, which shows desirable success rate, scalability, and robustness in RoboMimic. On the real world, WMPO also shows better performance compared to other finetuning methods such as direct GRPO and DPO."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The method section of the paper is concise and informative. \n2. I believe that there are sufficient ablations being done on the method, and I like that the authors have demonstrated good robustness of the method when dealing with OOD settings and desirable scalability."}, "weaknesses": {"value": "1. I believe that the paper did not address how this method can be extended into a generalist setting. The scope of the environment is also rather limited, albeit there are adequate ablations being conducted.\n2. In addition, the paper used OpenVLA-OFT as the base policy. This again limits how much promise the method can bring to generalist policies. If the authors can provide additional ablations without using OpenVLA-OFT, I believe this can strengthen the paper.\n3. I believe that the paper did not address the question of how adaptable this method is when concerned with language-conditioned settings (even though OpenVLA is language conditioned, there are no mention of how to use language labels in this paper), furthering this discussion can also be beneficial to the rating of the paper."}, "questions": {"value": "1. I might have missed this, but when you are increasing the rollout budget, do you use the larger set as well to finetune the world model?\n2. One reason for not using real-world demonstrations at all is due to it being more expensive. Have you considered using a few real-world demonstrations as regularization in your method, and if so, how does it compare to only using imagined trajectories?\n3. It seems that the authors chose to use on policy RL because of flaws with off policy RL [1] and propagating the correct value. Are there going to be any potential concerns when implementing such a method in the off-policy setting?\n\nMinor remarks:\n1. The main figure stated that BC from human demonstrations cannot achieve self-corrective behavior, but this is not correct when the training data contains corrective behavior. Similarly, you can do on-policy RL in real world if you have a suitable environment setup.\n2. Follow up on q3, it would be better to show differences in scalability if you have to use an off policy setting.\n\nReferences:\n[1] Park, S. et al., 2025. “Horizon Reduction Makes RL Scalable.” NeurIPS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "iI3EcqKQm5", "forum": "qE2FyvRvuF", "replyto": "qE2FyvRvuF", "signatures": ["ICLR.cc/2026/Conference/Submission7118/Reviewer_VLDi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7118/Reviewer_VLDi"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761866504284, "cdate": 1761866504284, "tmdate": 1762943414308, "mdate": 1762943414308, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}