{"id": "qpHDnFUsJo", "number": 24287, "cdate": 1758354959416, "mdate": 1759896772741, "content": {"title": "GraphKAN: An Efficient and Interpretable Kolmogorov-Arnold Graph Network for Source Detection", "abstract": "Source detection in graphs offers a viable solution to critical challenges such as rumor tracing. Yet existing GCN-based approaches squander non-embedding parameters and rely on fixed activation functions. We present GraphKAN: An Efficient and Interpretable Kolmogorov–Arnold Graph Network for Source Detection, which capitalizes on Kolmogorov–Arnold Networks (KANs) by assigning learnable activation functions to edge weights. Node features are first diffused through B-spline–based univariate activations, yielding expressive and localized transformations. We further devise a sparsity-aware neighborhood aggregation rooted in community clusters, where edge-level attention is adaptively strengthened through KAN-driven kernel learning. Unlike black-box GCNs, GraphKAN exposes interpretable intermediate representations via its learnable basis functions. Extensive experiments on twelve real-world datasets demonstrate that GraphKAN consistently outperforms state-of-the-art baselines in accuracy, efficiency, and interpretability.", "tldr": "An Efficient and Interpretable Kolmogorov-Arnold Graph Network for Source Detection", "keywords": ["Source Detection", "Social Networks", "Kolmogorov-Arnold Graph Network"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b7bb56d4e84f2ffee6e912b32966acd97f21e31e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "GraphKAN: An Efficient and Interpretable Kolmogorov–Arnold Graph Network for Source Detection, which capitalizes on Kolmogorov–Arnold Networks (KANs) by assigning learnable activation functions to edge weights."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. motivation\n- The design of community-guided sparsity and kernel-adaptive attention is well justified.\n- a compact yet expressive message-passing mechanism.\n\n2. Efficiency\n- GraphKAN achieves lower memory footprint and faster inference while preserving high accuracy.\n\n3. interpretability\n- The visualized spline activations and node-importance scores provide interpretability evidence."}, "weaknesses": {"value": "1. novelty\nSeveral recent studies already combine KANs with GNNs [1,2,3]. This paper should more clearly articulate what is new: e.g., is the contribution primarily the community-guided sparsity, the kernel-adaptive attention, or a new theoretical formulation? A direct experimental comparison with prior Graph-KAN variants is missing.\n\n2. evaluation\nMany datasets are generated using the Independent Cascade (IC) model with only 2% missing states. While controllable, this setting is far cleaner than real social network data. The paper would benefit from testing robustness to higher noise or incomplete observations.\n\n3. sensitivity\nThe effects of community granularity and sparsity parameters (e.g., top-k=5) are not systematically analyzed.\n\n[1] Kiamari, M. et al. GKAN: Graph Kolmogorov-Arnold Networks. arXiv:2406.06470, 2024.\n[2] Carlo Mastropietro et al. Kolmogorov-Arnold Graph Neural Networks. arXiv:2406.18354, 2024.\n[3] Bresson, R. et al. KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning. arXiv:2406.18380, 2024."}, "questions": {"value": "1. Novelty\nHow exactly does GraphKAN differ from existing Graph-KAN or KAN-GNN architectures [1,2,3]? If the difference lies in the community-guided sparse aggregation, can you provide a direct comparison or ablation versus a baseline KAN-GNN without sparsity?\n\n2. sensitivity.\nHow sensitive is performance to the choice of community size or top-k sparsity? Could you provide F1-vs-memory or top-k-vs-accuracy plots across datasets?\n\n3. robustness\nWhat happens when 20% or 50% of node timestamps are missing or randomly perturbed?\n\n4. scalability\nCould you report runtime trends on large graphs (e.g., >10^6 edges) and verify whether observed complexity matches the theoretical O(|E| log |V|) claim?\n\n[1] Kiamari, M. et al. GKAN: Graph Kolmogorov-Arnold Networks. arXiv:2406.06470, 2024.\n[2] Carlo Mastropietro et al. Kolmogorov-Arnold Graph Neural Networks. arXiv:2406.18354, 2024.\n[3] Bresson, R. et al. KAGNNs: Kolmogorov-Arnold Networks meet Graph Learning. arXiv:2406.18380, 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fRYQYuntTw", "forum": "qpHDnFUsJo", "replyto": "qpHDnFUsJo", "signatures": ["ICLR.cc/2026/Conference/Submission24287/Reviewer_QhaF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24287/Reviewer_QhaF"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760537511760, "cdate": 1760537511760, "tmdate": 1762943029474, "mdate": 1762943029474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces GraphKAN, an efficient and interpretable graph neural network for source detection. By integrating learnable B-spline activations from Kolmogorov–Arnold Networks, it enables flexible node feature transformations and adaptive edge attention. Experiments on twelve datasets show that GraphKAN surpasses existing methods in accuracy, efficiency, and interpretability."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed GraphKAN is a new method for source detection that uses the strengths of Kolmogorov–Arnold Networks to overcome the limitations of fixed activations in traditional GNNs, achieving greater flexibility and expressiveness.\n2. The effectiveness of GraphKAN is thoroughly evaluated on a wide range of real-world datasets, with additional interpretability analyses and ablation studies conducted to validate its design and demonstrate its promising performance.\n3. The paper presents clear motivation and background for the problem it addresses, with a well-structured and logically organized presentation."}, "weaknesses": {"value": "1. I think the novelty and contribution of this paper are insufficient for publication at ICLR. The key idea—extending KANs to graphs—is not new, as similar attempts have already appeared in many prior works. Moreover, the main contributions, proposed GraphKAN and its aggregation mechanism, are rather trivial. The paper simply combines several common components, such as state information, positional encodings, and channel-wise aggregation, without providing deeper theoretical analysis or intuition to justify these design choices. Overall, the model appears as a collection of loosely connected modules.\n2. The experimental results in Table 1 show unusually large performance gains, raising concerns that the results may be too good to be true. In addition, the paper does not provide code for reproduction, nor does it report error statistics from multiple experimental runs.\n3. The baselines used in the experiments are somewhat outdated, with only GIN-SD being a recent comparison. The paper should include more up-to-date baselines, such as [1] and [2].\n4. The overall writing quality requires improvement. Please refer to the minor comments below.\n\n[1] Cheng L, Zhu P, Gao C, et al. SDSI: Source Detection in Structurally Incomplete Social Networks[J]. IEEE Transactions on Network Science and Engineering, 2024.\n\n[2] Ali S S, Rastogi A, Anwar T, et al. Generalized Local Prominence for Source Detection in Real-World Rumor Networks[J]. IEEE Transactions on Knowledge and Data Engineering, 2025.\n\n**Minor comments:**  \n(1) Figure 1 lacks sufficient description and explanation in the main paper, making it difficult to understand its intended message.  \n(2) The mathematical notation is inconsistent. For example, Equation (1) uses $T, U$, and $P$ to denote different types of information, while later sections switch to $X^1, X^2, X^3$. Unify these symbols and follow the ICLR template conventions for representing vectors and matrices.  \n(3) The layout on pages 8 and 9 is poorly formatted, with figures and tables clustered together, making it hard to read."}, "questions": {"value": "1. Please begin by responding to the Weaknesses part.\n2. Could you elaborate on the theoretical advantages or underlying intuition of GraphKAN in terms of its expressive power or source detection capability compared with previous methods?\n3. Please explain why the results in Table 1 show such a significant improvement, and also report the standard deviations across multiple runs, as mentioned in the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "29VRktRpET", "forum": "qpHDnFUsJo", "replyto": "qpHDnFUsJo", "signatures": ["ICLR.cc/2026/Conference/Submission24287/Reviewer_di5c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24287/Reviewer_di5c"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760855573798, "cdate": 1760855573798, "tmdate": 1762943029118, "mdate": 1762943029118, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "GraphKAN is an efficient and interpretable Kolmogorov-Arnold graph network for source detection (e.g., rumor tracing). It aims to address issues existing in current GCN-based source detection methods, such as the waste of non-embedding parameters, reliance on fixed activation functions, and poor interpretability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.Across twelve real-world datasets, GraphKAN consistently outperforms state-of-the-art baseline methods in metrics including Accuracy (ACC), F1-score, and Area Under the ROC Curve (AUC). It can detect source nodes in graphs more accurately, establishing a new performance benchmark for source detection tasks. Particularly when dealing with complex network structures and different types of datasets (static networks and cascade datasets), it demonstrates stable and excellent detection capabilities.\n\n2.By exposing interpretable intermediate representations through learnable basis functions (univariate B-spline functions), GraphKAN enables the analysis of learned activation patterns and attention weights, clarifying the relative importance of each node in the source detection process. This breaks the black-box nature of traditional GCN methods, allowing researchers and users to clearly understand the model's decision-making process, enhancing trust in the model's results. It holds significant importance in scenarios requiring traceability and verification of detection results (e.g., responsibility determination for rumor source tracing).\n\n3.The adoption of a sparsity-aware neighborhood aggregation strategy based on community clustering reduces unnecessary message passing and lowers computational complexity."}, "weaknesses": {"value": "1.Over the past year, there have been numerous studies on the integration of graphs and KAN networks. Among them, peer-reviewed works include [1], [2], [3], etc., and non-peer-reviewed ones include [4], [5]. It is noticeable that there are multiple works named \"GraphKAN\". Personally, the authors should consider using a different name for their work.\n\n2.The RELATED WORK section of the manuscript lacks a summary of studies combining graphs and KAN networks. It fails to explain why previous works on the integration of graphs and KAN networks cannot be directly applied to source detection tasks. Additionally, it does not highlight the differences between the authors' work and previous studies.\n\n3.Although hyperparameters are provided in the Experimental Settings section, there is a lack of hyperparameter analysis. For instance, k-max in Equation 14 should be subjected to corresponding hyperparameter analysis. Moreover, the ablation study is also deficient in variants of community detection algorithms.\n\n[1] GraphKAN: An Efficient Graph Kolmogorov Arnold Networks for Traffic Forecasting\n\n[2] G-KAN: Graph Kolmogorov-Arnold Network for Node Classification Using Contrastive Learning\n\n[3] KAGAT: Kolmogorov-Arnold Graph Attention Network\n\n[4] GraphKAN: Graph Kolmogorov Arnold Network for Small Molecule-Protein Interaction Predictions\n\n[5] GraphKAN: Enhancing Feature Extraction with Graph Kolmogorov Arnold Networks"}, "questions": {"value": "1.It is necessary to summarize previous works that combine graphs with KAN, and clarify the advantages of the proposed work compared to these existing studies.\n\n2.In the experimental section, it is advisable to supplement comparisons with one or two works that integrate graphs and KAN.\n\n3.It is expected to add hyperparameter analysis and ablation experiments related to community detection algorithms."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NLTNV3BRss", "forum": "qpHDnFUsJo", "replyto": "qpHDnFUsJo", "signatures": ["ICLR.cc/2026/Conference/Submission24287/Reviewer_KtLa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24287/Reviewer_KtLa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485845492, "cdate": 1761485845492, "tmdate": 1762943028911, "mdate": 1762943028911, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Thank you for the opportunity to review this paper. The paper presents GraphKAN, a graph neural network based on Kolmogorov–Arnold Networks for efficient and interpretable source detection such as rumor tracing. It replaces fixed activation functions in conventional GCNs with learnable B-spline functions to capture flexible, localized nonlinear patterns, and introduces a community-guided sparse aggregation mechanism that focuses message passing within key regions of the graph. Experiments on twelve real-world datasets show that GraphKAN achieves higher accuracy, lower computational cost, and better interpretability than existing approaches."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation and organization quality overall are good and clear.\n2. The proposed framework seems reasonable.\n3. The experiments are comprehensive and complete.\n4. The literatures are comprehensive and complete."}, "weaknesses": {"value": "1. Some parts of presentation can be improved better.\n2. Experimental results may not be convincing.\n3. The code does not release. It may be difficult to reproduce results."}, "questions": {"value": "It is unclear what the key challenge is in applying the KAN model to the graph source detection problem. Specifically, why is incorporating graph topology into KAN difficult, and why can’t conventional GNN frameworks be directly adapted for this purpose?\n\nThe novelty of this paper is somewhat unclear. The authors may consider emphasizing the unique design components or mechanisms introduced to address the aforementioned challenges.\n\nThe experimental results lack information on standard deviations. Were the experiments repeated multiple times? Without this, the reported improvements may not be statistically significant.\n\nThe paper should provide descriptive statistics of the datasets used, such as the number of nodes, edges, and cascades, to help readers better understand the experimental setup. Are these graph large enough to test scalability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TTLJIhmIYG", "forum": "qpHDnFUsJo", "replyto": "qpHDnFUsJo", "signatures": ["ICLR.cc/2026/Conference/Submission24287/Reviewer_G44r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24287/Reviewer_G44r"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24287/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761605345253, "cdate": 1761605345253, "tmdate": 1762943028625, "mdate": 1762943028625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}