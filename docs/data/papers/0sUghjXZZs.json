{"id": "0sUghjXZZs", "number": 13682, "cdate": 1758220844053, "mdate": 1759897419875, "content": {"title": "Bayesian Social Deduction with Graph-Informed Language Models", "abstract": "Social reasoning -- inferring unobservable beliefs and intentions from partial observations of other agents -- remains a challenging task for large language models (LLMs). We evaluate the limits of current reasoning language models in the social deduction game Avalon and find that while the largest models demonstrate strong performance, they require extensive test-time inference and degrade sharply when distilled to smaller, real-time-capable variants. To address this, we introduce a hybrid reasoning framework that externalizes belief inference to a structured probabilistic model, while using an LLM for language understanding and interaction. Our approach achieves performance competitive with much larger models in agent-agent play and, notably, is the first language agent to defeat human players in a controlled study -- achieving a 67% win rate and receiving higher qualitative ratings than both reasoning baselines and human teammates. We release code, models, and a dataset to support future work on social reasoning in LLM agents.", "tldr": "A hybrid method for LLM agents to perform constrained probabilistic reasoning that is able to outperform reasoning LLMs in social deduction games and win against humans", "keywords": ["Social Reasoning", "Social Deduction Games", "Large Language Models", "Neurosymbolic Reasoning", "Bayesian Inference", "Factor Graphs", "Reasoning Models", "Reasoning Scaling", "LLM Agents"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/620a19e54a11aa8b023735d173a819cef398d89c.pdf", "supplementary_material": "/attachment/fcb8fcc7b8a11a208218ca1040bbe3050d14f939.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a neuro-symbolic method for a social deduction game. The proposed GRAIL utilizes a factor graph for Bayesian inference, estimate priors with language models, and propagate belief with factor graph. It augment language model outputs with the computed results.\n\nThe proposed model, with separately trained factor function approximation, outperforms other zero-shot reasoning model baselines, and surpasses human players in both winning rate and qualitative ratings.\n\nFurther analysis shows that the method uses fewer tokens overall. The ablation study demonstrates that both the language model and the graph components are essential for its success, with the graph determining the lower bound of performance.\n\nCombining language models with probabilistic reasoning is an interesting direction. However, the results are not entirely significant, as the model is trained on additional data, and the experiments focused only on one simplified game."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The overall presentation of the paper is clear and well-structured.\n- The proposed integration of language models with probabilistic inference is interesting.\n- The inclusion of both agent-agent and human evaluations makes the work more convincing. The experiments conducted in the Avalon domain are thorough."}, "weaknesses": {"value": "- The use of a trained neural network for factor function approximation reduces the significance of the reported performance gains and makes the comparison with other methods potentially unfair.\n- The experimental scope is limited. It remains unclear whether this approach generalizes to other domains or games. The trained neural network which is a key component of the system, needs to be specifically tailored and trained for each game rather than generalizable.\n- The simplified Avalon setting further limits the significance of the results. As the authors noted, the original version includes different player roles (like Merlin). The GRAIL is also only evaluated as the Good players. This simplification may bias the game toward probabilistic methods rather than language-based social deduction (as also suggested by the ablation results, where the graph component is substantially more important than the language model in GRAIL). Evaluating the method in a more realistic setting could make the experiments more meaningful and the results more significant."}, "questions": {"value": "- Is there a specific reason why the experiments were not conducted in a more realistic setting, such as evaluating GRAIL from both sides, and with different player roles? Also, could you clarify why including Merlin would 'introduce deception', as mentioned in the footnote on page 3?\n\n- My understanding is that modeling other players’ beliefs corresponds to first-order Theory of Mind, whereas modeling an agent’s own beliefs does not. Since GRAIL acts as a player with full access to its own information, is the model in this work actually modeling its own belief? Could you elaborate this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PfV7woYouL", "forum": "0sUghjXZZs", "replyto": "0sUghjXZZs", "signatures": ["ICLR.cc/2026/Conference/Submission13682/Reviewer_k7rN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13682/Reviewer_k7rN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536873103, "cdate": 1761536873103, "tmdate": 1762924243220, "mdate": 1762924243220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GRAIL, a hybrid framework for social deduction games such as Avalon. The proposed model separates linguistic understanding and probabilistic reasoning: an LLM is used to interpret player dialogues and produce coarse-grained “directional priors”, while a factor graph performs Bayesian inference via max-product belief propagation to estimate hidden player roles.\nBased on the inferred beliefs, the agent makes decisions such as party proposals and votes through a simple heuristic policy. The authors evaluate GRAIL against several reasoning-based and non-reasoning LLM baselines, showing improved win rates and greater stability, especially with smaller models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is clearly structured and easy to follow, with a clean separation of sections for modeling, inference, and experiments. The figures and appendices help convey the pipeline intuitively.\n\n2. While the individual components (factor graphs, LLM priors, heuristic decision rules) are standard, their combination into a unified social deduction agent is a novel design choice. The idea of using LLMs only for qualitative “belief direction” is interesting and differs from typical chain-of-thought reasoning.\n\n3. The method is technically sound, and the probabilistic formulation (max-product inference and neural factor approximation) is consistent. The experimental setup on Avalon is well-defined, and the human-agent evaluation adds credibility."}, "weaknesses": {"value": "1. While the hybrid structure is elegant, I am not convinced that delegating the entire reasoning process to an external Bayesian graph is the right direction for long-term progress in social reasoning. Many reasoning chains in social deduction are inherently complex and multi-step, and the need for long chain-of-thought reasoning is not a weakness but a feature of such problems. By removing these steps from the LLM and handling them purely through a pre-defined probabilistic structure, the framework may gain stability but loses the very capacity for nuanced, emergent reasoning that LLMs are increasingly capable of developing. This design feels more like an engineering shortcut than a scalable reasoning principle.\n\n2. The “higher / lower / same” directionality of language priors collapses the rich structure of social interaction into a single scalar. However, dialogue in social deduction games often contains entangled, relational cues (e.g., when one player’s statement implicitly reveals alliances or dependencies). These correlations cannot be expressed by independent prior adjustments on each player. In other words, the linguistic and relational signals are not separable; enforcing such separation risks discarding precisely the information that makes social reasoning interesting.\n\n3. The paper linearly increases β to strengthen priors over rounds, but this schedule appears hand-tuned. It is unclear whether the same parameterization works across different LLMs, languages, or domains. Since β directly controls the interaction strength between the language layer and the factor graph, its sensitivity and robustness deserve deeper analysis—perhaps through a controlled ablation or sensitivity plot.\n\n4. The paper primarily compares against older baselines (ReAct, DeepSeek-R1, etc.). However, recent works have advanced structured reasoning and dynamic workflow modeling for agentic systems—see AFlow (ICLR 2025), DyFlow (NeurIPS 2025), and MaAS (ICML 2025). These models would offer more competitive and conceptually relevant baselines. Without such comparisons, it is difficult to judge whether GRAIL’s improvements arise from genuine reasoning efficiency or simply from task-specific heuristics.\n\n5. The framework assumes that LLMs can meaningfully answer prompts such as “is player X more suspicious than before?”, yet this capability is never validated. It is plausible that these judgments are inconsistent or even random. A simple perturbation test—flipping or shuffling a portion of these qualitative outputs—would reveal whether they actually carry semantic signal. Likewise, evaluating the consistency of priors across models (e.g., GPT-4 vs Llama-70B) could strengthen the empirical grounding."}, "questions": {"value": "Most of my questions are already reflected in the Weaknesses section above, and I would appreciate detailed clarifications or additional experiments addressing those points. \n\nAlthough I remain skeptical about several design choices, I am open to discussion and would be glad to reconsider my evaluation based on the authors’ responses and further evidence provided during the rebuttal phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5d93b0rt91", "forum": "0sUghjXZZs", "replyto": "0sUghjXZZs", "signatures": ["ICLR.cc/2026/Conference/Submission13682/Reviewer_SM4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13682/Reviewer_SM4r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761672726851, "cdate": 1761672726851, "tmdate": 1762924242924, "mdate": 1762924242924, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses the social deduction game Avalon to evaluate and improve LLMs' social reasoning abilities. The authors introduce a hybrid probabilisitic reasoning framework called GRAIL, which achieves competitive performance compared to strong reasoning models and can defeat human players in a controlled study. They also perform thorough analysis on the method with different model families and sizes, allowing the reader to more deeply understand the strengths and limitations of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Social reasoning is an important topic in AI and LLM research, which this work engages with.\n- The proposed method is principled and performs well.\n- This work conducts many kinds of analysis, including on resource usage and hallucination. It tests the method using different models (e.g., Llama and DeepSeek, varying sizes).\n- This work conducts model vs human studies, supporting the effectiveness of the proposed method."}, "weaknesses": {"value": "- This work only studies one game: Avalon. While it is not necessary here to extend GRAIL to other domains, I'd appreciate if the authors include more discussions on where they think GRAIL can also apply to (e.g., other games or social reasoning settings) and where it would face challenges.\n- Why is ReCon an appropriate baseline and in fact the only non-reasoning-model baseline? The authors need to introduce ReCon more (given its current role) and argue why it makes sense here."}, "questions": {"value": "How is this study different from the previous studies that evaluate Avalon gameplay with LLMs, and how is GRAIL different from previous work that applies probabilistic graphical models to social deduction games? The authors do cite relevant work, but have not explicitly articulated what they consider to be significant or novel compared to prior work. This is important for clear paper writing. I would raise my score if the authors address this point."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bR44kbrm5s", "forum": "0sUghjXZZs", "replyto": "0sUghjXZZs", "signatures": ["ICLR.cc/2026/Conference/Submission13682/Reviewer_EHJ9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13682/Reviewer_EHJ9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13682/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762141793484, "cdate": 1762141793484, "tmdate": 1762924242528, "mdate": 1762924242528, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}