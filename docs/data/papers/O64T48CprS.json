{"id": "O64T48CprS", "number": 15461, "cdate": 1758251609015, "mdate": 1763632775748, "content": {"title": "SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning", "abstract": "The rich and multifaceted nature of human social interaction, encompassing multimodal cues, unobservable relations and mental states, and dynamical behavior, presents a formidable challenge for artificial intelligence. To advance research in this area, we introduce SIV-Bench, a novel video benchmark for rigorously evaluating the capabilities of Multimodal Large Language Models (MLLMs) across Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). SIV-Bench features 2,792 video clips and 8,792 meticulously generated question-answer pairs derived from a human-LLM collaborative pipeline. It is originally collected from TikTok and YouTube, covering a wide range of video genres, presentation styles, and linguistic and cultural backgrounds. It also includes a dedicated setup for analyzing the impact of different textual cues—original on-screen text, added dialogue, or no text. Our comprehensive experiments on leading MLLMs reveal that while models adeptly handle SSU, they significantly struggle with SSR and SDP, where Relation Inference (RI) is an acute bottleneck, as further examined in our analysis. Our study also confirms the critical role of transcribed dialogue in aiding comprehension of complex social interactions. By systematically identifying current MLLMs' strengths and limitations, SIV-Bench offers crucial insights to steer the development of more socially intelligent AI.", "tldr": "", "keywords": ["social interaction; social relation; video benchmark; multimodal large language models;"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/235d5d84c9f66464890e356c2e31ed8d72e9bc3c.pdf", "supplementary_material": "/attachment/0cb8344df7f59e506917bf527c8a9a0d845c5911.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SIV-Bench, a video benchmark for evaluating MLLMs on social interaction understanding across three dimensions: Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). The benchmark contains approx 2800 videos from TikTok/YouTube representing 14 relationship types, with approx 8800 QA pairs generated via human-LLM collaboration. Experiments on 10+  models reveal that while models handle SSU adequately, they struggle with SSR and SDP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Important problem: Social interaction understanding is relatively underexplored in video benchmarks\n* Scale and diversity: 2,792 videos across multiple genres, styles and languages\n* Comprehensive evaluation: 10+ models tested (commercial + open-source) with detailed results\n* Thoughtful dataset construction: Human-LLM collaborative pipeline with consensus validation, multiple subtitle conditions, relationship-centric \n* Reproducibility efforts: Detailed prompts, annotation guidelines in appendices"}, "weaknesses": {"value": "1. Consensus based selection and only gold labels provided: social reasoning inherently has multiple views/biases, i.e., different interpretations are often valid and should be included in the dataset; further, butchering the dataset to contain only unanimous decisions makes it less valuable (bcs you only focus on the simple unambiguous cases). This might explain also that SSU→SSR→SDP does not perform increasingly much worse (as expected).\n2. Absence of statistical testing - you confidence interval should be around .9% so differences below that should be treated with care - even if you assume independence among questions and you use 8.8K significance is around .3% again be careful when making claims.\n3. No human baseline so we are not able to contextualize model scores (is 76% good or poor?) - for these social tasks humans still serve as the benchmark\n4. Unvalidated task hierarchy - lacks some factor analysis for the SSU→SSR→SDP claimed chain or regression testing or correlation between subtasks. I would also like to see the microskill decomposition of the tasks/subtasks.\n5. Some mLLMs are used for both generating the data and evaluating performance. This is ok - it just needs to be clarified and carefully disclaimed.\n6. Important competing datasets are downplayed or never mentioned, e.g., Social Genome - need to be clear on what is novel here and do a detailed comparison on why this dataset is novel - claims have to be downplayed a bit in the text."}, "questions": {"value": "1. What is human performance on SIV-Bench?\n2. Can you provide statistical significance tests and confidence intervals for Table 2 results?\n3. Why does Chain-of-Thought show minimal improvement (Table 11)? \n4. What proportion of QAs were discarded due to disagreement?\n5. Can you provide soft labels or multiple valid answers where annotators disagreed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HPhreoLlVD", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Reviewer_A6Cm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Reviewer_A6Cm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776713166, "cdate": 1761776713166, "tmdate": 1762925748432, "mdate": 1762925748432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Responses to Reviewers (Part 1/2)"}, "comment": {"value": "We sincerely thank the reviewers for their thorough evaluation and constructive feedback. We recognize shared concerns regarding benchmark saturation, the need for a human baseline, reasoning depth, and fine-grained failure analysis. Here, we provide two supplementary studies as the global responses:\n\n- An in-depth analysis of a new **SIV-Bench-Hard** subset to establish a human baseline and evaluate reasoning quality beyond simple accuracy (the evaluation data and code on SIV-Bench-Hard are available in https://anonymous.4open.science/r/sivbench-hard-E8CC).\n- A **quantitative fine-grained analysis of the Relation Inference** task to empirically verify failure patterns across 14 relation types.\n\nWe have uploaded a revised version of our paper. All significant additions and modifications to the revised manuscript have been marked **in blue** for easy identification.\n\n> 1: SIV-Bench-Hard\n\nTo address some shared concerns, we conduct a new study. We first curate SIV-Bench-Hard, a challenging subset of **200 videos paired with 200 corresponding QAs** from SIV-Bench. We then benchmark **3 human annotators** and some MLLMs, requiring them to provide not only an answer but also a **free-text reasoning explanation** for their choice. Our results are summarized in the following two tables:\n\n- Table 1 compares the Accuracy:\n\n| Subject | Acc % |\n|---|-|\n| human 1| 67.00|\n| human 2| 65.50 |\n| human 3| 70.50 |\n| Human (Avg.)   | **67.77** |\n| Gemini-3-Pro | 45.50 |\n| GPT-5.1| 39.00 |\n| Gemini-2.5-Pro | 37.00 |\n| Gemini-2.5-Flash | 32.32 |\n| GPT-4o-mini  | 29.00 |\n| Qwen-2.5-vl-7b | 24.50 |\n\n- Table 2 details the Reasoning Quality scores for the MLLMs. To generate the scores, we use gpt-4o-mini as an LLM-Judge, provided with the question, the model's answer, and the 3 corresponding human explanations as a \"gold standard\" reference. We prompt it to score **(1-5)** on five dimensions, including `relevance`, `logical coherence`, `conciseness`, `alignment with human` and `depth of analysis`:\n\n| MODEL | rel  | align | logic | depth | conc | overall |\n|--|--|--|--|--|--|--|\n| Gemini-3-Pro | **4.66** | **3.30** | **4.67** | **3.49** | 4.87 | **4.10** |\n| GPT-5.1 | 4.58 | 3.29 | 4.65 | 3.26 | 4.88 | 4.00 |\n| Gemini-2.5-pro   | 4.57 | 3.26 | 4.65 | 3.41 | 4.89 | 4.05 |\n| Gemini-2.5-flash | 4.48 | 3.17 | 4.55 | 3.22 | 4.87 | 3.95 |\n| gpt-4o-mini      | 4.45 | 3.20 | 4.56 | 3.12 | **4.91** | 3.90 |\n| Qwen-2.5-vl-7b    | 4.00 | 2.89 | 4.21 | 3.05 | 4.45 | 3.63 |\n\nThis new analysis yields three crucial findings:\n\n1. As Table 1 shows, we establish a human baseline of **~67.7%**. In sharp contrast, SOTA MLLM performance collapses on this 'Hard' set. The newly tested **Gemini-3-Pro (released on 18 Nov.)** emerges as the best-performing model, achieving **45.50%**. This result is important because it shows that SIV-Bench can reliably detect real improvements in MLLMs, confirming its value as a diagnostic benchmark. But the performance gap still holds at a $\\sim$**22.2%** difference, highlighting that SIV-Bench correctly identifies the frontier of current AI social interaction understanding.\n\n2. While models score highly on structural metrics like `logical coherence` (4.65) and `conciseness` (4.89), their scores for the cognitive metrics of `alignment with human` ($\\sim$3.3) and `depth of analysis` ($\\sim$3.3) are lower.  This demonstrates that MLLMs are proficient at generating logically structured text, but they are not yet proficient at replicating human-like analytical depth or cognitive processes.\n\n3. Finally, to quantify this difference, we embed all reasoning texts (human and model) with `paraphrase-multilingual-MiniLM-L12-v2`.  A **Random Forest classifier** trained on these embeddings can distinguish between a human and a model explanation with **92.17%** accuracy (vs. 50% random baseline).  Statistical tests (**Mann-Whitney U**) also confirm the distributions are significantly different (**p < 0.05** on 8/10 dimensions). This observed divergence is consistent with findings in **Social Genome** (Mathur et al., 2025), which explicitly reports that generating reasoning traces with high semantic alignment to humans is inherently challenging, with SOTA models typically achieving similarity scores around 0.5.\n\nMathur, Leena, et al. \"Social genome: Grounded social reasoning abilities of multimodal models.\" arXiv preprint arXiv:2502.15109 (2025)."}}, "id": "FpyT6WOWNb", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763631688162, "cdate": 1763631688162, "tmdate": 1763631838444, "mdate": 1763631838444, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SIV-Bench, a new video QA benchmark designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to understand and reason about complex human social interactions. The dataset is diverse, covering various genres, presentation styles, and cultural backgrounds. The authors conducted comprehensive experiments on leading MLLMs and found that while models perform well on the foundational SSU (Social Scene Understanding) tasks, they significantly struggle with the more complex SSR (Social State Reasoning) and SDP (Social Dynamics Prediction) tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The dataset's scale and diversity are significant strengths. The curation across 14 distinct social relationship types, multiple languages, and varied cultural contexts provides a robust foundation for social reasoning evaluation.\n- The paper provides a thorough evaluation of a wide range of MLLMs. The inclusion of a comparative analysis of how audio and subtitles affect performance is also interesting and valuable."}, "weaknesses": {"value": "- The reliance on an automated pipeline (LLM-generated QAs and distractors) raises concerns about the benchmark's true difficulty and potential for shortcut learning. A large portion of the questions (3,273) are chosen from those that are answered correctly by Gemini-2.0-Flash, Gemini-2.0-Pro, and GPT-4o-mini. However, this also indicates that the questions are easier to answer. The paper also mentions limited adversarial filtering (e.g., removing questions solvable without video), which does not account for potential biases from the distractor generation model that cause questions to be easily answerable with superficial visual cues. Finally, while the authors used GPT to normalize option style, this falls short of more rigorous adversarial debiasing modules to mitigate superficial vision-text shortcuts. \n- The above concern about dataset difficulty is supported by the high accuracy of SOTA models, suggesting the benchmark may already be approaching saturation. For example, Gemini-2.5-Pro achieves over 90% accuracy on the SSU category and 70-75% on the SSR and SDP tasks. \n- In light of these high accuracies, the authors did not provide a human baseline, making the numbers hard to interpret. A 75% accuracy on could be near human-level performance (indicating limited room for improvement) or it could be well below it (indicating the benchmark is valid and challenging)."}, "questions": {"value": "- While I appreciate the dataset's scale and diversity, I am concerned about the quality and saturation of the dataset that could be due to the automated generation pipeline. Could the authors provide a human baseline as reference to ensure that the benchmark is not saturating?\n- The 14 distinct social relationship types are an important component of the benchmark's design. Do the authors have an analysis on model performance broken down by these 14 relationship types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bn7AyF68ov", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Reviewer_ZmxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Reviewer_ZmxA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862776762, "cdate": 1761862776762, "tmdate": 1762925747900, "mdate": 1762925747900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SIV-Bench is a video benchmark for social interaction understanding which is organized along three capabilities: Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). It contains 2,792 short videos collected from TikTok or YouTube, and 8,728 multiple-choice QAs built through a human–LLM collaborative pipeline. The benchmark is relation-centric, grounded in Fiske’s Relational Models Theory (4 model families instantiated as 14 relation types), and ships three subtitle conditions—origin, +sub (transcribed & translated dialogue), and –sub (on-screen text removed)—to study the role of linguistic cues. Experiments cover open/closed-source MLLMs; top systems excel at SSU but struggle on SSR (especially Relation Inference) and, to a lesser extent, SDP; subtitles and audio help, with Gemini-2.5-Pro peaking at 76.50% overall with +sub."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Relation-centric design grounded in Fiske’s theory with fourteen relation types; aligns mental state, perception, and dynamics to the social context.\n2. Diverse, real-world videos with multilingual presence; three subtitle conditions enable language-cue studies, which are controlled.\n3. The work is also equipped with video-dependence check, model-consensus filtering, and difficulty curation, which contribute a lot.\n4. Last but not least, standardized prompting or parsing, broad model set, and analyses (subtitle or audio ablations; fine-grained task radar; failure patterns)."}, "weaknesses": {"value": "1. Video-capable models get raw videos; image-only models receive 16 frames, risking budget-driven gaps; per-model frame/FLOP parity wasn’t normalized.\n2. Human verification is described, but inter-annotator agreement ( for instance, κ) isn’t reported; this matters for subtle social labels.\n3. Heavy use of LLMs for QA creation, filtering, and option standardization could induce stylistic priors; leakage/over-templating risks need auditing.\n4. Accuracy seems to be the only metric; also, no per-relation calibration curves or human baselines for comparison on SSR/SDP are provided. In addition, failure analysis is qualitative."}, "questions": {"value": "1. Can you provide an ablation with fixed frames/FLOPs/tokens across all models (including closed-source) to isolate modeling effects from budget differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QnZ41Nl0JU", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Reviewer_AQ4j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Reviewer_AQ4j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972665180, "cdate": 1761972665180, "tmdate": 1762925747601, "mdate": 1762925747601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SIV-Bench, a video benchmark for social interaction understanding that evaluates MLLMs along three dimensions: Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP), further decomposed into 10 fine-grained tasks (e.g., action/expression recognition, relation/intent/emotion inference, factual/counterfactual prediction). The dataset contains 2792 TikTok/YouTube clips and 8792 MCQs created via a human–LLM collaborative pipeline, with built-in controls to analyze the role of language through three subtitle settings: original, +sub (transcribed/translated dialogue added), and −sub (on-screen text removed). Experiments using VLMEvalKit show that models are relatively strong on SSU but struggle on SSR/SDP with Relation Inference the most consistent bottleneck."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- While many video benchmarks are action- or object-centric (e.g., classic action recognition and general video QA suites), human social interaction—relations, implicit mental states, and interaction dynamics—remains underrepresented. SIV-Bench explicitly fills this gap by defining a people-centered evaluation space (SSU/SSR/SDP), curating original, relation-typed videos (14 relation types), and providing subtitle/audio signals to probe multimodal social reasoning; this dataset definition and collection are, in themselves, a substantive contribution.\n\n- Clear empirical takeaways. Models are relatively strong on SSU but struggle on SSR/SDP, with Relation Inference the most consistent bottleneck; transcribed subtitles measurably help."}, "weaknesses": {"value": "- While the multiple-choice QA format simplifies evaluation and boosts reliability, it likely underrepresents the open-ended, interactive social reasoning required by agents. Expanding the benchmark with generative or dialog-based tasks (e.g., free-form rationales reasoning, intent explanations, multi-turn interactions) would provide a deeper probe of interactive social understanding."}, "questions": {"value": "- Several SSR failures involve decoding why people react as they do. The SMILE dataset [1] introduces Video Laugh Reasoning and text explanations for why people laugh in social videos. Could SIV-Bench incorporate this kind of reasoning task to probe this facet of social affect reasoning?\n\n\n[1] https://arxiv.org/pdf/2312.09818"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ePbupOT4PO", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Reviewer_Vx9j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Reviewer_Vx9j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087553094, "cdate": 1762087553094, "tmdate": 1762925747242, "mdate": 1762925747242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}