{"id": "O64T48CprS", "number": 15461, "cdate": 1758251609015, "mdate": 1759897305588, "content": {"title": "SIV-Bench: A Video Benchmark for Social Interaction Understanding and Reasoning", "abstract": "The rich and multifaceted nature of human social interaction, encompassing multimodal cues, unobservable relations and mental states, and dynamical behavior, presents a formidable challenge for artificial intelligence. To advance research in this area, we introduce SIV-Bench, a novel video benchmark for rigorously evaluating the capabilities of Multimodal Large Language Models (MLLMs) across Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). SIV-Bench features 2,792 video clips and 8,792 meticulously generated question-answer pairs derived from a human-LLM collaborative pipeline. It is originally collected from TikTok and YouTube, covering a wide range of video genres, presentation styles, and linguistic and cultural backgrounds. It also includes a dedicated setup for analyzing the impact of different textual cues—original on-screen text, added dialogue, or no text. Our comprehensive experiments on leading MLLMs reveal that while models adeptly handle SSU, they significantly struggle with SSR and SDP, where Relation Inference (RI) is an acute bottleneck, as further examined in our analysis. Our study also confirms the critical role of transcribed dialogue in aiding comprehension of complex social interactions. By systematically identifying current MLLMs' strengths and limitations, SIV-Bench offers crucial insights to steer the development of more socially intelligent AI.", "tldr": "", "keywords": ["social interaction; social relation; video benchmark; multimodal large language models;"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8bcd5cbb5d4952f00fa70b41b7601359643e37d4.pdf", "supplementary_material": "/attachment/0cb8344df7f59e506917bf527c8a9a0d845c5911.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces SIV-Bench, a video benchmark for evaluating MLLMs on social interaction understanding across three dimensions: Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). The benchmark contains approx 2800 videos from TikTok/YouTube representing 14 relationship types, with approx 8800 QA pairs generated via human-LLM collaboration. Experiments on 10+  models reveal that while models handle SSU adequately, they struggle with SSR and SDP."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Important problem: Social interaction understanding is relatively underexplored in video benchmarks\n* Scale and diversity: 2,792 videos across multiple genres, styles and languages\n* Comprehensive evaluation: 10+ models tested (commercial + open-source) with detailed results\n* Thoughtful dataset construction: Human-LLM collaborative pipeline with consensus validation, multiple subtitle conditions, relationship-centric \n* Reproducibility efforts: Detailed prompts, annotation guidelines in appendices"}, "weaknesses": {"value": "1. Consensus based selection and only gold labels provided: social reasoning inherently has multiple views/biases, i.e., different interpretations are often valid and should be included in the dataset; further, butchering the dataset to contain only unanimous decisions makes it less valuable (bcs you only focus on the simple unambiguous cases). This might explain also that SSU→SSR→SDP does not perform increasingly much worse (as expected).\n2. Absence of statistical testing - you confidence interval should be around .9% so differences below that should be treated with care - even if you assume independence among questions and you use 8.8K significance is around .3% again be careful when making claims.\n3. No human baseline so we are not able to contextualize model scores (is 76% good or poor?) - for these social tasks humans still serve as the benchmark\n4. Unvalidated task hierarchy - lacks some factor analysis for the SSU→SSR→SDP claimed chain or regression testing or correlation between subtasks. I would also like to see the microskill decomposition of the tasks/subtasks.\n5. Some mLLMs are used for both generating the data and evaluating performance. This is ok - it just needs to be clarified and carefully disclaimed.\n6. Important competing datasets are downplayed or never mentioned, e.g., Social Genome - need to be clear on what is novel here and do a detailed comparison on why this dataset is novel - claims have to be downplayed a bit in the text."}, "questions": {"value": "1. What is human performance on SIV-Bench?\n2. Can you provide statistical significance tests and confidence intervals for Table 2 results?\n3. Why does Chain-of-Thought show minimal improvement (Table 11)? \n4. What proportion of QAs were discarded due to disagreement?\n5. Can you provide soft labels or multiple valid answers where annotators disagreed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HPhreoLlVD", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Reviewer_A6Cm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Reviewer_A6Cm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761776713166, "cdate": 1761776713166, "tmdate": 1762925748432, "mdate": 1762925748432, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SIV-Bench, a new video QA benchmark designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to understand and reason about complex human social interactions. The dataset is diverse, covering various genres, presentation styles, and cultural backgrounds. The authors conducted comprehensive experiments on leading MLLMs and found that while models perform well on the foundational SSU (Social Scene Understanding) tasks, they significantly struggle with the more complex SSR (Social State Reasoning) and SDP (Social Dynamics Prediction) tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The dataset's scale and diversity are significant strengths. The curation across 14 distinct social relationship types, multiple languages, and varied cultural contexts provides a robust foundation for social reasoning evaluation.\n- The paper provides a thorough evaluation of a wide range of MLLMs. The inclusion of a comparative analysis of how audio and subtitles affect performance is also interesting and valuable."}, "weaknesses": {"value": "- The reliance on an automated pipeline (LLM-generated QAs and distractors) raises concerns about the benchmark's true difficulty and potential for shortcut learning. A large portion of the questions (3,273) are chosen from those that are answered correctly by Gemini-2.0-Flash, Gemini-2.0-Pro, and GPT-4o-mini. However, this also indicates that the questions are easier to answer. The paper also mentions limited adversarial filtering (e.g., removing questions solvable without video), which does not account for potential biases from the distractor generation model that cause questions to be easily answerable with superficial visual cues. Finally, while the authors used GPT to normalize option style, this falls short of more rigorous adversarial debiasing modules to mitigate superficial vision-text shortcuts. \n- The above concern about dataset difficulty is supported by the high accuracy of SOTA models, suggesting the benchmark may already be approaching saturation. For example, Gemini-2.5-Pro achieves over 90% accuracy on the SSU category and 70-75% on the SSR and SDP tasks. \n- In light of these high accuracies, the authors did not provide a human baseline, making the numbers hard to interpret. A 75% accuracy on could be near human-level performance (indicating limited room for improvement) or it could be well below it (indicating the benchmark is valid and challenging)."}, "questions": {"value": "- While I appreciate the dataset's scale and diversity, I am concerned about the quality and saturation of the dataset that could be due to the automated generation pipeline. Could the authors provide a human baseline as reference to ensure that the benchmark is not saturating?\n- The 14 distinct social relationship types are an important component of the benchmark's design. Do the authors have an analysis on model performance broken down by these 14 relationship types?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Bn7AyF68ov", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Reviewer_ZmxA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Reviewer_ZmxA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761862776762, "cdate": 1761862776762, "tmdate": 1762925747900, "mdate": 1762925747900, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SIV-Bench is a video benchmark for social interaction understanding which is organized along three capabilities: Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP). It contains 2,792 short videos collected from TikTok or YouTube, and 8,728 multiple-choice QAs built through a human–LLM collaborative pipeline. The benchmark is relation-centric, grounded in Fiske’s Relational Models Theory (4 model families instantiated as 14 relation types), and ships three subtitle conditions—origin, +sub (transcribed & translated dialogue), and –sub (on-screen text removed)—to study the role of linguistic cues. Experiments cover open/closed-source MLLMs; top systems excel at SSU but struggle on SSR (especially Relation Inference) and, to a lesser extent, SDP; subtitles and audio help, with Gemini-2.5-Pro peaking at 76.50% overall with +sub."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Relation-centric design grounded in Fiske’s theory with fourteen relation types; aligns mental state, perception, and dynamics to the social context.\n2. Diverse, real-world videos with multilingual presence; three subtitle conditions enable language-cue studies, which are controlled.\n3. The work is also equipped with video-dependence check, model-consensus filtering, and difficulty curation, which contribute a lot.\n4. Last but not least, standardized prompting or parsing, broad model set, and analyses (subtitle or audio ablations; fine-grained task radar; failure patterns)."}, "weaknesses": {"value": "1. Video-capable models get raw videos; image-only models receive 16 frames, risking budget-driven gaps; per-model frame/FLOP parity wasn’t normalized.\n2. Human verification is described, but inter-annotator agreement ( for instance, κ) isn’t reported; this matters for subtle social labels.\n3. Heavy use of LLMs for QA creation, filtering, and option standardization could induce stylistic priors; leakage/over-templating risks need auditing.\n4. Accuracy seems to be the only metric; also, no per-relation calibration curves or human baselines for comparison on SSR/SDP are provided. In addition, failure analysis is qualitative."}, "questions": {"value": "1. Can you provide an ablation with fixed frames/FLOPs/tokens across all models (including closed-source) to isolate modeling effects from budget differences?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QnZ41Nl0JU", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Reviewer_AQ4j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Reviewer_AQ4j"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972665180, "cdate": 1761972665180, "tmdate": 1762925747601, "mdate": 1762925747601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents SIV-Bench, a video benchmark for social interaction understanding that evaluates MLLMs along three dimensions: Social Scene Understanding (SSU), Social State Reasoning (SSR), and Social Dynamics Prediction (SDP), further decomposed into 10 fine-grained tasks (e.g., action/expression recognition, relation/intent/emotion inference, factual/counterfactual prediction). The dataset contains 2792 TikTok/YouTube clips and 8792 MCQs created via a human–LLM collaborative pipeline, with built-in controls to analyze the role of language through three subtitle settings: original, +sub (transcribed/translated dialogue added), and −sub (on-screen text removed). Experiments using VLMEvalKit show that models are relatively strong on SSU but struggle on SSR/SDP with Relation Inference the most consistent bottleneck."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- While many video benchmarks are action- or object-centric (e.g., classic action recognition and general video QA suites), human social interaction—relations, implicit mental states, and interaction dynamics—remains underrepresented. SIV-Bench explicitly fills this gap by defining a people-centered evaluation space (SSU/SSR/SDP), curating original, relation-typed videos (14 relation types), and providing subtitle/audio signals to probe multimodal social reasoning; this dataset definition and collection are, in themselves, a substantive contribution.\n\n- Clear empirical takeaways. Models are relatively strong on SSU but struggle on SSR/SDP, with Relation Inference the most consistent bottleneck; transcribed subtitles measurably help."}, "weaknesses": {"value": "- While the multiple-choice QA format simplifies evaluation and boosts reliability, it likely underrepresents the open-ended, interactive social reasoning required by agents. Expanding the benchmark with generative or dialog-based tasks (e.g., free-form rationales reasoning, intent explanations, multi-turn interactions) would provide a deeper probe of interactive social understanding."}, "questions": {"value": "- Several SSR failures involve decoding why people react as they do. The SMILE dataset [1] introduces Video Laugh Reasoning and text explanations for why people laugh in social videos. Could SIV-Bench incorporate this kind of reasoning task to probe this facet of social affect reasoning?\n\n\n[1] https://arxiv.org/pdf/2312.09818"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ePbupOT4PO", "forum": "O64T48CprS", "replyto": "O64T48CprS", "signatures": ["ICLR.cc/2026/Conference/Submission15461/Reviewer_Vx9j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15461/Reviewer_Vx9j"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15461/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762087553094, "cdate": 1762087553094, "tmdate": 1762925747242, "mdate": 1762925747242, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}