{"id": "70fI4ptT4b", "number": 14265, "cdate": 1758231490074, "mdate": 1759897380097, "content": {"title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback", "abstract": "For many real-world applications, feedback is essential in translating complex, nuanced, or subjective goals into quantifiable optimization objectives. We propose a language-in-the-loop framework that uses a large language model (LLM) to convert unstructured feedback in the form of natural language into scalar utilities to conduct BO over a numeric search space.  \nUnlike preferential BO that accepts only restricted comparison formats and requires carefully engineered customized models to handle problem-specific domain knowledge, our approach leverages LLMs to translate heterogeneous textual critiques into consistent utility signals and incorporate flexible user priors without manual kernel crafting, while still being able to retain the sample efficiency and principled uncertainty quantification of BO. We show that this hybrid method not only provides a more natural interface to the decision maker but also outperforms conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes.", "tldr": "This work introduces a novel language-in-the-loop framework that leverages large language models to transform free-form textual feedback into scalar utilities, enabling principled bayesian optimization with Gaussian Processes.", "keywords": ["bayesian optimization", "large language models", "in-context learning", "experimental design"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/47c75b7ab0398cb175042bdee18bb785ca81461e.pdf", "supplementary_material": "/attachment/51febf77ac2ab74cc7b4880038a39e1660342443.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a natural language preference elicitation method where: 1) two Gaussian processes (GPs) are used for belief maintenance over candidate utilities, 2) a noisy greedy acquisition function (\"noisy expected improvement\") selects a batch of candidates, 3) an LLM uses the selected batch to generate a set of preference eliciting natural language questions 4) a greedy acquisition function (\"expected value of best option\") selects (pairs of) candidates to get LLM judgements over. Based on experiments performed with an LLM user simulator and several underlying utility functions, the authors report improvements in utility maximization over LLM-only baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The authors aim to address the important research problems of active LLM-driven preference elicitation (PE), how to balancing exploration and exploitation in natural language PE, and how to combine LLM-based methods with established PE methods such as GP PE bandits. \n- The authors perform numerous simulations across several problem domains such as \"Vehicle Safety\", \"Thermal Comfort\", and \"Car Cab\".\n- The insight that using LLMs for pairwise comparison improved performance vs pointwise (scalar) judgments is interesting."}, "weaknesses": {"value": "- The motivation for using two GPs (195, 196) for 1) mapping \\mathcal{X} to a utility value (a real number) and 2) mapping y = f(x) to a utility value is not at all clear, the large existing volume of work on Bayesian optimization practically always relies on 1) only and the reason why the authors think it is a good idea to deviate from this setting does not come across.\n- It is not clear how the two GPs interact if at all for belief maintenance and candidate acquisition.\n- The pairwise GP (253) is not defined anywhere, it is just referenced -- the paper should be mostly self contained. \n- The candidate selection acquisition function (209), \"Noisy Expected Improvement\", which appears to just be the noisy greedy acquisition function, is never formally defined. It seems to be adapted to select a batch of candidates as opposed to a conventional acquisition function which selects one candidate, but this adaptation is not discussed. The same comment goes for the EUBO acquisition function (249) for LLM-judge candidates, which appears to be the greedy function adapted for a batch of pairs.   \n- Only one acquisition function per candidate selection step (one for question generation candidates (209), one for LLM-judge candidates (249)) is tested and discussed -- there are many alternatives such as UCB, entropy reduction, and Thomson sampling which are extremely common.\n- Many similarities to [1] which is never referenced. \n\n[1] Handa, Kunal, et al. \"Bayesian preference elicitation with language models.\" arXiv preprint arXiv:2403.05534 (2024)."}, "questions": {"value": "- The \"black-box\" function in standard Bayesian Optimization for PE is the user's utility over the candidate set \"\\mathcal{X}\". Why is there a need to introduce another function f(x) and model it with a second GP?\n- What are the formal definitions of a pairwise GP (including belief updates) and the acquisition functions?\n- Were other acquisition functions could be explored, for both candidate selection for question generation and for selecting candidates for LLM evaluation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "QUn40L2XWX", "forum": "70fI4ptT4b", "replyto": "70fI4ptT4b", "signatures": ["ICLR.cc/2026/Conference/Submission14265/Reviewer_EgVp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14265/Reviewer_EgVp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761233328670, "cdate": 1761233328670, "tmdate": 1762924717524, "mdate": 1762924717524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In the context of preferential Bayesian optimization (PBO), the paper proposes an algorithm that incorporates an LLM to improve optimization performance. From the main paper (e.g. Algorithm 1), it is clear that the LLM acts an acquisition function in the PBO loop and an assistant of communicating justifications of DM’s preferential feedback, but also as an auxiliary preferential feedback oracle giving directly the DM’s expected choice probabilities (Algorithm 3).  The paper demonstrates the improved performance and sample efficiency compared to various baselines on a variety of semi-synthetic environments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles an interesting and timely problem of incorporating large language model capabilities into the preferential Bayesian optimization loop. The paper provides a comprehensive solution to the problem involving end-to-end interaction loop with DM (domain expert) and the experimentation black-box, where the LLM acts in various roles, most importantly processing textual justification data about the DM’s preferences. The experimental section examines various baselines and setups, demonstrating improvements over both pure PBO methods and pure LLM-based baselines."}, "weaknesses": {"value": "I did not find any discussion of potential limitations of using LLMs to directly provide probabilistic judgments, that is the choice probability p_k (e.g. see Prompt 4). In the context of confidence elicitation in LLMs, there is important related work studying this aspect (Xiong et al., 2024). It seems that the proposed approach adopts so-called “black-box method” in contrast to “white-box method” (Xiong et al., 2024). Kadavath et al. (2022) proposed using token-level log-probabilities to estimate the LLM probabilities, i.e. relying on white-box access to internal LLM information (Xiong et al., 2024). A natural question arises: Why the paper adopted black-box method, while Llama is an open-source model, and more calibrated white-box method could have been adopted (Xiong et al., 2024)?\n“In this paper, we introduce Language-in-the-loop Optimization (LILO), a framework designed to combine the complementary strengths of BO and LLMs while avoiding their respective weaknesses”.. I think this claim is only partly true. For example, directly prompting an LLM to provide a probability estimate is not one of its strengths (e.g. Kapoor et al., 2024).\n\nThe proposed method is an algorithmic patchwork, and the presentation lacks consistency and a well-balanced level of abstraction. As a concrete example, let us consider an apparent inconsistency in the main paper:\n\nLines 250-252: “For each pair (y_k,y_k’), the LLM determines which outcome is more aligned with the DM’s preference, producing a label p_k \\in {\\0,1\\}  indicating whether or not y_k is preferred over y_k’”. \nLines 163-184 (Algorithm 1): There is no mention LLM making preference judgment nor any mention of data p_k. There is only mention of DM.get_answers, which refers to the decision maker (DM). \n\nHowever, a deeper look into the appendices reveals that the main component of the method (i.e. as the abstract states: “a large language model (LLM) to convert unstructured feedback in the form of natural language into scalar utilities”) is found in Appendix Algorithm 3 “fit proxy models” where it is clear that LILO.get_pairwise_pref (i.e. LLM) gives directly the probability p_k by prompting. \nThen, by digging further one can find that in Appendix B.3 “LLM-based simulation of the human preference feedback” there is discussion on using LLM as a DM proxy in the experiments. Still, some important details to evaluate the validity of the empirical experiments are buried in the code (Lines 1277-1279): “For the exact utility-specific versions of this prompt we refer the readers to our code which is made available as part of this submission in the supplementary materials.” \n\nIn summary, the manuscript would greatly benefit from revision that places more focus on the claimed main contributions such as “We show how to translate such natural language feedback into quantitative latent utilities that can be used effectively by a surrogate and acquisition function, systematically exploring the design choices required to render this approach both effective and practical.”\n\nReferences\n\nKadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., ... & Kaplan, J. (2022). Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.\n\nKapoor, S., Gruver, N., Roberts, M., Collins, K., Pal, A., Bhatt, U., ... & Wilson, A. G. (2024). Large language models must be taught to know what they don’t know. Advances in Neural Information Processing Systems, 37, 85932-85972.\n\nXiong, M., Hu, Z., Lu, X., LI, Y., Fu, J., He, J., & Hooi, B. Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. In The Twelfth International Conference on Learning Representations."}, "questions": {"value": "For preferential BO community, an interesting question is how LLM’s estimates about DM’s choice probabilities used to fit the GP surrogate for the utility function. In the last lines of Algorithm 3, I can find lines “fit_pairwise_gp”, but do not find in the paper what this subroutine does. Do you treat p_k as a scalar value and just GP regression, or do you use it in place of preference likelihood or what?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ETuTFOuRnu", "forum": "70fI4ptT4b", "replyto": "70fI4ptT4b", "signatures": ["ICLR.cc/2026/Conference/Submission14265/Reviewer_B7nh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14265/Reviewer_B7nh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761740371235, "cdate": 1761740371235, "tmdate": 1762924717012, "mdate": 1762924717012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Language-in-the-loop Optimization (LILO), a novel framework which integrates Bayesian Optimization (BO) with natural language feedback from a human decision-maker (DM). The key idea is to use a LLM as a translator component within a standard BO loop. LILO allows the DM to provide free-form text feedback, and then a LLM then interprets this text feedback to generate pairwise preference labels for observed outcomes. The LLM-generated labels are used to train a Gaussian Process (GP) preference model, which in turn guides the BO acquisition function to select new candidates. Experiments on several tasks (DTLZ2, Vehicle Safety, etc.) show that the proposed method outperforms both LLM-only optimizers (which lack principled uncertainty quantification) and traditional BO methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper identifies a limitation in standard and preferential BO. Using natural language rather single scalar values is intuitive to formulate complex real-work objectives. The study is well-motivated.\n\nThe design of using a LLM in the BO framework is reasonable, and the empirical finding is promising. Also, the ablation study provides useful observations, such as pairwise comparisons provide more reliable utility estimates than direct scalar predictions."}, "weaknesses": {"value": "The experimental results are from simulated environments, which does not reflect the real-world challenges and makes the strong claims not convincing.\n\nIn each iteration, true utility BO receives $B^{pf} = 2$ scalar outcomes, while LILO receives $2$ text natural language answers. As also argued by the authors, the text feedback is much richer. Therefore, the comparison seems not fair. A fairer comparison would be against a preferential BO baseline given a larger budget of pairwise comparisons, or a true utility BO with larger $B^{pf}$."}, "questions": {"value": "Please see the weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rs0W14y6O7", "forum": "70fI4ptT4b", "replyto": "70fI4ptT4b", "signatures": ["ICLR.cc/2026/Conference/Submission14265/Reviewer_BnvK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14265/Reviewer_BnvK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187131710, "cdate": 1762187131710, "tmdate": 1762924716334, "mdate": 1762924716334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes LILO, a BO pipeline that: (i) converts natural-language feedback into pairwise preferences using an LLM, (ii) fits two GP surrogates—$M_y:Y\\rightarrow U$ from labeled outcome pairs and $M_x: X\\rightarrow U$ for candidate selection—and (iii) uses LogNEI to pick new experiments and EUBO to choose outcome pairs for labeling; an optional “language prior” can warm-start round 1."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper is easy to follow and the writing is clear. The proposed method is easy to reproduce and analyze."}, "weaknesses": {"value": "1. Evaluation is dated and narrow. Core results are on four legacy simulators: DTLZ2 (2002), Vehicle Safety, Car Cab Design (2008), and Thermal Comfort (1970/2005). There’s no modern HPO or realistic high-dimensional outcome task (e.g., image/text) despite the paper’s motivation. The paper even acknowledges applicability to high-dimensional outcomes without demonstrating it.\n\n2. the DM prompt contains ground-truth utilities $g(y_i)$ or observed outcomes—an unrealistic advantage for a human and a potential leakage channel shaping the language. e.g., the paper says line 342-344, \"For methods involving natural language feedback, answers to questions posed by the LLM agent are simulated with another LLM containing a textual description of the ground-truth utility function\nin the prompt \". And in prompt 7, The prompt tells the DM:“You have observed the following outcomes with their corresponding utility values and contributions to the overall utility.” \n\n3. Short horizon and heavy feedback budget: The loop runs only T=8 rounds with \n$B_{exp}=d$ and $B_{pf}=2$, while internally labeling K=64 pairs per round—an unusually large annotation budget relative to very few optimization steps, which may not reflect real-world costs.\n\n4. Missing baselines: The paper compares to oracle true-utility BO, preferential BO, and two LLM-only variants (one “LLAMBO-like”). That’s a fair start, but it omits competitive BO+LLM or preference-BO variants such as embedding-based surrogates over language/strings [1], best-of-k preference-BO (top-k ranking) [2], and multi-objective preference-BO with learned/implicit scalarizations [3]—each of which would stress whether LILO’s dual-GP design is really needed.\n\n5. The paper lacks novelty: the optimization math (GP surrogates, preference likelihood, EUBO/NEI acquisitions) is standard BO/PBO; the main contribution is a well-engineered placement of an LLM to turn text into pairwise labels.\n\n---\n\n[1] Nguyen et al., Language Model Embeddings Can Be Sufficient for Bayesian Optimization, 2024.\n\n[2] Nguyen et al., Top-k Ranking Bayesian Optimization, AAAI 2021.\n\n[3] Ozaki et al., Preferential Multi-Objective Bayesian Optimization, 2024."}, "questions": {"value": "What safeguards ensure the DM’s responses don’t implicitly encode the exact utility (given that the DM was shown g(y) and that LILO doesn’t overfit to such patterns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mEEOA7ph82", "forum": "70fI4ptT4b", "replyto": "70fI4ptT4b", "signatures": ["ICLR.cc/2026/Conference/Submission14265/Reviewer_Q6Ph"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14265/Reviewer_Q6Ph"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762286636243, "cdate": 1762286636243, "tmdate": 1762924715821, "mdate": 1762924715821, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the human-in-the-loop Bayesian optimisation problem by introducing large language models (LLMs) as a user-friendly interface to facilitate and accelerate preference learning. Two surrogate models are proposed that seperately capture the mappings from inputs and  outputs to the preference space. A batched acquisition function is employed to generate evaluation candidates, after which the LLM elicits user preferences and potential narrative explanations. Another acquisition function selects the top-K informative pairwise comparisons, with preference labels inferred from the LLM ouputs. Across multiple benchmarks tasks, the proposed approach demonstrates competitive performance compared with classical BO, preference-based BO, and LLM-assisted optimisation baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**Significance:** The topic is increasingly important. Human-in-the-loop optimisation represents a crucial direction for advancing domains such as scientific discovery (aligning with expert knowledge) and healthcare (addressing individual needs).\n\n**Clarity:** The paper is clearly written and structured. The proposed approach is presented with sufficient detail and generality, making it adaptable to other optimisation frameworks. The illustrations (e.g., Figure 2, Example 1, Figure 3) effectively support understanding of the methodology.\n\n**Orignality:** The introduction of a two-surrogate framework that leverages LLMs both as an interactive interface and as preference labellers appears original."}, "weaknesses": {"value": "The contribution of the paper appears limited:\n1. Most algorithmic components, including the Gaussian process (GP) models and acquisition functions, are well established in prior work, as acknowledged in the paper.\n2. The proposed integration framework lacks sufficient technical discussion or analysis regarding its robustness and convergence properties. In particular, the method may be fragile to biased or inconsistent LLM outputs, and it is unclear whether the optimisation reliably converges to high-utility solutions.\n3. As the study involves multi-dimensional outputs, it remains uncertain whether LLMs can effectively handle multi-objective scenarios, such as those requiring Pareto-optimal trade-offs or modeling of decision-makers' stochastic/utopian preferences [1].\n\nThe experiment assessment is underdeveloped: The choice of limiting each run to only eight iterations is not well justified. Either a large number of iterations or an explicit analysis of the convergence gap to the optimal utility value would be necessary to demonstrate whether the method achieves a reasonable exploration-exploitation balance (an essential principle of Bayesian optimisation) rather than becoming trapped in suboptimal regions.\n\nThe experimental comparisons may not be entirely fair: For instance, in one query (line 227, Example 1), the decisionmaker (DM) is required to prioritise multiple metrics simultaneously. This imposes a substantially higher cogvitive load than a standard pairwise comparison. The paper should therefore account for DM workload beyond simple query counts to ensure a balanced and meaningful comparison between methods.\n\n[1] Direct Preference-Based Evolutionary Multi-Objective Optimization with Dueling Bandits, NeurIPS 2024."}, "questions": {"value": "1. What is the key distinguishing aspect from prior work?\n\n2. How can a fairer experimental comparison be achieved in terms of DM workload?\n\n3. Could you include experiments with a larger number of iterations and report the optimal utility values?\n\n4. How does the choice of $B^{\\text{pf}}$ affect performance on other tasks beyond the numerical DTLZ2 benchmark (Figure 6)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9hUS5crE3d", "forum": "70fI4ptT4b", "replyto": "70fI4ptT4b", "signatures": ["ICLR.cc/2026/Conference/Submission14265/Reviewer_yRqr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14265/Reviewer_yRqr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762847589257, "cdate": 1762847589257, "tmdate": 1762924715506, "mdate": 1762924715506, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the problem of optimizing the outputs of a black-box system with respect to a decision maker's (DM) preferences. The authors propose LILO, a hybrid framework that leverages large language models (LLM) to extract utility information from free-form textual feedback and principled Bayesian Optimization (BO) for candidate generation. Specifically, the utility estimation is achieved via training GP-based proxy utility models on LLM-generated pairwise comparisons. The method is empirically validated on synthetic and real-world test functions subject to various utility functions, demonstrating on-par or improved performance over LLM-based and GP-based baselines. \n\nOverall, this work is well-motivated and the paper is generally well-written. Given clarification of some empirical concerns, I am willing to increase the score."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Strong motivation:** The proposed method provides a DM-friendly and powerful solution to black-box optimization with subjective DM perferences. By introducing LLMs, it enables flexible, free-form DM feedback and the incorporation of different types of domain priors.\n2. **Empirical performance:** The results show that LILO achieves on-par or superior performance against both LLM-based and GP-based baselines on synthetic and real-world test functions across various utility functions. The current design is well-supported by a set of ablation study."}, "weaknesses": {"value": "The process of the LLM learning the utility given DM's natural language feedback and experimental data introduces a black-box mapping with unclear internal mechanism and implicit priors. This impacts the principled nature of the overall optimization."}, "questions": {"value": "1. Larger number of trials ($T$): The current results are limited to short horizion, $T=8$. Experimenting with a larger number of trials for LILO and the baselines would provide a more complete understanding of the method's convergence properties and sample efficiency. \n2. Figure 3: Could you explain the performance discrepancy in the LLM baselines for Thermal Comfort Type A and Type B? \n3. Line 475 - 476, \"In our experiments, we found this effect to be manageable by ablating over a variety of utility functions and test problem combinations\": Could you elaborate a bit more on the issue this statement refers to, and how you address it through ablating over utility functions and test problems?\n4. Would be valuable to visualize the estimated utility values.\n5. Minor comments: Figure 2 notation. In the Candidate Generation & Experimentation panel, the batch size of canidates should be denoted as $B^{\\text{exp}}$ instead of $B^{\\text{pf}}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "95ijNBXdJF", "forum": "70fI4ptT4b", "replyto": "70fI4ptT4b", "signatures": ["ICLR.cc/2026/Conference/Submission14265/Reviewer_ZCQ2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14265/Reviewer_ZCQ2"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission14265/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762892218050, "cdate": 1762892218050, "tmdate": 1762924714369, "mdate": 1762924714369, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}