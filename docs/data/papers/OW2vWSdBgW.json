{"id": "OW2vWSdBgW", "number": 12696, "cdate": 1758209550605, "mdate": 1759897493382, "content": {"title": "Efficient Best-of-Both-Worlds Algorithms for Contextual Combinatorial Semi-Bandits", "abstract": "We introduce the first best-of-both-worlds algorithm for contextual combinatorial semi-bandits that simultaneously guarantees $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret in the adversarial regime and $\\widetilde{\\mathcal{O}}(\\ln T)$ regret in the corrupted stochastic regime. Our approach builds on the Follow-the-Regularized-Leader (FTRL) framework equipped with a Shannon entropy regularizer, yielding a flexible method that admits efficient implementations.\nBeyond regret bounds, we tackle the practical bottleneck in FTRL (or, equivalently, Online Stochastic Mirror Descent) arising from the high-dimensional projection step encountered in each round of interaction. By leveraging the Karush-Kuhn-Tucker conditions, we transform the $K$-dimensional convex projection problem into a single-variable root-finding problem, dramatically accelerating each round. Empirical evaluations demonstrate that this combined strategy not only attains the attractive regret bounds of best-of-both-worlds algorithms but also delivers substantial per-round speed-ups, making it well-suited for large-scale, real-time applications.", "tldr": "", "keywords": ["best-of-both-worlds", "combinatorial semi-bandits", "follow-the-regularized-leader"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/68afc5df99758fe1bde9a719625411db834d636a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the contextual combinatorial bandit problems and investigates the best-of-both-worlds algorithm that achieves $O(\\sqrt{T} )$ regret in the adversarial regime.\nand polylog T regret in the stochastic regime. The paper proposes  the Follow-the-Regularized-Leader with a Shannon entropy regularizer, which is built upon the BOBW result of the contextual linear bandit. In addition to regret guarantees, the authors also propose the efficient projection subroutine in any FTRL with a Legendre regularizer. They show the reduction of the typical K-dimensional convex projection to a one-dimensional root-finding problem by KKT conditions. Finally, they provided the empirical experimental evaluations of the proposed methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- First result of the best-of-both-worlds algorithm for contextual combinatorial bandits.\n- The novel choice of the Shannon entropy regularizer, which is defined over the continuous actions of conv(Action space).\n- The computational advantage of the projection subroutine can be applicable to any FTRL framework in combinatorial semi-bandits. This advantage of Bisection over MOSEK/Newton is empirically validated."}, "weaknesses": {"value": "- The problem formulation for adversarial combinatorial semi-bandits is based on Zierahn et al. (2023). However, a comparison with Zierahn et al. (2023) is missing in the empirical evaluation.\n- There are several formulations in  existing works on Contextual combinatorial bandits. Qin et al. (2014) is another formulation for the stochastic setting while Zierahn et al. (2023) is only for the adversarial setting. The paper needs to discuss the regret bound for stochastic regimes more carefully.\n- Due to the use of Shannon entropy regularizer and Precision-matrix estimation, the regret bound in the stochastic regime is $O(\\log^3 T)$ as $\\kappa=O(\\log T)$. Arguments of $O(\\log {T} )$ bound in the abstract/Introduction might be overclaimed.\n- The algorithm design and theoretical analysis is built upon the contextual linear bandits of Kuroki et al. (2024). The main difference is induced by the fact that Shannon entropy is now defined over the convex hull of the combinatorial action set. Given the fact that the proposed algorithm is not applicable to the case with $\\alpha$-approximate oracles for the offline comibinatorial problem, the techniques and theoretical analysis employed here is not significantly different from known results."}, "questions": {"value": "- Does the stochastic regime coincide with the setting by Qin et al. (2014)? Is so, could you provide the comparison with existing regret bounds? If not, could you describe what is the existing bound and also discuss the difference in the existing problem setting including the assumptions. For example, many stochastic combinatorial bandits assume $R$-sub Gaussian noise, but in the paper, you can only deal with the bounded noise due to the technical reason in the best-of-both-worlds algorithms. Is the context space allowed to be infinite?\n- Regarding the reduction of K-dimensional projection to a 1D root-finding using KKT, in special structures such as spanning tree or matroid bases, one could instead apply a Frank–Wolfe update to obtain a convex decomposition at extreme points in the FTRL framework.\nFrank–Wolfe update is already wildly applicable for various combinatorial actions.  Could you comment on whether the KKT-based projection offers additional benefits or theoretical guarantees beyond such structure-dependent methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R4pLGIpIY7", "forum": "OW2vWSdBgW", "replyto": "OW2vWSdBgW", "signatures": ["ICLR.cc/2026/Conference/Submission12696/Reviewer_vDJH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12696/Reviewer_vDJH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761312955716, "cdate": 1761312955716, "tmdate": 1762923530018, "mdate": 1762923530018, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the best-of-both-worlds (BOBW) algorithm for linear contextual combinatorial bandits. The algorithm leverages the follow-the-regularized-leader (FTRL) framework with a data-dependent learning rate to achieve the BOBW guarantee. Due to the monotonicity of the first-order derivative, the authors also prove that the optimization problem of FTRL can be efficiently approximately solved using KKT conditions and bisection. Experiments on the running time and cumulative regret of the proposed algorithm are conducted."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work seems to be the first work in the literature to study the BOBW linear contextual combinatorial bandit.\n2. Most parts of this work are generally well-written."}, "weaknesses": {"value": "1. **Motivation**: I understand that the linear contextual combinatorial problem might be of some practical interest. However, given (a large number of) previous advances for BOBW combinatorial bandits, linear bandits, and linear contextual bandits, at this point, studying the BOBW linear contextual combinatorial problem seems somewhat not so technically appealing.\n2. **Novelty**: My main concern is about the novelty of this work, which is also partially related to the concern above. In recent years, it has been widely observed in various bandit problems that data-dependent learning rates can lead to BOBW regret guarantees. For this work, though the problem formulation is new, the used data-dependent learning rate $\\beta_t$ has a very similar form to those in previous works studying BOBW graph bandit [1], linear bandit [2], and linear contextual bandit [3]. The resulting intermediate regret bound (Lemma 2.4), depending on the summation of the entropy values, is also very similar to those in previous works. In this work, the use of FTRL with Shannon entropy over the convex set of super-arms, instead of the space of distributions over super-arms, has already been proposed in previous work for the adversarial contextual combinatorial problem [4]. Also, the used techniques for contextual bandits, including matrix geometric resampling and ghost context sampling, are standard in the literature for contextual bandits [5]. Therefore, to me, the main contribution of this work is that the authors find that the optimization problem of FTRL with Shannon entropy over the convex set of super-arms in special cases of $m$-set and partition matroid can be efficiently addressed by leveraging KKT condition and the monotonicity of the first-order derivative of the regularizer (btw, the definition of $m$-set seems to be that $\\sum\\_{k=1}^K(A)\\_k = m $ instead of $ \\sum\\_{k=1}^K(A)\\_k \\leq m$).\nTherefore, from the viewpoint of bandit learning, the novelty of the proposed algorithm and results appears to be limited. On the other hand, I am not working in the optimization area, so I think my sense for evaluating the novelty of the approximate solution to the FTRL optimization considered in this work might not be accurate. However, I also personally feel that using the KKT condition to solve a constrained optimization is standard, which I believe appears in most (basic) convex optimization courses.\n3. **Presentation**: It is good to see there are many discussions about the computation details of the FTRL update. However, for bandit learning, I personally think that the discussions and comparisons between regret bounds might be the most important parts to be included in the main paper body. For example, when specializing the results in the special cases of linear contextual combinatorial bandits, what are the advantages and disadvantages of the regret bound in this work when compared with previous works studying BOBW combinatorial bandits [6], linear bandits [2], and linear contextual bandits [3]? \n\n[1] Ito et al. Nearly Optimal Best-of-Both-Worlds Algorithms for Online Learning with Feedback Graphs. NeurIPS, 22.\n\n[2] Kong et al. Best-of-three-worlds Analysis for Linear Bandits with Follow-the-regularized-leader Algorithm. COLT, 23.\n\n[3] Kuroki et al. Best-of-Both-Worlds Algorithms for Linear Contextual Bandits. AISTATS, 24.\n\n[4] Zierahn et al. Nonstochastic Contextual Combinatorial Bandits. AISTATS, 23.\n\n[5] Neu et al. Efficient and robust algorithms for adversarial linear contextual bandit. COLT, 20.\n\n[6] Zimmert et al. Beating Stochastic and Adversarial Semi-bandits Optimally and Simultaneously. ICML, 19."}, "questions": {"value": "1. Typically, FTRL with Shannon entropy regularization requires updating the policy distribution over the action space. This is computationally inefficient for linear bandits if action sets are exponentially large. To avoid this drawback, previous works have leveraged FTRL with self-concordant regularizers, which is operated in the convex set of the action feature set and is even shown to be possible to achieve the BOBW bound for linear bandits [7]. Is FTRL with a self-concordant regularizer also applicable to the linear contextual combinatorial problem?\n2. I notice that the dependence on $\\log T$ of the proposed algorithm in this work seems to be slightly better than previous works using FTRL with Shannon entropy regularizer (say, [1,2]). Could the authors comment more on this?\n\n[7] Ito et al. Best-of-Three-Worlds Linear Bandit Algorithm with Variance-Adaptive Regret Bounds. COLT, 23."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BmSj2Hb9xL", "forum": "OW2vWSdBgW", "replyto": "OW2vWSdBgW", "signatures": ["ICLR.cc/2026/Conference/Submission12696/Reviewer_4Chh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12696/Reviewer_4Chh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761665277386, "cdate": 1761665277386, "tmdate": 1762923529410, "mdate": 1762923529410, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies a special case of contextual combinatorial semi-bandits and provides best-of-both-world guarantees for it. The setting is as follows: At each time step $t$, a $d$-dimensional context $X_t$ is revealed, then the learner has to select $m$ out of $K$ actions. Each action $k$ is associated with a $d$-dimensional vector $\\theta_{t,k}$, whose scalar product with $X_t$ defines its loss at time $t$. The instantaneous loss incurred by the learner is the sum of the losses of the action chosen at that time step. The total loss of the learner is given by the sum of the instantaneous losses, and it is compared to the total loss of the optimal context-dependent action map that achieves minimum loss in hindsight.\n\nThe authors study the (oblivious) adversarial setting, where the $\\theta_{t,k}$ vectors are chosen up-front by an adversary in an arbitrary way, the stochastic setting, where the  $\\theta_{t,k}$ are drawn i.i.d. from a fixed distribution for each action, and the corrupted case, where such distributions may slowly change over time, and some unbiased noise is added at each iteration.\n\nThe main contribution of the paper is a best-of-both-worlds algorithm, which the authors claim has an $O(\\sqrt{T})$ regret bound in the stochastic setting, and an $O(\\log T)$ instance-dependent bound otherwise."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The authors complemented their theoretical results with some experiments. Although this is not strictly mandatory for theoretical online learning submissions, it is a nice addition. \n- To the best of my understanding, the paper is correct\n- Best-of-both-world results are important and of practical value\n- The authors also propose a numerical speed-up that enhances applicability"}, "weaknesses": {"value": "- The topic of the paper is extremely narrow, as the problem studied is quite involved and specialized. I am unsure whether it will garner general interest among the vast ICLR audience. \n- It is very challenging to gain a comprehensive understanding of the state-of-the-art and to compare it with the results presented in the paper. There are many parameters (namely, the context dimension $d$, the number of base actions $K$, and the cardinality constraint $m$), as well as numerous results implied by past works. Therefore, I strongly suggest that the authors include a table comparing the regret rates across various settings/assumptions. \n- The modeling assumption that the contexts are independent and identically distributed (i.i.d.) is somewhat surprising. What can be said when they are generated adversarially as well?\n- The overall algorithm construction is non-trivial but not novel, as it follows the standard FTRL template for BOBW. \n- My main issue is with the presentation of the results, which seems to me quite misleading: in the statement of Theorem 1, there is a parameter $\\kappa$ that inexplicably hides some important terms, making the understanding of the regret rates complicated.  $\\kappa \\in \\Theta(\\log T)$ term, this means that the stochastic regret is $O(\\log^2T)$, and not $\\log T$ as claimed in the introduction. The term $\\kappa$ also depends linearly on $\\sqrt{K}$, so that the overall regret rates depend polynomially on $K$, and not only on $m$, which may be drastically smaller. \n\nMinor:\n- The title ``Combinatorial Semi-Bandits'' is a bit misleading, as I was expecting a model similar to the one in the seminal paper by Cesa-Bianchi and Lugosi, where the action set could have any combinatorial structure (thus not only the cardinality constraint proposed in this paper). I am aware that other papers have already employed the generic combinatorial term for this specific case, but I still find it somewhat unusual. \n- In line 99, the authors refer to a lower bound by Bubeck & Cesa-Bianchi (2012), but I think that the citation is to the wrong paper. Shouldn’t it be Bubeck, Cesa-Bianchi, and Kakade COLT 12? In particular, does the cited lower bound apply to the special case of combinatorial bandits studied in this paper?"}, "questions": {"value": "Please address my comments above. In particular, am I missing something about the $\\kappa$ parameter?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZmyPbzbvIH", "forum": "OW2vWSdBgW", "replyto": "OW2vWSdBgW", "signatures": ["ICLR.cc/2026/Conference/Submission12696/Reviewer_PYJa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12696/Reviewer_PYJa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909014253, "cdate": 1761909014253, "tmdate": 1762923528938, "mdate": 1762923528938, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes an FTRL-based algorithm for contextual combinatorial semi-bandits. It achieves best-of-both-worlds (BOBW) regret: $O(\\sqrt{T})$in adversarial settings and $O(\\log T)$ in corrupted stochastic settings.\nIt uses a Shannon-entropy regularizer together with an efficient projection routine derived from the KKT conditions over the convex hull of the m-set. This reduction converts a high-dimensional optimization into a one-dimensional root-finding task per round, yielding computational savings. Empirical evaluations demonstrate the effectiveness of the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) This paper presents the first best-of-both-worlds (BOBW) regret guarantee for contextual combinatorial semi-bandits. \n2) It introduces an efficient numerical scheme for the FTRL update that significantly reduces computational cost while preserving the theoretical guarantees. \n3) The theoretical analysis is well-structured, with a new auxiliary “ghost context” game that simplifies the regret analysis. \n4) Empirical evaluations demonstrate substantial runtime improvements compared with Newton and MOSEK solvers."}, "weaknesses": {"value": "1) This paper assumes a linear reward (or loss) function. The framework does not yet handle nonlinear or generalized linear models.\n2) The proposed efficient projection scheme is tailored to the m-set (and potentially partition matroids). For more general combinatorial structures, this computational savings may no longer hold.\n3) The Shannon-entropy regularizer adds an additional $O(\\log T)$ term in the adversarial regret, slightly weakening the asymptotic bound."}, "questions": {"value": "1) The analysis relies on linear reward assumptions. Could the proposed method be extended to generalized linear or nonlinear reward functions (e.g., incorporating a link function)?\n2) The efficient projection routine appears tailored to m-sets (and perhaps partition matroids). What are the main challenges for generalization, e.g., non-separability, lack of monotonicity, or absence of closed-form inverses?\n3) Shannon entropy has been widely used in FTRL. Is the contribution mainly its specific integration into the BOBW setting or the projection simplification that follows from its analytical form? How does this choice compare with other regularizers employed in similar contexts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LiJ5dgia4o", "forum": "OW2vWSdBgW", "replyto": "OW2vWSdBgW", "signatures": ["ICLR.cc/2026/Conference/Submission12696/Reviewer_SW5m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12696/Reviewer_SW5m"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12696/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762534853313, "cdate": 1762534853313, "tmdate": 1762923528556, "mdate": 1762923528556, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}