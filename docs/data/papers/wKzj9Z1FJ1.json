{"id": "wKzj9Z1FJ1", "number": 10106, "cdate": 1758160835724, "mdate": 1759897673541, "content": {"title": "Multi-play Combinatorial Semi-Bandit Problem", "abstract": "In the combinatorial semi-bandit (CSB) problem, a player selects an action from a combinatorial action set and observes feedback from the base arms included in the action. While CSB is widely applicable to combinatorial optimization problems, its restriction to binary decision spaces excludes important cases involving non-negative integer flows or allocations, such as the optimal transport and knapsack problems.\n    To overcome this limitation, we propose a general form of CSB, the multi-play combinatorial semi-bandit (MP-CSB) problem, where a player can select a non-negative integer action and observe multiple feedbacks from a single arm in each round. \n    We propose two algorithms for the MP-CSB. \n    One is a Thompson-sampling-based algorithm that is computationally feasible even when the action space is exponentially large with respect to the number of arms, and attains $O(\\log T)$ distribution-dependent regret in the stochastic regime, where $T$ is the time horizon. We show that this algorithm can be much more memory-efficient than naively expanding existing algorithms.\n    The other is a best-of-both-worlds algorithm, which achieves $O(\\log T)$ variance-dependent regret in the stochastic regime and the worst-case $\\tilde{\\mathcal{O}}( \\sqrt{T} )$ regret in the adversarial regime. \n    Moreover, its regret in adversarial one is data-dependent, adapting to the cumulative loss of the optimal action, the total quadratic variation, and the path-length of the loss sequence.\n    Finally, we numerically show that the proposed algorithms outperform existing methods in the CSB literature.", "tldr": "We generalize the ordinary combinatorial semi-bandit problem and provide algorithms for it.", "keywords": ["multi-armed bandit", "combinatorial optimization"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a12b21dc544a743a6edb9a11b9cfb07e2b418a3.pdf", "supplementary_material": "/attachment/767f39e1cbf510562168384ef659afce9878578a.zip"}, "replies": [{"content": {"summary": {"value": "The paper studies the multi-play combinatorial bandit problem, where at each round the learner selects a subset of arms (subject to combinatorial constraints) and receives a reward and observations from the selected arms with semi-bandit feedback. Under stationary and stochastic arm rewards, a variant of combinatorial Thompson sampling is shown to achieve $O(\\log T)$ regret. When arm reward distributions are general, an algorithm that can adapt to attain best-of-both-worlds type guarantees is proposed. Its regret is bounded in the stochastic and adversarial regimes as well as a spectrum of regimes in between. Simulations are performed to compare the performances of the proposed algorithms with those from the literature."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "This paper bridges the gap between multi-play and combinatorial bandits. It focuses on a setup that includes stochastic and adversarial reward regimes as special cases. For different regimes, it proposes different algorithms inspired by Thompson sampling and best of both worlds algorithms from the literature. It offers rigorous theoretical analysis with regret bounds. The problem is relevant to applications where multiple actions must be selected simultaneously under constraints."}, "weaknesses": {"value": "1. The main results extend known techniques rather than introduce fundamentally new methods. The writing of the paper does not clearly convey the technical novelty of the derivations; as a result, the technical novelty seems incremental. Please elaborate in detail why the multi-play CMAB setting is significantly more challenging to analyze than the single-play CMAB setting. \n\n\n2. Since this paper is highly theoretical in nature, the derivation of regret lower bounds for algorithms that do not use the duplication technique will greatly improve the contribution and will clearly demonstrate whether the derived upper bounds are optimal in some sense. \n\n3. The presentation is difficult to follow as the notation is very dense. While this is partly due to page limits, it negatively impacts the broader accessibility of the paper to the ICLR community. \n\n4. Simulations are limited in scope. No evaluation was performed in realistic settings where combinatorial multi-play naturally arises."}, "questions": {"value": "1. The authors claim that the GenCTS algorithm does not require enumerating all possible actions at the beginning of the game. However, for the stochastic setting, isn't this a property of the oracle rather than the algorithm itself? Isn't the competitor CTS or an algorithm like CUCB also computationally feasible when the action set is exponentially large in d? \n\n2. Is the compute complexity of the oracle included in the compute complexity of GenCTS? Isn't the competitor CTS also computationally feasible when the action set is exponentially large in d? The scalability of duplication CTS becomes an issue when the batch size is large. \n\n3. Many works in CMAB resort to approximation oracles. Combinatorial Thompson sampling may not work well with such types of oracles (See, e.g., Wang, Siwei, and Wei Chen. \"Thompson sampling for combinatorial semi-bandits.\" International Conference on Machine Learning. PMLR, 2018). Does this mean that the results in this paper cannot be extended to learning algorithms that use approximation oracles? \n\n4. Multi-play setting is reminiscent of the batched-bandits setting (see, e.g., Gao, Zijun, et al. \"Batched multi-armed bandits problem.\" Advances in Neural Information Processing Systems 32 (2019)). Please elaborate on how your work is connected to batched bandits."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "y1XqKUvd8j", "forum": "wKzj9Z1FJ1", "replyto": "wKzj9Z1FJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10106/Reviewer_YQJV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10106/Reviewer_YQJV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761414079261, "cdate": 1761414079261, "tmdate": 1762921487092, "mdate": 1762921487092, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper states the multi-play combinatorial semi-bandit (MP-CSB) problem, a generalization of the combinatorial semi-bandit (CSB) framework to multiple pulls per each arm on one round. This setting covers stochastic versions of optimal transport and knapsack problems, where binary decision CSB can hardly be applied. Two algorithms are proposed: GenCTS, a generalized Thompson sampling algorithm with O(log T) stochastic regret and GenLBINFV, and a best-of-both-worlds algorithm achieving variance-dependent logarithmic regret in the stochastic setting. Theoretical analysis guarantees improved regret bounds, and experiments confirm superior empirical performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. A novel problem setting is addressed: MP-CSB extends CSB to non-binary, integer-valued actions, what extends MABs to broader scope of stochastic combinatorial optimization problems.\n2. Algorithmic Efficiency: GenCTS avoids exponential enumeration of actions and produces in polynomial time\nRegret bounds: both algorithms achieve state-of-the-art regret bounds"}, "weaknesses": {"value": "1. The practical motivation for the proposed MP-CSB setting is somewhat unclear. The authors propose two particular cases of the problem for practical inspiration: stochastic versions of the Optimal Transport problem and the knapsack problem. Unfortunately, they do not cite previous papers on the proposed cases, and their practical significance remains unclear.\n\n2. The related work section seems incomplete. There are previous papers on non-binary combinatorial bandits, e.g., “Fixed-Budget Real-Valued Combinatorial Pure Exploration of Multi-Armed Bandit” by Nakamura et al. There are papers on stochastic versions of the Optimal Transport problem with bandits, e.g., “Bandit Optimal Transport” by Lorenzo Croissant. I expect a review with a comparison of the setting and algorithms to previous literature."}, "questions": {"value": "See Weak points."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "piFiE5zffu", "forum": "wKzj9Z1FJ1", "replyto": "wKzj9Z1FJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10106/Reviewer_uaEi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10106/Reviewer_uaEi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923394972, "cdate": 1761923394972, "tmdate": 1762921486734, "mdate": 1762921486734, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work considers an extension of the well-studied combinatorial semi-bandits (CSB) problem, where at each round the learner selects a subset of base arms a loss that is a (Lipschitz) function of the losses of the individual selected arms, while they observe all of the latter losses.\nThe extension considered here allows the learner to select a single arm multiple times in the same round, hence seeing the choice at any round as a vector with non-negative integer values instead of a binary vector.\nWhile this problem can be already solved by relying on existing approaches for linear bandits or CSB, the former approach would typically require to enumerate all combinatorial arms (which is generally impractical) while the latter requires to duplicate base arms and thus leading to possibly suboptimal performance and guarantees.\nThe authors show both a Thompson-sampling-based algorithm that achieves a variance-dependent logarithmic regret bound in the stochastic case where the loss can be any Lipschitz function of the losses of base arms, and an INF-based algorithm with log-barrier regularization and data-dependent learning rates that provides best-of-both-worlds regret guarantees in the case of linear losses.\nExperiments on synthetic data further show the improvements achieved by the proposed algorithms for this generalized CSB problem, compared to the performance of adaptation of existing algorithms for the vanilla CSB problem via base arm duplication."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The authors investigate a natural and interesting extension of the CSB problem, and clearly motivate their proposed techniques by observing crucial downsides coming from straightforward extensions of algorithms from prior work.\nThe generalized CSB problem considered in this submission is also well motivated by fundamental combinatorial optimization problems such as knapsack and optimal transport.\nThe proposed techniques work under assumptions with various degrees of generality, with the Thompson sampling algorithm in the stochastic setting with Lipschitz losses, as well as an INF-based algorithm that guarantees near-optimal regret even in adversarial environments."}, "weaknesses": {"value": "While the extensions of the algorithms lead to advantages compared to previous approaches, as underlined by the authors, the adaptation of existing techniques appears relatively straightforward.\nNonetheless, the authors show that carefully tuning the various steps of such algorithms (e.g., the extension of the log-barrier regularizer and the time-varying learning rate) and analyzing them carefully remain nontrivial tasks, at which they succeeded.\n\nIn the introduction, the description of the problem setting itself as an generalization of CSB in simple words is somewhat unclear and could benefit from a clearer presentation.\nIn particular, the authors mention decisions “taking non-negative integer values” without clearly stating in words what these integers represent.\nOne may intuitively deduce that these represent the multiplicity in the choice of each base arm, but this does not seem to be stated so explicitly, and a reader only truly understands this fact when reading the formal definition of the problem setting (which may be heavy to parse given the many objects that require a definition).\n\nAs a side note, the definition of the stochastic regime with adversarial corruptions at lines 168-171 seems slightly different from the more established one in Ito (2021).\nIn particular, the expectation in your definition is external, while Ito (2021) defines the corruption level $C$ by looking at mean losses of the stochastic ones and the corrupted ones (instead of the realized ones).\nIt would then seem that the $C$ you consider would need to be generally larger, and thus guarantees depending on your $C$ would be worse than depending on the original definition of the corruption level.\nAnyhow, this is not a major weakness but it does require some comment in my opinion.\n\nAnother side note is about the values of $b\\_i$’s from the duplication approach.\nIt is never specified, and thus left unclear, what values these parameters should be set to.\nHowever, at first glance it would seem that setting $b\\_i=n\\_i$ would work as intended.\nThe authors, unfortunately, never comment on that and they also leave this point unaddressed (e.g., see line 424).\nThis should really be looked into in order to perform a fair comparison with existing approaches adapted by duplicating base arms, especially because previous algorithms would be at a clear advantage if it suffices to set $b\\_i = n\\_i$.\n\nFinally, one of the main advantages of the proposed algorithms is runtime (and space) efficiency, but experiments never compare the runtime of the proposed algorithms to the baselines.\nIt then feels like experiments are not really covering all relevant aspects that should be empirically verified, leaving the reader with a sense of incompleteness."}, "questions": {"value": "1. What you describe at lines 161-164 is an adaptive (non-oblivious) adversary, right? If so, it would be better to clarify this aspect since it might not be straightforward to work with non-oblivious adversaries in partial feedback settings.\n2. Could you comment on the different definition of the stochastic regime with adversarial corruption compared to the one from Ito (2021)? See my comment in **Weaknesses**.\n3. The bands around the curves in the plots from the experiments section have semi-width equal to the standard deviation over the 100 trials, or don’t they? Please clarify this in the text.\n\n**Minor comments and typos:**\n- Line 137: the last $\\\\mathbb{R}$ is more precisely $[0,1]$ given line 133\n- Line 155-156: “independent” random variables?\n- Please, recall the definition of $N\\_i(t-1) = \\sum\\_{s=1}^{t-1} a\\_i(s)$ around eq. (3)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YmVXukJWpd", "forum": "wKzj9Z1FJ1", "replyto": "wKzj9Z1FJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10106/Reviewer_FyN2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10106/Reviewer_FyN2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762436579793, "cdate": 1762436579793, "tmdate": 1762921486232, "mdate": 1762921486232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors consider a variant of the combinatorial semi-bandits problem where each arm can be played multiple-times in a round.  The authors propose a Thompson sampling based method for the stochastic setting as well as a best-of-both-worlds method, obtaining regret for stochastic regret for the former and adversarial (as well as stochastic rewards with adversarial corruptions) for the latter.  The authors also run experiments, showing their work outperforms combinatorial semi-bandit method baselines where arm duplications are used."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The authors consider a variant of the combinatorial semi-bandit problem with multiple-arm pulls (multiple base arms can each be pulled multiple times in a single round)    \n- The authors propose a TS based algorithm (building on CTS from Wang and Chen 2018) for the multi-play CSB with stochastic environments and analyze regret and complexities (computational and storage)\n- the authors propose a best of both worlds algorithm (building on LBINFV from Tsuchiya et al 2023) for multi-play CSB analyzing regret bounds for adversarial, stochastic, and stochastic with corruptions.  \n- the authors include experiments, showing simple duplication and use of prior CSB methods is inefficient"}, "weaknesses": {"value": "### Major\n- There is no discussion of technical challenges in the main paper in extending CSB methods to multi-play settings.  There may be some significant technical hurdles in the regret analyses, which could merit significance to the contribution, but they are not made clear in the main paper. \n    - The authors do contrast regret bounds and complexities against the trivial approach of duplicating arms and blindly applying CSB methods.  It is obvious that such an approach is wasteful (all the losses from playing arm $i$ should be used together).  There may be some significant technical hurdles in the regret analyses, which could merit significance to the contribution, but they are not made clear in the main paper. \n    - line 317 mentions generalizing LBINFV “is not straightforward.  We show that, by carefully leveraging $n_i$’s, LBINFV can be generalized to MP-CSB.” And terms like the regularizer (5) and parameters (6) include dependence on $n_i$, but from my reading I did not see a clear articulation of what was not straightforward.\n- There is no discussion of, or even brief references to, past works on variants of “multi-play” bandits.  In terms of method design, analysis, and extent to which the changes from regular bandits is replicated here (in the change from CSB) vs extending CSB to multi-play has unique technical challenges?  from a quick search, I came across a few that sound relevant\n    - “Multiple-Play Stochastic Bandits with Shareable Finite-Capacity Arms” ICML 2022 this sounds fairly close to the setup considered in the paper, where each arm can be played multiple times (up to a capacity limit).  This ICML paper, interestingly, considers unknown capacities and an aggregate semi-bandit feedback for each arm (as opposed to a per-play feedback).     That presents challenges not present in the current work, as the capacity must be estimated.  \n\n    - Komiyama et al. (2015) “Optimal regret analysis of Thompson sampling in stochastic multi-armed bandit problem with multiple plays” in ICML 2015 for multi-play MAB.  In this setting arms are not repeatable in a round).  This may have been later generalized by CTS  (there are also later multi-play TS methods like “Multiple-Plays Multi-armed Bandit with Accelerated Thompson Sampling” https://ieeexplore.ieee.org/document/10695179 )\n    - There is also a large literature on multi-agent MAB (which is a generalization of multi-play MAB as the same arm can be played multiple times in the same round), including some with combinatorial action spaces (https://arxiv.org/abs/2405.05950 is one example came across, there are probably more that also consider combinatorial actions).   Works of this line would probably not generalize the problem the authors consider (e.g. the number of agents might need to match the per-round arm-play capacities $\\{n_i\\}$ for that to be the case), but there may still be relevant relations in terms of algorithm design and analysis that may merit discussion.     \n    - More broadly speaking, there is a lack of references to recent works. From quick search of the pdf, there are no citations to works published or pre-prints posted in 2024 or 2025.  This brings concern about how well characterized the state of the related literature is.  CTS is from 2018, and 2023 is the latest of the CSB references.  \n\n### Minor\n- Table 1 references CTS and LBINFV but reports the regret bounds and complexities not of those works (designed for the single-play setting) but what they would be if the learner naively duplicated base arms and then applied those methods (oblivious to the duplication).  That is misleading to the reader.   It is fine to include those formulas as a simple baseline, but that should be in the text, not the table.  Instead, mark the sets of rows as being related but different problems and report CTS and LBINFV for the regret and complexity for the single-play settings they were designed for.\n    - (minor) if the past works mentioned are not limited to linear, then that should be noted.\n    - (minor) If there are any CSB works since 2018 that outperform CTS, they should be included (eg the table for CSB should not be limited to the two that are extended in this work)\n    - (minor) I do not think linear bandit methods would be essential to include in the table\n- I think using space in lines 256-311 to highlight improvements over the trivial duplication technique is not helpful (can keep it, but put it in the appendix), as duplication is obviously inefficient statistically and computationally.  Instead, it would have been much better that the authors discuss technical challenges (if any) in the regret analysis and high level descriptions of they overcame those challenges.\n\n- For the motivating real-world applications, the descriptions need to be revised\n    - line 198, for OT, a real-world application what is the “cost of edges it passes through”?  \n    - The knapsack problem as described is not a multi-play problem.  There are simply $d$ items.  \n- joint reward function non-linearity (for the stochastic setting)\n    - Line 158 when does this assumption hold, other than linear?  If you want to claim your results hold for non-linear settings, you need to discuss what non-linear classes your results apply to.\n    - the OT and knapsack examples have linear objectives (the authors later explicitly acknowledge that); the ad example does not explicitly state the objective function.  \n- Figures 1 and 2 should have labels on the axes.  The main text does not clarify exactly what is plotted (what either axis is), just referring to figures as showing “the results”\n\n### Very Minor\n- line 307-311 memory discussion looks to be specializing to OT problem without mentioning that (I was confused by notation there at first)\n- line 238 “output[s]”\n- There are capitalization errors in the reference list, Kveton et al 2015 ref, Schrijver, combettes et al"}, "questions": {"value": "Several questions are included along with comments in the \"Weaknesses\" section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N7oUlPZgNX", "forum": "wKzj9Z1FJ1", "replyto": "wKzj9Z1FJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10106/Reviewer_tugJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10106/Reviewer_tugJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10106/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762452998557, "cdate": 1762452998557, "tmdate": 1762921485497, "mdate": 1762921485497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studied the regret minimization problem for combinatorial bandits where each arm is allowed to be pulled for more than once during each time step. It proposed and analyzed two algorithms for stochastic and adversarial settings respectively. It also conducted numerical results to indicate the superiority of proposed algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The author(s) compared the proposed algorithms and existing ones with Table 1.\n2. The applications are discussed."}, "weaknesses": {"value": "1. Beginning from LINE 185, some applications of MP-CSB are discussed, where there is a contraint $\\mathbf{a}\\in \\ldots$ for the optimal transport problem and another one for the knapsack problem. However, I failed to find a corresponding constraint in the problem formulation and hence feel a bit confused about the relation between the application scenarios and the studied formulation. \nMy major concerns are as below:\n1. A lower bound is discussed around LINE 176. However, the two works cited there studied the combinatorial bandits where one arm can be pulled at most once at each time step. I wonder how their results imply the lower bound here? Besides, if there is one, the lower bound should be included in Table 1.\n1. The GenLBINFV algorithm is claimed to be best-of-both-world. However, there is no discussion on its comparison to a lower bound. How can the algorithm be claimed to be BOBW?\n1. The pseudocode of GenLBINFV algorithm should be included in the main part of the manuscript as it is quite important.\n4. More discussions on numerical results are appreciated.\n1. This paper \\url{https://proceedings.mlr.press/v51/jun16.html} also allowed multiple pulls of one single arm but focused on the pure exploration problem. Such works should be mentioned as related works.\n\nMinor issue:\n1. Table 1: The names of existing algorithms and the corresponding references should be included altogether."}, "questions": {"value": "Please refer to the **Weaknesses** section above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "KxoBluPXhL", "forum": "wKzj9Z1FJ1", "replyto": "wKzj9Z1FJ1", "signatures": ["ICLR.cc/2026/Conference/Submission10106/Reviewer_N7EU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10106/Reviewer_N7EU"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission10106/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762956246107, "cdate": 1762956246107, "tmdate": 1762956246107, "mdate": 1762956246107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}