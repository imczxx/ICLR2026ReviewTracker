{"id": "MHA8oTxoei", "number": 19896, "cdate": 1758300334077, "mdate": 1763701699045, "content": {"title": "Investigating the Link Between Representational Similarity and Model Interactions", "abstract": "Researchers have shown that neural similarity among humans predicts social closeness and cooperative success, whereas innovation often emerges from interactions among dissimilar individuals. We investigate whether these principles extend to artificial intelligence by examining interactions between large language models. In our experiments, 276 model pairs interact across eight games spanning both cooperation and novelty. We find that pairs with more similar representation spaces achieve significantly higher cooperation but exhibit reduced novelty and creativity. These findings suggest that representational similarity can be an important consideration in multi-agent system design.", "tldr": "We examine whether representational similarity can predict the interactive behaviors of models.", "keywords": ["Representational similarity", "multi-agent system", "cooperation", "creativity"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2b0253514297dba8af2a0be91a53ce392b3d7091.pdf", "supplementary_material": "/attachment/938ecce39862e9bc4105a3f0157d90e7718cb102.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the relationship between the internal representational similarity of LLMs and their behaviors when interacting in multi-agent scenarios. The authors conduct a large-scale empirical study involving 276 pairs of 23 distinct open-weight LLMs. These pairs engage in eight tasks, systematically divided into two categories: four games and four generative tasks designed to measure novelty."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's core idea, bridging human neuroscience and multi-agent LLM systems, provides a lens (internal representations) through which to understand and predict complex emergent behaviors, moving beyond purely output-level analysis.\n- The scale of the study (276 pairs, 23 models) is substantial. Also, the choice of CKA is well designed."}, "weaknesses": {"value": "My main concerns are about the game designs:\n\n- As per my initial thoughts, for the \"word guessing game,\" its significant result (+66.2%) seems intuitively predictable. This game is essentially testing the \"predictive alignment\" of the two models. If two models have similar representations (especially if trained on similar corpora), their probability distributions for a \"target word\" given a start alphabet during decoding will naturally be more similar. Therefore, their ability to \"guess\" each other's word seems self-evident (This feels almost tautological). This is more a direct reflection of \"homogeneity\" than \"cooperation.\"\n\n- Another question is whether the agents' (LLMs') moves within a single round are generated simultaneously or sequentially. This ambiguity leads to a dilemma that challenges the validity of the \"collaboration\" findings:\n 1. If moves are sequential, i.e., one agent sees the other's prediction/move before generating its own: This would invalidate the tasks as true game-theoretic tests. It would introduce a profound first-mover advantage. For instance, in the \"Divide-a-Dollar\" game, the first agent could simply claim $0.99, leaving almost nothing for the second agent, thus preventing any meaningful test of collaborative strategy.\n 2. If moves are simultaneous, i.e., each LLM's generation is independent within the round (my guess this is what the experiment is doing.): This raises the fundamental question of how \"collaboration\" is being measured. If the agents' decisions are made in isolation without knowledge of their partner's current move, the outcome is simply the result of two independent generations. It is then unclear how this setup evaluates an interactive collaborative process versus just the pre-existing alignment of two models making independent decisions.\n\nThe authors may need to provide more explanation of this intra-round mechanism, as the current ambiguity makes it a bit difficult to interpret what is truly being evaluated as \"collaboration\" in these games.\n\n\nAnother weakness is about insufficiently contextualized Related Work\n\nIn the \"Related Work\" section, a significant and overlooked area is the existing body of work on the designing of multi-agent collaboration. The paper does not review prior methods or established practices for designing and optimizing multi-agent systems for cooperative success. This omission makes it difficult to assess the practical utility of the authors' proposal (using representational similarity) against current or alternative approaches in the field of multi-agent AI.\n\nInstead, the review dedicates substantial space to the analogy between human neural alignment and LLM representational alignment. While this serves as the primary motivation for the hypothesis, this link is arguably a speculative claim, given the fundamental differences between biological brains and artificial networks. The paper would be stronger if it grounded its claims more firmly within the established literature of multi-agent systems optimization, rather than relying so heavily on this novel neuro-AI analogy."}, "questions": {"value": "- Please explain regarding the game design, as stated in weakness section.\n- In section 5.2, your finding that increased representation similarity reduces uniqueness but has \"no systematic trend\" on quality is fascinating. This implies that using dissimilar models for brainstorming could be a \"free lunch\" (gaining novelty at no quality cost). Do you believe this is the case?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ywKIUeIVR5", "forum": "MHA8oTxoei", "replyto": "MHA8oTxoei", "signatures": ["ICLR.cc/2026/Conference/Submission19896/Reviewer_nxPD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19896/Reviewer_nxPD"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644574137, "cdate": 1761644574137, "tmdate": 1762932055028, "mdate": 1762932055028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how representational similarity between LLMs affects their interactions in multi-agent settings. Across 276 model pairs from 23 open-weight families, the authors measure representational similarity using CKA and test its relationship with collaborative behaviors in various tasks. Results show that higher representational similarity leads to stronger cooperation but reduced novelty and creativity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper studies an insightful problem: how representational similarity between LLMs predicts their collaborative behavior. This introduces an interesting new dimension for understanding and designing cooperative multi-agent systems.\n\n- The experimental scope is comprehensive, involving 276 model pairs from 23 open-weight LLM families and covering diverse tasks across cooperative and creative settings. The breadth of analysis allows for good statistical validity and generalizability.\n\n- The work performs multi-perspective analyses, examining both performance-based cooperation and creativity-based novelty metrics, which provides a balanced view of how model similarity influences collaboration."}, "weaknesses": {"value": "- Some experimental details are insufficient. The paper does not describe prompts for each task, provide qualitative examples of model outputs, or clarify how multi-turn collaboration unfolds. For instance, it is unclear whether both models generate responses at every round, how messages are shared, when interactions terminate, and how the final cooperative solution is determined.\n\n- I think the individual model’s capability could be a confounding factor. A model’s solo performance might dominate the pair’s overall outcome, and weaker models could create a bottleneck effect regardless of representational similarity. Have you observed such a phenomenon? If not, what might explain it, given that the dominance of one model seems intuitive? This raises the question of whether the observed correlation truly reflects representational alignment or simply performance disparities.\n\n- Based on the above point, I think a problem is that the paper reports correlations between representational similarity and cooperative performance, but it never establishes causality. It remains unclear whether similar representations cause better cooperation, or whether both emerge as a byproduct of other factors (e.g., model solo performance, model size, or architectural homogeneity). Without controlled ablations, the causal interpretation is weak.\n\n- Although the experiments span diverse tasks, all are relatively synthetic (simple cooperative games or creative generation). These setups may not generalize to realistic multi-agent scenarios involving planning, negotiation, or dynamic tool use.\n\n- Figure 9 is ambiguous and requires better explanation. It is unclear whether it represents a model’s averaged cooperative performance across all partners, or some normalized measure of interaction gain. A clearer caption, explanation, or example would improve interpretability."}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZuqAqULGX8", "forum": "MHA8oTxoei", "replyto": "MHA8oTxoei", "signatures": ["ICLR.cc/2026/Conference/Submission19896/Reviewer_ffcD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19896/Reviewer_ffcD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761878694541, "cdate": 1761878694541, "tmdate": 1762932054466, "mdate": 1762932054466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether representational similarity between large language models (LLMs) predicts their cooperative or creative behavior in multi-agent interactions. Using 276 model pairs spanning 23 open-weight LLMs from eight model families, the authors evaluate how Centered Kernel Alignment (CKA) scores between model representations relate to performance across four cooperative games (word guessing, public goods, divide-a-dollar, and Keynesian Beauty Contest) and four creative tasks (story writing, biography, haiku, and vacation brainstorming).\nEmpirical results show a consistent positive correlation between representational similarity and cooperation, and a negative correlation with novelty and diversity. Mixed-effects regressions are used to control for model-specific effects. The paper concludes that representational similarity is a key factor shaping inter-model dynamics in multi-agent LLM systems, suggesting a design tradeoff between cooperation and creativity."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Originality: The paper offers a novel and well-motivated perspective by quantitatively linking representational similarity with interactive behaviors among large language models.\n\n2.Methodological rigor: The experimental design is comprehensive, involving diverse cooperative and creative tasks, appropriate mixed-effects regression modeling, and ablations that control for model family, tokenizer, and size.\n\n3.Conceptual depth: The study effectively bridges AI multi-agent interaction research with principles from human social neuroscience, grounding its hypotheses in established cognitive findings.\n\n4.Relevance: The findings provide actionable insights for designing AI collectives, highlighting a meaningful tradeoff between representational similarity (stability and cooperation) and diversity (creativity and novelty)."}, "weaknesses": {"value": "1.Limited mechanistic insight: While correlations are strong, the causal mechanism behind representational similarity’s behavioral influence remains unclear.\n\n2.Task diversity: Cooperative and creative tasks are all text-based; inclusion of multimodal or grounded tasks could test generality.\n\n3.Single similarity metric: The study primarily uses CKA; incorporating neuron-level or mutual information–based representational measures could yield deeper understanding.\n\n4.Interpretability gap: The work does not yet pinpoint which aspects of representation alignment (semantic, syntactic, or higher-order reasoning) drive cooperation versus novelty.\n\n5.Potential dependence on fine-tuned model families: Since most models share similar instruction-tuning paradigms, the observed trends may differ for foundation models without instruction tuning."}, "questions": {"value": "1.Could the observed tradeoff between cooperation and novelty be an artifact of temperature or sampling strategy (e.g., 0.7 fixed temperature)?\n\n2.How stable are these trends across different conversation lengths or asymmetric tasks (e.g., mentor–student dialogues)?\n\n3.Does representational similarity within specific layers (e.g., middle vs. final layers) predict distinct aspects of cooperation or creativity?\n\n4.Would using a nonlinear similarity measure (e.g., RBF CKA or SVCCA) change the direction or magnitude of the observed effects?\n\n5.Could representational diversity be deliberately engineered (e.g., through orthogonalization penalties) to optimize the cooperation–creativity balance?\n\n6.Regarding Section 3.1, the computation of representational similarity is based on a two-step process (representation extraction and similarity calculation using CKA). Were alternative or complementary similarity estimation approaches (e.g., model stitching, activation distance, or feature subspace comparison) explored? Additionally, given that inputs share the same tokenizer, could this influence the measured similarity and thus confound interpretation? Please clarify the methodological justification and the source or precedent of this computation pipeline, and whether it follows a standard or widely accepted protocol."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vcUQB7fwu7", "forum": "MHA8oTxoei", "replyto": "MHA8oTxoei", "signatures": ["ICLR.cc/2026/Conference/Submission19896/Reviewer_SRfe"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19896/Reviewer_SRfe"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761904344602, "cdate": 1761904344602, "tmdate": 1762932053726, "mdate": 1762932053726, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates whether representational similarity (measured via CKA) predicts interaction outcomes in multi-agent LLM systems. Through experiments with 276 model pairs across 8 tasks (4 cooperation games, 4 creativity tasks), the authors find that higher representational similarity correlates with better cooperation but reduced novelty."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "**Novel research direction**: First systematic study linking internal representations to multi-agent behavior, bridging neuroscience insights with AI systems.\n\n**Comprehensive experimental design**: 276 model pairs from 23 models across 8 diverse tasks, with rigorous mixed-effects regression controlling for model-specific effects.\n\n**Strong theoretical grounding**: Well-motivated by established neuroscience literature on neural synchrony and cooperation (Parkinson et al., 2018; Reinero et al., 2021).\n\n**Robust analysis methodology**: Multiple CKA variants, probe datasets, and reference models tested to ensure findings aren't artifacts of specific choices.\n\n**Clear practical implications**: Provides actionable insights for multi-agent system design regarding model selection trade-offs."}, "weaknesses": {"value": "**Limited evaluation on coordination benchmarks**: No results on established multi-agent coordination benchmarks like MultiAgentBench(Zhu et al. ACL 2025), at least discuss them in related work. The custom games (Section 4.1) may not capture real coordination complexity.\n\n**Disconnect from practical deployment**: Current multi-agent systems typically use single model types for consistency and cost. The paper doesn't address why practitioners would adopt heterogeneous model deployments.\n\n**Missing comparison with homogeneous systems**: No empirical comparison showing when heterogeneous pairs outperform single-model multi-agent systems. This limits practical applicability.\n\n**Narrow task scope**: Focus on self-designed games and creative writing tasks. Missing evaluation on practical domains like collaborative coding, math problem-solving, or scientific reasoning where multi-agent systems are actually deployed.\n\n**Incomplete analysis of model selection**: While showing similarity effects, the paper doesn't provide methods for selecting optimal model combinations for specific tasks. How should practitioners choose model pairs given a task?\n\n**Expected findings**: While the results (similar models good at collaborations, different are more creative), the results seems just as expected, would expect some more deep analysis and deeper finding"}, "questions": {"value": "**Benchmark evaluation**: Why not evaluate on MultiAgentBench or similar established benchmarks that specifically test coordination and competition abilities?\n\n**Practical deployment**: Given that most production systems use single model types, what scenarios would justify the added complexity and cost of heterogeneous model deployment?\n\n**Performance comparison**: Do any heterogeneous model pairs outperform the best single-model multi-agent system? What's the performance-cost trade-off?\n\n**Task generalization**: How do findings transfer to complex reasoning tasks (coding, mathematics, scientific discovery) where multi-agent systems show real benefits?\n\n**Model selection algorithm**: Can you provide a principled method for selecting model pairs given task requirements and budget constraints?\n\n**Scaling implications**: How does representational similarity affect systems with >2 agents? Does the trend hold for larger agent networks?\n\n**Temporal dynamics**: Do interaction patterns change over multiple rounds? Does prolonged interaction reduce the effect of initial similarity differences?\n\n**Failure analysis**: What specific coordination failures occur with dissimilar models? Can you provide detailed case studies beyond aggregate metrics?\n\n**Cost-benefit analysis**: Given API costs (Table 1 mentions costs but doesn't analyze), when is diversity worth the expense compared to using multiple instances of the best model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Xrx7hsgKcn", "forum": "MHA8oTxoei", "replyto": "MHA8oTxoei", "signatures": ["ICLR.cc/2026/Conference/Submission19896/Reviewer_FxRY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19896/Reviewer_FxRY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19896/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978020453, "cdate": 1761978020453, "tmdate": 1762932053115, "mdate": 1762932053115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}