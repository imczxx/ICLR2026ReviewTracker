{"id": "Jz5OcxVJbP", "number": 2630, "cdate": 1757172832605, "mdate": 1763567867648, "content": {"title": "Cross-Modal Feature Disentanglement with Contrastive Task Alignment for Multi-Modal Image Fusion", "abstract": "Multi-modal image fusion suffers from feature entanglement, where modality-specific, content-specific, and task-specific information becomes conflated in unified representation spaces, leading to suboptimal fusion quality and limited generalization. This paper proposes Cross-Modal Feature Disentanglement with Contrastive Task Alignment (CMD-CTA), a principled framework that addresses this fundamental challenge through mathematically grounded feature separation and semantic alignment. The approach introduces two key innovations: (1) differentiable orthogonal feature decomposition that enforces separation into content, modality, and task subspaces with theoretical guarantees for mutual information minimization; and (2) contrastive task alignment that establishes semantic bridges through learnable prototypes and multi-granularity learning. Extensive experiments across six fusion tasks demonstrate consistent improvements of 5.8-7.3\\% over state-of-the-art methods while achieving 15.7× parameter efficiency. On downstream object detection, CMD-CTA achieves 96.8\\% mAP\\@0.5, surpassing previous results by 6.1\\%. This work establishes a new paradigm for representation learning in multi-modal fusion with broad implications for computer vision and autonomous systems.", "tldr": "", "keywords": ["Multi-modal image fusion", "Feature disentanglement", "Orthogonal feature decomposition", "Gram–Schmidt reparameterization", "Multi-granularity contrastive learning", "Cross-task generalization"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dd584b3164e82c19b795fef3347a2ba09344753d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a Cross-Modal feature Disentanglement and Contrastive Task Alignment  framework, which disentangles features based on mathematically formulated principles of feature separation and semantic alignment. The work is motivated by the observation that standard deep learning architectures tend to share representations, causing essentially different semantic components to become mixed due to differences in information content. Across extensive experiments on six fusion tasks, the proposed algorithm achieves state-of-the-art performance."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The method effectively performs feature disentanglement.\n\nThe proposed algorithm improves both efficiency and performance."}, "weaknesses": {"value": "- The algorithmic section is confusing. Although the underlying theory is mathematically derived, the overall algorithmic framework is not introduced, and many module pipelines in Figure 2 are not adequately explained, leaving the paper incomplete.\n\n- Orthogonalization can only ensure that three sets of orthogonal bases do not interfere with each other; it cannot guarantee which set corresponds to content/modality/task. The authors should strengthen the proof for this part.\n\n- The paper formulates prototype evolution as an SDE with Brownian noise but does not implement the key parts of the SDE. For example, there is no definition of L_\"prototype\" ; Equation (6) lacks a noise term and does not use β anywhere.\n\n- The use of hard negatives in Equation (6) is incorrect: it adds the mean of the hard negatives H_kto the prototype, even though H_kdenotes difficult samples that are far from the prototype. This definition is puzzling.\n\n- Some symbols in Equation (8) are undefined—what is Y_N? The “symmetric InfoNCE formulation” cited for Equation (9) is also puzzling, as Eq. (9) only writes a single direction (from z_1to z_2).\n\n- The experimental section lacks necessary explanations and citations for the baseline (comparison) methods, and the dataset sources are insufficiently explained. Table 1 is rather rough. The content of Table 2 is also puzzling, lacking explanations of terminology and containing errors.\n\n- Lack of  training details description.\n\nOverall, the paper is rough from algorithm to experiments: definitions are lacking, the logic is not smooth, and the experiments are simplistic and crude. In my view, it does not meet the standard of a qualified paper. Therefore, I am unlikely to raise my score for this paper."}, "questions": {"value": "In traditional spatial-domain image fusion methods, one line of work computes feature saliency to obtain a mask that selects or allocates modality features. In essence, how does this kind of “disentanglement” differ from yours?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Gehgzbklv9", "forum": "Jz5OcxVJbP", "replyto": "Jz5OcxVJbP", "signatures": ["ICLR.cc/2026/Conference/Submission2630/Reviewer_JhRU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2630/Reviewer_JhRU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761050163143, "cdate": 1761050163143, "tmdate": 1762916313626, "mdate": 1762916313626, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CMD-CTA, a novel framework that tackles the core problem of feature entanglement in multi-modal image fusion. It achieves this through two key innovations: a differentiable orthogonal decomposition module that mathematically separates features into content, modality, and task-specific subspaces, and a contrastive task alignment strategy that uses dynamically evolving prototypes to semantically align these features."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.Theoretically-Grounded Disentanglement: Introduces a principled, mathematically sound framework for feature separation via orthogonal decomposition, directly addressing the core issue of feature entanglement with theoretical guarantees.\n\n2.Unified yet Specialized Performance: Achieves state-of-the-art results across six diverse fusion tasks with a single model, eliminating the need for task-specific architectures while maintaining specialized performance."}, "weaknesses": {"value": "1.The framework combines multiple loss components and novel mechanisms, making it potentially sensitive to hyperparameter tuning for optimal performance across different tasks.\n\n2.The contrastive alignment relies on predefined task prototypes, which may limit flexibility for entirely new tasks without retraining or prototype redesign."}, "questions": {"value": "1.How is the initial set of task prototypes defined, and to what extent does the framework's performance depend on this initialization? Could it adapt to a new downstream task without manually designing a new prototype?\n\n2.While orthogonality minimizes mutual information for Gaussian features, to what extent does this hold for the actual, highly non-Gaussian features learned by the deep network? Is the empirical success driven more by the mathematical constraint or the overall architecture?\n\n3.The final fused image is generated by the content feature 'selectively aggregating' modality-specific features through a decoder. What is the specific mechanism and criterion for this 'selective aggregation'?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "R21RkbIMIS", "forum": "Jz5OcxVJbP", "replyto": "Jz5OcxVJbP", "signatures": ["ICLR.cc/2026/Conference/Submission2630/Reviewer_Hr4K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2630/Reviewer_Hr4K"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877816057, "cdate": 1761877816057, "tmdate": 1762916313492, "mdate": 1762916313492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles feature entanglement in multi-modal image fusion, where modality-specific, content-specific, and task-specific information become mixed and harm generalization. It proposes CMD-CTA, which performs differentiable orthogonal decomposition to split features into content, modality, and task subspaces, with an information-theoretic motivation for reducing mutual information, and aligns task semantics via contrastive learning using dynamically evolving prototypes across multiple granularities. Extended experiments on tasks report consistent gains over prior work, strong parameter efficiency, and improved downstream detection performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper identifies feature entanglement as a key factor that degrades fusion quality and generalization, which is insightful.\n- CMD-CTA improves across metrics supporting generalization claims and its efficiency is compelling relative to heavy baselines.\n- The objective differs from traditional fusion methods and is interesting, leading to superior visual performance."}, "weaknesses": {"value": "- The phrase “the semantic gap between pixel-level fusion requirements and high-level representations” in line 089 lacks a detailed explanation and theoretical grounding.\n- The paper’s task formulation is ambiguous: it appears to conflate multi-task fusion objectives (e.g., MEF, MFF) with downstream tasks (e.g., object detection, segmentation). The different meanings and goals of these task types are not disentangled, which undermines the motivation.\n- The claim in lines 191–193—\"from an information-theoretic perspective …\"—appears inconsistent with the principle that a fused image should retain comprehensive information; minimizing mutual information seems to contradict that goal.\n- The MI bound in Eq. (3) relies on Gaussianity and zero cross-covariance, yet the paper provides no empirical MI estimates to verify that MI is actually reduced. Given that many image-fusion methods increase MI [1–3], please report MI quantitatively (e.g., before/after the proposed decomposition and against baselines) to validate the claimed “theoretical guarantees for mutual-information minimization” and to enable fair comparison\n- The stochastic differential equation in Eq. 5 is conceptually appealing, but its discrete implementation specifics (e.g., step size, noise term) are not quantified. \n- The description “weights $w_g$ are determined by mutual information contribution” in Eq.15 is not operationalized with an explicit estimation procedure.\n\n[1] Zhao Z, Xu S, Zhang C, et al. DIDFuse: Deep image decomposition for infrared and visible image fusion[J]. arXiv preprint arXiv:2003.09210, 2020.\n\n[2] Zhang Y, Liu Y, Sun P, et al. IFCNN: A general image fusion framework based on convolutional neural network[J]. Information Fusion, 2020, 54: 99-118.\n\n[3] Zhu P, Sun Y, Cao B, et al. Task-customized mixture of adapters for general image fusion[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024: 7099-7108."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3aQURpGVHR", "forum": "Jz5OcxVJbP", "replyto": "Jz5OcxVJbP", "signatures": ["ICLR.cc/2026/Conference/Submission2630/Reviewer_5mqv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2630/Reviewer_5mqv"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910907754, "cdate": 1761910907754, "tmdate": 1762916313334, "mdate": 1762916313334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework, CMD-CTA, for multi-modal image fusion. The core problem it addresses is \"feature entanglement,\" where modality, content, and task-specific information are conflated in a shared representation space. The authors introduce two main technical contributions: 1) a differentiable orthogonal feature decomposition method to enforce separation between feature subspaces (content, modality, task), and 2) a contrastive task alignment mechanism with dynamic prototypes to build semantic connections across tasks. The method is evaluated on six different fusion tasks and demonstrates state-of-the-art performance in terms of quantitative metrics and visual quality, while also claiming high parameter efficiency."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly identifies and articulates the problem of feature entanglement in multi-modal fusion, which is a significant and practical challenge.\n2. The authors have conducted experiments across an impressive range of six distinct fusion tasks (IVIF, MFIF, MEIF, etc.). This comprehensive evaluation strongly supports the claim that the proposed method is a generalized framework rather than a task-specific solution.\n3. The proposed CMD-CTA framework consistently outperforms previous state-of-the-art methods across all evaluated tasks, as shown in Table 1."}, "weaknesses": {"value": "1. The paper's novelty appears limited. While the framework is new, its core components, such as feature orthogonalization and multi-granularity contrastive learning, are well-established techniques. Furthermore, the architecture seems to be an assemblage of existing modules (e.g., Mamba, Swin) without sufficient justification. The manuscript lacks a clear rationale for these specific architectural choices. For instance, what is the precise benefit of Mamba's long-range dependency modeling for this task? Why is a hybrid Mamba-Swin architecture superior to simpler or more conventional backbones like standard CNNs or ViTs?\n2. The ablation study is unconvincing as it only assesses the contributions of the orthogonalization and contrastive learning components. Crucially, it omits any ablation on the core architectural modules. To validate the design choices, experiments demonstrating the necessity and effectiveness of Mamba, GNN, and other key structural components are essential.\n3. here is an apparent contradiction regarding the model's efficiency. The authors claim a highly lightweight model (e.g., only 3.15M parameters), yet the architecture incorporates several complex modules typically associated with a large parameter count. The manuscript should provide a detailed breakdown of the parameter distribution across modules to clarify how this efficiency is achieved and substantiate the claim.\n4. The manuscript fails to provide any details or sensitivity analysis for the hyperparameters (α, β, and γ) in the loss function. It is unclear how these values were chosen or whether they are consistent across different tasks. For reproducibility, these values should be explicitly stated and their selection process justified.\n5. The set of quantitative evaluation metrics is incomplete. For a more comprehensive analysis, it is advisable to include information-theoretic metrics, such as Entropy (EN), which are commonly used and highly relevant in this research area.\n6. There are critical issues in Table 1. First, the SCD score for MUFusion in sub-table E is reported as a negative value. This is highly unusual and requires clarification—is it a typo, an evaluation error, or an actual result? If the performance is indeed that low, a more competitive baseline should be included. Second, the rightmost sub-table is cluttered and poorly formatted, which severely impairs its readability.\n7. The bibliography requires substantial revision. It is replete with inaccuracies, including frequent discrepancies in author names and publication years that do not match the original sources."}, "questions": {"value": "Please see Weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ifR6spscWZ", "forum": "Jz5OcxVJbP", "replyto": "Jz5OcxVJbP", "signatures": ["ICLR.cc/2026/Conference/Submission2630/Reviewer_HFUb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2630/Reviewer_HFUb"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2630/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762013238144, "cdate": 1762013238144, "tmdate": 1762916313143, "mdate": 1762916313143, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "## Closing remarks to all reviewers\n\n- **On soundness and significance.**  \n  We do not over-claim universal guarantees. We (i) use orthogonality as a principled mechanism to reduce cross-subspace interference, (ii) anchor task semantics via vMF contrastive prototypes with quantified dynamics, and (iii) demonstrate empirical MI reductions while preserving/increasing \\(I(F_c; Y)\\). These, together with broad SOTA improvements across six tasks and strong parameter efficiency (0.777M params; 9.42 GFLOPs), support both the methodology and its practical impact.\n\n- **On reproducibility and fairness.**  \n  We provide training specifics, dataset protocols, MI estimation procedures (KSG/MINE), hyperparameter sensitivity, and module-wise complexity, with dual-tool FLOPs/params checks and code references.\n\n- **On remaining suggestions.**  \n  We will (a) add MI comparisons against baselines, (b) expand backbone-swap ablations, (c) finalize reference corrections, and (d) further refine table layouts in the camera-ready.\n\nWe sincerely thank the reviewers for their insightful comments. We hope the clarified theory, added empirical evidence, expanded ablations, and strengthened reproducibility address your concerns and merit a positive reassessment."}}, "id": "QPcQyyXMx2", "forum": "Jz5OcxVJbP", "replyto": "Jz5OcxVJbP", "signatures": ["ICLR.cc/2026/Conference/Submission2630/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2630/Authors"], "number": 6, "invitations": ["ICLR.cc/2026/Conference/Submission2630/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763627269689, "cdate": 1763627269689, "tmdate": 1763627269689, "mdate": 1763627269689, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}