{"id": "69mFD2J9rg", "number": 17819, "cdate": 1758280919709, "mdate": 1763045069732, "content": {"title": "Link Prediction or Perdition: the Seeds of Instability in Knowledge Graph Embeddings", "abstract": "Embedding models (KGEMs) constitute the main link prediction approach to complete knowledge graphs. \nStandard evaluation protocols emphasize rank-based metrics such as MRR or Hits@$K$, but usually overlook the influence of random seeds on result stability. \nMoreover, these metrics conceal potential instabilities in individual predictions and in the organization of embedding spaces.\nIn this work, we conduct a systematic stability analysis of multiple KGEMs across several datasets. \nWe find that high-performance models actually produce divergent predictions at the triple level  and highly variable embedding spaces. \nBy isolating stochastic factors, $\\textit{i.e.}$, initialization, triple ordering, negative sampling, dropout, hardware, we show that each independently induces instability of comparable magnitude. \nFurthermore, our results reveal no correlation between high MRR scores and stability. \nThese findings highlight critical limitations of current benchmarking protocols, and raise concerns about the reliability of KGEMs for knowledge graph completion.", "tldr": "We show that knowledge graph embeddings are instable leading to concerns for link prediction in knowledge graph completion.", "keywords": ["Knowledge Graph Embedding", "Stability", "Link Prediction"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/f29225c963f879ba2e58d357a8fd2df7afa64bef.pdf", "supplementary_material": "/attachment/98d5ab6e1a4165b21bb0ce5b5f3b8a5d0a849c03.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates the stability of Knowledge Graph Embedding Models (KGEMs) under variations in random seeds and other stochastic factors during training. The authors propose using Jaccard similarity over top-K predictions (Pred-Jaccard@K) and embedding neighborhoods (Space-Jaccard@K) as complementary stability metrics. Their findings challenge the reliability of KG completion pipelines and call for more robust evaluation protocols that account for prediction-level consistency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "Given the increasing use of KGEMs in real-world applications, understanding seed-induced variability is essential for trustworthy deployment.\n\nThe authors isolate and evaluate five distinct sources of randomness, using multiple models, datasets, and stability metrics."}, "weaknesses": {"value": "The proposed metrics Pred-Jaccard@K and Space-Jaccard@K do not consider the order of candidate entities, which means they still cannot address the potential issues in Figure 1.\n\nWhile the paper includes representative models from major families, it omits some recent and widely used architectures, e.g., ComplEx and RotatE.\n\nSome datasets used in the paper are also small and outdated (e.g., Nations: 14 entities; Kinship: 104 entities).\n\nThe paper is purely empirical. A discussion or even preliminary analysis of why certain models (e.g., TransE) are more stable than others would add significant value.\n\nFrom my perspective, if we want, we can reproduce the results of most KGEMs precisely by replacing all the random modules with deterministic (pseudo-random) algorithms. In most cases, random doesn't mean unstable, just like LLMs. The authors may exaggerate the negative impacts of randomness in KGEMs."}, "questions": {"value": "Please see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "J8bSZVosFT", "forum": "69mFD2J9rg", "replyto": "69mFD2J9rg", "signatures": ["ICLR.cc/2026/Conference/Submission17819/Reviewer_vwJx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17819/Reviewer_vwJx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663145495, "cdate": 1761663145495, "tmdate": 1762927658573, "mdate": 1762927658573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "54NKeHNpUQ", "forum": "69mFD2J9rg", "replyto": "69mFD2J9rg", "signatures": ["ICLR.cc/2026/Conference/Submission17819/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17819/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763045069003, "cdate": 1763045069003, "tmdate": 1763045069003, "mdate": 1763045069003, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Performs an experimental study to assess the stability of multiple common knowledge graph embeddings models (KGEM) w.r.t. to changing training seeds. This is done by fixuing seeds for various sources of randomness (initialization, batching, negative sampling, dropout), running the model, and assessing stability of the top-k predictions. My main concern with this paper is its very limited contribution (see W1)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "S1. Provides additional experimental evidence for KGEM stability"}, "weaknesses": {"value": "W1. Limited contribution. Instability has been studied in prior work (as the paper clearly describes) and remedies such as ensembling have also been explored. The paper thus adds a data point to a well-studied problem, but no more.\n\nW2. Metrics not fully convincing. The paper assesses stability by (i) similarity of top-k predictions and (ii) similarity of entity embeddings. I do not find (i) convincing since it includes \"wrong\" predictions and ultimately assesses whether the model is unstable in its wrong predictions. That's not a very interesting question, and prior work indeed focused on the correct predictions instead. To emphasize this point, suppose that a model ranks the correct answer at the first position, and all other answers in random order afterwards. I'd not consider such a model unstable. As for (ii), the similarity of entity embeddings is useful, but is likely dependent on which notion of neighborhood is being used (e.g., Euclidean? Cosine?) and how the KGEM actually \"uses\" these embeddings.\n\nW3. The set of considered models is overly limited. Popular and well-performing KGEM such as ComplEx or RotatE or HAKE are missing."}, "questions": {"value": "-"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "qyOVmh6BEQ", "forum": "69mFD2J9rg", "replyto": "69mFD2J9rg", "signatures": ["ICLR.cc/2026/Conference/Submission17819/Reviewer_ToXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17819/Reviewer_ToXJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901206211, "cdate": 1761901206211, "tmdate": 1762927658154, "mdate": 1762927658154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Problem:** The paper investigates a methodological issue in the evaluation of Knowledge Graph Embedding Models (KGEMs), the standard method for link prediction (LP) in incomplete KGs. The core problem is a discrepancy between standard ML evaluation and common practice in the KGEM field. Standard procedure mandates reporting performance averaged over multiple runs with different random seeds, but KGEM literature often reports metrics (e.g., MRR, Hits@K) from a single \"best\" run. The paper's motivation is that these aggregate scores conceal \"local\" instabilities. It posits that two KGEM instances, trained with different seeds, could achieve nearly identical MRR scores while producing \"different triple-level predictions\" and learning \"distinct embedding spaces.\" This issue raises concerns about the reliability and reproducibility of KG completion, as the specific facts inferred by a model could vary based purely on the random seed used during its training.\n\n**Methodology:** A framework for quantifying instability.\n- The work systematically investigates the impact of four primary sources of randomness: Parameter Initialization (I), Triple Ordering (O), Negative Sampling (N), and Dropout (D).\n- To isolate sources of instability, a \"comparison group\" is defined as a set of model instances obtained by varying one of the four source of randomness---linked to different seeds, while keeping the others fixed.\n- The core of the analysis is to measure the pairwise similarity between all runs within a group using two primary metrics:\n   - Prediction Stability (Pred-Jaccard@K): Measures the Jaccard similarity between the sets of top-K predicted entities for the same query (e.g., (h, r, ?)), averaged over all test queries. A low score indicates the models are predicting different facts to complete the KG.\n   - Embedding Space Stability (Space-Jaccard@K): Measures the Jaccard similarity of the K-Nearest-Neighbors for each entity's embedding, averaged over all entities. A low score indicates the models learned different geometric representations.\n- The analysis is corroborated with additional similarity measures, i.e., Rank-Biased Overlap (RBO) and Centered Kernel Alignment (CKA).\n\n**Experimental Setup:**\n- 5 Models: TransE (geometric), DistMult (factorization), ConvE (convolutional), RGCN (GNN), and a Transformer-based model.\n- 4 Datasets: WN18RR, CoDEx-S, Kinship, Nations.\n- 4 GPUs: Nvidia 2080 Ti, 1080 Ti, T4, A40, A100.\n- 5 seeds: 42, 283, 358, 698, 887.\n- Hyperparameter search over embedding dimensions (128, 256, 512) and learning rates (10^{-6, -5, -4, -3, -2, -1}).\n\n**Results:**\n- *RQ1. Are KGEMs stable across different random seeds?*\n   - No. The analysis reveals that while aggregate MRR scores are deceptively stable (exhibiting low standard deviation), the models show \"pronounced variability\" at the local level. Runs with similar MRR scores produce divergent top-K predictions (Pred-Jaccard@K) and learn inconsistent entity neighborhoods (Space-Jaccard@K).\n- *RQ2. How do the different sources of randomness contribute to instability?*\n   - All sources independently cause instability. The study shows that varying any single one of the four sources (I, O, N, or D) is sufficient to induce significant instability.\n   - Running the exact same code and seeds on different GPU models produces variability of comparable magnitude to changing the software seeds.\n- *RQ3. Is model stability correlated with link prediction performance?*\n   - No. While the worst-performing models (lowest MRR) were, intuitively, highly unstable, the analysis found \"no correlation between predictive performance and stability when comparing the best and median configurations.\" A model with a state-of-the-art MRR is not guaranteed to be more stable than a median-performing one, meaning performance and reliability are separate evaluation dimensions."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Well-structured empirical analysis. The paper delivers a thorough, multi-dimensional analysis of stability in KGEMs, systematically isolating sources of randomness and including hardware effects.\n- Practical implications. The observed disconnect between high MRR and local stability is a direct challenge to current benchmarking methodology.\n- Clear experimental protocols; code, embeddings, and data availability is addressed, promoting transparent review."}, "weaknesses": {"value": "- *Limited theoretical depth.* The discussion of why such instability arises is mostly speculative rather than formal. For instance, no mathematical analysis is given, even in outline, connecting model non-convexity, overparameterization, and instability. The lack of structural insight limits the generative potential for new stability-enforcing methods.\n- *Scope of experimental coverage.* Only five KGEM models are analyzed, which—while representative—exclude many important recent advances, or those explicitly claiming robustness. The selected datasets (WN18RR, CoDEx-S, Kinship, Nations) are mostly small-sized and mid-sized benchmarks. As the authors themselves note, statistics—particularly on the smaller datasets—should not be broadly generalized to real-world settings. No results are shown for large-scale industry KGs or very large, noisy graphs, which are a key target application. \n- *Inappropriate datasets for metrics.* The analysis relies on Jaccard@K for K=10. However, two of the four datasets, Nations and Kinship, are extremely small. Nations has only 14 entities in total. A Space-Jaccard@10 (K-Nearest-Neighbors at K=10) analysis on a graph of 14 nodes is statistically meaningless, as K represents ~70% of the entire dataset. The paper notes this in a figure caption (\"scores... are inflated\") but proceeds to include these invalid results in its main heatmaps (Fig 4), which average results and potentially skew the visual conclusions.\n- *Limited bridging to solution space.* The paper identifies the problem with force but does not take the next step to propose, experiment with, or even outline concrete candidate mechanisms for stability.\n- *Baseline settings.* Hyperparameter sweeps are relatively narrow. E.g., choices such as fixed dropout (except for seed variation) may confound variance attribution.\n- *Uncontrolled implementation framework.* The study does not explicitly state that all 5 models (TransE, ConvE, RGCN, etc.) were implemented within a single, unified software library. The paper's appendix, in fact, references different sources (e.g., LibKGE, PyTorch Geometric) for different model configurations. This introduces a major confounding variable. The measured instability attributed to a model architecture may, in part, be an artifact of its specific library implementation.\n- *Rank-agnostic prediction metric.* The primary metric for prediction, Pred-Jaccard@K, treats the top-K predictions as an unordered set. This is a poor fit for link prediction, where rank is a crucial factor. While the paper includes the rank-aware RBO metric in the appendix, its reliance on a rank-agnostic set metric for its main claims is questionable.\n- *Contradiction in space metrics.* The paper's primary metric for embedding space, Space-Jaccard@K, shows very high instability (e.g., scores below 0.1 for RGCN). However, its secondary \"validation\" metric, CKA (Centered Kernel Alignment), shows very high stability (scores of 0.8-1.0) for the same models (e.g., TransE, ConvE). The paper dismisses this massive discrepancy by saying they show similar variation patterns, but the absolute scores tell different stories. More discussion is needed.\n\nTYPOS AND SUGGESTIONS\n- The figures appear to use default matplotlib or Draw.io color schemes. While the data is clear, the visual presentation does not match the quality of the research. Using more refined color palettes and ensuring high-resolution vector graphics would significantly improve the paper's presentation.\n- There is a contradiction regarding the experimental hardware. Section 4.1 (L289-L290) states: \"...the hardware H has been fixed to a GPU Nvidia GeForce RTX 2080 Ti (11 GiB).\" Section 4.2.2 (L374-L377) states: \"We run models on multiple GPUs: Nvidia GeForce RTX 2080 Ti... Nvidia GeForce GTX 1080 Ti... Nvidia Tesla T4... Nvidia A40... and Nvidia A100...\" The authors should better clarify that the 2080 Ti was the default hardware for all experiments except for the specific hardware-variance analysis conducted for RQ2.\n- Several acronyms are defined multiple times throughout the text, which disrupts the reading flow.\n- The paper would benefit from a final proofread to catch minor errors, such as missing spaces after punctuation (e.g., noted by the reviewer on L301)."}, "questions": {"value": "The Reproducibility Statement (L492) promises to make code and embeddings publicly available. To adhere to best practices in open science and to provide clarity for future users, the authors should specify the open-source license (e.g., MIT, Apache 2.0) under which the materials will be released."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "muuqvyN2YJ", "forum": "69mFD2J9rg", "replyto": "69mFD2J9rg", "signatures": ["ICLR.cc/2026/Conference/Submission17819/Reviewer_UujM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17819/Reviewer_UujM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934695148, "cdate": 1761934695148, "tmdate": 1762927657601, "mdate": 1762927657601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}