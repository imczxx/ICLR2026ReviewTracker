{"id": "XpoQ812d0A", "number": 15309, "cdate": 1758250166770, "mdate": 1759897314417, "content": {"title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models", "abstract": "Numerous applications of large language models (LLMs) rely on their ability to perform step-by-step reasoning. However, the reasoning behavior of LLMs remains poorly understood, posing challenges to research, development, and safety. To address this gap, we introduce landscape of thoughts (LoT), the first landscape visualization tool to inspect the reasoning trajectories with certain reasoning methods on any multi-choice dataset. We represent the textual states in a trajectory as numerical features that quantify the states' distances to the answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis with the landscape of thoughts effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks. It also uncovers undesirable reasoning patterns, such as low consistency and high uncertainty. Additionally, users can adapt LoT to a model that predicts the property they observe. We showcase this advantage by adapting LoT to a lightweight verifier that evaluates the correctness of trajectories. Empirically, this verifier boosts the reasoning accuracy and the test-time scaling effect.", "tldr": "We introduce a visualization tool for users to inspect the reasoning paths of chain-of-thought and its derivatives on any multi-choice dataset", "keywords": ["Large Language Models", "Visualization"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3c3a139a26c8e48e39c5d8ee1b1fb745f0295117.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a novel approach for monitoring long chains of thought by visualizing reasoning trajectories in a two-dimensional feature embedding space. The proposed monitoring method, however, is restricted to multiple-choice problems: by appending each answer option to the intermediate reasoning states and computing the corresponding perplexity scores, the model estimates the “distance” between its current reasoning state and each candidate answer."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a meaningful and challenging research problem—how to effectively monitor and audit reasoning traces (i.e., chains of thought), particularly when reasoning models generate extremely long CoTs that are infeasible for human evaluation to scale.\n\n2. The work demonstrates originality and novelty. Although the key components of the proposed method (e.g., perplexity and t-SNE) are not new, their combination and the new analytical perspective are refreshing and interesting."}, "weaknesses": {"value": "W1: The scope of this paper is narrowly limited to multiple-choice questions with known ground-truth answers. It remains unclear how the proposed visualization tool can generalize to broader reasoning analysis or practical model auditing. Chain-of-thought monitoring and scalable oversight are indeed underexplored areas, people may hope to solve problems like detecting and controlling hallucination or unsafe content generation—tasks where ground-truth answers or verifiable signals are typically unavailable. However, this paper focuses instead on visualizing the distance between reasoning traces leading to correct versus incorrect answers. The conclusions (for example, wrong reasoning paths converge to wrong answers faster than correct ones) depend heavily on the existence of ground-truth labels. Such relative convergence patterns would not hold in open-ended reasoning tasks without definitive answers, substantially limiting the applicability and significance of this visualization approach for real-world reasoning analysis.\n\nW2: It is not clear how **robust or faithful** the proposed visualization method and the conclusions derived from it actually are. The final reasoning landscape is constructed from state features computed via the perplexity of appending choice answers to intermediate reasoning steps. However, many unrelated factors could influence this measurement. For instance, even when the model follows a correct reasoning path, forcibly inserting the final answer into its ongoing reasoning can disrupt coherence and yield artificially high perplexity scores. Moreover, the phrasing of the final answer itself may significantly affect the computed perplexity. In other words, the distance metric may be driven by superficial textual factors rather than genuine semantic relations. While the resulting visualizations and observations are intriguing, the paper does not convincingly demonstrate the reliability or faithfulness of these conclusions.\n\n* Typo*:\n(1) Line 381, converge means... -> convergence means ..."}, "questions": {"value": "Q1. How does the number of sampled reasoning traces affect the resulting landscape? If the number of rollouts increases, will the overall structure of the landscape remain stable, or does it change significantly? A sensitivity analysis could help clarify the robustness of the visualization.\n\nQ2. In the visualizations, blue and red trajectories represent reasoning traces leading to correct and incorrect answers, respectively. For challenging questions where most rollouts fail, what meaningful insights can be extracted from those predominantly incorrect trajectories? \n\nQ3. The trends of uncertainty and perplexity are confusing. For example in Fig.2, all models exhibit a non-decreasing uncertainty over time, suggesting that they become less committed to a single final answer as reasoning progresses. Similarly, the perplexity for their own reasoning traces increases continuously. What does this imply? A clearer explanation would help readers interpret these metrics more meaningfully."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aQODVHd2Tg", "forum": "XpoQ812d0A", "replyto": "XpoQ812d0A", "signatures": ["ICLR.cc/2026/Conference/Submission15309/Reviewer_VKLY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15309/Reviewer_VKLY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760970297445, "cdate": 1760970297445, "tmdate": 1762925606891, "mdate": 1762925606891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a landscape of thoughts, the first visualization tool for users to inspect the reasoning paths of the chain-of-thought and its derivatives on any multi-choice dataset. The authors represent the states in a reasoning path as feature vectors that quantify their distances to all answer choices. These features are then visualized in two-dimensional plots using t-SNE. Qualitative and quantitative analysis, combined with the landscape of thoughts, effectively distinguishes between strong and weak models, correct and incorrect answers, as well as different reasoning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Strengths:\n1.\tImportant contribution towards understanding the large reasoning models\n2.\tA model can be trained to predict any property users observe\n3.\tGood visualization plots"}, "weaknesses": {"value": "Weaknesses:\n1. Only multiple-choice kind of questions are considered. I am not sure if the same insights transfer to the reasoning questions with no choices.\n2. More models could be considered."}, "questions": {"value": "Questions:\n\nQ1: In Figure 1, it says that the wrong paths quickly converge to wrong answers, while the correct paths slowly converge to correct answers. However, observation 3.1 says that the landscape converges faster as the model size increases. But shouldn't the bigger model converge slowly, as it is expected to give more correct solutions? Any reason for this discrepancy?\n\nQ2: Are the insights similar on reasoning questions without any multiple choices? \n\nQ3: What about other reasoning methods, such as a graph of thoughts or an algorithm of thoughts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bonLioy1DS", "forum": "XpoQ812d0A", "replyto": "XpoQ812d0A", "signatures": ["ICLR.cc/2026/Conference/Submission15309/Reviewer_fVdk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15309/Reviewer_fVdk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762029714849, "cdate": 1762029714849, "tmdate": 1762925606608, "mdate": 1762925606608, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Landscape of Thoughts (LoT), as introduced by the authors, is a general tool to visualize and analyze LLM reasoning trajectories on multi-choice tasks. The main idea is to represent each intermediate state in a chain-of-thought as a feature vector of distances to all answer choices, and the distance is the length-normalized inverse likelihood (perplexity) computed by the same LLM, after that, these state features (plus choice “landmarks”) are projected to 2D with t-SNE to form “landscapes. Beyond visualization, the authors adapt the state features into a lightweight verifier that re-weights trajectories at test time, improving accuracy and exhibiting strong test-time scaling without modifying base model parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper offers an interesting view to interpret LLM reasoning dynamics by transforming text-based chains of thought into measurable geometric trajectories. This provides an intuitive and scalable diagnostic framework for analyzing reasoning behavior.\n\n2. The study visualizes reasoning across multiple datasets, model sizes, and decoding strategies, making it one of the first to present a comparative, interpretable view of LLM reasoning evolution rather than static outputs.\n\n3. By extending the visualization features into a simple verifier that reweights trajectories at test time, the work shows a way to potentially improve reasoning reliability without modifying model parameters."}, "weaknesses": {"value": "1. It seems like, the paper assumes that reasoning trajectories can be meaningfully mapped into a 2D space by computing each thought’s log-probability distance to candidate answers and projecting them via t-SNE. However, these representations primarily capture the model’s surface-level likelihood distribution, and how confident it is about next tokens, but not the latent cognitive process of reasoning. Since t-SNE emphasizes local density and distorts global geometry, the observed “paths” and “convergence” patterns likely arise from projection artifacts or distribution sharpness (e.g., larger models having more peaked probabilities) rather than reflecting genuine logical progression or conceptual reasoning structure.\n\n2. The claim that larger models exhibit more consistent and reliable reasoning is not well supported. The observed trajectory regularity may simply reflect statistical effects of scale, larger models tend to produce sharper probability distributions and more uniform language patterns—rather than genuine improvements in reasoning stability. Moreover, trajectory consistency is heavily influenced by task semantics and prompt structure, which are not controlled in the experiments. Without isolating these confounding factors, it is unclear whether the smoother “paths” truly arise from deeper reasoning ability or from surface-level distributional sharpness.\n\n3. Some interesting findings are based on visual inspection and single-case illustrations without statistical validation or controlled comparisons, making the observed “convergence” patterns more narrative than scientifically substantiated."}, "questions": {"value": "1. The paper assumes that textual thoughts can be meaningfully embedded into a continuous numerical space. However, since each question has its own answer set, how is cross-question comparability achieved? Does a global semantic space actually exist, or are the landscapes a collection of locally defined subspaces stitched together?\n\n2. The visualization relies on t-SNE projections of log-likelihood-based features. Given that t-SNE introduces nonlinear distortions, to what extent do the “clusters” or “paths” in the landscape correspond to real structure in the high-dimensional reasoning space?\n\n3. Since each thought’s representation is derived from the model’s own likelihood distribution, how can we be sure that the resulting structure reflects reasoning dynamics rather than simple probability concentration or syntax-level similarity?\n\n4. The notion of “trajectory convergence” implicitly assumes a unified space across all problems and reasoning chains. How is this space defined mathematically? Is it constructed per dataset (via concatenation of all samples), or does it represent some theoretically consistent manifold of reasoning states?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XgHQHz5eat", "forum": "XpoQ812d0A", "replyto": "XpoQ812d0A", "signatures": ["ICLR.cc/2026/Conference/Submission15309/Reviewer_FwuD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15309/Reviewer_FwuD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762079266021, "cdate": 1762079266021, "tmdate": 1762925606178, "mdate": 1762925606178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to analysis the reasoning process of LLMs via visualisation. Specifically, it projects the intermediate states (defined by the sequence of output from prompt up to the token of the step being considered) of LLMs into 2d planes via t-SNE. Three metrics are introduced to measure the consistency, uncertainty, and perplexity of the reasoning/generation quality.\n\nEmpirical results on models with different number of parameters in the Llama-3.1/3.2 family and across multiple datasets are presented。 Based on the results, the paper provides several observations along the dimension of LLMs, tasks, and reasoning methods.\n\nFurthermore, a verification method based on convergence and consistency is proposed to re-weight answers generated by LLMs. This method is demonstrated to improve the accuracy of Llama models with 1B and 3B parameters."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents extensive results and comparisons between visualisation results of different LLMs on different tasks and with different reasoning methods. \n- Some observations, e.g., the consistency observation 3.8, might be useful for better regularisation of the reasoning process of LRMs."}, "weaknesses": {"value": "- Most of the presented visualisations and qualitative metrics are not significant enough to draw meaningful conclusions. It is difficult to tell if the t-SNE visualisation of reasoning states are showing consistent patterns across all the tested models without a systematic way of verifying their existence. Most of the visualisations in this paper can be seen as good case studies but not enough to convince me that the results are generalisable.\n\n- Only Llama 3.1/3.2 family of LLMs are tested in this paper. This further makes me doubt the generalisability of the observations in this paper.\n\n- The observations listed in section 3, although seemingly reasonable, are mostly not very surprising. Furthermore, most of the observations are not verified by other evidence (maybe except 3.7 and 3.8 which are at least partially verified in section 4). This makes me doubt whether these observations universally exist.\n\n- The readabilities of most of the figures are slightly low. The paper has to be read in a monitor, otherwise it is difficult to read the texts or recognise the patterns in the scatter plot."}, "questions": {"value": "Please refer to my concerns above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vshNLeIQRg", "forum": "XpoQ812d0A", "replyto": "XpoQ812d0A", "signatures": ["ICLR.cc/2026/Conference/Submission15309/Reviewer_WVGo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15309/Reviewer_WVGo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15309/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762185321089, "cdate": 1762185321089, "tmdate": 1762925605704, "mdate": 1762925605704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}