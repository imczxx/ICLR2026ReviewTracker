{"id": "JcRWcwL2ll", "number": 18434, "cdate": 1758287793347, "mdate": 1759897103856, "content": {"title": "Efficient and scalable MARL from images by trust-region autoencoders", "abstract": "Vision-based multi-agent reinforcement learning (MARL) suffers from poor sample efficiency, limiting its practicality in real-world systems. Representation learning with auxiliary tasks can enhance efficiency; however, existing methods, including contrastive learning, often require the careful design of a similarity function and increase architectural complexity. In contrast, reconstruction-based methods that utilize autoencoders are simple and effective for representation learning, yet remain underexplored in MARL. We revisit this direction and identify unstable representation updates as a key challenge that limits its sample efficiency and stability in MARL. To address this challenge, we propose the Multi-agent Trust Region Variational Autoencoder (MA-TRVAE), which stabilizes latent representations by constraining updates within a trust region. Combined with a state-of-the-art MARL algorithm, MA-TRVAE improves sample efficiency, stability, and scalability in vision-based multi-agent control tasks. Experiments demonstrate that this simple approach not only outperforms prior vision-based MARL methods but also MARL algorithms trained with proprioceptive state. Furthermore, our method can scale up to more agents with only slight performance degradation, while being more computationally efficient than the underlying MARL algorithm.", "tldr": "", "keywords": ["Multi-agent Reinforcement Learning", "Represention Learning", "Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/413d363134a7856909ea38b52bfaf9f9c146d85c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes MA-TRVAE, a Multi-agent Trust Region Variational Autoencoder for vision-based multi-agent reinforcement learning (MARL). The core motivation is that reconstruction-based representation learning (e.g., VAE) is simple and efficient but unstable when jointly trained with policy updates. The authors observe that smaller $\\beta$ in $\\beta$-VAE improves task performance but leads to unstable latent updates, harming policy learning. To mitigate this, they introduce a trust region penalty on latent posterior updates, constraining the representation change between consecutive steps. The resulting algorithm improves stability, sample efficiency, and scalability compared to MAPPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Simple yet elegant method.**  \n   Adding a KL penalty between consecutive posteriors is a minimal but effective extension that is computationally light.\n\n2. **Empirical insights.**  \n   The analysis of $\\beta$’s influence on stability and performance is detailed and well-motivated, providing clear intuition behind the necessity of the trust-region regularization.\n\n3. **Clarity.**  \n   The paper is clearly written and easy to follow, especially in methodology and experimental setup."}, "weaknesses": {"value": "1. **Technical novelty is moderate.**  \n   While the trust-region penalty for latent stability is reasonable, it is a straightforward adaptation of existing concepts (TRPO/PPO) to the representation space. The core mechanism resembles regularized VAE updates.\n\n2. **Limited evaluation scope.**  \n   Experiments are conducted solely on the MAQC benchmark. Although this is a strong testbed, additional validation on diverse MARL domains (e.g., SMAC with vision input) would strengthen generality.\n\n3. **Lack of theoretical justification.**  \n   The method is empirically effective but lacks formal analysis (e.g., convergence or stability bounds for latent updates)."}, "questions": {"value": "1. Could the authors further analyze the sensitivity of $\\beta_2$? It might help understand how strong the constraint should be for balancing stability vs. expressivity.\n\n2. Is the same trust-region idea applicable to contrastive or predictive representation learners (e.g., MA2CL)? Some comparison or discussion would be useful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XtJQQBz41h", "forum": "JcRWcwL2ll", "replyto": "JcRWcwL2ll", "signatures": ["ICLR.cc/2026/Conference/Submission18434/Reviewer_me5b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18434/Reviewer_me5b"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761724028447, "cdate": 1761724028447, "tmdate": 1762928129523, "mdate": 1762928129523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MA-TRVAE (Multi-Agent Trust-Region Variational Autoencoder), a new method to improve sample efficiency and stability in vision-based multi-agent reinforcement learning (MARL). The method incorporates a trust-region constraint into the VAE objective, ensuring smooth updates of latent representations. Combined with MAPPO, the method achieves better performance across multi-agent control tasks such as flock formation and leader-follower control"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper avoids complex attention or contrastive mechanisms while still outperforming prior visual MARL baselines. Its adaptation of trust-region optimization to the representation learning stage is novel and well-motivated, offering a new perspective on stabilizing learned visual embeddings."}, "weaknesses": {"value": "* The main limitation is the narrow empirical scope: all experiments are conducted in the MAQC drone environment, leaving uncertainty about generalization to other multi-agent domains (e.g., manipulation, different embodiments).\n* Moreover, although the method is computationally efficient, hyperparameter sensitivity of β₁ and β₂ is not deeply analyzed—these may be crucial for balancing reconstruction quality and stability."}, "questions": {"value": "* How sensitive is MA-TRVAE to the trust-region coefficient β₂? Could different values lead to either under- or over-constrained representations?\n* Since all tasks are vision-based drone control, have the authors tried other domains (e.g., multi-agent MuJoCo or social navigation) to confirm scalability?\n* How does the trust-region constraint compare against simpler stabilization methods such as target encoders or temporal smoothing of latent features?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "NTuEzeRyrG", "forum": "JcRWcwL2ll", "replyto": "JcRWcwL2ll", "signatures": ["ICLR.cc/2026/Conference/Submission18434/Reviewer_cW5t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18434/Reviewer_cW5t"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799902787, "cdate": 1761799902787, "tmdate": 1762928128731, "mdate": 1762928128731, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MA-TRVAE, a trust-region-regularized variational autoencoder for vision-based multi-agent reinforcement learning (MARL). The method targets the issue that, while autoencoder-based  state representations are simple and efficient, they suffer from instability and sample inefficiency in MARL due to representation drift. MA-TRVAE applies a KL-divergence-based trust region constraint to stabilize latent representations between optimization steps, enabling improved learning efficiency and scalability. Results on multi-agent quadcopter control (MAQC) tasks demonstrate improved sample efficiency, stability, scalability, and computational efficiency relative to multiple baselines, including MAPPO, MA2CL, and vanilla (beta-)VAE approaches."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Simplicity and Clarity of Approach**: The integration of a trust-region KL penalty into a VAE framework for MARL is conceptually simple yet effective, elegantly combining ideas from trust-region policy optimization and representation learning.  \n- **Analysis of Representation Drift**: Identifying and quantifying representation instability (via posterior KL divergence) as a key performance bottleneck in MARL is a valuable contribution. **Figure 2** and **Figure 4** clearly demonstrate how the trust-region constraint stabilizes representation drift in practice, reinforcing the core motivation behind MA-TRVAE.  \n- **Computational Efficiency**: Analyses of wall-clock time and training speed (e.g., **Figure 1** and **Figure 8**) show that MA-TRVAE is not only more sample-efficient but also significantly less resource-intensive than its strongest baseline (MA2CL).  \n- **Reproducibility**: The paper provides thorough implementation details, algorithmic pseudocode, and complete hyperparameter tables, along with a link to the source code, enhancing transparency and reproducibility."}, "weaknesses": {"value": "1. **Single Benchmark and Limited Environment Diversity**: The main experiments rely solely on the multi-agent quadcopter control (MAQC) simulator, focusing primarily on flocking and leader-follower tasks (see Section 5 and **Appendix C, Figure 7**). This considerably limits the claims of generality, applicability, and robustness to other vision-based MARL scenarios.  \n2. **Over-reliance on Reconstruction-based Baselines**: While strong baselines (MAPPO variants and MA2CL) are included, several recent state-of-the-art vision-based MARL methods (e.g., those utilizing transformers, cross-agent representation learning, or newer contrastive regularization techniques) are not compared. The absence of these may weaken the empirical claims. At minimum, it would be useful to compare against recent MAE-based approaches such as [1] or [2], as well as alternative MARL algorithms like [3] or [4].  \n3. **Limited Discussion of Failure Cases and Robustness**: Although **Appendix F.3, Figure 10** demonstrates that MA-TRVAE is robust to observation noise, there is no explicit analysis of more challenging conditions such as adversarial noise, partial observability, or real-world sensor failures.  \n4. **Ablation Studies**: The paper does not include ablations for different encoder/decoder architectures, latent dimensions, or the removal of individual regularization terms, making it difficult to fully assess the contribution of each component of the proposed method.\n\n\n[1] **Kang, S., Lee, Y., Kim, G., Chong, S., & Yun, S.-Y. (2025).** MA²E: Addressing Partial Observability in Multi-Agent Reinforcement Learning with Masked Auto-Encoder. In Proceedings of the 13th International Conference on Learning Representations (ICLR 2025). [https://openreview.net/forum?id=klpdEThT8q](https://openreview.net/forum?id=klpdEThT8q)\n\n\n\n[2] **Feng, J., Chen, M., Pu, Z., Xu, Y., & Liang, Y. (2025).** MA2RL: Masked Autoencoders for Generalizable Multi-Agent Reinforcement Learning. arXiv preprint arXiv:2502.17046.\n[https://arxiv.org/abs/2502.17046](https://arxiv.org/abs/2502.17046)\n\n[3] **Wen, M., Kuba, J. G., Lin, R., Zhang, W., Wen, Y., Wang, J., & Yang, Y. (2022).** Multi-agent reinforcement learning is a sequence modeling problem. In Proceedings of the 36th International Conference on Neural Information Processing Systems (NeurIPS 2022), 1201. Curran Associates Inc., Red Hook, NY, USA.\n\n\n\n[4] **Guo, X., Shi, D., & Fan, W. (2023).** Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism. In Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI-23), 126–134. International Joint Conferences on Artificial Intelligence Organization."}, "questions": {"value": "1. Were alternative trust-region penalty formulations (e.g., symmetric KL, Wasserstein distance, or JS divergence) explored, and how did they perform in preliminary experiments?  \n2. Is the scalability improvement shown in **Figure 6** and **Figure 9** consistent when scaling to a larger number of agents (e.g., 10, 20, or more)? Do training times remain reasonable under such conditions?  \n3. Regarding robustness to noise (**Appendix F.3**, **Figure 10**), what happens when both shared and independent noise sources are combined, or when the noise statistics are non-Gaussian or time-varying?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oYQAnVXMSd", "forum": "JcRWcwL2ll", "replyto": "JcRWcwL2ll", "signatures": ["ICLR.cc/2026/Conference/Submission18434/Reviewer_16Nd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18434/Reviewer_16Nd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761885548863, "cdate": 1761885548863, "tmdate": 1762928128394, "mdate": 1762928128394, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates representation learning for MARL with autoencoders. Section 2 sets up the preliminaries and background, on the RL objective in eq (1) for a decentralized POMDP. Then eq (2) summarizes multi-agent PPO to optimize the policies for that objective, which also estimates a value function with eq (3). Then, eq (4) highlights the \\beta-VAE objective for learning a latent representation. Section 3 starts from an extension of the single-agent setting in Yarats et al. using a \\beta-VAE for representation learning, finding two related trends 1) larger gaps between different \\beta values than in the single-agent setting, 2) less training stability from stochasticity in the representations, and 3) other instabilities of the changing representations. To address these, section 4 proposes a trust-region around the VAE updates so it does not move by too much, which is integrated via a penalty term in eq (6), controlled by \\beta_2. The experiments investigate two settings on multi-agent quadcopter control from pixels."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "I generally like the idea of representation learning for RL, and I think using a \\beta-VAE is a reasonable way of obtaining representations. This paper proposes a natural extension from the single-agent setting of Yarats et al., and provides clear experiments of the performance in the multi-agent settings for quadcopter control. Conceptually it is appealing and straightforward."}, "weaknesses": {"value": "While improved representations for MARL is appealing, I have some concerns with the experimental settings that hold me back from accepting the paper. This is because I feel a strong experiment section is necessary for this kind of paper, as conceptually the idea of \\beta-VAE representations and a trust-region stabilization is not sufficient.\n\nFirstly is that the quadcopter settings are limited.\nThe conclusion and limitations section of the paper acknowledges this, and I would like to acknowledge the difficulty of finding an isolated decentralized multi-agent visual RL environment, since this is often a small piece in a much larger experimental system that is not cleanly isolated as a benchmark.\nHowever, there are some other visual multi-agent RL settings that are great demonstrations and more connected to experimental robotics, such as [MAANS](https://sites.google.com/view/maans) and [robot soccer](https://arxiv.org/abs/2405.02425).\nI would find experimental comparisons in settings such as these significantly more convincing, although still acknowledge the difficulty of not having clear gym-like environments for these settings.\n\nFinally, scoping to only the quadcopter experiments, there aren't many direct comparisons to the previously-published results in MA2CL --- the MA2CL results reported in the paper under submission seem significantly worse than reported. Furthermore, only the flock and leader-follower environments from MA2CL are used, and not the state-based multi-agent starcraft and mujoco environments. The paper under submission argues they want to focus on visual settings, but the method would still work will in the state-based settings MA2CL evaluate in, and this seems scientifically interested to understand how the \\beta-VAE compares to contrastive learning. I would find it significantly more convincing to have more direct comparisons to MA2CL."}, "questions": {"value": "I am very open to further discussing my review throughout the discussion period, especially the question of if the current set of experiments.\n\nAnd out of curiosity, I do have one other question on the alternative to the trust-region constraint: did you also consider using an exponential moving average (EMA) on the weights of the VAE components to help prevent it from changing too much?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ShaHT3uspo", "forum": "JcRWcwL2ll", "replyto": "JcRWcwL2ll", "signatures": ["ICLR.cc/2026/Conference/Submission18434/Reviewer_Enue"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18434/Reviewer_Enue"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18434/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974413915, "cdate": 1761974413915, "tmdate": 1762928128061, "mdate": 1762928128061, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}