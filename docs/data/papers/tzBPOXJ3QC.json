{"id": "tzBPOXJ3QC", "number": 5854, "cdate": 1757940392980, "mdate": 1759897949673, "content": {"title": "Efficient-LVSM: Faster, Cheaper, and Better Large View Synthesis Model via Decoupled Co-Refinement Attention", "abstract": "Feedforward models for novel view synthesis (NVS) have recently advanced by transformer-based methods like LVSM, using attention among all input and target views. In this work, we argue that its full self-attention design is suboptimal, suffering from quadratic complexity with respect to the number of input views and rigid parameter sharing among heterogeneous tokens.  We propose \\textbf{Efficient-LVSM}, a dual-stream architecture that avoids these issues with a decoupled co-refinement mechanism. It applies intra-view self-attention for input views and self-then-cross attention for target views, eliminating unnecessary computation. Efficient-LVSM achieves 30.6 dB PSNR on RealEstate10K with 2 input views, surpassing LVSM by 0.9 dB, with 2× faster training convergence and 4.2× faster inference speed. \nEfficient-LVSM achieves state-of-the-art performance on multiple benchmarks, exhibits strong zero-shot generalization to unseen view counts, and enables incremental inference with KV-cache, thanks to its decoupled designs.", "tldr": "", "keywords": ["novel view synthesis", "novel view synthesis", "transformer", "large model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/12df3be27ecec3a284581fda671e80d236c6a00b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The work proposes an improvement to LVSM, an ICLR 2025 oral publication. The proposed method, called efficient-LVSM, uses a detached attention mechanism to enhance efficiency and final performance. The architecture separates the input view encoding from the output view encoding, except for layer-wise connections within the same layer. There are additional tricks, such as KV-cache, feature distillation, and alternating self-attention and cross-attention. The results are evaluated against a few other-shot novel-view synthesis methods. The additional detailed efficiency analysis and ablation are performed against LVSM."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- Good results\n- Detailed explanation about the components"}, "weaknesses": {"value": "- The work heavily relies on LVSM. In addition to final quality, the efficiency assessment needs to be compared with other methods. \n\n- The figures about the attention mechanism play an important role in understanding the main idea of the paper. However, they are somewhat repetitive and could be condensed into fewer figures—namely, Figures 1, 2, 3, and 6 (and Table 1 as well). I suggest reserving additional space to lay out the results better. The current layout is not very effective. The wrap-around text is hard to follow, and the figures and tables are referenced far from their locations. Some detailed results can be deferred to the appendix to improve the flow and completeness of the descriptions.\n\n- While the introduction describes the main contribution as decoupling the encoder and decoder with an efficient attention mechanism, the work is composed of multiple techniques from existing works, with too many subsections in both Section 2 and the results. While they are effective, the novelty can feel incremental, and the description is dispersed across several succinct sections, making it hard to get the whole picture."}, "questions": {"value": "- Figure 1 is hard to interpret, especially the items in the Table. Some of the attributes need to be detailed either in the caption or in the text. For example, what are \"spatialized pathways\"?\n\nMinor comments\n- line 042: \"to to\" -> \"to\"t.\n- In Figure 2, I believe there are \"M\" decoders instead of \"N\" decoders..?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "While generative models may incur ethical concerns in the big picture, the proposed work reconstructs pre-captured scenes and has no direct ethical issues if conducted within the given environment of the choice."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EKcXoVNoJi", "forum": "tzBPOXJ3QC", "replyto": "tzBPOXJ3QC", "signatures": ["ICLR.cc/2026/Conference/Submission5854/Reviewer_ygh6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5854/Reviewer_ygh6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761225286697, "cdate": 1761225286697, "tmdate": 1762918305522, "mdate": 1762918305522, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles feed-forward novel view synthesis task. It builds upon the framework of LVSM [1], which use a full-attention decoder-only transformer to render novel views conditioned on posed input views and target view plucker rays.  The key architecture change proposed by the authors are to replace the full attention to intra-image self-attention and cross-attention between target views and input views.  This change has two benefits:  1. When rendering new novel views, you don't need to recompute the key-value cache of input views. This also opens up application in incremental inference. 2. with above benefits, the performance (rendering quality) improves on Objecverse and Rel10K dataset, even with reduced training time.   The author also ablated with REPA showing that it can improve the PSNR. \n\n\n\n[1] LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clearly justified and very reasonable architectureal change.  The full-attention used by original LVSM restricts its effiency in lot of usecases. And when comparing with the encoder-decoder version of LVSM, the author identified the major problem:  we need to use key-value cache of all layers!      \n2. Strong empirical results. The experiment results on Objverse and Rel10K are quite strong, with much better rendering quality and less training time. \n3. The author shows a very interesting study about repa loss (distill from DINO v3). I original thought such semantic loss would not be useful for novel view synthesis task, but seems that it's quite helpful! This is very interesting."}, "weaknesses": {"value": "I do have a few comments. I think the authors should list more training details for their methods and their ablation experiments. The batch size, and total number of training iterations. I think it's missed.  \n\nFor training batch sizes, there are some tiny but important details, the original LVSM need to repeat the batch to make sure that each pass of the model only contains one target view, and this is one of the core-reason that the original decoder only LVSM is expensive in training and inference.  So highlighting this training details difference, and show that how many input views and target views totally being used during training is important."}, "questions": {"value": "Kind of minor, but for task like novel view synthesis, showing some video comparison would be great!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4zUyVZRt1L", "forum": "tzBPOXJ3QC", "replyto": "tzBPOXJ3QC", "signatures": ["ICLR.cc/2026/Conference/Submission5854/Reviewer_WANz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5854/Reviewer_WANz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968923482, "cdate": 1761968923482, "tmdate": 1762918304863, "mdate": 1762918304863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper proposes Efficient-LVSM, a transformer-based large view synthesis model designed to overcome the inefficiencies of the original LVSM architecture. \n\n- The key idea is to decouple the input-view encoding and target-view generation using a dual-stream co-refinement mechanism, combining intra-view self-attention for inputs and self-then-cross attention for targets. \n\n- The approach enables linear complexity in the number of input views, incremental inference with KV-cache.\n\n- Efficient-LVSM outperforms state-of-the-art LVSM by 0.9dB PSNR on the RealEstate10K benchmark with 50% training time and achieves 2−4 times speed acceleration in terms of both training iteration and inference."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper provides a systematic analysis of LVSM’s inefficiencies and derives a principled redesign via a decoupled encoder-decoder. The KV-cache design enabling incremental inference is a noteworthy contribution for real-time or interactive view synthesis, rarely explored in prior feedforward NVS models.\n\n- Efficient-LVSM achieves state-of-the-art reconstruction quality on both scene-level (RealEstate10K) and object-level (GSO/ABO) benchmarks. The reported 0.9 dB PSNR gain, 4× inference speed-up, and 50 % reduction in training time represent a strong improvement over prior LVSM baselines\n\n- The experiments are thorough: comparisons across datasets and baselines, scaling trends, ablations on architectural variants (self vs cross vs co-refinement), REPA distillation effects, model size, convergence curves, and zero-shot generalization.\n\n- The paper is well written and includes complete training details, REPA settings, and commitments to code release."}, "weaknesses": {"value": "- The dual-stream co-refinement design is highly similar in spirit to the MM-DiT block in terms of architecture introduced by Stable Diffusion 3 (2024). The authors are encouraged to cite MM-DiT and clarify how Efficient-LVSM extends this pattern to the feedforward NVS setting."}, "questions": {"value": "The paper is well presented; I don't have further questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "n5vB2v2hSm", "forum": "tzBPOXJ3QC", "replyto": "tzBPOXJ3QC", "signatures": ["ICLR.cc/2026/Conference/Submission5854/Reviewer_tC9E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5854/Reviewer_tC9E"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762162980022, "cdate": 1762162980022, "tmdate": 1762918304544, "mdate": 1762918304544, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study proposes Efficient-LVSM, which modifies the neural network architecture of LVSM to make its training and inference more efficient. The key modification is that Efficient-LVSM incorporates a cross-attention block, while stacking different self-attention blocks to extract query and key/value vectors from the target view and input views, respectively. By avoiding full self-attention between the input and target views, Efficient-LVSM reduces computational costs and enables feature caching of input views across different target views. Experimental results demonstrate the effectiveness of the proposed architecture."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "S1. Decomposing the full self-attention into different modules with cross-attention makes sense, as has been explored in various domains to design more efficient neural network architectures.\n\nS2. The experiments show significant improvements in novel view synthesis, while also enhancing inference efficiency.\n\nS3. The modified architectures can incorporate REPA to effectively train the hidden representations of input views."}, "weaknesses": {"value": "W1.\nThe technical contribution is limited. Instead of using full self-attention across input and target views, employing cross-attention is a typical design choice for improving training and inference efficiency [NewRef-1].\n\nW2.\nDespite the performance improvements, Efficient-LVSM cannot address the fundamental limitations of LVSM. For example, its architecture cannot account for the alignments either between generated target views or within the input views. Therefore, the overall impact of this study is limited to being an improved version of LVSM rather than a fundamentally new approach.\n\nW3.\nThe experimental analysis could be strengthened by incorporating more baseline methods. Please refer to my detailed comments below.\n\nW4. [Minor points — not affecting the score] The paper writing should be improved.\n- Line 42: \"ability to to learn\" -> \"ability to learn\"\n- Line 92: input and target inputs share the same subscription $i$ , making the readers confused.\n- Need to use mathbf clearly. For example, $S_i$ is also a set of tokens, but it does not use the bold text type. In Eq. (1), why $R_i$ uses the bold type, while the other parts do not use.\n- Figure 1 uses a check box, but rendering quality or efficiency cannot be described with the check box. For example, we cannot say that the baseline has no efficiency or quality.\n- In Figure 2, what does the asterisk mean? I guess it describes a shared weight between modules, but it is neither a common way to describe it nor explain the details.\n- Line 133 -- $p$ is not defined. $N$ is the number of input views, but $n$ is used for both input/target views.\n- Eq. (2) uses $l$ without any definition.\n\n\n[NewRef-1] Jeong, Yoonwoo, et al. \"NVS-Adapter: Plug-and-play novel view synthesis from a single image.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024."}, "questions": {"value": "Q1. Positional embeddings are important for maintaining alignment between views, but there is no description provided in the paper. How are positional embeddings applied in the self-attention and cross-attention layers?\n\n\nQ2. Can the authors discuss the limitations of this study to help readers understand potential directions for future work?\n\nQ3. While keeping a single attention block, could we use different masking strategies instead of using separate modules? For example, a causal mask between input and target views could enable KV-caching of input views. Additionally, attention between input views could incorporate a masking scheme to prevent them from attending to each other for efficiency. We could also optionally use different projection layers while sharing a single attention block, as demonstrated in MMDiT [NewRef-2]. Although I agree that the proposed architecture achieves significant improvements in the experiments, I believe the design could be further simplified.\n\n\nQ4. Figure 9(c) shows that Efficient-LVSM achieves 2× faster training to reach the same performance as LVSM, but the training does not appear to have converged yet. Could the authors provide more GPU hours to ensure full convergence and compare training efficiency more fairly? In addition, was REPA used in this setting? A comparison without REPA could also provide a clearer understanding of how Efficient-LVSM achieves efficiency and effectiveness purely from its architectural design, as analyzed in Table 1.\n\nQ5. I wonder why the authors keep the generated target views independent. In addition, input views do not align their features with other input views. Is there any specific reason to restrict the receptive fields in this way?\n\nQ6. In Table (d), could the authors provide a comparison between the 12-layer self-attention model and the 12+12 Efficient-LVSM? I believe the 12+12 Efficient-LVSM does not actually contain 24 layers but 12, so comparing it with the 12-layer self-attention model would be a fairer setting.\n\nQ7. I initially expected that increasing the number of input views in the previous LVSMs would significantly increase GPU memory usage. However, Figure 9(a) shows that the GPU memory of the LVSM Decoder-Only model does not increase much as the number of input views grows. Could the authors elaborate on this result?\n\nQ8. How much does KV-caching improve inference speed as the number of input views increases?\n\n\n[NewRef-2] Esser, Patrick, et al. \"Scaling rectified flow transformers for high-resolution image synthesis.\" Forty-first international conference on machine learning. 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1Y02YNsRkm", "forum": "tzBPOXJ3QC", "replyto": "tzBPOXJ3QC", "signatures": ["ICLR.cc/2026/Conference/Submission5854/Reviewer_fYv1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5854/Reviewer_fYv1"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5854/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762472115498, "cdate": 1762472115498, "tmdate": 1762918304298, "mdate": 1762918304298, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}