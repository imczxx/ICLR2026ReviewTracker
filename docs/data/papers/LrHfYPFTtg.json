{"id": "LrHfYPFTtg", "number": 20203, "cdate": 1758303650506, "mdate": 1763616581643, "content": {"title": "Keep the Best, Forget the Rest: Reliable Alignment with Order-Aware Preference Optimization", "abstract": "Direct Preference Optimization (DPO) has emerged as a powerful framework for aligning large language models (LLMs) with human preferences via pairwise comparisons. However, its performance is highly sensitive to the quality of training samples: when the reference policy is poorly aligned with human preferences, ambiguous pairs can dominate the gradient signal and degrade generalization. To address this, we propose RAPPO($\\textbf{R}$eliable $\\textbf{A}$lignment for $\\textbf{P}$reference $\\textbf{P}$olicy $\\textbf{O}$ptimization), a simple sample-aware modification of the DPO loss that mitigates reference-policy misalignment by filtering out the hardest, most ambiguous samples. We theoretically show that RAPPO yields improved generalization guarantees. RAPPO is lightweight and requires only a few lines of code to be integrated into any existing DPO-type algorithm. Surprisingly, With this simple modification, our simulations across a broad suite of alignment tasks and benchmarks show consistent gains over DPO and recent state-of-the-art baselines. On the PKU-SafeRLHF benchmark, RAPPO attains helpfulness $0.693$ ($+34.8\\%$ over DPO) and harmlessness $0.357$ ($-21.0\\%$ vs DPO).", "tldr": "We present RAPPO, an order-aware preference optimization framework that achieves tighter generalization guarantees and outperforms DPO baselines on multiple LLM tasks.", "keywords": ["Language Models; Preference Optimization; RLHF"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/326d7f386c444af849676096bccbaa2bcdd39d44.pdf", "supplementary_material": "/attachment/8a6dbb1ed8deac06a1f8d3caf691161ac0e0f4d3.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes RAPPO, a lightweight filtering mechanism built upon the DPO framework. RAPPO introduces two key ideas: (1) reference-policy awareness, where samples are first partitioned into Aligned and Unaligned subsets based on a reference-policy consistency threshold τ; and (2) in-batch ranking and pruning, where, within the Unaligned subset, the top-q samples with the highest individual DPO losses are temporarily discarded from the current update.\n\nThe authors show that RAPPO leads to a larger expected first-order risk reduction, lower gradient variance, and a tighter stability generalization bound. Empirically, RAPPO outperforms DPO, CPO, KTO, and SimPO on the PKU-SafeRLHF benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of incorporating reference-policy awareness into DPO is simple yet intuitively appealing, offering a principled way to mitigate the influence of noisy or misaligned preference data.\n2. The method demonstrates clear and consistent gains across multiple metrics on a competitive benchmark.\n3. The paper provides meaningful theoretical analyses on gradient variance, stability, and risk reduction, giving insight into why the filtering helps.\n4. RAPPO’s design is lightweight and can be readily integrated into existing DPO-style training pipelines with minimal overhead.\n5. The algorithmic formulation and ablation structure are clearly described, making the contribution easy to follow."}, "weaknesses": {"value": "1.  Since the gate relies on the reference policy’s relative probabilities, its robustness depends on the reference model’s reliability. If the reference policy is miscalibrated, important training signals might be filtered out. Including robustness comparisons would strengthen the claim.\n2.  Given the substantial empirical improvements, it would be valuable to release code and the corresponding commit hash.\n3. Using GPT-4o as a judgment model could introduce systematic bias. Multi-rater evaluation or human calibration would make the conclusions more convincing.\n4. Although the related-work section discusses Selective DPO, ORPO, and R-DPO, the large-scale comparisons include only DPO, CPO, KTO, and SimPO. Adding results for IPO, ORPO, R-DPO, or RRHF would provide a fairer empirical landscape.\n5. While results are reported for q = 1, 2, 4, the paper lacks a systematic sensitivity analysis for both q and τ. Understanding how performance depends on these thresholds would make the method more interpretable and reproducible."}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uJmSi0XX0Q", "forum": "LrHfYPFTtg", "replyto": "LrHfYPFTtg", "signatures": ["ICLR.cc/2026/Conference/Submission20203/Reviewer_QGJd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20203/Reviewer_QGJd"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761905552302, "cdate": 1761905552302, "tmdate": 1762933705639, "mdate": 1762933705639, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes RAPPO, a variant of DPO where some misaligned samples are filtered on a batch-by-batch basis. The specific algorithm hinges on two hyper-parameters. The threshold of the misalignment score in which to categorize the batch samples and q, the number of misaligned samples to toss out. Empirical validation on multiple, but not extensively thorough, demonstrate effectivenss over DPO"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea is straightforward and easy to implement\n- The empirical gains (at least the ones presented) are nice\n- Some theoretical analysis exist, which is always nice"}, "weaknesses": {"value": "- As the idea itself (filtering out misaligned samples) is quite straightforward, I believe a lot of the paper's contributino comes down to the execution and how well it can generalize. In this aspect, I don't think the empirical evidence shown in the submission is extensive enough. On the other hand, there is little analysis on the hyper-parameter sensitivity of tau and q. I feel like a lot of the algorithm's performance will depend on the exact value of those hyper-parameters and until I see some sensitivity analysis on them, I don't think I can say this method will generalize well."}, "questions": {"value": "- Hyperparameter q (the number of bad samples to throw out), seem to be a flat number. This seems to have unfavorable interaction as batch sizes are not constant over different training runs and research and saying to remove a flat number of samples when the batch size can differ in multiple orders of magnitude may not be helpful. Shouldn't this hyperparameter be a percentage of the batch size? If so, this makes the interplay between tau and q even more complex, as the number of Bad Samples in which to exclude the samples from is not pre-determined either."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wa7L5CKjez", "forum": "LrHfYPFTtg", "replyto": "LrHfYPFTtg", "signatures": ["ICLR.cc/2026/Conference/Submission20203/Reviewer_iNHd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20203/Reviewer_iNHd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762168212236, "cdate": 1762168212236, "tmdate": 1762933705199, "mdate": 1762933705199, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents RAPPO, an order-aware variant of DPO that filters high-loss “untrusted” pairs per mini-batch while always keeping “trusted” items. The method is simple, easy to implement, and accompanied by a stability analysis that yields a tighter generalization bound. Empirically, RAPPO outperforms strong DPO-style baselines on multiple LLM tasks. Overall: clear motivation, lean algorithm, solid theory; still room to strengthen rigor and reporting."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. Clear problem framing and insightful diagnostics (Fig. 1 and 2) quickly convey why reference-aware filtering helps.\n\nS2. Method is intuitive, minimally invasive to DPO, code is provided, and the analysis connects the selection rule to lower variance and tighter stability."}, "weaknesses": {"value": "W1. Lines 291–311 (and Theorem 4.7’s surrounding prose) use $q$ as the **kept** count per step, while Algorithm 1, Proposition 4.8, and the experimental setup (Line 424) treat $q$ as the **number removed**. This clash, along with redundant symbols, impairs readability. Please rewrite this block with a single convention, and verify whether the statement of this work still aligns after the change.\n\nW2. In the theory part, only (11)–(12) show how $q$ affects stability via a term proportional to $\\frac{1}{q}$ and $\\max_{i\\in\\text{Kept}} w$. This reveals a trade-off (keeping more reduces the $\\frac{1}{q}$ factor; removing more shrinks $\\max_{i\\in\\text{Kept}} w$), but there is no result suggesting how $q$ should scale for optimal decrease, convergence rate, or even convergence. It would strengthen the paper to (i) formalize the trade-off and relate the choice of $q$ to Thm 4.7’s decrease/variance terms; and (ii) add sensitivity studies across $q$ and batch sizes. I expect the optimal range to be task- and model-dependent.\n \nW3. Many results rely on reward models and GPT-4 judging. This may introduce systematic bias not captured in the theory. For the experiment, I suggest adding a small-scale human evaluation with inter-rater agreement (e.g., Kendall's $\\tau$), but I understand it may be challenging within the limited time. Some surrogate way, like (i) ablation, is to compare a Top-K with a “random K” to see whether mild randomness mitigates judge-specific bias without harming stability, and/or (ii) add judge/reward-model robustness checks."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GEO6GPvB81", "forum": "LrHfYPFTtg", "replyto": "LrHfYPFTtg", "signatures": ["ICLR.cc/2026/Conference/Submission20203/Reviewer_wPMb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20203/Reviewer_wPMb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20203/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762604278469, "cdate": 1762604278469, "tmdate": 1762933704117, "mdate": 1762933704117, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}