{"id": "X0CYp42qwx", "number": 11661, "cdate": 1758202887213, "mdate": 1759897562223, "content": {"title": "GeneLLM: Inheriting 1.25% of MoE-LLMs to Build Models of 8% Size that Retain 80% Performance", "abstract": "Despite the rapid progress of large language models (LLMs), their enormous training costs and deployment challenges remain significant limitations. To address these issues, the Learngene framework was proposed, offering a one-time extraction of a compact, reusable knowledge module from a large model, which enables smaller models of various sizes to generalize more effectively across downstream tasks. In this work, we are the first to apply the Learngene framework to LLMs. We propose GeneLLM, a low-cost initialization framework that allows rapid construction of lightweight domain-specialized models under limited data resources. By performing only forward inference on Mixture-of-Experts (MoE) LLM with multi-domain data, we identify experts of LLM with cross-domain common knowledge and apply tensor decomposition to refine a compact knowledge module, termed learngene. This module is then used to reconstruct and initialize the Feed-Forward Network (FFN) layers of small dense models, enabling efficient adaptation to downstream tasks with minimal computational cost. Experiments show that by inheriting only 1.25% parameters of the source LLM, GeneLLM enables target lightweight models, only 8.1% the size of the source LLM, to recover over 80% of its performance on certain tasks. Compared to training from scratch and distillation, GeneLLM consistently delivers superior task performance, cost efficiency, and faster convergence speed. The code is available at https://anonymous.4open.science/r/GeneLLM-main-0EB3/.", "tldr": "", "keywords": ["Large Language Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ca674f31cefc49a57ddab5fed400631c11b1655.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces GeneLLM, a framework for efficiently inheriting knowledge from large MoE LLMs to construct lightweight models. By leveraging the Learngene framework, the paper demonstrates how to extract reusable, compact knowledge modules (termed \"learngene\") from MoE-based LLMs. These modules are then used to initialize smaller, dense models that achieve high performance on downstream tasks with minimal computational cost. The method requires no additional training during knowledge extraction and achieves significant improvements in efficiency and task performance over baseline methods like distillation and random initialization. The experimental results show the effectiveness of GeneLLM, with lightweight models maintaining over 80% of the performance of their source LLMs while being only 8% of their size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed GeneLLM framework addresses a critical challenge in deploying large LLMs—reducing computational cost and model size—without sacrificing significant performance, which is especially important for resource-constrained applications.\n2. The use of MoE-based LLMs for extracting cross-domain knowledge is well-justified, and the combination of expert selection and Tucker decomposition provides a clear and effective approach for extracting and compressing reusable knowledge.\n3. This work includes thorough ablation studies to analyze the contributions of key components (e.g., expert selection and Tucker decomposition) and the impact of hyperparameters, enhancing the reproducibility and interpretability of the method."}, "weaknesses": {"value": "1. While the paper demonstrates strong results on specific downstream tasks, it does not sufficiently discuss how well the extracted learngene generalizes to entirely new domains or tasks that are significantly different from the pre-training datasets.\n2. The paper proposes a uniform initialization strategy for all downstream tasks. Task-adaptive initialization or hybrid approaches (e.g., combining learngene with techniques like LoRA or adapters) could further improve performance, but this direction is not explored.\n3. Certain aspects of the methodology, such as how the Tucker decomposition is applied when the target model dimensions differ from the source model, could be explained more clearly. While it is addressed in the Appendix, this is a critical component and should be included in the main text."}, "questions": {"value": "1. While the paper demonstrates strong performance on specific downstream tasks, how well does the extracted learngene generalize to completely new tasks or domains that were not part of the pre-training datasets (e.g., tasks requiring highly specialized or uncommon knowledge)? Could you provide additional results or insights on this aspect?\n2. The proposed method relies on MoE-based LLMs as the source models. How applicable is the GeneLLM framework to dense architectures, given that dense models are more prevalent in many real-world scenarios? Are there plans to adapt the approach for dense LLMs, and what challenges would this present?\n3. The paper adopts a uniform initialization strategy for all downstream tasks. Do you think task-specific initialization, such as combining learngene with LoRA, adapters, or other lightweight fine-tuning techniques, could boost performance? If so, could you briefly comment on how this might be integrated into the GeneLLM framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7stTK884NA", "forum": "X0CYp42qwx", "replyto": "X0CYp42qwx", "signatures": ["ICLR.cc/2026/Conference/Submission11661/Reviewer_7Enf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11661/Reviewer_7Enf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761566666652, "cdate": 1761566666652, "tmdate": 1762922721703, "mdate": 1762922721703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To alleviate the high training costs of LLMs, this work presents GeneLLM, the first trial of applying the Learngene framework to MoE LLMs. The core idea of Learngene is that knowledge-rich parameter modules exist within large models, and extracting them to initialize small models enables efficient training.\nThe key observation of this work is that in MoE architectures, certain experts are consistently activated across diverse domains, indicating they encode cross-domain generalizable knowledge—making them ideal candidates for Learngene extraction. In practice, the authors identify these experts via a single forward pass, then apply tensor decomposition to distill a compact learngene containing only 1.25% of the source model’s parameters, which is used to initialize the Feed-Forward Network (FFN) layers of small dense models.\n\nExperiments show that the resulting small models, with only 2.4%–8.1% the size of the LLM, recover over 80% of the full model’s performance on certain tasks after tuning on 2B–5B tokens, consistently outperforming baselines such as training from scratch and knowledge distillation, achieving higher data efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose an efficient approach to identify cross-domain common experts. They observe that certain experts in the model are consistently activated across multiple domains, suggesting that these experts encode domain-agnostic, general-purpose knowledge. To exploit this insight, the authors use multi-domain dataset and calculate expert activation frequencies through forward passes alone. This low-cost procedure enables selecting the subset of experts that constitute the core knowledge module embedded within the large model, or Learngene.\n2. The authors employ Tucker decomposition to extract the shared knowledge among these common experts. Tucker decomposition identifies dominant latent directions along each dimension of a high-dimensional tensor, effectively capturing the subspace jointly spanned by the common experts. The resulting components thus provide a mathematically principled representation of the shared knowledge, yielding a compact and theoretically grounded instantiation of the “Learngene” within MoE-based large language models."}, "weaknesses": {"value": "1. Limited significance: The authors emphasize that the core advantage lies in the efficient construction of small models that achieve approximately 80% of the performance of large models. However, it is trivial that a 0.6B～1B model can already attain ~80% of the performance of a 7B-scale model, which is reported in technical reports from Qwen or LLaMA. Moreover, such small-scale models have already been open-sourced by major model families. Therefore, although the authors highlight the efficiency of their approach, I argue that the problem they address lacks sufficient significance.\n2. Lack of theoretical depth: The authors refer to the empirical observation that certain experts in MoE LLMs act as cross-domain \"common experts,\" but they do not provide a deeper explanation for why this phenomenon occurs. The MoE gating mechanism is essentially a simple linear operation (we may disregard the softmax here, as it merely normalizes scores for weighted-sum without altering the relative ranking of expert activations). Specifically, the inner product between each input token and the column vectors of the gating matrix determines expert selection. The authors should conduct a more rigorous mathematical analysis to understand the origin of cross-domain common experts—ideally, they could directly identify such experts by analyzing structural properties of the gating matrix itself."}, "questions": {"value": "1. The authors introduce metrics such as \\(A_i\\) and \\(C_i\\) to identify common experts that are both frequently activated and consistently activated across multiple domains. As described, \\(A_i\\) is the mean activation frequency of expert \\(i\\) across domains, while \\(C_i\\) is essentially the first central moment. Why do the authors opt for the first central moment rather than more classical and widely-adopted the mean and the variance (i.e., the second central moment)?\n2. In Appendix A.4, the authors state: “if the number of Top-K selected experts becomes too large, the increased redundancy information in the experts can negatively impact the results.” However, since the purpose of Tucker decomposition is to extract a common principle components along different dimensions from a high-dimensional tensor and factorize out the factor matrices, why would increasing the number of experts still lead to performance degradation due to redundancy?\n3. The FFN module typically involves multiple weight matrices and nonlinear transformations. For instance, in the SwiGLU operator, there are three distinct linear matrices up_proj, down_proj, and gate_proj, and the Swish nonlinear activation function. Could the authors clarify whether, in GeneLLM’s Learngene extraction procedure, each matrix in the new FFN is initialized using only the corresponding matrices from the selected experts (e.g., aggregating all up_proj matrices to construct the new up_proj)? If so, since Tucker decomposition is a purely linear tensor factorization method that cannot capture or preserve nonlinear dynamics, why a linear decomposition of weights alone can faithfully extract a “Learngene” that meaningfully represents the experts’ common knowledge?\n4. I hope the authors could further clarify the experimental setting in Table 3. After pre-training GeneLLM-OLMoE on 5B tokens, was it subsequently fine-tuned with full-parameter supervised fine-tuning (SFT) on each individual task? If so, it appears that GeneLLM-OLMoE is inferior to OLMoE-PEFT in both performance and parameter efficiency."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ebcy2UP1vX", "forum": "X0CYp42qwx", "replyto": "X0CYp42qwx", "signatures": ["ICLR.cc/2026/Conference/Submission11661/Reviewer_ZRRS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11661/Reviewer_ZRRS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761722974272, "cdate": 1761722974272, "tmdate": 1762922721304, "mdate": 1762922721304, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a low cost model initialisation framework named GeneLLM, which aims to extract common knowledge from large Mixture of Experts (MoE) language models to initialise smaller, dense downstream models. The main steps of the algorithm include: selecting shared experts using $A_i - C_i$ as a metric, applying Tucker decomposition, and then averaging the $U_3$ factor matrix and reconstructing the weights to obtain the initial representation for the dense experts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Significance of the problem: The work addresses a real world challenge of deploying capable LLMs under resource constraints. GeneLLM offers a cost effective, scalable initialisation strategy that reduces both model size and pretraining data requirements, making it highly practical.   \n2. Clarity: The paper is well structured, with clear figures and a clear illustration of the method."}, "weaknesses": {"value": "1. Misleading Title and Low Absolute Performance: The title, \"Inheriting 1.25% of MoE LLMs to Build Models of 8% Size that Retain 80% Performance\", is somewhat misleading. The '80% performance' baseline may be misunderstood as a state of the art MoE LLM after full and sufficient training, suggesting GeneLLM recovers 80% of this top tier performance with a very small fraction of the parameters. This sets an expectation for a breakthrough paper. However, the baseline used in the paper is the source MoE model after identical (and very limited) 5B token pretraining plus SFT. On many datasets, this baseline's performance is mediocre. Consequently, the algorithm's absolute performance is quite low, despite technically retaining 80% of this weak baseline. For example, on the MMLU benchmark (Table 1), the best GeneLLM model scores only 30-33%. Considering MMLU is a four choice task where random guessing yields 25%, this is a very limited improvement. Most results are based on pretraining on fewer than 10B tokens, a very small amount for LLMs. This may mean all models are in an undertrained state, potentially exaggerating the relative advantage of GeneLLM. I hope the authors can report GeneLLM's complete performance when pretrained on a larger number of tokens, ideally enough to approach state of the art performance.  \n2. Justification of Tucker Decomposition against other Tensor Decomposition Methods: Appendix A.6 provides an explanation for using Tucker decomposition (i.e., filtering noise and preserving shared structures). However, these advantages may also be achieved by other tensor decomposition methods, such as CP decomposition, or SVD on the stacked matrices. More justification of this key methodological choice would be preferable, such as an empirical comparison or theoretical analysis against other tensor or matrix factorisation methods.  \n3. Unclear Computational Cost Reporting: The paper claims low cost (e.g., 72 vs 252 GPU hours), but the calculation of this figure is unclear. It is not specified whether these 72 GPU hours include the cost of learngene extraction (i.e., forward inference plus Tucker decomposition) or if it only covers the subsequent 5B token pretraining. The scalability and cost of the Tucker decomposition step itself, especially for large models with hundreds of experts, is not quantified."}, "questions": {"value": "1. How would the proposed algorithm perform if pretrained on a larger number of tokens, achieving relatively good absolute performance?  \n2. Is there empirical evidence or theoretical analysis for choosing Tucker decomposition over other tensor or matrix factorisation methods?    \n3. How exactly was the computational cost calculated?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "khu1OOWdmM", "forum": "X0CYp42qwx", "replyto": "X0CYp42qwx", "signatures": ["ICLR.cc/2026/Conference/Submission11661/Reviewer_CUPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11661/Reviewer_CUPy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847806432, "cdate": 1761847806432, "tmdate": 1762922720947, "mdate": 1762922720947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GeneLLM, a method to distill knowledge from large MoE (Mixture-of-Experts) models into much smaller dense models. The idea is to identify cross-domain “common” experts from the MoE teacher, stack their parameters into tensors, and extract shared structures using Tucker decomposition to form compact learngene representations. These representations are then used to initialize smaller dense models. The method reportedly inherits only 1.25% of the teacher’s parameters, yet achieves over 80% of the original model’s performance with just 8% of its size. Experiments on diverse understanding and generation tasks show faster convergence and better performance compared to scratch training, distillation, and expert pruning baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Intuitive Concept**: The paper leverages the sparsity and modularity of MoE models to extract transferable substructures (“learngenes”). Using Tucker decomposition to distill common knowledge across experts is technically appealing.\n2. **Comprehensive Empirical Evaluation**: Experiments span multiple understanding (BoolQ, MMLU, PIQA) and generation tasks (DollyEval, VicunaEval), with ablations on expert selection and decomposition rank. Results show consistent improvements."}, "weaknesses": {"value": "1. **Potentially Weak Baselines**: The distillation baseline seems under-specified. Key details such as loss type, temperature, use of intermediate layers, or teacher-assistant setup are missing. Without strong distillation comparisons, it’s hard to be sure GeneLLM’s advantage isn’t partly due to weaker baselines.\n2. **Dependence on MoE Access**: The method assumes full access to a large MoE model for expert activation statistics. This limits applicability when only dense or black-box models are available. The paper acknowledges this but doesn’t provide fallback strategies or experiments for such cases.\n3. **Limited Clarity on Cross-Dimension Mapping**: Section A.5 briefly mentions adaptation to different embedding dimensions, but the main paper lacks clear procedures or experiments showing how learngenes transfer when target model dimensions differ substantially.\n4. **Missing Statistical Test**: Results are presented as single numbers without variance or significance tests. Since small models can be seed-sensitive, reporting means and standard deviations over multiple runs would improve credibility.\n5. **Narrow Task Scope for Domain Specialization Claims**: Although the paper includes diverse benchmarks, they are all general-domain English tasks. Stronger evidence (e.g., domain-specific or multilingual settings) would better support claims of cross-domain or domain-specialized adaptability.\n6. **Ambiguity on the 80% Retention Metric**: The comparison point (teacher vs. SFT vs. PEFT model) isn’t always clear in tables and text. Clarifying what exactly “80% of performance” refers to would make claims more precise."}, "questions": {"value": "1. Please clarify the distillation baseline setup (loss function, temperature, use of intermediate features, or teacher-assistant). Could you also include a stronger distillation comparison (e.g., multi-stage or teacher-assistant distillation)?\n2. In cases where the target model’s dimensions differ from the source model, how exactly is the learngene adapted? A formula, pseudocode, or additional experiment (e.g., 1024/1536/4096 hidden sizes) would help.\n3. How sensitive is the expert selection metric to the threshold or Top-K choice? Have you compared it to other possible metrics (entropy, mutual information, correlation)?\n4. Could you provide a cost breakdown (time, GPU·hours, memory) for each phase — activation collection, Tucker decomposition, and reconstruction — to contextualize the reported savings?\n5. Have you tested the method’s stability across random seeds (e.g., 3–5 runs)? Please report mean ± std for main results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zbB214kQM3", "forum": "X0CYp42qwx", "replyto": "X0CYp42qwx", "signatures": ["ICLR.cc/2026/Conference/Submission11661/Reviewer_PnNW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11661/Reviewer_PnNW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900102001, "cdate": 1761900102001, "tmdate": 1762922720586, "mdate": 1762922720586, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}