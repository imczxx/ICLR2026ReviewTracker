{"id": "qg7zOTPtg6", "number": 3241, "cdate": 1757384447851, "mdate": 1759898100376, "content": {"title": "ASCIIEval: Benchmarking Models' Visual Perception in Text Strings via ASCII Art", "abstract": "Perceiving visual semantics embedded within consecutive characters is a crucial yet under-explored capability for both Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs). In this work, we select ASCII art as a representative artifact. It depicts concepts through careful arrangement of characters, which can be formulated in both text and image modalities. We frame the problem as a recognition task, and construct a novel benchmark, ASCIIEval. It covers over 3K samples with an elaborate categorization tree, along with a training set for further enhancement. Encompassing a comprehensive analysis of tens of models through different input modalities, our benchmark demonstrate its multi-faceted diagnostic power. Given textual input, language models shows their visual perception ability on ASCII art concepts. Proprietary models achieve over 70\\% accuracy on certain categories, with GPT-5 topping the rank. For image inputs, we reveal that open-source MLLMs suffer from a trade-off between fine-grained text recognition and collective visual perception. They  exhibit limited generalization ability to this special kind of arts, leading to the dramatic gap of over 20.01\\% accuracy compared with their proprietary counterparts. Another critical finding is that model performance is sensitive to the length of the ASCII art, with this sensitivity varying across input modalities. Unfortunately, none of the models could successfully benefit from the simultaneous provision of both modalities, highlighting the need for more flexible modality-fusion approaches. Besides, we also introduce approaches for further enhancement and discuss future directions. Resources are available at https://anonymous.4open.science/r/VisionInText-9EE5.", "tldr": "", "keywords": ["LLM evaluation", "MLLM evaluation", "ASCII art", "Visual Perception"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07e34aedc65bd8d3b8390ec5026291a3229e0a56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on evaluating the model's ability to understand ASCII art. To systematically assess this capability, the authors introduce ASCII-Eval, a benchmark comprising over 3,000 samples. Their comprehensive analysis reveals that proprietary LLMs such as GPT-5 demonstrate strong performance when processing ASCII art as text. The open-source MLLMs face a trade-off between text recognition and visual perception. Additionally, they find that model performance is sensitive to the length of the ASCII art, with the degree of sensitivity varying across input modalities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of evaluating the performance of LLMs and MLLMs on ASCII art is interesting.\n\n2. After proposing this benchmark, the authors also provided directions for enhancing the model's performance on it. (Low-resolution Prompting and Supervised Fine-tuning)"}, "weaknesses": {"value": "1. The authors claim that understanding how well models can capture visual semantics in text strings is valuable for both academic research and practical applications, yet they do not provide specific explanations. For example, what exactly is its value for academic research? And how does it benefit practical applications?\n\n2. The paper would benefit from a more in-depth analysis of the experimental results, rather than merely listing them. For instance, while Qwen2.5-VL-72B outperforms Llava-v1.5-13B on most benchmarks, the latter scores higher on the benchmark proposed in this paper. Why does this reversal occur? Additionally, why is the performance under the \"Image-only\" settings better than under the \"Text-Image\" settings?\n\n3. The observation that Qwen2.5-VL-72B (the latest version of Qwen-VL-Chat) performs worse than Qwen-VL-Chat on this benchmark raises questions about the benchmark's generalizability and reliability in evaluating model capabilities.\n\n4. Typo: In Figure 4, the authors evaluated Qwen2.5-VL-7B, while in Section 6.2 it is referred to as Qwen2.5-VL-8B."}, "questions": {"value": "See the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y9BLVxDjK1", "forum": "qg7zOTPtg6", "replyto": "qg7zOTPtg6", "signatures": ["ICLR.cc/2026/Conference/Submission3241/Reviewer_47dC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3241/Reviewer_47dC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761485301552, "cdate": 1761485301552, "tmdate": 1762916621023, "mdate": 1762916621023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ASCIIEval, a novel benchmark designed to systematically evaluate the visual perception capabilities of both Large Language Models and Multimodal Large Language Models. The core premise is that ASCII art represents a unique, modality-agnostic artifact where visual semantics are embedded within the 2D arrangement of text characters. The authors formulate the task as a multiple-choice question-answering problem for concept recognition. The benchmark is comprehensive, featuring a manually curated test set of over 3.5K samples across a detailed categorization tree, alongside a larger training set"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper is well written and easy to follow.\n2.The authors do extensice experiments and test a wide range of different LLMs and MLLMs.\n3.The problem of ASCII art for LLMs and MLLMs is interesting.\n4.The work is solid. It not only constructs a high-quality test set (ASCIIEval) with a multi-layer categorization system but also provides a training set (ASCIITune) for enhancing model capabilities. The exhaustive evaluation of over 50 mainstream models across three modalities makes its conclusions highly credible and representative of the current state of the art."}, "weaknesses": {"value": "**1.limited techincal contribution**：While the research question is intriguing, the paper's technical contribution remains relatively modest. The proposed Rationale-Assisted Training approach essentially leverages GPT to construct chain-of-thought data, which can be viewed as a form of capability distillation from a more powerful model.\n**2.Failure to Fully Explore Model Robustness**: The paper only briefly touches upon font sensitivity and character perturbation analysis, which should have been a more critical evaluation dimension. For visual perception, robustness to variations in artistic style, character substitution, and local occlusions is a key metric. The lack of systematic robustness testing of this kind is a shortcoming of the evaluation framework."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "B00OO1DGZB", "forum": "qg7zOTPtg6", "replyto": "qg7zOTPtg6", "signatures": ["ICLR.cc/2026/Conference/Submission3241/Reviewer_osj6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3241/Reviewer_osj6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761895971440, "cdate": 1761895971440, "tmdate": 1762916620870, "mdate": 1762916620870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The  submission introduces ASCIIEval, a multiple-choice benchmark for recognizing visual concepts depicted in ASCII art across text-only, image-only, and text-image settings, comprising 3526 test samples over 359 concepts (organized into 7 classes and 23 groups) plus a larger training set (ASCIITune); evaluating 50+ models from 2023–2025, the authors analyze scaling and generational trends, sensitivity to length, fonts, and perturbations, and propose rationale-assisted fine-tuning for LLMs (via GPT-5-distilled rationales) and two post-hoc MLLM improvements—low-resolution prompting and supervised fine-tuning—finding that proprietary models dominate, open-source MLLMs face an OCR-versus-holistic perception trade-off, text favors short ASCII while images favor long, text+image often degrades performance relative to image-only, and the proposed methods yield sizable accuracy gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Addresses an underexplored capability: visual perception in text strings; ASCII art is a strong, modality-agnostic testbed.\n\n(2) Carefully curated benchmark with taxonomy, safety filtering, human upper bound, and objective multiple-choice evaluation.\n\n(3) Broad, current evaluation with clear, actionable insights (OCR vs holistic trade-off; length effects; fusion failure) and simple, effective mitigations (low-res prompting; vision-backbone finetuning; rationale distillation)."}, "weaknesses": {"value": "(1) Ambiguity and label integrity: Although human filtering was applied, the paper acknowledges remaining ambiguity (<1.67%) and reports a relatively low accuracy (70%) for spot-checks in ASCIITune. More rigorous inter-annotator agreement (IAA), label adjudication protocols, and confusion analyses across similar concepts would strengthen trust in labels and distractors.\n\n(2) Potential source bias and reuse: The dataset draws heavily from online galleries with human-made ASCII, which is valuable but may contain stylistic and cultural biases. The paper could better quantify coverage (e.g., long-tail concepts, regional styles) and discuss domain shift to auto-generated ASCII or box diagrams.\n\n(3) Evaluation scope vs prompting sensitivity: The authors note that some models (e.g., Qwen-VL) are prompt-sensitive and adjust templates. It raises concerns about fairness across models. A more systematic prompt ablation or standardized multi-prompt evaluation could ensure robustness of the leaderboard.\n\n(4) Multiple-choice design and distractors: Distractors are within-group, but for ASCIITune they are LLM-generated (Llama-3-70B) and filtered by Perspective API. This may introduce distributional artifacts or lexical cues. Quantitative analysis of distractor hardness (e.g., human error with easy vs hard distractors, option entropy) is limited.\n\n(5) Correlation claims and causality: The negative correlation with OCR benchmarks is interesting but non-causal. Additional controlled experiments (e.g., training on OCR-heavy data vs balanced data; freezing vs finetuning OCR components) would better substantiate the trade-off claim.\n\n(6) Limited architectural exploration: The discussion rightly points to tokenization and 2D structure loss in text as a bottleneck, but no experiments probe alternative tokenizers or spatialized embeddings (e.g., 2D position encodings over monospace grids, rasterization-aware tokenization, or character-graph inputs). Even small-scale prototypes would strengthen the argument.\n\n(7) Text-image fusion analysis: The lack of synergy is a key finding, but the paper doesn’t deeply analyze where fusion fails (early vs late fusion, attention saliency across modalities, conflict diagnostics). Ablations with different fusion strategies or controlled noise in one modality would clarify the interference mechanism.\n\n(8) Reproducibility details: While appendices include prompts and some training details, more exact hyperparameters, seeds, and compute budgets for fine-tuning would help reproducibility. It’s unclear how many runs or confidence intervals were computed for key results."}, "questions": {"value": "(1) Dataset reliability:\n- What is the inter-annotator agreement (e.g., Cohen’s kappa) on concept labels and recognizability decisions? Can you share confusion matrices for top-confused concepts/groups?\n\n- How often do distractors share strong lexical overlap with ground truth (e.g., “cat” vs “kitten”)? Any adversarial distractor stress tests?\n\n(2) Prompt robustness:\n\n- Did you evaluate with multiple prompt templates per model and report mean/variance? Especially for models known to be prompt-sensitive (Qwen-VL)?\n- Any experiments with calibrated decoding (e.g., constrained output to options, logit-based choice selection) to reduce format errors?\n\n(3) OCR trade-off:\n\n- Can you show controlled training where you modulate OCR-heavy data vs ASCII-like data to quantify the trade-off? Are there models that buck the negative correlation trend?\n- In low-resolution prompting, do you measure OCR performance drop on OCRBench/TextVQA to confirm the intended trade?\n\n(4) Fusion failure:\n\n- What fusion architectures were used by the tested MLLMs (early/late)? Any evidence (attentions, gradient norms) that text tokens overshadow image features or vice versa?\n- Have you tried modality dropout or gating at inference (e.g., confidence-based selection) to approach the “oracle” bound?\n\n(5) Rationale distillation:\n- How sensitive are LLM gains to teacher choice (e.g., Gemini vs GPT-5) and rationale quality filters? Any comparisons across teachers or rationale lengths?\n\n- Do rationales generalize beyond seen concepts or primarily boost memorization of local patterns?\n\n(6) Tokenization alternatives:\n\n- Did you experiment with preserving 2D structure via newline-aware 2D positional encodings, blockwise tokenization, or line-by-line embeddings? Even small proof-of-concept results would be valuable.\n\n(7) Safety and misuse:\n\n- Since ASCII can be used to bypass safety (as you cite), did you measure whether improved perception correlates with better or worse adherence to safety policies in ASCII-mediated harmful content?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Nq6PGkdqu1", "forum": "qg7zOTPtg6", "replyto": "qg7zOTPtg6", "signatures": ["ICLR.cc/2026/Conference/Submission3241/Reviewer_Mraa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3241/Reviewer_Mraa"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975696299, "cdate": 1761975696299, "tmdate": 1762916620687, "mdate": 1762916620687, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a cross‑model benchmark for evaluating the ability of LLMs and multimodal models to interpret ASCII art presented in multiple formats. The authors curate approximately 3K samples covering 359 concepts across seven broad classes (e.g., Animals, Symbols), organized in a three‑level hierarchy, and evaluate a range of state‑of‑the‑art models, highlighting improvements in more recent systems. Alongside ASCIIEval, the authors release a larger but noisier training set, ASCIITune (~11.8K samples). They evaluate large number of models (both pure‑text LLMs and multimodal LLMs (MLLMs)) under three settings: text‑only input, image‑only input, and text‑plus‑image (to probe multimodal fusion)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The main strengths of the paper are as follows:\n\n1. A novel cross-modal benchmark that covers still underexpored domain (visual pattern recognition within text) is introduced. Prior research has heavily focused on reading text in images (OCR) or traditional image understanding, while ASCIIEval utilizes ASCII art as a modality-agnostic bridge between text and vision.\n2. The authors created the benchmark under the comprehensive methodology, e.g., 3-layer category hierarchy of the tasks that enables us to estimate the models performance in a more detailed manner. As reported, the resulting test set covers 359 distinct concepts (from pandas to light bulbs to “Aries” zodiac symbol) across 23 groups and 7 top-level classes. \n3. Comprehensive evaluation and analysis is remarkably thorough. The authors evaluate a wide range of models, including both closed-source models (GPT-4, GPT-5, Claude 3, Google Gemini, etc.) and open-source LLMs (LLaMA, Qwen, Mistral, etc.), and also open MLLMs. They report results in three conditions – text-only, image-only, and combined – allowing for insightful comparisons. The analysis surfaces several important findings backed by data: for instance, proprietary models outperform open models by a large margin.\n4. Interesting insight about trade-off of OCR training and ASCII comprehension is revealed.\n5. Despite the benchmarks and evaluation, the training recipes and ablations are conducted by the authors to improve models' ASCII art recognition."}, "weaknesses": {"value": "The main weaknesses of the paper are as follows:\n\n1. While ASCII benchmark is interesting itself, it is still a little bit narrow. From the motivation of the benchmark creation, it is a bit unclear. what general insights ASCII art evaluation provides beyond this specific format. For example, how well this translates to broader model capabilities. Stronger correlations with general benchmarks would help clarify relevance. While, some notes about trade-off between the OCR performance and ASCII performance is visible, it would be interesting to evaluate the mistakes provided by the models with strong OCR abilities on the ASCII-based tasks.\n2. Training data quality is low. ASCIITune is noisy (70% human accuracy), which may limit model learning. Also, using Llama-3 to generate distractors could introduce bias or unintended cues.\n3. While I don't consider it as a main weakness, still, the multiple-choice format makes evaluation easier but may oversimplify the task. Adding a generative variant could better reflect real understanding."}, "questions": {"value": "My questions to the authors:\n\n1. Have you analyzed correlations with broader vision-language or reasoning benchmarks to support its general relevance?\n2. Did you consider including an open-ended version of the task to test true recognition rather than choice elimination?\n3. Could you share how models performed on particularly ambiguous or confusable ASCII samples?\n4. Have you analyzed in detail (apart from Table 4), whether tokenizer compression ratio (e.g. number of tokens vs characters per ASCII sample) correlates with model performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DY6TgpqI7e", "forum": "qg7zOTPtg6", "replyto": "qg7zOTPtg6", "signatures": ["ICLR.cc/2026/Conference/Submission3241/Reviewer_p7db"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3241/Reviewer_p7db"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3241/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995789409, "cdate": 1761995789409, "tmdate": 1762916620212, "mdate": 1762916620212, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}