{"id": "KOvRxAMBzV", "number": 20892, "cdate": 1758311490085, "mdate": 1759896953363, "content": {"title": "Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks", "abstract": "We propose DenseMarks -- a new learned representation for human heads, enabling high-quality dense correspondences of human head images. For a 2D image of a human head, a Vision Transformer network predicts a 3D embedding for each pixel, which corresponds to a location in a 3D canonical unit cube.  In order to train our network, we collect a dataset of pairwise point matches, estimated by a state-of-the-art point tracker over a collection of diverse in-the-wild talking heads videos, and guide the mapping via a contrastive loss, encouraging matched points to have close embeddings. We further employ multi-task learning with face landmarks and segmentation constraints, as well as imposing spatial continuity of embeddings through latent cube features, which results in an interpretable and queryable canonical space. The representation can be used for finding common semantic parts, face/head tracking, and stereo reconstruction. Due to the strong supervision, our method is robust to pose variations and covers the entire head, including hair. Additionally, the canonical space bottleneck makes sure the obtained representations are consistent across diverse poses and individuals. We demonstrate state-of-the-art results in geometry-aware point matching and monocular head tracking with 3D Morphable Models. The code and the model checkpoint will be made available to the public.", "tldr": "Dense representation of human head images learned from in-the-wild image collections and point tracks, greatly improving monocular tracking and other applications.", "keywords": ["Human heads", "3D shape correspondence", "foundation models", "vision transformer", "point tracking"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7dc65ffd985022c6ed1d64dd9ab4339382c6d36e.pdf", "supplementary_material": "/attachment/96f4cd85edd9a5b109cfb73f47574ebb26709af3.zip"}, "replies": [{"content": {"summary": {"value": "The article proposes DenseMarks, a per-pixel embedding of head images into a 3D canonical unit cube. A ViT-based embedder maps each pixel to a 3D coordinate. These coordinates learned a volumetric latent grid that is smoothed with a 3D Gaussian to impose spatial continuity and interpretability. Training uses siamese supervision from automatically extracted point correspondences (CoTracker3) over in-the-wild talking-head videos, plus auxiliary landmark and segmentation losses to anchor semantics (300W landmarks via Mediapipe; parsing via FaRL/SegFormer). The embedding is intended to yield dense, semantics-aware correspondences robust to pose/occlusion, including hair and accessories. Experiments include qualitative point querying, dense warping comparisons to DINOv3, Sapiens, Diffusion Hyperfeatures, Fit3D, quantitative evaluation with ArcFace/Met3R on Nersemble, an application to monocular head tracking, and a small stereo reconstruction demo."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A compact 3D canonicalization with an explicit volumetric latent grid and smoothing makes the embedding queryable and visually interpretable. The design aims to cover the entire head (incl. hair/accessories) rather than just landmarkable skin.\n\nAvoids expensive 3D ground truth by leveraging point tracks from off-the-shelf trackers (CoTracker3) on large talking-head corpora. It integrates landmark and parsing constraints to stabilize semantics.\n\nShows tangible improvement when augmenting a parametric 3DMM tracker (VHAP + FLAME) with a DenseMarks photometric term, especially at extreme poses/occlusions, indicating practical usefulness in monocular tracking.\n\nDespite using 3D coordinates as the embedding key, it seems to produce cleaner dense warps than high-dimensional VFM features (DINOv3, Sapiens, DHFeats, Fit3D) in qualitative demos. The ArcFace/Met3R table suggests better same-person geometric consistency."}, "weaknesses": {"value": "The comparisons target generic dense features (DINOv3/Sapiens/DHFeats/Fit3D) rather than head-specific dense correspondence baselines (e.g., UV/texture-coordinate regressors for faces, functional-map-based human correspondences, dense face/ear/hair parsing and matching). Without task-matched baselines, SOTA claims are difficult to substantiate.\n\nTraining depends on CoTracker3 tracks, Mediapipe landmarks, and FaRL/SegFormer parsing, etc. These are all off-the-shelf outputs with their own biases/failure modes. The paper does not quantify noise tolerance or show that DenseMarks isn’t just using these tools’ error modes (e.g., failure on occlusions, hairstyles, accessories).\n\nArcFace similarity and Met3R measure identity or multi-view image consistency is not explicit correspondence accuracy. The paper lacks standard geometric metrics such as 2D endpoint error (on projected mesh vertices), PCK/NME on annotated parts, or reprojection errors with GT geometry. The Nersemble evaluation uses GS2Mesh meshes as a proxy, but the reported table aggregates identity/consistency rather than direct correspondence accuracy distributions.\n\nCurrent evidence is largely qualitative. Claims of robustness to occlusion/pose/appearance would benefit from controlled experiments (synthetic occlusions, blur, lighting/colour shifts) and cross-dataset generalization beyond CelebV-HQ/Nersemble. \n\nThe paper introduces a 3D latent grid E with Gaussian smoothing ($\\sigma$) but does not provide an ablation study for grid resolution ($N^d$), $\\sigma$, or memory/speed trade-offs.\n\nOnly a qualitative ablation removes landmark or segmentation losses. There’s no quantitative study of $λ_{lmks}$ / $λ_{segm}$ (Eqn. 1), or of how correspondence quality moves with/without each term, nor an analysis of the negative sampling strategy in the contrastive term."}, "questions": {"value": "Missing results on 2D EPE (in pixels) and PCK@$\\tau$ on projected mesh vertices for same-person pairs (Nersemble), and part-wise accuracy (ears, hairline, eyebrows) using manual landmarks? This would complement ArcFace/Met3R with true correspondence accuracy.\n\nMissing comparison to dense face UV / canonical correspondence [1, 2] (e.g., PRNet-style [3] face UV maps), CSE/functional-map-style human correspondences, and any dense face parsing + nearest-neighbour baselines. Also, add VFM+learned projection heads trained with the same track supervision to separate the value of canonicalization from pure feature learning.\n\n[1] Alp Guler, R., Trigeorgis, G., Antonakos, E., Snape, P., Zafeiriou, S. and Kokkinos, I., 2017. Densereg: Fully convolutional dense shape regression in-the-wild. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 6799-6808).\n\n[2] Guo, J., Zhu, X., Yang, Y., Yang, F., Lei, Z. and Li, S.Z., 2020, August. Towards fast, accurate and stable 3d dense face alignment. In European Conference on Computer Vision (pp. 152-168). Cham: Springer International Publishing.\n\n[3] Feng, Y., Wu, F., Shao, X., Wang, Y. and Zhou, X., 2018. Joint 3d face reconstruction and dense alignment with position map regression network. In Proceedings of the European conference on computer vision (ECCV) (pp. 534-551).\n\nProvide a grid of ($N^d$, $\\sigma$) vs accuracy/warp smoothness and memory, test no smoothing / different smoothers. This will validate that the canonical bottleneck (E + smoothing) is the right bias.\n\nQuantify how tracker noise and parsing/landmark errors affect training (e.g., by injecting synthetic correspondence noise, or filtering by CoTracker confidence). Show stability across different point-trackers.\n\nEvaluate cross-dataset (e.g., Nersemble images for testing only, or other head datasets) and stress tests (pose extremes, occlusions, motion blur, illumination changes). Report calibration/uncertainty of matches (entropy of nearest-neighbours’ distributions) under shift.\nProvide quantitative curves for $λ_{lmks}$, $λ_{segm}$, and study negative-pair sampling in the contrastive loss (within-frame vs cross-frame; hard-negative mining).\n\nShow representative failure cases (e.g., voluminous hairstyles, hats, masks, skin tones) and per-attribute breakdown to understand biases inherited from CelebV-HQ and the pseudo-label tools."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "I49RFNXgdD", "forum": "KOvRxAMBzV", "replyto": "KOvRxAMBzV", "signatures": ["ICLR.cc/2026/Conference/Submission20892/Reviewer_zpxw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20892/Reviewer_zpxw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761811016881, "cdate": 1761811016881, "tmdate": 1762999998730, "mdate": 1762999998730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes DenseMarks, a method to learn dense canonical 3D embeddings for human head images. A ViT network predicts per-pixel 3D locations in a canonical unit cube, trained using point tracks from CoTracker3 on talking head videos with contrastive loss plus landmark and segmentation constraints. The method shows improvements over foundation models in head tracking and correspondence tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **Sound overall pipeline** The approach of using point tracks for supervision combined with a 3D canonical space is reasonable and well-motivated\n- **Comprehensive evaluation** Good experimental validation across multiple tasks (monocular tracking, dense warping, point querying, application to downstream models, etc. with strong baselines\n- **Interpretable representation** The 3D cube canonical space with semantic structure is simple and queryable\n- **Practical improvements** Demonstrates clear improvements on downstream tasks like monocular head tracking with 3DMMs"}, "weaknesses": {"value": "- **Pseudo-GT Quality** CoTracker3 is not optimized for heads, yet no analysis of pseudo-GT quality or its impact on training. Critical questions remain: How do tracking errors affect learning? Should low-quality tracks be filtered? How reliable are tracks on challenging regions (hair, accessories)?\n- **Limited training data diversity** CelebV-HQ contains interview-style videos with predominantly frontal/near-frontal poses and limited expression variations. Impact on generalization to extreme poses unclear. Only 100 held-out videos from the same distribution used for evaluation\n- **Weak contrastive learning setup** Sampling pairs from the interview video means pose/expression/shape are very similar, making the task easier but potentially less robust. The current setup may not learn discriminative features for challenging cases\n- **Limited novelty** The method is more an engineering / pipeline work: standard ViT training with contrastive loss and auxiliary constraints. The main contribution is applying existing components (CoTracker + contrastive learning) to heads with a 3D cube representation"}, "questions": {"value": "- How does tracking quality from CoTracker impact the learned representation? Can you provide quantitative analysis of pseudo-GT reliability and explore filtering strategies?\n- Have you considered hard negative mining or sampling strategies that encourage learning from more diverse poses/expressions within the contrastive framework?\n- How does the method generalize to truly in-the-wild videos with extreme poses, fast motion, or severe occlusions beyond the interview-style training data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "83BIyLZDw1", "forum": "KOvRxAMBzV", "replyto": "KOvRxAMBzV", "signatures": ["ICLR.cc/2026/Conference/Submission20892/Reviewer_99jL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20892/Reviewer_99jL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761861492370, "cdate": 1761861492370, "tmdate": 1762999998788, "mdate": 1762999998788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DenseMarks, a new learned representation for the human head. The method uses a Vision Transformer to predict a 3D embedding for each pixel of an input image. In addition, the authors collect a dataset of pairwise point matches and employ multi-task learning with facial landmarks and segmentation constraints. The proposed representation can be applied to multiple downstream tasks. Experiments demonstrate state-of-the-art performance in geometry-aware point matching and monocular head tracking with 3D Morphable Models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- **High novelty and insightful contribution:**\nThe paper introduces DenseMarks, a novel dense representation for human heads that:\n(1) enables high-quality dense correspondences across complete head regions, including irregular features such as hair and accessories;\n(2) achieves robust tracking under challenging conditions such as severe occlusions; and\n(3) produces a structured, interpretable, and smooth canonical latent space that supports further exploration and interaction.\n\nCompared with conventional sparse landmark representations, DenseMarks demonstrates remarkable novelty and conceptual depth. Its design is ingenious and inspiring, introducing a unified dense representation that not only achieves fine-grained head correspondence but also exhibits strong generalization and versatility across diverse downstream tasks such as tracking, reconstruction, and semantic understanding. This elegant formulation presents a foundational and broadly applicable idea, with clear potential for extension beyond head modeling to full-body representations and other related domains.\n- **Comprehensive related work:**\nThe authors provide a thorough and well-organized review of related research, demonstrating a deep understanding of the field and clearly situating their contribution within existing literature.\n- **Excellent presentation:** The paper includes clear and informative visualization figures and is well written, clearly structured, and compact.\n- **Extensive experiments:** The experimental design is comprehensive and thorough, providing strong evidence for the effectiveness and generality of DenseMarks.\n- **Excellent visualizations demonstrating the effectiveness of DenseMarks:** Visualization results such as point querying, semantic region mapping on head images, and dense warping clearly show that DenseMarks captures rich and interpretable head semantics and generalizes well across various head-related tasks, including monocular tracking and stereo reconstruction."}, "weaknesses": {"value": "N/A"}, "questions": {"value": "**Questions:**\n\nN/A\n\n**Additional comments:**\n\nIt would be beneficial for the author to further explore the application of DenseMarks on additional head-related tasks such as head pose estimation [1], which requires both geometric and semantic understanding of head structures. This would provide stronger evidence supporting the generality and effectiveness of the DenseMarks representation.\n\n[1] Algabri, Redhwan, Ahmed Abdu, and Sungon Lee. \"Deep learning and machine learning techniques for head pose estimation: a survey.\" Artificial Intelligence Review 57.10 (2024): 288."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 10}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "RCWh2S48E4", "forum": "KOvRxAMBzV", "replyto": "KOvRxAMBzV", "signatures": ["ICLR.cc/2026/Conference/Submission20892/Reviewer_DAuy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20892/Reviewer_DAuy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915112928, "cdate": 1761915112928, "tmdate": 1762999999065, "mdate": 1762999999065, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DenseMarks, a learned dense representation for human head correspondence and tracking. Unlike traditional landmark- or region-based approaches that are limited to sparse and often skin-only regions, DenseMarks learns a per-pixel embedding space that maps every head pixel into a shared 3D canonical unit cube. The canonical embedding provides an interpretable, queryable, and spatially consistent representation across poses, expressions, and individuals, enabling applications such as dense semantic correspondence, 3D morphable head tracking, and view-consistent reconstruction.\n\nThe method leverages a Vision Transformer (ViT) backbone that predicts per-pixel embeddings guided by multiple complementary objectives:\n1.a contrastive loss based on point correspondences estimated from talking-head videos,\n2.semantic supervision from face landmarks and segmentation maps, and\n3.a spatial smoothness constraint via a 3D Gaussian-regularized latent feature cube.\n\nTogether, these components yield a structured latent space that is both interpretable and geometrically consistent, capturing the full head region, including challenging elements such as hair and accessories. Experimental results show promising performance in dense geometry-aware matching and monocular head tracking, outperforming existing pretrained vision foundation model (VFM) baselines."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Conceptually strong and well-motivated reformulation of dense correspondence into a 3D canonical latent space. Also, the multi-task supervision yields a highly structured and interpretable representation. Technically sound training strategy that leverages point-tracking data without requiring explicit 3D ground truth.\n2. Demonstrates robust performance across pose and occlusion variations, covering the full head including non-skin regions. This paper is easy to understand, and the video instructions are also very clear."}, "weaknesses": {"value": "Method.   \nThe approach depends on the quality and stability of off-the-shelf 2D point trackers, which may introduce bias or drift in contrastive supervision. \n\nExperiment.  \nExperimental fairness may be affected since the proposed model is trained specifically on face-centric data, while the compared baselines are typically pretrained on broader, more general datasets. This domain bias could partially account for the observed performance gap."}, "questions": {"value": "Is it possible to extend the construction of this representation to the whole body or even to objects in general?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wmW1fwXoR8", "forum": "KOvRxAMBzV", "replyto": "KOvRxAMBzV", "signatures": ["ICLR.cc/2026/Conference/Submission20892/Reviewer_F5bW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20892/Reviewer_F5bW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20892/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006439208, "cdate": 1762006439208, "tmdate": 1762999999899, "mdate": 1762999999899, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}