{"id": "VJZ56DDkin", "number": 19297, "cdate": 1758295162699, "mdate": 1759897047246, "content": {"title": "SKARL: Provably Scalable Kernel Mean Field Reinforcement Learning for Variable-Size Multi-Agent Systems", "abstract": "Scaling multi-agent reinforcement learning (MARL) requires both scalability to large swarms and flexibility across varying population sizes. A promising approach is mean-field reinforcement learning (MFRL), which approximates agent interactions via population averages to mitigate state-action explosion. However, this approximation has limited representational capacity, restricting its effectiveness in truly large-scale settings. In this work, we introduce $\\underline{S}$calable $\\underline{K}$ernel Me$\\underline{A}$n-Field Multi-Agent $\\underline{R}$einforcement $\\underline{L}$earning (SKARL), which lifts this bottleneck by embedding agent interactions into a reproducing kernel Hilbert space (RKHS). This kernel mean embedding provides a richer, size-agnostic representation that enables scaling across swarm sizes without retraining or architectural changes. For efficiency, we design an implementation based on functional gradient updates with Nyström approximations, which makes kernelized mean-field learning computationally trac .From the theoretical side, we establish convergence guarantees for both the kernel functionals and the overall SKARL algorithm. Empirically, SKARL trained with 64 agents generalizes seamlessly to deployments ranging from 4 to 256 agents, outperforming MARL baselines.", "tldr": "", "keywords": ["Reinforcement Learning; Multi-agent Reinforcement Learning; Reproducing Kernel Hilbert Space"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d61fee5692192c27c08482188c9807b7f4909b39.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces SKARL (Scalable Kernel MeAn-Field Multi-Agent Reinforcement Learning), a novel framework that integrates mean-field learning with reproducing kernel Hilbert space (RKHS) representations. By using kernel mean embeddings, SKARL provides agents with a rich, high-dimensional feature representation of the entire population distribution, rather than a coarse statistical summary. The authors develop an efficient algorithm using functional gradients and Nyström approximations, proving theoretically that their method is both highly expressive (a universal approximator) and robust to changes in the population (Wasserstein-Lipschitz continuous). Empirically, SKARL outperforms strong MARL baselines in large-scale cooperative tasks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. By representing the swarm as a distribution instead of a fixed set of agents, a policy trained with one number of agents (e.g., 64) can be directly deployed in environments with a different number of agents (e.g., 4 to 256) without any retraining. This overcomes a major limitation of traditional methods.\n\n2. Unlike previous mean-field methods that use simple statistics (like averages), SKARL uses kernel mean embeddings in a reproducing kernel Hilbert space (RKHS). This provides a much richer, high-dimensional representation of the agent population, allowing the system to capture complex structural information and higher-order differences between distributions.\n\n3. In experiments, SKARL is shown to achieve superior performance on large-scale cooperative tasks, \"consistently outperforming strong MARL baselines\" in both cumulative reward and training stability."}, "weaknesses": {"value": "1. The authors claim that existing mean-field approaches rely on first-order moment statistics, which provide only coarse summaries of the population. This simplification limits expressiveness and hinders adaptation across swarm sizes, since higher-order structural differences between distributions are ignored. I suggest the authors provide more theoretical or empirical discussion on this limitation instead of directly presenting their solutions, which would further strengthen the contributions of this work. For example, why is higher-order information valuable? Why do previous mean-field approaches limit expressiveness?\n\n2. This work improves the previous mean-field Q-function and embeds interactions using a kernel cylindrical representation. The Q-function is equal to the polynomial combinations of KME kernels. This combination increases the computational complexity compared to previous mean-field methods. Does this method scale to a large number of agents (from a kernel computational complexity perspective)?\n\n3. The authors claim that the representation method integrates seamlessly with standard multi-agent value-decomposition methods such as VDN (Sunehag et al., 2017), QMIX (Rashid et al., 2018), and QPLEX (Wang et al., 2020). However, they do not incorporate the representation method into these methods. It is unclear whether the representation method can improve the performance of current value-decomposition methods."}, "questions": {"value": "Please see the Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "1DwVI0J8xQ", "forum": "VJZ56DDkin", "replyto": "VJZ56DDkin", "signatures": ["ICLR.cc/2026/Conference/Submission19297/Reviewer_oYza"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19297/Reviewer_oYza"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549488788, "cdate": 1761549488788, "tmdate": 1762931249711, "mdate": 1762931249711, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SKARL, a new framework for multi-agent reinforcement learning (MARL) that aims to solve the challenges of scalability and flexibility in large swarms. The core idea is to move beyond simple mean-field approximations by representing the entire population of agents as a distribution embedded in a Reproducing Kernel Hilbert Space (RKHS). This allows the policy to learn from rich, size-agnostic features of the swarm, enabling zero-shot generalization to different population sizes. The method is supported by a theoretical analysis and empirical results showing strong performance on large-scale coordination tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.  The core idea of using kernel mean embeddings to represent the agent population is a powerful and elegant way to create a size-agnostic policy. It's a significant conceptual step up from traditional mean-field methods that rely on simple averages, though not necessarily entirely novel.\n\n2.  The paper is backed by theory, including convergence guarantees and a formal analysis of the zero-shot generalization error. This provides a solid foundation for the empirical results.\n\n3.  The zero-shot transfer experiments demonstrate that a single policy trained on 64 agents can be effectively deployed on swarms of up to 256 agents. This is desirable for real world applications, though standard for MF-based MARL."}, "weaknesses": {"value": "1.  The paper does not position itself within a growing body of work on kernel methods with mean-field systems and learning on distributions. The novelty of the proposed method is unclear without a discussion of highly relevant works, such as various works on general mean field control that does not rely on first-order approximations [1-2] or specifically kernel-based approaches with mean-field limits [3-8]. This omission makes it difficult for readers to assess the paper's unique contribution.\n\n2.  The experiments are confined to multi-particle navigation tasks. The experiments are also a bit small, training on only $64$ agents, which is a bit limited given the proposed complexity improvements. Moreover, the experiments are limited to a single problem dynamics.\n\n3.  The current ablation studies are useful but don't fully justify the complexity of the proposed RKHS machinery. A missing piece is a comparison against a simpler baseline, such as using a standard deep set, MARL based on mean field control, or a small MLP to process the mean-field statistics. Without this, it's hard to tell if the full power of kernel methods is truly necessary for the observed performance gains.\n\n[1] Carmona, R, et al. Model-free mean-field reinforcement learning: mean-field MDP and mean-field Q-learning. The Annals of Applied Probability 33.6B (2023): 5334-5381.\n\n[2] Mondal, W. U., et al. On the approximation of cooperative heterogeneous multi-agent reinforcement learning (MARL) using mean field control (MFC). Journal of Machine Learning Research 23.129 (2022): 1-46.\n\n[3] Fiedler, C., et al. (2023). Reproducing kernel Hilbert spaces in the mean field limit. Kinetic and Related Models, 16(6), 850-870.\n\n[4] Fiedler, C., et al. (2023). On kernel-based statistical learning theory in the mean field limit. Advances in Neural Information Processing Systems, 36, 20441-20468.\n\n[5] Fiedler, C., et al. (2025). Recent kernel methods for interacting particle systems: first numerical results. European Journal of Applied Mathematics, 36(2), 464-489. \n\n[6] Cui, K., et al. (2024). Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. ICLR.\n\n[7] Szabó, Z., et al. (2015). Two-stage sampled learning theory on distributions. In Artificial Intelligence and Statistics (pp. 948-957). PMLR.\n\n[8] Szabó, Z., et al. (2016). Learning theory for distribution regression. Journal of Machine Learning Research, 17(152), 1-40."}, "questions": {"value": "1.  Could you clarify the novelty of your framework in light of recent work on kernel methods in the mean-field limit and learning on distributions? Specifically, how does your approach relate to or differ from the works above?\n\n2.  To better justify the complexity of the kernel cylindrical functions, could you provide a comparison against a simpler architecture? For instance, a standard MFRL model where the mean-field term is processed by a more expressive neural network (e.g., a small MLP or a Deep Set).\n\n3.  Your framework works for homogeneous swarms. How do you see it adapting to settings with agent heterogeneity, which is a key feature of many complex MARL benchmarks? About the homogeneous agent assumption, what issues prevent one from simply adding heterogeneity via the states?\n\n4. \"MFRL simplifies further but lacks multi-scale coordination.\" This statement is too short for me to understand what exactly is lacking. Can you explain a bit more?\n\n5. Can you quantify or discuss the improvement in approximation over first-order methods such as MFRL?\n\n6. Given the work uses kernel methods, I am curious if the methodology will empirically scale to higher dimensions in states or actions, or if there are any limitations here?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oOTcrCShmc", "forum": "VJZ56DDkin", "replyto": "VJZ56DDkin", "signatures": ["ICLR.cc/2026/Conference/Submission19297/Reviewer_rZzU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19297/Reviewer_rZzU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761718749728, "cdate": 1761718749728, "tmdate": 1762931249344, "mdate": 1762931249344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SKARL, a reinforcement learning framework that leverages kernel mean embeddings (KME) and mean-field theory to address scalability issues in multi-agent reinforcement learning (MARL). The authors claim that SKARL can handle variable population sizes through a kernelized formulation in a reproducing kernel Hilbert space (RKHS). The paper introduces the notion of kernel cylindrical functionals, derives convergence guarantees using functional analysis (e.g., Frechet and Lions derivatives), and presents experimental results on swarm-like environments to demonstrate scalability and performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper tackles an important and challenging problem: scalable MARL under population uncertainty. The idea of using kernel mean embeddings for mean-field RL is conceptually interesting and could, in principle, lead to generalizable models across population sizes. The theoretical framework and use of functional-analytic tools (e.g., Lions derivatives, Nyström projection) suggest potentially strong mathematical grounding."}, "weaknesses": {"value": "1.\tMathematical notation is confusing and not self-contained: \nMany critical mathematical objects—such as the definition of the Lions derivative, the cylindrical functionals, or the exact meaning of $D$ in Eq. (4) are introduced without sufficient explanation. At first glance $D$ appears to represent the number of samples, but later it becomes clear that it actually denotes the number of different kernel components, which is confusing. Similarly, in lines 124–125, the quantity $R_f^i$ appears without prior definition and seems to be an undefined or inconsistent symbolThese ambiguities make the formulation extremely hard to interpret. (See also the questions section)\n2.\tLack of intuition and structural explanation:\nThe paper immediately dives into abstract functional definitions without providing a high-level overview of what the algorithm actually does.\nIt is unclear whether the kernel functions are fixed or learned, and if learned, how they are trained or parameterized.\nA clear intuitive summary—what is being optimized, what role the kernel plays, and how scalability arises—would make the method far more accessible.\nAt present, the algorithmic pipeline is opaque and difficult to connect to implementation."}, "questions": {"value": "1.\tIn Eq. (6), is the variable $x$  equivalent to the state-action pair $(s,a)$?\n2.\tThe parameter $\\theta_h$ is introduced but not subsequently used—does Eq. (7) describe an update on $\\theta_h$  or on $h$ itself?\n3.\tAre the kernel functions $g$ trained jointly with the policy/value function, or are they fixed? If they are trainable, how is this implemented in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "In7Z6G54sh", "forum": "VJZ56DDkin", "replyto": "VJZ56DDkin", "signatures": ["ICLR.cc/2026/Conference/Submission19297/Reviewer_cjA7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19297/Reviewer_cjA7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19297/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947728774, "cdate": 1761947728774, "tmdate": 1762931248880, "mdate": 1762931248880, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}