{"id": "AAJMQ0XmhQ", "number": 4240, "cdate": 1757645195989, "mdate": 1759898044290, "content": {"title": "BrainAE: Alignment-driven Autoencoder for Bidirectional Visual Encoding and Decoding", "abstract": "Modeling the bidirectional mapping between visual stimuli and neural activity is critical for both neuroscience and brain–computer interfaces (BCIs). Although significant progress has been made in independently addressing visual encoding and decoding, **unified latent representations for the bidirectional mapping remain lacking**. Here, we propose **BrainAE**, an autoencoder-based framework designed for both visual encoding and decoding. Contrastive alignment with image models drives the latent features **toward a shared representation space of visual stimuli and neural responses**. Once trained, the model supports **stimulus-to-brain encoding**, **brain-to-stimulus decoding**, and **whole-brain signal reconstruction**. We extensively evaluate the model on electrophysiology, including human electroencephalography (EEG) and magnetoencephalography (MEG), as well as macaque multi-unit spiking activity (MUA), spanning non-invasive and invasive recordings, macro- and micro-scales, and species. Results demonstrate competitive encoding and decoding performance, revealing spatial, temporal, and semantic patterns consistent with established neuroscience findings. BrainAE provides a methodological foundation for probing brain function and developing BCIs.", "tldr": "", "keywords": ["Brain encoding and decoding", "Representation alignment", "Autoencoder", "Electrophysiology", "Vision"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d0e47d7d55e0a04863cf47daedf68609136c0567.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "the paper describes an encoder/decoder system to enable a bidirectional dialogue between visual stimuli and corresponding neural activity. The paper is mainly oriented on the technical description of the system that has been built and its evaluation, through the selection of well-chosen benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The approach is solid and well described. The state of the art is also of good quality. Figure 1 is clear and describes well the method."}, "weaknesses": {"value": "Nevertheless, in spite of this globally good quality, I found the approach not very innovative. In addition, it is not very discussed *why* developing such a model could be interesting, its impact and limitations."}, "questions": {"value": "Could you clearly state why it would be interesting to have such a tool at disposal and in which domain(s) and for which reason the impact would be important ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bncXKH2PdL", "forum": "AAJMQ0XmhQ", "replyto": "AAJMQ0XmhQ", "signatures": ["ICLR.cc/2026/Conference/Submission4240/Reviewer_sswh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4240/Reviewer_sswh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760528768254, "cdate": 1760528768254, "tmdate": 1762917249342, "mdate": 1762917249342, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces BrainAE, an autoencoder-based model that is trained to reconstruct raw electrophysiology signals while aligning its latent space to the one of a visual embedding. This autoencoder can be used simultaneously for the task of visual encoding, visual decoding, and reconstructing masked brain activity (temporally or spatially). This bidirectional approach supports both visual encoding and decoding, which are often considered as separate tasks. It is evaluated on three datasets of Human EEG, Human MEG, and MUA, on visual encoding task (MSE, Pearson’s r), visual decoding task (template-based classification accuracy, retrieval top-1/top-5 accuracy, generation using image similarity metrics), and brain activity reconstruction. The paper shows superior correlations on encoding compared to a linear baseline and superior decoding performance on MEG / EEG compared to previous baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The approach is simple: a convolutional autoencoder, MSE and CLIP losses at raw and latent level. \n- The motivation for this approach is clearly stated (merging encoding and decoding into a single task) and the presented results correspond to the claims in the abstract.\n- The Methods section is clearly written and can be easily followed"}, "weaknesses": {"value": "- Several important sections of the paper are difficult to follow and lack details to be understandable (see questions)\n- Some important details are missing (e.g. the dimensionality of the latent space of the AE, training of the diffusion prior, which classes are used as templates and if they are the same as in with previous works, ...)\n- Some scores are reported only on one specific subject from the dataset and the reason is not discussed (e.g. Image similarity metrics on reconstruction task on MEG dataset is reported only for subject 2)\n- Image reconstructions shown are of SOTA quality compared to previous works but this is not discussed further, either the paper should include a broader / more faithful view of the quality of these reconstructions or argue that this reconstruction quality is consistent over all datasets."}, "questions": {"value": "- Section 4.3.1 on encoding refers to an ‘alignment’ process used for encoding, which is not defined prior to this section and compares the results of encoding with two ‘Linear Dec’ and ‘Nonlinear Dec’ settings which are not defined either. These processes are named ‘decoders’, which makes it even more confusing in the context of an encoding task. Since in the ‘Problem definition’ section, the encoding task is described as ‘Visual encoding, in Fig. 1(b), gets image embeddings by the encoder E_I and input to the decoder D_B for the encoded brain activity’, it is not clear whether this is the process actually done for evaluating encoding or if it’s something else involving an alignment module.\n- Figure 3. The companion text for panel A refers to top-1 accuracy changes with pointwise masking but the y-axis contains negative values (incompatible with an accuracy metric), is this plot actually showing an accuracy metric on the y-axis or something else whose value can be negative ? It is unclear what the error bars in panel B correspond to.\n- It would be great to have a clearer view of the quality of reconstructions obtained from SDXL, by sharing not only the best cases but some worse and failure cases.\n- The reconstruction process is not detailed: it is not clear how the training of the diffusion prior for mapping the image encoder embedding to an SDXL prompt embedding is done, nor how the IP-adapter is used.\n- The conclusion of Backbone section 4.5.3 is not clear: “Despite replacing TSConv and EVA-CLIP Sun et al. (2023), competitive results are achieved in encoding and decoding”. Does it mean that TSConv and EVA-CLIP provide better results than other models?\n- What time / space subsets are removed for obtaining Fig 3e ? \n- It is not clear what the dimension of the AE is, and whether ablations have been carried out on this dimension. It is implicitly assumed that it is the same as the output of the image encoder, but confusing because a linear layer is used to project the image encoder output to the AE latent. This should be clarified so we know whether the AE latent has the same dimension as the image encoder output or whether it differs, and if so, provide ablations of the chosen dimensionality."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gZKKbzuRI8", "forum": "AAJMQ0XmhQ", "replyto": "AAJMQ0XmhQ", "signatures": ["ICLR.cc/2026/Conference/Submission4240/Reviewer_wiW8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4240/Reviewer_wiW8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832644934, "cdate": 1761832644934, "tmdate": 1762917249051, "mdate": 1762917249051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes BrainAE, which aligns brain activity embeddings (from a temporal–spatial conv encoder/decoder) with frozen image embeddings (e.g., CLIP/EVA-CLIP) in a shared latent via a combination of InfoNCE, latent MSE (brain–image), and brain-signal reconstruction losses. Downstream, the model supports (a) image→brain “encoding,” (b) brain→image “decoding” via retrieval or a separate diffusion pipeline, and (c) masked brain-signal reconstruction. Experiments cover EEG/MEG and macaque MUA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. BrainAE is a simple, unified latent for both encoding and decoding is easy to reproduce and reason about.\n2. Broad data coverage (EEG/MEG/MUA) is welcome.\n3. Some loss ablations are included."}, "weaknesses": {"value": "**Methodological issues**\n\n1. It's not truly “bidirectional autoencoding.” The image side is frozen; there is no jointly trained image decoder. Image generation uses a separate diffusion prior conditioned on brain→CLIP. As implemented, this is an alignment-regularized brain autoencoder + frozen image encoder, not a symmetric/bidirectional AE. \n\n2. Contrastive setup under-specified. How are negatives formed? pure in-batch? Are batches mixed across subjects/classes? Any hard-negative mining? These choices crucially affect alignment strength and the attainable upper bound.\n\n3. (Important part) Baselines are weak/unclear. The paper cites a “linear decoder,” but the exact form is not specified. Also, strong, simple, more up-to-date, must-have baselines are missing (e.g. CLIP feature -> brain)\n\n\n**Experimental issues**\n\n1. Comparisons to recent work are incomplete. The paper does not line up with several strong recent methods, and the ATM classification scores reported here are far below the same metric reported elsewhere. This strongly suggests protocol/implementation mismatches (splits, zero-shot setup, template construction, backbone versions, etc.). Authors must align evaluation protocols.\n\n2. Template-based decoding is fragile and under-controlled. From where are the templates sampled? How many per class? Are they fixed across runs? Report the template lists, ensure zero-shot semantics where claimed, and show variance across random template sets.\n\n3. RSVP overlap confound (EEG). With ~200 ms SOA, single-trial epochs contain overlapping responses to multiple items. Without deconvolution/overlap correction, “image-specific” decoding claims are weakened. Either deconvolve or replicate the temporal-importance analysis in MEG (longer SOA).\n\n4. Motivation for visual encoding is under-argued. The paper should justify practical benefits (e.g., as a self-supervised signal that improves decoding, as in-silico stimulus design for closed-loop, or for data augmentation). No such downstream gain is currently demonstrated.\n\n5. Presentation. Citation style is inconsistent; several notation/typo issues remain."}, "questions": {"value": "1. Please add experimental details (e.g. loss weights, latent dimension, normalization, and batch construction (subject/class mixing).)\n2. Please strengthen baselines to ensure apples-to-apples comparisons."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4teKiyv0GR", "forum": "AAJMQ0XmhQ", "replyto": "AAJMQ0XmhQ", "signatures": ["ICLR.cc/2026/Conference/Submission4240/Reviewer_z5xD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4240/Reviewer_z5xD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4240/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030218232, "cdate": 1762030218232, "tmdate": 1762917248704, "mdate": 1762917248704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}