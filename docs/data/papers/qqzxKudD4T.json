{"id": "qqzxKudD4T", "number": 16735, "cdate": 1758268200473, "mdate": 1759897222199, "content": {"title": "PoseX: AI Defeats Physics-based Methods on Protein Ligand Cross-Docking", "abstract": "Recently, significant progress has been made in protein-ligand docking, especially in deep learning methods, and some benchmarks were proposed, such as PoseBench and PLINDER. However, these studies typically focus on the self-docking scenario, which is less practical in real-world applications. Moreover, some studies involve heavy frameworks requiring extensive training, posing challenges to convenient and efficient assessment of docking methods. To fill these gaps, we design PoseX, an open-source benchmark to evaluate both self-docking and cross-docking, enabling a practical and comprehensive assessment of algorithmic advances. Specifically, we curated a novel dataset comprising 718 entries for self-docking and 1,312 entries for cross-docking; secondly, we incorporated 23 docking methods in three methodological categories, including physics-based methods (e.g., Schrödinger Glide), AI docking methods (e.g., DiffDock) and AI co-folding methods (e.g., AlphaFold3); thirdly, we developed a relaxation method for post-processing to minimize conformational energy and refine binding poses; fourthly, we established a public leaderboard to rank submitted models in real-time. We derived some key insights and conclusions through extensive experiments: (1) AI-based approaches consistently outperform physics-based methods in overall docking success rate. (2) Most intra- and intermolecular clashes of AI-based approaches can be greatly alleviated with relaxation, which means combining AI modeling with physics-based post-processing could achieve excellent performance. (3) AI co-folding methods exhibit ligand chirality issues, except for Boltz-1x, which introduced physics-inspired potentials to fix hallucinations, suggesting that stereochemical modeling greatly improves the structural plausibility of the predicted protein-ligand complexes. (4) Specifying binding pockets significantly promotes docking performance, indicating that pocket information can be leveraged adequately, particularly for AI co-folding methods, in future modeling efforts.", "tldr": "a comprehensive benchmarking of various docking methods on a new curated dataset for self-docking and cross-docking", "keywords": ["AI docking", "AI co-folding", "protein-ligand interaction", "cross docking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/91776b33579aa96203c747741316c9f634d584df.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce PoseX, a deep learning benchmark for protein-ligand self and cross-docking. The benchmark is well built and comprehensive in terms of method selection, and empirical results match up to expectations. Nonetheless, some concerns remain regarding the novelty and presentation of these results in the context of a machine learning conference."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors' proposed benchmark is comprehensive and introduces results for protein-ligand cross-docking.\n2. Metrics reported for the benchmark are standardized and informative.\n3. The results of the benchmark match expectations overall, offering a few subtle insights, e.g., about how to improve ligand chirality prediction.\n4. The presence of an online leaderboard is a plus."}, "weaknesses": {"value": "1. The benchmark, at first glance, reads like an expanded version of PoseBench extended to protein-ligand cross-docking. For example, in Line 149, the authors highlight post-prediction relaxation as a contribution of their work, though a version of this technique was already heavily explored in PoseBench (with support for multi-ligands as well).\n2. Introducing cross-docking seems like a marginal change over previous works. It is simply a matter of selecting different protein conformations. From a biological perspective, it is interesting to study, but from a machine learning perspective, it is practically identical to what has been done before.\n3. The way the authors present their benchmarks' results (as well constructed as they are) is ideal for a scientific journal but less common for a machine learning venue. For example, with the font size and scale of Figure 2, it is hard to quickly discern which machine learning method performs \"best\" in the self and cross-docking contexts (with all caveats in mind). Maybe highlighting the \"most notable\" results with a box around them would be a way to address this? Also, the methods in Figure 2 are unorganized (i.e., without category labels), so it is hard for a machine learning person (without expertise) to know which method is a model based purely on deep learning and which is a physics-based method.\n4. Why did the authors select Astex Diverse as a benchmark dataset instead of the PoseBusters Benchmark dataset (which is arguably more well-known now and less prone to show up in each method's training set)?"}, "questions": {"value": "1. How extensible is PoseX's codebase? For example, how hard is it for current users to build on top of the benchmark to add new datasets or prediction tasks for evaluation? Do you have any case studies in this direction?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w1NfdjLxwX", "forum": "qqzxKudD4T", "replyto": "qqzxKudD4T", "signatures": ["ICLR.cc/2026/Conference/Submission16735/Reviewer_yfAx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16735/Reviewer_yfAx"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761067543908, "cdate": 1761067543908, "tmdate": 1762926785253, "mdate": 1762926785253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PoseX, an open-source benchmark for evaluating self-docking and cross-docking across 23 methods spanning physics-based docking, AI docking, and AI co-folding. It analyzes pocket-similarity effects and a standardized post-processing relaxation pipeline that mitigates steric clashes. On both self- and cross-docking, several AI methods outperform physics-based tools on top-1 success. Relaxation often improves PB-Valid while not fixing chirality. A public leaderboard is announced."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The scope is important. The benchmark emphasizes cross-docking, which better reflects real-world use than self-docking alone, and covers 23 diverse methods.\n+ The evaluation is informed. Success combines RMSD ($<2 \\AA$) with PB-Valid to penalize chemically implausible poses, and analyses of relaxation and chirality are informative.\n+ A leaderboard and an automated relaxation pipeline (e.g., OpenMM-based) improve accessibility and reproducibility."}, "weaknesses": {"value": "- It is unclear whether relaxation and other post-processing steps are applied consistently and fairly across method categories, since many\n  physics tools already perform internal minimization. Please list which methods received additional OpenMM relaxation and justify comparability.\n- The current success definition is top-1 only and couples RMSD $< 2 \\AA$ with PB-Valid. Reporting top-$k$ (e.g., $k = 5,10$) pose-ranking metrics, and separate PB-Valid vs. RMSD breakdowns with confidence intervals would better reflect screening use and decouple geometry from plausibility.\n- Pocket-similarity is computed against pre-2022 pockets to avoid training overlap, but more detail is needed to ensure ligand- or\n  pocket-level leakage is not reintroduced. Please provide formal leakage checks and family-level analyses."}, "questions": {"value": "Please refer to the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Omrz4EF7S1", "forum": "qqzxKudD4T", "replyto": "qqzxKudD4T", "signatures": ["ICLR.cc/2026/Conference/Submission16735/Reviewer_xw3v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16735/Reviewer_xw3v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761208386890, "cdate": 1761208386890, "tmdate": 1762926784381, "mdate": 1762926784381, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "An open-source benchmark that evaluates self-docking (SD) and cross-docking (CD) across 23 methods (physics-based, AI docking, and AI co-folding), using RMSD and PoseBusters validity (PB-Valid), plus an OpenMM-based relaxation module. The authors report that 9 AI methods surpass GNINA on CD (vs only 3 on SD), and that performance in CD strongly depends on pocket similarity (TM-score of residues within 10 Å, compared to pre-2022 pockets).\n\nPoseX curates 718 SD and 1,312 CD entries (2022–2025 PDB releases), applies filtering (e.g., ligand/protein proximity, symmetry-mate removal), clusters by sequence, aligns conformers by alpha carbons, and in CD evaluates a ligand against other conformers of the same target; success is top-1 RMSD < 2Å (optionally with PB-Valid), averaged at target level to correct for uneven per-target counts.\n\nCompared to the previous benchmarks, it has well-designed data processing workflow, leaderboard, and extensive comparison."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Timely scope & coverage**. Large-scale comparison over 23 methods spanning physics-based, AI docking, and AI co-folding models; inclusion of commercial tools improves external validity. Also, the authors used dataset with PDBs deposited after 2022.\n- **Cross-docking emphasis**. Clearly motivates CD as the practical setting and provides a concrete construction."}, "weaknesses": {"value": "- **Pocket-conditioning fairness**. CD results show pocket-conditioned AI docking models (e.g., SurfDock, UMD-V2, DiffDock-Pocket) against co-folding models that do not receive an explicit pocket mask. However, as pocket information is already known as very important information in predicting binding structure, I think comparing co-folding models' structure prediction performance (Figure 2) without pocket information with AI docking models are not a fair comparison. Furthermore, Pocket-based methods operate inside a pre-specified cavity/box, so even when the true pocket is dissimilar (apo$\\to$holo shift, induced fit), their search space, and thus the worst-case ligand RMSD, is tightly bounded by the pocket box. Co-folding models, by contrast, must first localize the site over the entire protein surface/volume and then place the ligand, facing an orders-of-magnitude larger search space and unbounded RMSD penalties for site mislocalization. Consequently, direct RMSD comparisons systematically favor pocket-conditioned docking, and are not information-matched or fair.\n\n- **Missing confidence interval**. The paper reports overall means (and references Table S4/Fig. 2), but it does not present thorough confidence intervals, per-target bootstraps, or seed-sensitivity analyses; important because many methods sample multiple poses and CD averages at the target level. It is therefore hard to tell whether SD$\\leftrightarrow$CD deltas are robust to sampling/seed counts and the number of receptor conformers considered.\n\n- **Per-method relaxation settings unclear**. While the OpenMM relaxation is described, it is not fully explicit whether built-in refinements for methods that include their own relaxation were disabled or double-applied, which can tilt PB-Valid and RMSD outcomes. Appendix B lists settings but the policy for overriding internal refinement isn’t crisply stated.\n\n- **Novelty of relaxation module**. The introduction of PB-Valid and OpenMM-based relaxation is a helpful addition, but it is not clear that these components represent a substantial methodological contribution. They appear to extend existing evaluation or refinement tools rather than introduce fundamentally new ideas."}, "questions": {"value": "- **Ground truth in CD is geometry-only**. CD \"truth\" is defined by transferring the ligand after receptor alignment; there is no check that interaction patterns are conserved. Did authors consider any kinds of interaction pattern comparison such as interaction-fingerprint between co-crystal structure and CD structures?\n- **Data-processing thresholds need justification**. How did authors choose the criteria of 0.2A in intermolecular distances in selection step (Table S1,2)?\n- **How did authors run DynamicBind?** Although DynamicBind needs apo protein structure, authors did not clarify any information about this.\n- **Relaxation policy**. For methods that already implement internal relaxation/refinement, did you disable their internal step before applying your OpenMM relaxation, or apply both?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lZLvTKyyZA", "forum": "qqzxKudD4T", "replyto": "qqzxKudD4T", "signatures": ["ICLR.cc/2026/Conference/Submission16735/Reviewer_DUgZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16735/Reviewer_DUgZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824104641, "cdate": 1761824104641, "tmdate": 1762926783892, "mdate": 1762926783892, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "PoseX is a new benchmark for a new variant in docking task using a series of new \"co-folding\" methods. Compared to previous benchmarks, PoseX have more design related to the recent changes in docking methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Since the development of AlphaFold3 and related methods, this task has drawn wildly attention. Old benchmark like PoseBusters show limitation in such tasks. Compared to traditional docking benchmark, cross-docking is harder, and able to detect more hallucinations cases which is often existed in AlphaFold 3 like models."}, "weaknesses": {"value": "There is little to show the difference between co-folding methods and traditonal methods, since co-folding methods might have strong advantages in docking on protein with native conformations. I suggest authors to add more examples and highlight these tasks including docking on AlphaFold3 predicted native conformation or solved native conformations to stress this difference."}, "questions": {"value": "1. Authors can report more details about other affecting factors, including the docking performance affected by molecule size, hard-ness of protein target, etc.\n2. The detailed parameters of the molecular dynamics simulation should be showed, which force field, the time length, the maximum round and the strategy, treatment of hydrogen bond, restrictions, solvent information and solvent model, etc."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "53nBGwTtOZ", "forum": "qqzxKudD4T", "replyto": "qqzxKudD4T", "signatures": ["ICLR.cc/2026/Conference/Submission16735/Reviewer_Az7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16735/Reviewer_Az7C"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16735/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906534139, "cdate": 1761906534139, "tmdate": 1762926783475, "mdate": 1762926783475, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}