{"id": "9422Ihqux3", "number": 17079, "cdate": 1758271888295, "mdate": 1759897199554, "content": {"title": "MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification", "abstract": "Generative modeling has emerged as a powerful paradigm for representation learning, but its direct applicability to challenging fields like medical imaging remains limited: mere generation, without task alignment, fails to provide a robust foundation for clinical use. We propose MAGIC-Flow, a conditional multiscale normalizing flow architecture that performs generation and classification within a single modular framework. The model is built as a hierarchy of invertible and differentiable bijections, where the Jacobian determinant factorizes across sub-transformations. We show how this ensures exact likelihood computation and stable optimization, while invertibility enables explicit visualization of sample likelihoods, providing an interpretable lens into the model’s reasoning. By conditioning on class labels, MAGIC-Flow supports controllable sample synthesis and principled class-probability estimation, effectively aiding both generative and discriminative objectives.\nWe evaluate MAGIC-Flow against top baselines using metrics for similarity, fidelity, and diversity. Across multiple datasets, it addresses generation and classification under scanner noise, and modality-specific synthesis and identification. Results show MAGIC-Flow creates realistic, diverse samples and improves classification.\nMAGIC-Flow is an effective strategy for generation and classification in data-limited domains, with direct benefits for privacy-preserving augmentation, robust generalization, and trustworthy medical AI.", "tldr": "We propose MAGIC-Flow, an invertible conditional normalizing flow that jointly learns generation and classification, improving both performance and explainability in medical imaging through exact likelihoods.", "keywords": ["Conditional normalizing flows; Invertible generative models; Joint generation and classification; Medical imaging; Explainability and likelihood visualization; Data augmentation; Trustworthy and generalizable AI;"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6ebd42d16506e6d555ef86b81f1a09ec9baf28d0.pdf", "supplementary_material": "/attachment/df38e695046e6347afa4e761578ef433cb45f845.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes MAGIC-Flow, a conditional multiscale normalizing flow framework that unifies image generation and classification within a single invertible architecture.\nThe model is designed as a hierarchy of bijective transformations with factored Jacobians, allowing exact likelihood computation and stable training.\nBy leveraging conditional coupling layers, the method supports controllable sample synthesis and likelihood-based classification, and enables explicit visualization of sample likelihoods as an intrinsic interpretability mechanism.\nExperiments on medical imaging datasets evaluate both scanner-conditioned and modality-conditioned generation, as well as scanner classification, showing visually coherent and diverse samples with stable performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes a normalizing flow model that unifies image generation and classification within a single invertible architecture, demonstrating flexibility across tasks.\n\n2. The use of likelihood attribution maps offers a theoretically grounded, faithful interpretability mechanism that is intrinsic to the model, rather than relying on gradient approximations.\n\n3. The model achieves stable training and competitive quantitative results on scanner- and modality-conditioned generation tasks compared with classic GAN-based methods"}, "weaknesses": {"value": "1. The paper mainly compares with GAN-based methods and lacks comparison with more recent state-of-the-art approaches, such as 3D medical latent diffusion models.\n\n2. Evaluation should be extended to more recent and representative datasets (e.g., BraTS2021) to better demonstrate generalizability.\n\n3. The paper presents qualitative visualization of likelihood attribution maps, which are common in flow-matching works; however, further clarification is needed on how these maps relate to classification or generation, and in what way they provide superior interpretability.\n\n4. The scanner-classification experiment lacks clear practical motivation—additional evidence is needed to show how it benefits downstream medical tasks, such as segmentation or diagnosis."}, "questions": {"value": "1. How do the highlighted regions in the likelihood attribution maps quantitatively relate to the model’s decision process in classification or generation?\n\n2. What is the motivation and practical utility of the scanner-classification experiment beyond demonstrating the model’s discriminative capability?\n\n3. Could the proposed likelihood-based explanation and conditional generation framework be extended to dense prediction settings, for instance by conditioning on segmentation masks as structured spatial inputs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mByHo29dQP", "forum": "9422Ihqux3", "replyto": "9422Ihqux3", "signatures": ["ICLR.cc/2026/Conference/Submission17079/Reviewer_yfGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17079/Reviewer_yfGQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760636389497, "cdate": 1760636389497, "tmdate": 1762927086788, "mdate": 1762927086788, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MAGIC-Flow, a conditional multiscale normalizing flow that unifies generation and classification within a single invertible backbone. By leveraging conditional affine couplings, squeeze/split hierarchy, and mask scheduling, the model preserves exact likelihoods and stable optimization while switching between tasks through task-specific coupling networks (expressive FiLM/attention blocks for generation vs. streamlined discriminative couplings for classification). Experiments on scanner-conditioned and modality-conditioned medical image generation and on scanner identification show improved FID/KID, PRDC, MS-SSIM, and class-balanced classification metrics over GANs, diffusion, VAEs, and strong CNN/ViT baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tUnified and practical design: The method uses the same invertible network for both generation and classification, leveraging exact likelihood computation for both. This is a conceptually clean and theoretically sound approach.\n2.\tThe method's performance is demonstrated on a challenging, domain-specific problem in medical imaging."}, "weaknesses": {"value": "1. Novelty concerns: “Unified generation + classification” sounds incremental relative to prior conditional flows. The paper claims the first conditional multiscale flow that supports both generation and classification on a shared invertible backbone. Yet the Related Work already notes conditional normalizing flows (cNFs), cINNs, and CAFLOW as frameworks for modeling p(x|y)and using label-conditioned transformations, which naturally enable likelihood-based decisions. The manuscript does not clearly articulate a necessary capability gap in cINN/CAFLOW/cNF that MAGIC-Flow uniquely closes. \n2. Missing same-family baselines (cINN/CAFLOW/cNF): Results compare against GAN/DDPM/CVAE for generation and CNN/ViT for classification, but not against strong conditional-flow baselines under matched parameters and budgets. \n3. Computational Complexity and Scalability: Normalizing flows are notoriously computationally expensive and memory-intensive due to the requirement of calculating Jacobian determinants. The paper's multiscale architecture with 24 flow steps, while expressive, likely exacerbates this issue. A discussion on training/inference time, memory footprint, and scalability to higher resolutions (e.g., 3D volumes, which is noted as a limitation) compared to baselines is missing but crucial for assessing practical utility. \n4. Limited Analysis of the \"Unified\" Claim: While the framework is unified, the implementation requires two different, task-specific coupling networks (for generation and classification). The paper does not adequately explore the interchangeability or shared learning between these two \"modes.\" An ablation studying a single, general-purpose coupling layer for both tasks would have strengthened the claim of a truly unified model. \n5. Indirect Classification Mechanism: The classification is performed by comparing likelihoods p(x∣y) for all possible y, which requires running the forward pass once for each class. This is an O(C) operation at inference time, making it significantly less efficient than a single-pass discriminative model (O(1)) when the number of classes C is large. This scalability limitation for large-class problems is not discussed. \n6. Niche Applicability of Classification Task: The primary classification task demonstrated is \"scanner identification,\" which, while challenging, is more of a technical or calibration problem than a core clinical task (e.g., disease diagnosis). The paper would be stronger if it also included results on a canonical pathology classification task to demonstrate the generalizability of its discriminative capabilities to semantically meaningful categories."}, "questions": {"value": "Although the method emphasizes minimal changes across tasks, the generation and classification couplings embed numerous capacity-boosting modules—FiLM conditioning, CBAM attention, residual blocks, global context, and ASPP/SE—introduced at multiple depths. This looks like substantial architectural augmentation rather than a light tweak, and the paper lacks component-wise ablations to justify necessity vs. over-engineering. A clear complexity–benefit tradeoff is needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1mEyks5T3l", "forum": "9422Ihqux3", "replyto": "9422Ihqux3", "signatures": ["ICLR.cc/2026/Conference/Submission17079/Reviewer_DT2P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17079/Reviewer_DT2P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761920740146, "cdate": 1761920740146, "tmdate": 1762927086295, "mdate": 1762927086295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces MAGIC-Flow, a conditional multiscale normalizing flow designed to unify image generation and classification within a single invertible framework. Unlike standard flows (e.g., RealNVP, Glow) that are purely generative, MAGIC-Flow leverages conditional  mappings to perform both tasks with exact likelihood estimation and explicit interpretability.\n\nThe architecture integrates hierarchical flow steps, squeeze and split operations, and task-specific  layers to capture multi-scale structure efficiently. Two coupling variants are proposed: one for expressive, label-conditioned generation (using FiLM  CBAM attention), and one for discriminative classification via likelihood-based decision-making.\n\nExperiments on multiple medical imaging datasets (MRI, PET) demonstrate strong performance on both conditional generation and classification . MAGIC-Flow achieves substantially lower FID/KID scores than GAN, diffusion, and VAE baselines, while matching or exceeding CNN and ViT classifiers in accuracy and balanced recall."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "I find this paper to be technically strong and clearly presented. The authors address an important gap between generative and discriminative modeling in medical imaging by proposing an invertible architecture that unifies both within a single framework. This is an elegant idea, as existing flow-based methods typically require separate models or auxiliary classifiers.\n\nThe paper provides a solid theoretical foundation showing that Jacobian factorization extend naturally to the conditional setting.  I also appreciate the task-specific layers, which are carefully adapted for generation and classification..\n\nThe empirical evaluation is extensive and convincing. The authors benchmark across multiple public MRI and PET datasets, demonstrating strong improvements in both generation quality (FID/KID, PRDC metrics) and classification performance (accuracy +  F1). The visual results look realistic and diverse."}, "weaknesses": {"value": "While the paper and the idea of unifying generation and classification in a conditional flow is appealing, I find the degree of novelty to be moderate. The architecture largely builds upon established components from RealNVP and Glow, and while the conditional and task-specific extensions are thoughtfully designed, they remain incremental rather than groundbreaking. The main conceptual contribution is more in integration and adaptation than in introducing fundamentally new flow mechanisms.\n\nFrom an experimental standpoint, although the results are strong, the evaluation focuses almost entirely on medical imaging datasets (MRI and PET). Furthermore, most comparisons are made against GANs, diffusion, and VAEs, but not against recent state-of-the-art conditional flows. \n\nA second limitation is interpretability validation: while the proposed likelihood attribution maps are interesting and conceptually sound, their evaluation remains qualitative. It would be beneficial to include quantitative validation to demonstrate that these maps meaningfully explain the model’s decisions.\n\nFinally, although the paper discusses clinical trustworthiness and interpretability, there is limited discussion on computational cost and scalability. Flow-based models are often resource-intensive, and it would help to see clearer comparisons of runtime, parameter count, and memory usage against other generative models."}, "questions": {"value": "- Could you clarify how MAGIC-Flow fundamentally differs from prior conditional or multi-scale flow models\n- Flow-based models are often resource-heavy. Could you provide more details on runtime, memory , and training stability compared to baselines? How does inference time scale with image size?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yV1d0lCaRD", "forum": "9422Ihqux3", "replyto": "9422Ihqux3", "signatures": ["ICLR.cc/2026/Conference/Submission17079/Reviewer_GBzt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17079/Reviewer_GBzt"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17079/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979004591, "cdate": 1761979004591, "tmdate": 1762927085764, "mdate": 1762927085764, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}