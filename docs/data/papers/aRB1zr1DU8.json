{"id": "aRB1zr1DU8", "number": 8066, "cdate": 1758057176224, "mdate": 1759897810647, "content": {"title": "CLEAR: A Cost-Aware Routing System for Edge-Cloud Language Model Collaborative Inference", "abstract": "Large language models (LLMs) demonstrate exceptional performance across various natural language processing tasks but face significant computational and memory constraints, making direct deployment on resource-limited edge devices impractical. To address this challenge, we propose CLEAR, a cost-aware edge-cloud collaborative inference framework that efficiently integrates cloud-based LLMs with small language models (SLMs) running on edge devices. CLEAR introduces a cost-aware router that dynamically evaluates SLM-generated outputs and selectively routes low-quality output to cloud-based LLMs for refinement, balancing quality and computational efficiency. The framework incorporates two key innovations: KV cache management system and reinforcement learning-based router training. The KV cache management system prevents cache eviction and minimizes redundant computations by limiting concurrent cloud requests and optimizing retrieval efficiency. Additionally, the router is trained using reinforcement learning to make adaptive routing decisions that minimize cloud usage while maintaining output quality. Our experimental results demonstrate that CLEAR significantly reduces inference cost and latency while maintaining high response quality, outperforming existing cloud-edge collaborative inference methods. Specifically, it achieves a cost reduction of 46% while achieving similar performance or a performance improvement of 15% with similar inference cost. These findings highlight the potential of CLEAR as an efficient and scalable solution for real-time edge-cloud inference applications.", "tldr": "", "keywords": ["cloud-edge collaborative inference", "efficient inference", "large language model"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/668755b1828be3d1dfc61d4758c9341fa72a7d94.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes CLEAR, a cost-aware collaborative inference framework for Large Language Models (LLMs) that intelligently balances the use of a lightweight Small Language Model (SLM) on an edge device and a powerful LLM in the cloud. The framework's goal is to reduce inference cost and latency while maintaining or even improving response quality. Experiments across four benchmark datasets show that CLEAR significantly outperforms existing cloud-edge collaborative methods, achieving up to a 46% cost reduction at similar accuracy, or a 15% accuracy improvement at similar cost. Notably, the framework is also shown to sometimes surpass the accuracy of using the powerful cloud LLM alone."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength is that it doesn't just propose an \"smart router\" algorithm in isolation. It co-designs the edge-side routing policy (the RL router) with a highly practical and novel cloud-side serving backend (the KV Cache Management System). This full-stack approach addresses the real-world bottlenecks of collaborative inference, which many prior works ignore. The cloud-side solution is clever, well-motivated, and solves a critical problem. The \"Query Queuing\" mechanism (Fig 5) correctly identifies that limiting concurrent queries (users) rather than requests is the key to preventing cache thrashing. The \"Multi-Token Generation\" mechanism (Table 2) is a smart optimization that exploits the block-based memory layout of modern systems (like PagedAttention) to reduce network overhead."}, "weaknesses": {"value": "The paper mentions that the router is warm-started using supervised learning (SFT) on \"pre-collected token-level routing information\" (Algorithm 2, line 3). This SFT-trained router is also used as a baseline (Fig 6). However, the paper does not specify how this \"ground truth\" routing data was generated. Was it from an oracle (e.g., comparing SLM vs. LLM output at each step)? Based on SLM confidence scores? This detail is important for reproducibility and for fairly assessing the SFT baseline."}, "questions": {"value": "How was the \"pre-collected token-level routing information\" used for the SFT warm-start (Algorithm 2) and the SFT baseline (Figure 6) generated? What defined the \"correct\" routing decision in this supervised dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "95BCaBWKBs", "forum": "aRB1zr1DU8", "replyto": "aRB1zr1DU8", "signatures": ["ICLR.cc/2026/Conference/Submission8066/Reviewer_Eeqa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8066/Reviewer_Eeqa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761409322557, "cdate": 1761409322557, "tmdate": 1762920056039, "mdate": 1762920056039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose CLEAR (Cost-aware LLM Edge-cloud Adaptive Routing), an optimized edge-cloud LLM collaborative inference framework. In collaborative inference, issues exist such as high latency, computational overhead, and increased communication costs due to transmitting full hidden states from the edge device to the cloud or requiring the cloud model to verify each output token. To solve these issues, CLEAR proposes an RL-based router and a KV cache management system. The router decides if some generated tokens from the edge are of low quality and should be routed to the cloud, and the KV cache system limits concurrent requests on the cloud to minimize cache swaps. Experiments on 4 datasets show that CLEAR is faster than baselines while maintaining higher accuracies."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The idea of using an RL-based router and KV cache management system is intuitive and easy to follow.\n- Comparison with baselines shows superiority of the proposed method"}, "weaknesses": {"value": "- The active connections in the KV management are limited by a fixed constant and might not be optimal in real-world settings. \n\nThe idea of constraining active connections on the cloud does help KV cache management, but making C fixed limits the adaptability of the system. I'd imagine the choice of C depends on the workload. Experiments are conducted on datasets, which are homogeneous in nature. Practically, workloads are mixed and vary by very complex factors. It makes more sense to make C adaptable to history or make it a learnable factor. The paper would be strengthened if more discussions on this were included.\n\n- More experiments are needed to support the generalization of the proposed method.\n\nThe current experiments are only evaluated on one model. It would be better to consider more mode choices. In addition, both edge and cloud are H100. More choices of edge devices (such as smaller GPUs or even CPU environments) are needed to understand the efficiency of the proposed method."}, "questions": {"value": "1. Line 361-364 says \"our method’s performance is so effective that it even surpasses the accuracy of using the powerful cloud-based LLM alone, with improvements of up to 7%.\" This is interesting. I may have missed the exact numbers, but I want to see this documented in a table. Also, does this generalize to different model choices?\n2. How faster is the proposed method compared to the full cloud setting? Currently, I only see the tradeoff between latency and accuracy, but it would be better to document the latency of using purely cloud LLMs."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3JZAoA7vOw", "forum": "aRB1zr1DU8", "replyto": "aRB1zr1DU8", "signatures": ["ICLR.cc/2026/Conference/Submission8066/Reviewer_rxqu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8066/Reviewer_rxqu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468595840, "cdate": 1761468595840, "tmdate": 1762920055307, "mdate": 1762920055307, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CLEAR, a cost-aware collaborative inference framework designed to bridge the gap between resource-limited edge devices and powerful cloud-based LLMs. The system utilizes a SLM on the edge, coupled with a lightweight, reinforcement learning-trained router that dynamically assesses the quality of the SLM's generated output. When the router detects low-quality tokens, it selectively sends a request to the cloud LLM for refinement, thereby balancing output quality with computational and network costs. To optimize cloud-side efficiency and reduce latency, CLEAR incorporates a KV cache management system that uses query queuing to prevent cache eviction during high concurrency and a multi-token generation mechanism to minimize network requests."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Performance and Cost-Efficiency: Demonstrate a strong trade-off between accuracy and inference cost, achieving up to a 46% cost reduction at similar performance, or a 15% accuracy gain at similar cost, compared to baselines.\n- KV Cache Management: Propose an effective cloud-side KV cache management system. This system uses query queuing to prevent cache eviction under high concurrency and a multi-token generation mechanism to reduce network latency and avoid redundant computations .\n- Low Communication Overhead: Compared to other methods that require transmitting full hidden states, CLEAR only sends token IDs, which reduces the volume of data transmitted and the associated communication latency."}, "weaknesses": {"value": "- Lack of innovation. The KV cache reuse mechanism has already been widely adopted in the industry, so it is not surprising. Similarly, token-level edge–cloud collaborative refinement has also been explored in related works, making the distinction insufficiently clear.\n\n- No cross-task generalization experiments. The paper lacks any discussion or experiments on generalization. The proposed router is trained separately for each specific task (dataset), showing no analysis of generalization across tasks. There is also no discussion of generalization across models—whether the approach would still work if different model combinations were used.\n\n- Tasks used in experiments are too simple. For example, Qwen3-8B already achieves 60.80% accuracy on math tasks, which is higher than the best result in Figure D. This raises the question of whether edge–cloud collaboration is still meaningful. Furthermore, the number of tasks is too limited; more complex reasoning tasks are needed to demonstrate the effectiveness of the proposed approach.\n\n- Too few baselines for edge–cloud collaboration. Existing work in this area goes far beyond the two baselines included in the paper."}, "questions": {"value": "Same as the weaknesses mentioned above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bWhHglojxe", "forum": "aRB1zr1DU8", "replyto": "aRB1zr1DU8", "signatures": ["ICLR.cc/2026/Conference/Submission8066/Reviewer_TLaJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8066/Reviewer_TLaJ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8066/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761530544572, "cdate": 1761530544572, "tmdate": 1762920054648, "mdate": 1762920054648, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}