{"id": "8WS5nDWIWE", "number": 18554, "cdate": 1758289082293, "mdate": 1759897096103, "content": {"title": "SteinsGate: Adding Causality to Diffusions for Long Video Generation via Path Integral", "abstract": "Video generation has advanced rapidly, but current models remain limited to short clips, far from the length and complexity of real-world narratives. Long video generation is thus both important and challenging. Existing approaches either attempt to extend the modeling length of video diffusion models directly or merge short clips via shared frames. However, due to the lack of temporal causality modeling for video data, they achieve only limited extensions, suffer from discontinuous or even contradictory actions, and fail to support flexible and fine-grained temporal control. Thus, we propose Instruct-Video-Continuation (InstructVC), combining Temporal Action Binding for fine-grained temporal control and Causal Video Continuation for natural long-term simulation. Temporal Action Binding decomposes complex long videos by temporal causality into scene descriptions and action sequences with predicted durations, while Causal Video Continuation autoregressively generates coherent video narratives from the text story. We further introduce SteinsGate, an inference-time instance of InstructVC that uses an MLLM for Temporal Action Binding and Video Path Integral to enforce causality between actions, converting a pre-trained TI2V diffusion model into an autoregressive video continuation model. Benchmark results demonstrate the advantages of SteinsGate and InstructVC in achieving accurate temporal control and generating natural, smooth multi-action long videos.", "tldr": "", "keywords": ["Generative Models", "Video Generation", "Diffusion Guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1ce397172dfb73c9aed2f790bd0e23475b38381.pdf", "supplementary_material": "/attachment/4de233f5fb667a486b13c0f442f0b7f54bbb1737.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Instruct-Video-Continuation (InstructVC), a two-stage inference-time framework for multi-action long video generation that adds temporal causality to pretrained video diffusion models. Stage 1 (Temporal Action Binding) uses an MLLM with in-context examples to decompose and expand a user prompt into a scene description plus a sequence of action-duration pairs. Stage 2 (Causal Video Continuation) converts an Image-to-Video (I2V) diffusion model into a history-aware autoregressive continuer via Video Path Integral (VPI), which integrates multiple I2V “video paths” from sampled historical frames to bias future trajectories toward history-consistent directions. The practical instance SteinsGate adds three optimizations—Guidance Interval, History-aligned Redistribution, and Path Convergence Guidance—to reduce compute and improve alignment. Experiments on a constructed InstructVC benchmark and ablations report improved temporal control, motion smoothness, and multi-action continuity vs. several inference-time baselines and show competitive performance with heavier training-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper isolates two well-motivated, complementary challenges for long video generation—temporal causality and temporal control—and proposes a coherent two-stage solution.\n\n2. Video Path Integral provides a principled, interpretable way to propagate temporal information from multiple history frames and to bias generation toward causally consistent continuations without retraining the foundation model.\n\n3. The three SteinsGate optimizations (Guidance Interval, History-aligned Redistribution, Path Convergence Guidance) address computational cost and estimation noise, making the method more viable in practice.\n\n4. Temporal Action Binding leverages MLLM capabilities for temporally grounding and duration prediction, enabling fine-grained temporal control over multi-action sequences.\n\n5. The paper includes qualitative continuation examples, a system-level comparison for multi-action long generation, and ablations that quantify contribution of each component.\n\n6. The method can be applied at inference-time to pretrained TI2V/DiT models, which increases practical applicability and lowers barrier for adoption."}, "weaknesses": {"value": "1. The VPI argument is mostly conceptual and analogy-driven (path integral intuition); formal bounds or rigorous analysis of when/why VPI converges to the true conditional distribution are missing.\n\n2. Performance and long-term consistency appear sensitive to how much and which historical frames are used, but the paper provides only heuristic choices and limited analysis of failure modes for varying history lengths.\n\n3. Although in-context learning is used to reduce implausible decompositions, reliance on an MLLM can still produce action sequences or durations that are out-of-distribution for the pretrained video model; mitigation and quantitative assessment are limited.\n\n4. Benchmark construction and evaluation transparency: the InstructVC benchmark is constructed by expanding dense-caption datasets, but details on dataset size, prompt diversity, split protocols, evaluation metrics definitions, and human evaluation procedures are sparse.\n\n5. Reported baselines are appropriate, but the paper lacks comparisons against recent large-scale training-based long-video systems (or more thorough hyperparameter-matched baselines) that could better contextualize headroom and limitations.\n\n6. The metrics focus on smoothness and CLIP alignment; deeper semantic correctness (action completeness, causal correctness judged by humans) and statistical significance of improvements are not fully reported."}, "questions": {"value": "1. Can the authors precisely define the notation and dimensionality used in Eq. (5) for Video Path Integral, including what each index (i, j, K, H) represents and how histories are sampled in practice?\n\n2. How exactly is the mapping performed from image-conditioned I2V vector fields to the history-conditioned vector fields ve(Zt | xj); give the algorithmic steps used at each denoising iteration.\n\n3. What is the precise schedule for Guidance Interval: is it a fixed fraction of the continuous time variable, and how was 0.3 chosen?\n\n4. When you say the Video Path Integral approximates p(Zi | Zh) by product of frame-wise conditionals (Eq. 6), what independence assumptions are implied and when do they break down?\n\n5. How many prompts and total videos are in the InstructVC benchmark (train/val/test splits), and what is the distribution of number-of-actions, scene types, and durations?\n\n6. What are the exact definitions and implementations for CSCV+, Motion Smoothness, and Text-Image Alignment metrics, and how were thresholds or evaluation pipelines calibrated?\n\n7. Were human raters used for causal correctness or action completeness? If so, how many raters, what instructions, and inter-rater agreement (e.g., Cohen’s kappa) were measured?\n\n8. For quantitative comparisons, were compute budgets and sampling steps matched across methods (same sampler, same number of denoising steps)? Please provide latency and GPU-memory numbers.\n\n9. Please provide a sensitivity analysis for the number of history frames K, the selection strategy for those frames, and the history-length ratio used for different action durations.\n\n10. How robust is SteinsGate to noisy or incorrect history (e.g., partially corrupted frames, temporal jitter, or mismatched last-frame poses)?\n\n11. How does performance degrade over very long sequences beyond 30s, and what empirical evidence supports the claim that VPI reduces error accumulation compared to I2V-AR?\n\n12. What failure modes are most common (motion reversal, skipped actions, object disappearance), and can you quantify their frequencies per baseline?\n\n13. What MLLM model/version and temperature/prompting strategy was used for Temporal Action Binding, and what in-context examples were provided (exact prompts/examples)?\n\n13. Please provide quantitative measures of MLLM output quality: rate of hallucinated or physically implausible action sequences, distribution shift vs. the TI2V training captions, and any post-filtering applied.\n\n14. Will code, exact prompts, model checkpoints (Wan2.1 config and weights), and InstructVC benchmark splits be released?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N0A8u6g8vx", "forum": "8WS5nDWIWE", "replyto": "8WS5nDWIWE", "signatures": ["ICLR.cc/2026/Conference/Submission18554/Reviewer_inkc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18554/Reviewer_inkc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761683392729, "cdate": 1761683392729, "tmdate": 1762928264042, "mdate": 1762928264042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework for multi-action long video generation, Instruct-Video-Continuation (InstructVC) that contains two control stages: Temporal Action Binding and Causal Video Continuation. The authors further introduce an inference-time instance SteinsGate."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized.  \n\n2. The topic of long video generation is worth exploring in the research community. This paper targets this important problem.\n\n3. The paper provides some video demos to help reviewers better evaluate the performance of the proposed method."}, "weaknesses": {"value": "There are some concerns and questions about this paper:\n\n1.\tThe third paragraph of the introduction is a bit confusing. The first part mentions two methods for generating long videos: temporal expanding and temporal decomposition. So, to which category does the latter part of the paragraph, I2V-AR, and another work, belong?\n\n2.\tIt is recommended that the author provide a corresponding video for Fig. 1. A few frames are insufficient to convey the author's intended meaning.\n\n3.\tThe author repeatedly emphasizes the term \"temporal causality.\" What exactly does this term mean? Why does its absence lead to phenomena such as temporal inconsistency and action direction conflict in the generated video?\n\n4.\tThe two paragraphs following the introduction cover a wide range of topics, such as Temporal Action Binding, Causal Video Continuation, Guidance Interval, History-aligned Redistribution, and Path Convergence Guidance. This can be quite confusing, as it raises the question: which part is the core of the proposed method? Which part is the key to solving long-action video generation?\n\n5.\tThe video demos in the supplementary materials all seem to involve very simple actions and are basically long videos within a single content. I think focusing on story-based long video generation would be better. Additionally, the video \"woman_gestures\" is clearly discontinuous, exhibiting obvious splicing artifacts from multiple video clips."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AFDPXk4Wej", "forum": "8WS5nDWIWE", "replyto": "8WS5nDWIWE", "signatures": ["ICLR.cc/2026/Conference/Submission18554/Reviewer_8cn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18554/Reviewer_8cn5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974870793, "cdate": 1761974870793, "tmdate": 1762928263512, "mdate": 1762928263512, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new framework, Instruct-Video-Continuation (InstructVC), to address the challenge of generating coherent, multi-action long videos. The framework contains two stages: Temporal Action Binding, which decomposes a user prompt into a sequence of segment plans with durations via LLM, and Causal Video Continuation, which autoregressively generates the video according to that plan. The authors introduce SteinsGate, a training-free, plug-and-play method of this framework that introduces a novel temporal guidance technique called Video Path Integral (VPI). VPI enforces causality by integrating multiple I2V video paths from different history frames."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is training-free and plug-and-play. It does not need the training resources, thus it could be integrated easily into methods. \n2. The novelty of the VPI technique is quite sound and reasonable. The writing is also good. \n3. The method is somewhat time-consuming when inference, while the authors recognise this and propose a speeding strategy of Guidance Interval GI. From the ablation study, this GI method achieves comparable results before and after speedup."}, "weaknesses": {"value": "Overall, the paper is logically rigorous, addresses an important problem, and proposes a novel method. I do not see major flaws. However, there are a few minor points that, if answered by the authors, I would consider revising my score after rebuttal:\n1. While VPI is a training-free method, it appears to introduce significant inference-time overhead. It will be better to quantify the increase in computational cost. Specifically, how many times more computationally expensive is the VPI method (which samples and integrates $K$ frames) compared to the baseline of generating from only the last frame?\n2. The Path Convergence Guidance (PCG) method introduces a new hyperparameter, $w_1$. The paper seems to suggest setting this parameter to 5.0, but lacks a detailed explanation. While the concept may be intuitive to some, why must $w_1$ be a value greater than 1 (implying \"over-correction\")? Why can it not be less than 1 (implying a smoothing correction, similar to an EMA) or exactly 1 (implying a full replacement of the weak model's path)? The paper would be strengthened by a qualitative analysis, or ideally, an ablation study or visualization that justifies this specific choice.\n3. The paper provides very little detail on how the \"segment history\" is selected. The Appendix (L657) only mentions that it is chosen based on a \"fixed ratio\" and that the frame count must satisfy a \"$4N+1$\" format. Why for both of these? Intuitively, the choice of history length seems highly sensitive: \n- A history that is too short may prevent VPI from capturing sufficient information to continue the motion naturally.\n- A history that is too long might make it extremely difficult to align a video generated from a very early frame with the distant future, potentially conflicting with the HR mechanism.\n4. The method of introducing different prompts for different temporal segments (with or without an MLLM) is not entirely uncommon. The paper may be missing citations to some related works that have explored similar multi-prompt temporal decomposition strategies:\n\n[1] Yan, Xin, et al. \"Long video diffusion generation with segmented cross-attention and content-rich video data curation.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[2] Bansal, Hritik, et al. \"Talc: Time-aligned captions for multi-scene text-to-video generation.\" arXiv preprint arXiv:2405.04682 (2024).\n\n[3] Oh, Gyeongrok, et al. \"MEVG: Multi-event Video Generation with Text-to-Video Models.\" ArXiv, 2023, https://arxiv.org/abs/2312.04086.\n\n[4] Villegas, Ruben, et al. \"Phenaki: Variable Length Video Generation From Open Domain Textual Description.\" ArXiv, 2022, https://arxiv.org/abs/2210.02399"}, "questions": {"value": "A baseline could be directly using the last frame of the last segment, with a new prompt, to obtain the new segment. And concat all segments together. Is this setting the same as the w/o VPI results?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "x6bPQZb45e", "forum": "8WS5nDWIWE", "replyto": "8WS5nDWIWE", "signatures": ["ICLR.cc/2026/Conference/Submission18554/Reviewer_nDSM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18554/Reviewer_nDSM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18554/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006449383, "cdate": 1762006449383, "tmdate": 1762928263121, "mdate": 1762928263121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}