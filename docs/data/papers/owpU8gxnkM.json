{"id": "owpU8gxnkM", "number": 2172, "cdate": 1757010850954, "mdate": 1759898164925, "content": {"title": "ENCOURAGING CRITICAL THINKING FOR MULTIAGENT DEBATE", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks in recent years. While prior work has explored leveraging LLMs to generate synthetic data for self-improvement, repeated iterations often suffer from diminishing returns due to the reliance on homogeneous reasoning patterns and limited exploration of alternative perspectives. In this paper, we introduce a novel framework that enriches the reasoning process by encouraging critical thinking among multiple agents. Rather than deploying an ensemble of models with identical prompts, we propose a strategy generator that produces customized instructions tailored to each individual LLM. Acting as a critical thinking agent, the generator is iteratively fine-tuned using carefully selected strategies that are both diverse and effective. This approach fosters specialization within each model while promoting diversity across reasoning paths, enabling the system to maintain varied solution trajectories and achieve sustained performance gains through iterative refinement. We demonstrate the effectiveness of our method across a variety of agentic frameworks and complex reasoning tasks.", "tldr": "We propose a framework with optimizable strategies to guide LLM solvers in solving different questions.", "keywords": ["Debate", "Critical Thinking", "Self-reflection"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4e36f9dc875aba53ed372517357cb765839bfa5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper shows how to improve LLM performance using multiagent debate extended with several new contributions: diversification of the reasoning paths taken by different agents, critical thinking. The diverse paths are obtained by a strategy generator that generates M strategies used to prompt M agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The approach is interesting and the experimental results seem compelling."}, "weaknesses": {"value": "One overall weakness for this line of work, not just restricted to this particular contribution, is that it is not clear why this approaches lead to better performance. I do not count this remark against this paper as I think that these experimental results are important. \n\nThe diversity metric seems to be a key element of the approach. It would be interesting to see some alternative measures of diversity and how they impact the performance of the algorithm.\n\nMinor comments:\n\nLine 153, where it says “answer, denoted as y_{1,i}, where the”, I believe it should say y_{i,1}\n\nWhere it says “table 4.1” it should say “table 1”"}, "questions": {"value": "The similarity threshold \\tau to evaluate the diversity of the proposed strategies could be context dependent. In some cases, a large diversity might be needed, while in other cases it might be difficult to propose very different strategies. How do you chose this parameter and have you observed context dependent differences? The results from figure 5 are a first step in this direction. \n\nI am not convinced that “Critical Thinking” is what the approach is doing. Could you denote in algorithm 1, what part is the one responsible of critical thinking?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OlBxwnfNfN", "forum": "owpU8gxnkM", "replyto": "owpU8gxnkM", "signatures": ["ICLR.cc/2026/Conference/Submission2172/Reviewer_82VM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2172/Reviewer_82VM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761931823883, "cdate": 1761931823883, "tmdate": 1762916093210, "mdate": 1762916093210, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Critical Thinking with Multi-Agent Debate (CMAD), a framework for improving the reasoning capabilities of LLMs by training a strategy generator to generate diverse, undefined reasoning strategies. The framework iteratively fine-tunes the generator using feedback on correctness (based on majority voting) and diversity (based on a similarity metric), aiming to balance exploration and exploitation. Empirical results on MATH, GSM8K, and GPQA show consistent improvements across several LLMs compared to baselines such as DMAD and CoT."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of using a trainable strategy generator to produce undefined reasoning paths is creative and differentiates CMAD from prior multi-agent debate like DMAD.\n2. The paper evaluates across multiple benchmarks and models (GPT-4o-mini, LLaMA-3, Qwen2.5, Nova Micro), and compare with various baseline methods.\n3. The introduction convincingly argues the need to move beyond homogeneous reasoning and fixed strategies."}, "weaknesses": {"value": "1. The process of solution sharing and summarization risks contaminating the agents’ independent reasoning based on their given strategies. If each agent accesses others’ intermediate solutions, the resulting fine-tuning data may lose diversity and no longer reflect distinct strategies. The authors should clarify how they prevent such convergence or bias.\n2. The paper focuses on the Multi-Agent Debate (MAD) setting, but it does not explain why this setting is necessary over simpler mechanisms such as majority voting or ensemble averaging. Clarifying this design choice, especially how debate interaction benefits strategy generation beyond aggregation, would strengthen the motivation.\n3. Figure 1 is visually cluttered; the text overlaps and the color scheme makes it difficult to interpret. \n4. The related work section omits prior studies exploring similar concepts of using a trainable model to guide another model [1]\n\nReferences:\n[1] Li, Zekun, et al. \"Guiding large language models via directional stimulus prompting.\" Advances in Neural Information Processing Systems 36 (2023): 62630-62656."}, "questions": {"value": "1. The paper does not specify the underlying model for the strategy generator and solution agents.\n2. How do you make sure that each strategy actually contribute to the the final solution, given that each agent can not only see its given strategy but also other's solution."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8PXGDA3Ydh", "forum": "owpU8gxnkM", "replyto": "owpU8gxnkM", "signatures": ["ICLR.cc/2026/Conference/Submission2172/Reviewer_efZd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2172/Reviewer_efZd"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762245741648, "cdate": 1762245741648, "tmdate": 1762916092870, "mdate": 1762916092870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the homogeneous reasoning patterns of complex reasoning in LLMs and proposes Critical Thinking with Multi-Agent Debate (CMAD).\n\nCMAD uses a strategy generator that produces reasoning strategies for multiple LLM agents. After multi-round debates, a feedback loop balances correctness and diversity to select high-quality strategies for fine-tuning.\n\nExperiments show the framework is model-agnostic and outperforms baselines on reasoning benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- The paper has a good motivation to enable LLMs to generate diverse reasoning strategies instead of relying on fixed prompts (such as CoT, PoT, Step-back).\n\n- The proposed method is simple yet effective, selecting high-quality strategies with both correctness and diversity metrics and using these data to fine-tune a strategy generator."}, "weaknesses": {"value": "- Inconsistent Reporting of Results\n\n(1) Line 355-356 says \"The average improvement over the second-best method ranges from 1.2% to 9.8%.\" However, Table 1 shows that the performance gaps between CMAD and DMAD (the second-best method) are all less than 5%.\n\n(2) Comparing Table 1 and Table 4, the reported results for baselines are identical, but CMAD’s results differ. What is the difference in evaluation settings between these two tables?\n\n- Missing Important Experimental Details\n\n(1) The paper does not explicitly specify which models were fine-tuned to produce the reported results. While Line 742–743 implies Qwen2.5-7B, Line 700–701 mentions full-model fine-tuning for Qwen1.5-7B and LLaMA-8B. \n\n(2) The paper does not provide the prompt used for the strategy generator or examples of its training data.\n\nThe above two concerns make the results less convincing."}, "questions": {"value": "- What if we directly use the initial answers with different strategies (instead of going through the full debate process) to construct training data?\n\n- The description of Figure 2 refers to “refine the pre-training data”—should this instead be “fine-tuning data”?\n\n- The description of Table 3 does not align with its contents. Is the “DMAD” listed in Table 3 a typo that should be “CMAD”?\n\n- Reference mistake: \n\nLine 742: Table C should be Table 4; \n\nLine 355: Table 4.1 should be Table 1; \n\nLine 375, the baseline is incorrectly cited as published in 2015; the correct publication year is 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DL1e8rNyxA", "forum": "owpU8gxnkM", "replyto": "owpU8gxnkM", "signatures": ["ICLR.cc/2026/Conference/Submission2172/Reviewer_uewh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2172/Reviewer_uewh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762537131022, "cdate": 1762537131022, "tmdate": 1762916092597, "mdate": 1762916092597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LLMs can be made into \"agents\" to solve a problem by adding a \"Strategy\" to the input prompt along with the problem, and then iteratively refining what this strategy is based on how well the LLM solves the problem. However, this needs a way to score answers, which may or may or may not exist.\n\nThis paper proposes a method to do so by first instantiating several different such strategies, finding the resulting answers, and using agreements and diversity between these to refine the strategies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Compares agains a comprehensive set of baselines."}, "weaknesses": {"value": "Main paper does not contain enough specifics of the method. It is also unclear how it differs from one of the references. (see questions below for both these points)"}, "questions": {"value": "It is unclear what the strategy generator is. Is it an open-weights LLM (if so which one)? It is also unclear what precisely is meant by a \"strategy\", and how a set of these are generated from a question. A simple example in the main text of the paper would have helped a great deal clarifying this.\n\nWhat precisely is the difference between the method in this paper and the one in (Subramaniam et al 2025)? Also, it seems this method has not been compared against.\n\nIn Table 1, some methods involve no fine-tuning / training of any sort, and others (like CMAD) do. So in some sense some of these are not fair comparisons. At the very least, training-free and fine-tuned approaches should be demarcated as such.\n\nMinor typo: A_i on line 159\n\nHow are strategies mapped to vectors (which are needed for the diverse sampling  in line 209)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ql0MRofdjN", "forum": "owpU8gxnkM", "replyto": "owpU8gxnkM", "signatures": ["ICLR.cc/2026/Conference/Submission2172/Reviewer_hHRL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2172/Reviewer_hHRL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2172/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762552020644, "cdate": 1762552020644, "tmdate": 1762916091818, "mdate": 1762916091818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}