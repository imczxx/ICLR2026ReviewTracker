{"id": "fU2R326Mqp", "number": 23567, "cdate": 1758345568741, "mdate": 1763523729778, "content": {"title": "Forgive and Forget to Create Robust, Interpretable Models", "abstract": "Reaching internal transparency is a key challenge in the development of machine learning models. Rather than trying to interpret the models’ internal structures, our approach aims to make that internal structure more interpretable. To this end, we introduce a trio of mechanisms that act on the FFNs of mT5 and the Channel Mixing layers of RWKV to produce similar outcomes: Proximal Forgetfulness, which considers weights spatially and forces them into clusters of similar magnitude; Forgiveness, which rewards close predictions to shape internal model structure and progression; and Fuzzy Recall, which shifts activations into related bands. In combination, these mechanisms are able to dramatically transform the models’ internal topology in a controllable manner without compromising the performance of pretrained networks. Additionally, these changes make the model extremely resilient against noise and spatial perturbations. We show the modified internal topology is more dependent on the loss function than specific model architecture and can be crystallized if desired when changing tasks. With this new structure in place, internal token pathways can be represented with encouraging accuracy using a series of spatial centers and magnitudes. This is done without the use of a sparse autoencoder and could open the door to simplified control and interpretation in the future.", "tldr": "We developed a trio of structural mechanisms that bring a controllable, geographic order to the weights and activations of language models and reduce friction in model interpretation.", "keywords": ["Mechanistic Interpretability", "Conceptual Interpretability", "Transformer Circuits", "Representation Geometry", "Multilingual Models", "Activation Clustering", "Model Transparency", "Concept Subspaces"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5d9edf4bbe298e0cc9463326f98adc608571ac7a.pdf", "supplementary_material": "/attachment/db93554de0be49650cceefe671019502fea4dda0.zip"}, "replies": [{"content": {"summary": {"value": "The paper aims to increase the internal interpretability of language models via structural changes. They show that they are able to have high spatial correlation with activations and weights, while improving noise robustness due to the methodology proposed."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The work is very relevant and shows promise. The overall approach presented here is sound, and the experimental analysis is fair, although some improvements can be made. \n2) The results are very interesting. Structural interpretability is a long-standing problem in modern DNN-based approaches; research in this direction is sure to be appreciated by a wide community.\n3) Section 4.2 is very interesting. I would like to see more emphasis/further experimentation in this direction. Perhaps a broader approach to quantifying spatial correlation can aide both the readability and the soundness of the claims presented. \n4) Section 4.1 is an important experiment and I am glad to see this line of experimentation presented in the work."}, "weaknesses": {"value": "1) Firstly, the paper is not grounded well in the literature. Achieving structural interpretability in DNNs is a long-studied problem, either via meta-models, functional approximations, or dimensionality reduction. Including these lines of work references is key to making sure the paper shows relevance. Hence, in this current state, I cannot recommend acceptance. \n2) Although Section 4 is prepared well, including general coherence metrics is key to showing the soundness of the methodology presented. \n3) The overall paper is generally very verbose to get simple points across, especially in sections 2 and 3. Although this is a minor criticism, taking a look at this would really help the readers. \n4) Even though I do like the experimental setup, as I mentioned, I do think this paper can benefit from a breadth of experimentation to aid the claims presented. \n\nComment: Even though I think the paper needs to ground itself well in the literature and add a few experiments to aid the claim, I do like the work and think this is a very interesting direction of research and will be appreciated by the community."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4g0gRzvGKB", "forum": "fU2R326Mqp", "replyto": "fU2R326Mqp", "signatures": ["ICLR.cc/2026/Conference/Submission23567/Reviewer_P58N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23567/Reviewer_P58N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944506506, "cdate": 1761944506506, "tmdate": 1762942714591, "mdate": 1762942714591, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to increase the internal interpretability of language models via structural changes. They show that they are able to have high spatial correlation with activations and weights, while improving noise robustness due to the methodology proposed."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1) The work is very relevant and shows promise. The overall approach presented here is sound, and the experimental analysis is fair, although some improvements can be made. \n2) The results are very interesting. Structural interpretability is a long-standing problem in modern DNN-based approaches; research in this direction is sure to be appreciated by a wide community.\n3) Section 4.2 is very interesting. I would like to see more emphasis/further experimentation in this direction. Perhaps a broader approach to quantifying spatial correlation can aide both the readability and the soundness of the claims presented. \n4) Section 4.1 is an important experiment and I am glad to see this line of experimentation presented in the work."}, "weaknesses": {"value": "1) Firstly, the paper is not grounded well in the literature. Achieving structural interpretability in DNNs is a long-studied problem, either via meta-models, functional approximations, or dimensionality reduction. Including these lines of work references is key to making sure the paper shows relevance. Hence, in this current state, I cannot recommend acceptance. \n2) Although Section 4 is prepared well, including general coherence metrics is key to showing the soundness of the methodology presented. \n3) The overall paper is generally very verbose to get simple points across, especially in sections 2 and 3. Although this is a minor criticism, taking a look at this would really help the readers. \n4) Even though I do like the experimental setup, as I mentioned, I do think this paper can benefit from a breadth of experimentation to aid the claims presented. \n\nComment: Even though I think the paper needs to ground itself well in the literature and add a few experiments to aid the claim, I do like the work and think this is a very interesting direction of research and will be appreciated by the community."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4g0gRzvGKB", "forum": "fU2R326Mqp", "replyto": "fU2R326Mqp", "signatures": ["ICLR.cc/2026/Conference/Submission23567/Reviewer_P58N"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23567/Reviewer_P58N"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944506506, "cdate": 1761944506506, "tmdate": 1763544372661, "mdate": 1763544372661, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces three novel mechanisms—Proximal Forgetfulness (PFG), Fuzzy Recall (FZR), and Forgiveness—that aim to make transformer models more interpretable by restructuring their internal topology. Applied to mT5 and RWKV models, these mechanisms spatially cluster weights and activations while maintaining pretrained performance. The paper demonstrates that the resulting models exhibit significantly enhanced robustness to noise and perturbations, and that token pathways can be represented using spatial centers and magnitudes without sparse autoencoders."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper takes a refreshing perspective by modifying network structure to be inherently more interpretable, rather than post-hoc analysis. \n2. The observation that conceptual groupings persist after FZR is removed (Section 3.1.2, Table 2) is particularly interesting and could enable pretrained interpretable models for downstream tasks.\n3. Testing on both mT5 and RWKV architectures strengthens the generalizability claims."}, "weaknesses": {"value": "1. The three mechanisms are interdependent, making it difficult to understand individual contributions. \n2. The RWKV experiments in Section 4.4 provide an initial indication of cross-architecture generalization. It might be helpful to expand these results, for example by including additional details or analyses in the appendix."}, "questions": {"value": "1. Have the authors considered exploring how the spatial clusters and token signatures could complement existing interpretability methods or provide concrete qualitative insights?\n2. Since TLF operates at the token level but the two-letter criterion is defined at the character level, could this mismatch lead to arbitrary or inconsistent training signals?\n3. What are the actual task performance metrics (BLEU, accuracy, etc.) comparing baseline and modified models across multiple tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2emQwKObcA", "forum": "fU2R326Mqp", "replyto": "fU2R326Mqp", "signatures": ["ICLR.cc/2026/Conference/Submission23567/Reviewer_N1yq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23567/Reviewer_N1yq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761956924154, "cdate": 1761956924154, "tmdate": 1762942713982, "mdate": 1762942713982, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel approach for inducing structured, spatially smooth internal topologies in language models, inspired by cortical map formation and mechanistic interpretability. Rather than analyzing networks post-hoc, the authors modify training dynamics so an interpretable structure emerges intrinsically through Proximal Forgetfulness, Forgiveness, and Fuzzy Recall. The empirical results show striking changes in geometric organization and robustness, although evidence for semantic interpretability and concept-aligned modularity remains preliminary. Overall, the paper presents a compelling conceptual direction and meaningful representational effects, but broader evaluation and semantic validation would strengthen the interpretability claims."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper proposes mechanisms encouraging locally coherent activation neighborhoods, stable token pathways, and modular feature organization analogous to cortical maps:\n(a) proximal weight smoothing for spatial clustering,\n(b) a “forgiveness” objective to promote smooth transitions and stabilize intermediate states, and\n(c) soft activation grouping to form band-like ensemble codes.\n\n2. Strong evidence of emergent topology is provided: near-perfect Moran’s I (~0.96), smooth weight map visualizations, and coherent activation bands.\n\n3. Robustness experiments show large gains (approximately 3–10× noise tolerance), indicating the induced structure has computational consequences rather than being cosmetic.\n\n4. The appearance of hub-like routing points and band-ensemble patterns is intriguing, suggesting potential analogies to population coding and specialization in the cortex.\n\n5. The work reframes interpretability as a potentially beneficial inductive bias rather than solely a diagnostic tool, raising the hypothesis that cortical maps may arise partly from computational advantages.\n\n6. This is one of the few attempts to deliberately architect topographic priors into a language model and directly visualize the resulting structure."}, "weaknesses": {"value": "1. Although geometric structure is clearly induced, the paper does not yet provide strong evidence that the structure is semantically meaningful or aligns with human-interpretable concepts.\n\n2. Interpretability claims rely on limited qualitative examples; systematic semantic evaluation (e.g., cluster purity, RSA, probing, causal ablations) is missing.\n\n3. The results show smoothness and robustness, but not clear semantic modularity or token-specialized circuits.\n\n4. Additional visualization of semantic embedding space or functional activation clusters would help substantiate interpretability claims.\n\n5. The link to cognitive meaning and concept-level organization remains more speculative than empirically demonstrated at this stage."}, "questions": {"value": "1. Beyond robustness, can the authors directly evaluate whether the induced structure improves abstraction, transfer, memory stability, or controllability?\n\n2. How does this approach relate to classical self-organizing maps and to Yamins’ TDANN framework? Are there shared principles or identifiable differences?\n\n3. Could this method be used to model or test hypotheses about cortical map development (e.g., in V1, V4, IT)? What biological predictions follow from this framework?\n\n4. If cortical maps are not optimized for human interpretability, what computational pressures underlie their emergence? Can the authors identify and measure such pressures in this model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "DEPS7x3Kg3", "forum": "fU2R326Mqp", "replyto": "fU2R326Mqp", "signatures": ["ICLR.cc/2026/Conference/Submission23567/Reviewer_PakW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23567/Reviewer_PakW"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975124301, "cdate": 1761975124301, "tmdate": 1762942713682, "mdate": 1762942713682, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces three simple training tweaks: Proximal Forgetfulness, Fuzzy Recall, and Forgiveness. These make weights more clustered and more robust to perturbations, without task degradation. The paper tests this on mT5 and RMKV and show visualisations of the emergence of these clusters."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "This is an original and composable recipe for creating structure in model activations.\nThe effect is reproduced across two architectures\nThe signatures demonstrated could provide a complementary approach to interpretability to SAE-based methods."}, "weaknesses": {"value": "The lack of degradation is not adequately supported by a clear set of benchmarks.\nThere is not clear comparison to strong interpretability baselines, and the token signatures are not validated that way.\nThe perturbations are synthetic, but it is less clear if this generalises to real distribution shifts."}, "questions": {"value": "Do the clusters let you steer outputs with local interventions?\nHow is the teacher model set up and does its biases affect the student?\nHow different are real world shifts relative to multiplicative noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eFLE910Pb2", "forum": "fU2R326Mqp", "replyto": "fU2R326Mqp", "signatures": ["ICLR.cc/2026/Conference/Submission23567/Reviewer_HLZ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23567/Reviewer_HLZ8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23567/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987313826, "cdate": 1761987313826, "tmdate": 1762942713259, "mdate": 1762942713259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}