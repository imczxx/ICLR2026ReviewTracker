{"id": "W7sVYFJAEp", "number": 3282, "cdate": 1757393233011, "mdate": 1759898097895, "content": {"title": "Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism", "abstract": "Transformer-based large language models (LLMs) have achieved remarkable success, yet their standard attention mechanism incurs quadratic computation and memory costs with respect to sequence length, posing a major bottleneck for long-context training. Prior work tackles this challenge along two directions: (1) kernel-level optimizations, which accelerate dense and sparse attention operators; and (2) module-level strategies, often referred to as distributed attention or context parallel training, which scale attention across multiple devices. However, systematic evaluation still remains limited: operator-level comparisons are often incomplete, while context parallel strategies are typically framework-specific, with unclear performance analysis across contexts. To address these gaps, we propose a unified benchmark that integrates representative attention kernels and context parallel mechanisms with a modular and extensible interface for evaluation. The benchmark evaluates methods along two critical dimensions: (1) attention mask patterns, which strongly affect efficiency, scalability, and usability, and (2) sequence length and distributed scale, which determine performance under extreme long-context training. Through comprehensive experiments on the cluster of up to 96 GPUs, our benchmark enables reproducible comparisons, highlights method-specific trade-offs, and provides practical guidance for designing and deploying attention mechanisms in long-context LLM training.", "tldr": "We build a unified benchmark to evaluate attention kernels and context parallel mechanisms for ultra-long sequence training, highlighting efficiency, scalability, and trade-offs.", "keywords": ["Long Context", "Dense Attention Kernel", "Sparse Attention Kernel", "Context Parallel Machenism", "Mask Pattern"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6a81137e71986875e80d22a9baba8963ffe8605c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents LongCA-Bench, a unified benchmark framework designed to evaluate long-context attention mechanisms across both single-device and distributed settings. The author integrates existing attention kernels (dense and sparse) and distributed context-parallel strategies into a modular, extensible evaluation platform. Through extensive experiments on up to 96 GPUs, the benchmark compares efficiency, scalability, and memory usage across diverse mask patterns, sequence lengths, and distributed configurations of existing methods. The work provides a valuable empirical reference for practitioners and researchers working on long-context training but does not seem to introduce novel attention algorithms or distributed mechanisms beyond benchmarking existing approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Comprehensive benchmarking.** The paper systematically benchmarks a wide range of attention implementations, including dense, sparse, and distributed mechanisms, under a unified framework. The experimental coverage (up to 96 GPUs and multiple mask types) is extensive and provides a clear view of current attention efficiency trends.\n2. **Sound experimental methodology.** The experiments are well-organized, use realistic settings (e.g., long context lengths, different mask patterns), and report consistent metrics (throughput, memory). The methodology is reproducible and technically competent.\n3. **Useful empirical reference.** The benchmark results could serve as a useful reference for practitioners and researchers seeking guidance on the performance trade-offs of existing attention kernels and distributed strategies."}, "weaknesses": {"value": "1. **Limited Analysis.** The paper reports extensive throughput and memory results, but offers limited discussion on the underlying causes of observed performance trends of the benchmarked methods.\n2. **Incomplete coverage of the most critical setting — distributed sparse attention.** The integration of sparse attention (particularly dynamic block-sparse attention with TopK/TopP selection criterion) into distributed contexts remains an unexplored and practically important challenge. The paper does not explore this aspect (does not touch block sparse distributed attention, only discusses full/causal/document), which somewhat limits its impact given the stated motivation of benchmarking “long-context attention.”"}, "questions": {"value": "1. How can the proposed framework offer insight on sparse attention in distributed settings? Typically, how should we overlap communication with computation, and address the load balancing problem of arbitrary block sparse pattern? The author does not need to provide a complete solution to this problem, and what I would like to see is how this work could contribute to future research on these challenges.\n2. Could the authors provide more analysis to understand the trends observed in kernel or distributed scaling performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qho1PNbvpW", "forum": "W7sVYFJAEp", "replyto": "W7sVYFJAEp", "signatures": ["ICLR.cc/2026/Conference/Submission3282/Reviewer_1Et8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3282/Reviewer_1Et8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760948418416, "cdate": 1760948418416, "tmdate": 1762916645503, "mdate": 1762916645503, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified benchmark framework for evaluating various dense/sparse attention kernels and context-parallel mechanisms in long-sequence scenarios (up to 512K tokens). The framework provides standardized assessments of computational efficiency (in terms of throughput and peak memory usage), scalability, and usability. The evaluation focuses on two main dimensions: (1) attention mask patterns, and (2) sequence length and scale of distributed computation. Firstly, the study performs unified data preprocessing for different attention mask patterns to ensure fair comparisons. Next, it integrates over a dozen attention kernels and incorporates three distributed mechanisms: All-to-All, Ring P2P, and Hybrid. Through these evaluations, the paper draws several insightful conclusions about the computational efficiency pros and cons of various dense and sparse attention kernel implementations. This lays a solid foundation for future research to weigh different backend implementations, explore directions for kernel optimization, further improve kernel implementations, and perform fair comparisons with existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) Extensive Method Integration: This work uses a unified interface to integrate 12 representative attention kernels and 5 distributed mechanisms.\n(2) Good Scalability: The evaluation is conducted on scenarios with sequence lengths up to 512K and across 96 GPUs.\n(3) Practical Insights: Through experimental evaluation, the authors obtain insightful conclusions regarding the impact of mask patterns, the trade-offs between kernel efficiency and usability, and the scalability characteristics of different distributed mechanisms."}, "weaknesses": {"value": "(1) Architectural Limitation: The study is limited to the Hopper architecture and does not discuss the generalization of experimental conclusions to other architectures.  \n\n(2) Performance Metric Limitation: The research only focuses on throughput and memory usage as performance metrics. It does not analyze how metrics such as memory bandwidth utilization and inter-node communication load vary over time across different kernels and distributed mechanisms."}, "questions": {"value": "Please provide at lease some convincing explanations for the two points mentioned in the Weaknesses section. I am willing to raise my score if my concerns are addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qSuIuiC9HS", "forum": "W7sVYFJAEp", "replyto": "W7sVYFJAEp", "signatures": ["ICLR.cc/2026/Conference/Submission3282/Reviewer_Ehzx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3282/Reviewer_Ehzx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925380349, "cdate": 1761925380349, "tmdate": 1762916645250, "mdate": 1762916645250, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes LongCA-bench, a unified benchmark for evaluating long-context attention mechanisms in large language model (LLM) training, covering both single-device kernels (dense and sparse) and distributed context parallel strategies. The benchmark addresses critical gaps in existing evaluations—such as incomplete operator comparisons and framework-specific context parallel designs—by integrating 12 dense/sparse kernels, 5 distributed attention mechanisms, and 14 attention mask patterns. It conducts large-scale experiments on up to 96 NVIDIA H100 GPUs, evaluating performance across sequence lengths (up to 512K tokens) and distributed scales. Key findings include hardware-optimized kernels (e.g., FlashAttention-3) outperforming general ones in regular masks, sparse kernels facing limitations in backward computation and flexibility, and hybrid distributed designs (USP, LoongTrain) balancing scalability and efficiency. The work provides actionable guidance for selecting attention mechanisms in ultra-long context training."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. LongCA-bench unifies diverse attention kernels (dense/sparse) and distributed mechanisms under a modular interface, enabling fair cross-method comparisons—addressing the fragmentation of existing evaluations.\n2. It systematically explores two understudied but impactful dimensions: 14 attention mask patterns (static/dynamic, regular/heterogeneous) and extreme long sequences (up to 512K) with large-scale distributed training (up to 96 GPUs), filling gaps in prior work.\n3. The benchmark uses real-world datasets (Pile, ProLong64K/512K) and realistic sampling strategies, ensuring results reflect actual LLM training scenarios. It also provides open-source code for reproducibility.\n4. Beyond performance metrics (TFLOPs, peak memory), the paper reveals trade-offs (e.g., hardware optimization vs. mask compatibility, computation-communication overlap in distributed systems) and identifies bottlenecks (e.g., sparse kernel backward pass inefficiency)."}, "weaknesses": {"value": "0. Why no linear attention kernels?\n1. The optimized distributed attention mechanisms only support 4 mask patterns (FULL, CAUSAL, FULL/CAUSAL DOCUMENT), excluding heterogeneous and dynamic masks—restricting its applicability to complex long-context tasks.\n2. The benchmark excludes FlexAttention from full evaluations due to severe out-of-memory issues, and most sparse kernels lack backward computation support or flexibility (e.g., fixed block sizes), limiting insights into trainable sparse attention.\n3. Key findings (e.g., FlashAttention-3’s superiority) are tailored to NVIDIA H100 GPUs, reducing generalizability to other hardware architectures (e.g., AMD GPUs, TPUs).\n4. Only 5 distributed strategies are evaluated, with Ring All-Gather’s results omitted due to resource constraints—missing opportunities to compare with emerging context parallel designs."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DJHjOmgWXT", "forum": "W7sVYFJAEp", "replyto": "W7sVYFJAEp", "signatures": ["ICLR.cc/2026/Conference/Submission3282/Reviewer_nrHk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3282/Reviewer_nrHk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3282/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993832535, "cdate": 1761993832535, "tmdate": 1762916644756, "mdate": 1762916644756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}