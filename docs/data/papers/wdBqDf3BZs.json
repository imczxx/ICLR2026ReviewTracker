{"id": "wdBqDf3BZs", "number": 7685, "cdate": 1758031899999, "mdate": 1759897839478, "content": {"title": "Stable Planning through Aligned Representations in Model-Based Reinforcement Learning", "abstract": "Integrating planning with reinforcement learning (RL) significantly improves problem-solving capabilities for sequential decision-making problems, particularly in sparse-reward, long-horizon tasks. Recently, it has been shown that discrete world models can be trained such that no model degradation occurs over thousands of time steps and states can be re-identified during planning. As a result, a heuristic function can be trained with data generated from the world model, and the learned world model and heuristic function can be used with planning to solve problems. However, this approach fails to solve problems with state transformations to which the world model and heuristic function should be invariant (i.e., noise), without re-training the world model and heuristic function. In this work, we introduce Stable Planning through Aligned Representations (SPAR), an efficient framework that trains a discrete world model and heuristic function in a clean Markov decision process (MDP) and trains an alignment network to map transformed states to their discrete latent state in the clean MDP. When solving problems, we exploit the underlying discrete latent representation and round the output of the alignment network in hopes that it matches the clean latent state exactly. As a result, adapting to transformations only requires training the adaptation network while the world model and heuristic function remain fixed. We then demonstrate its effectiveness on Rubik's Cube domain, and compare it with applying a similar approach to a world model with continuous latent representations. SPAR successfully solves over 89.39% of problems with 17 different visual transformations and real-world images. This adaptation process requires no additional world model or heuristic function re-training, and reduces re-training time by at least 95%.", "tldr": "SPAR is a framework that trains a discrete world model and heuristic function once in a clean environment, then efficiently adapts to visual transformations using only an alignment network, reducing adaptation time by 95%.", "keywords": ["Model-Based Reinforcement Learning", "Planning", "Reinforcement Learning", "Alignment Model", "Aligned Representation Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e6f5ff48a613e37e3d48aa067af3536fd4b6c1d1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes SPAR (Stable Planning through Aligned Representations) — a framework that improves robustness of model-based reinforcement learning (MBRL) under visual or environmental variations. SPAR first trains a discrete world model and a goal-conditioned heuristic function in a clean environment, then introduces a lightweight alignment network that maps visually transformed or noisy observations into the discrete latent space of the world model. This allows the system to perform long-horizon heuristic search without retraining the dynamics or heuristic components when visual changes occur."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "SPAR demonstrates strong performance on Rubik’s Cube and Sokoban domains, achieving >89% success under 17 different visual transformations and even transferring to real-world Rubik’s Cube images, all while reducing retraining time by ≥95% compared to retraining the full world model or heuristic."}, "weaknesses": {"value": "1. The alignment model requires paired variant–base observations, which may be unrealistic in some real-world settings without synchronized data collection.\n2. SPAR’s planning framework is not yet extended to continuous-control domains.\n3. The results primarily report quantitative success rates and MSE; visualizations of latent alignment quality or semantic consistency are limited."}, "questions": {"value": "More discussions about Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "djm83dEVL5", "forum": "wdBqDf3BZs", "replyto": "wdBqDf3BZs", "signatures": ["ICLR.cc/2026/Conference/Submission7685/Reviewer_MDoU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7685/Reviewer_MDoU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761844259846, "cdate": 1761844259846, "tmdate": 1762919745107, "mdate": 1762919745107, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces SPAR: train a discrete world model + goal-conditioned heuristic on a clean MDP; then learn a lightweight alignment network that maps transformed observations into the same binary latent so planning can use bitwise equality with rounding. Claims: solves >90% of Rubik’s Cube instances under 17 visual transformations and handles real photos; adaptation requires training only the alignment network and “reduces re-training time by at least 95%.” SPAR uses Q* search with exact latent goal tests; planning results are reported in Table 1 (clean/augmented/real). \n\nKey empirical findings: Rubik’s augmented success 89.39%; real-world success 50%; DeepCubeAI and greedy 0% off-domain; )  Stability: over 10k steps, discrete latents remain accurate while continuous drift, across 17 perturbations.  The alignment model is trained with paired (variant, base) frames at the same timestep, minimizing MSE to the rounded discrete codes; inference rounds A(s̄)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Simple, modular adapter: keep (E,T,D, heuristic) fixed; adapt perception via A + rounding bottleneck for exact state re-ID during search. \n\nLong-horizon stability evidence: discrete vs continuous 10k-step rollouts + MSE curves convincingly show drift in continuous latents and stability in discrete. \n\nReal-image demo: proof-of-concept transfer to photos (50% success) beyond synthetic perturbations.\n\nSimple and Elegant Solution: The core idea of decoupling the dynamics/planning module from the perception module is intuitive. Using a lightweight, separate alignment network to map new observations to a fixed, stable latent space is a clean and simple approach."}, "weaknesses": {"value": "Limited baselines: The paper does not compare against natural alternatives such as: training the encoder/world model with standard strong augmentations/domain randomization and then planning (how far does that go?),\n\nPaired data assumption: Training the alignment network requires paired variant/base frames at the same timestep. That’s realistic in sim, but non-trivial in many real settings—precisely where adaptation matters most. The paper acknowledges the need for pairs but does not explore unpaired/weakly-paired regimes or data-efficiency curves.\n\nTheory: This is primarily empirical/engineering; there are no formal guarantees (e.g., on rounding margins, alignment error bounds vs search completeness). That’s fine in principle, but then the empirical section should be more comprehensive on ablations/baselines.\n\nVague statement and lack citations: \"Although, deep learning techniques have shown promise in this area, they need to meet certain requirements to be usable for learning a heuristic function.\". Please cite the techniques. Also please explain the certain requirements.\n\nUnclear definition of the “noise” and the “noise transformation module” (Figure 1) in the introduction: The intro refers to Figure 1 but never formalizes the observation domains or the transformation. It’s ambiguous whether “noise” is (i) photometric (blur, brightness), (ii) geometric (crop, rotate), (iii) compositional (occlusion, background), or all of the above; whether transforms are deterministic or sampled from a distribution; whether compositions of transforms occur; and how this “noise” operator interfaces with the alignment module. It would be better to give explanation in the introduction."}, "questions": {"value": "Baselines: How does SPAR compare against training the encoder/world model with strong visual augmentations and then planning (i.e., no alignment adapter)? Please include success rates and compute. (This is the most important missing baseline.)\n\nOn the Paired Data Assumption: Could you clarify how you see this method being applied in a real-world setting where paired \"clean\" states $s$ are not available for the \"noisy\" states $\\overline{s}$? Does this not limit the method to sim-to-sim adaptation, or do you have a path toward unsupervised alignment?\n\nOn Failure Cases: The method fails on ~11% of augmented data and 50% of real-world data. This implies the method is extremely brittle to small errors. Could you comment on this sensitivity? Is the planner's success entirely dependent on the alignment network achieving 100% bitwise accuracy?\n\nOn Equation 2 and Figure 1: Could you please provide the correct, complete definition for the transition loss in Equation 2? e.g. detach() Furthermore, could you provide a revised diagram that more clearly illustrates the data flow for both training the alignment network and for performing planning at inference time?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8lSdQK4S7a", "forum": "wdBqDf3BZs", "replyto": "wdBqDf3BZs", "signatures": ["ICLR.cc/2026/Conference/Submission7685/Reviewer_hqcv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7685/Reviewer_hqcv"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953655309, "cdate": 1761953655309, "tmdate": 1762919744518, "mdate": 1762919744518, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper extends DeepCubeAI [Agostinelli et al., 2025] to handle distorted observations by learning an alignment model that maps augmented images to representations of their ground truth versions. This aims to make model-based search methods applicable to more realistic settings. However, while the motivation is strong, the empirical evaluation is limited and the presentation lacks clarity in several places."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a relevant and challenging problem: enabling model-based planning methods like DeepCubeAI to operate directly on inputs closer to real-life images.\n\n2. Evaluation on the two included environments is reasonable."}, "weaknesses": {"value": "1. The proposed discrete world model may not generalize to continuous environments such as robotics, limiting its practical scope.\n\n2. The evaluation is limited to only two environments (Rubik’s Cube, Sokoban), with only Sokoban being long-horizon.\n\n3. More diverse tasks (continuous or non-symbolic domains) would strengthen the paper.\n\n4. Training the alignment model requires access to clean, undistorted data. This is not a realistic assumption.\n\n5. Figures 3 and 4 are not well explained; it’s not obvious what new insight they add beyond prior work.\n\n6. Line 54 seems to contain a typo.\n\n7. Lines 44-48 could be rewritten to be clearer.\n\n8. Lack of related work, for instance Model-based visual planning with self-supervised functional distances [Tian et al., ICLR 2021], which seems directly relevant.\n\n9. The symbols in Equation 2 make it harder to read it."}, "questions": {"value": "1. How robust is this approach to unseen augmentations at test time?\n\n2. Should the “Rollout stability and reconstruction” paragraph point to a figure?\n\n3. The latent space is 400 dimensional. How many of those dimensions are actually used by the encoder model?\n\n4. Line 278: “However, it is possible to use a new dataset that includes visual variations and clean images that are not present in the original dataset.” – What is the difference in performance if you do that?\n\n5. How would using a discrete world model work if used in an environments that do not have a discrete state space such as for instance robotics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MI2SFLFAvw", "forum": "wdBqDf3BZs", "replyto": "wdBqDf3BZs", "signatures": ["ICLR.cc/2026/Conference/Submission7685/Reviewer_pAHV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7685/Reviewer_pAHV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762548174576, "cdate": 1762548174576, "tmdate": 1762919744214, "mdate": 1762919744214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SPAR is a method that learns how to make \"pathfinding via learning discrete latent world models and using DQN\"(DeepCubeAI) robust. It incrementally builds on the concept of DeepCubeAI that learn a disceret latent world model where every observation is encoded to an exact discrte latent state. SPAR's main contribution is to train an alignment model that maps noisy version of an observation (some jitter, orientation, noisy addition) to the exact latent state of its clean version. Then the alignment model can be used with a large scale DeepCubeAI setup."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The alignment model can be trained fast since it is a \"smaller model\" and can be adapted to large-scale pretrained DeepCubeAI setup, making it decoupled and more robust to noisy observations.\n\nThe alignment model helps DeepCubeAI scale to real-world."}, "weaknesses": {"value": "Novelty: The main concern I have with the paper is novelty. Based on my understanding, the only novel idea in this paper is \"mapping noisy observations to the same latent state of clean observations via MSE\" to reuse the large-model. The paper follows the exact same protocol as DeepCubeAI for all other things, including experimental setup.\n\nExperiment exhaustiveness: The paper shows results only for the rubics cube solving task while the augmentations in fig 2 is shown for both cube and sokoban. The experimental results in table 1 are also pretty obvious that vanilla DeepCubeAI won't work in the presence of augmentations since it was not in training data. \n\nData requirements: The paper mentions 10000 episodes (each with 30 steps) per augmentation (around 30) to train the alignment model. This challenges the scalability of the method. I believe for a task like rubik's cube, it is very important to understand each state (which can be a lot) and also consider \"all\" augmentations of each state (and that is the reason behind such a large number of datapoints)."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "khjKdkeohy", "forum": "wdBqDf3BZs", "replyto": "wdBqDf3BZs", "signatures": ["ICLR.cc/2026/Conference/Submission7685/Reviewer_8NTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7685/Reviewer_8NTx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7685/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762679924194, "cdate": 1762679924194, "tmdate": 1762919743881, "mdate": 1762919743881, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}