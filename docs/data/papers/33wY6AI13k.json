{"id": "33wY6AI13k", "number": 8846, "cdate": 1758099825148, "mdate": 1763723476237, "content": {"title": "The  Price of Amortized inference in Sparse Autoencoders", "abstract": "Polysemy has long been a major challenge in Mechanistic Interpretability (MI), with Sparse Autoencoders (SAEs) emerging as a promising solution. SAEs employ a shared encoder to map inputs to sparse codes, thereby amortizing inference costs across all instances. However, this parameter-sharing paradigm inherently conflicts with the MI community's emphasis on instance-level optimality, including the consistency and stitchability of monosemantic features. We first reveal the trade-off relationships among various pathological phenomena, including feature absorption, feature splitting, dead latents, and dense latents under global reconstruction-sparsity constraints from the perspective of training dynamics, finding that increased sparsity typically exacerbates multiple pathological phenomena, and attribute this trade-off relationship to amortized inference. By reducing reliance on amortized inference through the introduction of semi-amortized and non-amortized approaches, we observed that various pathological indicators were significantly mitigated, thereby validating our hypothesis. As the first step in this direction, we propose Local Amortized SAE (LocA-SAE), a method that groups polysemantically close latents based on the angular variance. This method is designed to balance the computational cost of per-sample optimization with the limitations of amortized inference. Our work provides insights for understanding SAEs and advocates for a paradigm shift in future research on polysemy disentanglement. The code is available at \\url{https://anonymous.4open.science/r/sae-amortization-5335}.", "tldr": "Amortized encoding's global optimality conflicts with monosemantic instance-level optimality. We advocate reducing investment in purely amortization-based methods.", "keywords": ["Mechanistic Interpretability", "Polysemanticity", "Sparse Autoencoders", "Amortization Inference"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/01dac6edc15ba823e3fda88b8f8e406c89631754.pdf", "supplementary_material": "/attachment/fa1a9762db29292da5c64294ff2a9c89d26715d8.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates fundamental limitations of Sparse Autoencoders (SAEs) for mechanistic interpretability, arguing that the amortized inference paradigm conflicts with the instance-level optimality required for monosemantic features. The authors demonstrate through training dynamics analysis that pathological phenomena (feature absorption, splitting, dead/dense latents) are interconnected trade-offs arising from parameter sharing under global reconstruction-sparsity constraints. They propose semi-amortized and non-amortized encoding methods as alternatives, showing improved reconstruction, reduced dead latents, and better performance on downstream intervention tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper articulates a fundamental issue that has been under-recognized in the SAE literature - that global optimization objectives may be misaligned with the goals of interpretability\n2. The training dynamics experiments across multiple checkpoints, sparsity levels, and SAE variants provide robust evidence for the claims\n3. The amortization gap perspective from variational inference provides a principled lens for understanding SAE limitations\n4. The downstream task experiments (TPP, GIS) demonstrate that the improvements translate to better interpretability and controllability"}, "weaknesses": {"value": "1. The computational cost of non-amortized inference (200 ISTA iterations × N samples) is mentioned as a limitation but needs more serious treatment. For large-scale interpretability applications, this may be prohibitive. The paper should provide concrete runtime comparisons and discuss when the trade-off is worthwhile.\n\n2. While the paper shows that pathological phenomena are correlated, it doesn't fully explain why instance-level optimality necessarily leads to better monosemanticity. The connection between reconstruction quality and semantic purity is assumed rather than proven.\n3. Semi-amortized inference shows promise but receives less attention than it deserves. More systematic exploration of the trade-off between number of refinement steps and performance could be valuable.\n4. The paper acknowledges some anomalous results (e.g., TopK with Gemma showing worse absorption in non-amortized setting) but doesn't investigate them thoroughly. This suggests the full picture may be more complex."}, "questions": {"value": "1. Can you provide theoretical results or additional empirical evidence connecting instance-level reconstruction optimality to monosemanticity? What properties of the data distribution determine whether this connection holds?\n2. For the outlier cases (e.g., Gemma-2-2b/L12 TopK absorption), can you provide analysis of what's happening? Does this suggest limitations of the non-amortized approach?\n3. Have you explored whether better training procedures (e.g., curriculum learning, better initialization) for amortized SAEs could narrow the gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "R4PLUJ8IME", "forum": "33wY6AI13k", "replyto": "33wY6AI13k", "signatures": ["ICLR.cc/2026/Conference/Submission8846/Reviewer_gX9Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8846/Reviewer_gX9Z"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761783195356, "cdate": 1761783195356, "tmdate": 1762920611964, "mdate": 1762920611964, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that amortized inference, central to SAE, conflicts with mechanistic interpretability goals. Amortization trades instance-level optimality for global efficiency, creating an amortization gap. This gap causes pathological effects that degrade monosemantic feature discovery. Through theoretical and empirical analysis, the authors show that increasing sparsity worsens these issues and that optimizing reconstruction-sparsity tradeoffs doesn’t improve interpretability. They test semi- and non-amortized methods, finding that these reduce reconstruction errors, dead latents, and improve interpretability and control. The conclusion urges a shift away from full amortization toward hybrid or per-instance inference."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear articulation of the conceptual tension between global efficiency and instance-level fidelity\n\n2. Novel experimental demonstration of semi-/non-amortized methods that improve feature interpretability.\n\n3. Situates itself well in current mechanistic interpretability discourse (SAEBench, monosemantic features)."}, "weaknesses": {"value": "1. The paper argues strongly against amortization but doesn’t fully explore potential middle grounds such as localized amortization, mixture-of-experts encoders, or hierarchical amortization. This makes the recommendation (“reduce investment in amortization”) feel a bit absolute.\n\n2. The broader meaning of “instance-level optimality” for interpretability is underexplored. For example, how does this affect feature alignment across runs, or practical model auditing? The paper hints at these but doesn’t go deep.\n\n3. While the paper demonstrates empirical correlations (e.g., lower amortization gap -> lower NMSE, fewer dead latents), it doesn’t provide causal interventions or ablations proving amortization itself causes the pathologies, rather than merely correlating with them."}, "questions": {"value": "1. Can amortization be regularized rather than abandoned, e.g., local amortization?\n\n2. How does the amortization gap quantitatively relate to interpretability scores beyond NMSE and sparsity metrics?\n\n3. Could amortization’s failure modes differ across modalities (e.g., vision vs language)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vhoKNFcaSL", "forum": "33wY6AI13k", "replyto": "33wY6AI13k", "signatures": ["ICLR.cc/2026/Conference/Submission8846/Reviewer_KjC9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8846/Reviewer_KjC9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761858268749, "cdate": 1761858268749, "tmdate": 1762920611613, "mdate": 1762920611613, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that the amortized inference framework, standard in Sparse Autoencoders (SAEs), is fundamentally misaligned with the goal of discovering monosemantic features for mechanistic interpretability. The authors state that the global optimization objective of a shared encoder inherently conflicts with the need for instance-level optimality, leading to pathological behaviors like feature splitting and absorption. They provide empirical evidence showing that moving towards instance-specific, non-amortized optimization significantly improves reconstruction and mitigates these issues, albeit at a high computational cost. Based on this, the authors advocate for a paradigm shift away from purely amortized methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper clearly articulates a critical, and perhaps under-appreciated, conflict in the SAE literature: the trade-off between the computational efficiency of amortized inference and the instance-level fidelity required for true monosemanticity. This reframing of common SAE pathologies as symptoms of an inference problem, rather than just an architectural one, is a valuable conceptual contribution.\n\n2. The core experiment comparing fully-amortized, semi-amortized, and non-amortized inference using the same trained decoder is convincing. It effectively isolates the inference strategy as the key variable, providing direct and convincing evidence that the amortization gap is a significant source of suboptimality and pathological behavior in SAEs"}, "weaknesses": {"value": "1. The paper's main weakness is that it diagnoses a problem without offering a viable solution. The proposed non-amortized alternative, which relies on per-sample iterative optimization (e.g., 200 ISTA steps), is computationally intractable for the very large-scale applications where SAEs are most needed. To make the call for a paradigm shift convincing, the authors should propose and evaluate at least one hybrid approach that balances instance-level fidelity with computational feasibility.\n\n2. The central concept, the \"amortization gap,\" is a well-established phenomenon extensively studied in the context of Variational Autoencoders and recent work on SAEs (e.g., O'Neill et al. 2024), which substantially detracts from the informativeness of this work. It would be great if the authors could make the ``delta'' more transparent."}, "questions": {"value": "Please see the weaknesses section.\n\nIn addition, the non-amortized approach appears to worsen some pathologies (feature splitting and absorption) for TopK SAEs on Gemma-2-2b. How do you reconcile this with your central thesis that amortization is the primary cause of these problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "xbiT3po4mI", "forum": "33wY6AI13k", "replyto": "33wY6AI13k", "signatures": ["ICLR.cc/2026/Conference/Submission8846/Reviewer_S8RY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8846/Reviewer_S8RY"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762004297396, "cdate": 1762004297396, "tmdate": 1762920611207, "mdate": 1762920611207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the potentially overlooked interrelation between mono-semanticity and reconstruction-sparsity tradeoff in the SAE context. Specifically, the (position) paper presents empirical findings including (1) improvement over the reconstruction-sparsity Pareto frontier comes at the expense of dictionary capacity and mono-semanticity, and (2) semi-/non- amortized encoding approaches show benefits in improving reconstruction and features (e.g., concept removal tasks) and in addressing dead latents issue. The paper argues that the conflict between parameter-sharing encoding and instance-level optimality stems from the SAE architecture itself, and cautions against the investment in amortization-based encoding for polysemy disentanglement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Writing\n\nOverall, the paper is relatively easy to follow. The problem setting of interest, the previous approaches and how they fit in the discussion, the purpose of empirical evaluations, the settings of experiments and findings are presented in an organized and clear way.\n\n### Significance\n\nThe paper (or position paper?) aims to present a case/caution against the investment in amortization-based encoding, if the goal includes polysemy disentanglement. This is carried out relatively well, since the arguments are well-structured along with empirical evidence."}, "weaknesses": {"value": "### Novelty\n\nThe paper strikes me more of a position paper than a regular conference paper (just stating this as a neutral comment). There are potential some concerns on the novelty (detailed in \"Questions\" section).\n\n### Minor Things\n\n- please fix the opening quotation marks in lines 160, 192, 243"}, "questions": {"value": "- Question/Comment on novelty\n\nThe paper lays out issues (\"pathological phenomena\") identified in previous works, e.g., dead latents (Gao et al., 2024), dense latents (Sun et al., 2025), feature splitting and feature absorption (Chanin et al. 2024), and also reproduces some exps from Karvonen et al. (2025). It would be very helpful if the authors can be more explicit about the unique contribution to address the potential concern on novelty."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kYBAhyMg1X", "forum": "33wY6AI13k", "replyto": "33wY6AI13k", "signatures": ["ICLR.cc/2026/Conference/Submission8846/Reviewer_Ycij"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8846/Reviewer_Ycij"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8846/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762051889508, "cdate": 1762051889508, "tmdate": 1762920610679, "mdate": 1762920610679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}