{"id": "eIMXY9qkKz", "number": 15820, "cdate": 1758255676772, "mdate": 1759897279753, "content": {"title": "FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments", "abstract": "Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters.\nTo address these limitations, FEDDAG introduces a clustered FL framework, FEDDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FEDDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization.\nExperiments on diverse benchmarks and data heterogeneity settings show that FEDDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy.", "tldr": "We introduce FEDDAG, a clustered FL approach that tackles data heterogeneity by combining data and gradient similarity for improved client clustering, and employs a dual-encoder architecture to enable representation sharing across clusters.", "keywords": ["Federated Learning", "Clustering", "Distributed Machine Learning"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/044c12c19733d100a5026ce3789aa63572926832.pdf", "supplementary_material": "/attachment/347271c6bc3041eb7a7955a2569eca3d7ebc26fe.zip"}, "replies": [{"content": {"summary": {"value": "This work proposes a novel clustering method for federated learning, FedDAG. FedDAG combines data and gradient information to cluster clients more effectively. FEDDAG leverages cross-cluster representation sharing and incorporates an efficient mechanism to automatically determine the optimal number of clusters. Experiments are conducted to evaluate the performance of the proposed mechanism."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. The structure of this paper is clear and reasonable.\n\n2. The paper is easy to follow.\n\n3. Experiments show that the performance of the proposed method is significantly better than existing methods in different scenarios."}, "weaknesses": {"value": "1. The literature review section should be expanded to include more complete related work and discuss several related subtopics.\n\n2. Directly reporting the class sample count will leak sensitive sample distribution information.\n\n3. The novelty of the dual encoder structure is limited, similar to the personalized/shared head of pFL works and the personalized/shared feature extractor of feature-skewed FL works.\n\n4. Although the author claims that FEDDAG is the only model that can solve all four data heterogeneity problems, the paper lacks experiments on feature distribution shifts. Commonly used datasets like Office-Caltech-10, PACS, and DomainNet should be studied.\n\n5. The baseline is mainly compared with clustered FL methods. It should also be compared with FL methods that address different heterogeneity issues.\n\n6. The layout of the paper can be improved. For example, the experimental results in Table 2 and Figure 1 are too far away from the experimental section; the method diagram in Figure 2 could be placed in the main text."}, "questions": {"value": "See weaknesses. I would like to change my score if the authors could address the concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2425f7ra8S", "forum": "eIMXY9qkKz", "replyto": "eIMXY9qkKz", "signatures": ["ICLR.cc/2026/Conference/Submission15820/Reviewer_8htn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15820/Reviewer_8htn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761422098691, "cdate": 1761422098691, "tmdate": 1762926048967, "mdate": 1762926048967, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes FedDAG, a novel clustered Federated Learning (FL) algorithm designed to handle diverse types of data heterogeneity, including label skew, feature skew, concept shift, and quantity shift. The core contributions are threefold: (1) a client similarity metric that fuses both data-based (via a weighted, class-wise principal angle comparison) and gradient-based information, with client-specific weights learned via an entropy loss; (2) a dual-encoder architecture that enables cross-cluster representation sharing to enrich feature learning without sacrificing cluster specialization; and (3) an adaptive clustering mechanism that automatically determines the optimal number of clusters using a novel federated-aware metric that penalizes over-splitting. Extensive experiments on multiple datasets and non-IID settings demonstrate that FedDAG consistently outperforms a wide range of state-of-the-art clustered and personalized FL baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Comprehensive Problem Formulation: The paper does an excellent job of identifying and articulating the key limitations of existing clustered FL methods, such as reliance on a single similarity modality (data or gradients), restricted intra-cluster knowledge sharing, and inability to handle all forms of data skew. The motivation is clear and well-justified.\n\n- Novelty and Technical Sophistication: The proposed method is technically sound and introduces several novel ideas. The fusion of data and gradient similarities is not merely a weighted average but involves a learned, client-specific weighting scheme. The class-wise, weighted data similarity is a clear and meaningful improvement over prior work like PACFL. The dual-encoder architecture for cross-cluster sharing is a creative and well-motivated solution to the problem of isolated clusters.\n\n- Thorough Empirical Evaluation: The experimental section is a major strength. The authors evaluate FedDAG across four different non-IID data distributions, encompassing all the heterogeneity types they claim to address. The comparison against a large set of strong baselines is comprehensive. The inclusion of ablation studies (FedDAG*, FedDAG+) effectively isolates the contribution of different components (clustering vs. sharing, and sharing vs. parameter increase).\n\n- Practicality and Completeness: The paper thoughtfully addresses practical concerns such as communication/computation complexity, privacy implications, convergence (with a provided theorem), and dynamic scenarios like newcomer integration and data distribution shift. This makes the work feel mature and applicable to real-world FL systems."}, "weaknesses": {"value": "- Computational and Architectural Overhead: The dual-encoder architecture inherently doubles the parameter count for the feature extractor compared to a single-model approach. While the ablation study (FedDAG†) convincingly shows that the gains are due to sharing and not just more parameters, this overhead is non-trivial for resource-constrained edge devices. The paper mentions the possibility of alternating training phases to mitigate this, but a more detailed discussion on the trade-offs (e.g., how much it slows convergence) would be beneficial.\n\n- Clustering Stability and Cost: The clustering mechanism, while adaptive, is performed initially and then assumed to be stable (Assumption A.4 in the convergence analysis). The method for handling distribution shift (Appendix A.8) is reactive and requires clients to locally re-train, which incurs additional communication and computation cost. The overall one-time cost of the initial clustering phase (local warm-up, SVD, gradient sparsification) is non-negligible, though the paper argues it's a small fraction of total training. A more explicit comparison of this \"clustering overhead\" against other methods would be helpful.\n\n\n- Clarity of the CC-Graph and Sharing Mechanism: The process for building the Cluster Complementarity Graph (CC-Graph) and the subsequent secondary encoder training is complex. The description in Section 4, while detailed, is challenging to follow on a first read. A more intuitive explanation of the \"demand\" and \"supply\" scores, and a clearer step-by-step walkthrough of the training phases, would improve accessibility."}, "questions": {"value": "- How does the performance of FedDAG scale with an increasing number of clusters? The current experiments show a comparison against methods with a pre-set or automatically determined K. It would be insightful to see how FedDAG's adaptive mechanism performs when the inherent number of true data distributions in the client population is large (e.g., >10).\n\n- In the dual-encoder training, the primary encoder is initialized from the locally pre-trained models to avoid redundancy. Was any other initialization strategy explored? Could randomly initializing both and using a regularization term to encourage diversity be a viable alternative?\n\n- The authors need introduce some more related works based on clustering or data similarity:\n\n   - Collaborative learning by detecting collaboration partners\n   - Personalized Federated Learning under Local Supervision\n   - Benchmarking data heterogeneity evaluation approaches for personalized federated learning,"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XdMMcPaKLu", "forum": "eIMXY9qkKz", "replyto": "eIMXY9qkKz", "signatures": ["ICLR.cc/2026/Conference/Submission15820/Reviewer_VAQF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15820/Reviewer_VAQF"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805735937, "cdate": 1761805735937, "tmdate": 1762926048459, "mdate": 1762926048459, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a clustered federated learning framework, termed FedDAG, which is designed to address performance degradation under heterogeneous (non-IID) client data. Focusing on different non-IID scenarios, FedDAG applied a learnable weight combination for data and gradient similarities integration.  It also includes an adaptive clustering metric to automatically determine the optimal number of clusters. In addition, FedDAG employs a dual-encoder architecture for cross-cluster representation sharing. Experiments across four types of non-IID scenarios (label skew, feature skew, concept shift, and quantity shift) show consistent improvements over several baselines, including PACFL, IFCA, and FedRC"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.  The paper clearly defines different types of non-IID heterogeneity and systematically designs corresponding modules to treat each.\n\n2. The framework integrates clustering, similarity measurement, and inter-cluster knowledge transfer into a cohesive system.\n\n3. FedDAG performs well across datasets and heterogeneity settings, demonstrating robustness in practice.\n\n4. Presentation is clear and easy to follow."}, "weaknesses": {"value": "1. The novelty of this work is limited. Most components extend existing approaches with minor tweaks. The framework lacks a central new idea or theoretical insight.\n\n2. There is no convergence, optimality, or complexity analysis; the method’s robustness under different heterogeneity settings is justified only empirically.\n\n3. The ablation studies are incomplete.\n\n- The sensitivity of the gradient similarity module to the number of local optimization steps and the sparsification ratio is not examined, leaving uncertainty about the balance between communication efficiency and clustering accuracy.\n\n- The data similarity module includes both class-frequency weighting and entropy-based weighting for fusion, yet their individual effects are not isolated or discussed.\n\n- The paper does not verify whether each designed module actually mitigates the corresponding non-IID type (e.g., whether the class-wise weighting improves label skew, or whether the entropy weighting helps with concept shift). This makes it difficult to assess whether the proposed design behaves as intended.\n\n4. All core experiments are conducted on relatively small and clean benchmark datasets (CIFAR-10/100, FMNIST, and SVHN). Although the appendix includes an additional comparison on Tiny-ImageNet, this test remains limited in scope and scale and is insufficient to demonstrate robustness or scalability to more realistic, large-scale federated settings."}, "questions": {"value": "1. Can the authors provide theoretical convergence or complexity analysis for FedDAG in different non-IID cases?\n\n2. How do the different modules (data weighting, gradient sparsification, adaptive clustering) behave under various types of non-IID settings?\n\n3. What is the computational and communication overhead compared to PACFL or IFCA?\n\n4. Can the authors test FedDAG on other modalities or large datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bTjfP9lZI7", "forum": "eIMXY9qkKz", "replyto": "eIMXY9qkKz", "signatures": ["ICLR.cc/2026/Conference/Submission15820/Reviewer_uk8Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15820/Reviewer_uk8Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826610802, "cdate": 1761826610802, "tmdate": 1762926047942, "mdate": 1762926047942, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "FedDAG proposed a new clustered FL algorithm, designed to tackle data heterogeneity. Specifically, first, to improve clustering, FedDAG introduces a hybrid similarity evaluation by combining both data and gradient similarity, and then obtains a novel federated-aware metric to evaluate candidate clusters, enabling the framework to automatically determine the optimal number of clusters. Second, to enhance global representation sharing, FedDAG employs a dual-encoder architecture. This design allows the model to learn both intra-cluster and inter-cluster representation simultaneously during training, which further increases FedDAG’s performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. FedDAG can address label skew, feature skew, concept shift, and quantity shift simultaneously. This comprehensive approach is rare in Federated Learning. \n2. FedDAG can adaptively determine the optimal number of clusters. This mechanism effectively solves a long-standing practical issues where most methods require setting the number of clusters in advance. It is practical to handle dynamic scenarios, such as the arrival of new clients (Appendix A.7) and data distribution shifts over time (Appendix A.8) .\n3. The proposed global representation sharing which is implemented via a dual-encoder architecture and a novel CC-Graph is a key innovation. By considering the \"supply\" and \"demand\" of class representations based on data rarity, this mechanism breaks the traditional information silos of clustered FL, allowing clusters to learn from complementary data sources across the entire network for the first time."}, "weaknesses": {"value": "1. FedDAG has large computational overhead due to its complex design, involving an intricate multi-step process (e.g., SVD, clustering, CC-Graph, dual-encoder training, MLP optimization,). Specifically, for computational cost, server-side operations are at least O(N^2) for computing the similarity matrix and clustering, making scalability beyond the 100 clients tested questionable. For communication overhead, the dual-encoder training phase appears to double the communication cost compared to FedAvg, and the proposed mitigation (alternating training) is not clearly presented in the main algorithm.\n\n2. The \"category frequency weighting\" in formulas (5) and (6) is specifically designed to address the issue of \"quantity shift\". Directly uploading category frequency is a serious privacy issue, as it completely exposes the label distribution of the client. Although the author acknowledges these issues in Appendix A.6 and proposes that “uniform weighting can be employed in place of class-frequency-based weighting of similarity values to prevent leakage of class distribution information.”, this solution logically completely denies one of the core contributions of the paper.\n\n3. The concept of CC-Graph is novel, but its metric function is overly simplistic. It merely reflects differences in the ranking of numbers of samples belong to the same class among clients, but cannot capture disparities in quantity or quality of samples.\n\n4. The design of the GRS mechanism is also open to debate. Traditional personalized federated learning treats the feature extraction layer as an aggregatable part of the whole parameters while preserving the personalization of the classifier layer. This design assumes feature extraction is globally similar across different datasets and different tasks. The dual-encoder architecture proposed in this paper introduces an independent global feature extractor. It concatenates representations from global extractor($\\theta^{2f}$) with those from the cluster feature extractor($\\theta^{1f}$) before feeding them to the downstream classification layer. This design lacks compelling justification and instead incurs significant communication overhead. More comparative experimental results and analysis should be included.\n\n5. There is a significant disconnect between the paper's theoretical guarantees and the algorithm's practical operation. The convergence analysis in Appendix A.4 critically relies on Assumption A.4 (Stable clustering). This assumption is directly contradicted by the FedDAG algorithm's own design, which includes a dynamic phase to determine clusters (Algorithm 1, Lines 1-14) and further mechanisms to change cluster assignments in response to newcomers (Appendix A.7) or distribution shifts (Appendix A.8). Consequently, the provided theory only applies to a static scenario and fails to model the algorithm's core dynamic behaviors."}, "questions": {"value": "see the above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RyFKpjnwMo", "forum": "eIMXY9qkKz", "replyto": "eIMXY9qkKz", "signatures": ["ICLR.cc/2026/Conference/Submission15820/Reviewer_Lm4p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15820/Reviewer_Lm4p"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761827357306, "cdate": 1761827357306, "tmdate": 1762926047382, "mdate": 1762926047382, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}