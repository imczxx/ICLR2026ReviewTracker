{"id": "zP2Tapo252", "number": 15661, "cdate": 1758253677951, "mdate": 1759897290538, "content": {"title": "AutoGraph-R1: End-to-End Reinforcement Learning for Knowledge Graph Construction", "abstract": "Building effective knowledge graphs (KGs) for Retrieval-Augmented Generation (RAG) is pivotal for advancing question answering (QA) systems. However, its effectiveness is hindered by a fundamental disconnect: the knowledge graph (KG) construction process is decoupled from its downstream application, yielding suboptimal graph structures. To bridge this gap, we introduce AutoGraph-R1, the first framework to directly optimize KG construction for task performance using Reinforcement Learning (RL). AutoGraph-R1 trains an LLM constructor by framing graph generation as a policy learning problem, where the reward is derived from the graph's functional utility in a RAG pipeline. We design two novel, task-aware reward functions, one for graphs as knowledge carriers and another as knowledge indices. Across multiple QA benchmarks, AutoGraph-R1 consistently enables graph RAG methods to achieve significant performance gains over using task-agnostic baseline graphs. Our work shows it is possible to close the loop between construction and application, shifting the paradigm from building intrinsically \"good\" graphs to building demonstrably \"useful\" ones.", "tldr": "We optimize knowledge graph construction directly for downstream task performance using reinforcement learning, improving RAG system effectiveness.", "keywords": ["Knowledge Graph Construction", "Reinforcement Learning", "Retrieval-Augmented Generation", "RAG"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7e04afbd7a0eb550d4bf80cc7512578495f1104f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper aims to build a knowledge graph (KG) for Retrieval-Augmented Generation (RAG) in an end-to-end manner. The authors propose training the KG constructor using reinforcement learning within a graph-based RAG system. They design two rewards corresponding to two applications: a graph knowledge retriever and a graph-based text retriever. The proposed method is trained on two multi-hop question answering (QA) datasets and evaluated on five QA datasets."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-written.\n2. The proposed approach consistently outperforms baselines."}, "weaknesses": {"value": "1. The novelty is limited, as the paper applies standard reinforcement learning techniques to KG construction without significant methodological innovation.\n2. The motivation is not clearly justified by the experiments, i.e., why is an RL-based approach necessary?"}, "questions": {"value": "1. In Section 4.2, the graph knowledge retriever reward requires computing the entire KG. Is this computationally expensive?\n2. In Section 4.3, what are the input and output of the KG construction policy model $\\pi_{\\theta}^{KG}$?\n3. The fine-tuned model is expected to outperform the zero-shot approach. The author should also include a strong pre-trained LLM as a baseline for knowledge graph construction.\n4. To demonstrate the contribution of RL-based graph construction, the authors should compare against other KG construction baselines, such as HippoRAG-2 (Gutierrez et al., 2025b).\n5. In Section 5.1, are there any experiments showing the effect of removing distractors during training? Also, why is this discussed in the dataset section rather than the methodology section?\n6. Moreover, additional baselines are needed to justify KG construction as an intermediate step. For example, why not fine-tune the dense triple retriever directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jlg4wMGAjG", "forum": "zP2Tapo252", "replyto": "zP2Tapo252", "signatures": ["ICLR.cc/2026/Conference/Submission15661/Reviewer_qEpk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15661/Reviewer_qEpk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761265466543, "cdate": 1761265466543, "tmdate": 1762925916928, "mdate": 1762925916928, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AutoGraph-R1, a reinforcement learning approach for improving knowledge graph construction from text. AutoGraph-R1 leverages feedback from retrievers (both graph-based and text-based) to design reward signals for triplet generation from textual content. The reward mechanism evaluates whether constructed triplets effectively support downstream query answering and retrieval of gold passages. Experimental evaluation against baseline prompt-based KG construction (Qwen-3/7B models) demonstrates consistent improvements across five datasets using various underlying retriever architectures."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- S1) AutoGraph-R1 addresses the interesting yet challenging problem of optimal knowledge graph construction from text. The experimental results demonstrate encouraging potential for reinforcement learning in this problem.\n\n- S2) AutoGraph-R1 achieves consistent performance improvements over simple prompting-based KG construction methods when using Qwen-3B and Qwen-7B models."}, "weaknesses": {"value": "- W1) A key effect of AutoGraph-R1 appears to be filtering redundant information (Figure 1). However, this may inevitably reduce the number of generated triplets, potentially degrading performance on out-of-distribution questions. A critical question arises: why address redundancy during KG construction rather than KG retrieval (as in Graph-R1; Luo et al., 2025)? An alternative approach would maintain broad triplet coverage during construction while training the retriever to filter appropriately, potentially mitigating the risk of under-constructing KGs. The authors should provide stronger justification on the implications of their KG construction design.\n\n- W2) AutoGraph-R1 evaluates against KG prompting using Qwen 3B and 7B models, but most existing work constructs KG with more powerful models like GPT-4o-mini (GFM-RAG). The authors should compare against more powerful black-box LLMs for KG construction prompting to better demonstrate the gaps in current literature.\n\n- W3) The paper lacks in-depth analysis of the constructed KG quality, including metrics such as triplet count, entity and relation numbers, and key differences from prompting-based KG construction methods. These details are essential for demonstrating AutoGraph-R1's specific benefits and its impact on graph structure."}, "questions": {"value": "- Q1) In Table 3, why is the performance gap between Qwen-3B and Qwen-7B so large for the Base case on NQ?\n\n- Q2) What is the size difference between KGs created via AutoGraph-R1 versus vanilla prompting-based methods? Would using stronger LLMs like GPT-4o reduce the performance gap between AutoGraph-R1 and prompting approaches?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JVqT5lgPAD", "forum": "zP2Tapo252", "replyto": "zP2Tapo252", "signatures": ["ICLR.cc/2026/Conference/Submission15661/Reviewer_PzXo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15661/Reviewer_PzXo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761510729331, "cdate": 1761510729331, "tmdate": 1762925916418, "mdate": 1762925916418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims to improve the existing KG-RAG task by developing more effective knowledge graph construction methods that better support downstream KG-based question answering. To this end, the authors propose an end-to-end RL pipeline for fine-tuning LLMs to generate outputs suitable for different downstream tasks. The proposed method is compatible with two mainstream retrieval models and achieves consistent improvements across various datasets and methods.\n\nHowever, the motivation and evaluation are not fully convincing. Details are discussed in the Weaknesses and Questions sections."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a complete and comprehensive overview of related work.  \n\n- Experiments demonstrate good performance across multiple datasets and retrieval methods."}, "weaknesses": {"value": "The motivation is not fully convincing. (1) The long reasoning paths illustrated in Figure 1 are not the main problem. The main issue appears to be the missing triple (“Golden State Edge”, “located in”, “California”). (2) The KG generated by AutoGraph-R1 is not sufficient; it fails when the query changes to “What is the Golden Gate Republic Bridge connected to?” due to the loss of information. In summary, the main issue seems to be the completeness of the KG. However, the paper lacks evaluations of such completeness. The evaluation on the KG-RAG dataset is not convincing as it may biased (see Question 2) with respect to train/tested queries.\n \n- The comparisons are limited. Only KGs constructed using zero-shot prompt learning are compared. The paper lacks comparisons with existing KG construction methods or with the KG construction processes used by baseline KG-RAG systems.\n \n- Some ambiguity remains regarding the Graph-based Text Retriever case (see Question 3)."}, "questions": {"value": "1. Why not compare AutoGraph-R1 with other KG construction methods or with the original KG-RAG baselines?  \n\n2. Following the earlier concern about motivation, could the observed improvement result from the RL model learning biases in the query–answer patterns? For instance, in Figure 1, it seems to prefer relations like “has government” on the right-hand side over other plausible relations such as “connects” on the left.  \n\n3. In the Graph-based Text Retriever case, does AutoGraph-R1 extract a subgraph or construct a new one? If it only extracts, why not build a new graph—for example, by treating each triple as a document?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AMfvs8qm07", "forum": "zP2Tapo252", "replyto": "zP2Tapo252", "signatures": ["ICLR.cc/2026/Conference/Submission15661/Reviewer_zf8Y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15661/Reviewer_zf8Y"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761643108085, "cdate": 1761643108085, "tmdate": 1762925915960, "mdate": 1762925915960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an interesting work on graph construction using RL. Graph-based indexing or retrieval has been a trending topic to improve the retrieval-augmented generation when relational knowledge is useful. Since then, most of the existing algorithms focus on optimizing the graph traversal instead of improving the graph quality. AutoGraph-R1 proposes two novel rewards: (1) knowledge carrying reward and (2) knowledge indexing reward to incentize model's behavior using a chosen retrieval strategy. The result shows some encouraing results on making the indexing corpus (graph) optimizable on unseen test benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a timely challenge that Graph RAG system is limited by a static graph construction algorithm. By optimizing the graph constrution, AutoGraph-R1 improves the baseline performance for various graph retrieval algorithms. \n\n2. The paper proposes two rewards proposed for two different graph retrieval paradigms, respecitvely. The explaination and demonstrataion of the F1-score failure is both insightful and convincing."}, "weaknesses": {"value": "1. Although the performance improves are observed, some abaltion and experital details to validate the claims, such as:\n (a) what's the performance of SFT using the reward function (e.g. deducible Judge) to select positive data and train the model. \n (b) what's the qualitative difference of RL trained and pre-trained base model extraction results? The author only showcases the retrieval and QA result. To help audience understand the effect of training, comparing result and provide some relationales are neccessary. \n\n2. The graph construction statics are not provided. The end2end result cannot clearly show the difference of Autograph-R1 and vanilla LLM, it's not clear that what's the number of triples in average per document and relation/entity type distribution."}, "questions": {"value": "1. What is the LLM choice for the decucible Judge? How does the author to make sure the LLM inference latency for an effective training roll outs. I am wondering if a less powerful deducible Judge will make the overall performance worse than reported, whether AutoGraph-R1 is sensitive to this? \n\n\n2. During the inference, do you input single document at a time or a batch of document? If I understand it correclty, during the training, 15 documents are fed into the AutoGraph-R1 for graph construction? \n\n3. Why do you use Qwen3-0.6B as the dense triple retriever while during training AutoGraph-R1 uses Qwen3-8B to retrieve distrator documents?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vmRMK1r7rz", "forum": "zP2Tapo252", "replyto": "zP2Tapo252", "signatures": ["ICLR.cc/2026/Conference/Submission15661/Reviewer_dm4n"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15661/Reviewer_dm4n"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15661/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762144613497, "cdate": 1762144613497, "tmdate": 1762925915572, "mdate": 1762925915572, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}