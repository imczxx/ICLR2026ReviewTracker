{"id": "wFSOtyvQ9d", "number": 12053, "cdate": 1758205431119, "mdate": 1763570076465, "content": {"title": "FreqKV: Key-Value Compression in Frequency Domain for Context Window Extension", "abstract": "Existing key-value (KV) cache compression methods for large language models (LLMs) often rely on token eviction, which risks losing critical local information in both long prefilling and decoding scenarios. When extrapolating beyond the pretrained context length, their performance degrades sharply on long-context benchmarks. Motivated by the observation in the frequency domain that the context information is concentrated in the low-frequency components, we propose FreqKV, a parameter-free and architecture-agnostic approach. It iteratively compresses the increasing KV cache in the frequency domain, allowing models to process lengthy contexts efficiently. With minimal training at 8K length, FreqKV extends the context window of LLaMA-2-7B up to 256K tokens while maintaining stable perplexity. Extensive experiments on both prefilling and decoding stages demonstrate that FreqKV enables robust context window extension and consistently outperforms existing KV cache compression methods, highlighting its effectiveness for both understanding and generation in long contexts.", "tldr": "This paper introduces FreqKV, an efficient context extension method that iteratively compresses key-value states in the frequency domain.", "keywords": ["Large Language Models", "KV Compression", "Context Extension"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9687c4d3ad5d63db8a554a7f0d79742474d2d1ba.pdf", "supplementary_material": "/attachment/aeaaca5a8ccf085d42756b47515d691b14f989d6.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a form of cache compression based on converting the tokens in the cache into the frequency domain for compressed storage. This results in lower storage requirements, lower attention complexity in decoding, and a kind of natural context extension since positional embeddings since the compression projects the cache into a fixed cardinality which will not expand to use up all of the positional embeddings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- KV cache compression is an important topic due to the efficiency gains and power consumption concerns of modern transformers.\n\n- In addition to compression the cache, the method also seems to cause an efficiency gain in the decoding attention operation."}, "weaknesses": {"value": "- Llama 3 is used as a baseline model. This is important because I believe the only reason some baselines show poor performance is because they have exceeded the number of training positional embeddings. However, Llama 3 is already 1.5 years old at this point. There are already 3 releases of Llama3 which go up to 3.3 and have 131K native positional embeddings. Can FreqKV be applied to these models and show the same good performance past 131K?\n\n- There is no comparison of latency with baselines such as SnapKV.\n\n---\n\nOverall, it would be more convincing if the authors could provide a case on what happens with extremely long contexts. Due to the compressive nature of the cache, it may only be able to hold information up to a certain point before the compression becomes noise. However, if the same trend witnessed at 8K-->16K can be witnessed on a 131K-->232K model, I would find this very compelling. This could be done with the exact same experimental setup and swapping in the Llama 3.x models."}, "questions": {"value": "For a clearer understanding of how the DCT and IDCT matrices transform the inputs, can you add the dimensions for both the DCT and IDCT matrices?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "76XHV2FKpR", "forum": "wFSOtyvQ9d", "replyto": "wFSOtyvQ9d", "signatures": ["ICLR.cc/2026/Conference/Submission12053/Reviewer_phR3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12053/Reviewer_phR3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761206725040, "cdate": 1761206725040, "tmdate": 1762923027305, "mdate": 1762923027305, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed FreqKV, a novel KV cache compression method. FreqKV performs KV compression in the frequency domain using the DCT, retaining only the low-frequency components. After the fine-tuning, LLMs (Llama2 and Llama3) show comparable performance to the uncompressed one, and are superior to existing KV compression methods. The paper also reports generation speed improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of applying frequency-domain compression to the KV cache is both intuitive and novel. The similarity to JPEG compression makes the concept easy to understand, and the empirical results demonstrate that this approach is competitive with or superior to existing baselines.\n* The paper includes extensive comparisons with a wide range of prior methods on multiple datasets."}, "weaknesses": {"value": "* The paper lacks a detailed analysis or intuitive explanation of why low-frequency components dominate in the KV cache. Moreover, the large magnitude of low-frequency components does not necessarily imply that they are semantically important, yet the paper seems to make this assumption. While the empirical evidence supports the method’s motivation and effectiveness, a more analytical approach would strengthen the argument.\n* Although the authors claim speed improvements in both the prefill and decoding stages, the experiments seem to focus on decoding latency. The paper should provide either empirical results or additional clarification regarding the prefill stage speed-ups.\n* In Table 3, competing methods fail completely (near zero point) at the 16K context length. This might not solely reflect a generalization failure but could be caused by an implementation difference: FreqKV applies pre-RoPE compression (on-the-fly RoPE just before inference), whereas others employ post-RoPE compression. It would be essential to clarify this difference.\n* FreqKV requires a training phase for parameter learning; however, it remains unclear how it performs without such fine-tuning. Furthermore, comparisons in Table 3 should also include other training-based methods (e.g., LoCoCo)."}, "questions": {"value": "* The related work section would benefit from a broader discussion of recent streaming-based long-context KV management methods, such as InfLLM, InfiniPot, and Minference.\n* The rescaling formulation (Eq. 5) is somewhat questionable. Instead of scaling by the number of retained coefficients, it might be more reasonable to adopt an (spectral) energy-preserving normalization. \n* (minor) The abstract could be updated to refer to Llama 3 rather than Llama 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RaQJTYibgx", "forum": "wFSOtyvQ9d", "replyto": "wFSOtyvQ9d", "signatures": ["ICLR.cc/2026/Conference/Submission12053/Reviewer_35T2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12053/Reviewer_35T2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906683396, "cdate": 1761906683396, "tmdate": 1762923026676, "mdate": 1762923026676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed FreqKV, a novel KV cache compression method. FreqKV performs KV compression in the frequency domain using the DCT, retaining only the low-frequency components. After the fine-tuning, LLMs (Llama2 and Llama3) show comparable performance to the uncompressed one, and are superior to existing KV compression methods. The paper also reports generation speed improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The idea of applying frequency-domain compression to the KV cache is both intuitive and novel. The similarity to JPEG compression makes the concept easy to understand, and the empirical results demonstrate that this approach is competitive with or superior to existing baselines.\n* The paper includes extensive comparisons with a wide range of prior methods on multiple datasets."}, "weaknesses": {"value": "* The paper lacks a detailed analysis or intuitive explanation of why low-frequency components dominate in the KV cache. Moreover, the large magnitude of low-frequency components does not necessarily imply that they are semantically important, yet the paper seems to make this assumption. While the empirical evidence supports the method’s motivation and effectiveness, a more analytical approach would strengthen the argument.\n* Although the authors claim speed improvements in both the prefill and decoding stages, the experiments seem to focus on decoding latency. The paper should provide either empirical results or additional clarification regarding the prefill stage speed-ups.\n* In Table 3, competing methods fail completely (near zero point) at the 16K context length. This might not solely reflect a generalization failure but could be caused by an implementation difference: FreqKV applies pre-RoPE compression (on-the-fly RoPE just before inference), whereas others employ post-RoPE compression. It would be essential to clarify this difference.\n* FreqKV requires a training phase for parameter learning; however, it remains unclear how it performs without such fine-tuning. Furthermore, comparisons in Table 3 should also include other training-based methods (e.g., LoCoCo)."}, "questions": {"value": "* The related work section would benefit from a broader discussion of recent streaming-based long-context KV management methods, such as InfLLM, InfiniPot, and Minference.\n* The rescaling formulation (Eq. 5) is somewhat questionable. Instead of scaling by the number of retained coefficients, it might be more reasonable to adopt an (spectral) energy-preserving normalization. \n* (minor) The abstract could be updated to refer to Llama 3 rather than Llama 2."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No concerns."}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RaQJTYibgx", "forum": "wFSOtyvQ9d", "replyto": "wFSOtyvQ9d", "signatures": ["ICLR.cc/2026/Conference/Submission12053/Reviewer_35T2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12053/Reviewer_35T2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761906683396, "cdate": 1761906683396, "tmdate": 1763602590568, "mdate": 1763602590568, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "authors propos FreqKV, a parameter-free, architecture-agnostic KV-cache compression method for LLMs. It applies a DCT along the sequence axis to KV tensors, keeping low-frequency components, then IDCTs back, with an iterative schedule that re-compresses older tokens as the cache grows."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear idea with strong intuition that low-frequency energy concentration in KV states\n\n2. Comprehensive experiments on multiple benchmarks illustrated the effectiveness of the proposed method.\n\n3. The ablation study provides more detailed information on frequency choice."}, "weaknesses": {"value": "1. While the low-frequency concentration is a great motivation, the paper doesn't explore why this happens or what information is stored in which frequency bands. For instance, is the low-frequency \"global context\" and the high-frequency \"local token-specific details\"? Authors may provide a deeper analysis here to provide valuable insights\n\n2. Attention heads, layers may carry different spectral content. A per-head adaptive $\\gamma$ or power-based cutoff might outperform fixed ratios. Do the authors have ablation or adaptive strategies here?\n\n3. Authors may need to compare the most recent baseline [1]\n\nReferences:\n\n[1] LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models, ICML'25"}, "questions": {"value": "see. weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sNPz3ioNcI", "forum": "wFSOtyvQ9d", "replyto": "wFSOtyvQ9d", "signatures": ["ICLR.cc/2026/Conference/Submission12053/Reviewer_DurU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12053/Reviewer_DurU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961912535, "cdate": 1761961912535, "tmdate": 1762923026373, "mdate": 1762923026373, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "authors propos FreqKV, a parameter-free, architecture-agnostic KV-cache compression method for LLMs. It applies a DCT along the sequence axis to KV tensors, keeping low-frequency components, then IDCTs back, with an iterative schedule that re-compresses older tokens as the cache grows."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear idea with strong intuition that low-frequency energy concentration in KV states\n\n2. Comprehensive experiments on multiple benchmarks illustrated the effectiveness of the proposed method.\n\n3. The ablation study provides more detailed information on frequency choice."}, "weaknesses": {"value": "1. While the low-frequency concentration is a great motivation, the paper doesn't explore why this happens or what information is stored in which frequency bands. For instance, is the low-frequency \"global context\" and the high-frequency \"local token-specific details\"? Authors may provide a deeper analysis here to provide valuable insights\n\n2. Attention heads, layers may carry different spectral content. A per-head adaptive $\\gamma$ or power-based cutoff might outperform fixed ratios. Do the authors have ablation or adaptive strategies here?\n\n3. Authors may need to compare the most recent baseline [1]\n\nReferences:\n\n[1] LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models, ICML'25"}, "questions": {"value": "see. weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sNPz3ioNcI", "forum": "wFSOtyvQ9d", "replyto": "wFSOtyvQ9d", "signatures": ["ICLR.cc/2026/Conference/Submission12053/Reviewer_DurU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12053/Reviewer_DurU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12053/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961912535, "cdate": 1761961912535, "tmdate": 1763605486891, "mdate": 1763605486891, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Action Needed: Review Rebuttal and Update Evaluation"}, "comment": {"value": "Dear Reviewers,\n\nThank you, as always, for your valuable contributions and efforts. The authors have now submitted their rebuttal. Please take a moment to review it and provide any necessary follow-up actions, such as additional questions, clarification requests, or updates to your review.\n\nSince the initial ratings ranged from 8 to 4, I kindly ask you to pay close attention to the perspectives of the other reviewers when preparing your final response.\n\nThank you again for your support."}}, "id": "S6hXrrkGxB", "forum": "wFSOtyvQ9d", "replyto": "wFSOtyvQ9d", "signatures": ["ICLR.cc/2026/Conference/Submission12053/Area_Chair_jj77"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12053/Area_Chair_jj77"], "number": 7, "invitations": ["ICLR.cc/2026/Conference/Submission12053/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763605051359, "cdate": 1763605051359, "tmdate": 1763605051359, "mdate": 1763605051359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}