{"id": "zwLpUxiqSE", "number": 20303, "cdate": 1758304593754, "mdate": 1763594340418, "content": {"title": "Space Filling Curves as Spatial Priors for Small or Data-Scarce Vision Transformers", "abstract": "Vision Transformers (ViTs) have become a dominant backbone in computer vision, yet their attention mechanism lacks inherent spatial inductive biases, which are especially crucial in small models and low-data regimes. Inspired by the masking in Linear Transformers and the scanning patterns of Vision SSMs, we propose VIOLIN, a lightweight masked attention mechanism that integrates Space Filling Curves (SFCs) to enhance spatial awareness with negligible computational overhead. VIOLIN scans the input image with multiple SFCs to build curve specific decay masks, which are averaged and multiplied with the attention matrix to encode spatial relationships. It yields notable gains in data-scarce settings: when fine-tuning on VTAB-1K, VIOLIN improves accuracy by up to 8.7% on the Structured group, and it can be combined with parameter-efficient tuning methods such as LoRA. Beyond fine-tuning, VIOLIN consistently improves various tiny or small scale ViT architectures (e.g., DeiT, DINO) during pretraining on ImageNet-1K, achieving gains of up to 0.9\\% on  on ImageNet-1K and 7.2\\% on pixel level CIFAR-100. Overall, VIOLIN offers a computationally efficient yet effective way to inject spatial inductive bias into ViTs, particularly benefiting small models and data-scarce scenarios.", "tldr": "A new attention mechanism for vision backbones using Space Filling Curves improving both fine-tuning and pre-training of ViTs.", "keywords": ["space filling curves", "ViT", "spatial priors"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/992855b34473e8b32d7eeedcdfa07a00963e8fe7.pdf", "supplementary_material": "/attachment/d45017e119c71b94623b10e5daa547b5e5c2c6d6.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes **VIOLIN**, a simple and plug-and-play spatial prior module for Vision Transformers (ViTs).  \nThe method introduces *Space Filling Curves (SFCs)* (e.g., Snake, Zig-zag, Peano, Hilbert) to define alternative scanning orders of image patches.  \nFor each curve \\(c\\), a decaying mask \\(M_c[i,j] = \\gamma_c^{|i-j|}\\) is constructed to encourage locality in attention.  \nAfter aligning these masks back to the standard patch order and averaging, the resulting mask \\(M_{\\text{VIOLIN}}\\) is multiplied with the attention score matrix before softmax.\n\nThe approach is extremely lightweight (+0.0002% params, +0.64% FLOPs) and can be applied to pretrained or finetuned ViTs without architectural changes.  \nExtensive experiments on **VTAB-1K**, **ImageNet-1K**, **DINO**, **pixel-level CIFAR-100**, and dense tasks (ADE20K / COCO) show consistent gains, especially on “Structured” VTAB tasks (+8.7%)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-defined target problem:** Focuses on *small models and data-scarce regimes* where ViTs lack spatial inductive bias — a meaningful and under-explored setting.  \n- **Simplicity and generality:** VIOLIN requires no retraining or re-architecture changes, making it truly plug-and-play.  \n- **Elegant formulation:** The SFC-based decaying masks are clearly derived; the permutation and averaging operations are well explained.  \n- **Strong empirical results:** Significant improvement on VTAB-1K (Structured group +8.7%) and pixel-level CIFAR-100 (+7.2%) convincingly show the benefit of spatial priors.  \n- **Low computational cost:** The added overhead is negligible, suitable for real-world low-resource finetuning.  \n- **Broad applicability:** Small but consistent gains on segmentation and detection tasks further validate its generality."}, "weaknesses": {"value": "1. **Novelty is limited.**  \n   The core idea—distance-decayed attention weights—is reminiscent of *linear attention*, *RMT*, and *RetNet*–style exponential decay mechanisms.  \n   The use of multiple SFCs and their averaged mask is an incremental extension rather than a fundamentally new concept.\n\n2. **Missing comparisons with strong baselines.**  \n   The paper compares mainly to vanilla DeiT/DeiT-III/DINO backbones.  \n   It lacks direct comparisons with existing locality-enforcing methods, such as:\n   - Relative positional bias (Swin / ViT-RPB),\n   - Convolutional stems or LocalViT,\n   - Manhattan-distance masks (RMT),\n   - Single-curve or random-curve baselines.  \n   Without these, it is unclear whether the large Structured-task gains stem from the proposed multi-SFC averaging or from any reasonable local bias.\n\n3. **Training details and fairness are under-specified.**  \n   VTAB-1K finetuning recipes (learning rate, γ initialization, α sharing) are buried in the appendix.  \n   It remains unclear whether baselines were tuned equivalently.  \n   The surprising claim that *untrained masks outperform pretrained ones* needs stronger justification.\n\n4. **Questionable mask effectiveness.**  \n   Figure 7 shows most γ₍c₎ values approach 1, suggesting the mask becomes nearly uniform.  \n   If so, why does the Structured group improve so dramatically?  \n   More analysis of per-head γ values and locality visualization is needed.\n\n5. **Computational overhead claim is not empirically verified.**  \n   Only theoretical FLOPs/parameter ratios are reported.  \n   Actual GPU memory and runtime increase (especially on dense tasks) should be measured.\n\n6. **Overstated framing.**  \n   The paper sometimes overclaims by calling VIOLIN a *principled spatial prior via SFCs*.  \n   In fact, the method does not exploit the geometric guarantees of SFCs; it only uses index distance \\(|i-j|\\) with exponential decay.  \n   Theoretical justification for averaging multiple SFC-induced metrics is weak."}, "questions": {"value": "Please refer to Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EQUAdPMCbf", "forum": "zwLpUxiqSE", "replyto": "zwLpUxiqSE", "signatures": ["ICLR.cc/2026/Conference/Submission20303/Reviewer_sQfG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20303/Reviewer_sQfG"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856638287, "cdate": 1761856638287, "tmdate": 1762933771190, "mdate": 1762933771190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes VIOLIN, a masked attention mechanism for Vision Transformers (ViTs) that incorporates Space Filling Curves (SFCs) to improve spatial inductive biases. Standard ViTs suffer from the lack of spatial awareness due to the permutation-equivalent nature of self-attention. Inspired by linear attention and SSMs, VIOLIN constructs curve-specific decay masks that model the relative spatial distance between image patches. These masks are averaged and applied to the attention matrix, introducing spatial priors without modifying the core ViT architecture."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Extensive empirical validation: tested on diverse model scales (5M–86M parameters) and training setups (supervised and self-supervised).\n\n- The paper is easy to follow.\n\n- The proposed method shows some improvment."}, "weaknesses": {"value": "- **Limited Contribution from the Core Method**: [6] has shown that average pooling can boost the DeiT's performance. Tab. 14 suggests that **the performance gain mainly comes from the average pooling**. The VIOLIN only provide marginal improvement for small models, and **even harms the performance of the large model ViT-B**.\n\n- **Limited Generalization**: Based on the the results in Tab. 8, **the improvements on Swin-T and Swin-S are below 0.2%**, which is likely within run-to-run variance and not statistically significant. This suggests that the proposed method  is rather an engineering optimization technique, which does not generalize well to different models.\n\n- **Limited Comparison**. The baselines used for comparison primarily rely on absolute positional embeddings, which are known to be suboptimal. Relative positional encodings, widely adopted in modern architectures [1-5], are simpler, more flexible, and have been shown to outperform absolute encodings in multiple settings. **It is not clear that space-filling curves offer any meaningful advantage over such approaches**. Without direct comparisons to relative positional encoding, the benefits of VIOLIN are difficult to justify.\n\n[1] Wu, Kan, et al. \"Rethinking and improving relative position encoding for vision transformer.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[2] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL, 2019. 1, 3, 7, 8\n\n[3] Liu, Ze, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n[4] Liu, Ze, et al. \"Swin transformer v2: Scaling up capacity and resolution.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[5] Zhou, Yuxuan, et al. \"SP-ViT: Learning 2D Spatial Priors for Vision Transformers.\" 33rd British Machine Vision Conference. BMVA Press, 2022.\n\n[6] Conditional Positional Encodings for Vision Transformers, ICLR2023."}, "questions": {"value": "Could the authors also compare VIOLIN to other methods related to spatial prior, such as relative positional encoding/bias in [1-5] ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uxpdFscEPD", "forum": "zwLpUxiqSE", "replyto": "zwLpUxiqSE", "signatures": ["ICLR.cc/2026/Conference/Submission20303/Reviewer_nX45"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20303/Reviewer_nX45"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761887713486, "cdate": 1761887713486, "tmdate": 1762933770564, "mdate": 1762933770564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the use of space filling curves as a way to introduce spatial priors to vision transformers. It extends upon the use of decay masks with image flattening as determined by different space filling curves. The use of different curves effectively reorders the patches of the image in different spatially meaningful ways as compared to a single zig-zag line scan used in transformer architectures. The proposed method improves upon previous data efficient methods under similar settings and can also be applied solely in the fine-tuning stage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors proposed a novel way to include hand designed spatial priors thru the use of SFCs and proposed an efficient and effective way to incorporate into ViT architectures. Their proposed method can also be included into pretrained models with fine-tuning only. The proposed method improves on previous data-efficient methods like DeiT. Well designed ablation studies were also included to show the effects of each of their proposed changes to the attention mechanism. The authors also include a rather commendable and substantial appendix with important key prior art."}, "weaknesses": {"value": "- Training flow is not immediately clear in the main paper. Since there are multiple stages to train a ViT with VIOLIN masks, it would be good to recap on the stages even though DeiT’s training recipe was followed. This would make the experiment section and the ablation studies clearer.  \n- The authors proposed the use of different hand selected SFCs, it would be interesting to see how a separately learned patch ordering, e.g. from Kutscher 2025, compares. After all, the mask decay method can take in any form of ordering.  \n- Minor issue: Typo in Figure 2\\. In the center block, VIOLIN is misspelled."}, "questions": {"value": "- DieT uses CNN as a teacher network. Is a CNN also used in this case?  \n- Since CNNs have the strongest spatial prior, could the authors also include a similarly size SOTA CNN? Especially if, similar to DieT training recipe, a CNN is used as a teacher network"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "W9hsPjgYqK", "forum": "zwLpUxiqSE", "replyto": "zwLpUxiqSE", "signatures": ["ICLR.cc/2026/Conference/Submission20303/Reviewer_cmWr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20303/Reviewer_cmWr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761937856508, "cdate": 1761937856508, "tmdate": 1762933770023, "mdate": 1762933770023, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript proposes a lightweight masked attention mechanism named VIOLIN that integrates Space Filling Curves (SFCs) to enhance spatial awareness in smaller visual transformers (ViT). By better filling the space in 2D images through specifically designed curves, a better neighborhood representation is achieved when applying ViTs. VIOLIN scans the input image with multiple SFCs to build curve specific decay masks which are averaged and then weighted with the attention matrix to encode spatial relationships.\n\nAs SFCs the authors use Snake, Zig-zag, Peano, and Hilbert curves together with their transposed variants to capture diverse scanning patterns in both row and column major order."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The author propose an approach to represent better the neighbourhoods through Space filling curves (SFC) in order to enhance the processing of the image with ViT networks.\n- The manuscript concludes that by using SFCs improves the performance in performance in small models and limited-data settings.\n- Extensive experimental results are provided.\n- The approach requires only limited extra computational demands\n- Extending the application of SFCs to video understanding is also assessed."}, "weaknesses": {"value": "- There is no systematic or any theoretical study about what the space filling curves are useful for in ViTs\n- It is not clear what applications can be used for such SPCs based representations in ViTs except for some particular filtering. In the manuscript it is indicated that it can be applied for classification, semantic segmentation or object detection.\n- It is not clear how such SPCs can be used to some other ViT models."}, "questions": {"value": "Could the multiple SFC scans be combined in a more efficient way than by simply averaging?\n\nHow would the proposed multiple SFC work in the case of other transformer networks than those tested in the manuscript? \nFor example how would they work for the Swin transformer proposed in the paper:\nZ. Liu et al., Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, ICCV 2021.\n\nHow it would work, when applied on videos, on some video transformers, like for:\nLimin, W. el al, VideoAME V2: Scaling}video masked autoencoders with dual masking, CVPR 2023."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HX5jH4abzP", "forum": "zwLpUxiqSE", "replyto": "zwLpUxiqSE", "signatures": ["ICLR.cc/2026/Conference/Submission20303/Reviewer_8zFr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20303/Reviewer_8zFr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20303/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990395513, "cdate": 1761990395513, "tmdate": 1762933769718, "mdate": 1762933769718, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}