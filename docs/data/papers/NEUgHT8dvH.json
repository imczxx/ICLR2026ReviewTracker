{"id": "NEUgHT8dvH", "number": 3855, "cdate": 1757555463386, "mdate": 1759898066099, "content": {"title": "MMPD: Diverse Time Series Forecasting via Multi-Mode Patch Diffusion Loss", "abstract": "Despite the flourishing in time series (TS) forecasting backbones, the training mostly relies on regression losses like Mean Square Error (MSE). However, MSE assumes a one-mode Gaussian distribution, which struggles to capture complex patterns, especially for real-world scenarios where multiple diverse outcomes are possible. We propose the Multi-Mode Patch Diffusion (MMPD) loss, which can be applied to any patch-based backbone that outputs latent tokens for the future. Models trained with MMPD loss generate diverse predictions (modes) with the corresponding probabilities. Technically, MMPD loss models the future distribution with a diffusion model conditioned on latent tokens from the backbone. A lightweight Patch Consistent MLP is introduced as the denoising network to ensure consistency across denoised patches. Multi-mode predictions are generated by a multi-mode inference algorithm that fits an evolving variational Gaussian Mixture Model (GMM) during diffusion. Experiments on eight datasets show its superiority in diverse forecasting. Its deterministic and probabilistic capabilities also match the strong competitor losses, MSE and Student-T, respectively.", "tldr": "We propose the MMPD loss for patch-based time series forecasting backbones to model complex future distributions, enabling them to generate multiple diverse predictions with corresponding probabilities.", "keywords": ["time series forecasting", "loss function"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22b26c2184857088a9cdac4cc0ae81867b5d100a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes MMPD as a training objective: it turns the loss into a conditional diffusion process. Given future latent tokens produced by an upstream patch-based backbone, a lightweight Patch-Consistent MLP performs denoising to model complex, multi-modal future distributions; at inference, a progressively-evolving variational GMM extracts several plausible futures along with their probabilities."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- **S1** Solid work with complete design and experiments: from motivation and method (loss = diffusion + Patch-Consistent MLP) to inference (evolving variational GMM) and ablations/efficiency analysis, the pipeline is very clear and well executed.\n- **S2** Motivation is important and well argued: the paper clearly articulates that MSE-centric training implicitly assumes a unimodal Gaussian with fixed variance, which struggles to cover real-world multiple possible futures and time-varying/asymmetric uncertainty. The authors cleverly interpret MSE through the lens of a distributional assumption and then extend it to a higher-level, more expressive formulation."}, "weaknesses": {"value": "- **W1** Baseline coverage is incomplete: while comparisons include point-estimation and certain probabilistic/mixed-distribution schemes, the work does not compare against D3U[1], which follows a related paradigm of decoupling deterministic and uncertain components.\n- **W2** Patch-based pipelines typically introduces padding. For these non-real-value patches, it is unclear whether the Patch-Consistent MLP applies an explicit distinction been made. Moreover, padding patches lack a valid $\\text{next}_{k_j}$ to model. The paper should provide further implementation details and experiments clarifying how padding patches are treated.\n\n[1]. Li, Q., Zhang, Z., Yao, L., Li, Z., Zhong, T., & Zhang, Y. (2025). Diffusion-based decoupled deterministic and uncertain framework for probabilistic multivariate time series forecasting. In The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "See W1 and W2. If the author can address my concerns, I will consider increasing my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4SCbsbDRJ8", "forum": "NEUgHT8dvH", "replyto": "NEUgHT8dvH", "signatures": ["ICLR.cc/2026/Conference/Submission3855/Reviewer_kPh3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3855/Reviewer_kPh3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761658216916, "cdate": 1761658216916, "tmdate": 1762917068565, "mdate": 1762917068565, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes time-series forecasting by making the training objective itself a conditional diffusion model. A patchified forecaster produces future latent tokens, and a lightweight, cross-patch Patch-Consistent MLP learns to denoise them, capturing inherently multi-modal futures. At inference, the method does not rely on ad-hoc sampling; instead, it tracks modes through the reverse process with a stepwise variational GMM, yielding a small set of trajectories with calibrated probabilities. The approach is plug-and-play for patch-based backbones and includes a direct deterministic path without sacrificing efficiency. Across eight benchmarks, it consistently improves Top-K diversity metrics while remaining competitive—or better—on standard MSE (point forecasting) and CRPS (probabilistic accuracy)."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- **S1**: The paper distills a clear limitation of point-estimation training and turns the objective into a flexible distribution model, which is highly instructive for time-series forecasting research. Importantly, the authors make an effort to generalize beyond patch-based designs, discussing (and partially validating) how the idea can extend to non-patch backbones—thus increasing the method’s relevance to a wider set of architectures.\n\n- **S2**: The empirical study spans diverse datasets, multiple backbones, and both deterministic and probabilistic metrics, complemented by ablations and efficiency analyses. This breadth and depth make the conclusions credible."}, "weaknesses": {"value": "- **W1**: Please consider releasing the training/evaluation code and experiment scripts (including configs, seeds, data preprocessing, and checkpointing details). This would enable exact reproduction of tables/figures and facilitate fair downstream comparisons.\n\n- **W2**: Recent LLM-style time-series methods also employ patching. Adding representative LLM-based patch baselines would strengthen the empirical positioning and clarify where MMPD stands relative to these emerging approaches.\n\n- **W3**: The paper states “We focus on univariate forecasting; for multivariate data, the loss is computed per channel and averaged.” Can author discuss how MMPD handles inter-channel correlations."}, "questions": {"value": "See W1 to W3."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "jYGEW3NUeX", "forum": "NEUgHT8dvH", "replyto": "NEUgHT8dvH", "signatures": ["ICLR.cc/2026/Conference/Submission3855/Reviewer_hVy7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3855/Reviewer_hVy7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761660052985, "cdate": 1761660052985, "tmdate": 1762917068224, "mdate": 1762917068224, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of MSE-based training in time series forecasting, which assumes unimodal Gaussian distributions and fails to capture multiple plausible future outcomes. The authors propose to use a diffusion model conditioned on backbone latent tokens to model multi-modal future distributions. A Patch Consistent MLP serves as the denoising network, and a variational GMM-based inference algorithm generates diverse predictions with probabilities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles the restrictive unimodal Gaussian assumption inherent in MSE loss, which is a genuine problem for real-world time series where identical inputs can lead to multiple plausible futures due to unobserved contextual factors.\n\n2. The work provides clear motivation from both data perspective and application perspective, making the contribution practically valuable.\n\n3. The proposed MMPD loss can be applied to any patch-based forecasting backbone without architectural changes, and the generality is validated across four different backbones, demonstrating broad applicability."}, "weaknesses": {"value": "1. While the paper compares against MSE and Student-T, it lacks comparison with more recent and sophisticated probabilistic forecasting methods (e.g., recent normalizing flows, autoregressive diffusion models, or transformer-based probabilistic forecasters), making it difficult to assess where this stands in the current landscape.\n\n2. Despite claims of being \"lightweight,\" the method requires iterative diffusion steps K during training, generating N samples and fitting GMM at each reverse step during inference, and storing and processing adjacent patches. No concrete analysis of training time overhead, inference latency, or memory usage compared to MSE baseline is provided.\n\n3. The used GMM assumptions may be too restrictive. While GMM is more flexible than single Gaussian, it still assumes modes follow Gaussian distributions with spherical covariance. Real multi-modal distributions may have asymmetric modes, correlated dimensions, or non-Gaussian shapes, limiting the method's ability to capture truly complex distributions."}, "questions": {"value": "1.  During training, you optimize diffusion loss on random (y^k, k) pairs. During inference, you fit GMM by assuming samples follow Eq. 10. Is there a theoretical or empirical guarantee that the learned diffusion model produces samples consistent with this GMM structure?\n\n2. The diffusion variance schedule is critical. Do you use standard schedules from image diffusion, or is adaptation needed for time series?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ktugXFrlte", "forum": "NEUgHT8dvH", "replyto": "NEUgHT8dvH", "signatures": ["ICLR.cc/2026/Conference/Submission3855/Reviewer_XDPi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3855/Reviewer_XDPi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761744969080, "cdate": 1761744969080, "tmdate": 1762917067902, "mdate": 1762917067902, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the challenge of diverse forecasting in time series analysis, where multiple plausible future outcomes exist rather than a single deterministic forecast. It introduces MMPD loss, a diffusion model-based probabilistic loss function that can be integrated with various patch-based forecasting backbones. The experiments on eight benchmark datasets and comparisons with several baselines demonstrate MMPD’s good performance in diverse, deterministic, and probabilistic forecasting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Introduces a diffusion-based loss capturing multi-modal future distributions, addressing fundamental limitations of traditional single-mode losses.\n2. Can be applied to any patch-based forecasting backbone, making it flexible for different architectures.\n3. Effectively generates multiple sharp predictions with corresponding probabilities, enabling risk-aware decision-making.\n4. The proposed AdaLN-MLP accounts for adjacent patches, reducing inconsistency and unrealistic discontinuities in forecasts."}, "weaknesses": {"value": "1. Multi-mode inference requires multiple samples and iterative GMM fitting, which may introduce latency for real-time applications.\n2. The diffusion-based loss and multi-mode inference algorithm add complexity to the training and prediction pipeline.\n3. How robust is MMPD to different patch sizes and ranges of adjacent patch conditions in the Patch Consistent MLP?\n4. Can MMPD be extended to explicitly handle multivariate forecasting jointly rather than per-channel averaging?\n5. How does MMPD perform under non-stationary or concept drift conditions in time series data?"}, "questions": {"value": "See above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sLAXZagObJ", "forum": "NEUgHT8dvH", "replyto": "NEUgHT8dvH", "signatures": ["ICLR.cc/2026/Conference/Submission3855/Reviewer_7ixF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3855/Reviewer_7ixF"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761964448285, "cdate": 1761964448285, "tmdate": 1762917067692, "mdate": 1762917067692, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}