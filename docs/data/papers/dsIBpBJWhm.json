{"id": "dsIBpBJWhm", "number": 24161, "cdate": 1758353447184, "mdate": 1759896779357, "content": {"title": "Physics-Informed Graph Convolutional Network for Data-Free Learning of Channel Flow Fields", "abstract": "Physics-informed neural networks (PINNs) have gained traction for solving partial differential equations (PDEs) by embedding physical laws directly into the learning objective. However, traditional PINNs, typically built on fully connected neural networks, often struggle to capture localized interactions, scale poorly with mesh complexity, exhibit limited generalization across geometric variations and suffer from training instability due to minimal spatial representation. To address these challenges, we propose physics-informed graph convolutional network (PIGCN) architecture that leverages mesh connectivity to enforce local spatial coupling between variables, enhancing the ability of the model to learn structured physical relationships. In addition, this work investigated on data-free learning to reduce reliance on data observation and improve the generalization capability, introduced global physics loss, and implemented two-level dynamic weighting scheme to adaptively balance between multiple PDE residuals (e.g., continuity, momentum, energy) and composite loss terms (e.g., boundary conditions, global physics, and data), improving convergence in multi-objective training. We evaluated PIGCN against PINN on a square channel with isothermal walls and L-shaped channel with bending flow, which exhibits complex fluid dynamics phenomena. For square channel geometry, baseline PIGCN outperformed baseline PINN by 56.6\\%, and was further improved with two-level dynamic weighting by 38.9\\%, which achieved 66.6\\% in rMSE error improvement compared to equivalent PINN. In L-shaped channel, baseline PIGCN improved rMSE error by 35.7\\% against baseline PINN, and the introduction of global physics loss further reduced rMSE error by up to 71.3\\% in comparison with baseline PIGCN. Our investigation also demonstrated that PIGCN achieved significantly faster training time and less graph memory consumption than PINN on various geometries, by 35.8\\% and 62.7\\% respectively. These results validated that graph architectures and hierarchical loss weighting can substantially enhance performance of physics-informed machine learning models for fluid dynamic analysis with scalable resource usage.", "tldr": "", "keywords": ["physics-informed machine learning", "graph convolutional network", "data-free", "fluid dynamics", "flow field prediction"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e24d3e2230bd8018016b89c373457ba66aa8f010.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces PIGCN—a Physics-Informed Graph Convolutional Network—that replaces the dense fully connected layers of standard PINNs with graph convolutions to better capture local spatial coupling in fluid flows.\nPIGCN is designed for data-free learning (training purely on PDE residuals + boundary conditions) and adds a two-level adaptive loss-weighting scheme plus a global flux-conservation loss to stabilize and accelerate training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "I love this paper because it shows it can work on fluid problem in this way. Graph architectures offer mesh-based inductive bias, naturally encoding locality and connectivity—ideal for fluid systems with irregular domains and boundary layers."}, "weaknesses": {"value": "The study only validates PIGCN on steady, laminar, 2D channel flows with moderate Reynolds numbers.\n\nAblation study is not enough.\n\nJustification is not enough (improving PINNs is not necessarily a good motivation).\n\nDynamic Loss Weighting is Empirical and Heuristic\n\nThe two-level GradNorm scheme introduces multiple hyperparameters (learning rates, normalization constants) that require manual tuning.\n\nNo theoretical analysis or convergence guarantee is provided for stability when loss gradients conflict."}, "questions": {"value": "How small a GCN can be (still working)? \n\nCan it be solved without Adam?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "krN9ecP0vg", "forum": "dsIBpBJWhm", "replyto": "dsIBpBJWhm", "signatures": ["ICLR.cc/2026/Conference/Submission24161/Reviewer_TnTr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24161/Reviewer_TnTr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761590570869, "cdate": 1761590570869, "tmdate": 1762942967247, "mdate": 1762942967247, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work investigates the use of GCN (Graph Convolutional Network) in PINN (Physics-Informed Neural Network), in addition to applying adaptive loss-weight balancing to facilitate effective model training. The authors compared the proposed model, PIGCN, with the vanilla PINN in straight and L-shaped channels, both in supervised and unsupervised settings."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The authors investigated both supervised and unsupervised settings, providing insights into the experimental performance of the models."}, "weaknesses": {"value": "1. The motivation to focus on GCN is weak. The authors say “most PIGNN works were conducted on multi-layer perceptron (MLP)-based message-passing blocks, while only few preliminary works discussed the potential of graph convolutional network (GCN) despite its superior ability of learning local information by mixing a node feature with those of its neighbors,” but in terms of expressible power, GCN is weaker than the standard message-passing GNN. In addition, existing work[Gao+ Comput. Methods Appl. Mech. Eng. 2021] has already considered a PINN with GCN. Therefore, the authors should clarify the motivation and benefit of focusing on GCN rather than GNN.\n2. The writing is not clear enough. The definition of $L_{flux}$ is missing. In addition, they provided almost no information regarding how the differentiation to compute the residual loss $L_P$ is performed. Since GCN is not a point-wise network and computational points interact, it is not obvious how to perform automatic differentiation with respect to spatial variables. The authors should clarify the definition of $L_{flux}$ and the method for computing differentiation in GCN.\n3. Presentation of the results is difficult to follow. The authors should provide tables to summarize the results rather than writing numbers in the main text.\n4. The authors might misunderstand the implementation of PINN. The vanilla PINN[Raissi+ J. Comput. Phys. 2019] uses the pointwise MLP, so GCN should be slower than that because the GCN model is pointwise MLP + graph convolution (it is, though unclear because no code is provided and a less informative description of the model construction). However, they say that “PIGCN required only around 60-70% of the PINN’s wall-clock time,” which seems to be a contradiction, provided that the number of layers and channels are comparable. In addition, the paper states, “PIGCN has 60% fewer trainable parameters than the dense network,” implying that the authors mistakenly implemented the vanilla PINN using a fully connected MLP that takes all computational points. Also, according to Table 1, PINN is reported to take more memory than PIGCN, which supports the reviewer’s insight. Even if the implementation is correct, minibatch training enables us to control the memory consumption of PINN. Therefore, the authors should clarify the correctness of the implementation of PINN and the reasons for the (seemingly) contradictory results.\n5. The experimental evaluation is weak. The work uses only one baseline, vanilla PINN. At least, the authors should add PINNs with GNNs to demonstrate their practical superiority over GNNs. In addition, the authors used only one setting for PINN, whereas PIGCN uses various hyperparameters, making this a less fair comparison.\n6. The practical and/or theoretical value of the research is unclear. For the unsupervised setting, the authors should compare with classical numerical solvers if they wish to claim practical benefit. In the supervised setting, the authors should also outline the practical or theoretical benefits of the proposed method.\n\n\nMinor points:\n\n* The subscript of $L_\\mathrm{data}$ is in roman, but others are in italic. When it represents a word rather than the multiplication of multiple variables, the reviewer recommends using roman, e.g., $L_\\mathrm{flux}$.\n* p. 7: The reviewer cannot comprehend the meaning “=PINN”, maybe it is a typo?"}, "questions": {"value": "* The paper says, “While both models provided satisfactory prediction accuracy, data may not be available in many real-world cases, and it can limit generalization capability, which can require retraining of the model with any design changes,” but the reviewer cannot comprehend the meaning. Could the authors elaborate more on this point? Why does one condition require retraining and the other does not?\n* According to Equation 4, x and y are nondimensionalized by different factors, which may distort the space and render the equation invalid. How do the authors guarantee that this scaling is correct?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pEMP5awGU5", "forum": "dsIBpBJWhm", "replyto": "dsIBpBJWhm", "signatures": ["ICLR.cc/2026/Conference/Submission24161/Reviewer_1qMu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24161/Reviewer_1qMu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834070260, "cdate": 1761834070260, "tmdate": 1762942967006, "mdate": 1762942967006, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors enforce the governing physics law on the loss function of the graph neural network and solve the problem of flow in an L-shaped pipe. Visual comparison and error analysis have been shown to support the idea. The advantages of using graph neural networks over traditional MLPs in the PINNs framework are discussed. Computational time and GPU memory were considered as criteria for comparison between different methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1) Easy to read and understand\n2) High-quality figures\n3) Details of N.S. equations were presented.\n4) Figure 1 is illustrative."}, "weaknesses": {"value": "The combination of graph neural networks with PINNs is not novel and was presented for the first time 3 years ago (2022). Here is a link to the journal paper:\n\nPhysics-informed graph neural Galerkin networks: A unified framework for solving PDE-governed forward and inverse problems\n\nhttps://doi.org/10.1016/j.cma.2021.114502\n\nIt is surprising that the authors referenced this article in their manuscript; however, they claimed that that was a data-driven method. Of course, they used sparse data for solving inverse problems. In the current manuscript, the authors also used data, as we can see in the schematic Figure 2 ($\\frac{\\partial L_{data}}{\\partial w}$). I am surprised, as the title of the work is \"PHYSICS-INFORMED GRAPH CONVOLUTIONAL NETWORK FOR DATA-FREE LEARNING OF CHANNEL FLOW FIELDS\" and it contains the term \"data-free\". But in Figure 2 and Eq. 1 and Eq. 2, we see a component relevant to the data in the loss function.\n\nFor ICLR 2026, high-quality novel work is expected; given that this work is not new in terms of methodology, I cannot support it. The chosen test case is also not challenging and is even simpler than those solved in the above-mentioned journal paper. ICLR is a place for introducing novel frameworks and methodologies, not using a previously developed method to solve a new problem (or new application), which is not challenging.\n\n--> As a side note, the abstract is too long.\n\n--> There is no detail or even a high-level explanation of the graph neural network."}, "questions": {"value": "Thanks for submitting this work to ICLR 2026. However, I am sorry to say that I do not have any specific questions, as this work does not present any novel ideas. Please see the Weaknesses box above. A suggestion: Try to use the framework for turbulent flow in 3D, which is indeed challenging."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "03UpgJlcN3", "forum": "dsIBpBJWhm", "replyto": "dsIBpBJWhm", "signatures": ["ICLR.cc/2026/Conference/Submission24161/Reviewer_WuFk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24161/Reviewer_WuFk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840548648, "cdate": 1761840548648, "tmdate": 1762942966760, "mdate": 1762942966760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This authors propose a Physics-Informed Graph Convolution framework for solving 2D incompressible steady flow problems without observational/numerical simulation data. The proposed method uses graph convolutions to capture local spatial dependencies on irregular meshes. The authors also propose two-level dynamic loss weighting and a global flux-conservation loss to stabilize training and ensure global physical consistency. Experiments on rectangular and L-shaped channel flows show that PIGCN achieves better accuracy, faster training, and lower memory usage than PINNs."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper introduces a new Physics-Informed Graph Convolution method to address key limitations of traditional PINNs, such as poor scaling and difficulty in capturing localized interactions.\n\n2. The proposed method shows significant improvements in both accuracy and memory efficiency compared to baseline PINN."}, "weaknesses": {"value": "1. **Novelty claim**: The dynamic loss weighting has been proposed before in [5], albeit not on graph, but this undermines the novelty claim of the paper. I encourage the authors to articulate the novelty in contrast to [5] to better position the paper’s novelty. \n\n2. **Lack of strong baselines**: One significant issue is that the paper does not discuss and benchmark against recently proposed strong GNN-CFD baselines such as MeshGraphNet [1], CFD-GCN [2], BSMS-GNN [4] and FVGCN [3].\n\n3. **Lack of challenging benchmarks**: The datasets are quite simplistic in the sense  that objects of varying geometry in the flow field such as cylinder flow problem has not been considered. I urge the authors to consider such problems. \n\n4. **Computational cost may not be realistic on large-scale problem**: Did you compute the derivative of the various losses with respect to model parameters w using auto-differentiation (refering to Figure 2)? That means you need to track gradients at each node of the graph including the boundary nodes. This could be computationally memory-heavy on large-scale mesh, which is often the case for CFD. This computational burden can be observed in Table 1 where PIGCN consumes 28 GB GPU memory when dealing with only 144K nodes. This is not practical on large-scale CFD problems with large-scale mesh. \n\n5. **Lack of clarity**: How did you discretize the computation of $L_c, L_u, L_v, L_e$ given that the input is a graph?\n\n6. **Lack of discussion about limitations**: To my understanding, the method  has the limitation that it needs to be retrained when the flow conditions remain same except only the geometry of the object changes. Can you confirm if that is the case?\n\n7. **Stability of Loss coefficient**: Can you discuss and demonstrate the stability of the eight loss coefficients in Equation 2?\n\n8. **Figure ambiguity**: The stack of graphs in GCN block in Figure 2 is confusing. Please consider revising the figure.\n\n[1]  Learning Mesh-Based Simulation with Graph Networks, ICLR 2021\n\n[2] Combining Differentiable PDE Solvers and Graph Neural Networks for Fluid Flow Prediction, ICML 2020.\n\n[3] Finite Volume Features, Global Geometry Representations, and Residual Training for Deep Learning‑based CFD Simulation, ICML 2024. \n\n[4] Efficient Learning of Mesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph Neural Network, ICML 2023.\n\n[5] Dynamic Weight Strategy of Physics-Informed Neural Networks for the 2D Navier–Stokes Equations, Entropy 2022."}, "questions": {"value": "Rather than computing global flux conservation, was there a reason not to consider computing averaged local flux conservation at each cell, as done in finite volume? + Weaknesses 1, 4-7."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z7yGWGbyv8", "forum": "dsIBpBJWhm", "replyto": "dsIBpBJWhm", "signatures": ["ICLR.cc/2026/Conference/Submission24161/Reviewer_TeTx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24161/Reviewer_TeTx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24161/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762150257321, "cdate": 1762150257321, "tmdate": 1762942966053, "mdate": 1762942966053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}