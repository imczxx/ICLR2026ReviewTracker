{"id": "SUNC1eJGMr", "number": 14717, "cdate": 1758242397391, "mdate": 1763662558544, "content": {"title": "A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition", "abstract": "When language models are trained on textual data, they acquire both knowledge about the structure of language as well as knowledge of facts about the world.\nAt inference time, their knowledge of facts can be leveraged to solve interesting problems and perform useful knowledge work for users.\nIt is well known that language models can verbatim memorize long sequences from their training data.\nHowever, it is much less well understood how language models memorize facts seen during training.\nIn this work, we propose a new dataset to specifically empower researchers to study the dual processes of fact memorization and verbatim sequence memorization.\nThe dataset consists of synthetically-generated, webtext-like documents about fictional events, as well as question-answer pairs about the events.\nWe conduct training experiments showing how synthetic data about fictional events can be effective in teasing apart different forms of memorization.\nWe also document the challenges in effectively building realistic, fictional synthetic data.", "tldr": "We develop a synthetic dataset that enables us to study memorization and generalization of factual knowledge in LLMs.", "keywords": ["large language models", "memorization", "knowledge acquisition", "datasets"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3347a3b45e9d8b2f4c53671124012261bf6e92bb.pdf", "supplementary_material": "/attachment/32a4d16b2da4cc8de43ac16f4a56141dfdacfc16.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a synthetic data generation pipeline to produce a controlled pretraining testbed for factual knowledge acquisition. They construct a synthetic dataset with information on fictitious events, that matches the surface-level characteristics of general web data but contains entirely fictional facts. They use this testbed to investigate factual knowledge acquisition and generalization in LM pretraining, demonstrating in a controlled setup some interesting phenomena, such as better generalization with more diverse presentations of the text, and that exposure to both the underlying style and content of a document outperforms seeing only the style in perplexity-based metrics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- Disentangling stylistic and factual memorization from verbatim memorization is an important and understudied area, as the field aims to reduce model hallucination and improve LLM capabilities.\n- The paper is very well-written and clear.\n- The justification for the dataset and experiment setup is strong. The splits are carefully constructed and are a good demonstration of how to conduct careful knowledge acquisition work, which will be important in engineering better pretraining datasets and synthetic data."}, "weaknesses": {"value": "- Some of the contextualization with respect to related work is incorrect. For example, the statement \"No prior work has studied the training dynamics of LLMs acquiring generalizable knowledge of facts (i.e.  fact memorization)\" (line 92) is false, e.g., [1]. The authors later claim [1] is contemporaneous; it was published in NeurIPS 2024 10 months ago. Hence, this claim in particular should be calibrated. \n- Moreover the authors ought to review various synthetic data pretraining methods that demonstrated the phenomena of greater surface-form diversity improving generalization, such as [2] [3] [4] [5], given that one of the main successes of this work is to recapitulate this phenomena in a more carefully controlled setting.\n\n[1] Chang et al., How Do Large Language Models Acquire Factual Knowledge During Pretraining? NeurIPS 2024. https://arxiv.org/abs/2406.11813\n\n[2] Maini et al., Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling. https://arxiv.org/abs/2401.16380\n\n[3] Ben Allal et al., Cosmopedia: how to create large-scale synthetic data for pre-training. https://huggingface.co/blog/cosmopedia\n\n[3] Yang et al., Synthetic continued pretraining. https://arxiv.org/abs/2409.07431v2\n\n[4] Ruan et al., Reasoning to Learn from Latent Thoughts. https://arxiv.org/abs/2503.18866"}, "questions": {"value": "- Key Questions\n    - See Weaknesses above for some key aspects of related work where further contextualization would improve the paper.\n    - It is claimed that the data produced by the authors matches the surface-level statistics of internet text; could you provide some explicit measures for this?\n    - Can you comment further on why you observe the MCQ Acc for the val splits to go up in Figure 7? This suggests that even style alone is useful to doing well on the MCQ? Or do you suspect some leakage of factual content?\n- Clarifications\n    - Typo around line 255."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sAgJrUbwnQ", "forum": "SUNC1eJGMr", "replyto": "SUNC1eJGMr", "signatures": ["ICLR.cc/2026/Conference/Submission14717/Reviewer_dQfK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14717/Reviewer_dQfK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961691381, "cdate": 1761961691381, "tmdate": 1762925080717, "mdate": 1762925080717, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a new dataset, called FictionalQA for studying memorization dynamics and knowledge acquisition on LLMs. They focus in particular on different types of memorization; specifically: factual, verbatim, and stylistic. The authors also report a number of experiments that help validate the quality of their data and provide initial insight into how different types of information presentation in the training data transfer to recall."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper focused on a highly relevant problem.\n- The authors take proper care to validate the generated dataset (by e.g. testing against a webtext baseline), which is highly commendable and helps increase reliability of the obtained results.\n- Apart from a few typos, the paper is well-written and well-organized, and is a pleasure to read. I especially appreciated the clear \"bird's eye view\" diagram of the dataset on Fig.1 that covers both the overall structure and some specific examples.\n- I find the initial results about the discrepancy in what is optimal for verbatim memorization vs what is best for transfer quite intriguing."}, "weaknesses": {"value": "I found the paper quite strong, however, a few things could be improved.\n\nMy main concern is regarding the presentation. The paper is bucking the common trend of over-stating its results, and I appreciate it a lot. However, it goes a bit too far in the opposite direction. In my view, the paper slightly underplays its own contribution. This is especially evident in the \"conclusion\" section, which almost entirely lists the limitations and challenges rather than the (promising) results and good features of the proposed dataset.\n\nI truly appreciate the intellectual honesty, but it's important to highlight the paper's strengths in the conclusion. I.e. not just express the hope that the dataset will be used: \"we hope that our results inspire the use of our dataset for studying topics we do not explore such as machine unlearning and privacy preserving training methods\", but reiterate the key features of the dataset and why it's a great new tool for the research community (and I say it believing that indeed it is a great new tool).\n\nIn a similar vein, it might be good to clearly state the name of the dataset in the title and in the Abstract. For example,\n\"FICTIONAL Q&A: A DATASET FOR STUDYING MEMORIZATION AND KNOWLEDGE ACQUISITION\", not \n\"A FICTIONAL Q&A DATASET FOR STUDYING MEMORIZATION AND KNOWLEDGE ACQUISITION\". Because the dataset name is no clear in the latter version. And, for the abstract, adding something like \"we propose Fictional QA, a dataset for...\" can help to consolidate the dataset's name in the reader's mind. For example, I at some point had to Ctrl+F search \"FictionalQA\" to double-check that the authors actually use it as the dataset name and not just a part of the description.\n\nSame goes for the conclusion, major illustrations, etc.\n\n- Practical applicability. I believe that the paper would have been stronger (broader impact) if the authors proposed some way to incorporate the obtained insights into the design of actual LLM training sets. For example, perhaps a metric / method for expanding/augmenting brief factual statements encountered in the training data into a form that'd promote factual recall. It's more of a possible extension than a clear \"weakness\".\n\n- Mild inaccuracies and lack of depth in the discussion about the relationship to human cognition. \nI found this quote concerning in particular: \n\"This implies that the surface form that might allow a human to learn facts efficiently could be suboptimal for LLMs since they acquire factual knowledge through different mechanisms.\"\nActually, humans are not very adept at memorizing random lists of random facts. Context (informational, emotional, etc.) matters a lot during memorization and best learning/memorization outcomes occurs when important facts are presented with appropriate context. \n\nSo while the memorization mechanism in LLMs and Humans are indeed very different, the results obtained in the current paper are in some ways actually similar to human learning. I think it'd benefit the paper to add proper nuance to this discussion and clarify the claim.\n\nDespite these weaknesses, I believe that the paper is highly relevant, well-executed and is sufficiently novel to be accepted."}, "questions": {"value": "Questions (mostly following up on the weaknesses mentioned above)\n\n- Do you think your results can be extended to improve current practical LLM training?\n- Could you clarify your position on how your results relate to human learning/memorization dynamics?\n- You mentioned application of FictionalQA to studying unlearning, but could you clarify what exactly makes it good for studying it? For example, what are some specific directions in unlearning research that you find particularly promising and that FictionalQA uniquely allows or facilitates?\n\nTypos:\nLine 272: \"While the the dataset\" (double the)\nLine 481: \"a significant about of duplicate questions\" (a significant amount?)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ds7Zq8Riaw", "forum": "SUNC1eJGMr", "replyto": "SUNC1eJGMr", "signatures": ["ICLR.cc/2026/Conference/Submission14717/Reviewer_9BFP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14717/Reviewer_9BFP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762186773169, "cdate": 1762186773169, "tmdate": 1762925079289, "mdate": 1762925079289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents FictionalQA, a new synthetically generated dataset for studying memorization and knowledge acquisition in large language models. The dataset is built hierarchically, starting from fictional \"seeds\" which are expanded into structured \"fictsheets\" and then into diverse, webtext-like documents. The dataset also includes question-answer pairs tied to these documents. The authors use this dataset to conduct controlled finetuning experiments. Their key findings are that factual memorization and verbatim memorization exhibit different training dynamics. They also show that models acquire factual knowledge more effectively from diverse, naturalistic documents compared to simple, structured lists of facts. The paper provides a valuable new resource and a strong methodology for isolating and analyzing how LLMs learn new information."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The FictionalQA dataset itself is a significant contribution. The design principle of creating data that is factually disjoint from the real world but stylistically realistic is a powerful idea. This creates a controlled \"laboratory\" setting to study memorization without confounding variables from existing world knowledge.\n2. The experimental finding that models generalize factual knowledge better from diverse documents than from structured \"Fictsheets\" is insightful. This suggests that the surface form and diversity of data play a crucial role in knowledge acquisition, not just the repetition of the atomic fact itself.\n3. The experimental design is strong, particularly the use of different data splits like the \"Event Split\" and \"Doc Split\". This allows for a more nuanced analysis of how models generalize knowledge to new documents versus entirely new fictional events."}, "weaknesses": {"value": "1. The most significant weakness is the \"leaky\" generalization shown in Figure 7. The MCQ accuracy for held out validation events (Val) also increases significantly, even in the Event Split. The authors rightly note this makes it difficult to cleanly separate true factual memorization from stylistic memorization. The model may be learning the pattern of the fictional data or Q&A rather than just the atomic facts.\n2. The multiple choice question (MCQ) evaluation seems to have some issues. The baseline accuracy for the base models is well above the random chance (25% for 4 choices), suggesting that the distractor answers are not challenging enough or that the questions themselves leak information. This slightly complicates the interpretation of the accuracy gains.\n3. The experiments are limited to finetuning settings where the fictional data makes up 5% of the training mixture. It remains an open question whether these findings on knowledge acquisition would hold during large scale pretraining, where this data would represent a much smaller fraction of the total corpus."}, "questions": {"value": "See Weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YzkCbsqlYN", "forum": "SUNC1eJGMr", "replyto": "SUNC1eJGMr", "signatures": ["ICLR.cc/2026/Conference/Submission14717/Reviewer_hkhq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14717/Reviewer_hkhq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762262727722, "cdate": 1762262727722, "tmdate": 1762925078813, "mdate": 1762925078813, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FictionalQA, a dataset designed to study how language models memorize facts versus verbatim sequences during training. The key contribution is a set of curated fictional events, structured fact sheets, webtext-like documents in multiple styles, and associated question-answer pairs created via a publicly-available pipeline. The authors conduct fine-tuning experiments on various small models to demonstrate: (1) that verbatim memorization and factual generalization can be separated experimentally, (2) that different text formats (e.g., structured lists vs. diverse documents) affect knowledge acquisition differently, and (3) that improvements in Q&A performance may reflect both factual and stylistic memorization rather than pure fact learning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-motivated and addresses a valuable research question, in how LLMs acquire factual knowledge. The use of fictional information for studies of this sort is not novel, but is performed in a careful, transparent, and reproducible manner, which is valuable for future use. The dataset is applied in various experiments to demonstrate its utility at a limited scale, with interesting results. While the results are limited due to the realism of the test setting, the authors are aware of this and make a reasonable trade-off considering the computational costs of full-scale training efforts. The writing is clear throughout, and I particularly appreciated the use of footnotes to comment on interesting points."}, "weaknesses": {"value": "I’m confused by the claim in the paper that the main contribution is a pipeline for creating fictitious datasets. This does not seem like the main focus of the paper, with most of the time being spent on a particular dataset and conducting experiments. If the data generation pipeline were the main contribution, I would expect more time to be spent demonstrating the quality of the generated content relative to previous works. \n\nThe “leaky” generalization finding is quite troubling, with a lackluster explanation. The possibility suggested by the authors that this indicates the model learning from the distributional features indicates a serious potential weakness in the data generation process that makes it possible to identify the “true” ficts without seeing them, or a weakness in the MCQ process having a similar effect. (Perhaps McCoy, 2019. “Right for the Wrong Reasons” is suggestive of causes here).\n\nA visual inspection of the fictitious scenarios shown in the main figure makes me question the realism of the generated data. It would not be surprising to me if tagging a text with a future date (2040s) would change how it is embedded and learned to be quite different from real texts. Similarly, the sudden flower blooming stories sound like something a human would assume was fake. It would be helpful to see some sort of simple test to verify that the stories are not obviously fake, maybe a human eval, or a small classifier to separate the embeddings of the texts.\n\nFinally, an opinion not included in my rating – moving the limitations to an appendix is bad for academic discourse. Limitations are an important part of responsible science and belong with the main work. I would encourage you to include at least some of them in the main body if the paper is accepted."}, "questions": {"value": "1. On the \"leaky\" generalization (Figure 7): Have you analyzed the overlap in n-grams, entities, or semantic content between training and validation documents in the Event Split? Could you quantify how similar the ficts are in content despite the event-level separation?\n\n2. Do you have any concerns about dual-use of this pipeline? How would you address them? \n3. Have you considered using any sort of mechanistic interpretability methods on the weight changes from fine-tuning? Does the model seem to represent the new “facts” in a similar way to the other new facts being used in training? For this, you might want to use real facts which occurred since the models were pretrained as a control."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cwbQZEaT5u", "forum": "SUNC1eJGMr", "replyto": "SUNC1eJGMr", "signatures": ["ICLR.cc/2026/Conference/Submission14717/Reviewer_tcbD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14717/Reviewer_tcbD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762389239393, "cdate": 1762389239393, "tmdate": 1762925078235, "mdate": 1762925078235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a benchmarking approach to study factual vs verbatim memorization of LLMs. They do it via a synthetic dataset generation pipeline with fictional events to create Q/A pairs. They then use these pairs to fine-tune small models and show a variety of research insights."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Originality:\n\n- Novel idea for the data generation pipeline from fictional seeds to fictsheets, to documents then to Q/A\n- With different types of styles it enables interesting controllable ways to test memorization\n\nQuality:\n\n- Thoughtful experimental design with multiple data splits & as above makes for a nice controllable setup\n\nClarity:\n\n- The paper is well written and the method is very easy to follow. Everything is pretty clear from Fig 1\n- The code release and prompts will also help with clarity \n\nSignificance:\n\n- Clearly tackles an important problem of better understanding memorization in LLMs and provides a good resource for the community studying this."}, "weaknesses": {"value": "(1) Leakiness result in Fig 8 undermines the previous results and is buried towards the end. The models on the val set perform almost equally well as those that were trained on. The core premise of previous results was that one could cleanly separate factual memorization from verbatim memorization. However, this result shows models don’t learn verbatim (since the validation documents are unseen) and don’t learn atomic (which wouldn’t transfer anyway). Instead, are learning style or textual distribution properties.\n\nSo then surely this undermines the utility of the dataset for the purpose of isolating factual memorization. It’s still an interesting result but I’d encourage the authors to reframe the paper, since currently in this form it feels like this big issue is being buried since it doesn’t fit the narrative.  \n\n(2) Weird MCQ behaviour: at step 0 the models achieve 30-40% accuracy before seeing the fictional training data. Shouldn’t this be random at 25%. This implies maybe the questions are easy or it’s possible to easily eliminate bad options. A more through analysis of the MCQs would help answer this\n\n(3) Dataset quality: from the stated facts it seems like more than half the data was thrown away during de-duplication, this suggests limited diversity of the LLM generator. Doing some eval of the remaining data quality is then useful - especially given there so much duplication whether the LLMs are inspired by real-world data (or are slight variations of real world data). i.e. whether the data is really disjoint from real-world knowledge or is still inspired by events models know from pre-training."}, "questions": {"value": "(1) Can you characterize the cases or types of questions for which there might be the clean or leaky transfer? Is there any special properties about them?\n\n(2) Can you do some assessment of the quality of the data? Both on question quality and the MCQ point raised above\n\n(3) Would a diversity of models for data generation be better for diversity rather than just gpt-4o?\n\n(4) What are the key differences in insights beyond the data itself vs  Chang et al. 2024, Park et al. 2025, Zucchet et al. 2025"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "X3E0bTJrcm", "forum": "SUNC1eJGMr", "replyto": "SUNC1eJGMr", "signatures": ["ICLR.cc/2026/Conference/Submission14717/Reviewer_m9Sr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14717/Reviewer_m9Sr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission14717/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762452818302, "cdate": 1762452818302, "tmdate": 1762925077273, "mdate": 1762925077273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "(1/4) A deeper analysis of the data behind Fig. 7"}, "comment": {"value": "One goal with our dataset’s construction (eg. the Event Split) was to make it possible to identify whether or not models are more likely learning factual relationships atomically or general distributional properties of the fictional data. This determination can be made by checking whether they improve on only the questions associated with training documents, or whether they also improve on questions associated with the validation documents. Even from just Fig. 3 (right), it is already clear that some sort of distributional transfer learning is happening across “Event Split” lines, so seeing some amount of similar transfer between the associated training and validation MCQ examples in Fig. 7 is also expected. However, whether or not a specific model or training algorithm appears to learn more atomically or more distributionally is the actual empirical question at hand. The whole point of developing our dataset and producing our results is to simply report the observations of what appears to be happening using our data as the probe.\n\nWith that as context, in response to a variety of similar questions from each of the reviewers we have performed some additional analyses of the data behind Fig. 7. In particular, we have analyzed the sample-wise performance of the 8B model on the training split’s MCQs and the validation split MCQs and compared successes and failures at step 0 of training to those at the final step of training. The goal of this analysis is to try and discern whether there are any particular defining characteristics of the validation question subset that the 8B model “learned” over the course of training, and whether there is any specific distributional link between the training questions the model learns during training, versus the validation questions that it simultaneously improved on.\n\n### A note on duplicated answers from distinct questions\n\nWe note that our question filtering process deduplicated the questions only on the question text, and not the answer itself. As an example, from two different documents based on the same seed event, we can see that two questions were produced that both have the answer “Lake Ypsilon”. \n```\nquestion_id: event_000_style_corporate_num_000_question_003\nQuestion: Where was the first pilot test of the Ring of Silence Protocol conducted?\nAnswer: Lake Ypsilon\n\nquestion_id: event_000_style_encyclopedia_num_000_question_002\nQuestion: Where was the sound-absorbing moat first implemented?\nAnswer: Lake Ypsilon\n```\nHowever this is an artifact of various documents and their questions ending up referring to shared concepts in the same seed event even though the way in which the question is posed (i.e. the specific fact pattern) is slightly different.\nIn the subsequent analyses we continue to present the questions as subselected for experiments in the main paper for continuity. During the analysis we also checked whether this additional deduplication seemed to change any of the statistics meaningfully, but did not observe any evidence of that."}}, "id": "nPMRCCjhij", "forum": "SUNC1eJGMr", "replyto": "SUNC1eJGMr", "signatures": ["ICLR.cc/2026/Conference/Submission14717/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14717/Authors"], "number": 10, "invitations": ["ICLR.cc/2026/Conference/Submission14717/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763661409385, "cdate": 1763661409385, "tmdate": 1763661409385, "mdate": 1763661409385, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}