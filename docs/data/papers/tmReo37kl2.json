{"id": "tmReo37kl2", "number": 20701, "cdate": 1758309170956, "mdate": 1759896963207, "content": {"title": "IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch", "abstract": "Reliable autoformalization remains an elusive goal even in the era of large language models (LLMs). Even the best LLMs struggle to translate natural language into formal constructs in languages like Lean. High-quality data has been a key bottleneck given the resource costs associated with manual curation and validation of these translations. On these lines, we introduce IndiMathBench, a human-verified benchmark designed to evaluate mathematical theorem proving, curated using an AI-powered human-assisted pipeline for formalizing natural language problems in Lean. IndiMathBench is composed of 416 formal Lean4 theorems paired with their corresponding informal problem statements, sourced from Indian Mathematics Olympiads. Our pipeline helps synthesize multiple candidate formalizations from an ensemble of LLMs, validating them using Lean prover, and finally summarizing results for the human validators through an interactive dashboard. This dashboard enables efficient validation and repair, while also capturing valuable human code editing data. We analyze the performance and failures of several state-of-the-art models through our pipeline while releasing IndiMathBench and human code editing analysis to facilitate further research on automated theorem proving.", "tldr": "", "keywords": ["autoformalization", "automated theorem proving", "human-ai collaboration", "benchmark", "lean"], "primary_area": "neurosymbolic & hybrid AI systems (physics-informed, logic & formal reasoning, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/821d3dc1d2d7c06b576941874840192768d46b0a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This research introduces IndiMathBench, a new benchmark for automatic theorem proving. It consists of 312 mathematical problems sourced from the Indian Mathematical Olympiad, each paired with a human-verified formalization in the Lean 4 theorem prover.\n\nThe benchmark was created using a novel AI-powered human-assisted pipeline. In this process, researchers first utilized an ensemble of LLMs with a category-based retrieval and self-correction loop to generate initial formalizations. Human experts then validated and corrected these AI-generated proposals through an interactive dashboard. Several ablation studies and comparative studies are designed to demonstrate the effectiveness of this pipeline. To facilitate collaboration between humans and AI for Lean annotations, the researchers also developed a Visual Studio Code extension aimed at improving the efficiency of the formalization process."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The formalization of the problems presented in the paper is of good quality, especially the clever formalization of the three integer roots in the problem inmo_2017_2 in appendix A.\n2. The pipeline of this paper has indeed improved the annotation efficiency by 2-3 times that of human experts, which is a certain improvement.\n3. It is original and has engineering value. It transforms an AI-assisted formalization framework into a user-friendly VSCode extension, which may be useful for the development of the Lean community."}, "weaknesses": {"value": "The two main contributions of the paper, the **formal benchmark** itself and the **autoformalization pipeline**, both have certain weaknesses.\n\nRegarding the formal benchmark itself,\n\n1. The benchmark is not substantially different from the existing IMO benchmark, and its coverage remains almost identical. Other formal benchmarks at the high school competition level, such as miniF2F [1] and Fimo [2], already exist.\n2. Concerning the issue of data contamination with MiniF2F and IMO, I believe that all public competition problems have comparable likelihood of having online solutions, while formalized solutions are generally unavailable. In this regard, IndiMathBench does not demonstrate a clear advantage. Additional analysis or empirical evidence is needed to support the claim that IndiMathBench can effectively mitigate data contamination.\n3. For the evaluation of this benchmark, I recommend conducting further experiments. First, for the formal theorem proving task, it would be more convincing to evaluate state-of-the-art theorem provers, such as Deepseek-prover-v2 [3], Kimina-prover [4], and Goedel-prover-v2 [5], with increased sampling budgets to obtain more robust results. Second, the experiments in Section 5.2 only involve syntax checking rather than semantic checking. Incorporating semantic checking would enhance the soundness of the evaluation.\n\nRegarding the autoformalization pipeline:\n\n1. Each component of the pipeline has stronger existing alternatives. For example, iterative refinement and LLM judges have already been adopted in recent works like Kimina-Autoformalizer [4]. The category-based retrieval method relies on static documentation, which is not sufficiently comprehensive. In the “Knowledge base for Mathlib” section of Figure 2, the group-related results shown are irrelevant to the intended topic.\n2. This work should also include a comparison with existing autoformalizers (e.g., Kimina-Autoformalizer, Goedel-Formalizer-V2 [5], and Herald [6]) on InMO and RMO benchmarks to better contextualize its contribution.\n3. In the ablation study corresponding to Table 3, the KB and FB components are evaluated together. Conducting additional experiments to separately measure the individual contributions of KB and FB would yield a clearer understanding of their respective effects.\n\n[1] Zheng, K., Han, J. M., & Polu, S. (2021). Minif2f: a cross-system benchmark for formal olympiad-level mathematics. arXiv preprint arXiv:2109.00110.\n\n[2] Liu, C., Shen, J., Xin, H., Liu, Z., Yuan, Y., Wang, H., ... & Liu, Q. (2023). Fimo: A challenge formal dataset for automated theorem proving. arXiv preprint arXiv:2309.04295.\n\n[3] Ren, Z. Z., Shao, Z., Song, J., Xin, H., Wang, H., Zhao, W., ... & Ruan, C. (2025). Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801.\n\n[4] Wang, H., Unsal, M., Lin, X., Baksys, M., Liu, J., Santos, M. D., ... & Li, J. (2025). Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. arXiv preprint arXiv:2504.11354.\n\n[5] Lin, Y., Tang, S., Lyu, B., Yang, Z., Chung, J. H., Zhao, H., ... & Jin, C. (2025). Goedel-prover-v2: Scaling formal theorem proving with scaffolded data synthesis and self-correction. arXiv preprint arXiv:2508.03613.\n\n[6] Gao, G., Wang, Y., Jiang, J., Gao, Q., Qin, Z., Xu, T., & Dong, B. (2024). Herald: A natural language annotated lean 4 dataset. arXiv preprint arXiv:2410.10878."}, "questions": {"value": "1. Why does the Masked Candidates method run at roughly twice the time cost compared to the Full system?\n2. How many human expert annotators contributed to the benchmark, and what are their academic or professional backgrounds?\n3. How well does the proposed pipeline generalize to non-competition problems, such as standard undergraduate-level mathematics problems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SkVMo8IhLt", "forum": "tmReo37kl2", "replyto": "tmReo37kl2", "signatures": ["ICLR.cc/2026/Conference/Submission20701/Reviewer_empU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20701/Reviewer_empU"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713975130, "cdate": 1761713975130, "tmdate": 1762934079152, "mdate": 1762934079152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents IndiMathBench, an automated theorem proving benchmark dataset that has been verified by human experts. The dataset is constructed using a formalization approach where AI plays the primary role and humans assist, with problem statements sourced from the Indian Mathematics Olympics, comprising a total of 312 problems. The authors propose an autoformalization pipeline featuring a category-based retrieval mechanism and a self-debug loop integrated with Lean, where multiple LLMs generate several formalization candidates. Additionally, the paper implements a VS Code Extension, and dashboard designed to accelerate the data annotation process. The study also provides a comparative analysis of the capabilities of several state-of-the-art models in formalizing problems from the Indian Mathematics Olympics."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. IndiMathBench covers problem types such as set theory, combinatorics, and geometry that are not typically included in existing ATP benchmark datasets (like miniF2F). This diversity enhances the practical value of the dataset as a comprehensive ATP evaluation resource.\n\n2. The proposed VS Code dashboard holds considerable potential for accelerating data annotation in this field. As shown in Figure 4 of the paper, the pipeline seems to significantly reduce the time required to annotate formal mathematical statements. The idea of allowing human annotators to reuse parts across multiple formalization candidates to improve labeling efficiency is both interesting and sensible.\n\n3. The analysis of formalization capabilities across various models, including state-of-the-art ones such as Claude Sonnet 4, Claude Opus 4, and OpenAI o3 High, provides valuable insights for selecting LLMs in future autoformalization work.\n\n4. The use of the relatively recent Lean 4.22 version ensures that the dataset remains relevant and aligned with current formalization tool environments."}, "weaknesses": {"value": "1. The proposed autoformalization pipeline is not truly novel; rather, it largely applies existing techniques to the Indian Mathematics Olympics dataset with some engineering optimizations. Specifically, the idea of a self-debug loop using error messages from the Lean compiler has appeared in prior works [1]; the concept of human feedback-assisted adjustment has been explored in earlier work [2]; and the use of retrieval from mathlib to enhance autoformalization capability is also present in previous research [3].\n\n2. The AUTOMATED THEOREM PROVING EVALUATION section relies on general-purpose LLMs for evaluation and lacks benchmark results from domain-specific automated theorem provers. This choice is unusual and not best practice within the field. Evaluating with general-purpose frontier LLMs alone typically yields poor results in theorem proving; at minimum, inclusion of specialized open-weight provers such as Deepseek Prover V2 or Goedel Prover v2 is needed. Given that Deepseek-Prover is cited in the paper (Line 100), the absence of its evaluation here is a notable omission. As a result, the experimental data in Table 4 are insufficient to convincingly demonstrate the dataset’s difficulty.\n\n3. The paper applies a static context retrieval approach per problem category, rather than generating retrievals tailored to each individual query as is common in typical RAG methods. Lines 239 (“There is one static context retrieved for each category of problems”) and Line 256 acknowledge the static nature of retrieval. Since each category may encompass numerous concepts (especially given the dataset is divided into only four broad categories; for example, the algebra category likely covers a wide range of prerequisite knowledge), employing a single static context per category potentially limits the pipeline’s generalizability and may restrict successful formalization to contents covered by the static context.\n\n4. The writing contains several minor errors that, although not obstructing readability, give the impression of hastiness. The issues noted include:\n\n   - Inconsistent spelling of “Lean4” and “Lean 4”; these should be standardized (e.g., Lines 18, 53, 58, and Figure 1 caption).\n   - Inconsistent casing of “MINIF2F” and “miniF2F” across the manuscript (e.g., Lines 75, 100, 162, 184).\n   - Possible typo “Wxe” on Line 53.\n   - “Visual Studio” on Line 60 should be corrected to “Visual Studio Code.”\n   - Missing appendix references on Line 263.\n   - Missing figure number citation on Line 295.\n   - Caption font size for Figure 5 is too small and difficult to read.\n   - Table 2 is placed below Tables 3 and 4, which is somewhat confusing.\n\n   [1] Lu, J., Wan, Y., Liu, Z., Huang, Y., Xiong, J., Liu, C., Shen, J., Jin, H., Zhang, J., Wang, H., Yang, Z., Tang, J., & Guo, Z. (2024). *Process-Driven Autoformalization in Lean 4* (No. arXiv:2406.01940). arXiv. https://doi.org/10.48550/arXiv.2406.01940\n\n   [2] Liu, C., Shen, J., Xin, H., Liu, Z., Yuan, Y., Wang, H., Ju, W., Zheng, C., Yin, Y., Li, L., Zhang, M., & Liu, Q. (2023). *FIMO: A Challenge Formal Dataset for Automated Theorem Proving* (No. arXiv:2309.04295). arXiv. https://doi.org/10.48550/arXiv.2309.04295\n\n   [3] Liu, Q., Zheng, X., Lu, X., Cao, Q., & Yan, J. (2024, October 4). *Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach*. The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=hUb2At2DsQ"}, "questions": {"value": "1. As noted in Weakness 2, my primary concern is why the work does not include evaluations using domain-specific automated theorem provers, given that this dataset is intended as an automated theorem proving evaluation set and that utilizing such provers is standard practice in the ATP field. Could the authors clarify the reasoning behind this omission?\n\n2. In constructing the autoformalization pipeline, the paper employs an LLM-as-a-judge approach to assess semantic alignment. However, when comparing the autoformalization capabilities of various frontier models later on, the evaluation does not seem to leverage the LLM-as-a-judge method (BEq and GTED only). Considering that LLM-as-a-judge has gained widespread adoption in the autoformalization domain [4], I am curious about how these models would perform if evaluated under this criterion. For example, which models tend to rank highest in zero-shot settings? What are the pass rates for each model? Additional details along these lines would be highly valuable.\n\n3. Line 396 mentions that Claude Sonnet 4 was used to compute BEq instead of following the original BEq implementation, but the paper does not explain the reason for this choice. Could the authors elaborate on why the model was replaced and what motivated this decision?\n\n[4] Ying, H., Wu, Z., Geng, Y., Wang, J., Lin, D., & Chen, K. (2024). Lean workbook: A large-scale lean problem set formalized from natural language math problems. *Advances in Neural Information Processing Systems*, *37*, 105848-105863."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gxtqvVPeWR", "forum": "tmReo37kl2", "replyto": "tmReo37kl2", "signatures": ["ICLR.cc/2026/Conference/Submission20701/Reviewer_f4fs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20701/Reviewer_f4fs"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727407320, "cdate": 1761727407320, "tmdate": 1762934078192, "mdate": 1762934078192, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new benchmark called IndiMathBench, designed to evaluate autoformalization capabilities. The benchmark comprises 312 problems from the Indian Mathematical Olympiad and their corresponding formal statements manually verified in Lean 4. The authors designed and employed an AI-assisted, human-in-the-loop workflow to construct the benchmark, which integrates retrieval based on categorical knowledge, a self-debugging loop using feedback from a symbolic verifier, and a multi-model ensemble. Furthermore, the paper releases a VS Code plugin to enhance annotation efficiency and evaluates the performance of current leading large language models on IndiMathBench."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "*   **Valuable Dataset Contribution:** Creating a high-quality, expert-verified dataset of formalized mathematics is, in itself, a beneficial contribution to the community. The release of IndiMathBench provides researchers with a new testbed that is potentially \"uncontaminated\" by existing models.\n*   **Detailed Annotation Efficiency Study:** The paper presents a time-cost comparison of different annotation workflows (manual-only, multi-model assisted, full system) in Figure 4. This provides quantitative evidence for the efficiency gains of AI-assisted formalization annotation, which is a concrete and meaningful analysis.\n*   **Open-Source Tools:** The authors have open-sourced both the dataset and the VS Code plugin used for annotation. This practice is commendable and helps promote the synergistic development of tools and data in the field."}, "weaknesses": {"value": "1.  **Lack of Novelty in the Core Contribution (Benchmark Creation):** The paper's main contribution is a new autoformalization benchmark. However, in the domain of competition mathematics, similar formalization benchmarks (e.g., MiniF2F, Putnam-Bench) are already abundant and continue to grow. The methodology used to create IndiMathBench—formalizing math competition problems—is essentially a straightforward migration of existing workflows and lacks methodological innovation. The paper claims the benchmark can mitigate data contamination, but this is not a fundamental solution, and it's questionable whether it's even a temporary one. The research community could easily apply existing workflows to any other country's or region's math competitions, and update them annually, to create \"new,\" uncontaminated benchmarks in the same manner. Therefore, the innovative value and barrier to entry for this work are relatively low.\n\n2.  **Evaluation Scope is Too Narrow and Fails to Reflect the State of the Art:** Research in autoformalization typically goes beyond evaluating the single-pass generation capability of individual general-purpose LLMs (like GPT-5 or Claude). More cutting-edge and impactful work evaluates a complete autoformalization **system** (e.g., DeepSeek-Prover or the framework described in \"Draft, Sketch, and Prove\"). These systems often integrate sophisticated retrieval mechanisms, multi-step reasoning strategies, iterative interaction with proof assistants, and more refined feedback loops. The experimental design in this paper, which directly tests base models, makes the evaluation results likely unrepresentative of the true capabilities of current autoformalization technology, limiting the depth and impact of its conclusions.\n\n3.  **Confusing Writing Structure; Contributions are Scattered and Their Originality is Unclear:** The paper's contributions and arguments appear scattered, making it difficult for readers to discern the authors' core original contributions. The introduction lists the benchmark, the VS Code plugin, a formalization framework (KB+FB), and model analysis as parallel contributions. However, the paper does not clearly argue for the originality of the annotation efficiency study, the human-computer interaction panel, or the knowledge-base and feedback-driven (KB+FS) system. I did not see a detailed comparison and discussion of these auxiliary systems against existing work. To my knowledge, similar assisted annotation systems and frameworks that use compiler feedback for code optimization are already widely used in the autoformalization field. Due to the lack of comparison with related work, I find it difficult to consider these components as independent, original contributions of this paper, which significantly weakens its overall weight. If these are indeed original works, the authors should highlight and clarify this in the introduction and related work sections.\n\n4.  **Chosen Evaluation Metrics Have Credibility Issues:** The paper relies on two core metrics, BEq and GTED, to assess model translation accuracy, but the validity of these metrics is not sufficiently justified and may not accurately reflect the models' true capabilities.\n    *   **Reliability of BEq (Bidirectional Equivalence) is Questionable:** The success of BEq depends not only on whether two formal statements are logically equivalent but also **highly on the performance of its internal prover**. A failed proof can result from two causes: either the statements are truly not equivalent, or they are equivalent but the prover is not powerful enough to find a proof within the search space. The prover used in this paper is itself LLM-driven and has limited capabilities. Thus, the BEq results confound the variables of \"translation quality\" and \"proving capability,\" which may bias the measurement results.\n    *   **Effectiveness of GTED (Generalized Tree Edit Distance) Lacks Justification:** GTED is a metric for syntactic similarity. Whether it is a good proxy for semantic correctness in formal mathematics requires rigorous justification. In formal languages, minor syntactic differences can lead to vast semantic gaps, while two semantically equivalent statements can have completely different syntactic tree structures. The paper does not provide sufficient evidence to support the reasonableness of GTED as a core evaluation metric.\n    *   **Lack of Data Support:** In Section 5.1, the paper claims to \"employ two complementary evaluation metrics that have demonstrated high inter-annotator agreement with human evaluations.\" However, the paper **provides no quantitative data** (e.g., correlation coefficients, Kappa scores) to support this crucial claim. This severely undermines the credibility of the entire evaluation framework. If sufficient data support were provided, the credibility of using BEq and GTED would be significantly enhanced."}, "questions": {"value": "1.  Why did the experimental section choose to directly evaluate general-purpose large language models instead of a **complete autoformalization system** that integrates retrieval, multi-step reasoning, and verification feedback, as seen in works like DeepSeek-Prover or Reaper?\n2.  The paper lists the benchmark, the VS Code plugin, and the formalization framework as contributions. Regarding the latter (e.g., the annotation efficiency study, the HCI panel, the KB+FS workflow), are they intended as auxiliary tools for benchmark construction, or as independent, generalizable methodological innovations? If the latter, could the authors please elaborate on their originality in the introduction and related work sections and provide a clear comparison with existing human-in-the-loop annotation systems or code generation frameworks?\n3.  Regarding the validity of the evaluation metrics:\n    *   For **BEq**, how do the authors disentangle cases of proof failure caused by \"formalization translation errors\" from those caused by \"inadequate prover capability\"? Does this confounding factor affect the fairness of the model rankings?\n    *   For **GTED**, could the authors provide a stronger argument as to why syntactic tree edit distance can serve as a reliable indicator of semantic correctness in the domain of formal mathematics?\n    *   The paper claims the chosen metrics have high agreement with human evaluations. Could you please provide **specific quantitative data** to support this claim?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "f5HuHSS3Z7", "forum": "tmReo37kl2", "replyto": "tmReo37kl2", "signatures": ["ICLR.cc/2026/Conference/Submission20701/Reviewer_uGPL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20701/Reviewer_uGPL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761730167409, "cdate": 1761730167409, "tmdate": 1762934077676, "mdate": 1762934077676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces IndiMathBench, a new benchmark for evaluating autoformalization, the translation of natural mathematical text into formal Lean4 theorems. The dataset contains 416 human-verified Lean4 formalizations paired with natural-language problem statements drawn from the Indian Mathematics Olympiads. To generate these, the authors employ a human-AI hybrid pipeline: multiple LLMs propose candidate formalizations, which are validated through the Lean prover, then summarized for human verification via an interactive VS Code dashboard. The pipeline also records human repair and edit traces, released along with the benchmark to support further study. Experiments evaluate current LLMs on IndiMathBench and show that even top models can solve only a single theorem, suggesting the benchmark’s difficulty."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely and relevant resource: the paper offers a curated Lean4 benchmark that fills a data gap for formal mathematics research, especially given recent interest in LLM-based theorem proving.\n\n2. Human-in-the-loop pipeline: it gets human involved in multi-LLM synthesis, Lean validation, human repair cycle, and is thoughtfully designed and practically useful for future dataset building.\n\n3. Open release commitment: Providing both dataset and dashboard promotes reproducibility and community adoption."}, "weaknesses": {"value": "1. Weak experimental insights: evaluation simply shows that current LLMs fail badly, but there's a shortage of deeper analysis (e.g., failure types, linguistic vs. logical errors, success conditions).\n\n2. Need more justification in reliability and feasibility of the benchmark: reporting that all models fail is not very informative without breakdowns or ablations. Besides, more theoretical proof or imperical evidence (at least human insight or so?) will be beneficial to make the benchmark more attractive to a wider range of audiences.\n\n3. Figures mostly about interface: much of the visual content is about the dashboard UI, not empirical findings or benchmark properties."}, "questions": {"value": "1. Validation: How do you ensure correctness and diversity across the 416 problems? Are they balanced by topic, proof type, or theorem complexity?\n\n2. Insights: Beyond \"number of problems solved\", can you provide qualitative failure analysis, such as, where models succeed or fail, and why?\n\n3. Scalability: How efficient is the human-AI curation loop? Could it realistically scale to thousands of problems, or is it constrained by manual review costs?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lLhotI0t0F", "forum": "tmReo37kl2", "replyto": "tmReo37kl2", "signatures": ["ICLR.cc/2026/Conference/Submission20701/Reviewer_k9Sf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20701/Reviewer_k9Sf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761800995378, "cdate": 1761800995378, "tmdate": 1762934077182, "mdate": 1762934077182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}