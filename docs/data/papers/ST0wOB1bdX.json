{"id": "ST0wOB1bdX", "number": 14825, "cdate": 1758244395224, "mdate": 1759897347149, "content": {"title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models", "abstract": "Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including the integration of target-language reasoning datasets. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to nine times smaller, and its effectiveness is further confirmed through extensive ablation studies. We will release our models and datasets publicly upon acceptance.", "tldr": "We introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages.", "keywords": ["reward model", "reasoning", "rubric"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90ffce0bcb48f10221de0f41ef10fa8c0dac044f.pdf", "supplementary_material": "/attachment/082f661f9a307b1e6174f46657fd11b84c82de3a.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces MR3, a multilingual, rubric-agnostic reward reasoning model trained on 72 languages. It studies multilingual dataset curation, curriculum strategies, and the role of language in rubrics, reasoning, and evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Impressive language coverage and strong results across diverse multilingual benchmarks.\n- Thorough study on how instruction and reasoning language affect performance.\n- Clear, well-organized methodology and strong experimental validation.\n- Open-sourcing of models and datasets is valuable for the community."}, "weaknesses": {"value": "- Gains are minuscule and not clear if significant, contribution is not novel enough.\n- Heavy reliance on GPT-generated rubrics and reasoning may limit true novelty.\n- Rubric generation pipeline and translation details could use stronger validation.\n- Limited exploration of low-resource languages beyond aggregate results.\n- Analysis could go deeper into why certain multilingual strategies work."}, "questions": {"value": "- How well does MR3 generalize to unseen or code-switched languages?\n- Could multilingual reasoning data generated by smaller models reduce reliance on large teachers?\n- Any evidence that MR3’s reasoning traces improve human interpretability in practice?\n- How robust is MR3 to noisy or inconsistent rubrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DGcXofj6zN", "forum": "ST0wOB1bdX", "replyto": "ST0wOB1bdX", "signatures": ["ICLR.cc/2026/Conference/Submission14825/Reviewer_hiQ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14825/Reviewer_hiQ8"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761726929755, "cdate": 1761726929755, "tmdate": 1762925175499, "mdate": 1762925175499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces mR3, a family of massively multilingual reward models designed to address the performance gap in using large language models as automated evaluators for non-English text. To train mR3, the authors collected a diverse dataset covering 72 languages. The authors demonstrate that mR3 achieves state-of-the-art performance in multilingual reward modeling benchmarks, with significantly fewer parameters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The research topic is interesting and relevant.\n2. The authors attached their code in the supplementary material and claim to release the dataset and models upon publication, which promotes reproducibility. The dataset could be a significant contribution to the research community."}, "weaknesses": {"value": "1. The paper lacks scientific findings and insights. It seems more like an engineering effort to build a multilingual reward model using supervised fine-tuning, rather than a research paper that advances understanding in the field. For instance, the authors did not provide any evidence to support their decision regarding curriculum learning. Another example is that the authors did not explain where the improvements come from. \n2. The paper is not well-written and hard to follow. The authors conduct a plethora of experiments, which is appreciated, but fail to provide clear organization and explanations, making it difficult to grasp the key contributions and results. Lots of important details are buried in the appendix.\n3. The evaluation methodology is potentially flawed. While it is understandable that most existing benchmarks are translated from English, which is beyond the authors' control, it raises the question of whether the improvements on these reward modeling benchmarks truly reflect human preferences in non-English languages. To strengthen the evaluation, the authors could consider conducting human evaluations in multiple languages to validate the model's performance. Additionally, the inclusion of RewardBench and IndoPerf in the evaluation is somewhat problematic, as these benchmarks are limited to English and Indonesian, respectively. If we treat all languages equally, including these benchmarks in the evaluation of a multilingual model is likely to bias the overall results towards English and Indonesian, making the model's performance appear better than it actually is in other languages.\n4. mR3 is a family of reward models, but the authors did not use them as the reward model to train any multilingual LLMs. It would be interesting to see how effective mR3 is in training multilingual LLMs compared to other reward models."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FZV6GuC7Vq", "forum": "ST0wOB1bdX", "replyto": "ST0wOB1bdX", "signatures": ["ICLR.cc/2026/Conference/Submission14825/Reviewer_oUA2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14825/Reviewer_oUA2"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761739585494, "cdate": 1761739585494, "tmdate": 1762925174716, "mdate": 1762925174716, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MR3, a suite of massively multilingual, rubric-agnostic reward reasoning models supporting 72 languages. The work describes a unified framework for building, training, and evaluating rubric-based multilingual reward models, along with an associated new benchmark, data curation pipeline, and extensive comparative experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. MR3 tackles reward model evaluation at an unprecedented language scale, reportedly supporting 72 languages.\n2. The system's ability to support task- and rubric-agnostic scoring, including point-wise, pair-wise, and binary evaluation."}, "weaknesses": {"value": "1. Much of the data pipeline (translation, generation, and filtering) relies heavily on LLMs such as GPT-4.1 and GPT-OSS. This introduces potential risks of learning artifacts or biases specific to these models, particularly in low-resource settings.  \n2. Although the dataset covers a wide range of languages, the evaluation of reasoning faithfulness and quality (Table 4) depends on LLMs primarily trained in English,  even for \"target\" language reasoning."}, "questions": {"value": "1. Given the strong dependence on LLM-based translation and rubric generation, what measures are taken to ensure robustness against model-specific artifacts or spurious correlations in the data pipeline?  \n2. Were any steps taken to detect or mitigate potential data leakage, especially considering overlaps and deduplication between MMMLU and INCLUDE? How might such leakage influence the reported results?  \n3. Can the authors provide more qualitative examples of reasoning traces—particularly contrasting high- and low-resource languages—to better illustrate observed strengths and weaknesses?  \n4. What further analysis can be provided on model performance and reasoning faithfulness in genuinely low-resource languages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PMhPDac97o", "forum": "ST0wOB1bdX", "replyto": "ST0wOB1bdX", "signatures": ["ICLR.cc/2026/Conference/Submission14825/Reviewer_XSh3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14825/Reviewer_XSh3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761751666948, "cdate": 1761751666948, "tmdate": 1762925174252, "mdate": 1762925174252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new multilingual rubric-based reward model. The model evaluates an input based on a provided rubric and generates a reasoning trace, an explanation, and a response quality score.\n\nTo train the model, the authors create a multilingual dataset covering 72 languages. The dataset is curated based on existing sources (Human Arena Preference, HelpSteer3-Preference, MMMLU, HumanEval-XL, MATH-500 Multilingual, PolyGuardMix). For datasets without rubrics, rubrics are automatically generated in English using GPT-4.1. Reference outputs are generated using GPT-OSS-120B. Examples that GPT-OSS-20B can solve correctly are discarded and the data is downsampled to 100k examples. Qwen3 models are then fine-tuned on the data using SFT with a curriculum of easy-to-hard examples. The fine-tuned model outperforms larger models such as GPT-OSS-120B and prior multilingual RMs.\n\nThe authors perform further analyses of the impact of the instruction and reasoning language (English works best) and of the reasoning faithfulness (factual correctness and logical coherence according to GPT-5-mini)."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The data construction process is extensive; the dataset combines multiple sources, covers many languages, and goes through additional filtering steps. Authors also perform human validation of GPT-4.1-generated rubrics, which is appreciated.\n    \n2. Table 1 is helpful as a comparison table to contextualize the proposed RM with regard to prior (mostly English-based) work.\n    \n3. The analyses related to the instruction/reasoning trace language including the use of translation are insightful.\n    \n4. The model is compared on an array of different datasets with various baselines."}, "weaknesses": {"value": "1. The dataset construction pipeline consists of many components but only the impact of the instruction/reasoning trace language is ablated. It would be important to ablate the other components (original set of sources, synthetically generated rubrics, difficulty filtering, final data selection, etc) to understand which parts are key and which parts could be removed.\n    \n2. Many of the existing reward models use a weaker base model and/or a weaker teacher model. For instance, Nemotron is based on Llama 3.3 and M-Prometheus is based on Qwen2.5. So the comparison is not really apples-to-apples. A comparison with a weaker base model (such as Llama 3.3) as well as using a weaker teacher model would be very important to understand whether the proposed dataset construction is superior to prior work or whether most of the gain comes from switching to a stronger teacher/base model.\n    \n3. Point-wise, pair-wise, and binary evaluation formats are introduced initially but it’s not further discussed how these should be balanced against each other and how each can be strengthened.\n    \n4. The proposed approach only leverages SFT while prior models such as Nemotron-Multilingual also employed RL. Demonstrating the effectiveness of the proposed approach compared to an RL paradigm would be important."}, "questions": {"value": "1. Curriculum training hasn’t been very effective in the past. What are the differences in performance that you observed with your different curriculum strategies? Are the differences statistically significant?\n    \n2. Do the final fine-tuned models only use examples with English instructions and English reasoning traces  (given that this performed best in the ablation) or is there a mix of English/multilingual instructions/traces in the final dataset?\n    \n3. How did you arrive at using 100k as the final dataset size? How does using a larger/smaller dataset perform?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "55tfjuJWuM", "forum": "ST0wOB1bdX", "replyto": "ST0wOB1bdX", "signatures": ["ICLR.cc/2026/Conference/Submission14825/Reviewer_KCai"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14825/Reviewer_KCai"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14825/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833186624, "cdate": 1761833186624, "tmdate": 1762925173856, "mdate": 1762925173856, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}