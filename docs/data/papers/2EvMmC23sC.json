{"id": "2EvMmC23sC", "number": 21500, "cdate": 1758318298678, "mdate": 1759896918873, "content": {"title": "Critique-Guided Distillation for Efficient and Robust Language Model Reasoning", "abstract": "Supervised fine-tuning (SFT) with expert demonstrations often suffers from the imitation problem, where models reproduce correct responses without internalizing the underlying reasoning. We propose $\\text{C{\\small RITIQUE-}G{\\small UIDED} D{\\small ISTILLATION} (CGD)}$, a multi-stage training framework that augments SFT with teacher-generated $\\textit{explanatory critiques}$ and $\\textit{refined responses}$. Instead of directly imitating teacher outputs, a student learns to map the triplet of prompt, its own initial response, and teacher critique into the refined teacher response, thereby capturing both $\\textit{what}$ to output and $\\textit{why}$. Our analyses show that $\\text{CGD}$ consistently reduces refinement uncertainty, improves alignment between critiques and responses, and enhances sample efficiency. On reasoning benchmarks, $\\text{CGD}$ achieves substantial gains across LLaMA and Qwen families, including +15.0\\% on AMC23 and +12.2\\% on MATH-500, while avoiding the format drift issues observed in prior critique-based fine-tuning. Importantly, on LLaMA-3.1-8B $\\text{CGD}$ approaches or exceeds the performance of SimpleRL-Zero, which is a DeepSeek-R1 replication, while requiring 60x less compute. Beyond reasoning, $\\text{CGD}$ maintains or improves general instruction-following and factual accuracy, matching baseline performance on IFEval, MUSR, TruthfulQA, and BBH. In contrast, prior critique-based methods degrade these capabilities (e.g., -21\\% on IFEval). Taken together, these results establish $\\text{CGD}$ as a robust and generalizable alternative to both conventional SFT and RL-based methods, offering a more efficient path toward advancing the reasoning and safety of large language models.", "tldr": "A simple yet powerful extension to supervised fine-tuning via critiques that teaches models not only what the correct answer is but also why it is correct.", "keywords": ["Large Language Models", "Knowledge Distillation", "Critique", "Iterative Refinement", "Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/72307205ded4cb48a55ed44b27a0734942acd702.pdf", "supplementary_material": "/attachment/9aae599ad979186f60cee0bd23058265e6e5a019.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces a new training framework named \"Critique-Guided Distillation\" (CGD), which aims to enhance the reasoning abilities of language models by having the student model learn from teacher-generated explanatory critiques and refined responses, rather than simply imitating correct answers. This method incorporates a critique mechanism during training but requires only a single forward pass at inference time. Consequently, it achieves significant performance gains on multiple mathematical and general reasoning benchmarks while maintaining high efficiency and avoiding output format drift."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.By consistently setting the training objective to generate \"refined answers\" rather than \"critiques,\" the model maintains a standard instruction-following format at inference time. It achieves significant and consistent improvements on several challenging mathematical reasoning benchmarks, substantially outperforming strong baselines, demonstrating the method's effectiveness.\n2.The paper demonstrates the robust performance of CGD across different model families, training datasets, (and under different hyperparameters) through extensive ablation studies."}, "weaknesses": {"value": "1.Despite module-level ablation studies, the paper fails to clearly reveal the interactions between sub-modules and their marginal contributions to the performance gains.\n2.Although inference efficiency is high, its multi-stage data generation process introduces significant up-front computational cost.\n3.While CGD excels in mathematical and scientific reasoning tasks, its generalization ability on more open-ended, creative, or cross-modal tasks (such as creative writing, open-domain dialogue, complex summarization, or counterfactual reasoning) remains insufficiently validated."}, "questions": {"value": "1.The paper notes that performance is influenced by the teacher model's quality. Besides using larger, stronger teacher models, are there plans or methods to automatically evaluate or filter low-quality critiques to mitigate the negative impacts of teacher model weaknesses?\n2.If the style of the teacher model is very similar to or the opposite of the student model's style, what impact would this have on CGD's effectiveness?\nThe current multi-stage data generation process is resource-intensive. Are there future research directions aimed at simplifying this process? For example, exploring whether high-quality (initial answer, critique, refined answer) triplets can be generated via a single model or more efficient sampling strategies?\n3.Have you analyzed which characteristics (e.g., pointing out specific error steps vs. giving high-level hints) of a critique are most critical for the student model's learning? Could you provide a more operational definition or metric for \"critique quality\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5BFCrRYI12", "forum": "2EvMmC23sC", "replyto": "2EvMmC23sC", "signatures": ["ICLR.cc/2026/Conference/Submission21500/Reviewer_BbhM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21500/Reviewer_BbhM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761830347977, "cdate": 1761830347977, "tmdate": 1762941807534, "mdate": 1762941807534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Critique-Guided Distillation (CGD), where a student model conditions on its own initial answer and a teacher critique to learn a refined answer. At inference, the student outputs the refinement in a single pass. Experiments on math benchmarks show improvements over SFT, distilled SFT, and CFT, while avoiding critique-format drift. The authors also claim better efficiency than RL-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Motivated by limitations of vanilla SFT and CFT\n\n2. Improves several math-reasoning benchmarks compared to SFT and CFT.\n\n3. Preserves general instruction-following where CFT degrades it."}, "weaknesses": {"value": "1. Insufficient motivation. While CGD exhibits empirical gains, the paper does not convincingly explain why conditioning on critiques during training, but omitting them at inference, should improve from-scratch reasoning. During training, the student learns to rely on critique signals that are not available at inference. The paper does not explain how critique-conditioned refinements translate into unconditional answer generation, nor does it analyze whether this reliance introduces brittleness.\n\n2. Susceptible to the same issues acknowledged for CFT. CGD may analogously drift toward producing improved answers relative to a latent critique signal if trained extensively, and it fundamentally depends on high-quality critiques. No evidence is provided that CGD is robust to noisy, biased, misleading, or answer-leaking critiques, despite the conceptual similarity to CFT.\n\n3. Potential critique leakage. Critiques can implicitly or explicitly reveal the correct answer or key intermediate steps. Without mitigation or measurement, the observed gains may partially reflect teacher leakage rather than genuine reasoning improvements.\n\n4. Missing comparison to inference-time self-correction by accuracy. The paper discusses latency advantages but omits accuracy comparison to strong self-refine baselines. Without this, the contribution’s significance and practical trade-offs are unclear.\n\n5. Lack of statistical rigor. No repeated runs, variance, confidence intervals, or significance testing are reported. Results may not be reliable given known variance in reasoning benchmarks.\n\n6. Narrative inconsistency in baseline comparison. The “second-best” baseline is often distilled SFT, not CFT, contradicting text that positions CFT as the primary competitive method.\n\n7. Ablation (§4.2.2). Removing critiques while forcing refinement of an incorrect answer is expected to underperform vanilla SFT, making the ablation unsurprising and uninformative.\n\n8. Figure clarity and consistency issues. Figure 1 does not clearly denote teacher vs. student outputs, and Figure 3 does not show that the student receives y′, contradicting the description and Algorithm 1.\n\n9. Prompts and templates omitted. Critique-based methods are highly prompt-sensitive. The absence of templates or formatting conventions limits reproducibility and makes it difficult to judge critique quality.\n\n10. Fragmented RL comparison. RL results are isolated in a separate subsection rather than integrated into the main results tables, making efficiency/performance comparisons harder to interpret."}, "questions": {"value": "1. Can you provide deeper evidence or analysis explaining why conditioning on critiques during training (but removing them at inference) improves unconditional reasoning? What internal behaviors does the model learn?\n\n2. How do you know the student is not implicitly relying on critique-style patterns that will not exist at inference? This concern may worsen with longer CGD training.\n\n3. How does CGD perform when critiques are noisy, biased, partially incorrect, or misleading? Do you have experiments quantifying this sensitivity?\n\n4. How did you ensure critiques do not reveal the answer directly (explicitly or implicitly)? Can you provide statistics on answers appearing in critiques?\n\n5. Did you observe signs of the model drifting toward “refinement-style” outputs (e.g., suggesting revision) when trained longer? \n\n6. Can you compare CGD’s accuracy (not only latency) to multi-pass self-correction/self-refine baselines? Is the trade-off still favorable?\n\n7. Can you report variance across multiple seeds, confidence intervals, or significance tests to substantiate improvements on high-variance reasoning tasks?\n\n8. Why is CFT described as the primary baseline when distilled SFT appears stronger in practice?\n\n9. Did you analyze cases where CGD underperforms? Any patterns in failure?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QvmtLXwYKZ", "forum": "2EvMmC23sC", "replyto": "2EvMmC23sC", "signatures": ["ICLR.cc/2026/Conference/Submission21500/Reviewer_NZNy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21500/Reviewer_NZNy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997458998, "cdate": 1761997458998, "tmdate": 1762941807267, "mdate": 1762941807267, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a fine-tuning framework that enhances reasoning in language models by teaching them to self-correct using teacher-generated critiques during training. Instead of merely imitating teacher outputs, CGD trains a student model to map from its own initial response and a teacher-generated critique to the refined answer, capturing both the “what” and the “why” behind correct reasoning."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "### **1. Integrating Critique and Correction**\n\nThe paper introduces a well-motivated training framework that unifies critique understanding and refinement learning within a single fine-tuning stage.\n\n### **2. Strong Empirical Performance Across Multiple Benchmarks**\n\nThe authors provide comprehensive experimental validation across both mathematical and general reasoning benchmarks.\nCGD achieves large and consistent gains (e.g., +15% on AMC23, +12.2% on MATH-500) over SFT and CFT baselines."}, "weaknesses": {"value": "1. **Novelty Concern and Overlap with Prior Work**\n   Although CGD is presented as a novel fine-tuning paradigm, its conceptual foundation bears strong resemblance to prior works such as **ORCA (Mukherjee et al., 2023)** and **Chain-of-Thought Distillation (Li et al., 2024)**. These earlier methods also transfer reasoning traces or critique signals from a stronger teacher to a smaller student.\n   CGD’s main distinction — conditioning the student on both its own response and the teacher’s critique — represents an incremental rather than a fundamental departure. The paper would benefit from a clearer articulation of how CGD meaningfully extends these established distillation paradigms beyond re-framing critique conditioning.\n\n2. **Unfair or Incomplete Compute Efficiency Comparison**\n   The claim that CGD requires “**60× less compute**” than reinforcement-learning–based frameworks such as **SimpleRL-Zero** or **DeepSeek-R1** may not be entirely fair or directly comparable. Large-scale RL-based models like DeepSeek-R1 are designed for **general-purpose reasoning** across a broad range of domains, whereas CGD’s results are largely restricted to mathematical and structured reasoning. Thus, the compute advantage should be interpreted cautiously, as it may not hold in broader or more diverse settings."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "C5Gv18N48Y", "forum": "2EvMmC23sC", "replyto": "2EvMmC23sC", "signatures": ["ICLR.cc/2026/Conference/Submission21500/Reviewer_pyJ3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21500/Reviewer_pyJ3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100392089, "cdate": 1762100392089, "tmdate": 1762941807050, "mdate": 1762941807050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a method for reasoning SFT called CGD. This method uses a strong teacher model to provide critiques of the initial response of the student model, then feeds the prompt + initial response + teacher critique into the SFT training stage. Then, during inference stage, the teacher critique is not needed, as the student model learns to do this process end to end.\n\nExperiments were done on Qwen and Llama, and evaluations were done on various math datasets, as well as general reasoning datasets and general instruction following evaluations. Results are strong and show that the method works well."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- method is simple and easy to understand\n- I like that it included both Llama and Qwen\n    - I also like that it explored both in-family (Llama student + Llama teacher) and cross-family (Qwen student + S1 teacher)\n- nice that the model is able to retain general instruction following, since this is something that is often lost when doing imitation SFT\n    - generally quite comprehensive evaluation sets\n- strong results\n- ablation experiments are nice. I especially liked section 4.2.1 (comparison with SimpleRL) and section 4.2.2 (comparison with the method without critique)."}, "weaknesses": {"value": "- because this field is so popular and this paper's contribution is relatively simple, I wouldn't be surprised if there's a few other concurrent work submitted to this conference that explores a very similar idea of incorporating critiques in SFT data.\n    - (I realize this is another way to say \"lacks novelty\"..., though I think it's slightly more nuanced than that, since this subtopic is one that's currently very popular) \n- Even within this simple method, I think there are a few other areas that I think would be nice to explore:\n    - out-of-distribution evaluation sets (to see how well the reasoning helps)\n    - analysis of the correctness of the critiques -- How often does the teacher critique get it right? Also, does it matter if the teacher critiques are correct, or is it more about the structure rather than the actual content? Relatedly, does a stronger teacher result in a stronger SFT-ed model, or does it not matter that much?\n    - for added completeness: maybe some other model scales or some other domain like code"}, "questions": {"value": "- In line 322, you mentioned about some regulatory issues preventing you from using GPT as teacher. Curious what these are? I see GPT teachers in these papers quite often. I think it would be nice to see what the results would look like with a frontier-level teacher model beyond Llama3-70B.\n- I was looking at the Mixtral and Olmo results and saw that the gains for those models are slightly smaller. Do you have a guess or a hunch as to why certain models benefit more from CGD?\n- Is there a reason you didn't do RL on top of the SFT-ed model? I feel like it's better for these types of papers to at least try doing RL on top of SFT, just to show that these gains still continue to hold after RL."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9ezppwhuJB", "forum": "2EvMmC23sC", "replyto": "2EvMmC23sC", "signatures": ["ICLR.cc/2026/Conference/Submission21500/Reviewer_Rwiv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21500/Reviewer_Rwiv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21500/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762140267318, "cdate": 1762140267318, "tmdate": 1762941806802, "mdate": 1762941806802, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Common Response"}, "comment": {"value": "# Response to All Reviewers\n\nThank you all for your thorough and constructive reviews. We address common questions and present new validation experiments below.\n\n## New Experimental Results (Rebuttal Period)\n\n### **1. Cross-Family Validation: Qwen2.5-Math-7B**\n\nTo address concerns about generalization and statistical robustness, we further validated CGD on a different model family with different teachers:\n\n| Model | Method | Teacher | MATH500 | Minerva | Olympiad | AMC23 | AIME24 | **Avg** |\n|-------|--------|---------|---------|---------|----------|-------|--------|---------|\n| Qwen2.5-Math-7B | Base | - | 55.4 | 13.6 | 19.9 | 40.0 | 10.0 | 27.8 |\n| Qwen2.5-Math-7B | **CGD** | Claude-3.7 | **79.4** | **44.1** | **41.2** | **67.5** | 20.0 | **50.4** |\n| Qwen2.5-Math-7B | **CGD** | S1.1-32B (weaker) | 79.6 | 48.5 | 41.3 | 62.5 | 13.3 | **49.0** |\n| Qwen2.5-Math-7B | CFT | GPT-4o | 79.2 | 45.2 | 40.7 | 62.5 | 16.7 | 48.9 |\n| Qwen2.5-Math-7B | SimpleRL-Zero | - | 77.2 | 33.5 | 37.9 | 62.5 | 33.3 | 48.9 |\n\n**Key Findings:**\n- CGD (50.4) outperforms SimpleRL-Zero (48.9) with **144× less compute**\n- CGD with weak teacher still outperforms CFT with strong teacher (49.0 vs 48.9), demonstrating teacher robustness\n- Cross-architecture validation: consistent gains on LLaMA (dense), S1.1 (small), Qwen (specialized math)\n\n### **2. Domain Generalization: HumanEval (Code)**\n\nOur paper already evaluates on variety of benchmarks from different domains and skills beyond pure math, including: **MMLU-PRO, GPQA, BBH, MUSR, IFEVAL**, covering science, humanities, logic, factual QA, and instruction-following. CGD improves or preserves performance on all. \n\nCGD improves general abilities, while CFT suffers catastrophic forgetting (−21% on IFEVAL).\n\nFor example (Llama-3.1-8B student):\n\n- MuSR: 37.8 $\\to$ 39.3\n- TruthfulQA: 54.0 $\\to$ 54.5\n- IFEval: 76.9 $\\to$ 76.1\n- MMLU-PRO: 31.2 $\\to$ 40.3\n- GPQA: 30.8 $\\to$ 35.9\n\nThis shows **broad generalization beyond math, without hurting fundamental capabilities**.\n\n**Two-Part Contribution**\n\n1. CGD improves reasoning without exposing the student to hidden chain-of-thought traces (no \"thinking+answer\" concatenations).\n\n2. CGD preserves and often improves general-purpose ability, which is critical for applying the technique across creative or open-ended domains.\n\n\n**New: Code Generalization (HumanEval)**\n\nTo validate generalization capability of CGD even futher, we evaluated our LLaMA-3.1-8B models on the **HumanEval** benchmark (Python Code Generation) in a zero-shot setting:\n\n| Model | HumanEval Pass@1 | Improvement |\n|-------|------------------|-------------|\n| LLaMA-3.1-8B Instruct | 59.75% | - |\n| LLaMA-3.1-8B + CFT | 60.36% | 0.61% |\n| LLaMA-3.1-8B + CGD | **64.63%** | **+4.88%** |\n\n\n**Key Findings:**\n\n1. CGD achieves a +4.88% improvement over LLaMA-3.1-8B Instruct,  outperforming the CFT baseline by **4.88% points**.\n\n2. This suggests that the self-correction logic internalized by CGD is domain-agnostic. The model applies the same structured self-correction to synthesize valid Python code, **even without explicit code critiques in the training set.**\n\n3. Unlike CFT, which barely improves over LLaMA-3.1-8B Instruct, CGD’s answer-focused training preserves and enhances the model’s ability to generate structured outputs.\n---\n\n## Statistical Rigor Clarification\n\n**Multiple seeds:** All key results report mean performance over **3 random seeds** (Appendix A.1). Variance is accounted for.\n\n**Magnitude of gains:** Our improvements (+15% AMC23, +12.2% MATH-500 on LLaMA3.1-8B; +22.6% overall on Qwen2.5-7B Math, +10.7 on Math-Reasoning benchmarks and +15.5 on General-Reasoning tasks over base S1.1-3B) are substantial and exceed typical variance.\n\n**Cross-model validation:** Consistent gains across 5 model families validate robustness.\n\n---\n\n## CGD vs CFT: Key Distinctions\n\nSeveral reviewers asked about CGD's relationship to Critique Fine-Tuning (CFT). The fundamental difference:\n\n| Aspect | CFT | CGD |\n|--------|-----|-----|\n| **Training objective** | Output critique | Output refined answer |\n| **Format stability** | Suffers drift (-21% IFEval) | Maintains stability |\n| **Hyperparameter robustness** | Collapses and overfits with suboptimal LR and epoch | Robust across settings |\n| **Teacher dependency** | Highly dependent | Extracts value from weak teachers |\n\nThis represents an empirical finding we replicate across multiple model families."}}, "id": "hydxGxYQyy", "forum": "2EvMmC23sC", "replyto": "2EvMmC23sC", "signatures": ["ICLR.cc/2026/Conference/Submission21500/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21500/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission21500/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763752146283, "cdate": 1763752146283, "tmdate": 1763752146283, "mdate": 1763752146283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}