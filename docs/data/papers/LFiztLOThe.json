{"id": "LFiztLOThe", "number": 24637, "cdate": 1758358807800, "mdate": 1759896757550, "content": {"title": "Next-Scale Autoregressive Forecasting for Time Series via Modular Multi-Scale Decoupling", "abstract": "Time series forecasting underpins critical applications in finance, energy, healthcare, and transportation. Although deep models have achieved strong results, most adopt single-scale modeling or restrict multiscale processing to the input side, causing a misalignment between multiscale inputs and single-scale outputs and limiting predictive power. We introduce the Modular Scale-wise Autoregressive Framework (MSAR), a model-agnostic design that forecasts progressively across multiple temporal resolutions. MSAR offers three advantages: (1) scale-wise aligned modeling, which disentangles heterogeneous temporal patterns by aligning inputs and outputs at each scale; (2) scale-wise autoregression, where coarse-scale predictions guide finer-scale forecasting through hierarchical information flow; and (3) a modular architecture, enabling seamless integration with diverse backbones such as CNNs, MLPs, and Transformers. Extensive experiments across a broad set of datasets and forecasting models demonstrate that MSAR achieves consistent improvements in both accuracy and efficiency, validating the effectiveness of scale-aligned autoregression for multiscale time series forecasting.", "tldr": "", "keywords": ["time series", "coarse-to-fine", "autoregressive generative models", "neural networks"], "primary_area": "learning on time series and dynamical systems", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/09ddde3f07020c7aa12de8585f1341e26cc26723.pdf", "supplementary_material": "/attachment/6fcffcaf3f69fd7e768d991512a0ff1aef1fbc0b.zip"}, "replies": [{"content": {"summary": {"value": "The paper is well-motivated: most “multiscale” forecasters mix multi-scale inputs but decode only at a single fine scale, creating input–output misalignment and weakening long-range modeling. MSAR aligns inputs and targets per scale and decodes coarse-to-fine with conditional refinement, yielding consistent gains across backbones with little overhead. The factorization across scales and an inference-aligned training schedule make the optimization practical."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The motivation is sharp, and the architectural answer directly targets misalignment. Coarse-to-fine conditioning concentrates long-range structure at low resolution and refines details with low variance, improving long-window accuracy without requiring heavy computation. Results are consistent across models/datasets, and the training procedure aligns cleanly with inference."}, "weaknesses": {"value": "- Novelty boundaries versus prior multiresolution decoders and hierarchical AR could be positioned more rigorously. \n- The paper does not clearly show why the method works, beyond high-level ablations. (e.g., ablate one scale at a time to see which scale helps which horizon)."}, "questions": {"value": "Does shortening the fine-scale look-back (the “flexible” variant) ever hurt rare-event or spike forecasting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "w2zl7GY3sh", "forum": "LFiztLOThe", "replyto": "LFiztLOThe", "signatures": ["ICLR.cc/2026/Conference/Submission24637/Reviewer_gW3R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24637/Reviewer_gW3R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671864207, "cdate": 1761671864207, "tmdate": 1762943142635, "mdate": 1762943142635, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article proposes a modular scale autoregressive framework (MSAR) to address the problem of scale misalignment between multi-scale inputs and single scale outputs in time series prediction. The core idea of MSAR is to perform scale aligned decoupling modeling, which independently predicts at multiple time scales and uses a coarse to fine autoregressive mechanism to guide fine scale predictions using coarse scale prediction results as contextual information. The author claims that this method has model independence and can be easily integrated into backbone networks such as CNN, MLP, Transformer, etc. Experiments on a large dataset have shown that MSAR can consistently improve the performance of baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "​​***Novel and Well-Motivated Concept​​***:​​ The idea of \"next-scale\" autoregressive forecasting is a noteworthy contribution. Framing the forecasting problem as a hierarchical, coarse-to-fine process aligns well with the multi-scale nature of time series data. \n\n​​***​​Comprehensive Evaluation and Analysis​​***:​​ The paper provides a thorough evaluation across numerous datasets (both long-term and short-term) and backbone architectures (Transformers, MLPs, etc.). The paper also includes well-designed ablation studies, sensitivity analysis, and efficiency benchmarks."}, "weaknesses": {"value": "1. This article does not specify which downsampling method was used. In my opinion, different downsampling methods can seriously affect the evaluation of prediction performance. Because labels are also downsampled, some downsampling methods may make labels smoother and relatively easier to predict. More importantly, this may affect the correctness of the conclusions drawn from the three observations in Sec3.1. for example,\n\n2.The findings drawn from observation 3 in sec 3.1 needs further discussion. Because according to Table 1, timemixer is trained without downsampling, strictly speaking, it is not the same task as mse (Δ=6), or there is a significant difference in the distribution of labeled data.\n\n3. The next scale approach of this method is in line with the characteristics of time-series data, but there is a lack of in-depth thinking or detailed explanation regarding the specific operations, such as whether EQ3 can ensure that the length of Y1 align ^ {<i} is consistent with X ^ i and has values at all time steps; In the Refinement in the Imputation Phase, full horizon prediction is performed, but the labels in this layer are downsampled, and the predicted results should be inconsistent with the label length. How to calculate the loss.\n\n4. Most importantly, the main purpose of this article is to address 'scale interference', but many of its operations conflict with this point. For example, the inputs and outputs processed by 'teacher forcing training', 'full height prediction', etc. are of different scales, and there is also bag interference.\n\n5.The expression 'To our knowledge... time series forecasting...' in the intro lacks objective information support, which makes the article appear very academic. It is recommended that the author improve."}, "questions": {"value": "​​1.Downsampling:​​ What specific downsampling technique (e.g., average pooling, strided sampling) was used to generate the coarse-scale sequences X^Δand Y^Δ? Please justify the choice. Could you provide an analysis showing that the key observations in Section 3.1 hold across different reasonable downsampling methods?\n\n​​2.Alignment Mechanism:​​ Could you elaborate on the exact procedure for implementing Equation 3, specifically the operation Align(·)? How do you ensure temporal alignment between Y_align^(<i)and X^i? What happens for fine-scale time indices that do not have a corresponding coarse-scale prediction?\n\n3.​​Loss Calculation:​​ In the \"Refinement in the Imputation Phase,\" the loss is computed over the \"entire prediction window.\" Given that the target Y^iand the model's output (which incorporates predictions from coarser scales) may have different inherent characteristics or even effective lengths at a given scale i, how is the loss function practically applied? Is any masking or weighting scheme used?\n\n4.Explanation: Please further explain how to eliminate the 'scale interference' caused by 'teacher forcing training' and 'full height prediction'"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "fY9erD0zCf", "forum": "LFiztLOThe", "replyto": "LFiztLOThe", "signatures": ["ICLR.cc/2026/Conference/Submission24637/Reviewer_GoR5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24637/Reviewer_GoR5"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761746507299, "cdate": 1761746507299, "tmdate": 1762943141960, "mdate": 1762943141960, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes MSAR (Modular Scale-wise Autoregressive Framework), a model-agnostic way to do time-series forecasting by (i) aligning inputs and outputs at multiple temporal scales, (ii) forecasting autoregressively across scales—coarse predictions condition finer ones—and (iii) keeping each scale’s forecaster modular so it can plug into CNN/MLP/Transformer backbones. Experiments across standard benchmarks and several popular backbones show consistent, usually modest, improvements."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Sharp diagnosis of input–output scale misalignment and a clean remedy: aligned, modular, coarse to fine AR without interpolation between scales. This keeps each scale’s patterns disentangled and uses coarser structure to guide finer prediction.\n2. Broad evaluation across 8+ long-term datasets and multiple backbones shows consistent (often modest) gains, including on strong models. Ablations (Table 4) convincingly show that the full combo (multiscale + alignment + scale-wise AR) matters.\n3. Model-agnostic plug-in that often improves accuracy with limited overhead; the “flexible” variant shows a nice accuracy–efficiency trade-off, which is attractive for real systems."}, "weaknesses": {"value": "1. Improvements, while consistent, are frequently small (fractions in MSE/MAE), and a few settings show near-parity with baselines. The paper would benefit from a clearer analysis of when the biggest gains occur (e.g., high-periodicity vs. non-stationary datasets, horizon length sensitivity beyond averages).\n2. Although unified settings and a search space are provided, some models are known to be sensitive to lookback/hyperparameters. Stronger per-model best-tuned comparisons (or replication of reported SOTA configs) would reduce residual doubts that MSAR’s gains partially come from configuration choices.\n3. The relation to existing multiscale input models (e.g., wavelet/tokenization paths) and coarse-to-fine generation in other modalities is discussed, but a sharper theoretical or empirical contrast (e.g., head-to-head with strong multi-resolution decoders or with learned interpolation) would strengthen claims of distinctiveness."}, "questions": {"value": "1. How sensitive are results to the choice and number of scales beyond the shown lists? Could the model learn the scale schedule (learnable downsampling / adaptive intervals) rather than fixing them?\n2. Can you quantify how errors at coarse scales affect fine-scale accuracy, perhaps via controlled perturbations of coarse predictions or curriculum schedules? Would selective detachment/stop-gradient help?\n3. A stratified analysis by dataset characteristics (seasonality strength, noise level, horizon length, variable count) would be helpful—e.g., meta-features predicting when aligned multiscale decoding yields the largest gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UnGtWdPDJl", "forum": "LFiztLOThe", "replyto": "LFiztLOThe", "signatures": ["ICLR.cc/2026/Conference/Submission24637/Reviewer_GjT5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24637/Reviewer_GjT5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970123313, "cdate": 1761970123313, "tmdate": 1762943141667, "mdate": 1762943141667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the mismatch between multiscale inputs and single-scale outputs in time series forecasting. The authors propose the Modular Scale-wise Autoregressive Framework ($\\text{MSAR}$), a model-agnostic design that reframes forecasting as a progressive, next-scale prediction task. $\\text{MSAR}$ has three features: (1) scale-wise aligned modeling (matching input/output scales); (2) scale-wise autoregression (coarse-scale predictions guide fine-scale ones); and (3) modularity (pluggable into $\\text{CNNs}$, $\\text{MLPs}$, $\\text{Transformers}$). The paper claims consistent improvements in both accuracy and efficiency.My preliminary assessment is that this work provides a solid and novel framework for multiscale forecasting. Its core concept (scale-wise autoregression) is well-motivated. However, the ablation study's analysis of its components is contradictory, and the claim of efficiency improvement is not universally supported by the data."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "S1. The core contribution, scale-wise autoregression, is a novel and significant conceptual shift. It moves beyond single-scale autoregression or multiscale-input/single-scale-output models. Formulating the prediction objective as a product of scale-wise probabilities, $p(Y^{1},...,Y^{N}|X^{1},...,X^{N})=\\prod_{i=1}^{N}p(Y^{i}|X^{i},\\hat{Y}_{align}^{<i})$, is an elegant new paradigm for enforcing hierarchical consistency.\n\nS2. The core hypothesis is well-motivated. A preliminary experiment shows that models trained with fully scale-aligned inputs and outputs (e.g., $T=576^{\\Delta=6}$, $H=336^{\\Delta=6}$) achieve superior coarse-scale accuracy ($\\text{MSE}^{\\Delta=6}$), providing strong support for $\\text{MSAR}$'s scale-aligned design.\n\nS3. The framework's model-agnostic design is a key strength. Experimental results (\"Base\" vs. \"+Ours\") show consistent performance improvements across five different $\\text{SOTA}$ backbones (including $\\text{DLinear}$, $\\text{PatchTST}$, $\\text{iTransformer}$), demonstrating the robust applicability of the $\\text{MSAR}$ framework."}, "weaknesses": {"value": "W1. Contradictory Ablation Study:The main ablation study in Table 4 is confusing. The baseline model ((2), \"Alignment\" only) achieves an $\\text{MSE}$ of $0.371$ on $\\text{ETTm1}$. However, all pairwise combinations of components perform worse than this baseline: (1)+(2) (Multiscale + Alignment) gets $0.385$; (2)+(3) (Alignment + AR) gets $0.395$; and (1)+(3) (Multiscale + AR) gets $0.399$. The paper claims \"Pairwise combinations bring partial gains,\" which is false; they clearly degrade performance. This suggests the components are not synergistic and may be brittle.\n\nW2. The abstract claims \"consistent improvements in ... efficiency,\" but the data does not support this.\n- Training Time: $\\text{MSAR}$ significantly increases the training time (ms/iter) for all five backbones.\n- Inference Time: $\\text{MSAR}$ reduces inference time for $\\text{PatchTST}$ and $\\text{TimesNet}$ but increases it for $\\text{DLinear}$ and $\\text{iTransformer}$. The claim of \"consistent\" efficiency improvement is inaccurate.\n\nW3. The architectural overview in Figure 2 is very confusing. It labels the coarsest-scale module \"Forecast\" but all finer-scale modules \"Imputation.\" This implies forecasting only happens at the coarsest scale, which contradicts the text and math (Eq. 2 & 4) stating that each $\\mathcal{F}_{\\theta}^{i}$ performs forecasting. This is a major presentation flaw."}, "questions": {"value": "Q1. Please clarify Table 4. Why do all pairwise combinations (e.g., (1)+(2), (2)+(3)) result in significantly worse $\\text{MSE}$ than the baseline ((2) only)? This suggests the components are not additive.\n\nQ2. Please clarify the terminology in Figure 2. Are the \"Imputation\" modules architecturally identical to the \"Forecast\" module? If so, why the different label? If not, how does this align with the single $\\mathcal{F}_{\\theta}^{i}$ function in Eq. 4?\n\nQ3. How is the claim of \"consistent improvements in ... efficiency\" justified when $\\text{MSAR}$ increases training time for all models and increases inference time for $\\text{DLinear}$ and $\\text{iTransformer}$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vHyZQyd8oI", "forum": "LFiztLOThe", "replyto": "LFiztLOThe", "signatures": ["ICLR.cc/2026/Conference/Submission24637/Reviewer_pwcz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24637/Reviewer_pwcz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24637/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762050677664, "cdate": 1762050677664, "tmdate": 1762943141367, "mdate": 1762943141367, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}