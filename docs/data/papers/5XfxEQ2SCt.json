{"id": "5XfxEQ2SCt", "number": 22814, "cdate": 1758335808481, "mdate": 1759896844892, "content": {"title": "Permanent and Transient Representations for Continual Reinforcement Learning", "abstract": "Continual Reinforcement Learning agents struggle to adapt to new situations while retaining past knowledge, resulting in a stability–plasticity trade-off. An appealing solution is to decompose the agent’s predictions into permanent and transient components---one for long-term retention and the other for rapid adaptation---thereby achieving a better balance~\\citep{anand2023prediction}. Building on this idea, we propose using different sets of feature representations to estimate permanent and transient value functions, enabling even faster adaptation. We demonstrate the effectiveness of our approach on small-scale examples for both prediction and control tasks, analyze its theoretical properties, and show its benefits on the Craftax-Classic benchmark using a novel non-parametric approximator for transient value function estimation. Our method facilitates online learning and outperforms the PQN baseline.", "tldr": "We propose to use different sets of feature representations for estimating permanent and transient value functions", "keywords": ["Continual Reinforcement Learning", "Reinforcement Learning", "Lifelong reinforcement learning", "value functions", "permanent and transient"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b2ffbfa922291b0864fee1a5612be694e55f4bfb.pdf", "supplementary_material": "/attachment/94a020d2ad947b597ad4eccce8e9467464fff12c.pdf"}, "replies": [{"content": {"summary": {"value": "The paper builds on previous work in continual reinforcement learning, where decomposing an agent into permanent and transient components was proposed as a way to address the stability–plasticity trade-off.\nIt extends this idea by using different feature representations for these components. In particular, instead of using a neural network for the transient component, it introduces a feature representation method that tokenizes and hashes observations, and estimates and updates values like tile coding.\nThe paper evaluates this approach on non-stationary prediction, control (chain problem), and large-scale control (Craftax) problems, and shows improvements over the previous baseline."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is well-motivated, the related literature review is covered, and the idea of separating permanent and transient feature representations is useful.\n2. The paper also has experimental rigor in most of the cases, as it mentions numbers of seeds, details of hyperparameters, and enough details of the environments and experiment settings.\n3. The paper is aware of its limitations. e.g., the semi-continual settings (agent is aware of the task boundaries), and parallelization of environments, which is not ideal"}, "weaknesses": {"value": "1. (line 269) The paper mentions feeding separate features to transient vs permanent modules in the control experiment. This hand-crafted separation can help the algorithm learn better, but ideally, a continual learner should be able to receive the raw stream of observations and attain the more important features for each component autonomously.\n\n2. The feature representation method works with symbolic observations, as mentioned in the paper. How can the method be generalized to environments with other forms of observations?\n\n3. The order of chapters should be improved. Before diving into the results, the method should be introduced. In particular, the order of chapters 5 and 6 should be changed.\n\n4. The proposed idea comes with hyperparameters, like $\\rho$. The paper would benefit if there were a paragraph about the effect of these hyperparameters and how they should be set for new environments and settings. (I see plots in the appendix on these hypers, but more explanation in a paragraph or two is needed in the main text)\n\n5. The large-scale experiment can be improved. Three seeds are not enough to confidently compare the algorithms. Furthermore, it is valuable to compare PQN(1) and PQN(32), to show the effect of update frequency and batch size, but they are still the same algorithm with different hyperparameters compared. The paper would benefit if a separate plot were presented for hypers.\n\nMinor comments:\n\n(Line 475)- And should be lower case.\n\nIt would be better to keep the supplementary material at the end of the paper, not taken out of the paper, so that referring to the appendix for algorithms and hyperparameter details is easier."}, "questions": {"value": "1- How will PT-TD(ours) perform if all features are fed both to the transient and permanent components?\n\n2- How do you tune the hyperparameters specific to your method (e.g. p)?\n\n3- How can you scale this representation method to other types of observation, like to images? Can you run some small experiments to show your method works for those settings as well?\n\n4- The paper states the idea that the permanent features of the observations are captured in the permanent network, and the changing, transient characteristics of the environment, in the transient component. This is an assumption, which hypothetically will help with better performance, and measuring performance can be one way to move toward validating this statement. My question is, how can you actually measure what is stored in which component? E.g., can measuring similarities between the feature representations, and permanent observation features vs transient ones help? Are there diagnostic metrics (e.g., representational similarity analysis, interference measures) that could confirm the intended division between permanent and transient features?\n\n5- Can you elaborate on what you mean by “append inventory, intrinsic values” in line 421?\n\n6- Some assumptions, such as full rank for feature matrices, in the theoretical results are overly strong. How will the theorems change in case of violations of these assumptions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "jEHPwbyalR", "forum": "5XfxEQ2SCt", "replyto": "5XfxEQ2SCt", "signatures": ["ICLR.cc/2026/Conference/Submission22814/Reviewer_CwA7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22814/Reviewer_CwA7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761199718315, "cdate": 1761199718315, "tmdate": 1762942398211, "mdate": 1762942398211, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The manuscript investigates the stability–plasticity trade-off in continual reinforcement learning (CRL) by decomposing value estimation into permanent and transient components with explicitly separate feature representations. The central idea is that permanent value functions should leverage slow, stable features, while transient value functions should use fast-evolving or non-parametric, reward-predictive features to adapt rapidly. To enable rapid adaptation at scale, the manuscript proposes a non-parametric transient memory based on MinHash signatures that preserve Jaccard similarity for symbolic observations, providing tabular-like fast updates with controllable local generalization. Theoretical analyses and small-scale experiments are presented to motivate the design, and larger-scale results on Craftax-Classic show improvements over a PQN baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear motivation and organization: from intuition to theory, small-scale validations, and finally large-scale experiments.\n2. Decoupling the representations used by permanent and transient value functions is a simple and compelling idea for improving the stability–plasticity balance.\n3. The manuscript includes theoretical results, algorithmic details, and pseudocode, which improves transparency and reproducibility.\n4. The method shows improvements on large-scale experiments over the PQN and demonstrates applicability.\n5. The proposed framework for separating representations of permanent and transient components may be useful beyond the presented setting, and the memory design could transfer to other CRL scenarios."}, "weaknesses": {"value": "1. The core advance over PT-TD is primarily the use of separate feature sets for the two value components; while intuitive and useful, it may be viewed as a modest extension rather than a substantial methodological leap.\n2. The work has significant limitations as it does not extend the applicability of the original PT-TD method (such as how to apply it to other policy based reinforcement learning algorithms)\n3. The large-scale evaluation compares mostly to PQN and omits direct comparisons to PT-TD and other CRL approaches, making it difficult to attribute gains to the proposed decomposition and memory design.\n4. It is not fully clear how the theoretical results directly support the central empirical claims. Clearer mapping from lemmas/theorems to observed phenomena would strengthen the argument.\n5. The manuscript lacks ablation studies and visual analysis.\n6. Reporting and clarity issues: notation is introduced (V, Q, s, etc.) without sufficient definitions; some figure elements (e.g., the dashed line in Fig. 2(a)) and experimental variants (e.g., “TM-only”) are not clearly explained."}, "questions": {"value": "1. In Section 5.2, the transient component uses illumination features that are directly reward-predictive, while the permanent component uses RGB features. If baselines receive all features, does this create a mismatch in inductive bias that favors the proposed method?\n2. If the observation similarity assumption (needed for MinHash/Jaccard) does not hold, does the transient memory still function effectively?\n3. Why is PQN the sole large-scale baseline, especially given PQN’s design goal of leveraging environment parallelization (line 436)? Please add comparisons to PT-TD and representative CRL baselines (e.g., EWC/L2 regularization for value networks, rehearsal/buffering, meta-learning-based CRL) or justify in detail why they are not included."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SzurngaiTy", "forum": "5XfxEQ2SCt", "replyto": "5XfxEQ2SCt", "signatures": ["ICLR.cc/2026/Conference/Submission22814/Reviewer_QcCf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22814/Reviewer_QcCf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761803469271, "cdate": 1761803469271, "tmdate": 1762942398008, "mdate": 1762942398008, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to extend the **Permanent-Transient** (PT) framework (Anand & Precup 2023) by introducing separate feature representations for the permanent and transient components of the value function. Additionally, the authors introduce a non-parametric transient memory based on MinHash and CMAC-like updates to enable rapid, online adaptation. The method is evaluated on small-scale toy problems and the Craftax-Classic environment, claiming improved performance over PQN baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The idea of separating permanent and transient representations is biologically and intuitively motivated.\n* Provides theoretical convergence proofs under linear approximation (although very classical).\n* Introduces a novel non-parametric memory module that could be reusable beyond this setting."}, "weaknesses": {"value": "* The claimed novelty (“separate feature representations”) feels incremental over Anand & Precup (2023). Why is this a significant step rather than a straightforward design variant?\n* The \"transient memory\" resembles *tile coding* with *hashing*, which is decades old. The description sounds more like a new combination of existing mechanisms rather than a fundamentally new idea.\n* The paper leans heavily on biological analogies (CLS theory) without providing an actual mechanistic link or empirical justification beyond metaphor.\n* The convergence result essentially repackages standard two-timescale SA arguments (Borkar 1997; Tsitsiklis & Van Roy 1996). What exactly is new here beyond the trivial extension to two feature matrices?\n* No discussion of how feature misalignment or correlation between permanent and transient features affects convergence speed or stability.\n* The assumptions (e.g., observable task boundaries, full column rank of feature matrices) are unrealistic in genuine CRL scenarios. How would the analysis hold when boundaries are unknown or tasks are not i.i.d.?\n* The toy tasks are far too simple to substantiate the claims. Gridworld and chain environments are saturated benchmarks for incremental TD methods; they say little about continual learning dynamics.\n* No measure of catastrophic forgetting or transfer between tasks; all reported metrics are single-task reward curves, so the continual aspect remains speculative.\n* Code release is promised “upon acceptance”, which should be available at submission time.\n* Heavy use of ChatGPT/Copilot is acknowledged; this raises questions about authorship and whether critical sections were written or validated by the authors themselves."}, "questions": {"value": "Questions are given under the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7aQEwBz6xf", "forum": "5XfxEQ2SCt", "replyto": "5XfxEQ2SCt", "signatures": ["ICLR.cc/2026/Conference/Submission22814/Reviewer_Joyf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22814/Reviewer_Joyf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909485283, "cdate": 1761909485283, "tmdate": 1762942397691, "mdate": 1762942397691, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper builds upon the Permanent-Transient (PT) value function decomposition framework for Continual Reinforcement Learning (CRL). The core idea is to use separate feature representations for the permanent (slow, stable) and transient (fast, plastic) value functions, arguing that this better embodies the complementary learning systems inspiration. The authors provide theoretical convergence guarantees for this setup and introduce a novel non-parametric approximator for the transient value function, which uses hashing and a slot-based memory to enable fast, online learning with controlled generalization. The method is evaluated on small-scale prediction/control tasks and the large-scale Craftax-Classic benchmark, where it is shown to outperform the PQN baseline, especially with a small number of parallel environments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-Motivated Core Idea**: The extension of the PT framework to use separate representations is intuitive and well-justified from a biological and functional perspective. It is a logical and meaningful step forward from prior work.\n- **Theoretical Analysis**: The paper provides a solid theoretical foundation, establishing convergence guarantees for the proposed method under linear function approximation (where previous works focused on tabular settings) using a two-timescale analysis. This rigor is a significant strength.\n- **Novel Non-Parametric Transient Memory**: The proposed non-parametric approximator is a creative and interesting contribution. Its design, combining ideas from tile coding, CMACs, and MinHash, is well-explained and appears effective for the environments tested.\n- **Empirical Success:** The method demonstrates strong empirical performance, convincingly outperforming the PQN baseline in both small-scale experiments and on the challenging Craftax benchmark."}, "weaknesses": {"value": "- **Limited Scope and Generalizability of Transient Memory**: The non-parametric transient memory is currently limited to symbolic, grid-like observations. The authors briefly mention that randomly initialized CNNs could be used for RGB images (line 470). Given that many important CRL domains (e.g., Atari, real-world sensors) involve high-dimensional pixel inputs, this is a significant limitation. The paper would be substantially stronger with an experiment demonstrating the method's applicability beyond symbolic domains.\n- **Lack of Ablation and Comparison to Related Memory Approaches:** The paper misses an opportunity to properly situate its novel transient memory within the existing literature.\n- **There is no direct comparison to classic function approximators like CMACs or Tile Coding**, which share similar intuitions about local generalization. Showing that the proposed method is superior to or offers unique advantages over these established techniques would strengthen the contribution.\n- **The connection to episodic memory methods in RL is very relevant but unexplored**. A discussion or comparison would help readers understand the relationship between this work and other memory-based approaches.\n- **Limitation of Transient Memory's Differentiability**: The non-parametric nature of the memory makes it non-differentiable, which may limit its integration with end-to-end learning systems. The authors do not discuss potential pathways to a differentiable variant, which would be a valuable direction to mention in the discussion. A possible variant can be episodic memories.\n- **Unconvincing and Potentially Self-Contradictory Online Experiment Setup:** The definition and setup of the \"online\" experiment are confusing and weaken the paper's narrative.\n  - The authors state that \"an ideal, general-purpose CRL algorithm should learn effectively from a single stream of experience\" (line 390). However, their \"online\" experiment uses two parallel environments. This seems to contradict their own stated ideal. The community standard for \"online\" RL is typically a single environment. The choice to use two environments without a compelling justification (e.g., a comparison to a true single-environment run) undermines the claim of online learning capability.\n  - Suggestion: The authors should either run an experiment with a single environment (updating the permanent network with a batch size of 1 or with a small batch collected sequentially, like 32) to truly validate their online learning claim, or they should reframe their \"online\" experiment as a \"low-parallelism\" setting and adjust their narrative accordingly.\n\n\n\nThis paper presents a valuable extension to the PT framework with a novel and effective non-parametric memory architecture. The theoretical analysis and strong empirical results on Craftax are commendable. However, the current version has significant weaknesses that prevent a higher score. The contribution would be substantially greater if the method's applicability to broader domains, like pixel-based observations, were demonstrated. Furthermore, a more rigorous comparison to existing memory-based approximators is needed.\n\nI lean towards rejection in its current form, primarily due to the limited scope of the transient memory. However, if the authors can address the major concerns, especially by adding experiments that demonstrate generalizability beyond symbolic inputs and add necessary ablation and baseline comparisons, I'll increase my score."}, "questions": {"value": "- Online Experiment Justification: Why was the decision made to use 2 environments in the \"online\" experiment instead of a single environment, especially given the stated ideal of learning from a \"single stream of experience\"? Can you provide results with a single environment to solidify this claim?\n- Generalizability: Can you provide any empirical evidence (even preliminary) that your hashing-based transient memory can be adapted to non-symbolic observation spaces like RGB images, for instance, using a random CNN projection as mentioned?\n- Baseline Comparisons: How does your non-parametric transient memory compare, in terms of performance and efficiency, to a simpler implementation using standard Tile Coding or a CMAC on the Craftax benchmark?\n- There is no ablation for the transit memory component system, which makes it hard to assess. Can the authors provide an ablation that support the design choices and coponents of that transient memory system?\n- Memory Consolidation: The paper leaves \"when and what to consolidate\" as future work. Did you experiment with any simple consolidation strategies (e.g., moving values from transient to permanent memory)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OZW2e3z2fU", "forum": "5XfxEQ2SCt", "replyto": "5XfxEQ2SCt", "signatures": ["ICLR.cc/2026/Conference/Submission22814/Reviewer_MWuz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22814/Reviewer_MWuz"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22814/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762199319015, "cdate": 1762199319015, "tmdate": 1762942397344, "mdate": 1762942397344, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}