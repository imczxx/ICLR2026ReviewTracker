{"id": "11QZITAMUO", "number": 13943, "cdate": 1758225619178, "mdate": 1759897401437, "content": {"title": "Query-Level Uncertainty in Large Language Models", "abstract": "It is important for Large Language Models (LLMs) to be aware of the boundary of their knowledge, i.e., the mechanism of identifying known and unknown queries. This type of awareness enables models to perform adaptive inference, such as invoking retrieval-augmented generation (RAG), engaging in slow and deep thinking, or abstaining from answering when appropriate. These mechanisms are beneficial to the development of efficient and trustworthy AI. \nIn this work, we propose a method to detect knowledge boundaries via \\textbf{\\emph{Query-Level Uncertainty }}, which estimates if a model is capable of to answering a given query before generating any tokens. \nTo this end, we propose a novel, training-free method called \\textbf{\\emph{Internal Confidence}}, which leverages self-evaluations across layers and tokens to provide a reliable signal of uncertainty.\nEmpirical studies on both factual question answering and mathematical reasoning tasks demonstrate that our internal confidence can outperform several baselines. Furthermore, we showcase that our proposed method can be used for adaptive inference,  such as efficient RAG and model cascading, thereby reducing inference costs while preserving overall performance.", "tldr": "In this work, we propose a method to detect knowledge boundaries via Query-Level Uncertainty, which estimates if a model is capable of to answering a given query without generating any tokens.", "keywords": ["Large Language Models", "Frugal AI", "Uncertainty Quantification"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/726deb49cf0f0a3605bbecbea33ccfcc53286e49.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a training-free, pre-generation method Internal Confidence to investigate the knowledge boundaries of LLMs and reduce inference costs.\nThe authors prompt LLMs with a yes–no question to self-assess whether they can answer a given query before generating an answer, and they adopt layer-level consistency to measure the reliability of the answers.\nExperiments show some efficiency improvements over simple baselines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The idea of identifying an LLM’s knowledge boundaries before allowing it to generate answers or take agentic actions is both interesting and impactful.\nAs the paper demonstrates, when used properly, such methods can significantly reduce the costs of applying LLMs to downstream tasks."}, "weaknesses": {"value": "- The method description is confusing, with some simple concepts made complex and other critical details missing.\n- The experimental results are not sufficiently robust.\n- The article is ambiguous throughout.\n- Although not required, the authors are encouraged to discuss the limitations of their work."}, "questions": {"value": "- \"We define a query as being within the model’s knowledge boundary if the LLM can produce a correct answer under greedy decoding, i.e., by selecting the highest-probability token at each step without sampling.\" --- I’m not sure how or if the answers’ correctness correlates with the model's knowledge boundary. Are there any previous works that support this statement?\n\n- Why introduce the unmapping weights instead of directly using the last-layer pre-activation logits for a clearer description? Did you modify the unmapping weights to keep only the rows corresponding to Yes and No tokens for computing the logits?\n\n- Why introduce Lines 211--235? Are the parameters theta retrained or from external layers? Why not simply state \"pre-/post-activation output of each Transformer block\"?\n\n- It is unclear what the word \"query\" refers to in different contexts. In general, it seems to indicate the model's prompt; but in the example in Lines 194–195, it seems to refer to the question; and in the discussion in Lines 237–245, both meanings do not make sense, and it seems \"query\" here refers to both the prompt and the model's answer. This is extremely confusing.\n\n- In Figure 3 and Lines 237--245, it is unclear which tokens these probabilities/scores are associated with. Suppose these are answer tokens; then we may expect these results, since the last token is highly likely to be EOS, so the next-to-last token is the actual Yes/No token. If not, I’m not sure how to interpret these results. In either case, the discussion does not carry much useful information.\n\n- Lines 253--258: I don't get the value of this paragraph. Equation (3) looks like a simple weighted average, so why form it into a \"hierarchical aggregation\" process?\n\n- Equations (4) and (5): How is delta related to epsilon? How is locality related to the weights w?\n\n- When calculating the runtime, is the time for generating the actual answers included? Why not include predictive entropy in Table 2? Why not include length-normalized predictive entropy in Tables 1 and 2? Semantic aggregation methods such as Semantic Entropy and SAR are not designed for a task with a deterministic answer, such as GSM8k; why include them in Table 2? Why not use TriviaQA?\n\n- Line 376: Area under what curve? I know the answer, but it may confuse others who are not so familiar with UQ.\n\n- Line 315: What is the necessity of evaluating the reasoning steps for GSM8k? What would happen if we do not do so?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "THxfQMvGKz", "forum": "11QZITAMUO", "replyto": "11QZITAMUO", "signatures": ["ICLR.cc/2026/Conference/Submission13943/Reviewer_pkEN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13943/Reviewer_pkEN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761414384004, "cdate": 1761414384004, "tmdate": 1762924446363, "mdate": 1762924446363, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Query-Level Uncertainty, which estimates whether an LLM can answer a query before generating tokens, addressing efficiency and trustworthiness concerns. The proposed Internal Confidence method aggregates P(YES) signals across layers and tokens using attenuated encoding weights, obtaining calibrated query-level uncertainty in a single forward pass. Experiments on factual QA and mathematical reasoning tasks show improvements over adapted answer-level methods. Applications to efficient RAG and model cascading demonstrate practical benefits with 30-600× speedups over answer-level approaches."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Query-level uncertainty is a genuinely different and practical perspective from existing answer-level methods. Rather than generating long answers to assess uncertainty, the method predicts answerability before token generation, directly addressing efficiency bottlenecks in real-world systems. The Internal Confidence method is elegantly simple—it aggregates P(YES) signals across layers and tokens with attenuated encoding weights, producing calibrated uncertainty in a single forward pass without requiring training.\n\nThe empirical results are strong and consistent. The method shows improvements over adapted baselines across three diverse datasets and works reliably across different model sizes (Phi-3.8B, Llama-8B, Qwen-14B). More impressively, it achieves 30-600× speedups compared to answer-level approaches like Semantic Entropy and SAR, making it genuinely deployable. Ablation studies systematically examine the impact of locality and hyperparameters, providing confidence in design choices.\n\nThe experimental design is rigorous with proper out-of-domain evaluation demonstrating robustness to domain shift. The applications to efficient RAG and model cascading are immediately actionable, with identified \"optimal points\" that practitioners can use directly. The paper is well-written with clear motivation and effective visualizations that make the approach accessible."}, "weaknesses": {"value": "The theoretical justification is weak. While P(YES) seems intuitive as a proxy for query answerability, the connection to actual answering capability is assumed rather than proven. Similarly, attenuated encoding is presented as effective, but other aggregation schemes aren't systematically compared to justify this choice. The \"decision center\" concept, which is central to the method, lacks theoretical grounding—the paper doesn't explain why the top-right position is optimal or how it should adapt across different models and tasks.\n\nThe evaluation scope is narrow and raises generalization concerns. Experiments are limited to factual QA and mathematical reasoning across just three datasets with 10K samples each. There's no evaluation on open-ended tasks, creative writing, code generation, or other generation types. Additionally, the method assumes greedy decoding for determining ground truth, which may not reflect real model behavior under beam search or sampling-based decoding strategies.\n\nGround truth definition is somewhat arbitrary and validation is incomplete. Correctness is defined solely by greedy decoding accuracy, and the paper lacks comparison against human-annotated \"answerability\" judgments. It remains unclear whether the model's internal uncertainty truly reflects knowledge or is confounded by input formatting, instruction phrasing, or other factors. The paper also doesn't analyze systematic biases—does the method correlate with question difficulty, answer length, or other confounds?\n\nThe method's claims of being \"training-free\" are overstated. While no training is required, hyperparameter selection (α value and decision center location) depends on validation data, and cross-dataset transfer shows these vary by task and model. The paper uses fixed hyperparameters across all settings despite acknowledging they're suboptimal, and the inconsistency between task-specific optimal centers and the fixed top-right choice isn't well justified.\n\nBaseline comparisons are limited to adaptations of answer-level methods, with no comparison to other query-level uncertainty approaches or simpler heuristics like maximum token probability of the query. Code availability isn't mentioned, and specific prompts and model configurations lack complete documentation, hampering reproducibility and follow-up work."}, "questions": {"value": "1. Why is P(YES) specifically the right formulation? Have you tried other self-evaluations (e.g., confidence level on a 1-10 scale)?\n\n2. How does the method perform on adversarial inputs where models give confident wrong answers?\n\n3. Can you provide correlation analysis between Internal Confidence scores and actual greedy decoding accuracy?\n\n4. How sensitive are results to the specific threshold used for deciding \"know/don't know\"?\n\n5. For the decision center analysis (Figure 3b), why does performance peak at h_5^(27) rather than final layer?\n\n6. Have you tested on models with different architectures (MoE, retrieval-augmented)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "71M3QCk4SG", "forum": "11QZITAMUO", "replyto": "11QZITAMUO", "signatures": ["ICLR.cc/2026/Conference/Submission13943/Reviewer_hjve"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13943/Reviewer_hjve"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761619420317, "cdate": 1761619420317, "tmdate": 1762924445889, "mdate": 1762924445889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Query-Level Uncertainty (QLU) for LLMs: a training-free method that estimates whether the model is likely to answer a query correctly before any generation. The core signal Internal Confidence (IC) aggregates self-evaluation logits (YES/NO) across layers $\\times$ tokens using a locality-aware attenuation around a fixed decision center (last token, last layer). Experiments on factual QA (TriviaQA, SciQ) and math reasoning (GSM8K) across multiple open models show that IC outperforms post-hoc, answer-dependent baselines in ranking “known vs unknown” queries, while being faster."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Framing “knowledge boundary” as pre-answer uncertainty is useful for agentic pipelines (RAG, slow-thinking, model cascades).\n* Training-free and fast. Single forward pass speed largely independent of answer length, high leverage for long-form tasks and tool-heavy agents.\n* IC uses only standard hidden states + unembedding, attenuation over layers and tokens captures the information where the “can I answer?” signal concentrates.\n* Fixed decision center and a single locality hyper-parameter yield out-of-the-box generalization without per-dataset tuning.\n* The paper demonstrates adaptive inference use cases: gating RAG and model cascading to reduce cost at similar accuracy."}, "weaknesses": {"value": "1. Operational definition of “knowledge boundary.” The binary label is tied to greedy decoding success. This could confuse capability with decoding heuristics and underestimate the success of non-greedy decoding. Sensitivity analysis of decoding strategies would help strengthen these claims.\n2. IC requires hidden states and full model access. Many production black-box APIs don’t expose them.\n3. The method fixes the decision center (last-layer, last-token) and selects a single decay schedule. A more complete ablation including alternative centers, learned decay parameters, or token-type weighting, would reveal the robustness–complexity frontier.\n4. Baselines could include some selective prediction and abstention methods (e.g., calibrated confidence, dedicated refuse-to-answer training). This will allow IC to be compared with the strongest alternatives."}, "questions": {"value": "Please refer to the previous weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dAl5faRWOj", "forum": "11QZITAMUO", "replyto": "11QZITAMUO", "signatures": ["ICLR.cc/2026/Conference/Submission13943/Reviewer_wAg8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13943/Reviewer_wAg8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761831932793, "cdate": 1761831932793, "tmdate": 1762924445436, "mdate": 1762924445436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a training-free, pre-generation measure of query-level uncertainty called Internal Confidence (IC). Instead of generating an answer and then judging it, IC probes the model’s hidden states at many token positions nnn and layers lll using the model’s unembedding rows for {YES, NO}. These probe scores are then aggregated with decay weights around a “decision center” (empirically near the last token & last layer) to produce a single confidence score. The paper motivates the center by showing that discrimination between known/unknown queries often peaks at an interior token×layer location, then ensembles nearby positions via an “attenuated encoding”. Experiments on factual QA and math reasoning claim better AUC/PRR and calibration (ECE) than query-level baselines, and show major speedups versus answer-level methods (e.g., SAR), enabling applications like cost-aware RAG routing and model cascading."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Pre-generation, single-pass signal. IC avoids generating answers and extra prompts; it’s computed from internal states with one forward pass, which is appealing for latency/cost.\n\nCenter-weighted ensembling across tokens×layers. The “decision center” idea is well-motivated by heatmaps showing the best separator isn’t always exactly the last position; the attenuated average reduces variance while keeping locality.\n\nPractical routing use-cases. Clear demonstrations for RAG triggering and small→large model cascading with thresholding on IC (trade-off/optimal regions)."}, "weaknesses": {"value": "1) Baselines. It is puzzling that the baselines differ between Tables 1 and 2; they should be consistent to allow a fair comparison of self-knowledge and efficiency across models and tasks. Moreover, the state-of-the-art claim cannot be substantiated without including a broader set of strong baselines. Following the recent TACL benchmark on uncertainty quantification [1] , at minimum the evaluation should cover half of these baselines (that outperform SAR as a top baseline from the paper): CCP, Maximum Sequence Probability (MSP), EigValLaplacian, Lexical Similarity (ROUGE-L), DegMat, TokenSAR, representative verbalized-uncertainty methods, and the recently proposed RAUQ. For references and straightforward replication, please see LM-Polygraph.\n\n\n2) Datasets. The current dataset selection provides limited coverage of multi-hop and genuinely challenging questions. Are the proposed methods generalizable to such settings? TriviaQA and SciQ contain many single-fact lookups and shallow cues, which can inflate confidence estimates. Please consider adding SimpleQA [2]  (challenging for LLMs and less likely to be contaminated by pretraining), TruthfulQA[3]  (tests suppression of “popular but false” answers), and multi-hop benchmarks such as MuSiQue[4] and 2WikiMultihopQA [5].\n\n        For mathematics, GSM8K largely features short, templated chains, where uncertainty may     correlate with sequence length or final-step carry errors rather than true multi-step epistemic gaps. We recommend evaluating on GSM8K-Hard[6], which require longer chains and introduce more realistic failure modes.\n\n3) Adaptive RAG. What you refer to as “efficient RAG” is a fast-moving area with a substantial body of adaptive RAG/adaptive retrieval baselines. To substantiate the contribution, it’s important to compare against representative prior work—for example DRAGIN [7], FLARE [8], SEAKR[9],  AdaptiveRAG[10], and recent approaches that leverage uncertainty [11]  and LLM-independent features [12]. Including these would better situate your method within the existing literature.\n\n4) Access assumptions. IC needs logits / unembedding and hidden states. That’s fine for open-weights models, but not available for black-box APIs, where answer-level methods (e.g., P(True)) might be more deployable. Can the method work if black-box models provide access to the last layer, some per-output-token log-probabilities?\n\n\n5) Definition of “knowledge boundary.” The authors evaluate answerability under greedy decoding as a proxy; this can misclassify queries where non-greedy decoding would succeed, so “known vs unknown” labels are not absolute.\n\n\nTypos:: Notation mismatch: “w = 1.0” vs Eq. (4). In §4.5 the text says “vary the w in Equation 4 … default w = 1.0 (Locality ≈ 0.7).” But Eq. (4) defines the attenuation with parameter α controlling locality. This reads like a symbol mix-up; likely they meant α = 1.0. Please unify notation.\n\n\n[1] Vashurin, R., Fadeeva, E., Vazhentsev, A., Rvanova, L., Vasilev, D., Tsvigun, A., ... & Shelmanov, A. (2025). Benchmarking uncertainty quantification methods for large language models with lm-polygraph. Transactions of the Association for Computational Linguistics, 13, 220-248.\n\n[2] Wei, J., Karina, N., Chung, H. W., Jiao, Y. J., Papay, S., Glaese, A., ... & Fedus, W. (2024). Measuring short-form factuality in large language models. arXiv preprint arXiv:2411.04368.\n\n[3] Lin, S., Hilton, J., & Evans, O. (2021). Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.\n\n[4] Trivedi, H., Balasubramanian, N., Khot, T., & Sabharwal, A. (2022). ♫ MuSiQue: Multihop Questions via Single-hop Question Composition. Transactions of the Association for Computational Linguistics, 10, 539-554.\n\n[5] Ho, X., Nguyen, A. K. D., Sugawara, S., & Aizawa, A. (2020). Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060.\n\n[6] Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., ... & Neubig, G. (2023, July). Pal: Program-aided language models. In International Conference on Machine Learning (pp. 10764-10799). PMLR.\n\n[7] Su, W., Tang, Y., Ai, Q., Wu, Z., & Liu, Y. (2024). DRAGIN: dynamic retrieval augmented generation based on the information needs of large language models. arXiv preprint arXiv:2403.10081.\n\n[8] Jiang, Z., Xu, F. F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., ... & Neubig, G. (2023, December). Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (pp. 7969-7992).\n\n[9]  Yao, Z., Qi, W., Pan, L., Cao, S., Hu, L., Liu, W., ... & Li, J. (2024). Seakr: Self-aware knowledge retrieval for adaptive retrieval augmented generation. arXiv preprint arXiv:2406.19215.\n\n[10] Jeong, S., Baek, J., Cho, S., Hwang, S. J., & Park, J. C. (2024). Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity. arXiv preprint arXiv:2403.14403.\n\n[11] Moskvoretskii, V., Lysyuk, M., Salnikov, M., Ivanov, N., Pletenev, S., Galimzianova, D., ... & Panchenko, A. (2025). Adaptive retrieval without self-knowledge? bringing uncertainty back home. arXiv preprint arXiv:2501.12835.\n\n[12] Marina, M., Ivanov, N., Pletenev, S., Salnikov, M., Galimzianova, D., Krayko, N., ... & Moskvoretskii, V. (2025). LLM-Independent Adaptive RAG: Let the Question Speak for Itself. arXiv preprint arXiv:2505.04253."}, "questions": {"value": "I appreciate the training-free setup and the promise of OOD-robust, pre-generation uncertainty. However, the evidence feels incomplete primarily because key, state-of-the-art baselines are missing. If the authors add stronger baselines (e.g., CCP, MSP, TokenSAR, DegMat/EigVal, RAUQ, robust verbalized-uncertainty) and evaluate on harder datasets, I could  revise my view. I also recommend reframing the main claim around efficiency (with explicit token/latency budgets) and presenting accuracy as on-par with SOTA where that is borne out."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "YunW1UpHD5", "forum": "11QZITAMUO", "replyto": "11QZITAMUO", "signatures": ["ICLR.cc/2026/Conference/Submission13943/Reviewer_ZpJR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13943/Reviewer_ZpJR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761929478467, "cdate": 1761929478467, "tmdate": 1762924444943, "mdate": 1762924444943, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}