{"id": "GcVVJ1zu3Y", "number": 1723, "cdate": 1756912299003, "mdate": 1759898191782, "content": {"title": "SUN-DSBO: A Structured Unified Framework for Nonconvex Decentralized Stochastic Bilevel Optimization", "abstract": "Decentralized stochastic bilevel optimization (DSBO) is a powerful tool for various machine learning tasks, including decentralized meta-learning and hyperparameter tuning. Existing DSBO methods primarily address problems with strongly convex lower-level objective functions. However, nonconvex objective functions are increasingly prevalent in modern deep learning. In this work, we introduce SUN-DSBO, a Structured Unified framework for Nonconvex DSBO, in which both the upper- and lower-level objective functions may be nonconvex. Notably, SUN-DSBO offers the flexibility to incorporate decentralized stochastic gradient descent or various techniques for mitigating data heterogeneity, such as gradient tracking (GT). We demonstrate that SUN-DSBO-GT, an adaptation of the GT technique within our framework, achieves a linear speedup with respect to the number of agents. This is accomplished without relying on restrictive assumptions, such as gradient boundedness or any specific assumptions regarding gradient heterogeneity. Numerical experiments validate the effectiveness of our method.", "tldr": "", "keywords": ["Decentralized optimization", "Bilevel optimization", "Nonconvex optimization", "Stochastic optimization", "First-order methods", "Heterogeneous data distributions"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a0d38d50a011d119f5a2255f19768788bd0f726b.pdf", "supplementary_material": "/attachment/1b9cceb14498ba0b124d9ace22a1c0753a4830ad.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes SUN-DSBO, a decentralized stochastic bilevel optimization framework built upon the Moreau envelope, based penalty method (Liu et al., 2024). The authors reformulate the penalty function as a nonconvex–strongly concave min–max problem and develop decentralized algorithms with convergence guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The unified framework idea is neat and general.\n2. Writing is clear and easy to read."}, "weaknesses": {"value": "1.  In line.163, this paper claims that the constraint   $G(x,y) - V_\\gamma(x,y) \\le 0 $  is equivalent to enforcing $ \\nabla_y G(x,y) = 0 $ under mild conditions.  This is not true in general unless $G$ satisfies convexity or the Polyak–Łojasiewicz (PL) condition.  The equivalence only holds approximately. \nI believe the authors are aware that the equivalence only holds under convexity or PL assumptions, yet they still claim to “handle general nonconvex lower-level objectives.” This is not technically incorrect but conceptually misleading, the proposed method actually optimizes a surrogate relaxation rather than the original nonconvex bilevel problem.\n\n2. The empirical evaluation measures performance while optimizing the surrogate objective  $\\Psi_{\\mu}(x,y) = \\mu F(x,y) + G(x,y)-V_{\\gamma}(x,y)$, not the original bilevel constraints. \nThere is no diagnostic of how well the solutions satisfy $G(x,y) - V_{\\gamma}(x,y) \\le 0$ or approximate the true lower-level stationarity/KKT conditions as $\\mu \\to 0$. \nHence, the experiments demonstrate practical effectiveness of the surrogate method but do not validate the claimed equivalence to the original nonconvex bilevel problem.\n\n\n3. The key building blocks of SUN-DSBO, gradient tracking for decentralized communication and the Moreau-envelope–based penalty for smoothing the lower-level objective are not novel. The paper does not introduce any new mechanism that fundamentally changes or extends these tools; it merely combines them in a straightforward way.Consequently, the “structured unified framework” is more of an incremental integration of existing methods rather than a conceptually new algorithmic design.\n\n\n4. Typo: (i) In Eq.(39), what is $t$ is $x_i^{t,k}$? (ii) The paper writes $\\mathcal{F}_0 :={ \\Omega, \\phi },$, is $\\phi $ from eq.(26)? Or do you mean empty set?"}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mb18UK5lZN", "forum": "GcVVJ1zu3Y", "replyto": "GcVVJ1zu3Y", "signatures": ["ICLR.cc/2026/Conference/Submission1723/Reviewer_Rt4P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1723/Reviewer_Rt4P"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761105097064, "cdate": 1761105097064, "tmdate": 1762915869758, "mdate": 1762915869758, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers Decentralized Stochastic Bilevel Optimization (DSBO) problems, where both upper- and lower-level objectives can be nonconvex. To tackle the non-convexity in both levels, the authors first reformulates DSBO as a nonconvex–strongly-concave min–max problem using a Moreau envelope–based penalty method, then the authors propose SUN-DSBO (Structured Unified framework for Nonconvex DSBO), which allows use of first-order stochastic gradients only.\n\nThey provide solid convergence analysis of the convergence rates and concensus error of the algorithms, and they conduct numerical experiments to support the claims of their paper and showcase the advantages over existing DSBO algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper proposes a novel algorithm, which solves the DSBO problem with non-convex upper-level and lower-level objective functions. They provide solid convergence analysis of the convergence to stationarity and concensus among different agents. The authors also conduct some experiments to support their findings.\n\n2. The algorithms achieve linear speedup effect, a non-trivial result in decentralized optimization literature.\n\n3. Source code is provided."}, "weaknesses": {"value": "1. One major limitation is, the idea and methodology of this paper seem to follow [1]. The proposed algorithms SUN-DSBO-SE/GT seem to be a direct extension of the centralized case considered in [1]. The techinques used in this paper, such as gradient tracking, consensus analysis, and convergence analysis are standard in existing distributed optimization literature.\n\n2. The experiments seem to be limited on relatively simple examples/problems.\n\nRef:\n\n[1] Moreau envelope for nonconvex bi-level optimization: A single-loop and hessian-free solution strategy."}, "questions": {"value": "1. Could the authors provide some discussions on why combining the existing methods, such as Moreau envelope–based penalty method, gradient tracking, and convergence/concensus analysis is non-trivial in this nonconvex DSBO problem.\n\n2. Would it be possible to consider some experimental setup in other types of problems, such as RLHF in [2]?\n\nRef:\n\n[2] PARL: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "aSYfaXYxhY", "forum": "GcVVJ1zu3Y", "replyto": "GcVVJ1zu3Y", "signatures": ["ICLR.cc/2026/Conference/Submission1723/Reviewer_S5im"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1723/Reviewer_S5im"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761443139719, "cdate": 1761443139719, "tmdate": 1762915869289, "mdate": 1762915869289, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SUN-DSBO, a Structured Unified framework for solving nonconvex decentralized stochastic bilevel optimization (DSBO) problems. The key idea is a Moreau-envelope based penalty reformulation that converts the bilevel structure into a nonconvex–strongly-concave min–max problem, allowing purely first-order, single-loop updates across distributed agents. Two algorithmic instances are studied in the paper: first, SUN-DSBO-SE, a decentralized SGD variant, and second, SUN-DSBO-GT, which incorporates gradient tracking (GT) for heterogeneous data. The authors establish finite-time convergence to stationary points under weaker assumptions than prior DSBO works without lower-level strong convexity, Hessians, or bounded-gradient conditions, and prove linear speedup in the number of agents."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1) This is the first DSBO method that rigorously handles *nonconvex–nonconvex* bilevel objectives in a decentralized, stochastic setting. Previous approaches (SPARKLE, D-SOBA, SLDBO) required lower-level strong convexity or Hessian-based hypergradients; SUN-DSBO works under far those milder assumptions. \n\n2) The Moreau-envelope penalty and auxiliary variable (\\theta) yield tractable gradient updates $((D_x,D_y,D_\\theta))$ computable via mini-batch gradients, which remove all second-order dependencies.\n\n3) The update directions depend linearly on both upper- and lower-level gradients, allowing plug-in communication schemes (GT, EXTRA, Exact-Diffusion). This unification could subsume several prior DSBO algorithms as special cases. \n\n3) The convergence analysis does not assume bounded gradients or homogeneity; yet SUN-DSBO-GT achieves $(O(1/\\sqrt{nK}))$ convergence and linear speedup. \n\n4) SUN-DSBO-GT remains stable as network connectivity weakens (higher ρ), confirming its robustness to limited communication."}, "weaknesses": {"value": "1) Only the GT strategy is analyzed theoretically; variants using **EXTRA** or **Exact-Diffusion** remain unexplored, and a unified convergence proof would strengthen the framework. \n\n2) The paper lacks a systematic study of the penalty parameters (\\mu) and (\\gamma), which affect stability and constraint tightness.\n\n3) Although GT achieves linear speedup, the communication-vs-computation trade-off (messages per ε-stationary point) is not quantified."}, "questions": {"value": "- How sensitive is the method to the choice of penalty parameters (\\mu_k) and (\\gamma)? The paper mentions coarse-to-fine tuning, but it would be useful to know whether the method remains stable across a wide range of settings or if specific heuristics were necessary.\n\n- For SUN-DSBO-GT, could the authors provide an analysis (empirical or theoretical) of the communication overhead versus the achieved linear speedup? For example, how many communication rounds per iteration are needed to maintain the reported convergence rate?\n\n- In Section 3, the equivalence between the penalized formulation and the original bilevel objective seems to rely on certain smoothness or PL-type conditions. Can the authors clarify precisely which assumptions guarantee that optimizing the penalized $(\\Psi_\\mu)$ leads to a stationary point of the original bilevel problem when the lower-level is fully nonconvex?\n\n- How does the method behave under sparse or dynamic communication graphs (larger $(\\rho)$)? The current experiments show robustness, but it would be valuable to understand theoretical or empirical scaling limits with respect to network connectivity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WR2srmGngJ", "forum": "GcVVJ1zu3Y", "replyto": "GcVVJ1zu3Y", "signatures": ["ICLR.cc/2026/Conference/Submission1723/Reviewer_sEex"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1723/Reviewer_sEex"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976887575, "cdate": 1761976887575, "tmdate": 1762915869101, "mdate": 1762915869101, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SUN-DSBO, a Structured Unified framework for non-convex decentralized stochastic bilevel optimization (DSBO), which generalizes prior work by relaxing the common assumption of strong convexity in the lower-level problem. The authors present two algorithmic instances under this framework: SUN-DSBO-SE (based on decentralized SGD) and SUN-DSBO-GT (incorporating gradient tracking to address data heterogeneity).\nThe framework is built upon a Moreau-envelope-based penalized reformulation of the bilevel problem and further recast as a min-max formulation, allowing for efficient stochastic estimation. The authors provide rigorous theoretical guarantees for both algorithms under relaxed assumptions and validate their claims through comprehensive experiments on hyper-cleaning (Fashion-MNIST) and hyper-representation (MNIST & CIFAR-10), showing improvements over several state-of-the-art baseline algorithms."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposed SUN-DSBO framework broadens the scope of DSBO by addressing the nonconvexity of the lower-level objective, a setting largely unexplored in decentralized bilevel optimization.\n2. SUN-DSBO is flexible enough to accommodate various decentralized schemes (e.g., GT, EXTRA), making it extensible and modular.\n3. The paper provides finite-time convergence analysis for both SUN-DSBO-SE and SUN-DSBO-GT under realistic and relaxed assumptions. Notably, SUN-DSBO-GT achieves linear speedup with respect to the number of agents."}, "weaknesses": {"value": "1. Although a direct comparison of the theoretical results with prior DSBO works may be somewhat unfair, given that those methods typically rely on additional assumptions such as strong convexity of the lower-level problem, it is still valuable to include such a comparison to highlight the differences and advancements introduced by this work.\n2. While the paper claims that SUN-DSBO-GT introduces more overhead than SUN-DSBO-SE, no quantitative comparison of communication cost, memory usage, or wall-clock time is provided. This is particularly important in decentralized scenarios where bandwidth and device constraints are bottlenecks.\n3. Although the framework claims to support a wide range of decentralized strategies (e.g., EXTRA, Exact Diffusion, and mixing strategies), only GT and SGD variants are empirically evaluated. This limits the practical demonstration of the claimed “unified” flexibility. A more comprehensive evaluation incorporating these additional strategies would significantly strengthen the contribution.\n4. While the chosen tasks, hyper-cleaning and hyper-representation, are meaningful and relevant, exploring other bilevel applications such as decentralized meta-learning would help demonstrate the broader applicability and generality of the proposed framework.\n5. The paper is notation-heavy, which may hinder readability. In particular, some notations could be streamlined or merged. For example, the distinction between $\\hat{D}$ and $\\tilde{D}$ in Line 191 could be reconsidered to simplify exposition."}, "questions": {"value": "In Line 155, the authors state that $G(x,y)-V_\\gamma (x,y)\\ge 0 $ by the definition of $V_\\gamma$. However, Equation (2) imposes the constraint $G(x,y)-V_\\gamma (x,y)\\le 0 $. Could the authors clarify how this constraint can be satisfied given the stated inequality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jts7T7hX6A", "forum": "GcVVJ1zu3Y", "replyto": "GcVVJ1zu3Y", "signatures": ["ICLR.cc/2026/Conference/Submission1723/Reviewer_sbxh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1723/Reviewer_sbxh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1723/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987726335, "cdate": 1761987726335, "tmdate": 1762915868481, "mdate": 1762915868481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}