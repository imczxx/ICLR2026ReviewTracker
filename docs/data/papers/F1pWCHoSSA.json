{"id": "F1pWCHoSSA", "number": 13122, "cdate": 1758213801934, "mdate": 1759897463037, "content": {"title": "Unbiased Visual Reasoning with Controlled Visual Inputs", "abstract": "End-to-end Vision-language models (VLMs) often rely on spurious visual cues, conflating perception with decision-making. We introduce VISTA (Visual Information Separation for Text-based Analysis), which enforces an explicit information bottleneck between a text-only reasoner and a stateless VLM sensor. The LLM reasoner decomposes each question and iteratively queries a VLM for visual facts; the VLM is instructed to reject queries that require high-level inference, creating an explicit information bottleneck. Trained on only 641 questions, VISTA yields large robustness gains on SpuriVerse across two vision backbones (+16.29\\% with Qwen-2.5-VL-7B and +6.77\\% with Llama-3.2-Vision-11B), while direct SFT or RL on the VLM fails to remedy spuriosity and can even exacerbate it. Despite never exposing the reasoner to raw pixels, VISTA slightly improves or remains on par with VLMs on everyday-scene benchmarks, including MMVP and SeedBench. Our learned reasoners transfer across sensors, indicating algorithmic rather than model-specific generalization. Together, VISTA enables spurious-resistant VQA by upgrading the brain, not the eyes.", "tldr": "", "keywords": ["Visual Question Answering", "Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b4e19eb066f788c0a9c9b44282eac5035e906d31.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes VISTA, a framework that improves visual reasoning by decoupling perception from reasoning. Instead of training a single vlm end-to-end, VISTA introduces a two-part system: a VLM sensor and a text-only LLM reasoner. The sensor acts like a visual probe, it sees the image but can only answer simple, factual perception questions such as object existence, color, or spatial relations, and rejects any inferential or subjective queries. The reasoner plans which visual facts to ask about, gathers them step-by-step, and then decides the final answer through reasoning alone.\n\nThis design enforces a strong information bottleneck, preventing the system from relying on spurious visual correlations (like background cues or stereotypes) that often mislead end-to-end models. Trained using reinforcement learning (GRPO) on a small set of 641 curated questions, VISTA achieves significant robustness gains on the SpuriVerse benchmark while maintaining comparable accuracy on MMVP and SeedBench. Ablation studies show that the rejection bottleneck is key to resisting bias, and removing it trades robustness for higher raw accuracy. The learned reasoning policy also transfers across unseen sensors, proving it learns algorithmic reasoning, not model-specific tricks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. VISTA introduces a clear and principled separation between perception and reasoning.\n2. The paper identifies and articulates a real, under-addressed failure mode of end-to-end VLMs.\n3. The sensor–reasoner design is modular, interpretable, and implementation-friendly."}, "weaknesses": {"value": "1.Larger VLM sensors (e.g., Llama3.2-Vision) sometimes underperform smaller ones (e.g., Qwen2.5-VL), contrary to expected scaling trends.\n\nQuestion: Can the authors explain why stronger sensors do not yield better reasoning outcomes under the VISTA setup? Is this due to an information bottleneck, training instability, or another factor?\n\n2.The GRPO reinforcement signal improves some benchmarks (e.g., SpuriVerse) but has negligible or negative effects on others (e.g., MMVP).\nQuestion: What causes this inconsistency? Did the authors experiment with alternative reward functions or training schedules to stabilize performance?\n\n3. The reasoning policy is trained on only 641 curated examples, which seems insufficient for robust generalization.\nQuestion: How sensitive are the results to this small dataset? Would scaling up training data or incorporating noisier but larger supervision alter the observed outcomes?\n\n4. While the controlled bottleneck improves robustness, it reduces accuracy on standard benchmarks such as SeedBench.\nQuestion: Can the authors quantify this trade-off and justify the reduction in clean-scenario performance as an acceptable cost for improved robustness?\n\n5. Evaluation is limited to SpuriVerse and MMVP, which, while interesting, lack diversity and scale.\nQuestion: Why were broader multimodal or reasoning benchmarks (e.g., GQA, VizWiz, ScienceQA) excluded from evaluation?\n\n6. Ambiguous causal attribution\nIt remains unclear whether the robustness gains arise from the rejection rule, RL regularization, or the architectural separation itself.\nQuestion: Did the authors isolate the effects of the architectural split from those of the training regime to identify the primary driver of improvement?"}, "questions": {"value": "Please see in weaknesses (6 questions)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FSnLxPjkyM", "forum": "F1pWCHoSSA", "replyto": "F1pWCHoSSA", "signatures": ["ICLR.cc/2026/Conference/Submission13122/Reviewer_XTin"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13122/Reviewer_XTin"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761448136652, "cdate": 1761448136652, "tmdate": 1762923847203, "mdate": 1762923847203, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents an interesting framework in which an LLM performs textual reasoning in a Chain-of-Thought (CoT) style and issues queries to a VLM that handles perception-only tasks. I regard this setup as an instantiation of the general ReAct framework, with the VLM functioning as the action executor. Nonetheless, the proposed approach remains interesting for two reasons:\n (1) it explicitly disentangles textual reasoning from visual understanding—a design philosophy commonly adopted by many visual reasoners; and\n (2) it provides some theoretical analysis of the framework, although it is unclear how this analysis connects to the central problem the paper aims to address: shortcuts that correlate spuriously with the correct answer. \n\nMy main concern lies in the empirical results: the proposed method generally underperforms compared to end-to-end training with reinforcement learning (as shown in Table 1). I am also curious why no end-to-end (RL) results are reported for Llama3.2-Vision. Is it because VISTA performs worse than the end-to-end (RL) counterpart on this model? While the authors provide some explanations in the experimental analysis, they are not sufficiently convincing to demonstrate that the proposed method offers clear value to the community."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "an interesting framework for visual reasoning"}, "weaknesses": {"value": "see my comments in Summary"}, "questions": {"value": "see my comments in Summary"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OMBTPLl9c7", "forum": "F1pWCHoSSA", "replyto": "F1pWCHoSSA", "signatures": ["ICLR.cc/2026/Conference/Submission13122/Reviewer_cy7Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13122/Reviewer_cy7Z"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761755020940, "cdate": 1761755020940, "tmdate": 1762923846925, "mdate": 1762923846925, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework named VISTA to address the issue of shortcut learning in vision-language models (VLMs), where models tend to rely on superficial visual cues rather than developing a deep understanding of the logical relationships between questions and visual inputs. The VISTA framework explicitly decomposes the reasoning process into a visual sensor (VLM) for perception and a reasoning module (LLM) for logical inference, thereby mitigating the influence of shortcut learning. Experimental results on two benchmarks, MMVP and SeedBench-500, demonstrate the effectiveness of the proposed approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors propose the VISTA framework to address shortcut learning in VLMs, which explicitly separates visual perception (sensor) from logical reasoning (reasoner) to mitigate reliance on spurious visual cues."}, "weaknesses": {"value": "1. While the VISTA framework attempts to address shortcut learning by employing a dual-agent architecture (VLM + LLM), this approach does not fundamentally solve the underlying issue within the VLM itself. The VLM component remains susceptible to shortcut learning, merely transferring rather than resolving this critical limitation.\n\n2. The evaluation is currently limited to established benchmarks. To better demonstrate the method's robustness and generalizability, performance should be validated on more recent and challenging VQA benchmarks such as MMMU and MMMU-Pro."}, "questions": {"value": "1. Does the VLM component itself still suffer from shortcut learning? In the proposed agent system, the VLM appears to be reduced to a perceptual module, leaving its inherent shortcut learning issues unaddressed.\n\n2. How does the method generalize to more comprehensive benchmarks? Evaluation on challenging benchmarks such as MMMU and MMMU-Pro would better demonstrate its generalization capability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "CqUZz8rb2Q", "forum": "F1pWCHoSSA", "replyto": "F1pWCHoSSA", "signatures": ["ICLR.cc/2026/Conference/Submission13122/Reviewer_y7z3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13122/Reviewer_y7z3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762016779949, "cdate": 1762016779949, "tmdate": 1762923846667, "mdate": 1762923846667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Visual Information Separation for Text-based Analysis (VISTA), a framework that enforces an information bottleneck between a text-only reasoner and a VLM to mitigate spurious visual correlations (hopefully). By restricting the sensor to answer only low-level perceptual queries, VISTA separates perception from reasoning and promotes evidence-seeking behaviors. On SpuriVerse, MMVP, and SeedBench, VISTA achieves claimed robustness gains while maintaining comparable general accuracy. Theoretical analysis links improved generalization to reduced information bandwidth across the sensor–reasoner interface."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, I like the high-level motivation which limits the VLMs to do what they can do. For this direction, actually I expect to see more analysis from how to determine what VLMs can do well, instead of pretty unclear queries accept or reject in a straightforward way. Anyway, targeting spurious visual correlations in VLMs is very related to recent progress in VLMs. \n\nEmpirical results across multiple benchmarks demonstrate certain robustness and cross-model generalization with minimal data and training cost. Some ablation studied are also included."}, "weaknesses": {"value": "- The biggest weaknesses to me is the experimental settings. MMVP is such a small-scale dataset with only 150 images pair, and the author randomly 500 samples subset from SeedBench. The choice of experiments are hard to delivery something reliable. Besides, as the author mentioned the evaluated datasets are \"everyday-scene benchmark\". However, as this paper is motivated by \"existing VLMs rely on spurious visual cues, conflating perception\", there are datasets suitable for this purpose, such as ViLP (https://arxiv.org/pdf/2501.00569) and HallusionBench (https://arxiv.org/abs/2310.14566). I would recommend the authors seriously consider extending the evaluation benchmarks, not limited to what I suggested. \n\n- The proposed theoretical bound seems not non-trivial."}, "questions": {"value": "- I am confused the difference of \"Are there multiple dots and a white flag with an orange pole in the painting?\" and \"What is in the image?\", the later question what is in the image requires more reasoning & descriptions, while the former one is evidence checking. I am actually confused what are boundaries of accepted vs. rejected queries. \n\n- How do you compute the advantage for the used GRPO? \n\n- Besides, I raised some questions above in the weakness section. \n\nI will adjust my final scores based on the response."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SUY6xWOACI", "forum": "F1pWCHoSSA", "replyto": "F1pWCHoSSA", "signatures": ["ICLR.cc/2026/Conference/Submission13122/Reviewer_DdDM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13122/Reviewer_DdDM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762049994420, "cdate": 1762049994420, "tmdate": 1762923846398, "mdate": 1762923846398, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle the persistent issue of spurious correlations in vision–language models (VLMs), where models often conflate perception with reasoning. To address this, the authors propose VISTA (Visual Information Separation for Text-based Analysis), a framework that enforces an explicit information bottleneck between a text-only reasoner and a stateless visual sensor. The reasoner iteratively queries the sensor for perception-level facts, while the sensor rejects high-level inference requests to prevent shortcut learning. Through reinforcement learning, VISTA develops neutral, evidence-seeking reasoning policies. Experiments on SpuriVerse, MMVP, and SeedBench show substantial robustness gains against spurious cues while maintaining comparable accuracy on everyday visual tasks. The results demonstrate that decoupling perception from reasoning improves generalization and mitigates visual bias in multimodal systems."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1)\tThe paper crisply identifies spurious-cue reliance and the conflation of perception and reasoning in end-to-end VLMs, motivating a modular remedy.\n2)\tVISTA enforces an explicit information bottleneck between a text-only reasoner and a stateless VLM sensor, cleanly separating decision-making from raw pixels.\n3)\tThe sensor accepts only six classes of perception queries and rejects high-level inference, with a concrete policy and examples."}, "weaknesses": {"value": "1)\tThe proposed information bottleneck between the sensor and reasoner is conceptually interesting, but it may also introduce new risks. By restricting the reasoner’s access to full and detailed visual information, the model could miss critical cues needed for complex reasoning. Moreover, if the stateless visual sensor makes errors or misinterprets the scene, the reasoner has no means to recover or verify the missing context, potentially amplifying mistakes. The paper should further analyze and discuss this trade-off between robustness to shortcuts and vulnerability to information loss\n2)\tIt is unclear whether the authors plan to release their code and trained models. Given that the paper’s main contribution lies in the proposed VISTA framework and its controlled perception–reasoning interface, public release is crucial for reproducibility and community validation.\n3)\tThe paper should further analyze whether the model truly learns accurate and coherent reasoning after GRPO training. Since the reward is assigned only based on the final answer correctness, it is unclear whether the intermediate chain-of-thought steps generated by the reasoner are logically sound or merely optimized for outcome matching. Without evaluating the quality or faithfulness of these reasoning traces, the claimed improvement in reasoning robustness remains uncertain.\n4)\tThe sensor accepts only a fixed set of perception query types, which may limit generalization to unseen reasoning formats or richer visual evidence needs.\n5)\tThe multi-turn setup allows up to 24 rounds and 8192 tokens per episode, yet there is no throughput/cost analysis to assess deployment practicality."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "eoCOnD7tjM", "forum": "F1pWCHoSSA", "replyto": "F1pWCHoSSA", "signatures": ["ICLR.cc/2026/Conference/Submission13122/Reviewer_gjSm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13122/Reviewer_gjSm"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13122/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762056167685, "cdate": 1762056167685, "tmdate": 1762923846115, "mdate": 1762923846115, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}