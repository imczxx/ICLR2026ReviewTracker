{"id": "ZPam0jRjTC", "number": 18188, "cdate": 1758284848969, "mdate": 1763685248328, "content": {"title": "Outcome-Aware Spectral Feature Learning for Instrumental Variable Regression", "abstract": "We address the problem of causal effect estimation in the presence of hidden confounders using nonparametric instrumental variable (IV) regression. An established approach is to use estimators based on learned \\emph{spectral features}, that is, features spanning the top singular subspaces of the operator linking treatments to instruments. While powerful, such features are agnostic to the outcome variable. Consequently, the method can fail when the true causal function is poorly represented by these dominant singular functions.\n\nTo mitigate, we introduce **Augmented Spectral Feature Learning**, a framework that makes the feature learning process **outcome-aware**. Our method learns features by minimizing a novel contrastive loss derived from an **augmented** operator that incorporates information from the outcome. By learning these task-specific features, our approach remains effective even under spectral misalignment. We provide a theoretical analysis of this framework and validate our approach on challenging benchmarks.", "tldr": "", "keywords": ["instrumental variable regression", "NPIV", "nonparametric statistics", "feature learning", "causal inference", "operator learning"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f3fc802ef938b77f0cb0d5838ed128f9bb459536.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles causal effect estimation under hidden confounding using nonparametric instrumental variable regression. Existing spectral feature learning methods rely on features from the top singular subspaces of the treatment–instrument operator, which are outcome-agnostic and may fail when the true causal function is misaligned with these subspaces. To overcome this, the authors propose Augmented Spectral Feature Learning, which introduces outcome-awareness by constructing an augmented operator that integrates information from the outcome variable. A novel contrastive loss enables learning task-specific spectral features robust to spectral misalignment. The paper provides theoretical guarantees and empirical results demonstrating improved accuracy and robustness on challenging synthetic and semi-synthetic benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper identifies a fundamental limitation of spectral NPIV methods (outcome agnosticism leading to failure under spectral misalignment) and proposes a principled solution.\n2. The proposed method is supported by rigorous theoretical analysis.\n3. Experimental results demonstrate that the proposed approach."}, "weaknesses": {"value": "1. The method enhances the operator using a rank-one augmentation, which simplifies theory but limits its ability to capture complex, multidimensional dependencies between $Y$ and the instrument–treatment relationship.\n\n2. All experiments are conducted on synthetic or semi-synthetic datasets (e.g., dSprites), leaving the method’s practical effectiveness in noisy, unstructured real-world environments untested.\n\n3. The paper evaluates the proposed method against only a few baselines (e.g., KIV, DFIV), missing more recent state-of-the-art approaches for nonparametric IV regression. For an ICLR submission, it would be beneficial to include comparisons with modern representation learning–based IV methods, to better contextualize the contribution and demonstrate broader relevance."}, "questions": {"value": "1. The paper assumes sub-Gaussian distributions (Assumption 4), which might be restrictive in practice. Could the authors clarify how sensitive their theoretical results are to this assumption, and whether the framework could extend to heavier-tailed (e.g., sub-exponential) settings?\n2. The paper is quite dense and may be difficult to follow for readers unfamiliar with spectral feature learning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "VYvoWFTDb3", "forum": "ZPam0jRjTC", "replyto": "ZPam0jRjTC", "signatures": ["ICLR.cc/2026/Conference/Submission18188/Reviewer_3F2d"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18188/Reviewer_3F2d"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761750538309, "cdate": 1761750538309, "tmdate": 1762927939111, "mdate": 1762927939111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the problem of nonparametric instrumental variable (NPIV) regression, a central challenge in causal inference when unobserved confounders bias treatment-outcome relationships. Existing spectral methods, such as SpecIV (Sun et al., 2025), learn low-rank representations of the conditional expectation operator T=E[h(X)∣Z], but are outcome-agnostic, leading to poor estimation when the structural function h_0 is misaligned with the top singular subspace of T. \n\nTo overcome this limitation, the authors propose Augmented Spectral Feature Learning (ASFL), which introduces an outcome-aware contrastive loss that modifies the feature learning process. This is done via an augmented operator T_δ incorporating outcome information through a regularization parameter δ. The method learns features that are both predictable from instruments and predictive of outcomes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "A generalization bound for the two-stage least squares (2SLS) estimator using learned features.\n\nAn analysis showing robustness of ASFL under spectral misalignment.\n\nEmpirical validation on synthetic and dSprites-based IV benchmarks demonstrate the performance of the proposed method。\n\nThe theoretical novelty and conceptual framing are impactful, even if the experiments are limited."}, "weaknesses": {"value": "The tuning parameter δ remains heuristic, with no principled selection strategy, which limits the method’s reproducibility and practical applicability.\n\nWhile theoretically elegant, real-world problems often require capturing multiple outcome-relevant directions. Extending the framework to multi-dimensional or adaptive augmentations would enhance its generality.\n\nThe experimental evaluation is also limited, with comparisons restricted to DFIV and KIV and no validation on real-world datasets."}, "questions": {"value": "Would higher-rank or learned outcome embedding improve robustness?\n\nHow does the computational cost of ASFL compare to SpelIV or DFIV?\n\nHow would the proposed method perform in real econometric or policy evaluation datasets (e.g., education or healthcare)?  \n\nThe theoretical contribution is elegant and addresses an important limitation of SpecIV. However, the empirical validation is limited to synthetic setups and toy datasets. Without evidence of real-world performance or a principled tuning strategy for δ, the paper’s practical impact remains unclear."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JUuaoJsIA9", "forum": "ZPam0jRjTC", "replyto": "ZPam0jRjTC", "signatures": ["ICLR.cc/2026/Conference/Submission18188/Reviewer_nQqr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18188/Reviewer_nQqr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761892258192, "cdate": 1761892258192, "tmdate": 1762927938705, "mdate": 1762927938705, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for outcome aware spectral feature learning in nonparametric instrumental variable regression. It solves a fundamental limitation in existing methods by making the feature learning process sensitive to the outcome variable through augmenting the spectral decomposition with outcome information. This approach maintains robust causal effect estimation even in challenging regimes of spectral misalignment where standard methods fail."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates high originality by identifying and tackling a fundamental, previously overlooked flaw in a state-of-the-art method. It formally characterizes the problem of \"spectral misalignment,\" where outcome-agnostic feature learning fails, and introduces a principled solution through the novel concept of an augmented operator and a corresponding contrastive loss.\n\nThe work is supported by exceptional methodological rigor, combining a compelling theoretical analysis with comprehensive empirical validation. It provides non-asymptotic generalization bounds for the proposed estimator and thoroughly benchmarks the method against strong baselines across synthetic and semi-synthetic datasets, convincingly demonstrating its superiority in challenging regimes."}, "weaknesses": {"value": "While the empirical results are compelling within the constructed synthetic and semi-synthetic frameworks, the practical significance of these benchmarks for real-world causal inference problems remains less clear.  The experiments, including the new dSprites benchmark, operate in controlled environments where the core assumptions of the model, such as the validity of the instrumental variable and the specific form of the structural causal model, are guaranteed by design.  In practice, these assumptions are untestable and often subject to intense debate.  A more convincing demonstration of impact would require application to a real-world dataset with a well-documented and long-standing causal puzzle, showing that the method can generate a plausible and interpretable estimate where traditional approaches have struggled. Furthermore, the paper's reproducibility is currently hampered by the absence of publicly available code.  \n\nI also note that my own expertise in the theoretical foundations of spectral methods for operator learning is limited, and I am therefore unable to provide a substantive assessment of the technical soundness or novelty of the theoretical contributions in Sections 3 and 4."}, "questions": {"value": "Please see my concerns in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "CnqQC7jdQ8", "forum": "ZPam0jRjTC", "replyto": "ZPam0jRjTC", "signatures": ["ICLR.cc/2026/Conference/Submission18188/Reviewer_aUZ6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18188/Reviewer_aUZ6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925214683, "cdate": 1761925214683, "tmdate": 1762927938299, "mdate": 1762927938299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a new method to enhance SSL on graph-structured data by explicitly incorporating outcome information into the feature alignment process. Traditional spectral SSL approaches often rely solely on the graph structure, which may overlook how node features relate to prediction outcomes. This work introduces an outcome-aware spectral feature alignment framework that adjusts the feature space according to the outcome distribution, leading to more discriminative embeddings and improved classification accuracy. The authors derive a principled optimization objective grounded in spectral theory and demonstrate that their approach can be efficiently implemented through eigendecomposition of an adjusted Laplacian matrix. Experimental results across several benchmark datasets show consistent performance gains over classical spectral and graph neural network baselines, highlighting the method’s robustness and scalability."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The paper introduces a novel “outcome-aware” perspective in spectral feature alignment, an area typically dominated by purely structure-based graph methods. It bridges the gap between spectral methods and outcome-driven modeling, creating a new formulation that incorporates outcome distributions directly into spectral regularization.\n- The paper presents a solid theoretical foundation, with clear derivations connecting the proposed objective to classical spectral theory and graph Laplacian properties.\n- The proposed framework is generalizable: it can be adapted to various graph learning settings, including GNN pretraining and kernel-based SSL, broadening its applicability."}, "weaknesses": {"value": "- The paper offers a rigorous spectral derivation, but lacks a clear theoretical link between outcome-aware alignment and generalization performance. The intuition that aligning features with outcomes leads to better predictive embeddings is compelling but not mathematically formalized.\n- The experimental evaluation mainly benchmarks against classical spectral and GNN methods (e.g., GCN, LapRLS), but omits comparison with more recent outcome- or label-sensitive graph models.\n- Although the results show performance gains, the paper provides little qualitative insight into how outcome-awareness modifies the embedding geometry."}, "questions": {"value": "- Does the proposed OSFA method preserve key spectral properties (e.g., orthogonality of eigenvectors, positive semi-definiteness of the Laplacian) after outcome integration?\n- Since OSFA explicitly depends on outcome information, how does it perform when labels are noisy or partially incorrect?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSjAHrVA1u", "forum": "ZPam0jRjTC", "replyto": "ZPam0jRjTC", "signatures": ["ICLR.cc/2026/Conference/Submission18188/Reviewer_s28J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18188/Reviewer_s28J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991949183, "cdate": 1761991949183, "tmdate": 1762927937972, "mdate": 1762927937972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a key limitation in nonparametric instrumental variable (NPIV) regression, where standard spectral feature learning methods learn features that capture the dominant relationship between the instrument Z and the treatment X but ignore the outcome variable Y. This can lead to poor performance when the true causal function is not aligned with the leading singular functions of the treatment-instrument operator. To address this, the authors propose a novel framework called Augmented Spectral Feature Learning, which incorporates outcome information into the feature learning process via an augmented operator and a contrastive loss. The method is supported by theoretical analysis, including a high-probability generalization bound, and is validated on synthetic and semi-synthetic benchmarks, demonstrating significant improvements in challenging misaligned settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper clearly identifies a fundamental weakness in existing spectral IV methods and introduces a simple yet powerful solution by augmenting the feature learning objective with outcome-aware regularization. The idea is intuitive and grounded in operator perturbation theory.\n\n2.The authors provide a non-asymptotic high-probability error bound for the 2SLS estimator and formally characterize when the proposed method outperforms standard spectral learning, particularly in “bad” alignment scenarios.\n\n3.The paper includes both controlled synthetic experiments and a new, more challenging real-world benchmark based on dSprites. The results convincingly show that the proposed method improves upon SpecIV and remains competitive with strong baselines like DFIV.\n\n4.The authors offer useful heuristics for selecting the key hyperparameter δ and propose a diagnostic tool for estimating the alignment between learned features and the true structural function, enhancing the method's practical utility."}, "weaknesses": {"value": "1.While useful heuristics are provided, the selection of δ remains somewhat ad-hoc. A more systematic approach, such as cross-validation or an adaptive strategy, would strengthen the method's scalability and user-friendliness.\n\n2.Some assumptions, such as the boundedness of whitened features and sub-Gaussian errors, may be difficult to verify in practice, especially with high-dimensional or non-stationary data.\n\n3.The current augmentation strategy is limited to a rank-one modification of the operator. It is unclear whether the framework can be extended to richer, higher-rank perturbations for more complex dependencies.\n\n4.The analysis relies on controlling the representation learning error, which is noted as an open problem in deep learning theory. More empirical or approximate bounds would help bridge this gap."}, "questions": {"value": "1.How sensitive is the method to the choice of δ across different datasets? Is there a theoretical or empirical guideline for choosing δ in a data-dependent manner?\n\n2.Could you comment on the computational overhead of your method compared to standard SpecIV, especially during training? How does it scale with sample size and feature dimension?\n\n3.Have you considered evaluating your approach against other recent deep IV methods, such as those based on adversarial training or moment matching, to better situate its performance?\n\n4.Can your method be extended to settings with high-dimensional or structured instruments (e.g., images or time series), and would the current architecture or loss require significant modification?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YSjAHrVA1u", "forum": "ZPam0jRjTC", "replyto": "ZPam0jRjTC", "signatures": ["ICLR.cc/2026/Conference/Submission18188/Reviewer_s28J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18188/Reviewer_s28J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991949183, "cdate": 1761991949183, "tmdate": 1763641469016, "mdate": 1763641469016, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Author Rebuttal by Authors (Part I)"}, "comment": {"value": "We thank all reviewers for their time, feedback, and thoughtful suggestions. Below, we provide a global response addressing two shared concerns and highlight the specific revisions incorporated into the paper. In the resubmitted PDF, all changes are highlighted in blue.\n\n1. **Evaluation on challenging problems.** While we agree that demonstrating real-world impact is important, we would like to emphasize that the core contribution of our paper is solving \"spectral misalignment.\" This is a geometric phenomenon that can *only* be cleanly diagnosed, tested, and validated in a controlled environment where the ground truth structural function $h_0$ is known (as in the dSprites benchmark). In a \"real-world\" scenario where $h_0$ is unknown, it is impossible to verify if misalignment is the cause of failure or if our method succeeds for the right reasons. Nonetheless, having in mind that the expected long-term “real-world” impact of our theoretical work is to make SpecIV more broadly applicable even in potentially misaligned settings, we have added a new, comprehensive **Off-Policy Evaluation (OPE)** experiment in the context of Reinforcement Learning in **Section 6 and Appendix D.7**, which bridges the gap between controlled theory and complex applications. We follow the framework of [1], which demonstrates that Q-function estimation constitutes a confounded regression problem that can be rigorously formulated and solved using NPIV methods. This provides a \"dynamic\" real-world test where the target Y changes iteratively. The results validate our method's robustness and competitiveness with state-of-the-art (SOTA) baselines like DFIV:\n- *Detecting Misalignment* (High δ): In the Cartpole environment, standard SpecIV suffers from high error, suggesting spectral misalignment. Crucially, the hyperparameter tuning protocol of [1] automatically selected a high regularization (δ=1) for our method. This allowed it to correct the misalignment and achieve near-zero error, significantly outperforming DFIV, which struggled in this setting.\n- *Preserving Alignment* (Low δ): In environments like Catch, where standard spectral features are already effective, a small δ ($10^{−2}$) was selected. This demonstrates that AugSpecIV is safe: it defaults to standard spectral behavior when adequate, but engages \"outcome-awareness\" when misalignment is detected.\n\nIn summary, AugSpecIV is consistently on par with or superior to SpecIV by adapting δ to the problem structure. Furthermore, it proves competitive with broad SOTA methods like DFIV, matching its performance on Catch and surpassing it on Cartpole. ​​We have included the detailed result in Section 6 and Appendix D.7 in the updated pdf."}}, "id": "HqIz6cucwt", "forum": "ZPam0jRjTC", "replyto": "ZPam0jRjTC", "signatures": ["ICLR.cc/2026/Conference/Submission18188/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18188/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18188/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763671634404, "cdate": 1763671634404, "tmdate": 1763671634404, "mdate": 1763671634404, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Global Author Rebuttal by Authors (Part II)"}, "comment": {"value": "**2. Higher rank perturbations.** The NPIV problem with a scalar response is fully characterized by the single integral equation $\\mathcal{T} h(Z) = \\mathbb{E}[Y | Z]$. Consequently, for scalar $Y$, the only canonical direction available to perturb $\\mathcal{T}$ is $\\mathbb{E}[Y\\mid Z]$ itself. This direction is distribution-dependent and adaptively reflects the geometry of $Y\\mid Z$; it is not obvious a priori which other directions would be equally natural or meaningful.\n\n(i) Perturbations using higher conditional moments. One possible extension is to introduce perturbations in the directions of higher conditional moments $\\mathbb  E[Y^k | Z],~(k=1,...K)$. Intuitively, this encourages the representation $\\psi(Z)$ to retain predictive information about $Y^k$. Our framework extends directly to this setting, and we evaluate the cases K=2,3 in the resubmitted version. While these variants produced small improvements in estimation error on both dSprites benchmarks, the gains were significantly smaller than those obtained from the core rank-one perturbation used in SpecIV.\n\n(ii) *Perturbations for vector-valued outcomes.* Another natural higher-rank setting arises when $Y$ is vector-valued, $Y=(Y_1,\\dots,Y_K)$, corresponding to simultaneously learning causal effects on multiple outcomes. In this case, the natural perturbation directions become $ \\mathbb E[Y_1 | Z], …,\\mathbb E[Y_K | Z]$. \n\n*Unified formulation.* Both scenarios can be handled in a common way. Let the perturbation directions be $f_1,... f_K$, where in case (i) $f_k =\\mathbb E[Y^k | Z]$ and in case (ii) $f_k =\\mathbb E[Y_k | Z]$. Our original regularisation $\\mathcal{R}_{\\delta} = - \\delta \\mathbb{E}[Y \\psi(Z)] C_Z^{-1} \\mathbb{E}[Y \\psi(Z)]$, extends naturally to a rank-K perturbation \n\n$ - \\sum_{k=1}^K \\delta_k \\mathbb{E}[f_k \\psi(Z)] C_Z^{-1} \\mathbb{E}[f_k \\psi(Z)]$.\n\nThe resulting objective is equivalent to approximating the SVD of $\\mathcal{T}$ perturbed by a rank-K operator in the span of $\\{\\delta_ 1 f_1, …, \\delta_K f_K\\}$.\n\n*Theoretical considerations.* While these appear to be the most natural higher-rank extensions of SpecIV, developing a full learning theory for general rank-K perturbations requires nontrivial extensions of linear perturbation theory and is beyond the scope of the present submission.\n\nA detailed discussion, including the full derivation, is now provided in **Appendix E** of the resubmitted PDF.\n\n[1] Yutian Chen, Liyuan Xu, Caglar Gulcehre, Tom Le Paine, Arthur Gretton, Nando de Freitas, Arnaud Doucet. \"On Instrumental Variable Regression for Deep Oﬄine Policy Evaluation\" JMLR (2022)"}}, "id": "UWlMiUWaD7", "forum": "ZPam0jRjTC", "replyto": "ZPam0jRjTC", "signatures": ["ICLR.cc/2026/Conference/Submission18188/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18188/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission18188/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763671672656, "cdate": 1763671672656, "tmdate": 1763671974157, "mdate": 1763671974157, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}