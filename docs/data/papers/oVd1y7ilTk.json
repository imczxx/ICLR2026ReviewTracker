{"id": "oVd1y7ilTk", "number": 9087, "cdate": 1758110226076, "mdate": 1759897744384, "content": {"title": "On Designing Diffusion Autoencoders for Efficient Generation and Representation Learning", "abstract": "Diffusion autoencoders (DAs) are variants of diffusion generative models that use an input-dependent latent variable to capture representations alongside the diffusion process. These representations can be used for tasks such as downstream classification, controllable generation, and interpolation. However, the generative performance of DAs relies heavily on how well the prior distribution over the latent variables can be modelled and subsequently sampled from. Better generative modelling is also the goal of another class of diffusion models—those that learn their forward (noising) process. While effective at adjusting the noise process in an input-dependent manner, they must satisfy additional constraints derived from the terminal conditions of the diffusion process. Here, we draw a connection between these two classes of models and show that certain design decisions (latent variable choice, conditioning method, etc.) in the DA framework—leading to a model we term DMZ—enable effective representations as evaluated on downstream tasks, including domain transfer, as well as more efficient modelling and generation with fewer denoising steps compared to standard diffusion models.", "tldr": "We propose DMZ, a diffusion autoencoder that combines efficient generation with meaningful representation learning, and study design choices and various applications.", "keywords": ["diffusion models", "diffusion autoencoders", "generative models", "representation learning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7f0e40af6db2c4d6f37220546c071ed2389d3070.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposed a novel diffusion autoencoder technique, DMZ, based on the empirical analysis of latent z. By setting z as discrete binary encoding, using cross-attention as conditioning, and using dense latent variable dimension, it achieved faster convergence, better generation quality, as well as without any prior or loss function. The experiment results and ablation study results demonstrated this conclusion."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. DMZ firstly established the theoretical fundation of learnable forward process of diffusion autoencoder.\n\n2. The design motivation of z is reasonable and inspiring. Correspondingly, the improvements (binary z, etc) are effective in the following experimental results. \n\n3. The experiments are comprehensive, not only demonstrating the effectiveness of each z components, but also extend the task to the other tasks dependent on z (such as stretch2pic). The ablation studies are also reasonable and promising."}, "weaknesses": {"value": "1. Although the learnable forward process is proposed with good motivation, there lacks formal induction or convergence analysis.\n\n2. Benchmark analysis on high-resolution datasets is recommended. \n\n3. There lack interpretability analysis of latent variables, incluing the semantic understanding, differentability, and the combinmation capability as condition for discrete binary z.\n\n4. The flow of method section should be adjusted. DA and learnable forward process should be discussed separately with suitable connections."}, "questions": {"value": "1. Can the authors discuss the connection between DMZ and REPA, which is also a promising baseline in diffusion representation field. In my view, binary z can be seen as a simplified type of external embedding guidance. If so, how about extending z to boarder fields like it is in REPA?\n\n2. How about the experimental results in high-resolution datasets? How does the efficiency change with the lantent dimension? \n\n3. Can the author propose more analysis and results on different types of priors rather than Bernoulli? \n\n4. Is there any insight towards the design of conditioning z by cross attention instead of conditioning it from the residue network?\n\n5. Can DMZ be combined with the current SOTA diffusion models, such as cosistency model and rectified flow?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TKfHu9VLGN", "forum": "oVd1y7ilTk", "replyto": "oVd1y7ilTk", "signatures": ["ICLR.cc/2026/Conference/Submission9087/Reviewer_yKM4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9087/Reviewer_yKM4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791565635, "cdate": 1761791565635, "tmdate": 1762920792182, "mdate": 1762920792182, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DMZ, kind of diffusion autoencoders.\nDMZ aims to improve the generative quality of diffusion models by guiding the sampling process using the latent representation $z$ of $x_0$.\nThe model is trained without any additional loss terms, following the standard DA training objective.\nUnlike conventional diffusion autoencoders, DMZ does not require an auxiliary latent sampler.\nInstead, it directly samples the latent variable z from a Bernoulli distribution, which improves sampling efficiency.\nFurthermore, the authors empirically show that conditioning only the Key and Value components of the cross-attention layers on z leads to better performance than other conditioning strategies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Unlike previous diffusion autoencoders, DMZ does not rely on an auxiliary latent sampler.\nBy directly sampling $z$ from a Bernoulli distribution, the method enables computationally efficient sampling.\n\n- The learned latent representation is shown to be effective even in a multi-modal framework, indicating its potential generality beyond standard generation tasks.\n\n- The proposed DDPM-based approach demonstrates clear improvements in generation quality, particularly when using a small number of denoising steps."}, "weaknesses": {"value": "- It is unclear how the latent variable can be sampled from a Bernoulli distribution without any prior regularization.\nIn standard DA frameworks, auxiliary latent samplers (such as [1,2]) or additional regularization terms (such as [3]) are typically used to properly model the latent prior.\nWithout such mechanisms, it is not evident how the encoder output would naturally follow a Bernoulli prior.\nThis appears to be a critical limitation of the proposed method.\n\n- The effect of conditioning z only on the Key and Value in the attention layers is not clearly explained.\nWhile the authors report that this approach outperforms the alternative of jointly conditioning with t, the reason for this improvement remains unclear.\nAdditional analysis or experiments would strengthen this claim.\n\n---------\n[1] [CVPR22] Diffusion autoencoders: Toward a meaningful and decodable representation\n\n[2] [NeurIPS 22] Unsupervised representation learning from pre-trained diffusion probabilistic models\n\n[3] [ICML23] Infodiffusion: Representation learning using information maximizing diffusion models"}, "questions": {"value": "- Was any specific encoder architecture or constraint introduced to make the encoder output binary? How is this discreteness enforced during training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "tM3sDpx8zW", "forum": "oVd1y7ilTk", "replyto": "oVd1y7ilTk", "signatures": ["ICLR.cc/2026/Conference/Submission9087/Reviewer_s5wS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9087/Reviewer_s5wS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934786218, "cdate": 1761934786218, "tmdate": 1762920791847, "mdate": 1762920791847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DMZ, a design for diffusion autoencoders that aims to improve both generation efficiency and representation learning. By incorporating a input-dependent encoder, DMZ explores the distribution choices, conditioning mechanisms, and learning strategies to enhance the performance of diffusion autoencoders. Experiments on CIFAR-10 and CelebA demonstrate the effectiveness of DMZ and its potential ability for style transfer and representation learning."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The focus on diffusion autoencoders is timely and relevant, addressing the need for efficient generation and representation learning.\n- The illustrations and explanations of DM/DA are clear.\n- The benchmarking tasks and datasets are appropriate for evaluating the proposed method."}, "weaknesses": {"value": "- The motivation and contribution of DMZ is unclear.\n- The algorithmic details of DMZ are insufficient.\n- The performance of DMZ is underwhelming compared to existing methods."}, "questions": {"value": "0. **DMZ meaning.** What does DMZ stand for? The acronym is not explained in the paper.\n\n1. **Motivation and contribution.** I feel confused about the motivation and contribution of DMZ, and believe the writing could be potentially largely improved for clarity. In the introduction section, the authors claim \"to draw a connection between DMs and DAs\". However, I could not find any discussion or analysis on DA/DMZs in the rest of the paper. How are DMZ and DA different? Is it the contribution of DMZ to propose a new DA framework, or explore the design space of DAs? Could the authors clarify the main contribution of this work?\n\n2. **Algorithmic details.** The algorithmic details of DMZ are insufficiently described. Only Eq.(5) describes the training objective of DA. Does DMZ use the same training objective as DA? Additionally, could the authors provide more details about the newly-proposed components in DMZ, including conditioning mechanisms and learning strategies? A more comprehensive description of the algorithm would help readers better understand the proposed method.\n\n3. **Performance comparison.** The performance of DMZ seems underwhelming compared to existing methods. In Table 1, DMZ achieves worse performance on CIFAR-10 compared to DDPMs. In Table 3, DMZ achieves worse performance compared to DDBMs. Could the authors provide more analysis on why DMZ underperforms compared to these methods? Are there any specific limitations or challenges in the DMZ design that lead to this performance gap?\n\n4. **Inconsistent experimental setup.** In Figure 3 the authors compare NLL on CIFAR-10 and FID on CelebA. Are there any specific reasons for using different datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MOFa2NMJfJ", "forum": "oVd1y7ilTk", "replyto": "oVd1y7ilTk", "signatures": ["ICLR.cc/2026/Conference/Submission9087/Reviewer_8Gtz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9087/Reviewer_8Gtz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977005299, "cdate": 1761977005299, "tmdate": 1762920791496, "mdate": 1762920791496, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a diffusion autoencoder framework, DMZ, with carefully designed strategies such as latent variable choice, conditioning methods, and more. The paper provides a comprehensive study of each component’s design choices. Empirically, DMZ shows consistently strong performance in both unconditional generation and representation learning. The authors also demonstrate that DMZ can be easily applied to multimodal tasks (e.g., image-to-image translation), highlighting the framework’s flexibility."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* The paper is clear and easy to follow. The comprehensive experiments convincingly isolate and evaluate the effects of each design choice.\n* The DMZ framework is fairly general: it performs well in unconditional generation and representation learning, and it can be extended to handle multimodal tasks such as image-to-image translation."}, "weaknesses": {"value": "* The cross-attention conditioning design is already widely used in modern diffusion transformers [1-2]. The current validation relies on an older U-Net architecture, so this component does not constitute a significant contribution by itself.\n* The choice of latent dimensionality $|z|$ appears ad hoc. For generation tasks it is guided by the label-space size (suggesting that relatively low dimensions yield better generation quality), whereas representation learning for downstream tasks benefits from more informative, higher-dimensional latents. This implies separate designs for different use cases within DMZ.\n* The effectiveness for generation is not fully convincing. If is tied to a (binary) label space, it can logically degenerate to a one-dimensional label with low dimensions. Sampling from this prior is then akin to sampling in label space for conditional generation. Although DMZ does not directly rely on a labeling function $f:X\\rightarrow Y$, could clustering be used to produce labels that achieve similar behavior in the \"unconditional\" setting? This would suggest DMZ may not be learning strong representations in these scenarios.\n* DMZ shows limited compatibility with DDIM in Table 10. It would help to evaluate DMZ with more recent denoising approaches and architectures such as DiT and SiT [1–2].\n* Extending experiments to larger benchmarks (e.g., ImageNet) would further strengthen the work’s claims and external validity.\n\n[1] Scalable Diffusion Models with Transformers\n[2] Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers"}, "questions": {"value": "See Weaknesses 3–5."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zPasy4Iy54", "forum": "oVd1y7ilTk", "replyto": "oVd1y7ilTk", "signatures": ["ICLR.cc/2026/Conference/Submission9087/Reviewer_twVn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9087/Reviewer_twVn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9087/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761990483770, "cdate": 1761990483770, "tmdate": 1762920791223, "mdate": 1762920791223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}