{"id": "wWmhcBXGWc", "number": 12747, "cdate": 1758210017039, "mdate": 1759897489724, "content": {"title": "Not All Tokens are Guided Equal: Elucidating Guidance in Autoregressive Modelling", "abstract": "Autoregressive (AR) models based on next-scale prediction are rapidly emerging as a powerful tool for image generation, but they face a critical weakness: information inconsistencies between patches across timesteps introduced by progressive resolution scaling. These inconsistencies scatter guidance signals, causing them to drift away from conditioning information and leaving behind ambiguous, unfaithful features. We tackle this challenge with Information-Grounding Guidance (IGG) — a novel mechanism that anchors guidance to semantically important regions through attention. By adaptively reinforcing informative patches during sampling, IGG ensures that guidance and content remain tightly aligned. Across both class-conditioned and text-to-image generation tasks, IGG delivers sharper, more coherent, and semantically grounded images, setting a new benchmark for AR-based methods.", "tldr": "This paper sheds light on a key issue with guidance in AR modelling and proposes a new guidance method to bring the performance of AR models closer to diffusion models.", "keywords": ["diffusion models", "autoregressive models", "guidance"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7f01828f0db06feedca1cff0c4689abf45545d4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "Classifier-free guidance (CFG) enhances conditional diffusion models by interpolating between unconditioned and conditioned predictions. However, the authors argue that a naive application of CFG fails to yield satisfactory results in scale-wise autoregressive (SwAR) models, where prediction occurs across scales. They argue that unlike diffusion models, guidance in SwAR does not account for the semantic relationships between tokens, a limitation the authors quantify using two newly proposed metrics. To address this, they introduce Information-Grounding Guidance (IGG), a method that anchors the guidance signal to semantically important regions via attention. Their experiments demonstrate the effectiveness of IGG across both class-conditioning and text-conditioning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1 - I think the authors raise an important question: why should we assume that classifier-free guidance (CFG), originally developed for diffusion models, will work out of the box for autoregressive models, specifically, scale-wise autoregressive ones?\n\n2 - I appreciate that the authors pinpoint the issue through analyses of guidance token evenness and divergence, and further reinforce their intuition with extensive visualizations.\n\n3 - The method is remarkably simple and incurs almost no additional cost compared to CFG, essentially just an extra dot product between tokens."}, "weaknesses": {"value": "My comments are intended more as questions than as criticisms, so I will include them in the “Questions” section."}, "questions": {"value": "Conceptually, the authors’ arguments are sound, and the visual results clearly illustrate the effect of their intervention. What concerns me, however, is its impact on final performance.\n\n1 - While the authors claim that the performance gap becomes more pronounced at higher resolutions—as shown in Table 2 (ImageNet 512×512)—this trend does not appear in the text-to-image benchmark (Table 3). In particular, the gap for Switti is actually smaller than for VAR-CLIP, which seems inconsistent with their stated hypothesis.\n\n2 - In Section 6.3, the authors emphasize the importance of maintaining a balance between evenness and divergence, rather than optimizing each independently, to achieve the best outcomes. If this holds true, I’m curious how they explain the pronounced imbalance between these two factors in diffusion models compared to SwAR, as shown in Table 1."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1MRMZ2kpaH", "forum": "wWmhcBXGWc", "replyto": "wWmhcBXGWc", "signatures": ["ICLR.cc/2026/Conference/Submission12747/Reviewer_7N6s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12747/Reviewer_7N6s"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515708228, "cdate": 1761515708228, "tmdate": 1762923567314, "mdate": 1762923567314, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies why classifier-free guidance (CFG) behaves poorly in scale-wise autoregressive (SwAR) image generators and introduces Information-Grounding Guidance (IGG), which reweights guidance across tokens using self-attention over the guidance “nudges.” IGG is intended to concentrate guidance on semantically salient regions, aligning content and conditioning more tightly, and is shown to improve FID/IS on ImageNet class-conditional generation and several text-to-image benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear diagnosis: the paper quantifies that SwAR guidance is uneven and often misaligned compared to diffusion, using evenness (PEI) and divergence (JSD) metrics and qualitative visualizations.\n- Method is simple and principled: IGG modifies CFG by multiplying the guidance signal with an attention-derived token-importance map, with a precise formulation (Eq. 5–6).\n- Useful analysis/ablations: the paper connects guidance scale schedules to evenness/divergence and shows mixed CFG+IGG schemes can improve metrics but harm FID, arguing for balance rather than one-sided optimization."}, "weaknesses": {"value": "- Magnitude of gains on class-conditional ImageNet is modest in places (e.g., VAR-d30 FID 1.93→1.92, Pre/Rec almost no gain), raising questions about practical significance at lower scales or resolutions.\n- Compute concerns: IGG applies global self-attention over guidance at each scale; authors suggest a sliding-window variant mainly for future efficiency at high resolutions, implying the baseline IGG may be costlier as resolution grows. Evidence of actual runtime/memory overhead is not reported.\n- Sensitivity and tuning: while the paper discusses schedules and scale, more systematic reporting on sensitivity to the guidance weight \\omega and the stability of results across seeds/backbones would strengthen claims."}, "questions": {"value": "- It seems that when comparing CFG and IGG, the parameter \\omega is set differently. Please explain the reason.\n- The authors use the notation (p) to denote both the model and its parameters, which may hinder readability; this is not recommended.\n- Have you tested IGG with non-VAR SwAR/tokenizers (e.g., different codebooks or continuous tokens) to assess architectural dependence?\n- For text-to-image, can you report human-eval or instruction-following error rates alongside CLIP/PickScore/IR to validate that IGG’s regional emphasis improves semantic faithfulness, not just aesthetics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0iyf6mtLDS", "forum": "wWmhcBXGWc", "replyto": "wWmhcBXGWc", "signatures": ["ICLR.cc/2026/Conference/Submission12747/Reviewer_sSUr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12747/Reviewer_sSUr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761832695405, "cdate": 1761832695405, "tmdate": 1762923566837, "mdate": 1762923566837, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes  Information-Grounding Guidance (IGG), a novel guidance framework for AR image generation models. Unlike classifier-free guidance (CFG), which uniformly applies guidance across all tokens, IGG introduces a token-specific guidance mechanism that dynamically adjusts the strength based on the semantic importance of each token. IGG is evaluated on class-conditional and text-to-image tasks, showing improvements in fidelity, coherence, and alignment over CFG."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Conceptual Innovation: The core insight—that not all tokens should be guided equally—is both intuitive and impactful. The proposal of token-level, interpretable guidance addresses a fundamental limitation of uniform CFG in AR models.\n2. Rich experiments: Experiments have been conducted on multiple datasets for verification, and the results are convincing."}, "weaknesses": {"value": "1. The improvement in quantitative indicators brought by IGG is insufficient, and it seems that in the visualization results provided in the paper, it is also difficult to see its obvious advantages over CFG\n2. Computational Overhead and Efficiency Concerns: IGG introduces an auxiliary network and additional inference steps to compute token weights. The paper does not report latency or FLOPs comparison with CFG. A small overhead per token could significantly impact practical usability. This is a critical omission for a method claiming to improve sampling.\n3. As a training-free guidance optimization method, merely verifying its effectiveness on the scale-wise autoregressive method cannot prove its universality. The authors should conduct experimental verification on more types of AR models.\n4. I think the explanation of the core method IGG in Section 5 is a bit crude and not conducive to understanding its working principle."}, "questions": {"value": "1. What is the computational cost of IGG per sampling step?\n2. Can IGG be applied to naive autoregressive models, such as LlamaGen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FLkrEF9MTd", "forum": "wWmhcBXGWc", "replyto": "wWmhcBXGWc", "signatures": ["ICLR.cc/2026/Conference/Submission12747/Reviewer_ksrD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12747/Reviewer_ksrD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761914045171, "cdate": 1761914045171, "tmdate": 1762923566437, "mdate": 1762923566437, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The idea central to this paper is that not all tokens require the same amount of guidance. The authors propose a novel, adaptive approach to improve the efficiency and quality of content generation by dynamically allocating the guidance strength based on the specific needs of each token during the generation process"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors argue that Adaptive guidance is helpful which makes sense in some way\n\n2. The results seems  to outperform cfg based sampling\n\n3. Reinforcing informative patches in generation makes some sense in a way"}, "weaknesses": {"value": "1. IGG seems to introduce more complex form of hyperparameter\n\n2. A table of comparison for comparative computation could be helpful\n\n3.  There are papers with much better results for example https://arxiv.org/abs/2410.19324, so I do not think the claim the present method is SOTA is well supported\n\n4. The comparison is very vague and does not say why the proposed method outperforms and what exactly the previous methods lack. Same goes for relatred work\n\n5. With this new method what are the new potential biases that can affect, a discission on that will be meaningful\n\n7. The method should evaluate on standard benchmarks like Compbench and Geneval to show the usefulness of this method\n\n8. Side by side sensitivity analysis and statistical analyis of variance in results must be conducted for both cfg and igg"}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lVBYny9ioS", "forum": "wWmhcBXGWc", "replyto": "wWmhcBXGWc", "signatures": ["ICLR.cc/2026/Conference/Submission12747/Reviewer_6KR7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12747/Reviewer_6KR7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12747/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995504004, "cdate": 1761995504004, "tmdate": 1762923566012, "mdate": 1762923566012, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}