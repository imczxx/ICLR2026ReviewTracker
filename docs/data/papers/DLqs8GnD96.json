{"id": "DLqs8GnD96", "number": 6277, "cdate": 1757963846369, "mdate": 1759897925285, "content": {"title": "VoxelPrompt: A Vision Agent for End-to-End Medical Image Analysis", "abstract": "We present VoxelPrompt, an end-to-end image analysis agent that tackles free-form radiological tasks. Given any number of volumetric medical images and a natural language prompt, VoxelPrompt integrates a language model that generates executable code to invoke a jointly-trained, adaptable vision network. This code further carries out analytical steps to address practical quantitative aims, such as measuring the growth of a tumor across visits. The pipelines generated by VoxelPrompt automate analyses that currently require practitioners to painstakingly combine multiple specialized vision and statistical tools. We evaluate VoxelPrompt using diverse neuroimaging tasks and show that it can delineate hundreds of anatomical and pathological features, measure complex morphological properties, and perform open-language analysis of lesion characteristics. VoxelPrompt performs these objectives with an accuracy similar to that of specialist single-task models for image analysis, while facilitating a broad range of compositional biomedical workflows.", "tldr": "", "keywords": ["Vision-Language Agent", "Medical Image Analysis", "Neuroimaging"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/936827d179fdae1b4c4a9d961870139fd48ceaf1.pdf", "supplementary_material": "/attachment/a72148dbcca511412b5b6862b3103bed9a94b9c5.zip"}, "replies": [{"content": {"summary": {"value": "This work focuses on improving generalization of language-vision medical image analysis to diverse practical clinical use cases across real-world lesions types. Specifically, VoxelPrompt is an agent-based approach that coordinates outputs from vision and language models to produce executable code that performs tasks such as ROI measurements, image manipulation, language characterization, and analyses across multiple ROIs, multiple acquisitions, and multiple visits. The authors also propose a CNN model that integrates information from language prompts and processes images in native resolution. Furthermore, they propose a procedure for constructing a dataset that that improves robustness to lesion types across different datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The use of agents to produce code seems like a promising approach to generating interpretable operations on medical images. \n- The evaluations seem to consider a broad range of relevant use cases\n- The lesion synthesis procedure seems to be a novel approach. \n- This work also highlights an important need for free-form workflow benchmarks that capture common practitioner use-cases"}, "weaknesses": {"value": "- **Unclear notation**: \n    - Section 3.1: The notation that is introduced is not precise. What exactly is $V, p, a, \\Omega, W$, etc.. Are these vectors, scalars, matrices, functions, etc.? \n        - Line 186: $\\phi$ is defined as “image-specific latent instruction embedding”. However, there is no further detail about what this object is or where it comes from. Similarly, where does $\\phi_s$ (line ~209) come from?\n        - Line 210: The shape notation $\\mathbb{R} ^{S,c}$ is not standard. I think you mean $\\mathbb{R} ^{S \\times c}$, but it is not clear in the text.\n- **Unclear description of model architecture**: \n    - Line 207: I’m not sure I understand the role of “streams”. How are these different than the standard intermediate activations from a transformer layer, where inputs are processed separately before fusion with an attention mechanism? \n    - Line 215-222: The section on native space processing is unclear. What is the base architecture and what is the upsampling and downsampling arm referring to? It would also be helpful to elaborate on what “common geometry” (line 222) mean in this context. What exactly is being updated to adapt the model for different resolutions? What mechanism allows enables the sharing of model weights across these different resolutions? \n- **Unclear description of Dataset generation procedure**: \n    - Overall, section 3.2 is quite vague. In general, it is not clear what the exact tasks are, what their inputs and outputs are, how they are generated, or what are the parameters that can be varied during generation. Concretely, here are a few examples:\n        - Line 244-250: In section “training code for quantitative ROI processing”, it is unclear if these tasks are considered separately or if they are combined together. How are multiple tasks sampled during training?\n        - Line 252: what exactly are the relevant metrics? Please be specific. What is the objective function that you are optimizing?\n        - Line 260: In section “training code for question answering”, Its not clear how correct natural language text response is generated. \n        - Line 263: how did you construct these templates? Can you provide some examples of what these templates look like?\n        - Line 266: How do you make sure that the generated prompt doesn’t produce invalid combinations?\n- **Unclear description/motivation for the evaluation**:\n    - Line 311: It is unclear what zero-shot lesion segmentation is intended to evaluate. Could you clarify what type of generalization the held-out pathology datasets are designed to test? Are we testing for generalization to unseen diseases or same diseases but unseen populations or unseen tasks, or something else?\n    - Section 4.2. What is specialist network referring to? Can you provide a citation here? \n    - Could the authors clarify how the evaluations in Section 4 validate the use cases presented in Figure 1? At present, it is not clear whether VoxelPrompt successfully accomplishes its intended use cases.\n- **Insufficient baselines**. For certain evaluations, the authors evaluate compare against only a single baseline which is insufficient in evaluating how the proposed approach compares to the existing literature.\n    - Fig 3D: Why not compare performance against other VLMs? My intuition says that other VLMs will achieve a similar runtime improvement when compared against FreeSurfer.  \n    - Fig 3E: Same comment here, why did you only choose to benchmark against SynthSeg? Why not compare against other segmentation or VLM models?"}, "questions": {"value": "Please see weakness section for the majority of questions. Here are a few additional questions:\n1. In line 198, How do you guarantee that the feedback procedure does not result in an infinite loop?\n2. How well does this approach generalize to new tasks, especially more complex multi-step tasks? It seems like the training procedure is quite specific to simple tasks that are provided during training. Is the dataset generation approach scalable? \n3. What is the effect of synthetic lesion training? Can you do an ablation to show that the proposed approach is effective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rHdDNHPEsI", "forum": "DLqs8GnD96", "replyto": "DLqs8GnD96", "signatures": ["ICLR.cc/2026/Conference/Submission6277/Reviewer_DgCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6277/Reviewer_DgCn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760895441645, "cdate": 1760895441645, "tmdate": 1762918587654, "mdate": 1762918587654, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents VoxelPrompt, a system that combines a language model agent with a jointly-trained vision network to perform complex neuroimaging analysis tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The joint training of language and vision components for code-based workflow generation is interesting. The code generation approach provides transparency and interpretability compared to black-box vision-language models, which is important for clinical applications."}, "weaknesses": {"value": "1. \"End-to-end\" is claimed throughout, but most quantitative evaluations are on segmentation subtasks, not complete clinical workflows.\n2. Training the language model from scratch on synthetic template-based prompts is a critical limitation explicitly acknowledged: \"limits their utility when given entirely unseen prompts\". The system can only use predefined library functions, limiting true flexibility.\n3. Missing quantitative evaluation of the complex multi-step workflows shown in Figure 1. And small sample sizes for some tasks (e.g., n=12 subjects for vascular territory classification).\n4. There are some Baseline Comparison Concerns. Different prompts for different baselines (shown in Table on p.19) is unclear if BiomedParse v2 and SAT received optimal prompting.\n5. Fine-tuning a pretrained language model (even small ones like CodeLlama, StarCoder) would likely improve prompt generalization significantly. The choice to train from scratch seems inefficient and limits performance.\n6. Some notation inconsistencies (e.g., E used for both encoder output and feature encodings).\n7. The longitudinal FreeSurfer comparison is somewhat unfair. FreeSurfer performs full cortical reconstruction, not just segmentation."}, "questions": {"value": "1. What percentage of queries result in code execution errors?\n2. How does the system handle code execution failures, invalid operations, or edge cases?\n3. What happens when users provide prompts significantly different from training templates?\n4. Can the approach extend to other body regions (chest, abdomen) or modalities without retraining from scratch?\n5. For RadFM, what performance would be achieved with the full 32-layer model? Could other memory optimization strategies enable fair comparison?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "L5AaTv8f3N", "forum": "DLqs8GnD96", "replyto": "DLqs8GnD96", "signatures": ["ICLR.cc/2026/Conference/Submission6277/Reviewer_dWyV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6277/Reviewer_dWyV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761715927239, "cdate": 1761715927239, "tmdate": 1762918587154, "mdate": 1762918587154, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a code agent framework to assist radiologist of free-form radiological tasks. Experimental reuslts show the current system seems work well in some author defined tasks."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written in structure and readers are easy to follow."}, "weaknesses": {"value": "1. An Inefficient and \"Weak\" Agent: The paper's most significant limitation is that its agent is trained from scratch on a curated, domain-specific dataset. This results in an agent that is, by definition, \"weaker\" and less capable in its reasoning, language understanding, and code-generation abilities than any modern, general-purpose foundation model (such as the Gemini or GPT series). The field has largely demonstrated that the emergent reasoning and planning capabilities of large-scale models are a prerequisite for robust agentic behavior.\n\n2. Ignores the Superior LGM-as-Agent Paradigm: As you noted, the tasks described (e.g., \"calculate progressive signal reduction\") are complex, multi-step analytical workflows. The current, established approach for this is to use a powerful, pre-trained foundation model as a central \"agent\" or \"orchestrator.\" This agent then intelligently calls upon a suite of specialized tools—which could include the VoxelPrompt vision network itself—via APIs. The paper's design, which builds a weak, custom agent instead of leveraging a powerful general one, seems to solve the wrong problem.\n\n3. Lack of Verifiable Generalizability: The \"from-scratch\" approach makes the agent \"brittle.\" Because the vision and language models are co-trained on this specific neurological dataset, the agent's \"intelligence\" is inextricably tied to this single domain. There is no evidence it could generalize to any other task (e.g., analyzing chest X-rays, a slightly different MRI protocol). This lack of zero-shot capability makes it impossible to verify the agent's generalizability, as its performance is completely coupled with its training data."}, "questions": {"value": "See Weaknesses for details."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H4NU5RJfGc", "forum": "DLqs8GnD96", "replyto": "DLqs8GnD96", "signatures": ["ICLR.cc/2026/Conference/Submission6277/Reviewer_dG9U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6277/Reviewer_dG9U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949556031, "cdate": 1761949556031, "tmdate": 1762918586723, "mdate": 1762918586723, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces VoxelPrompt, an end-to-end system that combines language-driven code generation with a unified vision model. The method combines a language model and a cnn by allowing the language model to augment the cnn with code, and thus perform unconstrained analysis of the input."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "The proposed method stands out for its originality, offering a fundamentally new approach in both functionality and design compared to existing medical image frameworks. Its flexibility and performance are highly compelling.\n\n* The innovative use of code as an output of a language model, enabling segmentation to serve its true role as an intermediate step toward downstream clinical or analytical objectives.\n* A thorough and convincing evaluation that clearly demonstrates the method’s effectiveness.\n* The joint training of a language model and a segmentation model from scratch, a novel and effective strategy.\n* The ability to process scans at native resolution is useful and non-trivial with cnns. \n* The used datasets are interesting, however underspecified. In particular the q&a pairs, would, if released, represent a contribution to the community."}, "weaknesses": {"value": "The primary limitation of the paper lies in the presentation of the proposed method. Its exact capabilities and constraints are not clearly defined. Crucial details regarding the architecture, datasets, and evaluation are relegated to the appendix, forcing the reader to consult supplementary material to grasp the approach.\n\n* The use of the term “zero-shot” is misleading, as the model has been exposed to the same ROIs during training; the evaluation therefore reflects domain transfer rather than true zero-shot performance.\n\n* The evaluation section omits essential information about the baseline specialist models, preventing a sound assessment of comparative performance. Similarly, the available tools and interfaces accessible to the language model are not specified.\n\n* The term “agent” is used without sufficient justification.\n\n* Key procedural details are missing regarding the generation of Q&A pairs: it is unclear whether templates are used, what their variation is, and whether the language model generalizes beyond them.\n\n* An ablation study examining the language model’s ability to correctly identify and interpret the intended task is also absent, limiting interpretability of the reported results.\n\n* Lastly, the paper lacks important information on the tools available for the agent to use -- i.e. which functions can be called when and in what order."}, "questions": {"value": "* Can the proposed network be zero-shot adapted to arbitrary tasks, or is adaptation limited to tasks represented in the training dataset?\n* How many distinct regions of interest (ROIs) are included in the dataset?\n* In this context, does “zero-shot” refer to ROIs unseen during training?\n* What are the specialist models referenced in Figure 4A, and do they represent current state-of-the-art baselines?\n* Do all evaluated tasks involve segmentation, or are other modalities or objectives included?\n* Will the datasets used in the study, particularly the Q&A pairs, be publicly released?\n* What is the solution space available to the language model? Which functions can be called when and in what order?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "l5l9JUxpjm", "forum": "DLqs8GnD96", "replyto": "DLqs8GnD96", "signatures": ["ICLR.cc/2026/Conference/Submission6277/Reviewer_RYFR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6277/Reviewer_RYFR"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762227843073, "cdate": 1762227843073, "tmdate": 1762918586334, "mdate": 1762918586334, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}