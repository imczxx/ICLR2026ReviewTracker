{"id": "f9BuANYtJf", "number": 24820, "cdate": 1758360674770, "mdate": 1759896746874, "content": {"title": "GRAF: Multi-turn Jailbreaking via Global Refinement and Active Fabrication", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks. Nevertheless, they still pose notable safety risks due to potential misuse for malicious purposes. Jailbreaking, which seeks to induce models to generate harmful content through single-turn or multi-turn attacks, plays a crucial role in uncovering underlying security vulnerabilities. However, prior methods, including sophisticated multi-turn approaches, often struggle to adapt to the evolving dynamics of dialogue as interactions progress. To address this challenge, we propose \\textbf{GRAF}(JailBreaking via \\textbf{G}lobally \\textbf{R}efining and \\textbf{A}daptively \\textbf{F}abricating), a novel multi-turn jailbreaking method that globally refines the attack trajectory at each interaction. In addition, we actively fabricate model responses to suppress safety-related warnings, thereby increasing the likelihood of eliciting harmful outputs in subsequent queries. Extensive experiments across six state-of-the-art LLMs demonstrate the superior effectiveness of our approach compared to existing single-turn and multi-turn jailbreaking methods.", "tldr": "", "keywords": ["LLM safety", "LLM jailbreak", "Multi-turn jailbreaking"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d6a780bf98a5a3ea2e90453fa234feef8a94236d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a new multi-turn jailbreaking method to stress-test LLM safety. To conceal malicious intent and expose vulnerabilities over multiple turns, the authors propose two main techniques: 1) iterative refinement, where all planned subsequent queries are updated based on the attack result of the current turn, and 2) response fabrication, where the target model's actual responses are altered. This fabrication involves removing rejection phrases and safety warnings, followed by rephrasing to maintain coherence and prevent future rejections.\n\nExperimental results on HarmBench, using both LLM-as-judge and string-matching metrics, demonstrate that this method outperforms existing single-turn and multi-turn baselines. The authors also provide ablation studies to verify each component's effectiveness and show that the attack can evade common moderation tools."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper's focus on multi-turn jailbreaking is a key strength. This is a difficult and increasingly important problem, as latent safety risks can build up over longer interactions, often just as a user's trust in the model is also increasing. The method's strong empirical performance against baselines is noteworthy."}, "weaknesses": {"value": "1. *Flawed Motivation and Unclear Real-World Grounding*: The fundamental goal of jailbreaking is to expose vulnerabilities in a specific target model. Given this, the decision to fabricate the model's own responses is methodologically unsound. These fabricated responses are not necessarily high-probability outputs from the actual target model. The authors are therefore attacking an **\"illusional\" model**, not the real one. In many real-world scenarios (e.g., online chatbot, agentic IDE), such fabrication is not supported or at least not straightforward to implement. Refusing to respond with follow-up queries when potential malicious intent is recognized is also a reasonable behavior for a trustworthy LLM, not necessarily a problem to avoid. While it is unsurprising that an attack succeeds with a fabricated history, the central point of jailbreaking is to find **realistic** vulnerabilities. This methodological choice casts significant doubt on the real-world applicability of the attacks and the utility of any insights gained.\n\n2. *Significant Efficiency Concerns*: The proposed method appears computationally inefficient. It initiates an attack trajectory and then iteratively modifies all subsequent queries based on the attack result of the current turn. This suggests an $O(n^2)$ generation cost relative to the number of turns ($n$). A simpler $O(n)$ baseline (also explored in previous works, as the author noted) would be to just generate the next attack query based on the dialogue history. The paper does not sufficiently justify this quadratic cost. Furthermore, as the dialogue unfolds and the topic shifts might unavoidably happen, refinements made in early turns (e.g., Turn 1) to planned queries late in the trajectory (e.g., Turn 10) are likely wasted computation, as the immediate-past dialogue state is a far more relevant context. No information would be lost, as the input information (e.g., 1st round attack results) for the early update (e.g., Turn 1 -> Turn 10) is still accessible in the dialogue provided for later updates (e.g., Turn 9 -> Turn 10). \n\n3. *Unclear Writing and Opaque Methodology*: The paper is difficult to understand, with key methodological details remaining vague. For example, L213-L222, which should explain the core refinement method, instead offers hand-wavy claims about its benefits. The paper fails to answer critical questions: What is the exact implementation of the refinement mechanism? What alternative design choices have been considered? As the author claims their method \"reduces the likelihood of generating off-topic or ineffective queries\", then how are topic deviation or query effectiveness detected? How is the \"probability that the trajectory reaches the target\" measured and empirically validated (this is distinct from the Attack Success Rate, as multiple rollouts are required)? Additionally, the \"Few-shot Attack\" mentioned in Section 4.3 does not appear to be in the corresponding tables, adding to the confusion. The paper requires significant rewriting for clarity."}, "questions": {"value": "1. Characterization of Baselines (L151): Regarding the citation of Russinovich et al. (2024), the paper seems to imply their method only uses the previous turn. However, in a recursive generation, long-term dependencies are still built implicitly turn-by-turn. Is the paper's characterization of this baseline as a \"short-term\" method accurate?\n\n2. Missing Experiment Details: Where are the results for the \"Few-shot Attack\" mentioned in Section 4.3? Is this a typo for one of the other baselines?\n\n3. Clarification on L407: The claim that the attack \"moves the representations of harmful queries closer to those of harmless queries\" is confusing. Does this simply mean the attack is stealthy? It's unclear why this demonstrates attack effectiveness rather than just obfuscation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JOHNrASSc7", "forum": "f9BuANYtJf", "replyto": "f9BuANYtJf", "signatures": ["ICLR.cc/2026/Conference/Submission24820/Reviewer_cpdw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24820/Reviewer_cpdw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761508222709, "cdate": 1761508222709, "tmdate": 1762943208228, "mdate": 1762943208228, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a multi-round escape attack method for GRAF. Existing multi-round escape techniques rely on fixed templates or partial updates, causing attack trajectories to deviate during conversations. GRAF enhances success rates by initializing complete attack trajectories, globally refining remaining queries in each interaction round, and proactively fabricating model responses. Experiments on six advanced LLMs using benchmarks like HarmBench demonstrate GRAF outperforms existing single-round and multi-round baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. GRAF enhances multi-round escape effectiveness through global refinement and active forgery, revealing harmful query representations shifting toward harmless ones. The theory formalizes this as an iterative process, analyzes representation shifts, and provides supporting details in the appendix;\n\n2. The experimental setup is generally reliable, baseline comparisons are comprehensive, and ablation studies quantify component contributions. However, evaluations rely on automated metrics like GPT-Judge, RS-Match, and Llama-Guard, which carry potential biases and inconsistencies, lacking human validation to assess their reliability.\n\n3. Testing is confined to non-reasoning models, failing to demonstrate effectiveness on models with enhanced reasoning capabilities.\n\n4. Robustness proofs are limited to two basic defense mechanisms, neglecting more advanced defenses.\n\n5. The paper excels in originality, quality, clarity, and significance. The GRAF framework introduces novel mechanisms: GLOBAL REFINEMENT and ACTIVE FABRICATION;\n\n6. Rigorous experimental design spans multiple benchmarks and models, with ablation studies and visualization characterizations providing empirical validation for theoretical algorithms;\n\n7. As a red-team tool, GRAF exposes deep vulnerabilities in LLMs during multi-turn interactions, informing future defense designs."}, "weaknesses": {"value": "1. While the ACTIVE FABRICATION mechanism is innovative, it fails to thoroughly explore potential risks of abuse.\n\n2. Evaluation relies on automated metrics, which may introduce bias. Human expert verification is recommended to ensure consistency.\n\n3. Defense testing is limited to two basic mechanisms, neglecting the latest advanced defense methods.\n\n4. It is recommended to extend testing to inference models while providing confidence intervals to enhance statistical reliability."}, "questions": {"value": "1. How does global optimization scale in long dialogues (10-20 turns)? Is the computational cost prohibitively high?\n\n2. Could ACTIVE FABRICATION introduce bias, making attack trajectories overly reliant on specific model response patterns?\n\n3. How effective is the method on reasoning models?\n\n4. While GRAF emphasizes a global perspective, can global refinement effectively recover if early turns fail?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wU2WYPPOsb", "forum": "f9BuANYtJf", "replyto": "f9BuANYtJf", "signatures": ["ICLR.cc/2026/Conference/Submission24820/Reviewer_iCfJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24820/Reviewer_iCfJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761615563222, "cdate": 1761615563222, "tmdate": 1762943207973, "mdate": 1762943207973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a new multi-turn jailbreak attack method by globally refining and adaptively fabricating the prompts. Also the safety-related warnings are suppressed to induce more jailbreak queries. Experiments in 6 LLMs show the effectiveness of the approach."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written and easy to follow. The motivation for a multi-turn jailbreak attack is sound and clear. The idea of globally optimizing queries along the trajectory is interesting."}, "weaknesses": {"value": "- My biggest concern lies in the generalizability of this method with the initialization of attack queries. Even through Sec 5.2 shows results of different initialization methods, the method itself is still based on the existing attack method. This requirement seems to be too strong due to the dependence of other attack methods.\n\n- In the global refinement, modifying the future sequence of queries does not make sense to me, since the attacker can only have previous history queries. If the method requires an initialization jailbreak trajectory from other jailbreak methods, again, it seems to be incremental and not practical as an independent attack method.\n \n- I would suggest conducting an experiment with initialization of only the first query instead of the whole trajectory, which makes more sense in practice. Also, in Table 3, the initial trajectory (Multi-turn) seems not to be effective in the ablation study, but it is actually the original implementation of [1], which is not consistent with the original results of [1]. It is suggested to conduct full samples in HarmBench instead of 50 randomly sampled in Table 3. \n\n- Showing that current defense methods cannot defend the new attack is important, but current results in 5.3 only use limited single-turn defense methods, which is unfair for the proposed multi-turn attack method. It is expected to compare the proposed method with the multi-turn defense methods [2,3].\n\n---\n\n[1] Ren et al. Derail Yourself: Multi-turn LLM Jailbreak Attack through self-discovered clues, 2024\n\n[2] Lu et al. X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from Multi-Turn Jailbreaks without Compromising Usability, 2025\n\n[3] Hu et al. Steering Dialogue Dynamics for Robustness against Multi-turn Jailbreaking Attacks, 2025"}, "questions": {"value": "See Weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ks78H6OAdq", "forum": "f9BuANYtJf", "replyto": "f9BuANYtJf", "signatures": ["ICLR.cc/2026/Conference/Submission24820/Reviewer_1zD9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24820/Reviewer_1zD9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972906197, "cdate": 1761972906197, "tmdate": 1762943207655, "mdate": 1762943207655, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GRAF, a multi-turn jailbreaking method composed of (i) global refinement, which iteratively revises subsequent queries to conceal malicious intent while keeping them on-topic, and (ii) active fabrication, which suppresses safety-related warnings to reduce subsequent rejections."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Motivation is clear. \n\n2. The method is straightforward. GRAF adheres to the standard multi-turn pipeline (initial trajectory + attacker-driven refinement) and adds two light-weight mechanisms (global refinement; active fabrication) without auxiliary models or heavy hyperparameter tuning. This simplicity is appealing for reproducibility.\n\n3. The main results are significant. Under GPT-Judge, GRAF outperforms both single-turn and multi-turn baselines across six targets (Table 1). The paper also discusses why RS-Match can be misleading and illustrates failure cases (Figure G.5).\n\n4. Experiments are extensive. Analyses probe robustness to (i) trajectory initialization (Section 5.2) and (ii) defense methods (Section 5.3). GRAF remains competitive when defenses are applied and delivers consistent gains across initialization schemes, supporting practical applicability."}, "weaknesses": {"value": "1. Side effects of discarding (qi, ai) pairs are under-analyzed. Section 3.3 asserts that dropping pairs helps downstream acceptance, but the paper does not quantify its frequency or impact on on-topic coherence and end-task success. Removed turns may encode semantic glue that carries the malicious intent; deleting them could derail or inadvertently sanitize the trajectory.\n\n2. Section 5.1 seems off-topic. The representation study (Figure 3) argues that more history turns can shift harmful queries toward harmless regions, but the visualization is noisy (heavy overlap among markers) and the link to GRAF-specific mechanisms is indirect. As written, this reads as a generic multi-turn observation reported by prior works (Jiang et al., 2024b; Ren et al., 2024; Cheng et al., 2024), not a diagnosis of why global refinement or active fabrication help here.\n\n3. Defense robustness is poorly discussed. Section 5.3 reports aggregate ASR reductions under two defenses and concludes robustness, but it neither analyzes why baselines (e.g., ReNeLLM, Derail) fail nor attributes GRAF’s gains to specific components. Moreover, Table 4 shows >20% absolute ASR drops for GRAF under defenses, indicating a non-trivial vulnerability that deserves discussion.\n\n4. Generalization beyond HarmBench. I suggest replicating the Section 4.2 analyses on at least one additional benchmark—e.g., AdvBench (Russinovich et al., 2024; Sun et al., 2024)—to support the generality of GRAF’s gains across datasets.\n\n5. Include case studies on discarding query-answer pairs. Provide a few end-to-end trajectories showing where discards occur, and quantify their frequency in the main experiment. Specifically, address that (i) removal increases downstream acceptance rates, and (ii) the trajectory remains on-topic after discards."}, "questions": {"value": "1. Quantify the representation-shift claim. For Figure 3, report the fraction of points on each side of the logistic-regression boundary per condition (0, 1, 2-4 turns). Consider repeating the analysis for Crescendo/CoA/Derail to assess whether GRAF shifts harmful queries toward harmless regions more effectively than the baselines.\n\n2. Clarify “cost” in Appendix D and Figure 6. Does “cost” count tokens for the target model only, or the attacker+target combined? Please specify precisely how it is computed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BqynKgiB1k", "forum": "f9BuANYtJf", "replyto": "f9BuANYtJf", "signatures": ["ICLR.cc/2026/Conference/Submission24820/Reviewer_wsS7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24820/Reviewer_wsS7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992931579, "cdate": 1761992931579, "tmdate": 1762943207464, "mdate": 1762943207464, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a black-box multi-turn jailbreaking attack called GRAF. The attack extends existing attacks such as PAIR to iteratively refine a multi-turn conversation. Relatively strong results are reported on HarmBench."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors report relatively strong, although not SOTA, attack success rates on HarmBench."}, "weaknesses": {"value": "- There are stronger baselines for this problem. For instance, the strongest baselines I'm aware of is X-teaming (https://arxiv.org/pdf/2504.13203), which reports stronger results than this paper.\n- I'm also curious as to how the authors tuned/adjusted their baselines? I'd expect an algorithm like PAIR to do much better for this problem, particularly as this method seems to be \"PAIR but for multi-turn jailbreaking.\"\n- For a similar reason, it's unclear what conceptual insight might be valuable to the community. While this does seem to work, the fact that it's relatively similar to other attacks and isn't SOTA seems to lessen the contribution of this work.\n- I don't understand Figure 3 -- it doesn't seem to show what the authors says it does (re: \"In addition, we observe that as more dialogue history is added, the representation of harmful queries shifts (slightly) closer to that of harmless ones\"). I don't see this trend at all. That is, I don't see any discernible signal that the history plays any role in where the dots lie between the harmless and harmful queries.\n- I'm a bit confused about the choice of baselines in Section 5.3. These baselines (both from 2023) are outdated. I think it would be worth trying LlamaGuard, and (even more recent) deliberative alignment, circuit breakers, etc."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Oqm6PkmMBm", "forum": "f9BuANYtJf", "replyto": "f9BuANYtJf", "signatures": ["ICLR.cc/2026/Conference/Submission24820/Reviewer_nWvJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24820/Reviewer_nWvJ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24820/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762379374115, "cdate": 1762379374115, "tmdate": 1762943207255, "mdate": 1762943207255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}