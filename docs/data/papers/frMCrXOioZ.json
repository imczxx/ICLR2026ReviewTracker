{"id": "frMCrXOioZ", "number": 10369, "cdate": 1758168542930, "mdate": 1759897655605, "content": {"title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs", "abstract": "We propose \\textbf{MindVL}, a multimodal large langauge model (MLLMs) trained on Ascend NPUs. The training of state-of-the-art MLLMs is often confined to a limited set of hardware platforms and relies heavily on massive, undisclosed data recipes, which hinders reproducibility and open research. To change the common perception that Ascend hardware is unsuitable for efficient full-stage MLLM training, we introduce \\textbf{MindSpeed-MLLM}, a highly efficient training framework that supports stable and high-performance training of large-scale Dense and Mixture-of-Experts (MoE) models on Ascend hardware. Based on this, we provide a systematic and open description of the data production methods and mixing strategies for all training stages. Furthermore, we present MindVL, a data-efficient multimodal large language model trained end-to-end on Ascend NPUs. In addition, we find that averaging weights from checkpoints trained with different sequence lengths is particularly effective and yields further gains when combined with test-time resolution search. Our experiments demonstrate superior data efficiency: \\textbf{MindVL-8B} matches the performance of Qwen2.5VL-7B using only 10\\% of its training data, while our MoE model, \\textbf{MindVL-671B-A37B}, matches Qwen2.5VL-72B using only 3\\% of the Qwen2.5VL training data, and achieves comparable performance with other leading multimodal MoE models. Our work provides the community with a valuable hardware alternative, open data recipes, and effective performance-enhancing techniques.", "tldr": "", "keywords": ["Efficient and Effective Training; Multimodal Large Language Models; Ascend NPUs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07d4d231f48d31bc1c8dfbe9fbea5fc3cda91c91.pdf", "supplementary_material": "/attachment/2c2e131cb2be28123280b0867db43b0ab6a79f46.zip"}, "replies": [{"content": {"summary": {"value": "This work presents MindVL, a competitive VLM which is trained with NPUs. The paper presents the data, infra, and training framework for developing this VLM. Additionally, some data statistics and framework features are included within the work.\n\nOverall, you do not need to submit this technical report to ICLR conference. If you would like to make such paper submission, you may need to claim and present your technical contributions. Outstanding VLMs like InternVL and QwenVL are just released on arxiv instead of included in conference proceedings. If you are confident about your work, then publish in tech report to earn your credits. No need to submit it to the conference to earn credits which are not helpful in building your reputation.\n\nTo make a valid paper submission, you should at least specify the following details in VLM development: 1) detailed data curation pipeline, which data quality classifier is used, the prompts for model based data curation, the original sources for collecting the data, the human annotator rubric and criteria, the clustering embedding models and sampling algorithm; 2) the online packing implementations and algorithms in Section 3.1.3. Please consider to follow SAIL-VL and OpenQwen2VL to reposition your contributions."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "Check summary."}, "weaknesses": {"value": "Check summary."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "iX6DGWrtZV", "forum": "frMCrXOioZ", "replyto": "frMCrXOioZ", "signatures": ["ICLR.cc/2026/Conference/Submission10369/Reviewer_4Gna"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10369/Reviewer_4Gna"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761695298466, "cdate": 1761695298466, "tmdate": 1762921692387, "mdate": 1762921692387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes MindVL, a multimodal LLM trained on Ascend NPUs. To deal with specific challenges on Ascend hardware, they propose an optimized training framework named MindSpeed-MLLM. They provide a new recipe of data construction pipeline for multimodal-LLMs. The resulted model is named MindVL and achieve on-par performance with Qwen2.5-VL."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "-\tIt seems to be a comprehensive work involving infrastructure, data curation, model tuning and implementation. The technical execution is solid, and the resulted model achieves good performance on multimodal understanding benchmarks."}, "weaknesses": {"value": "-\tThe whole paper looks like a technical report and does not have a core technical contribution. For the training library, the authors provide explanation on data loader, operator fusion, system-level scheduling optimization. However, these explanations are all on high-level, and it seems there is nothing new compared to existing training frameworks. The authors did not explain clearly what part of the Ascend hardware needs special handling for the large-scale training.\n-\tThe data curation part uses proprietary models (qwen, openAI, gemini, doubao) to annotate the data. It is used in multiple stages (filtering, annotation, rewrite), it is hard to know how much contribution of this external knowledge source has on the final performance.  Due to this, it is unfair to directly compare amount of training data with existing models (line 99-101).\n-\tSome technical details are not clear. 1. What does the author do specifically to support MoE models? 2. For mask compression (line 209-212), how is the memory overhead and computation efficiency improved?"}, "questions": {"value": "-\tThe authors mentioned the data will be open-sourced. However, given the scale of the data, how will it be open-sourced and made publicly accessible? (line 230)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HGF8zrZ2Tm", "forum": "frMCrXOioZ", "replyto": "frMCrXOioZ", "signatures": ["ICLR.cc/2026/Conference/Submission10369/Reviewer_pUiZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10369/Reviewer_pUiZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761930922939, "cdate": 1761930922939, "tmdate": 1762921691796, "mdate": 1762921691796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MindVL, a multimodal large language model trained on Ascend NPUs. Firstly, this work presents MindSpeed-MLLM, a highly efficient training library designed to support the training of MLLMs with both dense and MoE architectures on Ascend NPU hardware. Moreover, the paper introduces MindVL, a data-efficient MLLM, which is further enhanced through multimodal model weight averaging and test-time resolution search techniques. Experimental results demonstrate that MindVL achieves state-of-the-art performance while utilizing less training data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper provides a training framework that enables the training MLLMs on Ascend NPU hardware.\n- The trained MindVL model achieves competitive performance compared to state-of-the-art MLLMs, while using less data.\n- The training data recipe is open-sourced."}, "weaknesses": {"value": "- Although the data recipe is public, the data itself is annotated using closed-source API models including gemini, gpt, and doubao. This process is costly, making it difficult for other researchers to replicate.\n\n- For data construction, identifying low-quality data and performing re-annotation is crucial. However, the paper lacks a detailed explanation of how this process is specifically conducted.\n\n- The test-time resolution search method lacks comparison with other models. Additionally, it's unclear whether this test-time method can be effectively transferred to other models."}, "questions": {"value": "- Will the training data be open-sourced? While the data recipe is public, the actual data is annotated using closed-source models, making replication challenging for other researchers.\n\n- Could you elaborate on the process for filtering low-quality data and re-annotating it? Specifically, how is the combination of human and model verification utilized? This is a crucial aspect that requires clarification."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UPepd5FF0j", "forum": "frMCrXOioZ", "replyto": "frMCrXOioZ", "signatures": ["ICLR.cc/2026/Conference/Submission10369/Reviewer_gmxU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10369/Reviewer_gmxU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965724034, "cdate": 1761965724034, "tmdate": 1762921691393, "mdate": 1762921691393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper MindVL introduces an efficient framework (MindSpeed-MLLM) for training multimodal large language models (MLLMs) on Ascend NPUs, challenging the dominance of NVIDIA GPUs. It provides open, reproducible data recipes and introduces two key techniques—model weight averaging and test-time resolution search—to enhance performance. Results show that MindVL-8B matches or exceeds Qwen2.5VL-7B performance while using 10% of its training data, and MindVL-671B-A37B matches Qwen2.5VL-72B performance with only 3% of the data. Both models perform strongly on multimodal benchmarks, demonstrating the viability of Ascend NPUs for large-scale MLLM training."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Efficiency and Transparency: MindVL achieves high performance with far less data while openly detailing its data recipes and training setup, improving reproducibility.\n\n2. Hardware Innovation: It proves that Ascend NPUs can effectively train large multimodal models, expanding hardware options beyond NVIDIA GPUs.\n\n3. Practical Enhancements: Novel methods like model weight averaging and test-time resolution search boost robustness and multimodal alignment at low computational cost."}, "weaknesses": {"value": "1. Limited Architectural Novelty: The model mainly builds on existing designs like Qwen2.5-VL, offering engineering improvements rather than new modeling concepts.\n\n2. Incomplete Reasoning Performance: MindVL underperforms top models on complex multimodal and STEM reasoning tasks.\n\n3. Restricted Accessibility: Dependence on Ascend hardware limits reproducibility and adoption outside that ecosystem."}, "questions": {"value": "The paper claims high training efficiency but doesn’t provide concrete hardware utilization statistics (e.g., throughput, FLOPs/s, energy use) to compare with NVIDIA baselines."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7dizrtz4hT", "forum": "frMCrXOioZ", "replyto": "frMCrXOioZ", "signatures": ["ICLR.cc/2026/Conference/Submission10369/Reviewer_KT7G"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10369/Reviewer_KT7G"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10369/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762006146218, "cdate": 1762006146218, "tmdate": 1762921690704, "mdate": 1762921690704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}