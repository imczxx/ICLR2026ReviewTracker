{"id": "uVKtkLB6BZ", "number": 13384, "cdate": 1758217277845, "mdate": 1763620235334, "content": {"title": "Adapting Noise to Data: Generative Flows from learned 1D Processes", "abstract": "We introduce a general framework for learning data-adaptive latent distributions (noise)\nin generative models based on 1D quantile functions through minimizing a statistical\ndiscrepancy between noise and data samples. Our quantile-based parameterization naturally\nadapts to heavy-tailed or compactly supported target distributions while shortening transport\npaths by capturing marginal structure. This construction, originally motivated by the study\nof 1D processes beyond the usual diffusion, integrates seamlessly with standard training\nobjectives, including flow matching and consistency models. Numerical experiments\nhighlight both the flexibility and the effectiveness of our approach, achieved with minimal\ncomputational overhead.", "tldr": "", "keywords": ["generative modelling", "flow matching", "noise learning", "optimal transport"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bf88dc8dbc69f0a513875410ad2796c4b733c86e.pdf", "supplementary_material": "/attachment/01fe844e9bdbd0fb670d57d87ec818cc4f645498.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose a method to learn the initial distribution of a flow matching model. Usually, this distribution is chosen to be Gaussian noise. Here, they parameterize the initial distribution with a learnable quantile function. Beyond the initial distribution, this distribution also determines all intermediate distributions (commonly called probability path), while having a fixed linear interpolant. They jointly learn this initial distribution with the velocity field. Experiments on synthetic data sets and small-scale image datasets show the validity of the method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This is a natural idea that has not been explored in the literature as much and that could be a powerful way of improving flow-based generative models.\n- It is a simple training objective that has minimal computational overhead."}, "weaknesses": {"value": "- Overall, the writing of the paper could be improved significantly. The motivation is not well-explained in the text (both in the introduction and later in the text). Further, illustrations and examples are lacking.\n- The experiments are limited and the presented results are not very strong. For example, for CIFAR10, the flow baseline is worse than standard baselines (https://github.com/facebookresearch/flow_matching). FM achieves an FID on CIFAR10 <=3.0.\n- The training objective requires more elaboration: The parameters phi that parameterize the initial distribution underlie a trade-off: They can be either used to  minimize the first or second term in the training objective L(theta, phi). Therefore, even for lambda=0, minimizing this objective might be valid (i.e. one minimizes then effectively the residual variance of the CFM loss). As such discussions are at the core of the idea, it would be good to elaborate on this more."}, "questions": {"value": "- L26: \"Consistency models like the recently introduced inductive moment matching (IMM) Zhou et al. (2025)\" → Consistency Models are generally speaking different from IMM. I would rather present them as different methods.\n- Proposition 2 is known prior to this work, e.g. it is a special case of Proposition 4 in [1] and should be referenced.\n- Why are Consistency Models discussed? They are not really used anywhere?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U2GxB6xNsT", "forum": "uVKtkLB6BZ", "replyto": "uVKtkLB6BZ", "signatures": ["ICLR.cc/2026/Conference/Submission13384/Reviewer_Ghhr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13384/Reviewer_Ghhr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760571607006, "cdate": 1760571607006, "tmdate": 1762924025113, "mdate": 1762924025113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We thank all reviewers for their constructive feedback and valuable suggestions! We have revised the manuscript carefully to address all concerns, and welcome any additional feedback.  \nModifications in the revised version are marked in blue. \nHere are the main general changes:\n\n1. To enhance clarity and better highlight the significance of our work, we revised the abstract and introduction, and reordered Sections 3 and 4. Section 4 presents our main contribution, while Section 3 establishes our underlying motivation, which is independently noteworthy.\n2. We incorporated all proposed references. Many thanks for pointing out!\n3. We invested big effort to improve the writing of the numerical part and the experimental results including quantitative comparisons which hopefully underlines the potential of our method.\n4. We enhanced the appendix by additional explanations and numerical examples."}}, "id": "n2Rh9ok98A", "forum": "uVKtkLB6BZ", "replyto": "uVKtkLB6BZ", "signatures": ["ICLR.cc/2026/Conference/Submission13384/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13384/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13384/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763571843425, "cdate": 1763571843425, "tmdate": 1763571843425, "mdate": 1763571843425, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel generative modeling framework that constructs flow-based models using learned one-dimensional noising processes. Instead of relying on a fixed Gaussian latent distribution, the method learns the noise distribution directly through quantile functions that adapt to the data. This formulation integrates naturally with the flow matching framework, enabling more flexible and data-dependent noise modeling. The authors further illustrate the approach through several examples of one-dimensional processes, including the Wiener process, the Kac process, and an MMD gradient flow, and show that learning quantile-based noise can substantially enhance the flexibility and transport efficiency of generative models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of constructing generative flows through learnable 1D quantile processes is original.\n\n2. By using quantile parameterizations, the approach can handle distributions with compact support or heavy tails, going beyond the Gaussian assumptions typical in flow and diffusion models.\n\n3. The framework is compatible with standard objectives such as Flow Matching and Inductive Moment Matching, showing practical extensibility."}, "weaknesses": {"value": "1. Lack of sufficient baselines.\n\nThe paper lacks adequate baseline comparisons to clearly demonstrate the advantages of the proposed method. In Section 5.1, no baseline is provided for reference, and Figure 5 includes only a single baseline whose selection and description are not well explained. The experimental evaluation should include more detailed quantitative comparisons against standard diffusion or flow-based models to better substantiate the claimed improvements.\n\n2. Clarity and presentation issues.\n\nThe overall clarity of the paper can be improved. The abstract does not effectively summarize the key contributions and contains some redundancy. For example, the first and third sentences are quite similar. Several methodological details are also unclear. For instance, the statement “we pre-train our quantile” (Line 356) does not explain why pre-training is necessary or which experiments rely on it. Similarly, the introduction of the regularization term that penalizes the expected negative log-determinant of the Jacobian (Line 374) is mentioned without justification or analysis of its impact. These elements should be clarified to improve the transparency and reproducibility of the work.\n\n3. Expressive power of one-dimensional processes.\n\nThe paper does not provide sufficient theoretical or empirical evidence regarding the expressive power of using one-dimensional denoising processes. While the decomposition into independent one-dimensional components makes the approach more tractable, it may limit the model’s ability to capture complex dependencies across dimensions. A deeper discussion or ablation study evaluating this trade-off would strengthen the paper’s technical soundness."}, "questions": {"value": "1. Generality of one-dimensional flows\n\nIs there a universal or systematic way to construct one-dimensional flows, beyond the three specific examples discussed in Section 4.1?\n\n2. Sampling efficiency\n\nWhat is the sampling time or computational cost of the proposed method compared to standard flow matching or diffusion-based models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y6zroyJt0V", "forum": "uVKtkLB6BZ", "replyto": "uVKtkLB6BZ", "signatures": ["ICLR.cc/2026/Conference/Submission13384/Reviewer_vyUw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13384/Reviewer_vyUw"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761549541198, "cdate": 1761549541198, "tmdate": 1762924024583, "mdate": 1762924024583, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a framework for 1D per-dimension noising processes for generative \nmodels. They propose to learn the latent distribution to reduce the transport paths of \ngenerative models. The latent distribution is modeled via learned quantile functions, which \nare modeled via rational quadratic splines. The quantile functions are learned from data by \nminimizing the Wasserstein-2 distance between the data distribution and the modeled latent \ndistribution.\n\nMain contribution:\n- Decomposition of multidimensional flows into 1D noising processes\n- Quantile-based formulation of latent distribution: learn the quantile function of latent\nnoise instead of fixing it to a Gaussian distribution\n- Experimental validation on synthetic data (checkerboard, funnel, Gaussian mixture)\nand image data (MNIST, CIFAR-10)"}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Minimal computational overhead via rational quadratic splines\n- improved noise distribution adapted to target\n- Good explanation of the math fundamentals\n- provides a general framework for independent 1D noising processes and an\nexpressive way to parameterize them in practice via quantile functions and rational quadratic splines"}, "weaknesses": {"value": "- Missing ablation study on the velocity not exploding outside of the support of the\ndistribution. A simple 2D example showing the vector field would be nice.\n- Missing benchmarks on larger problems -> how scalable and stable is this approach\nwith an increasing problem dimension? -> potentially unstable quantile training\n- lack of quantitative metrics\n- unclear generalization capability for shifting data distributions"}, "questions": {"value": "The weight on the quantile loss and the regularization weight are both new\nhyperparameters that need to be tuned. How sensitive are they to different problems? On the funnel target, the quantiles are pre-trained -> another hyperparameter.\n- How stable is the joint optimization of quantile and flow networks in practice?\n- Is learning quantiles equivalent to learning transport maps under certain\nassumptions?\n- How does the learned quantile noise compare to learned latent priors in VAEs or\nnormalizing flows?\nCan this method scale to modern high-resolution diffusion tasks?\n- What happens for out-of-distribution conditional data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OXvJ1u7Y88", "forum": "uVKtkLB6BZ", "replyto": "uVKtkLB6BZ", "signatures": ["ICLR.cc/2026/Conference/Submission13384/Reviewer_ySRN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13384/Reviewer_ySRN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945595536, "cdate": 1761945595536, "tmdate": 1762924024198, "mdate": 1762924024198, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces an approach that uses one dimensional processes and quantile functions to learn generative models in a component-wise manner. The author shows how this approach is compatible with the flow matching and consistency model frameworks, and can better handle difficult settings such as heavy tails and compact supports."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Originality: The core ideas (e.g., using 1D processes and quantile functions to learn generative models in a way that is compatible with consistency and flow matching frameworks, etc) are, to the best of my knowledge, original and innovative. \n\nClarity: the paper is written in a clear and self-contained manner. Even in the more technical portions, everything is defined and explained clearly. This is a major strength of the paper. \n\nQuality: I find the quality of the theoretical and empirical sections to be sufficient. While one can always perform more experiments on more datasets/simulations, the current experiments sufficiently demonstrate/support the main points of the paper. While I did not check proofs/appendix in detail, the technical portions of the main paper are, to the best of my knowledge, sound and correct. \n\nSignificance: The topic of learning generative models is timely and significant. The proposed method is also a significant contribution in my opinion, more than enough to meet the bar for ICLR."}, "weaknesses": {"value": "There are minor points and questions which I bring up below: \n\n- When the authors mention the difficulty of learning multimodal and heavy-tailed targets on page 1, Hagemann and Neumayer (2021) and Salmona et al (2022) are cited. However, there are other highly relevant literature that should have been cited. These include:\n\n- Concentration of Measure for Distributions Generated via Diffusion Models. R Ghane, A Bao, D Akhtiamov, B Hassibi\n- On the Statistical Capacity of Deep Generative Models. E Tam, D Dunson\n- Copula & Marginal Flows: Disentangling the Marginal from its Joint. M Wiese, R Knobloch, R Korn\n\n- Runtime/computational costs: does the one dimensional approach that the authors propose lead to higher runtime or computational complexity in practice compared to other FM/consistency based approaches? (I am NOT looking for any computational complexity bounds/results, I am mainly interested in just a couple of sentences that comment on the runtime/computational aspects of things so readers can get a rough idea)."}, "questions": {"value": "See above section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 10}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bEVo5w8IzL", "forum": "uVKtkLB6BZ", "replyto": "uVKtkLB6BZ", "signatures": ["ICLR.cc/2026/Conference/Submission13384/Reviewer_v4Fw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13384/Reviewer_v4Fw"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13384/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762365614692, "cdate": 1762365614692, "tmdate": 1762924023863, "mdate": 1762924023863, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}