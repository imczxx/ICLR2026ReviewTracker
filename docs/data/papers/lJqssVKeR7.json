{"id": "lJqssVKeR7", "number": 13051, "cdate": 1758213105229, "mdate": 1759897468799, "content": {"title": "Converge Faster, Talk Less: Hessian-Informed Federated Zeroth-Order Optimization", "abstract": "Zeroth-order (ZO) optimization enables dimension-free communication in federated learning (FL), making it attractive for fine-tuning of large language models (LLMs) due to significant communication savings. However, existing ZO-FL methods largely overlook curvature information, despite its well-established benefits for convergence acceleration. To address this, we propose **HiSo**, a Hessian-informed ZO federated optimization method that accelerates convergence by leveraging global diagonal Hessian approximations, while strictly preserving scalar-only communication **without transmitting any second-order information**. Theoretically, for non-convex functions, we show that HiSo can achieve an accelerated convergence rate that is independent of the Lipschitz constant $L$ and model dimension $d$ under some Hessian approximation assumptions, offering a plausible explanation for the observed phenomenon of ZO convergence being much faster than its worst-case $O(d)$-bound. Empirically, across diverse LLM fine-tuning benchmarks, HiSo delivers a 1$\\sim$5× speedup in communication rounds over existing state-of-the-art ZO-FL baselines. This superior convergence not only cuts communication costs but also provides strong empirical evidence that Hessian information acts as an effective accelerator in federated ZO optimization settings.", "tldr": "", "keywords": ["Zeroth-Order Optimization", "Federated Optimization", "Hessian"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3252afbb60add7463414849859431c8edceeddd5.pdf", "supplementary_material": "/attachment/1d6ace410a72482e47a61699500a95b764d433b3.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes HiSo, a novel Hessian-informed zeroth-order optimization method for federated learning that achieves dimension-free communication while accelerating convergence. The key innovation lies in leveraging global diagonal Hessian approximations as preconditioners without transmitting second-order information, thus preserving the scalar-only communication paradigm. Theoretically, the authors establish a convergence rate independent of model dimension and Lipschitz constant under certain Hessian approximation assumptions, providing the first such result for ZO methods in FL with multiple local updates. Empirically, HiSo demonstrates 1-5× speedup in communication rounds and up to 90 million times communication savings compared to first-order baselines across diverse LLM fine-tuning tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "The primary strength is the elegant resolution of a fundamental tension between leveraging second-order information for acceleration and maintaining extreme communication efficiency. HiSo represents a conceptually novel approach with non-trivial theoretical analysis and compelling empirical results. The scalar-only communication property holds significant practical value for bandwidth-constrained federated LLM fine-tuning."}, "weaknesses": {"value": "The main weakness concerns the foundational assumptions underlying its theoretical advantages. The critical \"well-approximated Hessian\" condition is challenging to verify empirically, particularly during early training stages. While outperforming ZO baselines, the accuracy gap with first-order methods remains, highlighting inherent limitations of the ZO approach."}, "questions": {"value": "The theoretical advantages rely on the \"well-approximated Hessian\" condition. Do you have any empirical evidence (e.g., comparing with true Hessian on small models) suggesting your learning method effectively captures principal curvature directions?\n\nIn extreme FL scenarios with massive client populations (e.g., millions), could maintaining client participation history become a server-side memory bottleneck? How do you assess this scalability aspect?\n\nThe Hessian smoothing parameter ν appears to have minimal impact. Does this indicate HiSo is genuinely insensitive to this hyperparameter, or are there implicit guidelines for its selection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cHkvDxWs9Z", "forum": "lJqssVKeR7", "replyto": "lJqssVKeR7", "signatures": ["ICLR.cc/2026/Conference/Submission13051/Reviewer_7YFj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13051/Reviewer_7YFj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833710408, "cdate": 1761833710408, "tmdate": 1762923783757, "mdate": 1762923783757, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes HiSo, a novel federated learning (FL) algorithm designed for communication-efficient fine-tuning of Large Language Models (LLMs). HiSo innovatively combines zeroth-order (ZO) optimization for dimension-free communication with Hessian-informed preconditioning to accelerate convergence. The core idea is to use a global, diagonal Hessian approximation to guide the random search directions in ZO optimization, effectively creating a \"natural gradient\" style update. Crucially, this Hessian information is learned and applied without transmitting any second-order matrices, preserving the scalar-only communication property that makes ZO methods attractive for high-dimensional problems. Theoretically, the authors provide a convergence analysis suggesting that under a \"low-effective-rank\" assumption for the Hessian, HiSo can achieve a rate independent of the model dimension d and Lipschitz constant L. Empirically, HiSo demonstrates significant speedups (1-5x in communication rounds) and up to 90 million times lower communication cost compared to first-order baselines like FedAvg on LLM fine-tuning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.  Novel and Well-Motivated Idea: The combination of Hessian-informed updates with scalar-only communication is highly innovative. It directly addresses the primary weakness of ZO methods (slow convergence) while preserving their key advantage (low communication cost). The motivation is clear and grounded in the practical challenge of federated LLM fine-tuning.\n2.  Strong Empirical Evaluation: The experiments are comprehensive, spanning from simple CNN/MNIST setups to LLM fine-tuning on standard NLP benchmarks (SST-2, QQP, SQuAD). The comparisons against a wide range of baselines (FedAvg, FedAdam, FedZO, DeComFL) are convincing and demonstrate clear improvements in convergence speed and final accuracy for ZO methods.\n3.  Significant Practical Impact: The reported communication savings (MBs vs TBs) are monumental. If applicable to even larger models, HiSo could have a substantial impact on the feasibility of federated fine-tuning in bandwidth-constrained environments.\n4.  Theoretical Depth: The paper goes beyond mere algorithm design by providing a non-trivial theoretical analysis. The introduction of the \"low whitening rank\"  to explain the accelerated convergence is a valuable conceptual contribution that helps reconcile the practical efficiency of ZO methods with their pessimistic worst-case theoretical bounds."}, "weaknesses": {"value": "1.  Limited Discussion on Computation Overhead: While communication is the primary bottleneck, the computational cost of ZO methods is inherently higher than first-order methods due to the need for multiple forward passes. The paper briefly mentions the preconditioning time is negligible but does not provide a full comparison of the total wall-clock time (computation + communication) against first-order methods. This is crucial for assessing real-world utility, as increased computation time might offset communication savings.\n2.  Validation of Theoretical Assumptions: The core theoretical improvement hinges on the \"well-approximated Hessian\" condition (Eq. 17) and the low-effective-rank property. While the empirical results are strong evidence, the paper does not directly validate that the learned diagonal matrix H actually satisfies this condition for the LLMs used in the experiments. A more rigorous analysis or measurement of the whitening rank  during training would strengthen the theoretical claims.\n3.  Comparison with Parameter-Efficient Fine-Tuning (PEFT): The discussion of FL+PEFT baselines (like FedLoRA) is brief and relegated to the appendix. Given that PEFT is a dominant approach for efficient LLM fine-tuning, a more thorough comparison in the main text is warranted. HiSo's claim of \"full-parameter\" tuning is a different paradigm, but a direct comparison on metrics like final accuracy, communication cost, and memory usage would better situate HiSo within the existing landscape.\n4.  Hyperparameter Sensitivity: The ablation study on the Hessian smoothing parameter ν shows robustness, but the performance of adaptive methods like HiSo can be sensitive to other hyperparameters like the learning rate and the exponential moving average decay factor. A more detailed sensitivity analysis would be helpful for practitioners."}, "questions": {"value": "1.  How does the total energy consumption (a function of both communication and computation) of HiSo compare to first-order methods and DeComFL, especially when considering the additional forward passes required for ZO estimation?\n2.  Could the HiSo framework be naturally extended to incorporate PEFT techniques (e.g., applying Hessian-informed updates only to low-rank LoRA parameters)? Do you see this as a promising future direction?\n3.  The theory suggests performance degrades to match DeComFL if the Hessian approximation is poor. Did you observe this in practice during the initial stages of training or on any specific tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "t9JC58MxVZ", "forum": "lJqssVKeR7", "replyto": "lJqssVKeR7", "signatures": ["ICLR.cc/2026/Conference/Submission13051/Reviewer_3tFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13051/Reviewer_3tFi"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994752429, "cdate": 1761994752429, "tmdate": 1762923783472, "mdate": 1762923783472, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors propose HiSo, a Hessian informed, scalar only, federated zeroth order optimizer for large language model fine tuning.  The authors first generalize the scalar only paradigm into a reusable federated optimization framework, where the server and clients exchange only scalar update codes and can reconstruct each other's parameter trajectories without ever sending high dimensional tensors. Within that framework, they introduce HiSo, which maintains a global diagonal Hessian approximation on the server and uses it to precondition the zeroth order perturbation directions. Effectively, HiSo samples Hessian aware update directions, similar in spirit to natural gradient or approximate Newton steps, but still communicates only scalars."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. Theoretical contribution:  The paper proves non convex convergence bounds where the rate depends on a whitening rank related to the effective Hessian spectrum instead of the raw model dimension, and extends DeComFL style theory to multiple local steps per round.\n\n2. Strong and concrete motivation:  Existing scalar only ZO methods solve bandwidth but converge painfully slowly. HiSo squarely targets this convergence bottleneck without giving up the scalar only advantage.\n\n3. The paper repeatedly reports actual bandwidth numbers in KB and TB, and highlights 10^7 to 10^8 fold savings compared to FedAdam and FedAvg."}, "weaknesses": {"value": "1. Benchmark scale and diversity：The main LLM experiments involve six clients with two sampled per round, and tasks are classification and extractive QA. Although these are standard NLP benchmarks and good stress tests for convergence and accuracy, they are still small compared to industrial federated networks across hospitals, phones, or enterprises. The paper would be stronger if it included either larger federations or at least a stress test with many more clients and skew patterns.\n\n\n2. Backbone model: The authors only conduct experiments on OPT series models, how about the Qwen series models? I am curious about the performance of proposed method on those models. \n\n3. Theory assumptions and coverage:\n   The convergence guarantee depends on a low effective rank Hessian spectrum and on the quality of the diagonal Hessian approximation. While the paper provides evidence that the estimated diagonal Hessian has a long tail and argues that large language models empirically satisfy this, it does not show failure cases or quantify how often the whitening rank assumption holds in domains beyond language. This makes it harder to judge how robust the dimension independent claim really is."}, "questions": {"value": "see weakness 1,2,3"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BwvVFzkpvL", "forum": "lJqssVKeR7", "replyto": "lJqssVKeR7", "signatures": ["ICLR.cc/2026/Conference/Submission13051/Reviewer_FQxG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13051/Reviewer_FQxG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762152463509, "cdate": 1762152463509, "tmdate": 1762923783169, "mdate": 1762923783169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies federated learning via zeroth-order optimization. By proposing a method that preserves scalar-only communication and avoids transmitting second-order information, it significantly reduces computational costs. Theoretically, they demonstrates an accelerated convergence rate under a suitable Hessian structure."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-written, with a well-motivated research goal and a clear description of the  algorithm."}, "weaknesses": {"value": "I have the following concerns regarding the paper:\n\nTheoretical Practicality and Depth: The theoretical analysis relies on a good approximation of the Hessian, yet the method employed in practice is only a diagonal approximation. This gap makes it difficult to appreciate the practical relevance of Theorem 1. Furthermore, a simple non-convex analysis seems insufficient, as it fails to capture the specific landscape properties of neural network loss functions.\n\nExperimental Comprehensiveness: The experimental validation appears somewhat limited. It would be strengthened by including plots of the training loss against both iterations and wall-clock time on more experiments. Additionally, experiments on more datasets and with larger models would better demonstrate the scalability and robustness of the proposed method.\n\nNovelty of Insight: The core idea seems limited. Since zeroth-order optimization inherently accesses only scalar information at each step, the advantage of communicating solely scalars appears straightforward. I did not find significant novel algorithmic insights in the current work."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "46c9maUbgL", "forum": "lJqssVKeR7", "replyto": "lJqssVKeR7", "signatures": ["ICLR.cc/2026/Conference/Submission13051/Reviewer_HtDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13051/Reviewer_HtDg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762430695204, "cdate": 1762430695204, "tmdate": 1762923782738, "mdate": 1762923782738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors of this manuscript propose HiSo, a Hessian-informed federated zeroth-order (ZO) optimization method. It aims to accelerate the convergence of ZO-FL, which is attractive for LLM fine-tuning due to its scalar-only (dimension-free) communication, but typically suffers from slow convergence. The method computes a global diagonal Hessian approximation using an Adam-style update rule, which does not require transmitting any second-order information. The authors use this approximation to inform the ZO update direction. They provide a theoretical analysis showing convergence rates independent of model dimension and empirical results demonstrating a 1-5x speedup in communication rounds over the DeComFL baseline."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1- The improved convergence by HiSo is significant in comparison to other ZO-FL benchmarks, while keeping the communication costs significantly small. The 1-5x speedup over DeComFL is a practical and valuable improvement.\n\n2- The paper provides a theoretical analysis to support the method, proving a convergence rate independent of model dimension `d` for non-convex functions.\n\n3- The proposed method addresses a clear and important bottleneck in federated LLM fine-tuning, namely the high communication cost of traditional first-order methods and the slow convergence of previous ZO methods."}, "weaknesses": {"value": "1- The experimental validation is limited to the OPT family of models (OPT-125M to OPT-2.7B). These models are somewhat dated and are known to be undertrained. The effectiveness of HiSo on newer, more capable models (e.g., smaller variants of LLaMA-3.2, Qwen-2.5, Gemma 3, or SmolLM) is not demonstrated. It is unclear if the same optimization behavior will hold on these new architectures.\n\n2- The claim that HiSo is a \"Hessian-informed\" or \"second-order\" method is potentially misleading. The proposed update for the diagonal Hessian approximation in Equation (12) is a recursive exponential moving average of the squared updates. This formulation is functionally identical to the variance/second-moment tracking in first-order adaptive optimizers like RMSProp or Adam. This makes the contribution appear more as an application of an adaptive preconditioner to the ZO setting, rather than a true second-order method."}, "questions": {"value": "1- How does the performance of HiSo change when applied to newer generations of models, such as the HuggingFace SmolLM or smaller variants of Qwen-2.5, LLaMA-3.2, and Gemma 3? Do the same convergence benefits hold?\n\n2- The authors call the method \"Hessian-informed.\" However, the update in Equation (12) is mathematically very similar to the variance/second-moment update in the Adam optimizer (a first-order method). Could the authors clarify why this should be considered a second-order method and not a first-order adaptive method? A direct comparison of the HiSo formulation with Adam would be insightful.\n\n3- The paper focuses on fine-tuning. Can the proposed ZO-FL framework be used for the pretraining of LLMs? What are the primary challenges in applying this method to the pretraining regime? I understand pretraining is expensive and do not expect experiments, but insight into the challenges would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "LDLsbmrAJr", "forum": "lJqssVKeR7", "replyto": "lJqssVKeR7", "signatures": ["ICLR.cc/2026/Conference/Submission13051/Reviewer_NVun"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13051/Reviewer_NVun"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13051/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762462120852, "cdate": 1762462120852, "tmdate": 1762923782492, "mdate": 1762923782492, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}