{"id": "aNUVttHlU8", "number": 21651, "cdate": 1758320122350, "mdate": 1759896910645, "content": {"title": "A Lightweight Heuristic for Detecting Unfair Tests in Software Engineering Benchmarks", "abstract": "As existing software engineering benchmarks age, the danger of model contamination is driving interest in automated curation pipelines. Unfortunately, ensuring high-quality task instances without manual assessment is a significant challenge. An obvious choice is to use large language models (LLMs) in place of human annotators, but this comes with the usual drawbacks: complex scaffolding, prompt sensitivity, hyperparameter dependence, lack of reproducibility, and substantial environmental cost. We buck this trend by proposing a lightweight, deterministic algorithm for detecting overly-specific software tests, and in doing so we support the selection of high-quality benchmark tasks. We evaluate our heuristic against the human annotations used to develop SWE-bench Verified and we compare the resulting accuracy to the accuracies of two LLM-based alternatives. We find that the accuracy of our heuristic is slightly higher than the reported accuracy of all non-fine-tuned LLM configurations across both alternatives, and slightly lower than the reported accuracy of a fine-tuned model. Given the additional effort, complexity and environmental impact associated with fine-tuning, we consider this to be a positive result. We further propose a version of our heuristic that is less precise, but more sensitive, and we use it to highlight the importance of balancing precision and recall. Our work demonstrates that straightforward, analytical techniques remain a viable alternative to both manual, and LLM-based, benchmark curation methods.", "tldr": "Some SWE-bench tests are bound to fail because they enforce conditions that aren't specified in the corresponding issue descriptions. We propose a lightweight algorithm that identifies these 'unfair' tests with a similar degree of accuracy to LLMs.", "keywords": ["Large Language Models", "Software Engineering", "Benchmarks", "Benchmark Curation", "SWE-bench", "SWE-bench Verified", "Static Analysis"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ea316490d9396f965ab637c115cffe5c508ab56.pdf", "supplementary_material": "/attachment/db7f48081d5fdaa9e7a48c50a937c6fa63cf59b1.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes a lightweight heuristic for detecting unfair tests in software engineering benchmarks, a subset of underspecified issues where validation tests rely on information absent from the issue description. The heuristic aims for simplicity, determinism, and reproducibility, and is positioned as an alternative to LLM-based curation methods. The heuristic identifies instances as “unfair” when overlapping string, numeric, or identifier tokens between the test and solution patches are missing from the issue description. Through experiments on SWE-bench and SWE-bench Verified, and by comparison with SPICE and SWE-Rebench, the paper shows that the heuristic achieves accuracy slightly above non–fine-tuned LLM models, though below fine-tuned ones."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The method is simple, deterministic, and reproducible. It avoids the complexity and cost of LLM-based alternatives.\n- The empirical evaluation compares directly against two existing automated curation pipelines (SPICE and SWE-Rebench).\n- Implementation and datasets are documented and reproducible."}, "weaknesses": {"value": "- The contribution is narrow. The paper solves a subproblem (focusing only on unfair tests, a limited subset of underspecification issues) in an already niche topic (software engineering benchmark curation).\n- As a result, the practical utility is unclear: the paper does not convincingly show that unfair tests are a major bottleneck in creating benchmarks, compared to the overall effort, nor that removing them improves benchmark usefulness.\n- Comparisons are weak: experiments only compare against random baselines and indirect LLM results. As a side note, a random classifier is always expected to yield 50% accuracy.\n- Strong assumptions about variable naming and code quality limit (many valid tests may use inconsistent or auto-generated identifiers).\n- Due to mild performance, the proposed heuristic risks flagging many benign instances, which raises anew the need for manual inspection (the very problem that the proposed approach aims to avoid)."}, "questions": {"value": "I do not have any specific question whose answer could change my opinion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pXAvYsN3Ha", "forum": "aNUVttHlU8", "replyto": "aNUVttHlU8", "signatures": ["ICLR.cc/2026/Conference/Submission21651/Reviewer_CiXK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21651/Reviewer_CiXK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760533474164, "cdate": 1760533474164, "tmdate": 1762941871456, "mdate": 1762941871456, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a lightweight, deterministic heuristic to identify \"unfair tests\" in SWE-bench, which is presented as a low-cost, reproducible alternative to expensive, non-deterministic LLM-based curation pipelines."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates a valid test case issue in its methodology section."}, "weaknesses": {"value": "- The paper fails to provide a clear and formal method. In its method section, it shows an example instead of a formal methodology. Additionally, the example seems hard to generalize to other problems for software engineering benchmarks.\n\n- While high-quality test cases are needed, the method proposed in this work is abstract, making it difficult to find real-world test cases that meet the filtering requirements (as question requirements, patch fixes, and test case token distributions are often completely different). Crucially, the paper does not sufficiently justify the filtering, as an unfair test case that causes all models to fail would not affect the relative benchmark scores.\n\n- The paper is densely written and very hard to follow."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 0}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "HMggo8oF88", "forum": "aNUVttHlU8", "replyto": "aNUVttHlU8", "signatures": ["ICLR.cc/2026/Conference/Submission21651/Reviewer_K3xD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21651/Reviewer_K3xD"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761424498146, "cdate": 1761424498146, "tmdate": 1762941871223, "mdate": 1762941871223, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a lightweight heuristic method for detecting \"unfair tests\" in SWE-bench–style benchmarks by comparing identifiers, literals, and AST-extracted tokens between issues and patches. The goal is to identify cases where tests reveal patch content that the model should not have access to. The authors compare the heuristic approach to SWE-bench Verified, SPICE, and SWE-Rebench methods, reporting competitive precision while avoiding LLM inference cost."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Practical problem: test fairness in SWE-bench-style datasets\n- Reproducible and deterministic method\n- Lightweight — no compute burden unlike LLM curation pipelines\n- Good comparison against benchmark curation systems"}, "weaknesses": {"value": "- Limited novelty (heuristic token matching)\n- Low recall limits real-world usefulness\n- Over-generalized claims vs existing LLM curation tools\n- Relies on noisy SWE-V labels as \"ground truth\"\n- No hybrid or prompting baselines\n- No error analysis or qualitative insights\n- Python-only implementation limits generalizability"}, "questions": {"value": "1. Why no comparison to a simple GPT-4 zero-shot fairness-classification prompt?\n2. Can you report confusion matrix and qualitative cases?\n3. How sensitive is performance to project domain or code style?\n4. Could hybrid approaches (heuristics + small model) improve recall?\n5. Are multi-file patches or cross-file identifiers considered?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "stUEn09NLW", "forum": "aNUVttHlU8", "replyto": "aNUVttHlU8", "signatures": ["ICLR.cc/2026/Conference/Submission21651/Reviewer_t32Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21651/Reviewer_t32Q"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21651/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907817582, "cdate": 1761907817582, "tmdate": 1762941870922, "mdate": 1762941870922, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}