{"id": "PHWcPmkU9I", "number": 19855, "cdate": 1758300034123, "mdate": 1763485323421, "content": {"title": "Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise", "abstract": "Gradient clipping is a widely used technique in Machine Learning and Deep Learning (DL), known for its effectiveness in mitigating the impact of heavy-tailed noise, which frequently arises in the training of large language models. Additionally, first-order methods with clipping, such as \\algname{Clip-SGD}, exhibit stronger convergence guarantees than \\algname{SGD} under the $(L_0,L_1)$-smoothness assumption, a property observed in many DL tasks. However, the high-probability convergence of \\algname{Clip-SGD} under both assumptions -- heavy-tailed noise and $(L_0,L_1)$-smoothness -- has not been fully addressed in the literature. In this paper, we bridge this critical gap by establishing the first high-probability convergence bounds for \\algname{Clip-SGD} applied to convex $(L_0,L_1)$-smooth optimization with heavy-tailed noise. Our analysis extends prior results by recovering known bounds for the deterministic case and the stochastic setting with $L_1 = 0$ as special cases. Notably, our rates avoid exponentially large factors and do not rely on restrictive sub-Gaussian noise assumptions, significantly broadening the applicability of gradient clipping.", "tldr": "", "keywords": ["stochastic optimization", "gradient clipping", "generalized smoothness", "Clip-SGD", "high-probability convergence", "heavy-tailed noise"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1e6814c45a25d53ed3ae84b82c4707f836e4b250.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors study Clip‑SGD for convex objectives that satisfy $\\left(L_0, L_1\\right)$-smoothness under heavy‑tailed gradient noise. The authors aim to answer the question of how to choose the clipping level so that it simultaneously respects the geometry induced by $\\left(L_0, L_1\\right)$ and controls heavy tails (which typically demand growing thresholds)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. High-probability convergence guarantees for Clip-SGD under both $\\left(L_0, L_1\\right)$-smoothness and heavy-tailed noise fills a gap, assuming that this is the first work to do so as is claimed.\n\n2. The authors analyze standard Clip-SGD, which is both a strength and a weakness. As a strength, it is closer to what is done in practice, as a \"first step\" toward more complex analyses."}, "weaknesses": {"value": "1. There is no empirical section, and the paper would possibly benefit greatly from even synthetic experiments that support their theory. There are no experiments to illustrate behavior versus unclipped SGD or double‑sample methods (even toy convex problems would be useful).\n\n2. Could the authors discuss the primary technical obstacles in extending this high-probability analysis to the non-convex setting (that is, without assumption 1)?"}, "questions": {"value": "Please see weaknesses.\n\nAs a small suggestion, one work also may be worth looking into in the literature review is the paper [1], which also heavily studies clipping in the presence of heavy-tailed noise, though in a different setting than this paper. \n\n[1] Lee et al., Efficient Distributed Optimization under Heavy-Tailed Noise. ICML, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kDBDcfLbW8", "forum": "PHWcPmkU9I", "replyto": "PHWcPmkU9I", "signatures": ["ICLR.cc/2026/Conference/Submission19855/Reviewer_HYK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19855/Reviewer_HYK1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760645069877, "cdate": 1760645069877, "tmdate": 1762932027428, "mdate": 1762932027428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors study Clip‑SGD for convex objectives that satisfy $\\left(L_0, L_1\\right)$-smoothness under heavy‑tailed gradient noise. The authors aim to answer the question of how to choose the clipping level so that it simultaneously respects the geometry induced by $\\left(L_0, L_1\\right)$ and controls heavy tails (which typically demand growing thresholds)."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. High-probability convergence guarantees for Clip-SGD under both $\\left(L_0, L_1\\right)$-smoothness and heavy-tailed noise fills a gap, assuming that this is the first work to do so as is claimed.\n\n2. The authors analyze standard Clip-SGD, which is both a strength and a weakness. As a strength, it is closer to what is done in practice, as a \"first step\" toward more complex analyses."}, "weaknesses": {"value": "1. There is no empirical section, and the paper would possibly benefit greatly from even synthetic experiments that support their theory. There are no experiments to illustrate behavior versus unclipped SGD or double‑sample methods (even toy convex problems would be useful).\n\n2. Could the authors discuss the primary technical obstacles in extending this high-probability analysis to the non-convex setting (that is, without assumption 1)?"}, "questions": {"value": "Please see weaknesses.\n\nAs a small suggestion, one work also may be worth looking into in the literature review is the paper [1], which also heavily studies clipping in the presence of heavy-tailed noise, though in a different setting than this paper. \n\n[1] Lee et al., Efficient Distributed Optimization under Heavy-Tailed Noise. ICML, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kDBDcfLbW8", "forum": "PHWcPmkU9I", "replyto": "PHWcPmkU9I", "signatures": ["ICLR.cc/2026/Conference/Submission19855/Reviewer_HYK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19855/Reviewer_HYK1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760645069877, "cdate": 1760645069877, "tmdate": 1763497380646, "mdate": 1763497380646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies clipped stochastic gradient methods for convex $(L_0,L_1)$-smooth objectives under heavy-tailed noise with finite $\\alpha$-th moments $(\\alpha\\in(1,2])$. It presents high-probability guarantees using a single-sample clipping scheme and claims improved comparisons to prior analyses (e.g., avoiding certain exponential dependences) together with brief illustrative experiments."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The presentation connects $(L_0,L_1)$-smoothness with heavy-tailed noise in a single framework and recovers several special cases.\n\n- Technically careful: the proofs are self-contained and the algorithmic template (standard clipping) is simple to implement.\n\n- The organization is very clear."}, "weaknesses": {"value": "- Problem novelty is weak. Both \\emph{heavy-tailed} robustness for SGD (with clipping/truncation) and the \\emph{$(L_0,L_1)$-smoothness} framework have been extensively studied; the paper largely resembles a \\emph{combination} of two well-trodden threads (``A + B''), rather than introducing a new core idea or methodology.\n- Topic saturation and maturity. Techniques used (clipping-based potential arguments, tail-sensitive concentration) are standard in this area; the contribution reads as a consolidation within known toolkits rather than a conceptual advance.\n- Practical guidance is limited: the guarantees hinge on several constants and iteration thresholds (with explicit $\\delta$-dependence), yet the paper does not delineate when its schedules outperform light-tailed baselines, nor provide actionable tuning rules when $\\alpha$ and $L_1$ are unknown."}, "questions": {"value": "While technically careful, the paper addresses a non-novel combination: $(L_0,L_1)$-smoothness and heavy-tailed robustness via clipping have each been widely covered, and the present work effectively composes these mature lines without a fresh idea that shifts the frontier. \n\nAnalyzing the advantages of Adam over SGD may be more interesting."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qV40QTgnVe", "forum": "PHWcPmkU9I", "replyto": "PHWcPmkU9I", "signatures": ["ICLR.cc/2026/Conference/Submission19855/Reviewer_e9SL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19855/Reviewer_e9SL"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761411114188, "cdate": 1761411114188, "tmdate": 1762932026628, "mdate": 1762932026628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the convergence of Clipped-SGD for convex optimization under $(L_{0},L_{1})$-smoothness and heavy-tailed noise where the gradient estimate has a bounded $\\alpha$-th central moment for $\\alpha \\in (1, 2]$."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The authors identify a conflict in prior work: to handle $(L_{0},L_{1})$-smoothness, the clipping threshold $\\lambda$ is typically set to a fixed constant, whereas to handle heavy-tailed noise, $\\lambda$ needs to grow with the number of iterations $K$.\nThe main contribution of this paper is to bridge this gap, providing a high-probability convergence bound for Clipped-SGD under both conditions simultaneously with an unified clipping threshold strategy. This result also successfully avoids the exponential dependence on $L_{1}R_{0}$ that appeared in previous work.\n\nThe paper is in general well written, and the comparisons with related works are clear to me, especially the one with (Gaash et al. 2025), which make the technical contribution of the paper more clearer and interesting."}, "weaknesses": {"value": "- Dependence on  $1/\\delta$: To establish the high-probability bound, Theorem 1 (case 2) requires the total number of iterations $K = \\Omega(\\frac{(L_{1}R_{0})^{2+\\alpha}}{\\delta})$. This polynomial dependence on $1/\\delta$ is not standard comparing with  $\\log(1/\\delta)$ in Theorem 1 (case 1).  Could the authors comment on whether it might be possible to use more advanced probabilistic tools to improve the dependency to $\\log(1/\\delta)$?\n\n- As noted by the authors in the final section, the paper starts with the convex case, which is reasonable, and it would be also interesting to consider the non-convex case."}, "questions": {"value": "- Could the authors comment on whether it might be possible to use more advanced probabilistic tools to improve the dependency to $\\log(1/\\delta)$ for Theorem 1 (case 2)?\n\n- Does the analysis could be also applied to the more general noise models where $\\sigma^2 = A(f(x) - f^*) + B \\|\\nabla f(x)\\|^2 + C$ (Yu et al. 2025)?\n\n- As the authors mentioned a lot on ``while in the presence of heavy-tailed noise, the threshold is often required to grow with the total number of iterations to ensure stability and convergence,'' it would be also good to add the threshold parameter in Table 1.\n\n- Missing a “max” in the convergence rate of Thm 1(case 1)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4MB8jQqnqZ", "forum": "PHWcPmkU9I", "replyto": "PHWcPmkU9I", "signatures": ["ICLR.cc/2026/Conference/Submission19855/Reviewer_aGea"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19855/Reviewer_aGea"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761840773149, "cdate": 1761840773149, "tmdate": 1762932026098, "mdate": 1762932026098, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies stochastic optimization for convex and $(L_0, L_1)$ smooth functions under heavy-tailed gradient noise. Clipped-SGD is analyzed and the first high-probability bound is shown for Clipped-SGD for this problem class."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The high-probability bound of Clipped-SGD is derived for the first time for the considered problem. \n2. The presented bounds recover the current best result when $L_1  = 0$."}, "weaknesses": {"value": "1. The problem class and algorithm are both motivated by some deep learning in particular attention models, but the theoretical results are only presented for convex functions. It would be great if non-convex case can be studied. \n2. The considered problem class (for more general nonconvex functions) has been studied in [1] and optimal in-expectation rate using normalized SGD has been derived. It would be helpful if this work can be compared with. \n3. No numerical experiments presented. \n\n\n[1] Liu, Zijian, and Zhengyuan Zhou. \"Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping.\" The Thirteenth International Conference on Learning Representations."}, "questions": {"value": "1. Given that existing works have studied high-probability convergence for clipped-SGD for convex smooth functions, it would be helpful if the challenges in dealing with additional $(L_0, L_1)$ smoothness can be highlighted."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "u9DRt5GacG", "forum": "PHWcPmkU9I", "replyto": "PHWcPmkU9I", "signatures": ["ICLR.cc/2026/Conference/Submission19855/Reviewer_QWn5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19855/Reviewer_QWn5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762855687323, "cdate": 1762855687323, "tmdate": 1762932025819, "mdate": 1762932025819, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}