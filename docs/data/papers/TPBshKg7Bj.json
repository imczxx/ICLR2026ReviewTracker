{"id": "TPBshKg7Bj", "number": 22793, "cdate": 1758335488916, "mdate": 1759896845878, "content": {"title": "AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems", "abstract": "Agroecosystem, which heavily involves human interactions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately don't exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM, EA-LSTM, TCN, Transformer, iTransformer, and Pyraformer. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.", "tldr": "", "keywords": ["Datasets and Benchmarks", "AI for Social Impact", "Agriculture and Food", "Environmental Sustainability", "Natural Sciences"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/98a14422e118e130d2995c8a826aa44862f9c186.pdf", "supplementary_material": "/attachment/c028634eb89d182f9f661ccab25ec51f3ea06e33.zip"}, "replies": [{"content": {"summary": {"value": "This paper presents AgroFlux, a proposed benchmark dataset and evaluation framework for agricultural carbon and nitrogen flux prediction. AgroFlux integrates simulated data generated from two process-based models (Ecosys and DayCent) with limited observational data from flux towers and controlled-environment experiments. The benchmark defines standardized tasks (temporal extrapolation, spatial extrapolation, and transfer learning), metrics (R², RMSE, MAE), and protocols for evaluating deep learning models, including LSTM, TCN, Transformer, Pyraformer, and iTransformer. The authors report extensive baseline results and suggest that AgroFlux can serve as a unifying testbed for AI-driven modeling of greenhouse gas fluxes in agroecosystems"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles an important environmental and agricultural challenge: modeling carbon and nitrogen fluxes to support climate-change mitigation. The introduction provides solid context linking agriculture, greenhouse gas emissions, and the need for AI-ready benchmarks.\n\n2. The authors define consistent data splits, metrics, and tasks for both simulated and observational datasets, facilitating fair model comparison.\n\n3. Baseline results for six state-of-the-art time-series models (LSTM, TCN, Transformer variants) are carefully reported, providing useful empirical baselines for the community."}, "weaknesses": {"value": "1. The major limitation is that AgroFlux primarily relies on synthetic simulations from process-based models (Ecosys and DayCent). Although the paper claims to integrate observational data, the latter is extremely limited in both spatial and temporal coverage (e.g., 11 flux-tower sites and one small-scale N₂O experiment).\n\n2. The contribution of this work is primarily dataset engineering, and the idea of combining PBM simulations with flux-tower observations has been explored before (e.g., FLUXCOM, KGML-Ag). Thus, the work provides incremental improvements rather than a conceptual leap.\n\n3. Because both the training and testing phases rely heavily on data derived from the same simulation pipelines, the evaluation risks validating models on artificial distributions, which may not transfer to real-world agroecosystem variability.\n\n4. The claim that AgroFlux is the “first benchmark for agricultural flux prediction” overlooks existing efforts such as FLUXNET2015, GHG-Europe, and X-BASE/FLUXCOM, which already provide large-scale flux data. The novelty here lies mainly in the combination with simulations, not in benchmarking per se."}, "questions": {"value": "1. Given that most of AgroFlux consists of model-simulated data, how do you ensure that the benchmark faithfully represents real-world agroecosystem dynamics rather than the internal biases of Ecosys and DayCent?\n\n2. Can you justify why evaluating ML models primarily on simulated data provides meaningful insights for real-world deployment? \n\n3. Have you assessed domain shift between simulated and observed distributions (e.g., feature statistics or covariate shift metrics)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c1rna1ZT0w", "forum": "TPBshKg7Bj", "replyto": "TPBshKg7Bj", "signatures": ["ICLR.cc/2026/Conference/Submission22793/Reviewer_5EHZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22793/Reviewer_5EHZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894977638, "cdate": 1761894977638, "tmdate": 1762942389992, "mdate": 1762942389992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces a dataset for carbon dioxide and nitrous oxide surface fluxes of soy- and maize agricultural lands in the midwest region of the US. The task is approached in a two-fold way: first by providing simulations from two process-based biogeochemical models over a gradient of environmental conditions, and then second, by assembling eddy covariance CO2 flux data from 11 sites alongside N2O flux data from six chamber experiments and colocating it with environmental covariates such as weather and soil conditions. The study then proceeds by training a range of sequence-based deep learning models such as LSTM-variants or Transformer-variants on the datasets and evaluating them in temporal and in spatial cross-validation for the simulated and the observed datasets. Finally, experiments are presented were models are first pretrained on the simulation data and then fine-tuned on the observations, which provides performance improvements in some cases."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The monitoring of greenhouse gas surface fluxes is highly relevant for verification of international climate agreements, and at the same time quite challenging due to gaps in our understanding of ecosystem processes and limited observations.\n2. This work allows for a multi-task (NEE, GPP & N2O flux) training of models, with potential benefits across tasks.\n3. The presented process-based model simulations allow for synthetic experiments to better illucidate the extrapolation capabilities of current models due to the limited observational network\n4. The combination of simulation data and observations allows to study transfer learning from process-based models. This study presents first results that highlight potential benefits of such an approach."}, "weaknesses": {"value": "Major comments:\n\n1. A lot of the data has already been introduced in Liu et al 2024. What exactly is the contribution of this paper?\n2. Insufficient baselines. FluxCom X-Base should be included, as should be an XGBoost trained on your dataset and the two process-based models Ecosys and Daycent.\n3. Mediocre performance on the synthetic dataset. Why do you achieve only mediocre performance on a mere emulation task? Are your models too small or not properly tuned? Are you extrapolating in feature space?\n4. You argue existing flux upscaling is not suitable partly because not enough Agri sites (L.97f.) - but then only use 11 agri sites.\n5. This is also an argument itself: 11 sites is very few (!)\n6. Some of the sites are very close-by. Thus you might overestimate spatial extrapolation skill. I suggest a spatially-blocked CV, as Fluxcom X-Base does.\n7. Please cite Flux Data (and all other used datasets) correctly. For an example, see the license and citation for US-Bo1 here: https://ameriflux.lbl.gov/sites/siteinfo/US-Bo1\n8. There should not be that great differences between the different models (especially for the emulation task in Table 1). How did you do the hyperparameter tuning?\n9. Especially for N2O, where you are in a way more data-limited setting, it would be important to include simpler baselines\n10. The title is too general, your benchmark is only valid for Corn & Soybean agricultural systems in 3 states of the US.\n11. One important experiment lacks: You could use your simulated data for a synthetic observation system simulation experiment (OSSE), i.e. trying to assemble a training dataset whose distribution reflects the sparse distribution of observation stations, and to then test model extrapolation in such a synthetic set-up.\n12. Experiment A.4 is potentially unfair: your task-specific models potentially have access to 4x the parameters.\n13. It would be useful to extend A.4 also to the observational datasets, as there might be more benefits there from multi-task learning\n14. Why does pretraining on Ecosys data (esp. Table 3, but also table 4) lead to much worse results than just training directly on observations (Table 2)? Also: to make this analysis easier. Best would be to add a \"gain\" column in Table 3 & 4 which shows the improvement over training from scratch.\n\nWriting needs to be greatly improved:\n\n15. Limitations is way to short and misses key aspects, for example problems due to missing covariates, limited transferability to arbitrary agricultural fields, … Especially N2O is a chamber measurement from the lab, and thus there is a gap between this data and real ecosystem response\n16. It is not explained exactly which predictors are used for the observational data. And it is a bit fuzzy, which are used for the simulated data. Can you comment on the quality of spatial maps of these parameters in case you want to apply your model afterwards to produce maps?\n17. The compared methods are not explained. It would be important to mention why these methods where chosen, and how they differ. To then draw potential conclusion on why a certain method outperforms over another.\n18. Vast literature on flux upscaling using ML models not mentioned in introduction, and only 2 papers cited in related works. Also there are many works using sequence models. Cite them !\n19. The abstract has poor language quality. First sentence has a missing word \"are\", and i believe you can't use \"agroecosystem\" without an article in this case. Disentangle the existing approaches and their respective challenges in line 18ff. Line 10 seems colloqiual, what is a “AI-ready benchmark dataset” or an “AI-empowered model”. Use scientific terms. Also be accurate, there are already benchmarks for evaluating data-driven flux models (e.g. you are citing Fluxcom-X).\n20. Beyond this, language could also be improved throughout the rest of the manuscript.\n\nMinor comments:\n\n21. Please call it NEE and not CO_2 .\n22. Specify which method is used to partition GPP (daytime or nighttime) in L.229.\n24. I would prefer if R^2 colorbar would go from 0 to 1, and RMSE & MAE colorbars would start at 0 (e.g. in Fig. 3).\n25. Add the performance gain for the pretrain-finetune strategies (e.g. as a color-code). So it becomes more clear the benefit\n26. Add spread of your metrics across CV folds, i.e. uncertainty\n27. In Fig.2 it looks like your models only learn to predict the mean seasonal cycle, and not much beyond. It could be interesting to, akin to X-Base, also explicitly disentengle this by reporting metrics for the MSC, the anomalies from MSC and the IAV next to the raw scores.\n28. Fig. 1 the legend are overlapping the plotted data."}, "questions": {"value": "See the Weaknesses above."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The used observational data has not been cited appropriately (as detailed in the data license - see https://ameriflux.lbl.gov/sites/siteinfo/US-Bo1#data-citation )."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "JJnH40ap4n", "forum": "TPBshKg7Bj", "replyto": "TPBshKg7Bj", "signatures": ["ICLR.cc/2026/Conference/Submission22793/Reviewer_HE94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22793/Reviewer_HE94"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907641135, "cdate": 1761907641135, "tmdate": 1762942389722, "mdate": 1762942389722, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "AgroFlux is a spatiotemporal benchmark suite designed to enhance machine learning–based prediction of agricultural greenhouse gas (GHG) fluxes, particularly carbon dioxide (CO₂) and nitrous oxide (N₂O). Recognizing the computational limits and biases of traditional process-based models (PBMs) like Ecosys and DayCent, AgroFlux integrates simulation outputs with real-world observational data from flux towers and controlled environments to provide standardized, reproducible datasets. It defines multiple prediction tasks, temporal and spatial extrapolation, simulation-to-observation transfer, and cross-domain learning, and evaluates several baseline and transfer learning models to establish reference performance. AgroFlux offers a consistent foundation and leaderboard for advancing robust, generalizable GHG flux prediction in agricultural systems."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Comprehensive integration of PBM simulation data and real-world observational datasets at daily granularity.\n2. Covers a wide range of environmental and management variables across different sites and conditions.\n3. Standardized prediction tasks and consistent evaluation metrics (R2, RMSE, MAE) enable fair, reproducible assessment.\n4. Includes transfer learning benchmarks, pushing forward domain adaptation research.\n5. Provides baseline performances on state-of-the-art sequential deep learning models (LSTM variants, TCN, Transformers).\n6. Supports development of accurate, scalable AI-driven models to better understand and mitigate agroecosystem climate impacts."}, "weaknesses": {"value": "1. The dataset may still be limited to certain regions or crop types, possibly restricting generalizability.\n2. Complexity in data integration from multiple sources may pose application challenges.\n3. Machine learning models’ performance could be sensitive to the high spatio-temporal variability of agricultural fluxes.\n4. PBMs themselves have inherent biases that might propagate into benchmarks.\n5. Only a data-driven approach might not be sufficient to capture vastly complex agricultural fluxes.\n6. The benchmark did not incorporate all regression-based models, such as State Space models.\n7. The paper does not benchmark the computational costs of the considered models."}, "questions": {"value": "1. How do discrepancies between simulated (PBM) data and observational data affect benchmark robustness? Are there plans to mitigate PBM structural biases in the dataset?\n2. How can the N2O flux prediction be benchmarked with higher quality? Is there a plan/algorithm/model for such a task?\n3. Are there any results for State Space models?\n4. Although N2O flux prediction performs very well based on MAE and RMSE, it performs lower than CO2 and GPP. The paper mentions that this is due to management practices. Additional results or evidence in support of this statement can help assess the paper.\n5. Benchmarks of models on the Flux prediction datasets provide insights into models only based on the considered metrics, but not on the computational complexity and expense of the models. Presentations of such results help to assess the paper comprehensively."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rJBgNinX00", "forum": "TPBshKg7Bj", "replyto": "TPBshKg7Bj", "signatures": ["ICLR.cc/2026/Conference/Submission22793/Reviewer_XjEB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22793/Reviewer_XjEB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761946062882, "cdate": 1761946062882, "tmdate": 1762942389420, "mdate": 1762942389420, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a spatio-temporal benchmark for the prediction of geophysical variables. It evaluates several architectures (LSTMs, 1D-CNNs, Transformer-based models) across multiple tasks.\n\nWhile this is valuable work for the biophysical / geoscience community, the impact for the machine learning community feels limited. The empirical results are somewhat inconclusive — depending on the task, different architectures “win”, with no clear take-home message on what matters model- or data-wise. In addition, the study would benefit from including a domain-specific baseline (e.g. a random forest on hand-crafted features), to better contextualize how domain experts approach these problems today and to quantify the real ML lift.\n\nOverall, I do not see a strong fit for ICLR. However, I do see this as a good paper for a domain journal or a domain-specific workshop, where the target audience would likely appreciate its contributions more directly."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "* Extensive empirical comparison of multiple deep learning architectures (LSTM, 1D-CNN, Transformer) across a diverse set of biogeophysical prediction tasks.\n* The paper reflects substantial domain knowledge from the biogeosciences, both in how variables are selected/aggregated and how the tasks are framed."}, "weaknesses": {"value": "* Limited relevance for the broader ML community outside biogeosciences. The paper does not clearly articulate which underlying modeling patterns or dataset properties are generalizable beyond this specific domain, or why this benchmark represents a unique opportunity for ML research at large.\n* Results are largely inconclusive and there is no methodological innovation. The baseline comparisons do not yield a clear insight or takeaway that advances our understanding of model behavior or design principles."}, "questions": {"value": "In terms of topical alignment:\n* What underlying patterns are visible in the results that a ML- researcher from a different application domain could utilize for their research? \n* What are the unique patterns in the data of this benchmark that makes this benchmark a unique opportunity for the ML community as a whole?\n\nTechnical Question\n* How relieable are the simulated data for a benchmark? As the simulation follows presumably a mathematical model for the underlying process and a noise model, I see a clear threat of deep learning models approximating the underlying functions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TafRj6voaX", "forum": "TPBshKg7Bj", "replyto": "TPBshKg7Bj", "signatures": ["ICLR.cc/2026/Conference/Submission22793/Reviewer_sfFj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22793/Reviewer_sfFj"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22793/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762019072023, "cdate": 1762019072023, "tmdate": 1762942389219, "mdate": 1762942389219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}