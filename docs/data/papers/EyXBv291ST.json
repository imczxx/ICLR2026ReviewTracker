{"id": "EyXBv291ST", "number": 18548, "cdate": 1758289001613, "mdate": 1759897096494, "content": {"title": "Effective Unlearning in LLMs Relies on the Right Data Retention Strategy", "abstract": "Unlearning in Large Language Models (LLMs) has gained increasing attention in recent years due to its critical role in ensuring ethical and legal compliance. Although significant progress has been made in developing unlearning algorithms, relatively little attention has been devoted to the data perspective. In particular, the role of retain-set selection in preserving model utility remains underexplored, even though it is critical for making unlearning practical in real-world applications. In this work, we explore strategies for constructing effective retain sets by adapting methods from coreset selection and prior unlearning research. We evaluate these approaches on two complementary datasets: (i) a monotonic dataset built from a benchmark dataset, and (ii) a mixed, larger-scale dataset combining WPU, TOFU, and Dolly, which better reflects realistic scenarios where forget and retain samples are not explicitly defined. We find that both model utility and forget quality are strongly influenced by the variance of the model’s representations within the selected retain set. Moreover, we show that simply choosing data samples with high semantic or syntactic similarity to the forget set can yield substantially better results than standard coreset techniques. To the best of our knowledge, this work represents the first systematic study of retain-set selection for LLM unlearning, highlighting both its importance and the challenges it poses in practical settings.", "tldr": "We focus on Retain set selection for LLM Unlearning in realistic scenarios. Additionally, we investigate how this selection is impacting forget quality and model utility through hidden state representations.", "keywords": ["LLM Unlearning", "data perspective", "retain set selection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/aec85fd226b03353f5cfe5925e50e9367d7aae66.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on retention data construction for the large language models (LLMs) unlearning problem. Specifically, it claims that the role of retain-set selection in preserving model utility remains underexplored, and this work explores the strategies to construct effective retain sets for unlearning, e.g., a monotonic dataset built from a benchmark dataset, and a mixed, larger-scale dataset combining different unlearning benchmarks, that systematically analyze the influence of retain-set selection for LLM unlearning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This work explores an interesting problem in retain-set data selection/construction for LLMs unlearning.\n2. This work presents several useful experimental results for the practical guidance of retain-set data selection and construction.\n3. This research work also provides a reproducible code repo for enhancing future work on related problem settings."}, "weaknesses": {"value": "1. The current presentation has limited discussion on those unlearning works assuming no access to the retaining data (both on conventional machine unlearning scenarios and LLM unlearning settings), which should be considered and carefully discussed to justify the significance and rationality of the current motivation.\n2. In the evaluation, model utility is a comprehensive evaluation metric to reflect whether the model retains the general capabilities, which can be used as an index, but not a fully reliable metric to support the claims, as proposed in TOFU. Regarding that most of the claims are based on the emprical evidence of this metric, the authors should consider one more metrics that defined by diverse benchmarks to verify the empirical conclusions.\n3. For the technical part, I can hardly get the core new contribution regarding the research problem, as the work mainly adapt former existing methods from coreset selection and prior unlearning research to conduct the systematic exploration.\n4. The research setting maybe problematic and unrealistic, as it is questionable when the developer of the model should conduct retention set selection instead of using full retention data.\n5. The experiment is not comprehensive, which doesn't consider diverse benchmarks and LLMs, and there is limited insights draw from the empirical findings. The presentation could be enhanced by considering involving more aspect of systematic exploration."}, "questions": {"value": "1. Please also discuss more unlearning works that do not use the retention data. It is important to give a comprehensive overview of the research in the related area.\n2. Please consider the weakness part for further revision and enhance the presentation and experimental parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5niud16jML", "forum": "EyXBv291ST", "replyto": "EyXBv291ST", "signatures": ["ICLR.cc/2026/Conference/Submission18548/Reviewer_fMtL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18548/Reviewer_fMtL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761777723402, "cdate": 1761777723402, "tmdate": 1762928252594, "mdate": 1762928252594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles an important but underexplored problem in LLM unlearning: how to select an effective retain set that preserves model utility while achieving good forgetting quality. The authors evaluate several data selection strategies including coreset methods (MODERATE, GRAND), semantic similarity, and syntactic similarity on two datasets: a monotonic WPU dataset and a more realistic mixed dataset. Their key findings are: (1) hidden state variance (HSV) of the retain set correlates with both utility drop and forget quality, suggesting a fundamental trade-off; (2) simple semantic/syntactic similarity-based selection surprisingly outperforms sophisticated coreset methods; (3) effective methods tend to over-sample clusters containing forget samples rather than pursuing diversity."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Important and practical problem: Retain set selection is a critical bottleneck for real-world deployment that hasn't received sufficient attention. The mixed dataset setup is much more realistic than typical benchmarks.\n\nValuable empirical findings: The discovery that semantic similarity-based selection achieves comparable or better results than using the full retain set with only 20% of data (Table 3: 12% RUD vs 26.67% full retain on Mix) is practically significant. The computational efficiency advantage (30s vs 3000s for GRAND) makes it immediately applicable.\n\nInsightful analysis: The HSV-based analysis provides intuition about why different selection strategies work, and the cluster-level preference analysis (Figure 3) explains the mechanism behind semantic/syntactic methods' effectiveness."}, "weaknesses": {"value": "Limited experimental scope undermines generalizability: Testing only Gradient Difference on a single 8B model is a significant limitation. Will these findings hold for other unlearning algorithms (NPO, RMU) or different model scales? The authors claim this is the \"first systematic study,\" but one algorithm barely qualifies as systematic. The failure to get NPO working (footnote 1) raises concerns about whether the findings are algorithm-specific.\n\nUnfair method comparison creates validity concerns: Semantic and syntactic methods explicitly select samples closest to the forget set, while coreset methods operate blindly on the retain distribution. This isn't a fair comparison - you're essentially asking \"does targeted patching work better than blind diversity?\" The answer is obvious. A fairer setup would be: given that we'll receive future unlearning requests for entity B, should we select the retain set based on entity A's characteristics or use coreset methods? This generalization question is never addressed.\n\nContradictory and unexplained results: Why does HSV strongly correlate with RUD on Mix (r=0.70, p=0.01) but show no correlation on WPU (Figure 2)? This is a central claim of the paper but the contradiction is glossed over. Also, Table 5 shows semantic_20 achieving MU=0.66 on Mix while full retain only gets 0.65 - this seems impossible and suggests potential experimental issues or cherry-picking of epochs."}, "questions": {"value": "Generalization to new unlearning requests: Your semantic/syntactic methods select samples similar to the current forget set. If I use this retain set but later need to unlearn a different entity, will it still work? Can you provide experiments where you select Dr using entity A but then unlearn entity B? This addresses whether you're finding a general principle or just overfitting to specific forget samples.\n\nWhy does GRAND fail so badly with cross-entropy?: Table 2 shows GRAND(CE) getting 82.67% RUD on Mix at 5%, which is worse than random. Since GRAND is a well-established method, this suggests something might be wrong with the implementation or setup. Can you provide more diagnostic analysis? Did you verify the gradient norms are being computed correctly?\n\nWhat's driving the WPU vs Mix difference in HSV correlations?: This is crucial for understanding when your findings apply. Is it about dataset homogeneity, sample length, or something else? The paper's main claim about HSV's role is based on Mix results, but WPU contradicts it. Which setting should practitioners trust?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "G2vqSf1jA4", "forum": "EyXBv291ST", "replyto": "EyXBv291ST", "signatures": ["ICLR.cc/2026/Conference/Submission18548/Reviewer_yEhr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18548/Reviewer_yEhr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761795891248, "cdate": 1761795891248, "tmdate": 1762928252049, "mdate": 1762928252049, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical gap in Large Language Model (LLM) unlearning: the selection of a compact yet effective retain set to preserve model utility (MU) while ensuring high forget quality (FQ) for undesired knowledge. Prior work in LLM unlearning has focused primarily on algorithm design (e.g., gradient ascent, preference optimization) rather than data-centric retain-set construction—especially critical for real-world scenarios where full Dr (dataset minus forget set Df) is computationally prohibitive. The authors adapt two coreset methods (GRAND, MODERATE) for unlearning and propose two simpler strategies: selecting samples with semantic or syntactic similarity to Df. Tests on two datasets—(1) monotonic Wikipedia Person Unlearn (WPU, entity-level unlearning) and (2) mixed (WPU + TOFU + Dolly, realistic heterogeneity)—using LLaMA 3.1 8B (LoRA fine-tuning) and Gradient Difference (GD) unlearning. Metrics include FQ (1 - mean of ROUGE-L, conditional probability, truth ratio on Df), MU (harmonic mean of ROUGE-L, conditional probability, cosine similarity on Dr), and Relative Utility Drop."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper fills a understudied niche in LLM unlearning by centering data retention rather than algorithm design. While prior work touches on coreset selection for unlearning, this is the first systematic study to: \n\nCompare coreset methods with similarity-based (semantic/syntactic) retain-set strategies.\n\nDemonstrate that targeted similarity-based selection (not just distributional diversity) is more effective for real-world unlearning—addressing a key limitation of coreset methods optimized for diversity.\n\nThe experimental design is rigorous:\n\nComprehensive Metrics: FQ and MU are computed using multiple complementary metrics (ROUGE-L, conditional probability, cosine similarity), avoiding overreliance on a single measure. RUD further quantifies utility preservation, a critical practical concern.\n\nComputational Cost Analysis: The authors explicitly compare the time overhead of selection methods (e.g., semantic selection takes 30s on mixed data vs. GRAND’s high cost), which is rarely done but essential for practical adoption.\n\nReproducibility: Code, config files, and detailed appendices (dataset splits, hyperparameters, metric definitions) are provided, enabling validation of results."}, "weaknesses": {"value": "Limited scope of evaluation: The experiments use only one unlearning algorithm (Gradient Difference) and one base model (LLaMA 3.1 8B).\nBroader validation (e.g., with preference-based or representation-level unlearning methods such as NPO, RMU, or WAGLE) would strengthen generality.\n\nDataset diversity: The datasets, while mixed, are still synthetic or benchmark-oriented (WPU, TOFU, Dolly). Testing on naturally entangled corpora or privacy-sensitive real-world data (e.g., names, medical facts) would better validate the claim of “realistic scenarios.”\n\nCausality: The paper identifies correlations between HSV and utility/forget quality but stops short of establishing causal mechanisms. Controlled experiments manipulating representation variance could test whether HSV directly influences forgetting outcomes.\n\nAlgorithmic baselines: The coreset implementations (GRAND, MODERATE) are borrowed from vision tasks without full adaptation to text or transformer dynamics. A comparison against language-specific data selection methods (e.g., LESS, Instag, DEITA) would be fairer."}, "questions": {"value": "Generalization of retain strategies:  How does the proposed retain-set selection perform when using other unlearning frameworks (e.g., DPO-based or model-editing methods)? Do the HSV trends persist across paradigms?\n\nScaling behavior: The paper mentions that 20% retain data can preserve utility, but this may still be huge for large corpora. Can you provide scaling experiments or a heuristic for selecting the minimal Ds?\n\nReproducibility and openness: The paper mentions anonymized code. It would help to confirm whether datasets and hyperparameters (e.g., LoRA ranks, λ settings) are fully reproducible for reviewers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9kZin7Tr01", "forum": "EyXBv291ST", "replyto": "EyXBv291ST", "signatures": ["ICLR.cc/2026/Conference/Submission18548/Reviewer_dkf8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18548/Reviewer_dkf8"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924072332, "cdate": 1761924072332, "tmdate": 1762928249436, "mdate": 1762928249436, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates retain-set selection for LLM unlearning, emphasizing its role in balancing FQ and MU. It adapts coreset methods like GRAND and MODERATE from classification tasks to unlearning, using last-token penultimate-layer hidden states for clustering and selection. Additionally, it proposes simple baselines: selecting top-n samples semantically or syntactically similar to forget samples via SBERT embeddings or normalized Levenshtein distance on POS tags. Experiments on WPU (monotonic) and a mixed dataset (WPU+TOFU+Dolly) show that HSV in retain sets correlates negatively with MU but positively with FQ in mixed settings. Semantic similarity outperforms coresets, achieving near full-retain MU with 20% data. Cluster analysis reveals top methods oversample forget-heavy clusters, explaining efficacy over diversity-focused coresets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper stands out for its rigorous empirical foundation, conducting experiments across multiple retain-set sizes (5-20%) and datasets, ensuring results are robust and directly comparable, which elevates its contribution to practical unlearning research.\n- The evaluation is comprehensive, using a stack of metrics (ROUGE-L, probability, cosine similarity, truth ratio) aggregated into FQ/MU, and including per-source breakdowns in mixed datasets."}, "weaknesses": {"value": "- The experiments rely solely on Gradient Difference unlearning with LoRA on LLaMA 3.1 8B, limiting generalizability since results might differ with other methods like NPO or RMU, which the authors tried but couldn't tune effectively.\n- In practical applications, the more difficult problem in LLM unlearning is maintaining MU, while the issue of overly large retain sets is not common, thus limiting the real-world value of retain-set screening for efficiency gains."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kIvOHcfsMZ", "forum": "EyXBv291ST", "replyto": "EyXBv291ST", "signatures": ["ICLR.cc/2026/Conference/Submission18548/Reviewer_BxqL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18548/Reviewer_BxqL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18548/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985041616, "cdate": 1761985041616, "tmdate": 1762928248573, "mdate": 1762928248573, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}