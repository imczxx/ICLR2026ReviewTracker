{"id": "BsEYEXjkVO", "number": 1862, "cdate": 1756955488483, "mdate": 1759898181876, "content": {"title": "From Shortcuts to Reasoning: Robust Post-Training of Theory of Mind with Reinforcement Learning", "abstract": "Theory of Mind (ToM) is a must-acquire skill for modern foundation model systems to operate effectively and safely in the real world. Recent works have explored honing ToM via post-training; however, we show that such progress is confounded by a pervasive “shortcut” issue: tasks can reach up to 99% accuracy by simply exploiting spurious causal correlations, leading to a false sense of ToM. Motivated by this, we first develop a framework to systematically examine ToM datasets for shortcuts and provide guidance for future development. We find that questions reducible to pure state tracking (e.g., “belief”) are especially shortcut-prone compared to mind questions (e.g., “intention”) where reasoning beyond tracking is required. Using four shortcut-free datasets across three ToM contexts, we then comprehensively study whether reinforcement-learning fine-tuning with verifiable rewards and explicit reasoning (Thinking-RFT) elevates ToM beyond supervised fine-tuning (SFT). Our key findings are: 1) Thinking-RFT effectively improves ToM in all scenarios (+6% vs. SFT), particularly in complex higher-order reasoning (+10% vs. SFT) and multimodal cases (+7% vs. SFT), and generalizes notably better to unseen domains and higher-order queries while being more robust to counterfactuals. 2) ToM benefits specifically from the joint effect of reasoning and RL: Thinking-RFT outperforms No-Thinking-RFT by 7% on average. 3) RFT works by learning to ground its reasoning on anchor cues (keywords/state changes) that correspond to causal factors. We believe our study is useful for developing effective and robust ToM post-training datasets and advancing critical ToM capabilities in foundation models.", "tldr": "", "keywords": ["theory of mind", "reasoning", "reinforcement finetuning", "large language model"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b782039e3f2927002f0e7245335beea350e45be.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work investigates potential shortcuts in existing ToM datasets, and train LMs via RL on the shortcut-free datasets.\n\nThe main findings are as follows:\n\n(1) several datasets are prone to shortcut exploitation and are therefore unsuitable for training;\n\n(2) when trained on shortcut-free datasets, the performance follows the order thinking-RFT > no-thinking-RFT > SFT;\n\n(3) thinking-RFT exhibits strong generalization to unseen domains and higher-order ToM questions; and\n\n(4) RFT improves performance by learning to ground its reasoning in cues that correspond to causal factors.\n\nOverall, the paper attempts to address an interesting question, but it lacks important methodological details and sufficient evidence to support its claims, especially (1) and (4). The writing is generally clear, though there are some typos. I will outline my specific concerns below."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The writing is mostly clear and easy to follow.\n\n- A major contribution of this work lies in its attempt to identify and analyze shortcuts in existing ToM datasets. Training on such datasets may encourage models to exploit superficial correlations rather than develop genuine ToM capabilities.\n\n- The finding that RFT training outperforms SFT is not entirely surprising, since similar trends have been observed in other domains. However, this work reaches different conclusions from Lu et al. (2025) that explored similar questions.\n\n- I appreciate the use of mechanistic analyses to investigate where RFT demonstrates its advantages, although the presented evidence is not fully convincing.\n\nReference:\nLu, Y. L., Zhang, C., Song, J., Fan, L., & Wang, W. (2025). Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?. arXiv preprint arXiv:2504.01698."}, "weaknesses": {"value": "- The paper reaches a different conclusion from Lu et al. (2025) but does not discuss the differences in detail. It seems the authors suggest that Lu et al. (2025) trained on shortcut-prone datasets, which may explain their opposite findings. Please provide a more detailed comparison and discussion in the related work section to clarify the source of these discrepancies.\n\n- Some terms and methods are not clearly illustrated.\n  - In Section 2.1, what exactly is the “stratified seed set”? For example, does it include (x, y) pairs where x is a question and y is the answer? How is the set “stratified”? \n  - How are the heuristics implemented and combined? These parts should be described more clearly for reproducibility and clarity, since it's suppose to support a crucial claim of this work.\n\n- Not enough evidence to support the claims:\n\n  - The paper claims that training on shortcut datasets can harm ToM abilities, but there isn’t enough evidence to support this. It would be more convincing if the authors trained the same model on these shortcut datasets using the same method and compared the results, instead of only showing a qualitative figure with reasoning-trace errors (Figure 2).\n\n  - Using procedurally generated data is not very convincing for testing generalization (e.g., generating data from 'apartment' to unseen 'outer space' for MMToM), since such data still follow similar logic as the training data. It would be stronger to test on truly out-of-domain datasets, e.g., the shortcut-prone datasets mentioned earlier, to see if RL really improves generalizable ToM abilities.\n\n- Typos for revising the manuscript:\n  - Line 118: casual -> causal\n  - Line 152: there should be '.' before 'On'\n  - Line 155: mentioned section 2 -> mentioned in Section 2\n  - Line 159: Table 2 -> Figure 2"}, "questions": {"value": "- Regarding generalization, could you test on existing evaluation datasets instead of constructing new ones derived from the training data? This would make the generalization claim more convincing.\n\n- In Section 5.2, you mention that 'we manually mark the minimal cues in each narrative that establish the causal hinge between agents’ intentions and outcomes.' This appears to suggest a quantitative evaluation across multiple data, yet the paper only provides a single qualitative example. Please include quantitative results or clarify how the experiments were truly performed to better support the crucial claim of 'RFT improves performance by learning to ground its reasoning in cues that correspond to causal factors'."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics concerns."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "opkQj4L53R", "forum": "BsEYEXjkVO", "replyto": "BsEYEXjkVO", "signatures": ["ICLR.cc/2026/Conference/Submission1862/Reviewer_c2tS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1862/Reviewer_c2tS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761452185249, "cdate": 1761452185249, "tmdate": 1762915918273, "mdate": 1762915918273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to tackle the generalizability and counterfactual robustness of foundation models on theory-of-mind (ToM) reasoning tasks. They first design a principled filtering approach that identifies the shortcut issue in the existing ToM benchmarks, especially for the datasets like Hi-ToM higher-order queries. After identifying the shortcut-free dataset, the authors further make a comprehensive comparison among the performance of zero-shot and different post-training approaches, including SFT and RFT (with or without thinking tokens). The results demonstrate that RFT with thinking outperforms other post-training approaches in most tasks. RFT enjoys not only the best accuracy on different ToM benchmarks, but also better generalizability to higher-order reasoning cases and better causal consistency to counterfactual probing. The qualitative studies on the attention map also reveal that the model after RFT is more capable of capturing the semantics in the contexts."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is generally well-written and easy to follow. The clarity on the experiment settings is good. \n* The shortcut perspective on the current ToM benchmark is interesting and worth studying deeper for the ToM research community. \n* The empirical results on different post-training approaches, as well as the qualitative studies are clearly presented and can justify the major claim of contribution in this paper."}, "weaknesses": {"value": "* **Incoherent contribution in section 2 and 3:** It's a little bit unclear what is the necessary logical connection between section 2 and 3. Section 2 only identifies those dataset that potentially has confounded shortcuts that should be precluded from post-training. However, these datasets with shortcut are also held away from evaluation set in the final experiments with generalization. \n* **Missing baselines:** There exists quite a few baselines in solving ToM-QA tasks, such as SimToM [1], AutoToM [2]. However, the paper misses these baselines that are currently leading approaches in the MMToM benchmark. \n* **Limited analysis on the higher-order generalizability:** Since the authors only use shortcut-free dataset like OpenToM to conduct post-training and evaluation, the claim on 'generalizability to higher-order queries' does not seem to be sufficiently supported as they only care about the generalization from first-order to second-order ToM. A more comprehensive studies on third-order and fourth-order ToM (like the ones in Hi-ToM) will be interesting. \n* **Missing evaluation on cross-dataset generalizability:** It is hard to justify whether the results the authors present are coming from better overfitting to a single dataset, or indeed a better capability in general reasoning across different ToM datasets. Therefore, it is necessary to add the evaluation on different ToM datasets after the SFT/RFT, rather than a small test split which is less different in the QA domain. \n* **Limited scale of evaluation:** The OpenToM dataset, the authors only select 100 samples from each category as evaluation. They also missed the evaluation on the test split provided by the MMToM leaderboard. These limits the contribution of the proposed method. \n* **Limited scale of qualitative analysis**: The author only demonstrates one pair of qualitative comparisons on the attention map. More qualitative examples can be provided in the appendix to make the claim of causality-coherent reasoning more solid. \n* **Missing analysis on the 'spurious correlation'**: The authors mentioned their motivation comes from the observation that the model 'simply exploiting spurious correlations'. However, the analysis terminates after section 2 right after they preclude the Hi-COM and other datasets in the finetuning and evaluation dataset. It will be more reasonable if they can conduct **quantitative** analysis on how (a) the data filtering (judged by simple rules and lexical association), as well as (b) the SFT/RFT-style post-training, can help mitigate such spurious correlation exploitation, even on those benchmarks with shortcuts. \n\n> [1] Wilf, Alex, et al. \"Think twice: Perspective-taking improves large language models' theory-of-mind capabilities.\" ACL 2024.\n>\n> [2] Zhang, Zhining, et al. \"Autotom: Automated bayesian inverse planning and model discovery for open-ended theory of mind.\" *ICLR 2025 Workshop on Foundation Models in the Wild*. 2025."}, "questions": {"value": "See the weakness section for the main questions I have and the comments. I would consider re-adjust my assessment if the majority of them get resolved during the rebuttal phase."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FRsUmlmKje", "forum": "BsEYEXjkVO", "replyto": "BsEYEXjkVO", "signatures": ["ICLR.cc/2026/Conference/Submission1862/Reviewer_MqzA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1862/Reviewer_MqzA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761599349187, "cdate": 1761599349187, "tmdate": 1762915917548, "mdate": 1762915917548, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies how RL–based post-training can enhance ToM abilities in large language models. The authors identify that existing ToM datasets often contain shortcut patterns that allow models to achieve high accuracy without genuine reasoning. They propose a framework to audit such shortcuts and select four datasets for further study. Using these datasets, the paper compares Reinforcement FT, Supervised Fine-Tuning, and zero-shot baselines across several ToM tasks. The results show that RFT consistently outperforms SFT, particularly in higher-order reasoning, unseen domains, and counterfactual settings. Further analysis indicates that explicit reasoning steps and reinforcement learning jointly improve ToM robustness and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Original framing of the ToM shortcut issue.\n\n- The paper evaluates models on narrative, conversational, and multimodal ToM tasks.\n\n- By connecting ToM, with RL techniques, the paper bridges human mental reasoning and computational learning frameworks in a creative and interdisciplinary manner."}, "weaknesses": {"value": "- The introduction needs revision. It's hard to quickly grasp what challenge the paper aims to address from the current content.\nSince Table 2 is mentioned early, it might help to first provide clear examples in the introduction so readers can easily understand the problem.\n\n- The authors should double-check the references to figures and tables to ensure that all mentions correctly correspond to the intended visual content.\n\n- It seems the authors did not re-implement or compare with previous ToM training methods; their comparison is limited to RFT, SFT, and zero-shot.\nFor a more complete experimental design, prior ToM methods should also be retrained on the authors' cleaned datasets and compared directly with RFT.\n\n- Line 422–423: The explanation is unclear.\nThe authors should describe in more detail how to interpret Figure 5, and explicitly explain what the example is doing.\n\n- The claim that explicit reasoning improves performance is not very novel, many NLP works have shown this.\nWhat exactly is its effect in this specific task?\nThe authors should further analyze the reasoning content itself, both good and bad cases."}, "questions": {"value": "- Line 62–63: \"More importantly, the trained model produces incoherent and illogical reasoning traces as shown in Table 2.\"\nHow can this be observed from Table 2?\n\n- Line 159–164: \"As shown in Table 2, RFT mixes 'Jack's' own observation with the 4th order query being asked, ignoring intermediate ToM.\"\nHow can this be seen from Table 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mN1UJrIUNW", "forum": "BsEYEXjkVO", "replyto": "BsEYEXjkVO", "signatures": ["ICLR.cc/2026/Conference/Submission1862/Reviewer_8As9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1862/Reviewer_8As9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843316237, "cdate": 1761843316237, "tmdate": 1762915917109, "mdate": 1762915917109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "“From Shortcuts to Reasoning” examines how reinforcement fine-tuning (RFT) can enhance Theory of Mind (ToM) reasoning in language models. The authors first audit existing ToM datasets and reveal that many contain shortcuts—spurious cues that let models perform well without genuine reasoning. To address this, they curate shortcut-free datasets and compare RFT with supervised fine-tuning (SFT). Experimental results show that RFT yields stronger performance, particularly on second-order, multimodal, and counterfactual reasoning tasks. The paper concludes that reinforcement learning promotes more robust, causally grounded reasoning—but only when the data itself demands true inference rather than pattern matching."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper provides a thoughtful and systematic critique of current Theory of Mind  benchmarks. It convincingly shows that many datasets contain *shortcuts* — spurious correlations or superficial cues that allow models to achieve high scores without genuine reasoning. This diagnostic insight is valuable, as it highlights that performance on standard ToM tests often overestimates a model’s true inferential ability.\n- Through detailed experiments and analyses, the paper explores where RL-trained models perform better. It examines multiple dimensions — reasoning depth (first- vs. second-order ToM), modality (text vs. multimodal), and robustness (counterfactual consistency, generalization to unseen contexts)."}, "weaknesses": {"value": "- While the paper thoroughly shows that RL outperforms SFT, much of it confirms a result that is already broadly recognized in the field — that RL tends to yield higher task-specific performance. What’s missing is a deeper analysis of **how** RL-trained models differ qualitatively from SFT models in their reasoning traces. The attention map analysis is only an **indirect indicator**, and without a direct comparison between RL and SFT attention patterns, it’s difficult to understand whether RL’s gains reflect genuinely improved reasoning or simply better optimization.\n- A key claim of the paper is that RFT improves reasoning only on shortcut-free datasets, while shortcut-prone ones hinder learning. However, this is **not empirically demonstrated**—the authors audit shortcut datasets but never show RFT results on them. As a result, the claim remains **conceptually convincing but experimentally unverified**; direct comparisons would have provided stronger evidence for RFT’s limitations."}, "questions": {"value": "- Where is the result for comparing RFT including the datasets with shortcuts?\n- What does the attention map look like for SFT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bRmI6xadfa", "forum": "BsEYEXjkVO", "replyto": "BsEYEXjkVO", "signatures": ["ICLR.cc/2026/Conference/Submission1862/Reviewer_rr8F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1862/Reviewer_rr8F"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1862/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993174464, "cdate": 1761993174464, "tmdate": 1762915916739, "mdate": 1762915916739, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}