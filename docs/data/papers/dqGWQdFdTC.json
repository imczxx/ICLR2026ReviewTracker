{"id": "dqGWQdFdTC", "number": 22188, "cdate": 1758327451537, "mdate": 1759896881397, "content": {"title": "Language Model Planning from an Information Theoretic Perspective", "abstract": "The extent to which decoder-only language models (LMs) engage in planning, that is, organizing intermediate computations to support coherent long-range generation, remains an open and important question, with implications for interpretability, reliability, and principled model design. Planning involves structuring computations over long horizons, considering multiple possible continuations, and selectively reusing past information, but how effectively transformer-based LMs realize these capabilities is still unclear. We address these questions by analyzing the hidden states at the core of transformer computations, which capture intermediate results and act as carriers of information. Since these hidden representations are often redundant and burdened with fine-grained details, we develop a pipeline based on vector-quantized variational autoencoders that compresses them into compact summary codes. These codes enable measuring mutual information, allowing systematic analysis of the computational structure underlying model behavior. Using this framework, we study planning in LMs across synthetic grammar, path-finding tasks, and natural language datasets, focusing on three key aspects: (i) the planning horizon of pre-output computations, (ii) the extent to which the model considers alternative valid continuations, and (iii) the reliance of new predictions on earlier computations. By answering these questions, we advance the understanding of how planning is realized in LMs and contribute a general-purpose pipeline for probing the internal dynamics of LMs and deep learning systems. Our results reveal that the effective planning horizon is task-dependent, that models implicitly preserve information about unused correct continuations, and that predictions draw most on recent computations, though earlier blocks remain informative.", "tldr": "", "keywords": ["language model", "LLM", "deep learning", "planning", "explainability", "interpretability", "information theory"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7d61bce5de5a40106e4cb827c826ea14e872a5b.pdf", "supplementary_material": "/attachment/ddedfadeef43a1d3691769a5e22ae5d86835729e.zip"}, "replies": [{"content": {"summary": {"value": "This paper investigates whether decoder-only language models (LMs) engage in internal planning, defined along three axes: forward-looking (planning beyond the next token), branch-aware (considering multiple valid continuations), and stateful (reusing earlier computations). The authors propose an information-theoretic framework that compresses high-dimensional hidden states into discrete codes using a modified Vector-Quantized Variational Autoencoder (VQ-VAE), enabling mutual information (MI) estimation between different parts of the model’s internal computations. They apply this pipeline across synthetic (CFG, path-finding) and natural language (OpenWebText) tasks, finding that planning behavior is task-dependent. Stronger evidence of non-myopic computation is observed in structured reasoning tasks than in syntactic or natural language settings."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated. Previous studies on the internal mechanisms of LMs often rely on linear probing, which is susceptible to confounders and may lead to unreliable conclusions. The proposed method avoids these confounders, making the analysis of internal planning in LMs more reliable.\n- The use of mutual information over compressed discrete codes provides a principled and scalable approach to analyzing high-dimensional activations.\n- The experimental design covers a thoughtful range of tasks—from highly structured symbolic problems to natural language—allowing for nuanced insights into when and how planning emerges.\n- The validation experiment in Appendix A.3 reasonably demonstrates that the VQ-VAE compression preserves meaningful statistical dependencies."}, "weaknesses": {"value": "- Overall, the empirical findings are somewhat incremental. For instance, the observation that LMs plan more in path-finding tasks and exhibit myopic behavior in CFG tasks is somewhat expected. Additionally, the proposed framework does not offer a mechanistic explanation of how such planning is implemented, which limits the significance of the findings.\n- In my view, using VAE-based methods also has drawbacks compared to linear probing:\n  - Interpretability is relatively poor. Since the latent space of VAEs is inherently difficult to interpret, even if changes in MI are detected, it remains challenging to explain the underlying mechanisms behind these changes.\n  - The VQ-VAE introduces its own set of hyperparameters and design choices (e.g., codebook size, cosine penalty), and the sensitivity of the results to these choices is not thoroughly explored.\n- The reliance on normalized MI makes it difficult to assess the absolute strength of planning signals. Moreover, the paper does not convincingly demonstrate that the observed MI reflects causal planning rather than passive statistical correlation."}, "questions": {"value": "- I am somewhat confused about the authors’ definitions of planning properties: \"Stateful\" involves reusing earlier computations, but isn’t this also a manifestation of being \"forward-looking\"? Could the authors provide more intuitive examples to clarify the distinction between these two concepts?\n- Regarding the use of VAE for compressing information to obtain distributions:\n  - Could the authors briefly explain why this method is unlikely to introduce external information? Is it because the two positions being compared are compressed by two independent VAEs? Is it possible that the learned encoder and codebook inadvertently encode external information, leading to misleading conclusions?\n  - Could the same MI patterns arise in a model that does not \"plan\" in any meaningful sense?\n- Based on the observed patterns of LM planning across different tasks, could the authors propose any improvements to the LM architecture or algorithms, such as a blueprint for enhancement?\n- If earlier blocks retain \"nontrivial information,\" as claimed, why doesn’t ablating or perturbing them significantly degrade performance? Such a test could potentially strengthen the argument for statefulness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3ejWmcjlRn", "forum": "dqGWQdFdTC", "replyto": "dqGWQdFdTC", "signatures": ["ICLR.cc/2026/Conference/Submission22188/Reviewer_rbc9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22188/Reviewer_rbc9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761480598095, "cdate": 1761480598095, "tmdate": 1762942108428, "mdate": 1762942108428, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "For interpretability purposes, it is useful to be able to estimate the mutual information (MI) between two distinct blocks of activations in a transformer model. The authors propose to do this by quantizing these activations using a specially trained Vector-Quantized Variational Autoencoder (VQ-VAE), then measuring the MI between the resulting discrete codes. This novel interpretability technique is then used to measure the presence of planning in transformer models in both toy settings and natural language."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- As far as I'm aware, the use of VQ-VAE to estimate the mutual information between high-dimensional continuous activations is novel.\n- Several distinct aspects of planning are investigated in both toy settings and natural language.\n- The authors acknowledge that their absolute MI estimates may be misleading, and focus on relative comparisons.\n- The authors provide a basic sanity check to validate their MI via VQ-VAE technique (A.3)."}, "weaknesses": {"value": "1. I am not fully convinced by the authors' philosophical argument that MI via VQ-VAE is more appropriate than existing linear probe techniques. \n\t- To fix notation, say that we care about two model activations $h_1,h_2\\in\\mathbb{R}^d$. We can either train two VQ-VAE encoders $E_1,E_2$ and estimate the mutual information $I(E_1(h_1);E_2(h_2))$, or we can train a probe $\\phi\\colon\\mathbb{R}^d\\to\\mathbb{R}^d$ and measure the $\\ell_2$ loss $||\\phi(h_1)-h_2||_2^2$. \n\t- My understanding is that *both* of these, up to some constant factor, are valid lower bounds for the quantity of interest $I(h_1; h_2)$. The former is a lower bound by the data processing inequality, while the latter is by [1, Prop 1.5].\n\t- The extent to which these bounds are loose is due in the former case to the information discarded by $E_1,E_2$, and in the latter case to the inability of the probe architecture to represent the true conditional probability $P(h_2\\mid h_1)$. It's not clear to me why the former approach is better or more principled than the latter.\n\t- I'm not sure I understand what is meant by the \"confounding effect\" of learned probes mentioned in the introduction. Both the VQ-VAE approach and the probing approach require training separate auxiliary models, and the quality of the lower bound depends on the representational capacity of the auxiliary model in either case.\n\t- I would argue that in the neural network setting, *both* approaches are somewhat philosophically dubious. It is often the case that $h_2$ is a deterministic function of $h_1$ (for example, in the setting of Section 3.1, where $h_1=h_{1:T}^{1:L-1},h_2=h_{T+\\tau}^L$, this is true if sampling temperature is zero). In this case $I(h_1;h_2)=H(h_2)$, telling you nothing about the relationship between $h_1$ and $h_2$.\n2. Experimental results have no comparisons with baselines such as linear probes.\n\t- Since the theoretical/philosophical discussion is somewhat unclear, an empirical comparison would give much stronger evidence for the suitability of MI via VQ-VAE vs probes for interpretability.\n\t- As it stands, the paper applies an unproven method to novel toy settings without established baselines. Without proper baselines, it is hard to be confident that these results are correct or meaningful.\n3. MI between past and future model activations is a fairly weak information-theoretic condition, and is e.g. always present in the zero-temperature autoregressive setting. In my opinion, \"planning\" is a somewhat misleading term for this, as it implies intentionality in the model which may not really be present.\n\n [1] Xu et al. \"A theory of usable information under computational constraints\". ICLR 2020."}, "questions": {"value": "- How canonical are the three aspects of planning you study here (forward-looking, branch-aware, stateful)? Are you claiming that possessing all three properties is either necessary or sufficient for being a good planner?\n- One advantage of linear probes is that they are cheap and simple to implement. How expensive is the VQ-VAE to train? Does it scale to the kinds of LLMs that may be used in practice (billions of parameters)?\n- What temperature was used for sampling in the experiments?\n- I strongly suggest adding a comparison to linear probe baselines. I.e., wherever $I(E_1(h_1), E_2(h_2))$ is measured, instead fit a linear probe from $h_1$ to $h_2$ and report $R^2$.\n- In the introduction, I recommend fleshing out the discussion of MI vs probes and citing [1].\n- In \"training objectives: next-token vs multi-token\", I recommend citing [2], which shows that cross-token gradients induce forward planning even with a next-token objective.\n- I like the sanity check performed in A.3, but having a discrete domain with support only 0-1 OOMs larger than codebook size is maybe too artificial. Are there continuous distributions for which the theoretical MI can be computed and compared with the estimated MI via VQ-VAE? Alternatively, what if the support is exponentially larger than codebook size (which is what happens in practice, because there are exponentially many possible input sequences)?\n\n[1] Xu et al. \"A theory of usable information under computational constraints\". ICLR 2020.\n\n[2] Wu et al. \"Do language models plan ahead for future tokens?\" COLM 2024."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yjZgWmczgy", "forum": "dqGWQdFdTC", "replyto": "dqGWQdFdTC", "signatures": ["ICLR.cc/2026/Conference/Submission22188/Reviewer_cc54"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22188/Reviewer_cc54"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761519572064, "cdate": 1761519572064, "tmdate": 1762942108258, "mdate": 1762942108258, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines whether decoder-only language models exhibit planning behavior. It formalizes planning through three abilities: forward-looking horizon (how far future information is encoded), branch awareness (whether multiple plausible continuations are internally represented), and statefulness (how much earlier computations influence later predictions). Using an information-theoretic framework, the authors compress hidden states with a VQ-VAE and compute normalized mutual information between prefix and future representations to quantify these properties. Experiments across grammar, path-finding, and natural text tasks show that planning behavior increases with task complexity, while models remain mostly short-sighted and multi-token prediction provides only minor gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Please find the strengths below:\n1. The paper provides a novel perspective by examining whether decoder-only language models possess planning capabilities from an information-theoretic viewpoint, making the study a fresh and meaningful contribution to the understanding of planning in LMs.\n2. The use of VQ-VAE to discretize hidden states and compute normalized mutual information across layers offers a new theoretical approach for analyzing information flow, which could potentially be applied to other research problems.\n3. The paper presents intuitive figures and maintains a clear logical flow, helping readers grasp the key ideas and results effectively."}, "weaknesses": {"value": "Please find the weaknesses below:\n1. Intuitively, mutual information captures only statistical correlations rather than causal relationships, and the paper does not provide theoretical justification that higher MI truly reflects stronger planning or reasoning ability; this assumption may thus be overly simplified.\n2. Although the experiments cover diverse tasks, they mainly rely on small-scale models and synthetic settings, without evaluation on more recent large language models. In addition, the VQ-VAE discretization process may introduce information loss or noise, yet the paper lacks a rigorous analysis or quantification of its impact on measurement accuracy.\n3. The multi-token prediction objective yields only marginal improvements, suggesting that the proposed framework, while informative, offers limited practical gains in enhancing model performance."}, "questions": {"value": "The questions are related to the weaknesses:\n1. Can the authors provide a more rigorous theoretical analysis linking mutual information to actual planning or reasoning ability, rather than relying on intuitive correlation?\n2. Could future experiments include larger-scale and more capable reasoning models to test whether the observed phenomena generalize beyond small synthetic settings?\n3. Can the proposed framework offer more concrete guidance or insights on how to enhance models’ reasoning or planning capabilities?\n4. Could the authors further analyze how the framework applies to reasoning behaviors under Chain-of-Thought (CoT) or Tree-of-Thought (ToT) prompting settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "2E5xAdYa4B", "forum": "dqGWQdFdTC", "replyto": "dqGWQdFdTC", "signatures": ["ICLR.cc/2026/Conference/Submission22188/Reviewer_T5EQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22188/Reviewer_T5EQ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939656597, "cdate": 1761939656597, "tmdate": 1762942108094, "mdate": 1762942108094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies planning capabilities of LLMs through an information-theoretic lens. The authors develop a framework using Vector-Quantized Variational Autoencoders (VQ-VAE) to compress the hidden states of the LLM into discrete codes. This allows them to compute mutual information (MI) estimates between different \"computational blocks\"/hidden states. \nUsing this approach, they analyze three key aspects of planning: (i) the planning horizon (how far ahead models plan before generating tokens), (ii) branch awareness (whether models internally represent alternative valid continuations), and (iii) computational history dependence (which earlier computations inform current predictions). \nThe framework is evaluated across synthetic grammar tasks, path-finding problems, and natural language (OpenWebText), comparing next-token prediction (NTP) versus multi-token prediction (MTP) training objectives."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- I think the idea behind the paper is quite novel: thee use of information-theoretic measures via VQ-VAE compression to study planning is a creative methodological tool\n\n- Also, the specific focus on quantifying planning through three different characteristics is exciting. Namely, via horizon, branching, and history dimensions. This provides a more structured framework for understanding LM internal computations. \n\n- The experimental analysis is comprehensive, including controlled synthetic tasks (CFG, path-finding) to natural language. \n\n- The path-finding task with disjoint correct and decoy paths is particularly well-designed for testing branch awareness. I really liked the idea. \n\n- This work addresses an important open question about whether and how LMs engage in planning-like computations. The finding that planning behavior is task-contingent has implications not only for model design, but also for training strategies. \n\n- I believe the framework itself could also be valuable for broader interpretability research, not just planning."}, "weaknesses": {"value": "While I really enjoyed this paper, I think it also has room for improvement:\n\n- The main text simply says \"we train a VQ-VAE\", but since this is a key aspect of the method, I would expect more information about it in the main text. However, crucial aspects are only in the Appendix. \n\n- Given the details in the appendix, the number of hidden states to store is huge, which suggests that the framework is non-trivial since the VQVAE is difficult to train. \n\n- The experiments were carried using only GPT-3 Small models (12M-202M parameters). Findings may not generalize to larger models where emergent capabilities differ. It also becomes more difficult to apply the framework for larger models.\n\n- The core pipeline (freeze LM -> sample block -> train VQ-VAE -> quantize -> estimate MI -> ...) is convoluted. A figure summarizing the pipeline/steps of the full framework would help a lot. Right now the reader keeps jumping between sections in the main text and appendix to figure out what is the full framework."}, "questions": {"value": "- For each experiment, do you train one VQ-VAE per dataset/task (e.g. one for CFG prefix, one for PF prefix, one for PF paths, one for OpenWebText blocks), or per model checkpoint?\n\n- Roughly how many sequences and optimization steps were needed to train your VAEs?\n\n- What are the overall conclusions for NTP versus MTP?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "AaeKKZoXcB", "forum": "dqGWQdFdTC", "replyto": "dqGWQdFdTC", "signatures": ["ICLR.cc/2026/Conference/Submission22188/Reviewer_Cb1t"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22188/Reviewer_Cb1t"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22188/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970450862, "cdate": 1761970450862, "tmdate": 1762942107875, "mdate": 1762942107875, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}