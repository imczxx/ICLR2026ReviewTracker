{"id": "c2ozZYoZFd", "number": 22905, "cdate": 1758336934346, "mdate": 1759896840610, "content": {"title": "A $\\texttt{Min-p}$ Blueprint for More Rigorous Science in Empirical Machine Learning Research", "abstract": "In light of a growing crisis of rigor in empirical machine learning research, this paper provides a blueprint for conducting more meticulous science.\nWe present a detailed case study of \"Turning Up the Heat: $\\texttt{Min-P}$ Sampling for Creative and Coherent LLM Outputs\" (Nguyen et al. 2024), a high-visibility ICLR 2025 Oral paper that introduced a new method for sampling from language models called $\\texttt{min-p}$.\nThe original work claimed that $\\texttt{min-p}$ sampling achieves superior quality and diversity over established methods.\nHowever, our comprehensive re-examination of the original paper's four main lines of evidence demonstrates that its conclusions are invalidated by its own data.\nOur re-analysis reveals that: (1) The original human evaluations omitted one-third of the collected data, applied statistical tests incorrectly, and inaccurately described qualitative feedback; a correct analysis shows $\\texttt{min-p}$ did not outperform baselines. (2) Extensive hyperparameter sweeps on NLP benchmarks show $\\texttt{min-p}$'s claimed superiority vanishes when controlling for the volume of hyperparameter tuning. (3) The LLM-as-a-Judge evaluations suffered from methodological ambiguity and appear to have reported results inconsistently, favoring $\\texttt{min-p}$. (4) Claims of widespread community adoption were found to be unsubstantiated and were retracted.\nFrom this case study, we derive a blueprint for more rigorous research. Key lessons include the critical need to compare methods fairly by controlling for hyperparameter tuning, to apply statistical tests transparently and correctly (e.g., correcting for multiple comparisons), to practice full data transparency, and to scrutinize qualitative summaries, methodological clarity, and potentially selective reporting.\nAdhering to these principles is essential for ensuring the validity of scientific claims and fostering genuine progress in the field of machine learning research.", "tldr": "Min-p sampling does not improve quality, diversity or a trade-off between the two.", "keywords": ["language models", "sampling", "samplers", "min-p", "large language models", "evaluations", "reproducibility", "peer review", "ML conferences"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ce38e07608cc261002d472ba288d6313a8ae97ce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work challenged the experimental results of the min-p method (ICLR 2025 oral), claiming that most of its conclusions are invalid upon closer examination. The paper ends with a list of general lessons for conducting more rigorous empirical ML studies."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- A comprehensive re-examination of a seemingly popular method for token sampling in LLMs\n\n- While most of the key lessons pointed by the authors are not new per se in established guidelines on reproducibility, it is nonetheless a good case study to draw more attention to the current questionable practices in ML research"}, "weaknesses": {"value": "- This work seems to be an odd fit for ICLR, as there is no new result or particularly novel insights. Perhaps the authors should consider submitting this work to the ML reproducibility challenge. \n\n- Some of the claims (eg., S2.1, S2.4, S4.3), including private communications to the original authors of min-p, are difficult to verify for the reviewers. \n\n- Since the work is mostly focused on criticizing one particular paper, it feels only just if the original authors were given a chance to respond before publication. However, I do not see how the existing ICLR protocol could accommodate this exchange. \n\n- I agree with the authors that there is a crisis of rigor in empirical ML research. This is a widespread problem in the entire community (sometimes caused by realistic constraints such as resource). However, I do not think it is a good idea to \"go after\" one particular paper or group. This work would be much more useful and impactful if it is organized by the current ill practices in empirical ML (like the lessons that the authors listed on Line 28-30), with each lesson illustrated on a **different** high profile paper. We need to distinguish criticizing a paper/group (more subjective and not interesting to the community) from criticizing a practice (more objective and interesting to everyone)."}, "questions": {"value": "Let me be clear: I applaud the authors' efforts and I agree with the listed lessons (though I can't say any of them is really new or unexpected). I just do not think ICLR is the right place for such efforts, and I prefer not to single out any paper or group when the underlying problem is widespread. \n\nThe disclaimer on Line 456 (new evidence might lead to different conclusions) makes me feel even more uneasy, if the original authors of min-p were not given a chance to respond. But I don't know how this could be accommodated."}, "flag_for_ethics_review": {"value": ["Yes, Other reasons (please specify below)"]}, "details_of_ethics_concerns": {"value": "I am not sure if this type of reproducibility study is a right fit for ICLR. Given that the publication of this work could be inflammatory to the original authors of min-p, it only feels just if the original authors were given a chance to respond. But how? \n\nAlso, some of the claims in this work are difficult for a reviewer to verify (e.g., private communications)."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7gIHZnEPoJ", "forum": "c2ozZYoZFd", "replyto": "c2ozZYoZFd", "signatures": ["ICLR.cc/2026/Conference/Submission22905/Reviewer_2vSc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22905/Reviewer_2vSc"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760841297371, "cdate": 1760841297371, "tmdate": 1762942431829, "mdate": 1762942431829, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a detailed case study of min-p, which is a high-visibility paper published in ICLR 2025. The authors have conducted a lot of rigorous experiments and found the results claimed from the min-p no longer hold. The authors thus derive a blueprint for more rigorous empirical ML research. This paper very much reminds me of one of the ML reproducibility challenge papers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- This paper critically examines the validity of empirical machine learning research through carefully designed statistical tests and rigorous experimental design. It offers valuable insights for the community and serves as a warning about the noise and inconsistency in current ML research reporting.\n\n- The paper is clearly written and presented very nicely."}, "weaknesses": {"value": "- This paper could have a greater impact if it positioned itself as a position paper outlining guidelines for standard practices in rigorous ML experimentation. At present, its conclusions are drawn from a single case study, and their external validity depends on whether Min-P is truly representative of broader systemic issues."}, "questions": {"value": "- Regarding 3.1, I get that when controlling for the hyper-parameter tuning budget, there is no distinction between min-p and other sampling methods. Did you run experiments where all the samplers are fully tuned? Does min-p have an advantage in this case?\n\n- The authors are questioning the community adoption. While the original paper may have exaggerated, min-p seems to be well accepted by the community as I can see. Is there further community evidence that does not support min-p’s claim of superiority?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "V7KKhsJ8hY", "forum": "c2ozZYoZFd", "replyto": "c2ozZYoZFd", "signatures": ["ICLR.cc/2026/Conference/Submission22905/Reviewer_buMc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22905/Reviewer_buMc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761754404533, "cdate": 1761754404533, "tmdate": 1762942431645, "mdate": 1762942431645, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-analyzes the ICLR 2025 paper _“Turning Up the Heat: Min-P Sampling for Creative and Coherent LLM Outputs”_, arguing that its main claims about min-p sampling’s superiority are unsupported. The authors identify overstatements in the original work’s human evaluations, benchmark tests, and reporting practices. Using this case study, they propose a blueprint for improving rigor and transparency in empirical ML research."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This paper conducts a detailed and transparent re-examination of a prior work, carefully identifying exaggerations and verifying claims through rigorous statistical and experimental checks.\n- The discussion section provides thoughtful and necessary guidance for improving rigor and transparency in scientific research, offering lessons that are broadly applicable across scientific disciplines."}, "weaknesses": {"value": "- Although the authors claim “From this case study, we derive a blueprint for more rigorous research,” the proposed blueprint only appears briefly in the final discussion section (less than one page). This portion is disproportionately small relative to the claimed contribution and lacks the depth or generalization needed to stand as a substantive methodological advance.\n\n- While the manuscript provides an important and well-executed re-analysis of a previous study, it primarily functions as a reproducibility commentary rather than a novel empirical or theoretical contribution. I recognize its educational and community value, but it does not clearly fit within any of the main ICLR paper categories. The authors might consider submitting it as a position paper or workshop contribution, where its insights on research rigor and transparency would be more appropriately framed."}, "questions": {"value": "Please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8TcguGSOlQ", "forum": "c2ozZYoZFd", "replyto": "c2ozZYoZFd", "signatures": ["ICLR.cc/2026/Conference/Submission22905/Reviewer_TzxU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22905/Reviewer_TzxU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22905/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975352012, "cdate": 1761975352012, "tmdate": 1762942431366, "mdate": 1762942431366, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}