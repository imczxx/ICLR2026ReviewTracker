{"id": "edbmmvFykY", "number": 13220, "cdate": 1758215232066, "mdate": 1759897455521, "content": {"title": "Error Propagation in Dynamic Programming: From Stochastic Control to Option Pricing", "abstract": "This paper investigates theoretical and methodological foundations for stochastic optimal control (SOC) in discrete time. We start formulating the control problem in a general dynamic programming framework, introducing the mathematical structure needed for a detailed convergence analysis. The associate value function is estimated through a sequence of approximations combining nonparametric regression methods and Monte Carlo subsampling. The regression step is performed within reproducing kernel Hilbert spaces (RKHSs), exploiting the classical KRR algorithm, while Monte Carlo sampling methods are introduced to estimate the continuation value. To assess the accuracy of our value function estimator, we propose a natural error decomposition and rigorously control the resulting error terms at each time step. We then analyze how this error propagates backward in time-from maturity to the initial stage-a relatively underexplored aspect of the SOC literature. Finally, we illustrate how our analysis naturally applies to a key financial application: the pricing of American options.", "tldr": "We analyze how errors propagate in kernel-based dynamic programming for stochastic control, with American option pricing as a case study", "keywords": ["dynamic programming", "stochastic optimal control", "kernel methods", "statistical learning theory", "american options pricing"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a160f06a1b8075e762a46fd003162dfcbf5512c1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper develops a framework to analyze error propagation in discrete-time stochastic optimal control solved via a specific form of approximate dynamic programming. For this, the authors combine Monte Carlo estimation and kernel ridge regression within reproducing kernel Hilbert spaces to approximate value functions backward in time.\nA detailed error decomposition, into regression, sampling, and propagation errors, leads to non-asymptotic convergence bounds (Theorem 1). The approach is applied to American option pricing, where the proposed algorithm achieves competitive accuracy."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is well written and the problem is clearly presented. The structure is easy to follow, starting from the theoretical setup, over the algorithmic formulation, to the numerical illustration. A particular strength is the detailed and rigorous error decomposition, which separates regression, Monte Carlo, and propagation errors, and analyzes each of them carefully. The corresponding theoretical guarantees are well motivated and technically solid. The application to American option pricing is also well described and interesting, showing how the proposed framework can be applied to a practically relevant problem and how the theoretical insights translate into a concrete numerical algorithm."}, "weaknesses": {"value": "While the numerical experiment demonstrates that the runtime of the proposed method is superior to the benchmark algorithms, it would have been valuable to numerically verify some of the theoretical claims, such as the predicted convergence rates or the scaling of the individual error components. This would help assess how tight or meaningful the theoretical bounds are in practice. Moreover, it remains unclear how informative the bound in Theorem 1 actually is for problems with large horizons, since the recursive error term grows with the time horizon $T$. This raises questions about the practical usefulness of the theoretical result for long- or infinite-horizon control problems."}, "questions": {"value": "Questions:\n\n1) It is unclear to me what the assumptions on the state and action spaces are. Reading Section 2 (lines 108–161), it seems they could be very general (e.g., Polish spaces). However, later in the paper compactness of the state space appears to be required, and in bound (26, line 386) the argument seems to rely on a finite action space. This creates confusion. Could you please clearly state the precise assumptions on the state and action spaces already in Section 2 when introducing the problem?\n\n2) I do not agree with the statement :\n“Despite its practical relevance, discrete-time SOC has historically received less theoretical attention and often presents greater challenges due to the absence of many of the mathematical tools available in continuous time.”\nPlease consider the excellent textbooks Discrete-Time Markov Control Processes by Onésimo Hernández-Lerma and Jean-Bernard Lasserre. In my opinion, discrete-time SOCs are extremely well studied, both theoretically and practically.\n\n3) How can one verify Assumption 3 in practice? In particular, how can one determine or estimate the smoothness parameter $\\beta_t$?\n\n4) It seems that the Error-Backpropagation term in Theorem 1 grows with the horizon $T$, which raises the question of how useful the result remains for large-horizo (or even infinite-horizon) problems. Could the authors please comment on this limitation?\n\n5) Typos:\n\na) Eq. (8): should read $\\mathcal{U}_t$ instead of $U_t$.\n\nb) Line 260: “$\\mathcal{U}$” should likely be $\\mathcal{U}_t$.\n\nc) Line 239: “defiend” → “defined.”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "pwDM7AR60a", "forum": "edbmmvFykY", "replyto": "edbmmvFykY", "signatures": ["ICLR.cc/2026/Conference/Submission13220/Reviewer_1Tfg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13220/Reviewer_1Tfg"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761550803581, "cdate": 1761550803581, "tmdate": 1762923910578, "mdate": 1762923910578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper considers a regression-based approach to solving discrete-time stochastic optimal control problems. Given a controlled, possibly nonstationary, Markov process, the goal is to design a control law that maximizes the cumulative sum of rewards over a finite horizon $t\\in [0, T-1]$ plus a terminal reward at $t=T$. The paper proposes a regression-based approach to solve the dynamic programming problem and analyses the error propagation.\n\nThe algorithm is designed by first formulating the Bellman equation, $V_t(x)=ess sup_u F_t(x,u) + E[V_{t+1}(X_{t+1}^u)\\mid X_{t}^u=x]$. To solve for the value function at time $t=0$, the algorithm starts from $t=T$ (where $V_T$ is known), and at each time step estimates $V_{t}$ using kernel ridge regression (KRR) with simulated data. The data is obtained by sampling input states according to the dynamics $x_t\\sim \\mu_t$ and generating the corresponding labels $y$ using the Bellman equation, where the expectation term is approximated with Monte Carlo (MC) simulation and $V_{t+1}$ is replaced with the function estimate $\\hat{V}_{t+1}$ from the preceding time step. \n\nTo analyse the error $\\|\\hat{V}_0-V_0\\|$, the authors start by providing an upper bound of the error $\\|\\hat{V}_t-V_t\\|$ consisting of the sum of three naturally arising errors: the KRR error, the MC approximation error, and the propagation error associated with the Bellman operator. The KRR error scales as a function of the number of data samples $n_t$ and a parameter $\\beta_t$ which captures how well the value function can be approximated by an estimator from the RKHS. The MC error can be made to scale as the KRR error for a sufficiently large number of MC samples. The propagation error amplifies the error at the preceding time step with the Lipschitz constant $c_P$ of the Bellman operator. The final upper bound $\\|\\hat{V}_0-V_0\\|$ therefore scales as a function of $n_t$, $\\beta_t$, and $c_P$.\n\nTo verify the theory, the authors prove two auxiliary lemmas in addition to leveraging well-known results in statistical learning theory. The authors provide a running example with the pricing of an American max-call option, and evaluates the optimal policy under the algorithm in such a setting while comparing with a theoretical benchmark and baseline methods."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The problem setting is rigorously formulated and the background on stochastic optimal control is well-explained. The proposed algorithm is relatively simple to understand, and the chosen means of function approximation (KRR) is motivated from the perspective of being able to derive error bounds. The error decomposition is natural (although somewhat arbitrary, since it is formed by adding and subtracting relevant terms and using the triangle inequality). To the best of my knowledge, the theoretical results are correct in most cases (see “Weaknesses” for exceptions), even if it is unclear how much they inflate the upper bound on the error. I appreciate the running example on American-style max-call options, which clarifies the motivation (that traditional methods break down for sufficiently large baskets) and offers a nice illustration the methodology resulting in simple expressions.\n\nThe paper is generally clearly written and follows a logical structure. \n\nThe proof of Lemma 1 is correct from my understanding. While Lemma 2 holds, the proof has a significant mistake, I believe (see “Weaknesses”).\n\nThe experiments show that the algorithm yields"}, "weaknesses": {"value": "I believe there are major weaknesses: 1) significant error in a proof, 2) incorrect citation format, 3) missing experimental evidence of how the algorithm can be designed to control the error by inducing favorable values of key parameters ($n_t, \\beta_t, c_P$), and 4) missing theoretical discussion on appropriate kernel design (which defines the RKHS) given the form of the value function.\n\nIt is difficult for the reader to understand the usefulness of the error bounds. The authors claim that the error bounds can be used to control the error (L057); however, this is not experimentally verified. The paper would benefit from an experimental evaluation of how the error relates to the key parameters identified in the error analysis, such as $c_P$ and $\\beta_t$, by repeating experiments under settings reflecting varying $c_P$ and $\\beta_t$. For example, $\\beta_t$ might be controlled by choosing more or less appropriate kernels given the reward function.\n\nI am missing a more detailed discussion on the choice of kernel, and thus the range of $\\beta_t$, given the form of the value function, generally or for specific reward functions. For example, in the American call option example, the authors note that $V_T$ is non-smooth and $\\beta_t$ thus can be small (which inflates the error bound), yet proceeds to use the RBF kernel in experiments which represents smooth functions and therefore is unsuitable.\n\nThe proof of Lemma 2 has a mistake, although the conclusion still holds. Specifically, in proving the Lipschitz property, the authors claim that $\\|ess sup_u [F_t(\\cdot, u)+P_t^u g] – ess sup_u [F_t(\\cdot, u)+P_t^u f]\\|=\\|ess sup_u [ P_t^u g –P_t^u f]\\|$, but if suffices to consider a binary control with a pretty arbitrary lookup table for $P_t^ug, P^u_tf$ in the special case that $X^u_{t+1}$ is independent of $X^u_t$ to see that it does not hold. I think it should be an inequality sign, in which case the final inequality still holds.\n\nIn terms of presentation, the paper is not using the correct ICLR citation convention which separates between \\citet, \\citep. Instead, the authors exclusively use \\citet, while \\citep is the correct option in many places. Statements such as L039, *“While continuous-time SOC has been extensively studied in the literature Bertsekas (2012), […], where decisions are made at fixed time intervals Bertsekas & Shreve (1996); Puterman (2014).”*, has a significant effect on readability. Especially the paragraph L058-L072 is tough to read as result.\n\n*Minor comments (shared for the benefit of the authors, not affecting my assessment)*\n\nIn Theorem 1, the authors refer to the decay rate $\\lambda_t\\sim n_t^{-1/(\\beta_t+1)}$. This arises from a corollary in Appendix B, and the main text is missing a reference to this result or an explanation.\n\nTypo/grammatical error in L054, L065, L239.\n\nThe equation notation is inconsistent, see L158 vs L202."}, "questions": {"value": "In Algorithm 1, how would you generate the input values $x_t\\sim \\mu_t$ when the control influences the dynamics of $x_t$  beyond sending it to the cemetery state? In the case of the American option, the dynamics of $x_t$ is effectively independent of $u$, where $u$ just sends it to the cemetery state when activated. So here I see that forward simulation of the SDE (Eq. 19) would work. It is not clear to me how to implement the algorithm, which starts from $T$ and goes backward, when the SDE contains a general function of $u$ (which $\\pi_t$ allows).\n\nWhat fundamentally separates your method from benchmark methods based on Gaussian Process Regression (GPR), e.g., GPR-MC, considering that GPR can be viewed as KRR (such that the posterior mean function relates to the KRR function estimate)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "qIIDdNR7PU", "forum": "edbmmvFykY", "replyto": "edbmmvFykY", "signatures": ["ICLR.cc/2026/Conference/Submission13220/Reviewer_a8rA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13220/Reviewer_a8rA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843442372, "cdate": 1761843442372, "tmdate": 1762923909979, "mdate": 1762923909979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the theoretical and methodological foundations for solving discrete-time stochastic optimal control problems. The authors formulate the control problem within a general dynamic programming framework and propose an approximation algorithm that combines nonparametric regression with Monte Carlo  sampling. Specifically, it employs Kernel Ridge Regression within a RKHS for value function estimation and MC sampling to estimate the continuation value."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **explicit error propagation bound**: The paper’s central theoretical contribution lies in rigorously quantifying how regression, sampling, and propagation errors interact and accumulate backward in time. This explicit and interpretable result provides a understanding of error propagation in DP-based SOC learning.\n\n2. **good presentation**: Overall, I really enjoyed reading this paper. The authors present the main problem clearly and structure the exposition in a logical and accessible manner.\n\n3. **Time relevant reserach topic**: Rigorous analysis for SOC is a challenging research area. The paper's value lies in its attempt to directly address this critical problem, aiming to provide a complete end-to-end theoretical guarantee for a specific (kernel-based) methodology."}, "weaknesses": {"value": "1. **Lack of justification for KRR and ambiguous contribution positioning**: The choice of KRR as the main regression tool is not convincingly justified. The paper implicitly positions KRR as theoretically superior by emphasizing its analytical tractability, while underestimating the rich theoretical results already available for neural networks in similar settings. For instance, neural network–based approaches have achieved strong theoretical guarantees in high-dimensional SOC, PDE approximation, and deep reinforcement learning.\n\n[1] Tsang, Ka Ho, and Hoi Ying Wong. Deep-learning solution to portfolio selection with serially dependent returns. SIAM Journal on Financial Mathematics 11.2 (2020): 593–619.\n\n[2] Lim, Dong-Young, et al. Langevin dynamics based algorithm e-THEO POULA for stochastic optimization problems with discontinuous stochastic gradient. Mathematics of Operations Research 50.3 (2025): 2333–2374.\n\n[3] Huré, Côme, et al. \"Deep neural networks algorithms for stochastic control problems on finite horizon: convergence analysis.\" SIAM Journal on Numerical Analysis 59.1 (2021): 525-557.\n\n[4] Beck, Christian, et al. \"Deep splitting method for parabolic PDEs.\" SIAM Journal on Scientific Computing 43.5 (2021): A3135-A3154.\n\n[5] Neufeld, Ariel, Philipp Schmocker, and Sizhou Wu. \"Full error analysis of the random deep splitting method for nonlinear parabolic PDEs and PIDEs.\" arXiv preprint arXiv:2405.05192 (2024).\n\n2. **Strong assumption (Asumption 1)**: The second condition in Assumption 1 appears quite restrictive, as it conveniently simplifies the propagation error.\n\n3. **Moderate theoretical novelty**: Consequently, Theorem 1 essentially aggregates three well-known components: (1) a standard KRR estimation bound, (2) a standard Monte Carlo sampling bound, and (3) a propagation bound derived almost trivially from Assumption 1. As such, the overall theoretical novelty may be limited.\n\n\n4. **Scalability issue**: Kernel methods are known to suffer from the curse of dimensionality, making their application to high-dimensional SOC settings challenging. The experiments are restricted to small-scale problems, and no comparison with deep learning–based methods is provided."}, "questions": {"value": "1.  If the error propagation (Term III) is fundamentally determined by the MDP's properties ($c_P$) and not the regression model, what is the fundamental theoretical advantage of using KRR, and accepting its critical scalability issues, over neural networks, whose approximation and generalization bounds are also well-studied? Does the explicit convergence rate for Term I obtained via KRR justify the cost of curse of dimensionality?\n\n2. Beyond discounted option pricing, under what classes of SOC models does $c_p\\leq 1$ hold? \n\n3.  How does the use of the FALKON approximate solver in Section 5 align with the theoretical analysis in Section 4? \n\n4. It would be benefittial to include comparisons with deep learning–based approaches under the same DP setting. \n\n5. Can this approach be extended to high-dimensional state spaces (e.g., 100 dimensions)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "3anlN4cnuO", "forum": "edbmmvFykY", "replyto": "edbmmvFykY", "signatures": ["ICLR.cc/2026/Conference/Submission13220/Reviewer_mxZL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13220/Reviewer_mxZL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915640223, "cdate": 1761915640223, "tmdate": 1762923908961, "mdate": 1762923908961, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Paper describes how approximation errors (from function approximation and sampling) propagate backward  through discrete‑time stochastic control / dynamic programming (DP), and do so in a mathematically clean way that scales to practical Monte Carlo pipelines. \nCast the value‑function recursion in (L^2) spaces and approximate each stage with Kernel Ridge Regression (KRR) in an RKHS, while estimating conditional expectations by Monte Carlo. The paper introduces a **three‑term error decomposition** (regression, sampling, propagation) and derives rates under **model misspecification** (source conditions). It then specializes the analysis to American option pricing and gives a simple, fast implementation using FALKON"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper provides a transparent decomposition of where error comes from and how it composes across time. This makes it easier to reason about sample allocation per stage and to justify backward pipelines widely used in practice. \nMany DP papers state rates in well‑specified cases; here the source‑condition analysis (and the no smoothing under max) keeps expectations realistic for options and other kinked payoffs."}, "weaknesses": {"value": "The square‑integrability + bounded pushforward density  (Assumption 1 and App. D) are natural for options (and are verified), but for general controlled Markov dynamics—especially continuous actions—the paper relies on either a finite control set or Lipschitz transitions on compact sets to get the (\\widetilde O(M^{-1/2})) MC rate . The continuous‑action coverage is thus limited to fairly regular models and compact truncation.  \n\nThe clipping/truncation device (Eq. 18 and discussion just after) is standard but introduces bias,  whose impact is not quantified end‑to‑end, in the main bound. \n\nThe best‑case rates depend on spectral decay, but the main theorem states the weakest capacity version. While fine for readability, practitioners may want *guidance on kernels / data distributions that realize the faster rates. \n\nThe experiments rely on FALKON (Nyström‑sketched KRR), but the theory is for exact Kernel Ridge Regression (KRR.) There is no explicit error budget split showing how sketching error composes with the three terms in Eq. (23). \n\nComparisons focus on GPR‑based methods; classical LSMC (Longstaff–Schwartz) and primal/dual methods are acknowledged in the related work but not benchmarked. Since those are the workhorses in practice, including them (even on (d\\leq 10)) would make the case more compelling. (The paper itself lists these families in Sec. 1.) \n   * **Hyperparameter protocol** is light. Tables 1–2 report strong timings, but the **number of training points P** used by GPR‑Tree/EI (1 000) is fixed from a prior paper, while KRR‑DP uses **much smaller (n,M)**; it would help to **sweep (n,M)** to show price‑error vs time trade‑offs and confirm the speedup is not just from fewer samples (App. C gives some sizes). \n   * All experiments use **lognormal GBM** with (T=9), fixed (\\sigma,\\rho). Stress‑tests (larger (T), heterogeneous vols, path‑dependent payoffs) would probe the **error‑propagation** claims more thoroughly. \n\n5. **No adaptive allocation policy**\n   The theory suggests how errors enter, but the algorithm does not exploit it to **adapt ((n_t,M_t,\\lambda_t))** to **measured difficulty** (e.g., smaller (\\beta_t) near maturity). An **adaptive budget** could yield tangible gains. \n\n6. **Missing constants / finite‑sample guidance**\n   The **“with high probability”** bounds do not expose constants, so direct **sample‑size planning** per stage remains heuristic. A **worked numerical sizing** using the elements in App. B would be valuable."}, "questions": {"value": "Tighten the algorithm–theory link by incorporating a sketched‑KRR error term into Eq. (23) and recommending landmark sizes that keep it below the regression term. \nAdd baselines beyond GPR: LSMC, a primal‑)dual method, and a basic deep BSDE / deep stopping approach [5,10]. Even a small table would broaden the empirical takeaways. \nReport ablations: curves of **price error vs. (n_t), (M_t), (\\lambda_t)** and a head‑to‑head **time–accuracy frontier** with GPR‑MC. (The text notes that sample sizes grow with (d) in App. C; plotting their impact would be instructive.) \nQuantify clipping/truncation bias in a lemma (e.g., tails under GBM), or show empirically that relaxing bounds does not change prices beyond CIs. \nDiscuss continuous‑action SOC beyond stopping: explicitly state how the Lipschitz/compact assumptions ensure Eq. (26), and add one small continuous‑control example to illustrate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DZX1d0WnUu", "forum": "edbmmvFykY", "replyto": "edbmmvFykY", "signatures": ["ICLR.cc/2026/Conference/Submission13220/Reviewer_z7ZE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13220/Reviewer_z7ZE"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13220/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976300227, "cdate": 1761976300227, "tmdate": 1762923908474, "mdate": 1762923908474, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}