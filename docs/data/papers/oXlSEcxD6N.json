{"id": "oXlSEcxD6N", "number": 20812, "cdate": 1758310500852, "mdate": 1759896957395, "content": {"title": "Trust-Region Adaptive Policy Optimization", "abstract": "Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. \nWe address this inefficiency with TRAPO  (\\textbf{T}rust-\\textbf{R}egion \\textbf{A}daptive \\textbf{P}olicy \\textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.", "tldr": "", "keywords": ["Large Language Models", "Reasoning Model", "Reinforcement Learning", "Trust Region", "Knowledge Distillation"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ccf5f5ae76ca502905fab85c3ac3b8dc643eae3c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces TRAPO (Trust-Region Adaptive Policy Optimization), a novel one-stage hybrid framework that unifies Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) for post-training large language models (LLMs). The authors argue that the conventional two-stage pipeline—first SFT, then RL—creates a mismatch: SFT enforces rigid imitation that suppresses exploration and induces catastrophic forgetting, thereby weakening RL’s learning potential.\n\nTo overcome this, TRAPO interleaves SFT and RL within each training instance. It performs:\n- SFT loss on expert prefixes, to absorb expert reasoning patterns.\n- RL loss on model rollouts, to maintain exploration and self-improvement.\n\nA key innovation is Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL to encourage mode-seeking rather than mode-covering. This ensures stable and meaningful updates beneficial for RL. An adaptive prefix-selection mechanism dynamically adjusts expert prefix length according to model performance, balancing guidance with exploration.\n\nExperiments across five mathematical reasoning benchmarks (AIME2024, AMC, MATH-500, Minerva, OlympiadBench) show TRAPO outperforms SFT, RL, and the SFT-then-RL pipeline by +2–6 points, and achieves the highest average accuracy of 56.6%. TRAPO also generalizes well to non-mathematical tasks (ARC-c, MMLU-Pro) and exhibits superior test-time scaling properties."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel Conceptual Integration\nThe paper’s central insight—recasting SFT and RL as interleaved processes at the instance level—is original and well-motivated. By linking expert prefix imitation with RL rollouts in a trust region, TRAPO presents a principled path toward more synergistic post-training.\n\n2. Theoretical Clarity\nThe authors diagnose a specific instability—distribution blending due to the mode-covering nature of forward KL—and formally derive how the clipped gradient in TrSFT converts the optimization dynamics toward reverse KL’s mode-seeking behavior. This analysis provides strong theoretical grounding rarely seen in empirical LLM post-training work.\n\n4. Comprehensive Evaluation\n\nThe paper offers convincing empirical validation:\n- Clear improvement over both pure RL baselines (GRPO, PRIME-Zero, etc.) and hybrid methods (LUFFY, ReLIFT).\n- Extensive ablation studies show the distinct contribution of micro-group sampling and TrSFT."}, "weaknesses": {"value": "1. Limited Scope of Evaluation\n\nAll major experiments focus on mathematical reasoning benchmarks. While the generalization test on ARC-c and MMLU-Pro is included, more diverse domains (e.g., commonsense reasoning, coding, dialogue) would better validate TRAPO’s universality.\n\n2. Lack of Cost and Efficiency Analysis\n\nTRAPO’s training procedure—especially with multiple micro-groups and prefix sampling—appears more computationally expensive than standard RL. The paper does not quantify overhead (e.g., training time, token cost, GPU hours), which is important for practical adoption.\n\n3. Trust-Region Parameter Sensitivity\n\nAlthough the trust-region boundary parameter α is analyzed in the appendix, the main paper provides limited discussion on its tuning sensitivity and stability under different model scales."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "N2HfhGsBaJ", "forum": "oXlSEcxD6N", "replyto": "oXlSEcxD6N", "signatures": ["ICLR.cc/2026/Conference/Submission20812/Reviewer_3vHn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20812/Reviewer_3vHn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855842968, "cdate": 1761855842968, "tmdate": 1762999982221, "mdate": 1762999982221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TRAPO, a one-stage post-training framework that interleaves SFT and RL at the instance level. For each prompt, the model receives an expert prefix (possibly empty), then rolls out its own completion; training applies TrSFT (a trust-region variant of SFT) on the expert tokens and an RL loss on the self-generated suffix. The authors argue TrSFT curbs the mode-covering tendency of forward-KL SFT by clipping per-token gradient weights outside a probability \"trust region,\" effectively shifting behavior toward mode-seeking (reverse-KL-like) updates that are friendlier to RL. They also introduce micro-group sampling, which adaptively increases the prefix length only when earlier unguided (or shorter-guided) rollouts underperform. Empirically, on five math-reasoning benchmarks, TRAPO outperforms SFT, pure RL, and the common SFT-then-RL pipeline. On average it improves over SFT and GRPO by and beats SFT-then-RL."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The per-instance coupling, SFT on prefixes + RL on suffixes, neatly addresses the two-stage inconsistency the authors highlight. This method directly targets the lack of exploration space for SFT'ed policies, which is a concrete and important problem in RL today. The micro-group schedule is clear and practical.\n2. Clipping the token-level gradient weight with a threshold α seems to be an intuitive way to prevent outsized updates on low-probability expert tokens. Relevant hyperparameter study is inlcuded.\n3. The pseudocode for the adaptive sampling and the combined gradient update is helpful for me to understand the method in general."}, "weaknesses": {"value": "1. One (or a type of) baseline I consider missing from the paper is existing work that directly improve sampling diversity during rollout in RL. Simple baseline might include increasing temperature, or other algorithms that encourage diverse samples. I recognize the the authors' argument that SFT may decrease sampling diversity, but there is also simple entropy control mechanisms like [1] (and possibly some others).\n2. Training relies on OpenR1-Math expert trajectories (DeepSeek-R1), and the core evaluations are all math-reasoning; stronger non-math results would better support claims of broader reasoning gains from TRAPO.\n3. The RL baseline is implemented with a GRPO variant and no KL penalty; the paper should clarify whether this favors TRAPO (which already regularizes via TrSFT).\n\n[1] He, Jujie, et al. \"Skywork open reasoner 1 technical report.\" arXiv preprint arXiv:2505.22312 (2025)."}, "questions": {"value": "1. If you add back a standard KL penalty to the RL objective, how do performance and stability change relative to TrSFT alone?\n2. How does TRAPO perform if the expert prefixes come from a weaker trajectory model, or from noisy/partial solutions? Does TrSFT still help, or does it overfit to poor guidance?\n3. Does the dynamic prefix selection strategy incur computation overhead? If so, by how much?\n4. Can you show ablations on code or multimodal reasoning, where the expert prefixes may differ stylistically from the target model's distribution?\n5. You randomly pick one expert trajectory per micro-group. Would prioritizing trajectories (e.g., by diversity or difficulty) further help?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "n/a"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UG0z0lzL2t", "forum": "oXlSEcxD6N", "replyto": "oXlSEcxD6N", "signatures": ["ICLR.cc/2026/Conference/Submission20812/Reviewer_Ux2e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20812/Reviewer_Ux2e"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761944753881, "cdate": 1761944753881, "tmdate": 1762999982241, "mdate": 1762999982241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TRAPO, a hybrid framework that unifies external supervision and self-exploration through alternating optimization within each training instance. Concretely, it employs TrSFT, an SFT method that modulates optimization strength via a trust region to eliminate the \"mode-covering\" defect of standard SFT, and calculates RL loss on the model’s self-generated completions to preserve exploratory ability. Experiments on 5 mathematical reasoning and 2 general-domain reasoning benchmarks demonstrate that TRAPO outperforms standard SFT, pure RL, and SFT-then-RL pipelines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-organized and easy to follow. The method is clearly defined, and the figures are clear and helpful.\n\n2. The ablation results support the design. Micro-group sampling alone improves over GRPO, while replacing TrSFT with standard SFT causes a clear drop. This suggests the trust region brings stability, and the adaptive prefixes provide useful guidance."}, "weaknesses": {"value": "1. This paper only validates TRAPO’s performance on established benchmarks, including 5 mathematical reasoning benchmarks and 2 general-domain reasoning benchmarks, while excluding newer, harder datasets like AIME25 and GPQA.\n\n2. The micro-group sampling of TRAPO requires sequential processing for each prompt’s micro-groups within a mini-batch. This sequential logic for micro-groups is far less parallelizable than baselines like pure GRPO. Additionally, the training time is not provided, so the tradeoff between TRAPO's performance and efficiency remains unproven.\n\n3. TRAPO relies on multiple hyperparameters that are difficult to tune, including micro-group-specific ones and the TrSFT trust-region parameter alpha. While Figure 7(b) shows accuracy variations of Qwen2.5-Math-7B on mathematical benchmarks with different alpha values, it does not demonstrate stable or optimal performance across parameter adjustment; instead, it reflects that alpha is sensitive to performance, let alone the combined tuning complexity of all hyperparameters.\n\n4. TRAPO feels overengineered: it combines staged micro‑group scheduling with per‑prompt SFT prefix adjustment, which together make the pipeline complex and difficult to control."}, "questions": {"value": "1. How does TRAPO perform on newer, harder benchmarks such as AIME25 and GPQA?\n\n2. How robust is TRAPO to low‑quality or noisy expert trajectories in SFT data? For example, with wrong or meaningless prefixes, does performance degrade gracefully, and what mitigation strategies work best, such as prefix filtering or confidence weighting?\n\n3. How dependent is TRAPO on the Expert Model? What happens if the Expert is weaker or partially misaligned? Specifically, if the Expert Model is identical to the Policy Model, how do performance and stability change?\n\n4. How should these hyperparameters be tuned, such as the number and sizing of micro-groups, the prefix ratios and return thresholds, and the TrSFT trust‑region alpha? Are there recommended defaults or automatic tuning strategies?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PEvhpodAiR", "forum": "oXlSEcxD6N", "replyto": "oXlSEcxD6N", "signatures": ["ICLR.cc/2026/Conference/Submission20812/Reviewer_Hux5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20812/Reviewer_Hux5"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975785829, "cdate": 1761975785829, "tmdate": 1762999982477, "mdate": 1762999982477, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes TRAPO, a one-stage post-training framework that interleaves SFT and RL at the instance level. For each prompt, the method applies SFT on an expert prefix and RL on the model-generated suffix. The key ingredient is Trust-Region SFT (TrSFT), which replaces the standard per-token weight 1/p with 1/max(p, alpha) to prevent large updates on very low-probability tokens (reducing forward-KL “mode-covering” drift and behaving more mode-seeking outside the trust region). A second component, micro-group (adaptive prefix) sampling, increases the prefix length only when the current group’s pass rate is low, providing “just enough” guidance. On five math benchmarks with Qwen2.5-Math-7B, TRAPO reports higher average accuracy than SFT, GRPO, SFT-then-RL, LUFFY, and ReLIFT; it also shows gains on ARC-c and MMLU-Pro. Ablations indicate that (i) micro-group sampling helps even without explicit prefix learning, (ii) naively adding a standard SFT loss during RL can collapse performance, and (iii) TrSFT recovers and improves on top of micro-groups.\n\nDisclosure: I used assistive writing tools to draft this review; all evaluations and judgments are my own."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "Motivation is concrete. The paper clearly diagnoses why forward-KL SFT can hurt exploration when combined online with RL (distribution blending, degenerate rollouts) and proposes a minimal, targeted fix (clipping the per-token weight with alpha).\n\nSimple and practical. TrSFT is a one-line modification of the SFT loss; micro-group sampling is a lightweight, per-prompt procedure (ratios 0, 0.2, 0.5, 1.0; thresholds −1, 0.5, 0.7, 0.9; group sizes {4, 2, 1, 1}).\n\nConsistent improvements. On Qwen2.5-Math-7B, TRAPO’s average across AIME2024, AMC, MATH-500, Minerva, and Olympiad is 56.6 vs 55.5 (LUFFY), 54.3 (SFT-then-RL), and 50.4 (GRPO). It also leads on ARC-c/MMLU-Pro (avg 68.3).\n\nUseful ablations and training dynamics. The paper shows why naïvely summing SFT+RL fails, why adaptive prefixes help, and how TrSFT stabilizes learning. Reward/length/entropy curves and Pass@k scaling support the narrative that TRAPO preserves solution-space diversity and improves with test-time compute."}, "weaknesses": {"value": "Alpha inconsistency; please clarify.\nThe main setup and alpha sweep indicate alpha = 0.1 works best. In Appendix C.2, however, TRAPO is described “except for setting the trust-region parameter alpha to 1,” which would nullify clipping and contradict the rest of the paper. Please reconcile and state the alpha used for every table/figure.\n\nTheory–practice gap and Proposition 1 details.\nThe toy GMM example and the TrSFT optimum (Proposition 1) are helpful intuitively, but the proof sketch appeals to behavior (e.g., step-function arguments) that could be made more rigorous. More importantly, there are no transformer-level diagnostics to show TrSFT’s intended effect in practice (e.g., distributions of per-token gradient weights, expert-mode coverage over time). Please add empirical diagnostics on real runs.\n\nCompute parity and disclosure.\nMicro-groups introduce guided rollouts; budgets may differ from baselines. Please disclose training tokens/steps, rollout counts, wall-clock/GPU hours, and decoding settings for every method, and either confirm compute parity or normalize the comparison.\n\nStatistical rigor.\nResults appear single-seed. Small-n suites like AIME/AMC are noisy. Please add multi-seed (≥5) results with 95% CIs (paired bootstrap for Pass@k) and significance tests for headline numbers; report standardized effect sizes where appropriate.\n\nData hygiene and verifier robustness.\nTraining uses OpenR1-Math-46k-8192 plus extra R1 trajectories; evaluation uses Math-Verify and OAT-Grader. Please report exact/near-duplicate removal between training and test problems, and provide at least a small human-audited calibration of grader errors. A robustness check with an alternative grader or injected noise would help, since tail-emphasizing supervision can amplify mislabels.\n\nMicro-group sensitivity and usage statistics.\nThe thresholds and ratios are reasonable, but the paper does not show sensitivity curves or a histogram of actual prefix usage by difficulty/time. This would distinguish “smarter guidance” from “more guidance,” and quantify how often the full-trajectory prefix is triggered.\n\nBaseline coverage and consistency.\nTRAPO beats LUFFY and ReLIFT, but closely related “single-stage SFT+RL” or “prefix-guided SFT” methods (e.g., Prefix-RFT, AMFT, SRFT, HPT, UFT) are not compared head-to-head under the same codebase and budgets. Given conceptual proximity, a direct comparison would clarify novelty/attribution. Also, unify the RL variant description (GRPO vs Dr.GRPO) and ensure all baselines use the same variant and decoding settings.\n\nLLM-usage disclosure inconsistency.\nAppendix C.1 uses an LLM to count reasoning behaviors (backtracking/backward-chaining), while Section F says LLMs were used only for language polishing. Please align these statements."}, "questions": {"value": "Clarify alpha everywhere and correct any plots/tables if needed; report which alpha was used per experiment.\n\nCompute parity and statistics: publish a single evaluation script; report tokens/steps/rollouts/GPU-hours; add 5+ seeds and 95% CIs (paired bootstrap for Pass@k).\n\nMechanistic diagnostics: track per-token gradient weights 1/max(p, alpha) vs 1/p, token-level entropy, and expert-mode KL through training to demonstrate TrSFT’s intended effect in transformers.\n\nData and grader robustness: dedup train vs test; report a small human-audited error rate; test sensitivity to grader noise or an alternative grader.\n\nMicro-group sensitivity: ablate thresholds, ratios, and group sizes; show a histogram of prefix usage; include a fixed-prefix baseline to isolate the value of adaptivity.\n\nBroader baselines: add at least one of Prefix-RFT, AMFT, SRFT, HPT, or UFT under matched compute and training code.\n\nStronger generalization test: add Putnam-AXIOM (especially the functional variations) with matched compute, 95% CIs, and p-values. This suite is designed to probe true extrapolative generalization. If TRAPO shows robust gains there, it would materially strengthen the claim that the method expands the solution space rather than just re-ranking known patterns. I would raise my score if you include convincing Putnam-AXIOM results (see https://arxiv.org/abs/2508.08292)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PHJ2GvmFi6", "forum": "oXlSEcxD6N", "replyto": "oXlSEcxD6N", "signatures": ["ICLR.cc/2026/Conference/Submission20812/Reviewer_KiUB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20812/Reviewer_KiUB"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20812/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240191025, "cdate": 1762240191025, "tmdate": 1762999982312, "mdate": 1762999982312, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}