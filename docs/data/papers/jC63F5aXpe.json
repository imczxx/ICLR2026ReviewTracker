{"id": "jC63F5aXpe", "number": 19346, "cdate": 1758295514677, "mdate": 1759897044062, "content": {"title": "CPMöbius: Iterative Coach–Player Reasoning for Data-Free Reinforcement Learning", "abstract": "Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce \\framework, a collaborative \\textbf{Coach–Player} paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play frameworks, \\framework inspired by multi-agent collaboration treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player’s capability and receives rewards based on changes in the Player’s performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player’s mathematical reasoning ability. Remarkably, \\framework achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on the Qwen2.5-Math-7B-Instruct, our method improves accuracy by overall average +4.9 and out-of-distribution average +5.4, which exceed RENT for +1.5 on overall accuracy and R-zero for +4.2 on OOD accuracy.", "tldr": "This paper introduces CPMobius, a data-free reinforcement learning paradigm where a \"Coach\" and a \"Player\" model collaborate to improve LLMs reasoning without any external training data.", "keywords": ["LLMs reasoning", "Reinforcement Learning", "Unsupervised Learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6f8d9d64717a1a301eaeb8788cef28bafe0934c4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper focuses on improving the reasoning capability of LLMs without extra data by a sollaborative coach-player paradigm. The coach LLM proposes instruction to maximize the player's accuracy on validation set. Then the player LLM  conduct self-training on these generated instructions with pseudo-labels. Experiments on Qwen2.5-Math and OctoThinker are conducted to validate the proposed method."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed method is simple and straightforward.\n- Most parts of this manuscript are easy to follow up with."}, "weaknesses": {"value": "- Limited novelty and contribution. There have been several works focuses on leveraging data-free or label-free RL for reasoning. From my reading of this paper, the proposed method seems very similar to absolute zero. However, direct comparison or indirect but comprehensive discussion are missing. Besides, the author should at least mention EMPO[1] and Intuitor[2] in the related works. I also enourage to include TTRL or EMPO baselines if it is possible.\n- The experiments results are noticeably lower than state-of-art results when considering the same Base model. For example, on Qwen2.5-Math-7B, zero RL can achieve 40% and 65% accuracy on AIME24 and AMC23 benchmarks respectively [4]. Even without groundtruth, unsupervised RL can achieve 20% and 65% accuracy on these two datasets [3]. Although the proposed method is devised in a data-free manner, such a large margin raise concerns about the effectiveness of the proposed coach-player training paradigm. Besides, it is also quesntionable whether the performance gain of the proposed method is higher than prompt engineering (especially for Base model).\n- The so-called OOD setting does not make sense to me. In the experiments, the model is trained and evaluated on both Math domain. Besides, all the Base model are pretrained with math-specific data corpus. Thus the OOD generation capability of the proposed method is overclaimed.\n- Serveral words and sentences are weired to me and seldom ocurred in acedamic writting. I doubt if this manuscript is written by LLMs without enough polishment.\n- Figure 1 and 2 are not informative. They are not helpful for the reviewer to follow up with the core idea.\n\n[1] Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization, NeurIPS25\n\n[2] Learning to Reason without External Rewards\n\n[3] DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning"}, "questions": {"value": "- Why there are only 100 steps in Figure 5 but not 1000?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jUVItxqmnN", "forum": "jC63F5aXpe", "replyto": "jC63F5aXpe", "signatures": ["ICLR.cc/2026/Conference/Submission19346/Reviewer_cag2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19346/Reviewer_cag2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761403057640, "cdate": 1761403057640, "tmdate": 1762931284156, "mdate": 1762931284156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CPMöbius, a two-agent training loop for improving math reasoning without using external supervised datasets. The two agents are a Coach who generates problems, and a Player who solves them using reinforcement learning.\nThe Player is updated using GRPO, a critic-free RL method that normalizes rewards, while the Coach receives a reward based on how much the Player’s validation accuracy improves on a fixed benchmark (AMC), making the Coach cooperative rather than adversarial.\nThe authors report that this process improves math reasoning for multiple model sizes (1.5B–7B) and outperforms baselines such as RENT and R-Zero on both overall and out-of-distribution benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The Coach’s reward is directly linked to the Player’s improvement on a validation set (Δt), providing a clear optimization signal rather than relying on self-assessed progress. This makes the training objective interpretable and well-grounded.\n2. The paper outlines a concrete four-stage loop with pseudo-code showing the alternation between question generation, Player updates, validation, and Coach updates. The implementation is more transparent than in many prior self-play RL works.\n3. Experiments span models at different training stages, including pretrained, SFT, and RL-tuned. It indicates the method’s generality beyond a single setup. Disabling the Coach update, warm start, or difficulty filter each degrades performance, showing that each component contributes meaningfully to the final results."}, "weaknesses": {"value": "### “Data-free” is oversold\n\n - The paper's \"data-free\" and \"no external training data\" claims seem to overlook significant dependencies on human-curated data.\n\n - The Coach isn't built from scratch. It's initialized from Qwen2.5-7B-Instruct and then \"cold-started\" using the PRIME Eurus-2-RL-Data. \n\n - That dataset consists of human-produced math problems, meaning the Coach inherits a substantial amount of math-specific signal before the main loop even begins.\n\n - The chosen Player models (like OpenMath-Nemotron-1.5B or Qwen2.5-Math-7B-Instruct) are selected precisely because they have already undergone extensive math SFT or RL tuning on large-scale, human-created math datasets.\n\n - The \"progress reward\" isn't self-contained; it relies on the AMC benchmark as a held-out validation set in every single round. AMC is a standard benchmark of human-written problems.\n\nWhile the incremental fine-tuning data is self-generated, the entire system is scaffolded and guided by existing human data. The claims should be qualified. A more accurate description might be \"no new human-curated supervision is introduced after initialization\" or that improvement is driven by \"self-generated instructions validated against a fixed external benchmark\".\n\n---\n\n### “Majority vote = ground truth” can self-confirm errors\n\nThe Player generates n answers, then takes majority vote as pseudo-label y*. Every sample that matches y* is rewarded as “correct\".\nThis assumes that the Player’s most common answer is correct. That is not always true, especially early in training. This could reinforce systematic mistakes and drive mode collapse toward confident but wrong heuristics.\n\nThe paper partially addresses this by:\n\n -  limiting training to questions where the Player isn’t trivially consistent (accuracy <0.8) but also not random (accuracy >0.2), and\n -  adding KL regularization to keep the Player near the reference policy.\n\nBut we still don’t see a direct analysis of “Are pseudo-labels actually correct?” or “Does the Player ever get stuck in a bad attractor?” The consistency plots (Fig. 4) show difficulty trends, but they don’t measure correctness against ground truth.\nThis is a safety hole. The method is only as truthful as its self-verifier.\n\n---\n\n### Baseline fairness\n\nThey compare against RENT and R-Zero. They say R-Zero “failed” on OpenMath-Nemotron-1.5B because “the challenger could not be trained\", implying instability.\n\nThat’s interesting, but if R-Zero was never able to produce any challenger for that model, that’s not a clean head-to-head comparison. It’s more like: “Our method trains where theirs exploded\". That’s valuable but should be stated clearly as robustness, not purely accuracy. Baseline parity matters a lot because small % gains (2–5 point average) can come from prompt engineering and sampling strategy choices.\n\nIf those issues are addressed, this is a solid submission. It has a crisp story (“stop adversarial self-play, make the teacher care about student progress”) and shows consistent quantitative gains across heterogeneous base models, which is rare in this space."}, "questions": {"value": "### Is AMC leaking into training?\n\nThe authors evaluate OOD performance separately from AMC to argue generalization. They justify this because AMC is used as validation during training and thus becomes in-distribution.\nThat’s fair, but practically:\nThe Coach reward directly uses Δt = Acc_val(new) − Acc_val(old) on AMC.\n\nSo AMC is shaping the Coach policy.\nIn other words, AMC is not just “a metric\", it’s part of the RL signal for the Coach. At that point AMC is effectively training data, even if you call it “validation\". This weakens the purity of the “out-of-distribution” story. The paper is transparent about splitting “overall avg” vs “OOD avg\", which is good, but will likely still push on whether this is just AMC overfitting plus some transfer.\n\n**Follow-up I’d want:**\n\n -  Show performance on AMC variants that are not exactly the subset used online for Δt during training (e.g., hold out 20% AMC for Coach signal and 80% for final report, or rotate validation sets).\n - Show whether the Coach starts overfitting to AMC style (short algebra questions vs e.g. Olympiad-style constructive proofs). Right now we only see final numbers, not qualitative drift.\n\n---\n\n### Can the authors provide more compute / stability details?\n\n - How often does the Coach collapse (e.g., starts generating garbage or trivial variants)?\n - How often does Player training diverge (very high KL, mode collapse, etc.)?\n - GPU hours / improvement curve.\n\nYou already plot training curves for AMC accuracy and “answer consistency\". It would be great to add similar curves for: question length growth, answer length compression, etc., but quantify them more precisely (you mention this qualitatively in Section 4.3 and appendices).\n\nFurthermore, please be explicit about how RENT and R-Zero were run:\n\n - same base checkpoints?\n - same rollout counts (n=16)?\n - same sampling temperatures / pass@k evaluation protocol?\n\n ---\n\n### Can the authors be clear about the positioning?\n\nI think the real pitch here is:\n\n> “We turn math reasoning improvement into an online cooperative curriculum design problem, where a Coach LLM learns to propose maximally educational tasks for a Player LLM, and the only scalar of truth is measured generalization gain on a fixed verifiable benchmark\".\n\nThat is new compared to standard RLHF (needs human labels), standard RLVR (needs verifiers with ground truth), and adversarial self-play (unstable).\nI would foreground that as the main conceptual contribution, more than “data-free\"."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BPgszKhRqz", "forum": "jC63F5aXpe", "replyto": "jC63F5aXpe", "signatures": ["ICLR.cc/2026/Conference/Submission19346/Reviewer_7o3P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19346/Reviewer_7o3P"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454134980, "cdate": 1761454134980, "tmdate": 1762931283741, "mdate": 1762931283741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CPMöbius, a collaborative Coach-Player framework for data-free reinforcement learning aimed at improving mathematical reasoning in large language models. The Coach model generates instructional tasks calibrated to the Player's current capability frontier, while the Player model is trained via GRPO using majority-voted pseudo-labels from self-consistency. The Coach receives rewards based on both the Player's training performance and validation accuracy improvements, creating a cooperative optimization loop. Experimental results show improvements over unsupervised baselines across multiple mathematical reasoning benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The shift from adversarial to cooperative multi-agent learning is conceptually interesting and appears more stable than prior adversarial approaches.\n- The paper is well written and easy to follow."}, "weaknesses": {"value": "Please see my detailed questions and concerns below."}, "questions": {"value": "- What prevents the Coach from simply memorizing patterns from the validation set rather than learning general curriculum design principles? With only AMC as feedback, how do you ensure the Coach learns transferable instruction generation?\n- How do you handle the case when $\\Delta t$ is negative? Does the Coach receive negative rewards, and if so, how does this affect REINFORCE gradient estimation stability?\n- What proportion of generated instructions are rejected by the filter at different training stages?\n- How do you ensure the Coach doesn't collapse into generating trivially easy problems that consistently fall within the 0.2-0.8 range?\n- What is the distribution of problem types generated by the Coach over training?\n- What proportion of the improvement comes from the cooperative framework versus simply having more compute for self-training? Is this a fair comparison to RENT/R-Zero in terms of computational budget?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rU2Hj2kz8i", "forum": "jC63F5aXpe", "replyto": "jC63F5aXpe", "signatures": ["ICLR.cc/2026/Conference/Submission19346/Reviewer_EwJr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19346/Reviewer_EwJr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761552679490, "cdate": 1761552679490, "tmdate": 1762931283246, "mdate": 1762931283246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a two‑agent cooperative Coach–Player framework to improve LLM mathematical reasoning without external training data for the Player. The Coach generates tasks calibrated to the Player’s current ability; the Player solves them and is trained with GRPO using verifiable rewards obtained via majority vote pseudo‑labels. The paper reports consistent gains over unsupervised baselines (RENT, R‑Zero) across four base models and six math benchmarks, with particularly large improvements on Minerva and MATH."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The method is novel and goal is clear. The Coach is rewarded by instruction‑level Player reward multiplied by the global validation improvement, directly incentivizing tasks that cause real learning instead of adversarial “gotchas.”  \n2. The paper shows consistent gains across models and benchmarks. In Table 1, this method improves Qwen2.5 Math 7B by 4.9, and shows large Minerva jumps (e.g., 34.6 to 44.9 for 7B; 16.3 to 28.0 for 1.5B).  \n3. The ablation study is complete. Removing Coach updates, the difficulty filter, or the Coach warm‑up degrades results (Table 2), and Figs. 3–4 show steady AMC validation gains and adaptive difficulty."}, "weaknesses": {"value": "1. Although Player training is “data‑free,” Sec. 4.1 says the Coach is initialized with Qwen2.5 7B Instruct and \"a preliminary cold‑start phase on mathematical problems sourced from PRIME Eurus‑2‑RL‑Data\". This undercuts the top‑line \"no external training data\" message and should be framed as Player data‑free, Coach warmed up with external math data. Please quantify how much PRIME data is used, and show results without any warm‑up (beyond the ablation) across all base models, not just on Qwen2.5‑Math‑1.5B.   \n2. The Coach reward uses global $\\Delta_t$ for all instructions in a batch (Eq. 6). This yields credit assignment ambiguity: one “good” instruction can mask others, and variance could be high. A baseline or control variate for $\\Delta_t$, or per‑instruction contribution estimates (e.g., off‑policy influence functions or leave‑one‑out $\\Delta_t$ ), would strengthen learning stability and attribution claims. No variance or stability metrics are reported. \n3. No confidence intervals or multiple‑seed variance are reported for Table 1. Since improvements are very close to 1% depending on the benchmark, error bars matter (especially for AIME with mean@32 sampling). \n4. Only two unsupervised baselines are considered. R‑Zero reportedly “failed” to train on OpenMath‑Nemotron‑1.5B, but the paper does not detail alignment of budgets or hyper‑params or the failure mode. Including additional self‑play or co‑optimization baselines (e.g., URPO‑style, self‑rewarding corrections) and equalizing compute would make the comparisons more robust."}, "questions": {"value": "Suggestions:\n1. Maybe providing error bars for Table 1 tasks that this paper's method is **not significantly outperformed** by other baselines will be more convincing. \n2. Making figure captions larger will be better for reading. For example, the text showing models and tasks in Figures 3 & 4 is small, and the author can consider making them as titles for each figure, not as captions. \n3. Quantify the Coach warm‑up corpus and show full results with no warm‑up for all base models. Clarify the marketing claim: \"data‑free Player training with optionally warmed‑up Coach,\" or similar.\n\nQuestions:\n- For majority-vote pseudo-labels, as majority vote can reinforce systematic biases if the Player is consistently wrong. Did you explore additional verifiers (symbolic solvers, consistency under paraphrasing)? \n- Why that exact range for difficulty filtering (0.2 to 0.8)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "lP1UVX1Go3", "forum": "jC63F5aXpe", "replyto": "jC63F5aXpe", "signatures": ["ICLR.cc/2026/Conference/Submission19346/Reviewer_zyLm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19346/Reviewer_zyLm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971175585, "cdate": 1761971175585, "tmdate": 1762931282828, "mdate": 1762931282828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}