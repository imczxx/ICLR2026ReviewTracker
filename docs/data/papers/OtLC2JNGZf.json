{"id": "OtLC2JNGZf", "number": 11293, "cdate": 1758195537972, "mdate": 1759897595800, "content": {"title": "UrbanFeel：A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective", "abstract": "Urban development impacts over half of the global population, making human-centered understanding of its structural and perceptual changes essential for smart city planning. While Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various domains, existing benchmarks that explore their performance in urban environments remain limited, lacking systematic exploration of temporal evolution and subjective perception of urban environment that aligns with human perception. To address these limitations, we propose UrbanFeel, a comprehensive benchmark designed to evaluate the performance of MLLMs in urban development understanding and subjective environmental perception. UrbanFeel comprises 14.3K carefully constructed visual questions spanning three cognitively progressive dimensions: Static Scene Perception, Temporal Change Perception, and Subjective Environmental Perception. We collect multi-temporal single-view and panoramic street-view images from 11 representative cities worldwide, and generate high-quality question-answer pairs through a hybrid pipeline of spatial clustering, rule-based generation, model-assisted prompting, and manual annotation. Through extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5 Pro achieves the best overall performance, with its accuracy approaching human expert levels and narrowing the average gap to just 1.5%. Most models perform well on tasks grounded in scene understanding. In particular, some models even surpass human annotators in pixel-level change detection. However, performance drops notably in tasks requiring temporal reasoning over urban development. Additionally, in the subjective perception dimension, several models reach human-level or even higher consistency in evaluating dimension such as beautiful and safety. Our results suggest that MLLMs are demonstrating rudimentary emotion understanding capabilities. Our UrbanFeel benchmark will be made publicly available.", "tldr": "", "keywords": ["Benchmark", "Urban Change", "Urban Perception", "Multimodel Large Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3616452d8a3f9d076c66519fa48c65182b1d0049.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper constructs a comprehensive evaluation benchmark named UrbanFeel with a rich dataset containing 14.3k question-answer pairs to assess MLLM's understanding of temporal changes in urban environments and human subjective perceptions of urban settings. This benchmark comprises three major categories of tasks: Static Scene Perception, Temporal Change Perception, and Subjective Environmental Perception. This paper confirms through extensive experimentation the performance of current mainstream MLLMs on this benchmark and their gap relative to human experts."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Organizing urban imagery on a temporal scale offers a novel perspective for observation. On one hand, it extends beyond conventional benchmark tasks to further test the comprehensive temporal and visual understanding and reasoning capabilities of MLLMs. On the other hand, it provides datasets and fresh insights for research on urban development.\n2. Based on temporal image data, this paper proposes targeted evaluation methods such as comparison and temporal sorting, effectively supporting the benchmark process.\n3. The text and figures are easy to follow."}, "weaknesses": {"value": "I appreciate the research perspective of this work, but it still has some specific issues. If all the following issues are properly addressed, I will adjust the rating to \"Accept\".\n1. This paper does not sufficiently explore the impact and value of the proposed benchmark, which may limit its future popularity.\n2. Some figure descriptions are unclear and confusing. For example, the meanings of the numbers in Figure 2 (a) are very confusing. What is the relationship between 1100 and 2200 in the SCR subtask? And what are the meanings of yellow and grey colors? And what is the meaning of the abbr \"str\" in the \"View Type\" part? I hope the author will carefully examine every detail of these large and complex images to ensure they are self-descriptive.\n3. The details of the experimental evaluation require specific clarification, particularly regarding the accuracy metrics or calculation formulas used for each subtask, along with necessary parameters such as thresholds. Currently, I do not appear to have seen relevant descriptions in the paper."}, "questions": {"value": "1. Can the benchmark or design approach proposed in this paper be applied to research beyond smart cities, such as topics related to sustainable development? \n2. Cities may undergo a process of decline alongside development. Can MLLMs effectively distinguish such scenarios? Or do MLLMs always assume that more prosperous neighborhoods are newer?\n3. What is your open-source plan?\n4. Would incorporating reasoning processes or using simple prompts to guide MLLMs in performing basic reasoning before answering questions enhance their performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7Bdl905ZRS", "forum": "OtLC2JNGZf", "replyto": "OtLC2JNGZf", "signatures": ["ICLR.cc/2026/Conference/Submission11293/Reviewer_XNMk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11293/Reviewer_XNMk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760842429186, "cdate": 1760842429186, "tmdate": 1762922441738, "mdate": 1762922441738, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces UrbanFeel, a benchmark for evaluating multimodal LLMs on urban visual tasks, focusing on three dimensions: (1) Static Scene Perception (e.g., co-location, dominant element extraction), (2) Temporal Change Understanding (e.g., pixel-level change, temporal sequence reasoning), and (3) Subjective Environmental Perception (beauty, safety, wealth, liveliness). The data come from ~4,000 multi-temporal street-view images from 11 cities (2007–2024), expanded into 14.3K QAs using a mixed pipeline (rule-based → model-assisted → manual). The authors evaluate 20 closed/open MLLMs (GPT-4o, Gemini-2.5-Pro, Qwen-VL, InternVL, LLaVA, etc.) and claim: (i) models are decent on static tasks, (ii) models drop sharply on temporal reasoning (especially TSR), (iii) some models reach or exceed human performance on pixel-level change with panoramic inputs, and (iv) MLLMs show early signs of “human-aligned” subjective perception."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Timely topic: urban, multi-temporal, human-centered evaluation is indeed underexplored.\n\n2. Breadth of models: evaluating 20 MLLMs gives the community a rough leaderboard.\n\n3. Task diversity: binary, MCQ, sorting, open-ended — this mirrors what MLLMs actually output.\n\n4. Clear structure of 3 dimensions: static → temporal → subjective is easy for readers to follow."}, "weaknesses": {"value": "1. Lack of Novelty: The paper primarily aggregates existing datasets and tasks (e.g., Place Pulse, CityPulse, and ChangeScore) without presenting significant novel contributions. There is no new model architecture, learning objective, or groundbreaking evaluation framework introduced. The novelty is largely confined to combining pre-existing ideas into a single benchmark rather than offering a new approach or insight.\n\n2. Questionable Evaluation Methodology:\n\n- The paper uses GPT-4o as both a model being evaluated and the evaluator of other models, which introduces bias and conflicts of interest. The evaluation is thus not independent, and the scores could be inflated due to the evaluator’s own performance.\n\n- Inter-annotator agreement for the subjective tasks (beauty, safety, wealth, etc.) is not provided, which is essential for verifying the reliability of the subjective labels. Additionally, the paper does not address the cultural bias inherent in such subjective dimensions, as they are highly culture-dependent and may not generalize across regions.\n\n- There is no clear statistical analysis to validate the significance of the results. Performance differences across models (e.g., human-level performance claims) are not accompanied by statistical tests or confidence intervals, undermining the robustness of the claims.\n\n3. Dataset Issues:\n\n- The paper relies on Google Street View and Mapillary imagery, but there is no clear mention of how the dataset complies with legal and ethical guidelines for reuse. The potential issues with data licensing and privacy concerns (e.g., faces, license plates) are not adequately addressed.\n\n- The paper does not disclose whether the data used for subjective perception tasks might have been contaminated by biases from prior LLM training, as many subjective labels (e.g., beauty and safety) are culture-specific.\n\n4. The tasks in your benchmark involve single-shot answers (e.g., “Is this beautiful?”). In contrast, real-world urban analysis often requires ongoing, multi-step reasoning where feedback loops are important. How do you reconcile this simplification, and do you believe that such a limited approach can reflect the long-term reasoning and decision-making required in actual urban environments?\n\n5. The proposed benchmark includes multiple question types (binary, multiple-choice, sorting, open-ended). How do these abstract and highly controlled question formats relate to real-world urban decision-making? In real-world scenarios, urban planning, safety assessments, or aesthetic evaluations are not performed in this rigid, isolated format — how do you justify their relevance?"}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rjvUtM5FDl", "forum": "OtLC2JNGZf", "replyto": "OtLC2JNGZf", "signatures": ["ICLR.cc/2026/Conference/Submission11293/Reviewer_7xMU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11293/Reviewer_7xMU"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761999916309, "cdate": 1761999916309, "tmdate": 1762922441299, "mdate": 1762922441299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UrbanFeel, a comprehensive benchmark designed to assess the performance of Multimodal Large Language Models in urban development and subjective perception tasks. It spans three key dimensions: Static Scene Perception, Temporal Change Understanding, and Subjective Environmental Perception, leveraging over 14,300 questions derived from multi-view and multi-temporal street-view imagery from 11 global cities. The evaluation of 20 MLLMs reveals significant gaps in spatial-temporal reasoning and the alignment of models with human perception, particularly in tasks requiring long-term urban evolution understanding and subjective environmental evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. UrbanFeel introduces of a comprehensive benchmark, going beyond traditional benchmarks by integrating spatial, temporal, and subjective perception tasks, providing a holistic evaluation framework for urban scene understanding.\n2. UrbanFeel defines 11 tasks across three dimension, Static Scene Perception, Temporal Change Understanding, and Subjective Environmental Perception. This framework allows for an in-depth assessment of MLLMs, examining not only their ability to recognize static features in urban scenes but also their capacity to reason about urban changes over time.\n3. High-Quality Data and Methodology: The dataset consists of 14.3K carefully curated questions, covering a broad range of urban environments and spanning over 15 years.\n4. The paper comprehensively evaluates 20 state-of-the-art MLLMs, providing detailed insights into their strengths and weaknesses across the benchmark’s tasks."}, "weaknesses": {"value": "1. Despite the paper claims to include cities from diverse regions and covers underrepresented regions like Africa and South America, the cities selected from these two continents are Cape Town and Mexico City, which are far more developed comparing to other cities around. This might make this benchmark \"only for the developed ones\". And this also limits the generalizability of models trained on UrbanFeel, especially for culturally sensitive subjective perception tasks.\n2. While the benchmark covers urban changes over a long period, it lacks causal annotations that could provide deeper insights into the reasons behind urban transformations, limiting the ability to conduct more advanced spatiotemporal reasoning.\n3. The judgement VQAs from Fig 25-28 seems to be over subjective, and lack of proper context. Visual comparison would make more sense in these cases."}, "questions": {"value": "1. How do the authors plan to address the geographic imbalance in future versions of UrbanFeel, and how will they ensure better cultural representation in subjective perception tasks?\n2. Would it be possible to include external metadata or causal labels in future releases to help improve the interpretability and reasoning capabilities of models trained on UrbanFeel?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WW28lS5Erz", "forum": "OtLC2JNGZf", "replyto": "OtLC2JNGZf", "signatures": ["ICLR.cc/2026/Conference/Submission11293/Reviewer_2E4Z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11293/Reviewer_2E4Z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762078111517, "cdate": 1762078111517, "tmdate": 1762922440828, "mdate": 1762922440828, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes UrbanFeel, a novel and comprehensive benchmark designed to evaluate MLLMs on the crucial but underexplored tasks of temporal evolution and subjective perceptual understanding of urban scenes from a human perspective. By focusing on time-variant analysis, the work addresses a key limitation of existing urban environment benchmarks. The authors present a structured task set and an initial evaluation. The focus on time-variance and human-centric perception makes this a unique and potentially valuable contribution to the MLLM communities."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The core problem of analyzing time-variant street-level data for human-centric perceptual understanding is interesting and currently under-addressed in the MLLM community. This focus on \"change over time\" is timely and relevant.\n\n2. The paper is very well-structured and clearly written. The task design and evaluation methodology are presented in a logical manner, making the work easy to follow."}, "weaknesses": {"value": "1. Although the paper repeatedly frames the benchmark goal around assessing the models' generalization capability, the experimental results and subsequent discussion dedicated to proving or analyzing this specific aspect are surprisingly sparse. More focused experiments or discussion are needed here.\n\n2. The discussion on highly related existing work needs substantial improvement. Specifically, the authors should include and contrast their work with recent efforts like Visual Chronicles at ICCV 2025 and relevant research from the urban planning domain that studies temporal changes and subjective perceptual understanding of street view data. This is essential to fully justify the novelty.\n\n3. Key illustrative elements, specifically Figures 2 and 3, lack sufficient detail and are difficult to read at standard resolution. Improving the clarity of these figures is necessary for the reader to fully understand the task design and examples.\n\n4. More comprehensive details are required regarding the data collection process, particularly the source (e.g., if commercial street view data like Google Streetview data was used). Potential licensing/ethical/legal risks must be explicitly addressed, and more thorough documentation of the benchmark construction process is necessary for credibility and reproducibility.\n\n5. The analysis section, particularly the error analysis and the investigation into the reasoning process of the tested models, should be significantly deepened to provide greater insight into MLLM capabilities and shortcomings on this benchmark."}, "questions": {"value": "1. Given that the temporal aspect is the paper's core innovation, please significantly elaborate on the comparison with existing dynamic understanding and subjective perceptual understanding of street view.  A dedicated sub-section in Related Work contrasting task types is highly recommended.\n\n2. Please provide a more detailed analysis of the model performance, e.g.,  a qualitative breakdown of typical successful and unsuccessful reasoning chains of thinking models, and a dedicated, in-depth error analysis section categorized by the type of failure. This is critical for the paper's utility to the MLLM community.\n\n3. Please clarify the specifics of data acquisition. If the data is derived from commercial sources, please outline the measures taken to address potential licensing/copyright issues (the \"potential risks\") for open-sourcing the dataset. Furthermore, what additional detailed information will be provided alongside the release to fully document the benchmark construction process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "1eGV6lkt6o", "forum": "OtLC2JNGZf", "replyto": "OtLC2JNGZf", "signatures": ["ICLR.cc/2026/Conference/Submission11293/Reviewer_tZ4v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11293/Reviewer_tZ4v"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11293/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762099806438, "cdate": 1762099806438, "tmdate": 1762922440393, "mdate": 1762922440393, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}