{"id": "GL3Vl1NPeY", "number": 7178, "cdate": 1758010725202, "mdate": 1759897868470, "content": {"title": "Parallel Test-Time Scaling with Multi-Sequence Verifiers", "abstract": "Parallel test-time scaling, which generates multiple candidate solutions for a single problem, is a powerful technique for improving large language model performance. However, it is hindered by two key bottlenecks: accurately selecting the correct solution from the candidate pool, and the high inference latency from generating many full solutions. We argue that both challenges are fundamentally linked to verifier calibration. A well-calibrated verifier not only improves answer selection, but also enables early-stopping strategies to reduce latency. However, existing verifiers are limited as they score each candidate in isolation, overlooking rich contextual information across the set of candidates. To address this, we introduce the Multi-Sequence Verifier (MSV), the first verifier designed to jointly process all candidate solutions and model their interactions. MSV achieves state-of-the-art calibration, which directly enhances best-of-N selection performance. We further introduce a streaming MSV variant that empowers a novel early-stopping framework. Our novel framework fully leverages parallel decoding, which contrasts with the existing multi-sequence early exit works that decode sequences one by one and thus incur significant latency. In this novel setting, MSV can achieve the same target accuracy with around half the latency that would be required with its counterpart that scores each solution in isolation.", "tldr": "", "keywords": ["test-time compute", "calibration"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/05212719e9c1b4ee4769371d932265dfe23fae50.pdf", "supplementary_material": "/attachment/9d03a9e412404eda618b0fdffcc3557c5e1eea26.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Multi-Sequence Verifier (MSV), a training algorithm to train a verifier model for test-time scaling, that can score multiple sequences in parallel, importantly taking into account all other sequences when computing a score for a sequence. This contrasts it from sequence-level methods like best-of-n with external verifiers.\n\n**I think the paper should be accepted.** However, some crucial aspects should be addressed by the authors (see \"Weaknesses\" below), most importantly a more detailed latency breakdown, and comparing against other commonly used baselines (e.g. BoN with external verifier)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "### Writing\nThe paper is very well written and well structured. All concepts are cleanly defined and explained (except a unifying overview of the method, see \"Weaknesses\" below).\n\n### Contribution\nThe proposed method seems quite novel and interesting. It features various novel ideas (see section 3).\n\n### Results\nThe results are look promising. Not only does MSV seem to lead to accuracy gains, but it also seems to lead to well-calibrated models, which is possibly independently relevant to the research community."}, "weaknesses": {"value": "### Calibration\nFrom a theoretical point of view, it's not clear (at least to me) that the proposed procedure of computing $\\tilde{y}$ (which I'm assuming is used as the predictive probability $p$ in section 3.3) actually enforces calibration (even if the experiments show this happens empirically). Yet the authors make it seem like this naturally follows from the proposed training objective (e.g. in line 299). If this is indeed supported by theory, the authors should expand on it.\n\n### Relation to Existing Literature\nI believe some of the proposed setting in which MSV is trained, e.g. terminal answers vs streaming answers, the idea of extracting intermediate answers at delimiters, etc., is insufficiently put into context w.r.t. previous literature. If all of these ideas are novel, this should be made more clear, but I would assume that in particular the \"terminal answers\" vs \"streaming answers\" viewpoint has been studied before.\n\n### Method\nIt would help to understand the method better if the authors provided an algorithm of how MSV is trained, that combines all the parts of section 3 and makes it easy to grasp what parts are being trained and how (as there are various trainable parameters scattered throughout section 3).\n\n### Experimental Results\nThe proposed MSV contains various design choices that seem somewhat arbitrary, e.g. the choice of masks (section 3.2). Ablations over these masks and their affect on downstream performance would be helpful.\n\nFurthermore, while the authors claim they show accuracy-latency tradeoffs, e.g. in Figure 7, they actually usually just show accuracy-token position tradeoffs. The only latency results I could find are in Table 4 (line 864 ff), and those are not very explicit. For example, how is \"decoding\" in this table defined? Is this only the latency for decoding tokens that would be generated as part of the sequences themselves, or does this include latency for decoding of the intermediate extracted answers? (Which should go into the latency overhead of MSV, but should be separated from the latency of BoN or other baselines.) The authors should include a more detailed latency comparison, that shows training time/compute, and a detailed breakdown of the latency overhead that MSV adds to simpler baselines.\n\nThe authors compare to various baselines in their experiments. However, a) some of these baselines are not explained at all (e.g. Probe + WV). Moreover, one baseline, MSV 1, is their own method in a single-sequence setup. Comparing to other, commonly used methods in the literature would make the experimental results much stronger. (E.g., how does this compare to simple BoN with an external verifier?)"}, "questions": {"value": "- minor: some of the fonts in figures and plots are too small to read (e.g. Figures 1, 4, and many of the tables)\n- minor: line 895: \"since forward pass\" -> \"since *a* forward pass\"\n- sometimes the authors write \"multi-sample verifier\" instead of \"multi-sequence verifier\" (caption of Figure 1, and line 59)\n- line 158: \"to end of $n$th sequence\" -> \"to *the* end...\"\n- minor notational inaccuracy: In the \"Streaming Answers\" setting, the authors should don't clarify which of the $K^{(n)}$ answers is the terminal one (I assume it's $a^{(n)}_{K^{(n)}}$, since the other answers are defined to correspond to the intermediate steps; this should be clarified though since saying that $a^{(n)}_1$ is the terminal answer in the terminal answers setting seems to contradict this notation)\n- it would help to explain in more detail at what points in the generation intermediate answers are extracted in the streaming answers setting. The authors say \"whenever a delimiter [...] is encountered\". Could you say what delimiters the LLM you're using can produce exactly (any other than \"wait\"?), how often this happens, etc.?\n- how do the \"terminal answer\" vs \"streaming answer\" viewpoints relate to existing literature?\n- in the input concatenation (line 202), how do you concatenate exactly? Is there any delimiter/special structure by which you concatenate answers, or just raw concatenation? (Looking at the following section defining the masks, it is probably raw concatenation, but why this is reasonable only becomes clear in the following section, it might help to clarify this here.)\n- the difference between \"within-sequence mask\" and \"within-answer mask\" could be made more clear, both in text as well as in Figure 1 where they look identical. (As I understand it, the former is all answers within that sequence up to a time $t$, while the latter is only one answer in that sequence, but it's not entirely clear.) In particular, there's some notation here (ans(u), seq(u), step(u)) that could be defined more clearly.\n- the definition of the final feature (line 256) is not entirely clear to me. Why only take the last token's representation (and not a (weighted) mean across tokens)? Does the MLP simply map from $\\mathbb{R}$ to $\\mathbb{R}^d$?\n- maybe I'm missing it, but it seems like the correctness probability $p$ (section 3.3) is not defined anywhere. Is this simply the predicted $\\tilde{y}$?\n- the training set seems to be very small (224 problems, cf. line 730). Could the authors elaborate on why the training set is chosen so small? How does the validation error look like during training, how long does training take, etc.? If MSV can be trained quickly, this could be highlighted as an advantage in the paper.\n- the end of section 2.2 seems to be missing\n- line 81: \"... that latency doesn't grow\" -> This claim doesn't seem to be supported anywhere (in particular, I was not able to find latency tradeoffs as $N$ grows). Also, if latency doesn't grow, scaling beyond $N=64$ would be interesting to see."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ztz7GvOqCn", "forum": "GL3Vl1NPeY", "replyto": "GL3Vl1NPeY", "signatures": ["ICLR.cc/2026/Conference/Submission7178/Reviewer_cGUP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7178/Reviewer_cGUP"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761514559282, "cdate": 1761514559282, "tmdate": 1762919338121, "mdate": 1762919338121, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper aims to address two key bottlenecks in parallel test-time scaling for large language models (LLMs)—accurate selection among multiple candidate solutions and high inference latency—by introducing the Multi-Sequence Verifier (MSV). This improved calibration directly enhances best-of-N selection accuracy and enables more reliable confidence estimation. The authors further propose a streaming variant of MSV that supports a novel parallel early-stopping framework: by evaluating intermediate outputs from all sequences simultaneously during parallel decoding, decoding can terminate as soon as any sequence reaches a confidence threshold."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "Overall, the work makes three key contributions:\n (1) a novel design of MSV as the multi-sequence joint verifier, \n(2) the demonstration that superior verifier calibration directly improves parallel scaling performance, \n (3) the introduction of a practical, low-latency parallel early-stopping framework enabled by streaming MSV, which fundamentally rethinks how test-time compute can be scaled without proportional latency costs."}, "weaknesses": {"value": "Overall, the paper presents a moderately novel approach but falls short of a truly significant conceptual leap. The work proposes two main contributions: (i) an improved verifier for best-of-N selection, and (ii) a streaming early-stopping framework.\n\n(1) For the first, the core innovation lies in explicitly incorporating the proportion of sequences that produce symbolically equivalent answers (i.e., consensus frequency) as an auxiliary feature, while still relying on standard attention mechanisms to model cross-sequence interactions. While this integration of frequency-based signals is sensible, it builds incrementally on existing ideas like self-consistency rather than introducing a fundamentally new paradigm. \n\n(2)Regarding the second contribution—the streaming answer setting—the motivation is strong and practically relevant. However, the implementation is limited: intermediate answers are extracted only when a predefined delimiter token (e.g., “Wait”) appears, which relies on ad hoc prompting rather than genuine analysis of the model’s internal reasoning states or semantic completeness. A more principled approach would involve detecting answer readiness from the model’s latent representations or logical progression, not just surface-level trigger tokens.\n\n(3) The experimental evaluation is somewhat narrow. The authors evaluate only a single base LLM (DeepSeek-R1-Distill-Qwen-1.5B), which limits the generalizability of the findings. More importantly, the baselines omit Self-Consistency—a standard and highly relevant method in this domain that selects answers based purely on occurrence frequency without any verifier. This omission makes it difficult to assess whether the gains from MSV truly stem from its joint modeling capability or simply from using any verifier at all. Additionally, given the rapid progress in open-source LLMs, it would strengthen the paper to include stronger and more recent models (e.g., Qwen3 or Llama-3.1) as base generators to demonstrate the robustness of MSV across architectures.\n\n(4) The comparison is not entirely fair with respect to the paper’s central claim of addressing high inference latency. While the paper reports accuracy–latency trade-off curves (e.g., Figure 7), it lacks concrete wall-clock timing measurements (e.g., milliseconds per query) that account for the full pipeline—including MSV’s own inference overhead. Since MSV processes all N sequences jointly through a multi-mask Transformer, its computational cost could be non-negligible, especially as N grows. The paper should explicitly report: (a) the end-to-end latency of MSVₙ vs. baselines (including verifier runtime), and (b) how much additional latency MSV incurs to achieve its accuracy gains. Without this, the efficiency claims remain partially unsubstantiated."}, "questions": {"value": "1.Why is the equivalence relation (∼) assumed to be perfect?\nThe method relies heavily on symbolic equivalence (e.g., via SymPy) to define answer identity and compute γ. But in many real-world tasks (e.g., open-ended QA, code generation, or non-math reasoning), such a deterministic equivalence checker may not exist. How would MSV generalize to domains where answer equivalence is fuzzy or subjective?\n\n2.How sensitive is MSV to the choice of delimiter token (“Wait”) in the Streaming setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SMswX2nGCp", "forum": "GL3Vl1NPeY", "replyto": "GL3Vl1NPeY", "signatures": ["ICLR.cc/2026/Conference/Submission7178/Reviewer_5z5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7178/Reviewer_5z5v"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761845189015, "cdate": 1761845189015, "tmdate": 1762919337677, "mdate": 1762919337677, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "### General Response\n\nWe thank the reviewers for their time and effort in reviewing our paper. We are glad that they saw many strengths in our paper. In particular, reviewers noted that the paper is clearly written, well-structured, and easy to follow (R-cGUP). They found the problem setting important and well-motivated (R-SzPu, R-5z5v), and regarded our proposed Multi-Sequence Verifier (MSV) as a novel and interesting contribution (R-5z5v, R-cGUP). Reviewers highlighted the conceptual strength of designing MSV as a joint verifier (R-cGUP), and its use in parallel test-time scaling for better accuracy and reduced latency, while also producing well-calibrated confidence scores (R-SzPu, R-5z5v, R-cGUP). They all agreed that our method empirically achieves promising improvements across diverse benchmarks, outperforming pertinent baselines such as MSV 1 and Probe (R-SzPu, R-5z5v, R-cGUP). We address their main concerns in the three common responses below."}}, "id": "1UC2M7rHWw", "forum": "GL3Vl1NPeY", "replyto": "GL3Vl1NPeY", "signatures": ["ICLR.cc/2026/Conference/Submission7178/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7178/Authors"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7178/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763558295886, "cdate": 1763558295886, "tmdate": 1763558295886, "mdate": 1763558295886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper argues that the existing verification-based methods either suffer from poor accuracy or use too much inference compute because they wait for the full solutions from the generator. To fix these issues, the paper proposes a multi-sequence verifier, a technique that processes all the candidate solutions together and terminates early even if the solution is not fully generated. In particular, there are two variants of the method: terminal answers and streaming method. Further, the framework applies a multi-mask strategy to capture interactions between multiple solutions, final answers, and equivalent answers."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles an important problem of lack of accurate, calibrated, and fast inference with existing verifiers. To mitigate this, the paper proposes multi-mask training in the final answer and streaming answer scenarios. \n\n2. Ultimately, the proposed method provides decent performance improvements across diverse evaluation benchmarks. Further, it seems to be working better than pertinent baselines such as MSV_1, and Probe.\n\n3. The paper also shows that the MSV achieves better calibration and accuracy-latency tradeoff."}, "weaknesses": {"value": "1. The experiments are performed with just one model size and model family i.e., deepseek-r1-distill-qwen-1.5B.  It would be better to try the method on more models and at various sizes. \n\n2. It feels that having many attention masks that operate on similar sequences is a bit of an overkill. If you have enabled full attention (every sequence attends to every other thing), it remains unclear why other attention masks are needed in practice. There is no ablation which shows that each attention mask adds something to the performance. \n\n3. While it is fascinating to attend to many solutions at a time, I think there are scalability issues with this paradigm. Existing thinking models can generate upto 16K tokens and you can’t control whether the first final answer occurs. The context length and latency will blow up pretty quickly in such scenarios if the number of solutions is in the order of 100s. Whereas, the vanilla verification can operate on solutions independently and start performing well."}, "questions": {"value": "Mentioned above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sxgoGe0evc", "forum": "GL3Vl1NPeY", "replyto": "GL3Vl1NPeY", "signatures": ["ICLR.cc/2026/Conference/Submission7178/Reviewer_SzPu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7178/Reviewer_SzPu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7178/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965355674, "cdate": 1761965355674, "tmdate": 1762919337129, "mdate": 1762919337129, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}