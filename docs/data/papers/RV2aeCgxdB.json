{"id": "RV2aeCgxdB", "number": 14953, "cdate": 1758246078958, "mdate": 1759897339433, "content": {"title": "ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation", "abstract": "As large language models (LLMs) become increasingly integrated into daily life, there is growing demand for AI assistants that are not only reactive but also proactive and personalized. While recent advances have pushed forward proactivity and personalization individually, their combination remains underexplored. To bridge this gap, we introduce ProPerSim, a new task and simulation framework for developing assistants capable of making timely, personalized recommendations in realistic home scenarios. In our simulation environment, a user agent with a rich persona interacts with the assistant, providing ratings on how well each suggestion aligns with its preferences and context. The assistant’s goal is to use these ratings to learn and adapt to achieve higher scores over time. Built on ProPerSim, we propose ProPerAssistant, a retrieval-augmented, preference-aligned assistant that continually learns and adapts through user feedback. Experiments across 32 diverse personas show that ProPerAssistant adapts its strategy and steadily improves user satisfaction, highlighting the promise of uniting proactivity and personalization.", "tldr": "Developing Proactive and Personalized AI Assistants through User-Assistant Simulation", "keywords": ["Benchmark", "Agent Simulation", "Personalization", "Proactivity"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/bbb2f43d69c2a6110d16134276afd35ced0335e3.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces ProPerSim, a simulation framework for developing AI assistants that integrate proactivity and personalization in home environments. It also presents ProPerAssistant, a retrieval-augmented AI assistant trained using ProPerSim to adapt its strategies based on user feedback. ProPerAssistant demonstrates improved user satisfaction across 32 diverse personas, highlighting its ability to adapt to individual preferences and contexts. This work bridges the gap between personalized and proactive AI systems, advancing the development of context-aware, user-centric recommendations."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper addresses an important gap by combining personalization and proactivity, offering a new perspective on AI assistant design.\n\n2. The simulation framework is well-structured and supports realistic evaluation through diverse user personas and rich feedback loops.\n\n3. ProPerAssistant demonstrates measurable improvements in user satisfaction, showcasing its practical applicability and potential for real-world integration."}, "weaknesses": {"value": "1. The dataset mentioned in the paper is neither provided in the supplementary materials nor shared with an anonymous link. This raises concerns about whether the dataset will be made publicly available in the future.\n\n2. While the paper claims to integrate proactivity and personalization as its motivation, the focus of both the dataset and the proposed method appears to be predominantly on personalization. The treatment of proactivity seems rigid, as it only involves providing suggestions at fixed time intervals rather than adapting to users' behaviors. Such an approach lacks flexibility and does not align well with real-world user scenarios.\n\n3. The paper includes comparisons with only a limited set of baselines, specifically a few variations of ProPerAssistant. However, it does not compare the proposed method against other existing personalization baselines. This omission raises concerns about the effectiveness of the proposed approach.\n\n4. The authors report performance results based on only one type of large language model (LLM) and do not conduct ablation studies on other LLMs. This raises doubts about the generalizability of their method and dataset across different models."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fAoNll5GqC", "forum": "RV2aeCgxdB", "replyto": "RV2aeCgxdB", "signatures": ["ICLR.cc/2026/Conference/Submission14953/Reviewer_b3zB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14953/Reviewer_b3zB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761097152393, "cdate": 1761097152393, "tmdate": 1762925292232, "mdate": 1762925292232, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the proactive personalization problem in the conversational ai scenario. Due to the rare study in this track, this work proposes ProPerSim to simulate this scenario and further propose ProPerAssistant which coupled with a memory system and a preference-aligned training strategy to proactively generate personalized interventions for user."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The research scenario is very interesting and to the community’s focus.\n\n* The proposed ProPerSim simulation is concrete and consider personality, environment, user’ memory.\n\n* The proposed ProPerAssistant learns from user feedback via intermediate state and user preference is novel."}, "weaknesses": {"value": "Some minors from practical:\n\n* The baseline selections are not convincing, the ProPerSim would of course outperforms than no memory, AR memory and ARS memory since ProPerSim are trained based on these configurations and contain them all. The comparisons here are more like ablation. Authors may consider different preference alignment comparisons etc. \n\n* The experiments lack the evaluation of intervention. Since it’s proactive personalization, it would be straightforward to see what the rate of successfully proper interventions is.\n\n* Latency/efficiency analysis on the daily data generation should be included."}, "questions": {"value": "* How does adopting a purely time-based framing of proactivity potentially limit the assistant's ability to respond to specific user-triggered events (which prior work used) that might require instantaneous intervention, rather than waiting for the next predetermined timestep?\n\n* Is the challenge of improving performance for these complex personas fundamentally attributable to the memory system's inability to retain the necessary temporal and content granularity required to meet these highly specific demands, given that the memory relies heavily on time block summarization?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UO8uRvPefM", "forum": "RV2aeCgxdB", "replyto": "RV2aeCgxdB", "signatures": ["ICLR.cc/2026/Conference/Submission14953/Reviewer_PbEY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14953/Reviewer_PbEY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603985381, "cdate": 1761603985381, "tmdate": 1762925291877, "mdate": 1762925291877, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this work, the authors introduce a simulation framework and benchmark for developing AI assistants that are both proactive and personalized. The framework models a simulated home environment where a user agent performs daily routines, with multiple distinct user personas and realistic activity patterns generated using GPT-4o. The assistant’s objective is to determine when and what actions to recommend to the user. To address this, the authors propose a retrieval-augmented, preference-learning assistant that maintains a memory of past interactions, generates multiple recommendation candidates, and learns from user feedback through preference optimization. Extensive experiments demonstrate that this approach substantially improves user satisfaction across a wide range of simulated personas."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The main strength of this paper lies in its timely motivation to create assistants that are both proactive and personalized. The authors create extremely controlled agent setup to study the design of such systems. Under this setup, they consider multiple constraints that the assistant need to adhere to, creating an extremely rich setting to ablate different scenarios. The authors also validate their proposed setup with a human evaluation study, which increases validity of their claims about naturalness and persona alignment.\n\nIn addition, the authors propose intuitive system that builds on 2 key components: (a) structured memory storing past experiences, and (b) DPO training based on user feedback. Through extensive evaluation, their proposed system shows consistent improvement across all their persons. Overall, this is an extremely strong paper that will be of great interest to the scientific community."}, "weaknesses": {"value": "I don't see many weaknesses with the paper. In fact, the authors have gone to extreme details in designing their system.\n\nI have a few questions with the experimental setup.\n\n(a) The definition of proactivity is narrow. Under the current definition, proactivity is reduced to timing control, not anticipation of latent goals or needs of the user. This is because the authors define it as whether an action should be taken at a particular time. However, by proactivity, I would also define it in terms whether an action can return a reward faster or change the trajectory of decisions on the user. How can that be modeled under the current framework?\n\n(b) Noisy or delayed feedback: One key missing component in the current setup is that it assumes almost perfect feedback from the user always. User Preferences can be noisy or even delayed. How would such noise be incorporated into such a setup? And how would the proposed system change under presence of such noise?\n\n(c) LLM-as-a-judge is performed with Gemini 2.0 Flash. The rubric used in table 6 is extremely broad. How would the results change when the evaluation is performed with a different LLM?"}, "questions": {"value": "Please check my questions above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "mJxFE2QwrO", "forum": "RV2aeCgxdB", "replyto": "RV2aeCgxdB", "signatures": ["ICLR.cc/2026/Conference/Submission14953/Reviewer_6KAu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14953/Reviewer_6KAu"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762410343594, "cdate": 1762410343594, "tmdate": 1762925291454, "mdate": 1762925291454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper created a benchmark for simultaneously evaluating the proactivity and personalization of an agent. The benchmark is a simulated environment where a user which is powered by an advanced LLM conducts daily activities, simulating human behaviors. At the same time, an agent observes the activities of the user and decides whether or not to interact with the user by providing recommendations. The agent will receive a score of its recommendation based on the user's personalized rubric evaluated by an LLM. The users are created to cover a broad set of persona. Besides the benchmark, the authors also provide an agent that performs well under this simulated environment by utilizing preference alignment and retrieval-augmented generation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper considers the problem of benchmarking both proactivity and personalization of an agent. It can augment the existing evaluations which mainly focus on either proactivity or personalization. \n\n2. The presentation of this paper is clear and easy to follow."}, "weaknesses": {"value": "1. The definition of proactivity is too narrow. In this proposed benchmark, proactivity is mainly described as the ability to initiate the interaction with the user by providing recommendations. On the contrary, previous works on benchmarking proactivity covers a broad range of tasks including coding, writing, and daily life scenarios. Also, the quantification/evaluation of proactivity seems to be mixed into personalization. For example, how can one infer from the evaluation results that the agent is proactive but bad at personalization?\n\n2. The proposed ProPerAssistant can be regarded as a baseline of this benchmark instead of a contribution, since there is no new innovation in this agent which mainly based on memory retrieval and preference optimization. For example, the statement at line 406 \"Notably, ProPerAssistant achieves this without relying on computationally expensive reinforcement\nlearning methods, instead leveraging an efficient DPO-based approach to learn user preferences\". This is the contribution of DPO. And I think the statement is a little bit misleading since the agent is trained with online DPO, which necessitates the collection of online trajectories, it is still expensive.\n\n3. The benchmark is not practical to use. the simulation and evaluation are both powered by advanced LLMs which induces lots of API costs. And since the evaluation is based on LLM-as-a-judge. There can be bias or hallucination. Although in section 4.3, the authors conducted manual checking of the actions and evaluations, it is a limited checking and I still concern the reliability of the benchmark.\n\n4. As an agent capable of actively engaging the interaction and infer the personality of a user based on interaction history, it should be able to generalize to users with other persona without re-training. But there is limited discussion (not sure the first paragraph in section 6.4 is under this setting?) on the generalization ability of ProPerAssistant."}, "questions": {"value": "I also raised these questions in the weaknesses part. Please refer to that part for more contents.\n\n1. How can one infer the separate performance of proactivity or personalization from the evaluation results? It seems low proactivity or bad personalization will decrease the score. I know the goal of this benchmark is to evaluate the joint performance. But it is also crucial to know what the agent is bad at currently for future improvement.\n\n2. What is the setting of Adaption Across Diverse Personas in section 6.4? What's the training set of users and what's the evaluation set?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "voOoSHC0mG", "forum": "RV2aeCgxdB", "replyto": "RV2aeCgxdB", "signatures": ["ICLR.cc/2026/Conference/Submission14953/Reviewer_z9Wn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14953/Reviewer_z9Wn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14953/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762480582848, "cdate": 1762480582848, "tmdate": 1762925290359, "mdate": 1762925290359, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}