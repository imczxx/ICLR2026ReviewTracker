{"id": "FDG2G7JDWO", "number": 24089, "cdate": 1758352607996, "mdate": 1759896782301, "content": {"title": "Reflexion: Language Models that Think Twice for Internalized Self-Correction", "abstract": "Large Language Models (LLMs) have achieved widespread adoption, yet their reliability is fundamentally undermined by their tendency to generate plausible but incorrect content, a phenomenon known as hallucination. This unreliability is a critical barrier to their safe deployment in high-stakes domains, and current mitigation strategies, such as external tool use or Reinforcement Learning from Human Feedback (RLHF), are largely reactive, treating the model as a black box and failing to correct the flawed reasoning processes that lead to these errors. This paper investigates a new paradigm: can we endow LLMs with an internalized skill of self-correction? To address this, we introduce Reflexion, a framework that trains a single, unified model to explicitly follow a generate \n→\n critique \n→\n refine reasoning trace. To enable this process-based supervision, we developed ReTrace, a novel dataset of 200,000 structured self-correction examples bootstrapped from a teacher model. Furthermore, we propose an efficient inference mechanism, Uncertainty-Triggered Deliberation (UTD), which dynamically engages this deliberative process only when the model is uncertain. Our experiments show that a Reflexion-trained 8B model significantly outperforms its baseline counterparts, achieving a 15.2% absolute improvement on the TruthfulQA benchmark and a 9.8% improvement on GSM8K. Notably, our 8B model surpasses the performance of a standard 70B model on factuality, demonstrating the immense parameter efficiency of our approach. Our findings establish that supervising the reasoning \\textit{process}, not just the final outcome, is a more direct and effective path towards building reliable AI. Reflexion represents a critical step away from reactive, black-box fixes and towards creating more transparent, trustworthy, and self-correcting AI systems.", "tldr": "We introduce Reflexion, a framework that trains language models to find and fix their own mistakes by learning a 'generate-critique-refine' process, significantly improving reliability and making smaller models perform like much larger ones.", "keywords": ["Self-Correction", "Process Supervision", "AI Safety", "Trustworthy AI", "Uncertainty Quantification", "Hallucinations", "Generative Models", "Large Language Models", "Interpretability"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/52e58bd88053df440a88cd59bfd391b32a49af4d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes Reflexion, a framework that teaches language models to self-correct by explicitly learning a three-step reasoning process: generate, critique, and refine.\nUsing a new dataset called ReTrace, which contains 200k structured examples of self-correction generated by GPT-4o, the model learns to improve its own reasoning internally.\nExperiments show that a Reflexion-trained 8B model outperforms larger baselines on factuality and reasoning benchmarks, indicating that supervising the reasoning process itself can be more effective than simply scaling model size."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper presents a novel framework, Reflexion, that enables language models to perform explicit self-correction through a structured “generate–critique–refine” reasoning process.\nIt introduces ReTrace, a large-scale dataset of 200k self-correction traces, allowing the model to internalize reflective reasoning rather than rely solely on outcome supervision.\nEmpirical results show that an 8B Reflexion model achieves significant gains in factuality and reasoning, even outperforming a 70B baseline, demonstrating impressive parameter efficiency and reliability."}, "weaknesses": {"value": "- The paper does not conduct a rigorous comparison with other methods that explicitly learn reasoning processes, such as CoT, which is based on approaches. Therefore, it remains unclear whether the proposed method is actually superior to these existing techniques.\n- Because the proposed method includes an additional refinement stage, its training cost is considerably higher. The same computational budget could be used to train baseline models with additional data instead. Since the paper does not provide results under equal computational conditions, its practical efficiency is uncertain."}, "questions": {"value": "- It is unclear why training only on the final answers leads to such a large degradation in performance. This setting corresponds to standard imitation learning, and such a substantial drop appears somewhat unnatural."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "B6WsdC47JA", "forum": "FDG2G7JDWO", "replyto": "FDG2G7JDWO", "signatures": ["ICLR.cc/2026/Conference/Submission24089/Reviewer_JPEV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24089/Reviewer_JPEV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761864672506, "cdate": 1761864672506, "tmdate": 1762942932392, "mdate": 1762942932392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Reflexion, a framework to teach language models an internalized skill of self-correction. The core idea is to train a model on a three-step reasoning trace: generate -> critique -> refine, distilled from a teacher model. The authors make three primary contributions: (1) A simple SFT method on teacher traces, (2) a new dataset, ReTrace, distilled from GPT-4o, and (3) an efficient inference mechanism, Uncertainty-Triggered Deliberation (UTD)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is generally easy to follow.\n- The SFT approach is simple and seems effective.\n- The ReTrace dataset could be a useful contribution for research on process supervision."}, "weaknesses": {"value": "The paper's potential contributions are undermined by multiple, critical flaws in its literature review, evaluation, and claims.\n- The paper's primary flaw is its complete failure to engage with the relevant literature from the last two years (the related work section stop in 2023). LLM Self-correction is an active research area, and this omission leads to unsubstantiated claims of novelty. For example, the paper frames Reflexion as a \"new paradigm,\" but this claim is unsupportable. It ignores past methods (e.g., RISE, NeurIPS 2024) that already explore iterative refinement, and it fails to position its \"single-shot\" trace against these more general approaches. The paper advocates for SFT and critiques RL. However, it fails to cite or compare against recent work (e.g., SCoRe, ICLR 2025) that directly argues offline SFT (like Reflexion's) is \"insufficient\" and that an online RL approach is necessary.\n- The empirical results are not strong enough to support the paper's claims. The qualitative analysis is anecdotal, not analytical. The corresponding section consists of only four lines and a single example, which provides no insight into the model's failure modes or the general quality of its self-correction. The performance gains on UTD are marginal.\n- The presentation of results is unorthodox, for example, the tables bold results that are not the best.\n\nref\n- [SCoRe, ICLR 2025] Training Language Models to Self-Correct via Reinforcement Learning\n- [RISE, NeurIPS 2024] Recursive Introspection: Teaching Language Model Agents How to Self-Improve"}, "questions": {"value": "I suggest the authors conduct a new, thorough literature search for 2024-2025 papers on self-correction, self-refinement, and process supervision. Consider rewrite the Introduction and Related Work sections to accurately position the paper against the actual state-of-the-art and re-frame the paper's central claim."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "pvc26RDmOL", "forum": "FDG2G7JDWO", "replyto": "FDG2G7JDWO", "signatures": ["ICLR.cc/2026/Conference/Submission24089/Reviewer_aapg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24089/Reviewer_aapg"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877413336, "cdate": 1761877413336, "tmdate": 1762942931978, "mdate": 1762942931978, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Reflexion, a framework that trains language models to self-correct through a three-step process—generate, critique, and refine. Using a large synthetic dataset and an uncertainty-based trigger, it improves factuality and reasoning."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is clearly written and well organized.\n\n2. The proposed method is conceptually straightforward and easy to implement."}, "weaknesses": {"value": "1. From my perspective, the approach mainly combines ideas from knowledge distillation and prior works such as [1,2], which limits the contribution of this paper.\n\n2. The ReTrace dataset is part of the contribution. However, the human verification of this generated dataset is quite limited, raising some concerns about its overall quality and reliability.\n\n3. The paper would be stronger with comparisons to closely related methods like ToT, STaR, or Self-Refine, which share similar motivations.\n\n4. The analysis of hyperparameters is rather brief; adding experimental evidence or ablation results would help substantiate the discussion.\n\n```\n[1] Yao, Shunyu, et al. \"Tree of thoughts: Deliberate problem solving with large language models.\" Advances in neural information processing systems 36 (2023): 11809-11822.\n[2] Lightman, Hunter, et al. \"Let's verify step by step.\" The Twelfth International Conference on Learning Representations. 2023.\n```"}, "questions": {"value": "Please see the Weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NDfV7XxBsP", "forum": "FDG2G7JDWO", "replyto": "FDG2G7JDWO", "signatures": ["ICLR.cc/2026/Conference/Submission24089/Reviewer_s46B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24089/Reviewer_s46B"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761954424318, "cdate": 1761954424318, "tmdate": 1762942931662, "mdate": 1762942931662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces reflexion, a framework aimed at reducing hallucinations in large language models (llms) by instilling an internalized self-correction capability. The authors contribute a large-scale dataset of 200,000 structured self-correction traces, bootstrapped from gpt-4o, and an inference-time mechanism that activates the full critique-refine loop selectively based on the model's detected uncertainty in its initial thought."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper demonstrates that an 8b parameter model fine-tuned with reflexion significantly outperforms its baseline counterparts and even rivals the performance of a much larger 70b model on key benchmarks like truthfulqa and gsm8k, highlighting impressive parameter efficiency."}, "weaknesses": {"value": "1.  The paper's review of related work is limited, primarily citing pre-2023 literature. it fails to adequately situate itself among numerous existing works exploring similar generate-critique-refine paradigms (e.g., self-refine, crt, self-rag), thereby undermining its claim of being a \"new paradigm.\"\n\n2.  \n- The primary result presented in Table 2 combines the effects of fine-tuning on ReTrace with the use of UTD at inference. It is impossible to determine how much of the performance gain originates from the dataset (ReTrace) versus the inference-time mechanism (UTD), respectivly. \n- In Table 3,the model trained solely on the final refined answers (`Trained only on R`) yields a score of 52.5% on TruthfulQA, which is very close to the Llama 3 8B CoT baseline score of 51.3%. Does this indicate that the primary improvement in performance does not stem from the ReTrace dataset providing superior answer knowledge, but rather from the UTD-activated self-correction process during inference? This calls into question the dataset's claimed contribution.  \n- If the ReTrace dataset is a significant contribution, its effectiveness should be demonstrated by fine-tuning models from various  models (e.g., Gemma, Mistral), not just Llama 3. Its generalizability remains unproven.  \n- The efectiveness of  the framework (Table2) lacks comparisons with other models as  baselines. It remains untested whether UTD can be applied as a plug-in inference strategy for other models, which is essential for establishing its general utility."}, "questions": {"value": "Please refer to the above weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bNT1pe3tsO", "forum": "FDG2G7JDWO", "replyto": "FDG2G7JDWO", "signatures": ["ICLR.cc/2026/Conference/Submission24089/Reviewer_etWC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24089/Reviewer_etWC"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761980423330, "cdate": 1761980423330, "tmdate": 1762942931246, "mdate": 1762942931246, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Reflexion, a process-supervised framework that teaches an LLM to “think twice” by explicitly generating a three-part trace (Initial Thought , Self-Critique, Refined Answer), using SFT on concatenated sequences. It also introduces ReTrace, a 200k-example dataset of structured self-correction traces bootstrapped from a teacher model, and an inference policy Uncertainty-Triggered Deliberation (UTD) that engages the critique-and-refine loop only when token-level surprisal signals high uncertainty. On multiple datasets spanning factuality, reasoning, and safety, an 8B model trained with Reflexion improves substantially over 8B baselines and approaches a 70B model, while UTD balances quality and compute."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The UTD approach, which dynamically triggers self-correction based on average uncertainty over recent tokens, is quite interesting.\n* The method shows good parameter-efficiency and strong results in improving accuracy, with 8B model trained on the dataset closing the gap on factuality and reasoning against a 70B baseline.\n* The ablations (R-only, no UTD and always correct) and the sensitivity analysis on window size and the uncertainty threshold are very helpful (although specific results of the sweeps are not reported)."}, "weaknesses": {"value": "* The paper claims the approach is \"a more data-efficient path to reliable reasoning\". However, it does not provide clear evidence or analysis on data-efficiency. Also, it relies on a strong teacher LLM to generate high-quality critiques, which can be costly and unscalable in practice.\n* Critically, the comparisons are against standard SFT 8/70B, or with CoT. There are no head-to-head evaluation against any other process-supervised or self-deliberation /self-refine style training method (a large body of works, e.g. [1-7]). \n* (Related to the last point) The paper claims that \"ReTrace stands as the first large-scale dataset specifically designed to teach the process of internalized self-correction\". I believe this over-claims the contribution. There are well-established, similar datasets that incorporate feedback/critic and refinement, both in general domains (e.g. CriticBench [8]) or specific domains like Math reasoning  (e.g. [4, 9]) or safety (e.g. [10]).\n* The most informative ablation (R-only) is helpful, but missing controls include: (1). training on I+R without C (to test whether critique text is essential versus just longer supervision), and (2). a two-model variant (separate critic/solver models instead of the same backbone) to disentangle performance gains from joint vs. modular training.\n---\n\n[1]. Learning to Refine with Fine-Grained Natural Language Feedback\n\n[2]. STaR: Bootstrapping Reasoning With Reasoning\n\n[3]. Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models\n\n[4]. Let's Verify Step by Step\n\n[5]. Process Reward Models That Think\n\n[6]. Self-Generated Critiques Boost Reward Modeling for Language Models\n\n[7]. Recursive Introspection: Teaching Language Model Agents How to Self-Improve\n\n[8]. CRITICBENCH: Benchmarking LLMs for Critique-Correct Reasoning\n\n[9]. ProcessBench: Identifying Process Errors in Mathematical Reasoning\n\n[10]. Training Socially Aligned Language Models on Simulated Social Interactions"}, "questions": {"value": "* Since UTD hinges on uncertainty, can you report uncertainty-related metrics such as ECE / Brier score and AURC? This would show whether Reflexion improves not just accuracy but reliability under selective prediction.\n* What are the specific results for sweeping w and tau?\n* Nitpick: I would avoid (intentionally) using the same name from a well-established prior work (\"Reflexion\"), although this can be fairly minor and depend on personal taste."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VVtBmTJ8o1", "forum": "FDG2G7JDWO", "replyto": "FDG2G7JDWO", "signatures": ["ICLR.cc/2026/Conference/Submission24089/Reviewer_ksNr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24089/Reviewer_ksNr"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission24089/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998038523, "cdate": 1761998038523, "tmdate": 1762942930706, "mdate": 1762942930706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}