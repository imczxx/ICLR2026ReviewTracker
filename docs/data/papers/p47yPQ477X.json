{"id": "p47yPQ477X", "number": 13863, "cdate": 1758223977433, "mdate": 1759897407827, "content": {"title": "ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever", "abstract": "Retrieval-Augmented Generation (RAG) is a powerful technique for enriching Large Language Models (LLMs) with external knowledge, allowing for factually grounded responses, a critical requirement in high-stakes domains such as healthcare. However, the efficacy of RAG systems is fundamentally restricted by the performance of their retrieval module, since irrelevant or semantically misaligned documents directly compromise the accuracy of the final generated response. General-purpose dense retrievers can struggle with the nuanced language of specialised domains, while the high accuracy of in-domain models is often achieved at prohibitive computational costs. In this work, we aim to address this trade-off by developing and evaluating a two-stage retrieval architecture that combines a lightweight ModernBERT bidirectional encoder for efficient initial candidate retrieval with a ColBERTv2 late-interaction model for fine-grained re-ranking. \nWe conduct comprehensive evaluations of our retriever module performance and RAG system performance in the biomedical context, fine-tuning the IR module using 10k question-passage pairs from PubMedQA. Our analysis of the retriever module confirmed the positive impact of the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points compared to its retrieve-only counterpart. When integrated into the biomedical RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on the five tasks of the MIRAGE question-answering benchmark, outperforming strong baselines such as MedCPT (0.4436). Our ablation studies reveal that this performance is critically dependent on a joint fine-tuning process that aligns the retriever and re-ranker; otherwise, the re-ranker might degrade the performance. Furthermore, our parameter-efficient system achieves this result with an indexing speed over 7.5 times faster than leading baselines, providing a practical pathway for developing trustworthy biomedical RAG systems. Our implementation is available at: \\url{https://anonymous.4open.science/r/biorag-MC-9F3D/}", "tldr": "This paper introduces a two-stage retrieval system with ModernBERT and ColBERTv2 that boosts biomedical RAG accuracy and efficiency, achieving state-of-the-art MIRAGE results with low computational cost.", "keywords": ["Medical Imaging", "LLM", "RAG", "Agents"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4fe6b7d0c23fa27078724bb0ddb5421f9025648d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a two-stage retrieval pipeline for biomedical RAG, comprising a lightweight ModernBERT bi-encoder for efficient initial retrieval and a ColBERTv2 late-interaction model for fine-grained re-ranking. The system is trained end-to-end on 10k QA pairs from PubMedQA and evaluated on the MIRAGE benchmark. Experiments demonstrate that the proposed method improves both retrieval quality and downstream RAG performance. The authors further highlight the benefits of joint fine-tuning and the efficiency of their parameter-efficient design."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The combination of ModernBERT and ColBERTv2 is well-motivated and conceptually sound for clinical information retrieval.\n- The experiments demonstrate the competitive accuracy of the proposed method, with the best average accuracy on the MIRAGE benchmark.\n- The proposed system is practically efficient, making it useful for continuously updated biomedical knowledge bases."}, "weaknesses": {"value": "- The improvement over MedCPT (0.4448 vs. 0.4436 on MIRAGE) appears marginal. It is unclear if the gain is statistically significant.\n- The choice of ModernBERT as the bi-encoder is not fully justified. It is unclear whether it outperforms established domain-specific encoders such as BioBERT or PubMedBERT for the given tasks.\n- The rationale for using ColBERTv2 as the re-ranker needs more explanation. Alternative approaches, such as the cross-encoder reranker in MedCPT, are not compared in the experiments.\n- Details on training cost and resources (e.g., time, hardware) are limited, making it difficult to assess the efficiency relative to off-the-shelf retrievers that require no additional training."}, "questions": {"value": "- The MIRAGE performance gain over MedCPT seems small. Is the improvement statistically significant?\n- Why was ModernBERT chosen as the bi-encoder? How does it compare with BioBERT or PubMedBERT on the same benchmarks?\n- What motivated the choice of ColBERTv2 as the re-ranker instead of other cross-encoder rerankers? Were any comparisons attempted?\n- Can the authors provide more training cost details, such as hardware and training duration?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HLzQz43Mi3", "forum": "p47yPQ477X", "replyto": "p47yPQ477X", "signatures": ["ICLR.cc/2026/Conference/Submission13863/Reviewer_qGRQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13863/Reviewer_qGRQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761847994090, "cdate": 1761847994090, "tmdate": 1762924381383, "mdate": 1762924381383, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage retrieval module for biomedical RAG that combines a lightweight ModernBERT bi-encoder (fast initial retrieval) with a ColBERTv2 late-interaction re-ranker (token-level precision). The authors fine-tune both stages (with several negative-sampling strategies and similarity functions) on PubMed/PubMedQA subsets, evaluate retrieval Recall@k and end-to-end RAG accuracy on the MIRAGE benchmark, and report that the ModernBERT+ColBERT pipeline (with in-batch negatives + cosine similarity) achieves the highest macro-average MIRAGE accuracy. They provide ablations showing the re-ranker improves Recall@k only when jointly fine-tuned with the retriever, and report latency/indexing numbers and dataset / training setup details."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper evaluates multiple negative sampling strategies, similarity functions, and reports retrieval (Recall@k), end-to-end RAG accuracy across 5 MIRAGE sub-tasks, plus latency/indexing tradeoffs. The ablation showing re-ranker usefulness only when jointly fine-tuned is valuable.\n- The strong emphasis on indexing speed and realistic deployment tradeoffs (indexing time, inference latency) is useful for practitioners working with dynamic biomedical corpora."}, "weaknesses": {"value": "- Marginal end-to-end gain, statistical significance unclear. The reported macro-average improvement over MedCPT is very small (0.4448 vs 0.4436).\n- Evaluation limited to one generator LLM. All RAG evaluations use a single generator (Llama-3.3 8B). While the retrieval module is the focal point, end-to-end RAG accuracy depends heavily on the generator and prompt template."}, "questions": {"value": "See Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TDTlcZ3rcX", "forum": "p47yPQ477X", "replyto": "p47yPQ477X", "signatures": ["ICLR.cc/2026/Conference/Submission13863/Reviewer_NMqc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13863/Reviewer_NMqc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953723004, "cdate": 1761953723004, "tmdate": 1762924380973, "mdate": 1762924380973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a 2stage retrieval (ModernBERT+ColBERT) to improve biomedical RAG performance. The model is jointly fine-tuned on PubMedQA and evaluated on the MIRAGE and achiev avg accuracy (0.4448) while being faster in indexing than MedCPT. Results highlight that joint retriever–re-ranker fine-tuning is essential for optimal performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The proposed techinique (ModernBERT+ ColBERT) of using  two-stage retrieval architecture is technically smart. Combining ModernBERT’s long-context bi-encoder with ColBERT’s late-interaction mechanism leverages both scalability and semantic precision. Also the results are presented with clarity and transparency such as including ablation studies that isolate the effects of similarity metrics, sampling strategies, and fine-tuning alignment"}, "weaknesses": {"value": "The MIRAGE benchmark for biomedical QA is a great starting point. however, it also means that the current experiment setup of this paper is restricted to just one domain. The paper would benefit from cross-domain/corpus validation/testing to substantiate claims of generality and scalability across different biomedical text types.\n\nI found few grammatical errors (minor ones). for examp: model name “ModernBERT” is misspelled as “ModerBERT” in line L147. Just wanted to bring attention in case missed.\n\nI believe that the paper can also benefit if little qualitative insight was also included. that would tell why certain retrieval or re-ranking errors occur. Examples of retrieved passages and failure cases could help elucidate the system’s limitations ."}, "questions": {"value": "1. This is related to the weakness section but i am curious to know if there was any additional experiments to see how the model perform on other biomedical text types such as clinical notes, EHR summaries? If it was a different domain, would that significantly impact retrieval quality?\n\n2. Would it be possible to add a qualitative analyhsis of the results so that the readers can understand common eror points?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "AV9xdCJ5Rb", "forum": "p47yPQ477X", "replyto": "p47yPQ477X", "signatures": ["ICLR.cc/2026/Conference/Submission13863/Reviewer_sB88"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13863/Reviewer_sB88"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761961988022, "cdate": 1761961988022, "tmdate": 1762924380533, "mdate": 1762924380533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a two-stage retrieval architecture for biomedical Retrieval-Augmented Generation (RAG), combining a ModernBERT bi-encoder for initial dense retrieval with a ColBERTv2 late-interaction model for re-ranking. The system is fine-tuned using PubMedQA question–passage pairs and evaluated on the MIRAGE benchmark. Results show that the ColBERT re-ranking improves retrieval Recall@k and leads to state-of-the-art average RAG accuracy (0.4448), slightly outperforming strong baselines such as MedCPT, while maintaining significantly faster indexing speed (over 7.5× improvement). The paper claims to offer a practical balance between efficiency and accuracy for biomedical QA."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The proposed method achieves state-of-the-art performance while being computationally efficient."}, "weaknesses": {"value": "- The performance is not strong enough. In table 1, the proposed method only outperforms baselines in MedMCQA and in average. However, the average accuracy is not that high when compared with MedCPT (0.4448 vs 0.4436). The recall@k in Figure 2 also shows that the proposed method doesn't outperform MedCPT.\n- The innovation is also limited. As the title suggests, the proposed method basically combines ModernBERT for retrieval and ColBERT for re-ranking.\n- The index time is largely reduced. However, given the time for reranking, the total time is longer than MedCPT."}, "questions": {"value": "- Which model do you use for DPR? I don't think this is specified in the paper.\n- Why the recall@k of MedCPT is the highest, but the scores in table 1 are not always the highest?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Wh2UR3fgLR", "forum": "p47yPQ477X", "replyto": "p47yPQ477X", "signatures": ["ICLR.cc/2026/Conference/Submission13863/Reviewer_5pjf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13863/Reviewer_5pjf"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761978497879, "cdate": 1761978497879, "tmdate": 1762924379796, "mdate": 1762924379796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}