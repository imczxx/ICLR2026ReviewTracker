{"id": "PeWBpXLUxr", "number": 17039, "cdate": 1758271467613, "mdate": 1759897202746, "content": {"title": "Trace Reconstruction with Language Models", "abstract": "The general trace reconstruction problem seeks to recover an original sequence from its noisy copies independently corrupted by deletions, insertions, and substitutions.\nThis problem arises in applications such as DNA data storage, a promising storage medium due to its high information density and longevity.\nHowever, errors introduced during DNA synthesis, storage, and sequencing require correction through algorithms and codes, with trace reconstruction often used as part of data retrieval. In this work, we propose TReconLM, which leverages a language model trained on next-token prediction for trace reconstruction.\nWe pretrain the model on synthetic data and fine-tune on real-world data to adapt to technology-specific error patterns.\nTReconLM outperforms state-of-the-art trace reconstruction algorithms, including prior deep learning approaches, recovering a substantially higher fraction of sequences without error.", "tldr": "We propose TReconLM, a language model trained on synthetic and real-world data for trace reconstruction, which outperforms existing methods by recovering more sequences corrupted by insertions, deletions, and substitutions.", "keywords": ["Trace reconstruction", "DNA data storage", "Deletion-insertion-substitution errors", "Decoder-only language models"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cd7fff90f60aec18ec1094baa863503bb5d61148.pdf", "supplementary_material": "/attachment/40b8e613756dfefb2d8cd4d685006d511d84a35e.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a method called TReconLM, which applies language models to the problem of trace reconstruction in DNA data storage. Specifically, it aims to recover the original DNA sequence from multiple noisy copies of it. The authors reframe this algorithmic challenge as a next-token prediction task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The paper is well-structured and clearly written, providing a concise and easily understandable overview of both the problem and the proposed solution.\n* It transforms a classic combinatorial optimization problem into a generative language model task. As demonstrated by the results, this approach is effective.\n* The authors also provide theoretical proof, offering a degree of interpretability for the model."}, "weaknesses": {"value": "See Questions."}, "questions": {"value": "I am not an expert in the field of DNA, but I have the following questions about this paper:\n* TReconLM is capable of handling complex errors involving insertions and deletions, yet the theoretical analysis (Section 6) only considers a simplified model with substitution errors. Is there any theoretical framework that can extend to models including insertions and deletions?\n* The paper presents a scaling law, but from a language model perspective, a 300M-parameter model is relatively small. Could the authors‘ conclusions be limited by the model size?\n* The paper only reports results on two real datasets. Could the authors provide comparisons across more real-world datasets using different models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "sqDzUBTxAc", "forum": "PeWBpXLUxr", "replyto": "PeWBpXLUxr", "signatures": ["ICLR.cc/2026/Conference/Submission17039/Reviewer_xHcq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17039/Reviewer_xHcq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761640614929, "cdate": 1761640614929, "tmdate": 1762927056852, "mdate": 1762927056852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper reframes trace reconstruction as next-token prediction: concatenate N noisy reads with delimiters (y_1 | ... | y_N) and let a decoder-only transformer generate the clean sequence. Models are pretrained on synthetic data and fine-tuned on real datasets; they outperform classical and neural baselines on synthetic data and two real datasets for length 60, 110, and 180. The authors study the scaling and provide a theory for the case where the noise is substitution plus attention visualizations showing some of the model's behaviours"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Simple formulation that beats baselines on synthetic and real data.\n- Useful scaling-law study to pick model size at fixed FLOPs.\n- Empirical interpretability via attention visualizations."}, "weaknesses": {"value": "- Results on real data use N≤10 and short L; Quadratic attention over concatenated reads may bottleneck for large N or L.  Inputs are an unordered set of traces but the method imposes an order via concatenation; no explicit permutation-invariance or ablation on trace order. Attention maps suggest same-index focus but don’t guarantee invariance. The method is really data hungry perhaps because of this. More on this in questions!\n- The theory explains only substitution; The main challenging cases, where the optimal predictor is not simply majority voting, are not studied."}, "questions": {"value": "- Beam search yields only small gains, did you try larger beams or alternative decoders? On the non-iid experiment, this was surprising, was wondering if you have thought of further justification for the irrelevance of decoding strategy.\n- What would it take e.g. in terms of wall-clock to train RobuSeqNet with matching compute as other models?\n- Since inputs are sets, did you try shuffling trace order during training or resetting positional embeddings per trace to enforce permutation invariance? \n- Are there tracing datasets out there with longer sequences that you are aware of? It could strengthen the paper significantly to study length generalization to longer sequences. I wonder if you can achieve that to some extent already by including the ordering invariance property of input sequences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4cz6NfxdiT", "forum": "PeWBpXLUxr", "replyto": "PeWBpXLUxr", "signatures": ["ICLR.cc/2026/Conference/Submission17039/Reviewer_uEFb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17039/Reviewer_uEFb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856805992, "cdate": 1761856805992, "tmdate": 1762927056290, "mdate": 1762927056290, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TReconLM, a novel LLM based method for the trace reconstruction (TR) problem. This algorithmic TR problem seeks to recover an original sequence from multiple noisy copies (traces) corrupted by insertions, deletions, and substitutions. This problem is highly relevant in the field of DNA data storage, where such errors are common during data synthesis and sequencing.\n\nThe paper's core contribution is to frame trace reconstruction as a next-token prediction task. The paper shows that a decoder-only transformer, TReconLM, can be trained to solve this task effectively. The authors' approach involves pretraining the model on synthetic data and subsequently fine-tuning it on real-world data. This two-stage process allows the model to learn generalized reconstruction patterns and then adapt to specific, technology-dependent error statistics."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written and motivated. The results are clear, where the authors’ LM outperforms several algorithmic and deep learning competitors. They evaluate their method on both synthetic and real data, and they perform several experiments in different settings (e.g., noise rate, cluster size).\n\n* The results also include a theoretical analysis of how transformers can solve the TR problem. The basic idea is to first look at logistic regression for this problem. Then extend this analysis to construct a 2-layer transformer that theoretically achieves low error for certain error types. While the analysis is relatively elementary, I think it adds a lot to the paper, showing that there is a reason why transformers should be able to solve TR well, and not just relying on the fact that training transformers on basically any task tends to work well.\n\n* The authors also perform a brief scaling law analysis, showing how both smaller and larger models perform at the task.\n\n* In the appendix, the authors also compare to few-shot prompting GPT, which is a reasonable baseline. They find that their method outperforms few-shot GPT."}, "weaknesses": {"value": "* The theoretical analysis only considers substitution errors, which are often much easier to analyze than full edit distance errors. It would strengthen the paper a lot to have new theoretical results for insertions and deletions, since this would represent a challenging theoretical problem.\n\n* Similarly, the paper does not quite explain why the transformer they train works well for the full TR problem with insertions and deletions. I think this is kind of mysterious, given that their may be many sub-optimal but empirically good algorithms for TR in practice. I would be curious to hear more about this.\n\n* Overall, the paper’s results are not especially surprising. It is kind of expected that if the model is trained on data from a similar distribution, then the transformer will learn to solve the TR problem to some extent. This doesn’t diminish the results, but it means the paper might be only of interest to a smaller community that cares about TR and DNA data storage."}, "questions": {"value": "* What happens if you try to extend the theory to insertions/deletions? Are their any concrete conjectures you can state in this case? Can you make partial progress on them?\n\n* If you trained a much larger model (e.g., 1B) or fine-tuned an existing LLM (e.g., Qwen or Gemma say with 4B to 9B params) can you get better results? In practice, it might be the case that we can afford more computation time in order to get lower error results. Also does thinking + finetuning give you anything for TR? This would be a natural extension of the few-shot explorations.\n\n* One thing that happens in the clustering step is that some strands might be clustered incorrectly and end up in the wrong cluster. This means that a cluster may contain strands that are essentially random looking, and they do not follow the error model in terms of a perturbation of the center. I would be curious to see (i) does performance degrade in this case if you add 1-2 random sequences to the cluster? (ii) do you have ideas for how to mitigate this, perhaps training with some spurious sequences."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4ZeeWza6kt", "forum": "PeWBpXLUxr", "replyto": "PeWBpXLUxr", "signatures": ["ICLR.cc/2026/Conference/Submission17039/Reviewer_wtz7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17039/Reviewer_wtz7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761942156897, "cdate": 1761942156897, "tmdate": 1762927055860, "mdate": 1762927055860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a transformer based next token prediction approach to recover DNA traces, an important problem in DNA-based data storage. Pre-Training on synthetic and finetuning on real data is shown to improve the performance over state of the art reconstruction algorithms. Experiments are performed to understand and explain the scaling behaviour where the performance doesn't linearly improve with model size."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This is a very important problem for improving DNA-based data storage retrieval speeds.\n2. The results show clear improvement and paper is clearly written, while being easy to read."}, "weaknesses": {"value": "1. The message and contribution from the paper is unclear - is it a new architecture, insight into training strategies or way to finetune on real datasets?\n2. The improvements over DNAFormer, are these obtained by finetuning on the real data? Is DNAFormer also pretrained on synthetic and finetuned on real data as well."}, "questions": {"value": "1. Please elaborate on the exact architecture of the proposed approach, how is it different from a simple transformer. Is there any innovation that the authors had to do for improving performance over DNAFormer?\n2. Are there any areas of future work that can be inspired from the author findings?\n3. Since authors use a simpler architecture, were there any latency improvements in reconstruction as well?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XaUlGPKtCs", "forum": "PeWBpXLUxr", "replyto": "PeWBpXLUxr", "signatures": ["ICLR.cc/2026/Conference/Submission17039/Reviewer_yn3a"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17039/Reviewer_yn3a"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission17039/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762497496515, "cdate": 1762497496515, "tmdate": 1762927055255, "mdate": 1762927055255, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}