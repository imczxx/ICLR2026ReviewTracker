{"id": "0TV81QK0l8", "number": 17797, "cdate": 1758280587321, "mdate": 1759897153164, "content": {"title": "3D-CoS: A New 3D Reconstruction Paradigm Based on VLM Code Synthesis", "abstract": "Most recent 3D reconstruction and editing systems operate on implicit and explicit representations such as NeRF, point clouds, or meshes. While these representations enable high-fidelity rendering, they are inherently low-level and hard to control automatically.\nIn contrast, we advocate a new \\bfunderline{3D} reconstruction paradigm based on vision-language-models (VLMs) \\bfunderline{Co}de \\bfunderline{S}ynthesis (\\bfunderline{3D-CoS}), where 3D assets are constructed as executable Blender code, a programmatic and interpretable medium.\nTo assess how well current VLMs can use code to represent 3D objects, we evaluate leading open-source and closed-source VLMs in code-based reconstruction under a unified protocol. We further introduce two generic improvements: a planning stage that produces a ratio-based, part-level blueprint before code synthesis, and Retrieval-Augmented Generation (RAG) over well-organized Blender API documents.\nTo demonstrate the unique advantages of this representation, we also present an evaluation focused on localized, text-driven modifications, comparing our code-based edits to state-of-the-art mesh-editing methods. \nOur study shows that code as a 3D representation offers strong controllability and locality, exhibiting significant advantages in edit fidelity, identity preservation, and overall visual quality.\nOur work also analyzes the potential of this paradigm and specifically delineates the current capability frontier of VLMs for programmatic 3D modeling, demonstrating the promising future of reconstruction by code.", "tldr": "We propose a new paradigm for 3D reconstruction and editing by generating code with VLMs, and evaluate it on various models.", "keywords": ["3D Reconstruction", "3D Edition", "VLM", "Code Synthesis"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2ab15eef90cf147ffb77a0a5cd5b99c985f5b1ce.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces 3D-Cos, a reconstruction framework that leverages vision-language models (VLMs) to generate 3D models as Blender Python code, given single images as input. To improve reconstruction quality, the authors propose a planning stage for blueprint inference and Retrieval-Augmented Generation (RAG) over Blender API documentation. A code-based reconstruction benchmark is introduced to evaluate the capabilities of existing VLMs in 3D reconstruction and 3D editing tasks. Results show that planning and RAG improve 3D-Cos's reconstruction performance, and that code-based 3D representation has advantages in 3D editing."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This work treats 3D reconstruction as a Python code generation problem, which allows fine-grained control and editing of the generated 3D models. \n- The proposed planning stage and RAG over Blender API documentation make sense and are shown to improve reconstruction quality over the single-call paradigm.\n- The authors introduce a comprehensive benchmark for Blender code reconstruction from images, and they evaluate several existing VLMs, including both open-source and closed-source models, on this benchmark.\n- The paper is well-written and easy to follow."}, "weaknesses": {"value": "- I would view this work as more of an evaluation paper than a methodological paper, as the proposed techniques (planning and RAG) are mostly prompt engineering efforts. While the idea of reconstructing 3D models via code generation is interesting and useful, the technical novelty is limited.\n- In the proposed pipeline, the authors consider only planning and RAG, but other techniques such as in-context learning or iterative refinement were not explored, making the study less comprehensive.\n- The proposed approach is limited to 3D shapes with relatively simple geometries. It would struggle to handle more complex shapes, such as organic shapes, as discussed in the limitations section."}, "questions": {"value": "- As mentioned in the weaknesses, planning and RAG may not be sufficient to ensure 3D reconstruction quality. It might be helpful to leverage the in-context learning capabilities of VLMs and prompt the models with a few examples. For challenging shape reconstruction, a single pass of the proposed pipeline may not guarantee satisfactory results. Incorporating a critic model to evaluate the generated shapes and provide feedback for iterative refinement could be beneficial, as demonstrated in a concurrent work LL3M [Lu et al. 2025].\n- The proposed method focuses on single-image reconstruction. This is a challenging task of course, but it would be interesting to see how the method performs with multi-view inputs, which should lead to better reconstruction quality.\n- The proposed benchmark seems not very comprehensive, for example, it lacks ground-truth for 3D editing tasks. While there is CLIP score evaluation, a perceptual user study to evaluate the quality of the reconstructed shapes is missing.\n- I did not find a promise of code and data release, which would limit the reproducibility.\n\nMinor: \n- When using \"3D edition\" in the several places of the main text, I guess the authors mean \"3D editing\".\n- Fig. 7's caption is missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "N8HZzkIKlZ", "forum": "0TV81QK0l8", "replyto": "0TV81QK0l8", "signatures": ["ICLR.cc/2026/Conference/Submission17797/Reviewer_n5AR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17797/Reviewer_n5AR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461754316, "cdate": 1761461754316, "tmdate": 1762927642395, "mdate": 1762927642395, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TLDR: Systematic evaluation of off-the-shelf VLMs for image-to-Blender-code 3D reconstruction without fine-tuning.\n\nThis paper benchmarks VLMs (Gemini, o3, Claude, etc.) on generating Blender Python code from single RGB images. Evaluates three paradigms (Single-call, Planning, RAG) on ModelNet10. Best results lag behind InstantMesh, especially on complex shapes."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Comprehensive benchmark of VLM capabilities (open-source + closed-source)\n- Unified evaluation protocol with registration and multi-view metrics\n- Demonstrates zero-shot capability (no fine-tuning) works reasonably for simple shapes\n- RAG + Planning improvements are well-motivated"}, "weaknesses": {"value": "- No feedback loop: only error correction, no visual/quality-based refinement\n- Questionable whether zero-shot approach is viable without 3D knowledge injection, basically rely on the base model.\n- Poor presentation quality: inconsistent color coding in figures (teal/orange/colorful), confusing visualization\n- Figure 7 has template caption \"Caption\" instead of actual description - indicates low attention to detail\n- Missing citations of seminal works on shape programs/DSLs (e.g., Kenny Jones et al.'s ShapeAssembly and related foundational papers that pioneered code-based 3D representation)"}, "questions": {"value": "Why 3D shapes in Figures use different color code?\nHow do you compare with MeshCoder?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7X1A8LlXEz", "forum": "0TV81QK0l8", "replyto": "0TV81QK0l8", "signatures": ["ICLR.cc/2026/Conference/Submission17797/Reviewer_gLSo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17797/Reviewer_gLSo"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971935458, "cdate": 1761971935458, "tmdate": 1762927642042, "mdate": 1762927642042, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "3D-CoS foucs on 3D reconstruction with Code Synthesis, where 3D objects as executable Blender Python code produced by large vision-language models (VLMs). \n\nThe authors propose a unified benchmark to systematically evaluate both open- and closed-source VLMs in this setting, focusing on single-image reconstruction and code-based editing. They also introduce two key improvements: a planning stage that produces part-level blueprints before code generation, and a retrieval-augmented generation (RAG) step leveraging Blender API documentation. Expeirment shows these two method enhenace the reconsturciton."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper presents a valuable benchmarking effort for evaluating VLMs on code-based 3D reconstruction.\n- The proposed technique is planning and rag shown to work effectively.\n- Overall, the benchmarking design and experimental validation represent a meaningful step toward understanding the potential of code-based 3D modeling."}, "weaknesses": {"value": "This is a compelling paradigm, but its practical scope is likely to be highly constrained. \n\n- While the idea is interesting, the paper reads more as an insight-driven study rather than a technical contribution. \n- Many of the observed improvements may naturally result from future advances in general-purpose VLM/LLM rather than from the specific domain techniques proposed here. \n\n- As such, the work feels more exploratory than methodologically substantial."}, "questions": {"value": "Using code synthesis to model simple, rigid objects is not particularly challenging, as the geometry is relatively easy to capture. What would be more meaningful is demonstrating the ability to model articulated objects that maintain correct articulation and structural relationships. It would also be interesting to examine the capability of VLMs to handle such cases."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GM62oZnxhA", "forum": "0TV81QK0l8", "replyto": "0TV81QK0l8", "signatures": ["ICLR.cc/2026/Conference/Submission17797/Reviewer_WYcf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17797/Reviewer_WYcf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992747516, "cdate": 1761992747516, "tmdate": 1762927641485, "mdate": 1762927641485, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new 3D reconstruction method, by using blender code as unified representation, authors highlight the editability for this representation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "1. This paper shows the great potential of using blender code as 3D representation, and directly reconstruct from single image. This is very valuable as it provides special insights into how VLMs understand the spatial informations."}, "weaknesses": {"value": "1. I feel using blender code for 3D reconstruction is somehow a too early exploration, it's still hard for VLMs to have a good spatial understanding, like the scale, angle, curvature. It is not precise enough. In terms of edition, it is somehow not closely connected with reconstruction task, it does show the advantages of using code for a 3D representation (which is not new), but cannot justify why we need to use the code for reconstruction.\n2. I feel this would be harder to extend to scene level reconstruction than traditional methods, as current VLMs are still struggling understanding spatial informations.\n3. The performance can be largely improved by fine-tuning a model on specific data, as discussed in sec 4.3\n4. The code representation is not possible to reconstruct texture"}, "questions": {"value": "1. Instead of directly generate blender code from input image, we can also reconstruct using traditional method, then use MeshCoder to convert these representation to blender code. What's the advantage of 3D CoS against this type of method?\n2. What is the time and token cost?\n3. What is the Failure rate of o3 and gemini?\n\nComments: In general, I feels it is weird to use blender code for 3D reconstruction, but I really appreciate authors effort in making it work! I feels this is valuable to test the spatial understanding of VLMs, but we will not end up using code for 3D reconstruction. If authors can provide more justification on this besides edition, I would appreciate it!"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OHYwj1heLT", "forum": "0TV81QK0l8", "replyto": "0TV81QK0l8", "signatures": ["ICLR.cc/2026/Conference/Submission17797/Reviewer_J3XK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17797/Reviewer_J3XK"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17797/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762158966639, "cdate": 1762158966639, "tmdate": 1762927640966, "mdate": 1762927640966, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}