{"id": "2ltBRzEHyd", "number": 23664, "cdate": 1758346867831, "mdate": 1759896802419, "content": {"title": "Chessformer: A Unified Architecture for Chess Modeling", "abstract": "Chess has played a uniquely important historical role as a testbed domain for artificial intelligence. Applying new architectures to improve absolute chess performance, and more recently to predict human moves at specified skill levels, has therefore garnered attention in the machine learning literature. Current approaches to these problems employ transformer models with widely varying architectural designs, and use unintuitive tokenization schemes that are not amenable to interpretability techniques, which hinders their applicability for teaching and human-AI interaction. We introduce Chessformer, a novel chess transformer model design that consists of an encoder-only model which processes chessboard squares as input tokens, instead of moves or the entire position, a dynamic positional encoding scheme that allows the model to flexibly adapt to the unique geometries present in chess, and an attention-based policy output design. We show that Chessformer advances the state of the art in all three major chess modeling goals: it significantly improves the chess-playing performance of a state-of-the-art chess engine, it surpasses the previous best human move-matching prediction performance with a much smaller model, and it enables substantial interpretability benefits. Our unified approach constitutes a broad advance across several important tasks in chess AI, and also demonstrates the benefits of carefully adapting transformers' tokenization systems, output systems, and positional encodings to reflect the structure of a domain of interest.", "tldr": "", "keywords": ["Transformers", "Interpretability", "Human-Aligned AI", "Chess", "Action Prediction"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d7e33215c415424d588a55369acb29b2c9266364.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work introduces a transformer architecture meant to be trained over next-move prediction in chess. The two main changes are (1) GAB and (2) formulating the policy prediction over moves more simply (from start to end square). \n\nThe authors take care to provide pseudonyms for models/works that are presumably related to their work. (Sometimes it makes it harder to understand the paper and the relative position of what they are trying contribute.)"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* The writing itself is clear\n* Figure 2 was interesting ; The tables 1,2 are clear\n* Introduces a new intuitive way of framing how to format moves (an attention-based “source-destination” policy head)"}, "weaknesses": {"value": "* The first point raised in the abstract is only stated: \"it significantly improves the chess-playing performance of a state-of-the-art chess engine.\" However, it does not describe this or give any information on this claim. While I am sympathetic to the challenge of anonymity issues, we cannot really take these 1-line statements into consideration. \n* The first (and primary?) set of results looks at the loss and accuracy of the models vs. another model (Figure 1) vs. ablations on human scores (Table 1). The differences while real and positive are not really shown to be material. GAB exceeds the Absolute approach by 0.16 percent. Even if this were statistically significant it is not obvious it is meaningful. Likewise, the Figure 1 results are similar. \n* Overall, it seems that the result is a strong one--a good architectural improvement--but that the demonstration of this based off of trust, not clear scientific ablations or clear documentation of what was done. This makes it hard to understand/evaluate the position of the paper.\n* I did not find the interp. section compelling. There is a single figure comparison (cherry-picked? randomly-picked? representative?) in the first subsection. Likewise, in the next subsection they make similar one line note about SAE-like results. Again, it is not that I totally doubt the results are real. Rather, the results are insufficiently demonstrated."}, "questions": {"value": "# Main Questions\n* What is the primary goal of this work? It seems that the architecture is being sold as separate from the \"state-of-the-art chess engine.\" When this is stated is this in reference to Table 3 (Table 3: Main results for raw playing strength.). \n* The second listed contribution, the human-emulation matching, again, seems better, but marginally so, to the point it is unclear (or not sufficiently). The third, again, is the interpretability, and it seems again likely/possibly interesting, but just not shown.\n* Do you report or comment on the stat. range or std of the scores reported in your tables? Anything to help contextualize the results would be helpful. The accuracy deltas being so small make it hard to appreciate the results. Are we near a ceiling of performance? Is the task hard or is the data stochastic? \n* Q: \"Empirically, we find that GAB is a key driver of these gains.\" Note that in Table 1, GAB also requires more FLOPS and param. Where is this clearly demonstrated?\n\n\n# Minor Questions\n* Q: In section 3.2, \"we concatenate representations of the past 7...\" How are the concat? I am reading this as \"stacking\" the information onto each of the 64 tokens. What are the final dimensions?\n* Q: Are the embeddings for the weak/strong players learned/updated? (Any comments on the relatively large dimension for this 128dim; the rest of the embeddings is 12 for the pieces + some other auxiliary information?) It is unclear what the actual/final architecture and dimensions of the model are.\n\n# Minor Notes\n* Table 2. The parameter sizes are very different from those before. I think grouping together the different models and sizes actually used into a clear section would help.\n* As shown in Table 5.3 --> As shown in Table 3,\n* L429 \"finder\"\n* L247 \"achieves its largest gains at the highest calibers of strength *shows that* Chessformer\" --> \"suggests that\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gYVBEczy7Y", "forum": "2ltBRzEHyd", "replyto": "2ltBRzEHyd", "signatures": ["ICLR.cc/2026/Conference/Submission23664/Reviewer_6GBt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23664/Reviewer_6GBt"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761805503255, "cdate": 1761805503255, "tmdate": 1762942754193, "mdate": 1762942754193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a novel transformer architecture specialized to playing chess (e.g. approximating optimal play, or for imitating human play). A key novel component is GAP (\"geometric attention bias\"), which essentially adds an additional component computed by an MLP to the pre-softmax attention scores, as a more dynamic alternative to traditional positional encodings. The new \"Chessformer\" architecture also uses a new type of policy head. Experiments for both optimal and human-imitating play show clear performance gains over other architectures, even at lower parameter counts and inference costs. The new architecture also has properties that make it easier to interpret, and the paper takes initial steps towards understanding the functions of different model components."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The architecture contains clever ideas and is well-motivated for the domain of chess.\n- The empirical results are very impressive, demonstrating clear gains over previous work and ablations, even when using over an order of magnitude less inference compute than prior work. The evaluation methodology is convincing.\n- I feel that there are lessons to be learned beyond only chess. The paper is a great example showing that a simple off-the-shelf transformer baseline can be decisively beaten in non-language domains with a more specialized architecture. And the motivation for GAP could apply to other domains as well where positions/distances aren't well-described by a static approach.\n- I found the SAE interpretability results (in appendix B.1) highly intriguing."}, "weaknesses": {"value": "- While I think some of the lessons from this paper could generalize to other domains, the target audience may still be a bit narrow.\n- The description of the architecture (in particular GAP and the policy head) could likely be made easier to follow with some figures showing those novel components"}, "questions": {"value": "1. Does GAP fully replace any positional encoding? If so that seems interesting/surprising, since if I understand correctly, GAP only directly affects attention scores, so MLPs and attention value vectors would not directly receive any positional information. Did you experiment with e.g. both GAP and an absolute positional encoding? And do you have guesses for why putting positional encodings directly into the residual stream of the transformer (rather than only attention patterns) isn't important for performance?\n2. How cherry-picked are the two SAE features shown in the appendix? E.g. did you look at 100 features and these were the only ones this interpretable, or are half the features roughly this clean?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UVwKNfUx7p", "forum": "2ltBRzEHyd", "replyto": "2ltBRzEHyd", "signatures": ["ICLR.cc/2026/Conference/Submission23664/Reviewer_FWRW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23664/Reviewer_FWRW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977152141, "cdate": 1761977152141, "tmdate": 1762942753676, "mdate": 1762942753676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents Chessformer, a transformer-based model for chess that unifies engine play, human move prediction, and interpretability on board attention. The model represents chess positions using 64 square tokens and introduces a Geometric Attention Bias (GAB), which is a dynamic positional encoding that adapts to board geometry. It also uses an attention-based policy head aligned with chess’s “from–to” move structure. Experiments show Chessformer outperforms prior models like Allie in both playing strength and human move prediction while being more efficient and interpretable."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The introduction of GAB is a creative and well-motivated innovation that aligns the model’s attention with the spatial and semantic structure of the chessboard.\n- The experiments are very comprehensive, covering both engine and human benchmarks. The ablation studies also show consistent performance gains."}, "weaknesses": {"value": "- While the Geometric Attention Bias is central to the paper’s contribution, it is only presented in pseudocode within the appendix. A main-text figure illustrating its structure, input–output flow, and how it modulates attention across the board would make the concept far more intuitive and strengthen readers’ understanding.\n- The description of how Lichess data were sampled and balanced across Elo levels is brief and lacks specific counts or sampling ratios, which may limit reproducibility."}, "questions": {"value": "- How was the Lichess 2023 dataset processed in practice? How many samples per Elo range were used, and what criteria guided the downsampling to balance skill levels?\n- You mentioned \"Chessformer models mainly adapt the GAB biases to global positional features like the game stage (opening, middlegame, endgame), rather than the locations of individual pieces.\" Why is GAB able to recognize different game stages?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zyl6ikaZ1F", "forum": "2ltBRzEHyd", "replyto": "2ltBRzEHyd", "signatures": ["ICLR.cc/2026/Conference/Submission23664/Reviewer_XrJD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23664/Reviewer_XrJD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981205724, "cdate": 1761981205724, "tmdate": 1762942753470, "mdate": 1762942753470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Chessformer, a novel transformer architecture specifically designed for the domain of chess, which significantly improves move prediction and playing strength over prior approaches. Chessformer makes multiple domain-specific architectural improvements: encoding the 64 board squares as tokens, using an attention-based “source-destination” policy head instead of naively one-hot encoding all possible legal moves, and using a Geoemtric Attention Bias (GAB) to convey positional information. Evaluations on move prediction and game-playing show that the Chessformer matches or outperforms prior approaches at a fraction of the cost."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed approach is sensible and appears to work well in practice. The empirical validation is comprehensive and shows that Chessformers match or outperform prior work at a fraction of the scale and cost. The paper conducts ablations to show that the Geometric Attention Bias outperforms traditional positional encoding schemes. The Geometric Attention Bias and the “source-destination” attention head are novel and well-suited to chess. The proposed architecture modifications promise to facilitate domain-specific interpretability research by being more suited to the geometry of chess. The paper is well-written and easy to follow."}, "weaknesses": {"value": "The main weakness of this work is that it is restricted to chess and, therefore, likely to be of marginal importance beyond the chess-ML community. Chess has served as an important testbed for many ideas in AI; however, this paper’s contribution is to make chess-specific adaptations to Ruoss et al. (2024) to obtain better performance. While the paper does a fine job at that, it is not quite clear to me what anyone outside of the chess community can learn from this work.\n\nGiven the above, the primary contribution of this paper should be to advance the state-of-the-art in the narrow subdomain of searchless chess modeling in a _reproducible_ manner, i.e., by releasing the code, model parameters, and/or the dataset. To the best of my knowledge, the paper does not address any of these aspects, unlike prior work (Ruoss et al., 2024).\n\nThe paper makes the unsubstantiated claim that “Chessformer significantly improves the chess-playing performance of a state-of-the-art chess engine” and “contributed to match wins over Stockfish in multiple computer-chess tournaments”. However, there is no empirical evidence to back up this claim.\n\nThe paper claims to use a novel encoding scheme; however, it is quite similar to the one proposed by Ruoss et al. (2024), who utilize an expanded FEN notation, i.e., 64 board states (and some additional information, which this paper also encodes). The main difference between the two approaches is that Ruoss et al. (2024) only feed the current board state to the transformer, while this paper proposes to concatenate it with the previous 7 board states.\n\nThere are a few typos:\n* L269 “name changed”\n* L352 “absolute position”\n* L355 “Table 3”\n* L376 “A recent line”\n* L429 “much finer interpretation”"}, "questions": {"value": "* How did the paper arrive at concatenating the current and the 7 past positions? It would be interesting to ablate this particular architectural choice."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "CMw1EQIzaE", "forum": "2ltBRzEHyd", "replyto": "2ltBRzEHyd", "signatures": ["ICLR.cc/2026/Conference/Submission23664/Reviewer_kykM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23664/Reviewer_kykM"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23664/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762369999298, "cdate": 1762369999298, "tmdate": 1762942753263, "mdate": 1762942753263, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}