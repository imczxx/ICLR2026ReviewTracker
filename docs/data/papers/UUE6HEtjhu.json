{"id": "UUE6HEtjhu", "number": 13313, "cdate": 1758216364424, "mdate": 1763526159437, "content": {"title": "AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory", "abstract": "Vision-language-action (VLA) models have shown promise as generalist robotic policies by jointly leveraging visual, linguistic, and proprioceptive modalities to generate action trajectories. While recent benchmarks have advanced VLA research in domestic tasks, professional science-oriented domains remain underexplored. We introduce AutoBio, a simulation framework and benchmark designed to evaluate robotic automation in biology laboratory environments—an application domain that combines structured protocols with demanding precision and multimodal interaction. AutoBio extends existing simulation capabilities through a pipeline for digitizing real-world laboratory instruments, specialized physics plugins for mechanisms ubiquitous in laboratory workflows, and a rendering stack that support dynamic instrument interfaces and transparent materials through physically based rendering. Our benchmark comprises biologically grounded tasks spanning three difficulty levels, enabling standardized evaluation of language-guided robotic manipulation in experimental protocols. We provide infrastructure for demonstration generation and seamless integration with VLA models. Baseline evaluations with SOTA VLA models reveal significant gaps in precision manipulation, visual reasoning, and instruction following in scientific workflows. By releasing AutoBio, we aim to catalyze research on generalist robotic systems for complex, high-precision, and multimodal professional environments.", "tldr": "AutoBio offers a novel simulation and benchmark with biologically-grounded manipulation tasks for precise, multimodal robotic automation in digital biology laboratories.", "keywords": ["robotics", "robot learning", "vision language action model", "biology experimental operation", "AI for science"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/82a5f7b77a8d9bb0d83df8bf2e6b0616962a7b98.pdf", "supplementary_material": "/attachment/39a01109a6f1804a67f5f0bc0c89daefb6503bae.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces AutoBio, a simulation framework and benchmark aimed at evaluating vision-language-action (VLA) models for biology lab automation. Beyond standard rigid-body interaction, AutoBio adds (i) a digitization pipeline to turn real instruments into simulation-ready assets, (ii) MuJoCo physics plug-ins for lab-specific mechanisms, and (iii) a dual rendering stack (fast MuJoCo + Blender PBR bridge) with reactive instrument UIs. The benchmark contains 16 tasks across three difficulty levels; the experiments train/evaluate π0, π0.5, and RDT on 9 tasks, plus DP/ACT imitation baselines. Results: near-ceiling performance on “easy” tasks; sharp drops on medium/hard tasks involving precision screw motions, UI following, liquid-level reasoning, and rotor symmetry."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-Motivated and Novel Domain: The paper successfully identifies a clear and important gap in current robotics research. Moving from generalist domestic tasks to specialized, high-precision professional domains like a biology lab is a logical and necessary next step for the field. The unique challenges of this domain (precision, transparency, complex tool-use, protocol-following) are well-articulated.\n2. New Simulation Features: The authors engineered new capabilities to meet the domain's demands, like an asset digitization pipeline, custom physics plugins.\n3. Rigorous Benchmark and Clear Results: The benchmark tasks are well-designed and thoughtfully scaffolded from Easy to Hard. The experimental results are stark and unambiguous: SOTA VLA models fail significantly as task complexity increases. This demonstrates that AutoBio is a challenging and valuable benchmark that is not \"solved\" and will be effective at driving future research."}, "weaknesses": {"value": "1. Lack of full, long-horizon protocol tasks: The introduction emphasizes \"Long-horizon workflows\"  as a key challenge. However, the benchmark consists of 16 discrete, relatively short-horizon tasks. The \"long-horizon\" experiment in the appendix (B.4) is weak, merely concatenating two easy tasks (open/close lid). A true test would be a multi-stage protocol combining several different primitives (e.g., pick tube, unscrew, aspirate, transfer to a new tube, place in centrifuge, operate panel).\n2. Evaluation uses 9 of 16 tasks: the main results cover a subset (3 per level). It’s unclear whether conclusions generalize across all 16 tasks; several compelling tasks in Appendix B.2 (e.g., vortex mixing, multiple centrifuge variants) are left unreported. A fuller sweep or cross-task generalization metric would strengthen the case.\n3. Analysis of VLA failures: The analysis that imitation learning fails on high-precision tasks due to \"compounding errors\"  is correct but not particularly novel. The procedural nature of the demonstrations  (i.e., \"perfect\" kinematic paths) is a known confounding factor for IL, as models struggle to mimic this non-human data. The paper would be stronger if it discussed this limitation or compared performance with teleoperated human data.\n4. No sim-to-real validation: the work is framed as a stepping stone to lab automation, yet no real-robot evaluation is presented. Given the Blender PBR bridge, a small real-robot demo (even on easy tasks) would substantially improve credibility."}, "questions": {"value": "1. The \"Operate thermal mixer panel\" task requires reading a reactive display. The analysis notes that models struggle, in part due to low-resolution inputs obscuring the numbers. Was the \"basic\" OpenGL renderer or the \"advanced\" photorealistic Blender renderer used to generate the visual data for training the VLA models?\n2. When training a single policy across all 9 (or 16) tasks, does performance degrade vs per-task finetuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "tTQjsaDmfd", "forum": "UUE6HEtjhu", "replyto": "UUE6HEtjhu", "signatures": ["ICLR.cc/2026/Conference/Submission13313/Reviewer_knDW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13313/Reviewer_knDW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761713868416, "cdate": 1761713868416, "tmdate": 1762923977443, "mdate": 1762923977443, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents AutoBio, a robotics benchmark in simulation evaluating robotic agents' ability to perform lab tasks/experiments important for biology research. It is motivated by increasing AI-based automation of scientific research and the importance and difficulty of lab research. The key contributions are a biology lab simulation environment, including implementations of simulated phenomena that are crucial for lab biology research; a benchmark of biological tasks; and systematic evaluation of VLAs and simpler imitation learning baselines. \n\nThe paper discusses the simulation implementation: first, a process for digitizing assets, particularly lab equipment. Next, physics implementations to augment existing simulation capability: thread mechanism, detent mechanism, eccentric mechanism, and quasi-static liquid. For rendering, the benchmark uses basic rendering from MuJoCo, contributes advanced rendering blending MuJoCo's simulation state with blender's rendering pipeline, and contributes a reactive user interface. The benchmark itself includes randomized scene initialization, procedural demo generation, and task evaluation via predefined status checks on various pieces of state information. Tasks come in three difficulty levels, increasing in difficulty of vision and manipulation precision as well as task requirement/language instruction variation and abstraction/complexity. \n\nThe experiments are conducted by fine-tuning VLAs (pi_0, pi_0.5, and RDT) on either 100 or 20 demo trajectories. Non-VLA imitation learning baselines are trained from scratch on the data. Results show that RDT is more consistent on easy tasks, while pi_0 and pi_0.5 are better in challenging scenarios. No significant advantage is seen from pi_0.5 over pi_0 on medium- to hard-level tasks, which the paper ascribes to high-level complexity and reasoning requirements. The paper also presents failure analysis: failures on easy tasks are usually due to gripper slippage, whereas medium and hard tasks are due to compounding precision manipulation issues; the paper suggests that this indicates a need for algorithms with stronger closed-loop learning abilities, including RL. Failures on these tasks are also due to language understanding limitations, and partial observability. These are exacerbated by low input resolutions, a limitation of the benchmark. Imitation learning baselines are able to match performance on a single task when they are trained on that task alone, but not in the multi-task setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "### Quality\n- Design makes sense and is well-explained.\n- Task difficulty clearly matters and is differentiating, which is key - I find this to be the hallmark of a good benchmark \n- Failure analysis is really promising (see weaknesses for thoughts on how to improve, but great addition) \n- Experiment suite is very good! I especially appreciate the holdout experiments, both because I feel it's important to fundamental ML science, and because I suspect this is a domain where there will be plenty of unseen info for a long time. \n- Simulation additions are clearly valuable and significant \n### Clarity\n- Incredibly well-written paper overall! Well-structured and very easy to follow. \n- Visual figures are really useful \n### Originality and significance \n- From what I understand, there are bio-lab benchmarks, but this one's design is unique and well-proven \n- Very important problem and this paper presents a benchmark that took considerable work in various aspects, so no concerns about significance."}, "weaknesses": {"value": "### Quality\n- Some simulation features presented do already exist, though I suspect not with the fidelity/design choices needed here, meaning they do not take away from the contributions of this paper. However, they would be worth adding to RW: transparency and liquids are simulated in multiple simulators including OmniGibson (BEHAVIOR-1K, Li et al. 2022), and a threading mechanism is used in TRANSIC (Jiang et al., 2024). \n- Failure analysis needs more detail. Would benefit from examples of failure modes, to convince the reader that the explanations are correct \n- Rendering issues leading to low-quality visual observations is a failure of the benchmark, not the method, and should be attributed accordingly \n- Some analysis/stats on the quality of the simulation would be valuable, though I think it's less central than better exposure of the method results \n### Clarity \n- Results are presented in simple tables. This makes them very hard to follow, especially for a benchmark where all the statistics are important. It would really help to have charts. This should include direct comparison between the baselines and VLAs, comparison between similar tasks, and direct comparison between difficulty levels, as in my opinion this is the most salient comparison toward judging the quality of the benchmark\n- Overall, results section just needs more structure. The figures I talked about are a big part of it, but also a claims-driven structure - right now it's quite an info dump, which is hard to follow. \n\nThe reason for the 6 is mainly that I want the results to be presented better. Given that, I would recommend a higher score."}, "questions": {"value": "- What is the difference between \"what\" and \"why\" in Fig 1? \n- Questions about grounding failure analysis, as listed in Weaknesses\n- Where do the procedural policies for demo generation come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k88y611rWe", "forum": "UUE6HEtjhu", "replyto": "UUE6HEtjhu", "signatures": ["ICLR.cc/2026/Conference/Submission13313/Reviewer_yC5L"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13313/Reviewer_yC5L"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960180437, "cdate": 1761960180437, "tmdate": 1762923977028, "mdate": 1762923977028, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a high-fidelity simulator targeting the biology lab automation domain for robotics. This domain has particular characteristics, like the preponderance of specialized machines (e.g. thermal mixers, centrifuges), clear / colored liquids, and transparent/translucent material (e.g. test tubes) which present difficulties for off-the-shelf robotic simulators like MuJoCo and its base OpenGL renderer. To solve this, they introduce a pipeline which digitizes real lab instruments through a 3D Gaussian Splat representation, adds  MuJoCo plugins for specialized physics (e.g. thread, detent, and eccentric mechanisms, liquid deformations) and more realistic rendering for transparent liquids through Blender physically-based rendering (PBR) and dynamically loading texture maps to make simulated machine control panels and displays dynamic.\n\nIn addition, given this simulator, the authors then develop a benchmark targeting 16 biologically-grounded tasks across three difficulty levels (easy, medium, hard) to systematically evaluate language-guided robotic manipulation in lab protocols. They then build infrastructure to generate 100 demonstration trajectories and then carefully evaluate state-of-the-art VLAs and smaller imitation learning baselines after fine-tuning them on the demonstrations."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Overall, the paper provides clear motivation for a focused effort on targeting simulation for the biology lab use-case, a significant area of both research and industrial importance and therefore has the potential for high impact.\n\nThe work is a high-quality, well-executed set of improvements targeting the precise difficulties of simulation in this domain. In particular, the improvements to base Mujoco (both physics and rendering) are extremely relevant to the domain. In particular, interacting with the control panels / displays of specialized machines is relatively rare in robotic simulation, and dynamically loading texture maps to achieve dynamic feedback is an example of a small but highly important infrastructural addition.\n\nThe primary originality of the work lies in the comprehensiveness and thoughtfulness of new techniques targeting the domain itself. While each technique (e.g. adding fluid modeling) is not extremely novel, the breadth of these which target domain-specific simulation issues demonstrates creativity.\n\nFinally, the careful set of benchmark tasks and baseline evaluation of VLAs / imitation learning policies is likely to be highly impactful, demonstrating the clear gap between current SOTA methods on this benchmark. The analysis of these results is thorough."}, "weaknesses": {"value": "Especially given that most of the work is aimed at achieving stronger realism, the paper would be strongly improved by any real-world experiments demonstrating that the methods result in transfer onto real robotic hardware.\n\nA significant aspect of the domain is executing longer-term procedures, and much of the work is motivated by the long task horizons. However, the main text has minimal emphasis on this aspect in the experiments / tasks, and the long-horizon task in the appendix is simply concatenation of short trajectories.\n\nThe simulation is also focused on static manipulation, which may preclude true autonomous execution of multiple steps in more realistic lab environments given the lack of mobility.\n\nThe evaluation trained separate models for each task and evaluated tasks independently, while VLAs are motivated by cross-task transfer and language generalization; evaluation of a VLA trained and tested across multiple tasks could be informative."}, "questions": {"value": "As you mention, low input resolutions exacerbate instruction following. Is this an issue with the task setup in the \"Operate thermal mixer panel\" which actually prevents the task from being fully executed?\n\nCan you expand on the physical realism of the \"aspirate with pipette\" task? Presumably some level of tactile or force feedback is necessary for working with pipettes in the real world in order to control the amount of liquid aspirated.\n\nIs there any value in providing instrument readings as an observation to the policies directly? Presumably some lab devices are IoT-enabled, and it could be a reasonable test of whether perception is fundamentally limiting policy performance or not.\n\nDo you expect increasing the number of demonstrations to significantly affect performance on the medium/hard tasks? Further investigation on data scaling effects (perhaps for a subset of the tasks) could be highly informative."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rMNnjeY8RH", "forum": "UUE6HEtjhu", "replyto": "UUE6HEtjhu", "signatures": ["ICLR.cc/2026/Conference/Submission13313/Reviewer_CmrM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13313/Reviewer_CmrM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976135291, "cdate": 1761976135291, "tmdate": 1762923976704, "mdate": 1762923976704, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes AutoBio, a simulation benchmark designed specifically for laboratory tasks and environments. The authors provide a laboratory equipment asset generation pipeline, relevant physics plugins, rendering which supports e.g. transparent materials, and a data generation pipeline. The authors also benchmark VLA and IL baselines on tasks of varying levels of difficulty."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Useful task suite in an area of robotic manipulation with few realistic benchmarks\n- Careful consideration of physics, rendering, assets, etc in the context of biology tasks\n- VLA and IL baselines are relevant and highlight weaknesses in more complex tasks\n- The presentation is clear and contributions well-explained"}, "weaknesses": {"value": "- Seeing as the realistic assets, physics, and rendering are a central focus, validation on a real robot setup (even on the simpler tasks) would support claims of realism\n- The paper notes VLAs may perform well as multi-task agents in the discussion, however this setting is not evaluated"}, "questions": {"value": "- Are benchmarks on simulation performance available (e.g. simulation speed) to gauge evaluation speed, and potentially the applicability of online learning methods?\n- The authors note the potential multitask capability of the VLA models. Did authors run multitask experiments on the VLA and IL models?\n- Are there future plans to provide additional tasks (e.g. longer horizon, more subtasks, etc), leveraging the same physics and rendering framework?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EkyMYUsEdL", "forum": "UUE6HEtjhu", "replyto": "UUE6HEtjhu", "signatures": ["ICLR.cc/2026/Conference/Submission13313/Reviewer_qBEk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13313/Reviewer_qBEk"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13313/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979475215, "cdate": 1761979475215, "tmdate": 1762923976207, "mdate": 1762923976207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}