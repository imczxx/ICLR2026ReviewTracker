{"id": "UJqXhFFzKu", "number": 24844, "cdate": 1758361005687, "mdate": 1759896745792, "content": {"title": "Learning Continually at Peak Performance with Continuous Continual Backpropagation", "abstract": "Training neural networks under non-stationary data distributions, as in continual supervised and reinforcement learning, is hindered by loss of plasticity and representation collapse. While recent approaches employ periodic, full neuron reinitialization to sustain gradient flow and restore plasticity, they often sacrifice asymptotic performance and still suffer frequent collapse. To address these limitations, we propose Continuous Continual Backpropagation (CCBP), which instead continuously, partially resets units. Empirically, CCBP preserves the long-term performance of standard optimizers while maintaining the plasticity of reset-based methods, and uniquely prevents policy collapse. Ablations further show how CCBP can be tuned to smoothly trade off plasticity and asymptotic performance, highlighting gradual reinitialization as a promising direction for continual deep learning.", "tldr": "We introduce Continuous Continual Backpropagation (CCBP), which introduces utility-scaled partial resets inplace of full neurons to maintain peak performance throughout Continual Reinforcement Learning", "keywords": ["Continual Learning", "Reinforcement Learning", "Plasticity", "Dormant Neurons", "Optimizer", "Continual Reinforcement Learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7a4c827fdaf317a4157c11eeac44cdc875d80924.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes Continuous Continual Backpropagation (CCBP), an extension of Continual Backpropagation (CBP), to maintain neural network plasticity in continual reinforcement learning. Unlike CBP which uses binary thresholding to identify and reinitialize dormant neurons, CCBP employs a continuous reset mechanism for network units, aiming to stabilize learning in continual learning setting."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- CCBP demonstrates improved performance over baselines in two continual RL tasks.\n- The paper includes a range of ablation and analysis experiments that help validate the effectiveness of the proposed method."}, "weaknesses": {"value": "- Figure 1 lacks error bars, making it difficult to assess the statistical significance of performance differences.\n- The modification from CBP to CCBP appears incremental, with limited conceptual novelty.\n- The definition of dormant neurons in Section 2.2 is problematic. For instance, under Tanh activation, a neuron that consistently outputs +1 (saturated but active) would have a high mean activation relative to the layer average and thus not be classified as dormant, even though it contributes no useful gradient signal.\n- Experiments are restricted to only two continual RL environments, limiting the generalizability of the claims.\n- CCBP introduces two additional hyperparameters, increasing tuning complexity.\n- Line 7 in Algorithm 1 should be placed closer to Equation 6 for improved readability and logical flow."}, "questions": {"value": "- How to compute the neuron-utility score $S_i^l(t)$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "i2ctI1Y5O1", "forum": "UJqXhFFzKu", "replyto": "UJqXhFFzKu", "signatures": ["ICLR.cc/2026/Conference/Submission24844/Reviewer_u1V2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24844/Reviewer_u1V2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission24844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762018500151, "cdate": 1762018500151, "tmdate": 1762943217213, "mdate": 1762943217213, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Continuous Continual Backpropagation (CCBP), a soft-reset variant of Continual Backpropagation (CBP) aimed at mitigating loss of plasticity and policy collapse in continual reinforcement learning. Instead of hard reinitializations (resetting incoming weights to a unit and outgoing weights to zero), CCBP partially pulls the incoming weights toward samples from the initialization distribution while decaying outgoing weights toward zero, using a utility-based modulation derived from per-unit gradient magnitudes. The authors evaluate CCBP on two non-stationary MuJoCo-based environments (SlipperyAnt and SlipperyHumanoid), demonstrating improved performance compared to reset-based baselines such as CBP, ReDo, and ReGraMa."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper addresses loss of plasticity, which is an important problem in continual learning.\n- The paper’s idea of softening unit reinitialization to improve stability is conceptually simple and intuitively motivated, extending prior work on reset-based plasticity maintenance. \n- Clear motivation to soften binary resets for improved performance and sustained plasticity.\n- Strong performance on the two tasks discussed (SlipperyAnt and SlipperyHumanoid)."}, "weaknesses": {"value": "- The paper's writing requires more work to improve its clarity and accuracy.\n- Limited evaluation. The authors only considered two MuJoCo tasks with a single algorithm (PPO)\n- Limited ablation. The paper only discusses the sensitivity of the replacement rate parameter. There are many components in the algorithms for which the authors didn’t provide any ablation. For example, why is using Eq. 6 important? Can't we simply use CBP and perform soft resets instead for the units to be replaced, based on CBP statistics? The paper fails to give answers to such important questions by showing a thorough analysis.\n- I think the definition of the utility function may be incomplete or incorrect. It was unclear how the authors compute the derivative of the loss wrt the activation $h_i^l$, so I had a quick look at the code, and it seems like the authors do some sort of averaging over the absolute weight gradient, which is not equivalent to what is written in Eq. 5. I might be wrong, so please correct me.\n\n\n**Less critical issues:**\n- The use of the word continuous is confusing in both the title and the paper. Having 'continuous' and 'continual' in the same name is confusing for many readers. I suggest the authors use something along the lines of “Soft Continual Backpropagation” to represent their approach, which is a soft reset version of CBP.\n- Linearized-neuron ratio and Dormant-neuron ratio definitions are presented in the background, but no references to the original papers are provided. Is the Linearized-neuron ratio a metric introduced in this work or prior works?\n- The hypothesis that hard reset-based algorithms incur instability due to hard resets didn’t hold for CBP, but the authors didn’t discuss it.\n- The authors talk about asymptotic performance in the abstract. The asymptotic behavior of a function is not defined if the function oscillates indefinitely, which is the case for the performance plots that continue to oscillate with task changes. I suggest the authors drop using this technical term and replace it with something more suitable. For example, the authors may choose to present the average task performance (averaged over the entire task) on the y-axis, allowing them to discuss asymptotic performance."}, "questions": {"value": "- How well does it work with a single environment? \n- The authors didn’t consider other settings where CCBP might be useful, such as with high replay ratios or with other algorithms/networks/action-space types/etc. How well do the authors expect CCBP to perform?\n- In line 038, “observe that an increasing number of neurons lay dormant and do not activate throughout training under these conditions” – what conditions?\n- In the abstract: “CCBP preserves the long- term performance of standard optimizers” what does that mean exactly? Are the authors comparing CCBP with Adam in stationary setting? The sentence is vague.\n- In line 243, “full partial reset” is an oxymoron. I think the authors need to drop the word full?\n- “Let $u_l$ i be a per-layer normalized utility, with mean 1” — I don’t see why the mean is 1. Can the authors explain why? \nIn line 306, “rollout size of 2048 ×32 ×5” — what do 32 and 5 represent here? which one is per-enviornment rollout length?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hIyzhMnfji", "forum": "UJqXhFFzKu", "replyto": "UJqXhFFzKu", "signatures": ["ICLR.cc/2026/Conference/Submission24844/Reviewer_iU14"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24844/Reviewer_iU14"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission24844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762119294642, "cdate": 1762119294642, "tmdate": 1762943216846, "mdate": 1762943216846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new algorithm for continual learning, called Continuous Continual Backpropagation (CCBP). CCBP resets low utility neurons in the network in a softer manner than previous resetting methods. The paper claims that smoother resets lead to more stable learning, which improves performance. The method is evaluated on two non-stationary RL environments. In both environments, CCBP outperforms all prior neuron-reinitialization methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Empirical analysis is well done. A wide range of settings for hyperparameters is tested for all algorithms, and all algorithms are evaluated on 15 seeds.\n* CCBP is a good way to extend prior neuron-reinitialization methods. The specific proposal for smoothing resets allows for fine-grained control on how exactly neurons should be reset. \n* CCBP performs well in both of the tested environments. The performance of CCBP is particularly impressive in Slippery Humanoid."}, "weaknesses": {"value": "Limited environments: The proposed algorithm is only evaluated on two environments. That is not sufficient. The algorithm should be tested on other environments. Non-stationary RL environments, such as the sequential Atari environments proposed by Abbas et al. (2023), or continual supervised learning problems like class-incremental CIFAR, and even simpler problems like random label MNIST, could be useful for validating the effectiveness of CCBP and its sensitivity to its hyperparameters.  \n\nExtra hyperparameters: \n* The algorithm adds five new hyperparameters. Two of these hyperparameters, \"decay-rate\" and \"update-frequency,\" are fairly intuitive and easy to set in most applications. But for the remaining three hyperparameters, there is no clear intuition on how to set them.\n* The authors already conducted a wide grid search for CCBP's hyperparameters on the two non-stationary RL environments. The results for those experiments can reveal the sensitivity of CCBP to these hyperparameters. Further evaluation of CCBP in other environments will further reveal the sensitivity of CCBP to its hyperparameters. \n* Another suggestion is to plot Equation 6, with utility ($u_i$) on the x-axis and the fraction of reset ($r_i$) on the y-axis, for different values of $\\kappa$, $\\tau$, and $\\rho$. These plots will help the reader understand CCBP.\n* And perhaps most importantly for hyperparameters, Table 3 in the appendix shows that the highest value of threshold tested was 0.95, and it was the best-performing value in both environments. This raises the obvious question: What if the threshold, $\\tau$, was 1? Would it perform better or similar to 0.95? If it does, then the default value of $\\tau$ can be set to 1. That eliminates both hyperparameters $\\kappa$ and $\\tau$, and essentially Equation 6, making CCBP much simpler and easier to use."}, "questions": {"value": "1. What is the unit on the y-axis of Figure 1b? What does \"collapse frequency\" mean? I think it's the number of seeds (out of 15) in which the algorithm collapsed. But please make it clear in the figure exactly what the y-axis is.\n2. Where is evidence for the first contribution? The first claimed contribution is \"We demonstrate that current reset-based continual learning algorithms suffer from unstable training dynamics.\" The closest thing to evidence is Figure 4. But Figure 4 merely shows that the gradient norm is lower in CCBP. Higher gradient norm does not mean \"unstable training dynamics\". Something like \"Largest total weight change in the network,\" as plotted by Dohare et al. (2024) in Extended Data Fig. 5c, is a more direct evidence of unstable training dynamics. However, even that does not directly mean unstable training dynamics. Please either remove this claimed contribution or provide more direct evidence of \"unstable training dynamics.\"\n3. In Equation 5, is $h_i$ the post-activation or the pre-activation value?\n4. I suggest writing line 7 of Algorithm 1 alongside Equation 6. Currently, Equation 6 does not fully represent the relationship between utility and the amount of reset, as well as its connection to all the hyperparameters. \n5. The name \"replacement-rate\" for hyperparameter $\\rho$ seems imprecise. \"replacement-rate\" as used in CBP has a very specific meaning of fraction of units replaced per step. However, the hyperparameter $\\rho$ in CCBP has a very different function. It determines the maximum proportion by which a unit will be reset. I suggest changing the name of this hyperparameter. Something like \"max_per_neuron_reset\" will be a more precise name.\n6. What exactly does rollout size on line 306/307 (rollout size of 2048 × 32 × 5) mean? What exactly are the dimensions of 2048, 32, and 5?\n7. Was layer normalization used in these networks? Addition of two baselines, one where learning networks have LN, and the second where these networks include LN with L2 regularization. These two baselines will provide further perspective on CCBPs' performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Un1UZ5JfbS", "forum": "UJqXhFFzKu", "replyto": "UJqXhFFzKu", "signatures": ["ICLR.cc/2026/Conference/Submission24844/Reviewer_h1wL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24844/Reviewer_h1wL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission24844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762193246295, "cdate": 1762193246295, "tmdate": 1762943216579, "mdate": 1762943216579, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes an algorithm called Continuous Continual Backpropagation. Instead of using the neuron utility score as something used to decide whether the neuron should be reset based on some threshold, as several other works do, this work uses the utility score to do soft partial resets of every neuron based on its utility score. The reset is done similarly to how Shrink and Perturb does it, but with a different strength for each neuron based on its utility. The evaluation of the algorithm is done on Continual RL environments Slippery Humanoid and SlipperyAnt where the friction coefficient int the environment changes for every task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is intuitive, it makes sense that doing partial resets of neurons could be a better way of resetting neurons, rather than thresholding.\n- The paper shows that partial resets can help mitigate policy collapse when training in a continual RL setting."}, "weaknesses": {"value": "- I think it would be important to see the performance of this method in other settings as well. This could be something like the continual learning or warmup setting from [1] or maybe even single task RL. \n- It’s not clear based on the metrics provided why CCBP is better than say CBP. The metrics of dormant/linearized neurons and of gradient norm are essentially the same, and there isn’t evidence showing that what the paper says is happening is the reason for the improved performance. \n- The hyperparameters searched over for Shrink and Perturb seem too conservative compared to what is used in the literature [1,2]. I am not sure it is a fair comparison.\n- Eq 6 is missing $\\rho$, and it seems to be called r in some places and $\\rho$ in others."}, "questions": {"value": "- How does a method like Soft Shrink and Perturb [2], that essentially does SnP on every step at a much smaller strength, do in this setting?\n- What does this line mean: “All baselines and CCBP were tuned for the stability–plasticity trade-off”?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rwoTf2Ow1F", "forum": "UJqXhFFzKu", "replyto": "UJqXhFFzKu", "signatures": ["ICLR.cc/2026/Conference/Submission24844/Reviewer_xF2J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission24844/Reviewer_xF2J"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission24844/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762478540633, "cdate": 1762478540633, "tmdate": 1762943216354, "mdate": 1762943216354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}