{"id": "At8fNDHiWw", "number": 19198, "cdate": 1758294308073, "mdate": 1759897052673, "content": {"title": "Efficient Implicit Neural Surfaces via Multiscale Residuals and Nested Training", "abstract": "Encoding input coordinates with sinusoidal functions into multi-layer perceptrons (MLPs) has proven effective for implicit neural representations (INRs) of surfaces defined as zero-level sets. This approach enables the capture of high-frequency detail and supports geometric regularization through MLP derivatives, such as the Eikonal constraint for signed distance function (SDF) fitting. However, existing methods typically rely on a single large MLP to learn the surface across the entire domain — a design that hinders efficient modeling of fine-grained details. Scaling the model may enable enhanced surface modeling, but at the cost of a larger number of MLP parameters and expensive inference, since mesh extraction or sphere tracing requires querying the MLP at many off-surface points. To address these issues, we propose M-plicits (Multiscale Implicit Neural surfaces), a multiscale framework for representing and training INRs to encode surfaces as SDFs, enabling both high-quality reconstruction and efficient inference. To increase representational capacity, we model the INR as a residual sum of MLPs, where each component captures a specific level of detail, modulated by the sinusoidal input encodings. To improve efficiency, a small MLP captures coarse geometry, while finer residual MLPs are trained within a sequence of nested neighborhoods around the zero-level set. This design concentrates modeling capacity near the surface, improving reconstruction and reducing computation by relying on coarse approximations for off-surface points. Experiments show that M-plicits achieve state-of-the-art accuracy in surface reconstruction across standard benchmark datasets, while maintaining a compact representation. Our method also supports real-time sphere tracing and efficient high-resolution mesh extraction. Code and pretrained models will be released.", "tldr": "M-plicits uses residual MLPs in a multiscale SDF framework, restricting fine networks to nested neighborhood for detail and efficiency, enabling compact models, real-time sphere tracing, and fast reconstruction.", "keywords": ["Implicit Neural Representations (INRs)", "Signed Distance Functions (SDFs)", "Sinusoidal Input Encoding", "Multiscale Neural Networks", "Residual MLP Architectures", "Sphere Tracing", "High-Frequency Detail Modeling", "Real-Time Mesh Extraction", "Efficient Inference"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/06d2af9c713f46298c986e63494680826265eae2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses a major problem with implicit neural surfaces (like SIREN): they look great but are extremely slow to render or mesh. This slowness comes from using a single, giant neural network for the entire shape.\n\nThe authors propose M-plicits, a new method that splits the network into multiple scales. A very small, fast \"base\" network learns the coarse, overall shape. Then, other \"residual\" networks are added on top to handle the fine-grained details.\n\nThe key idea is in the training: these detail networks are only trained in a very narrow band right around the object's surface, not everywhere. This concentrates the network's power exactly where it's needed.\n\nAs a result, the method is much faster. When rendering, any query far from the object only needs to run through the tiny base network, which is extremely cheap. The paper shows this approach achieves real-time rendering (beating NGLOD) and dramatically faster meshing (beating IDF) while keeping the model size incredibly small."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Solves an important problem: The paper makes pure implicit models, which are usually very slow, practical and fast. This is a really useful contribution.\n\n2. Clever training method: The core idea of \"nested training\" (only training details near the surface) is simple but works extremely well. The ablation study shows this is a key reason for the high quality and it's not just a small trick.\n\n3. Extremely fast and efficient: This is the best part of the paper. The results are impressive: It's incredibly fast at making meshes, blowing other methods (like IDF) out of the water (e.g., 1 second vs. 67 seconds). It can render high-quality shapes in real-time (70 FPS), which is even faster than the low-quality version of NGLOD. The models are tiny (very small file size), which is great.\n\n5. Good experiments: The authors did a good job with ablation studies to prove their different ideas work, like the custom normal calculation."}, "weaknesses": {"value": "1. Robustness of the Neighborhood $\\delta$: The neighborhood size $\\delta$ is a critical hyperparameter. While the paper proposes an adaptive strategy (Eq. 5) and a brief ablation (Table 8), its robustness is not exhaustively discussed. The implementation detail of \"normalizing all point clouds to the unit sphere\" may be hiding sensitivity to scale. How does the adaptive strategy for $\\delta$ perform on point clouds of arbitrary or varying scales (e.g., in meters vs. centimeters) without this normalization step?\n\n2. Missing Ablation on Number of Levels: The paper's core is a \"multiscale\" design, yet it primarily shows 2 and 3-level results. A dedicated ablation study on the number of levels (k) is missing. A systematic study is needed to show the relationship between k, the final reconstruction quality (CD), parameter scaling, and inference speed. This is crucial for understanding the true trade-off curve and demonstrating the scalability of the proposed multiscale architecture.\n\n3. Generalization to Scene-Scale Point Clouds: I noted the strong results on DTU scenes in Appendix F. However, this experiment integrates M-plicits into NeuS, which relies on multi-view images and volume rendering . This is a fundamentally different task and input modality from the main paper's focus on surface reconstruction from oriented point clouds (Sec 3, Stanford dataset). Does this imply that the core NNT method does not scale directly to large-scale, sparse, or noisy scene point clouds (e.g., from LiDAR scans) and must instead rely on image-based volume rendering to handle scene-scale tasks?"}, "questions": {"value": "See Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LfEprFSmUI", "forum": "At8fNDHiWw", "replyto": "At8fNDHiWw", "signatures": ["ICLR.cc/2026/Conference/Submission19198/Reviewer_o828"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19198/Reviewer_o828"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761652871610, "cdate": 1761652871610, "tmdate": 1762931195656, "mdate": 1762931195656, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces M-plicits, which models the surface SDF using a base network and three smaller MLPs (coarse, medium, and fine) to capture surface details at different scales. The final SDF value is obtained by summing the predictions of all networks, with each finer MLP providing residual corrections to the base SDF, leading to progressively refined outputs.\nThe key contribution of these multiple MLPs lies in concentrating the modeling accuracy near the surface, enabling efficient high-resolution mesh extraction and real-time rendering. However, the residual refinement, though aimed at capturing high-frequency signals (geometric details), provides limited improvement in surface reconstruction accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposes multiple MLPs to compute residual SDF values of points near the surface at different scales (base, coarse, medium, and fine). This hierarchical design progressively narrows the query region from distant to near-surface points, reducing unnecessary evaluations far from the surface.\n2. Efficient sampling strategies are introduced to ensure dense coverage of regions close to the surface, resulting in more effective training of the residual networks and faster inference for mesh extraction and sphere tracing. \n3. An additional neural network is introduced alongside the multiple MLPs for SDFs to represent texture attributes."}, "weaknesses": {"value": "1. The contribution of the multiscale residual MLPs to surface reconstruction accuracy appears limited. For example, in Table 1, for the Thai Statue reconstruction, the results of “Ours (fine)” and “Ours (coarse)” show no significant improvement, and for Armadillo, “Ours (coarse)” even performs better than “Ours (fine)”.\n2. Results on only four shapes in Table 1 are not compelling to support the claim of high-quality reconstruction. It would strengthen the paper to (i) include additional evaluation metrics beyond Chamfer Distance (CD) and (ii) provide visual comparisons of reconstructed meshes across different methods.\n3. How does a single base SDF network perform in terms of reconstruction accuracy if trained with a comparable number of parameters to the combined residual SDF networks? This comparison could help isolate the benefit contributed by the multiscale decomposition, which so far appears to mainly improve computational efficiency by reducing the query space near the surface.\n4. It would be helpful to visualize the reconstructed surfaces of the proposed method at different residual levels to highlight potential improvements in surface detail."}, "questions": {"value": "(1) How many points are sampled for the training of neural SDF at different scales?\n\n(2) In the neural normal mapping (Lines 282–283), only the input derivatives of the finer-level neural SDF are used. What does the gradient of the combined SDF with respect to $x$ look like, i.e., $\\nabla_x (f_1(x) + r_1(x) + \\dots + r_i(x))$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Fpm2DwVFBk", "forum": "At8fNDHiWw", "replyto": "At8fNDHiWw", "signatures": ["ICLR.cc/2026/Conference/Submission19198/Reviewer_wPDQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19198/Reviewer_wPDQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734924320, "cdate": 1761734924320, "tmdate": 1762931195219, "mdate": 1762931195219, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The M-plicits framework introduces a novel architectural and training paradigm for INRs of surfaces, specifically designed to address a fundamental challenge in the field: the inherent tension between representational capacity and computational efficiency. Traditional approaches that employ a single, large MLP to model a surface as the zero-level set of a SDF often achieve high fidelity at the cost of prohibitive inference times. M-plicits re-architects the INR to overcome this limitation by concentrating its modeling power and computational resources specifically in the regions of high geometric complexity—near the surface itself."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The strength of M-plicits lies in its pragmatic and geometrically-grounded design. provides a system that is simple, robust, and effective for the task of reconstructing and rendering high-fidelity surfaces in real time. Its ability to simultaneously achieve top-tier reconstruction quality, the highest rendering framerates, and the most compact model sizes, as demonstrated in its empirical evaluation, marks a substantial step forward."}, "weaknesses": {"value": "The paper's core weakness is its limited conceptual novelty. Its central ideas:the multiscale residual architecture, and the multi-resolution sphere tracing (cf. existing techniques) are not entirely novel."}, "questions": {"value": "1) Please discuss your multi-scale sphere tracing algorithm in the context of existing coarse-to-fine SDF rendering techniques\n2) Please clarify the fundamental conceptual difference between your \"nested neighborhood training\" and the \"cascaded optimization\" used in BANF. \n3) In Figure8,why the middle level is 96 not 128"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hkOZQa6r67", "forum": "At8fNDHiWw", "replyto": "At8fNDHiWw", "signatures": ["ICLR.cc/2026/Conference/Submission19198/Reviewer_PWMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19198/Reviewer_PWMZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761876814679, "cdate": 1761876814679, "tmdate": 1762931194765, "mdate": 1762931194765, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes to use multi-Scale residuals for efficient learning Implicit Neural Representations (INRs) of 3D surfaces. The authors propose to replace the conventional single large MLP with a cascaded architecture employing Multi-Scale Residuals, where each subsequent, typically smaller MLP learns a residual correction focusing on progressively higher-frequency geometric detail. To this end, they introduce Nested Training, a sequential optimization scheme where each residual network scale is trained by supervising only near the coarser converged zero-level set, which significantly accelerates the overall training process. Furthermore, the authors further introduce a GEMM-accelerated normal computation method and an efficient mesh extraction pipeline that  fully supports normal and texture mapping by utilizing the multi-scale features. Experiments demonstrate that the method achieves superior or comparable geometric accuracy to state-of-the-art baselines while offering significant improvements in both training and inference efficiency."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The submission is clearly written and well-organized, presenting a technically sound pipeline. The step-by-step introduction of the \ncascaded architecture and training scheme is easy to follow. The multi-scale residual architecture, combined with the Nested Training strategy, effectively addresses the inefficiency of a single large MLP. This design leads to notably faster training and inference times, making the method more practical for high-detail modeling. The paper includes clear experiments and detailed ablation studies that thoroughly justify the effectiveness of the proposed components. Crucially, the authors include additional experiments—such as surface reconstruction from posed images, training a textured SDF from images/noisy point clouds, and attribute mapping (e.g., neural texture, depth mapping)—which effectively demonstrate the method's versatility and practical utility."}, "weaknesses": {"value": "The core idea of modeling shape using a coarse network plus a finer residual correction is very similar to existing displacement field methods (IDF). Although the authors mention a simplification (L129, avoiding interpolation), the novelty of the multi-scale residual architecture should be more clearly articulated and distinguished from similar prior work in the related work and discussion sections.\n\nThe quantitative evaluation is too limited, currently relying only on Table 1 across 4 meshes. A more comprehensive assessment is necessary using standard, more complex datasets, such as models from the Thingi10K dataset as in NGLOD, to fully validate the method's scalability and robustness in capturing complex geometry. \n\nWhile the authors claim M-plicits enables fast inference through real-time multiscale sphere tracing and mesh extraction, the reported efficiency gains are not clearly superior when compared against recent works that combine Instant-NGP/hash-grids with Neus to modeling complex geometry with high efficiency. This lack of a distinct advantage should be discussed.\n\nMinor: some related works are missing it would be helpful to cite and discuss the following two papers that also explore multi-scale implicit representations for 3D shapes:\n[1] Dou, Y., Zheng, Z., Jin, Q., & Ni, B. (2023). Multiplicative Fourier Level of Detail. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).\n[2] Saragadam, V., Tan, J., Balakrishnan, G., Baraniuk, R., & Veeraraghavan, A. (2022). MINER: Multiscale Implicit Neural Representations. European Conf. Computer Vision (ECCV)."}, "questions": {"value": "In L306, it is mentioned that different $w_0$ values are used in SIREN to capture specific frequency bands. Why is the choice of $w_0$ for the fine-level network dependent on the complexity of the shape? What would be the effect if a consistently high value for $w_0$ at the fine level?\n\nThe results in Table 1 show a counter-intuitive outcome for the Armadillo dataset, where the final, corrected fine shape yields a higher (worse) Chamfer Distance than the initial coarse shape alone. What is the underlying cause for this degradation in performance following the fine-level residual?\n\nSphere Tracing Generalization: The paper proposes a real-time multiscale sphere tracing method. Can this efficient tracing method be readily applied to accelerate inference in other existing multiscale INRs, or is it tightly coupled with the specific architecture and Nested Training scheme proposed in this paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "AbDD7WACsg", "forum": "At8fNDHiWw", "replyto": "At8fNDHiWw", "signatures": ["ICLR.cc/2026/Conference/Submission19198/Reviewer_ctkd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19198/Reviewer_ctkd"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19198/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891872747, "cdate": 1761891872747, "tmdate": 1762931194388, "mdate": 1762931194388, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}