{"id": "aCVfhY4Qen", "number": 9407, "cdate": 1758121408401, "mdate": 1763742670219, "content": {"title": "PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Generation", "abstract": "Automatically generating interactive 3D environments is crucial for scaling up robotic data collection in simulation. While prior work has primarily focused on 3D asset placement, it often overlooks the physical relationships between objects (e.g., contact, support, balance, and containment), which are essential for creating complex and realistic manipulation scenarios such as tabletop arrangements, shelf organization, or box packing. Compared to classical 3D scene generation, producing complex physical scenes introduces additional challenges: (a) higher object density and complexity (e.g., a small shelf may hold dozens of books), (b) richer supporting relationships and compact spatial layouts, and (c) the need to accurately model both spatial placement and physical properties.\nTo address these challenges, we propose PhyScensis, an LLM agent-based framework powered by a physics engine, to generate physically accurate 3D scenes with high complexity.\nSpecifically, our framework consists of three main components: an LLM agent iteratively proposes assets with spatial and physical predicates; a solver, equipped with a physics engine, realizes these predicates into a 3D scene; and feedback from the solver informs the agent to refine and enrich the configuration. \nMoreover, our framework preserves strong controllability over fine-grained textual descriptions and numerical parameters (e.g., relative positions, scene stability), enabled through probabilistic programming for stability and a complementary heuristic that jointly regulates stability and spatial relations.\nExperimental results show that our method outperforms prior approaches in scene complexity, visual quality, and physical accuracy, offering a unified pipeline for generating complex physical scenes for robotic manipulation.", "tldr": "", "keywords": ["Physical Scene Generation"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b0388061ad1c0daec49e99396ccc13162dc3ea99.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a scene generation system for complex 3d scenes that prompts an LLM to produce scene descriptions using a scene predicate domain-specific language. Candidate predicates are processed by a 2d layout constraint solver and 3d physics engine to optimize and evaluate the scenes. Feedback from the DSL generation (syntax) and solvers is used to enable the LLM to iterate toward functional scenes with physical properties like asset placement instability.\n\nExperiments test against two prior systems for 3d scene layout and ablations of some of the system components. Evaluations compare whether scenes match prompts, preference comparisons when evaluated by GPT, and the physical distances assets move after initialization (as a physical check). There is also an evaluation of model learning from demonstrations on these scenes."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "# originality\nThe primary novelty is integrating multiple solver types as optimization and feedback mechanisms. This extends prior works and shows how to integrate more parts into generation and LLM guidance.\n\n# quality\nShows some promising results on learning from scene demonstrations.\n\n\n# clarity\nProvides ample qualitative examples of the method to complement quantitative results.\n\n\n# significance\nWill be of interest to the robotic manipulation community."}, "weaknesses": {"value": "# originality\nNo single component of the system is particularly novel. And the use of a DSL is more constrained than the more generic code generation of the 3DGeneralist prior work. None of this is horrible, but limits novelty.\n\n# quality\nSee the questions for detailed remarks and suggestions. The primary concerns are:\n- (1) Lack of statistical testing for differences and their magnitudes.\n- (2) Need for scaling analysis to get a clearer sense of the cost-benefit trade-off of the new approach.\n- (3) Lack of clarity on the demonstration generation and training process. The results are strong, but this is marred by ambiguity on how much the task reflects a particularly strong scenario for PhyScensis compared to previous efforts (the dinner table setting task).\n\n# clarity\nSee the questions for minor comments. The demonstrations point (3) is related."}, "questions": {"value": "# questions\n- Table 1, 2, 3: Results should include statistical tests for differences and effect sizes. Some of the outcomes look to have overlapping standard deviations, suggesting the differences may not be large.\n- Section 4.3: How were demonstrations generated: by humans? an automated process?\n\t- The section on demonstration generation and training is very compressed and hard to follow. I was not clear on what the demonstrations were, what training was done, and how evaluation was done.\n- What costs are involved in each method evaluated (including ablations) and how do they scale?\n\t- For example: how many LLM queries used, how many iterations / computation (for solvers), how much wall clock time?\n\t- How do these costs scale with the scene size or number of assets? Other relevant input or output parameters?\n- Table 2: Why are the VQA Score, GPT Ranking, Settle Distance not included?\n- Is there any evidence around output scene diversity? How that impacts learning the outcomes?\n\t- It's often desirable that a generator can produce many different outputs from the same prompt, but this can be in tension with controllable outputs.\n\t- These metrics could be computed from the output scenes themselves, to measure things like number of assets, asset diversity across generations, placement diversity, and so on.\n\n\n# suggestions / minor comments\n- Figure 1: Why does \"more compact\" look the same?\n- How would the dock scenario shown in the figures be used for manipulation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Jixdfnmf5M", "forum": "aCVfhY4Qen", "replyto": "aCVfhY4Qen", "signatures": ["ICLR.cc/2026/Conference/Submission9407/Reviewer_ummC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9407/Reviewer_ummC"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760930134404, "cdate": 1760930134404, "tmdate": 1762921013535, "mdate": 1762921013535, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces PHYSCENSIS, a framework that automatically generates interactive and physically plausible 3d environments for robotic manipulation. PHYSCENSIS leverages an LLM to propose realistic scene configurations, including spatial relations and object properties. A physics solver then checks if the proposed configuration is feasible and places the objects in the scene. If objects are not solvable, a feedback will be provided to the LLM to refine the scene configuration. Experiments show that PHYSCENSIS outperforms two baselines and that the environments can be used to train an IL policy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is very well written, well motivated, and easy to follow\n- The methodology, although not entirely novel, is promising.\n- The results and ablations show that the individual design choices result in improved generation speed and scene quality."}, "weaknesses": {"value": "### The main weakness of the paper is the experiments. In particular, the downstream experiment fails to showcase the advantages of the approach compared to existing scene generation pipelines in the robotics domain:\n- The VQA-based evaluation is questionable. It’s not clear if this metric works well for complex 3D tabletop environments. The high variance across models suggests it may not be reliable. Comparing it with human judgments could help validate this.\n- It’s unclear whether the same VQA model and scores are used both for evaluation and for providing feedback during generation. If so, this would bias results in favor of PHYSCENSIS, since it would directly optimize for the evaluation metric.\n- The chosen baselines are rather weak. The authors should explain why they did not compare against similar LLM + physics-based methods (e.g., ClutterGen, RoboGen, SimGen) and elaborate on the choice of baselines further\n- The downstream manipulation task is too simple and does not demonstrate the framework’s claimed advantages. The task does not depend on accurate physics or object properties. More challenging tasks like stacking, unstacking, and manipulating objects with different stability would provide stronger evidence.\n- The authors state that the cup and plate are fixed for each scene. Is this also the case for the baselines? During evaluation, are the plate and cup also fixed? If yes, the policy would not need to rely on visual cues.\n\n\nThe proposed method is not entirely novel, but it combines existing approaches. However, the problem is very relevant, and the framework could allow for training policies more robust to difficult settings in the pick and place task. However, in its current state, the experiments fail to showcase the effectiveness of the framework in that regard. Thus, in its current state, I tend towards reject. Performing extended robot experiments in more diverse environments and clarifying the evaluation methodology would strengthen the contribution and further support the paper's claims."}, "questions": {"value": "See above and\n- How does the framework compare against other frameworks in runtime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pBkHx84eJ0", "forum": "aCVfhY4Qen", "replyto": "aCVfhY4Qen", "signatures": ["ICLR.cc/2026/Conference/Submission9407/Reviewer_1kHh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9407/Reviewer_1kHh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761758132242, "cdate": 1761758132242, "tmdate": 1762921013259, "mdate": 1762921013259, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses the task of generating physically plausible environments. To tackle challenges in both spatial arrangement and physics, the authors propose the PhyScensis framework, which leverages a large language model (LMM) to generate predicates and employs a physics engine as the physics solver. Their framework also incorporates feedback from the physics engine back to the LMM for further refinement, resulting in realistic layouts and physically stable scenes. Experimental results demonstrate superior performance compared to previous methods."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The overall system, which integrates LLM-based predicates, a physics-based solver, a geometry-based spatial solver, and feedback to the LLM, is well-designed. This results in layouts that are both reasonable and physically stable.\n* Physics-plausible scene generation is an interesting and important direction, particularly for large-scale scene generation.\n* The experiments are thorough, including ablations and additional evaluations on downstream robotics tasks."}, "weaknesses": {"value": "* It is unclear what text prompts are used in the test set for all methods. How many prompts are there, and how diverse are they?\n* There is no discussion of failure cases, particularly regarding physics. What are the limitations of the current predefined predicates?\n* Regarding the LLM, it is unclear how it determines object sizes and how it selects objects from the candidate object set."}, "questions": {"value": "* How are objects selected—only at the category level, or is there a more detailed retrieval?\n* Are there predefined rules for selection, or is it random? For example, in the “table for 4” case, why are all plates the same? Is this constraint imposed by the LLM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bZY3RF3O4a", "forum": "aCVfhY4Qen", "replyto": "aCVfhY4Qen", "signatures": ["ICLR.cc/2026/Conference/Submission9407/Reviewer_MLGL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9407/Reviewer_MLGL"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761947360546, "cdate": 1761947360546, "tmdate": 1762921012969, "mdate": 1762921012969, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a framework that leverages LLMs and a physics engine to generate physically plausible scenes. Specifically, given a library of 3D assets and a language caption, an LLM is first used to iteratively propose relevant assets and predicates that determine their initial positions. Then, spatial and physics solvers are used to ensure the layout is collision-free and physically plausible. Experiments show that the model achieves good performance compared with previous baselines, especially on scenes with cluttered objects."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well written and easy to follow.\n2. The generation results look good.\n3. The proposed method enables a certain level of controllability, such as the distance between objects and the stability of objects."}, "weaknesses": {"value": "1. I think the term scene generation used here is misleading. The paper mostly focuses on “object arrangement” [1] or “layout generation” [2], where the goal is to place objects of similar sizes on a given surface (e.g., a bookshelf or a table). This is implied by all the qualitative examples. In contrast, scene generation usually refers to generating larger and more complex indoor scenes containing objects of various sizes and more diverse object relationships, which is not demonstrated in the experiments. I agree that the paper tackles a challenging problem involving arranging a large number of objects in confined space, but it is different from the indoor scene generation problem that the baselines address. If the authors want to keep using this term and still treat Architect / 3D-Generalist as the main baselines, they should provide more results on indoor scenes to demonstrate the effectiveness of the method.\n2. LayoutVLM [2] is an important missing baseline. This model uses a similar asset library to specify spatial relations between objects. It would be useful to compare these methods. Unlike PhyScenesis, which only leverages an LLM and a physics simulator to avoid collisions, LayoutVLM prompts a VLM to get initial object positions, which may yield more semantically meaningful results.\n3. The technical contribution is limited. SceneCraft [3] also proposes an agentic framework that generates 3D scenes using an LLM to generate Blender code with a feedback loop. Spatial and physical predicates are also used in LayoutVLM [2]. The design of the spatial and physics solvers here is largely heuristic and feels ad hoc. They seem specifically designed for cluttered scenes with small objects, which may limit applicability to other scenarios like full indoor scene generation or settings with fewer objects.\n4. I appreciate the authors’ effort in building such a complex system to achieve good results. However, it would be better to provide more experiments and details to systematically justify the design choices. See my questions below for more information.\n\nMinor Point:\nLine 106: SceneThesis also has a module for optimizing the physical plausibility of generated scenes, including collision avoidance and stability.\n\nReferences:\n1. Line 106, Scenethesis also has a module for optimizing the physical plausibility of the generated scene including collision avoidance and stability. \n\n[1] LEGO-Net: Learning Regular Rearrangements of Objects in Rooms. Qiuhong Anna Wei, et al. CVPR 2023 (Missing citation)\n\n[2] LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models. Fan-Yun Sun, et al. CVPR 2025 (Missing citation) \n\n[3] SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code. Ziniu Hu, et al. ICML 2024 (Missing citation)\n\nI am happy to raise my score if my questions and concerns are addressed."}, "questions": {"value": "1. Many experimental details are missing:\n\n   a. How many prompts are used in the experiments, and how were they created?  \n   b. How many examples were generated per prompt?  \n   c. How exactly are the VQA score and GPT-based ranking implemented?\n\n2. From the method description and the prompt in A.3.2, it seems the proposed method does not support placing objects on other objects at a specified height (e.g., a certain shelf level). How is this achieved in Figure 5?\n\n3. The robot experiment is an interesting way to show the diversity of generated scenes from a single prompt:  \n   a. Why is the success rate of reaching much higher than that of placing?  \n   b. How are the human-designed test scenes different from the generated scenes?  \n   c. What do the generated scenes from each model look like? It would be helpful to show sample visualizations to illustrate quality and diversity.\n\n4. What are the failure cases of the model? I am especially interested in scenes with low VQA scores or GPT rankings.\n5. It would be helpful to provide qualitative examples of the ablation study between Random, LLM-Only, and the full method. What about using only the spatial solver without the physics solver?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "OxGtXVgQqU", "forum": "aCVfhY4Qen", "replyto": "aCVfhY4Qen", "signatures": ["ICLR.cc/2026/Conference/Submission9407/Reviewer_LKXm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9407/Reviewer_LKXm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9407/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967259869, "cdate": 1761967259869, "tmdate": 1762921012534, "mdate": 1762921012534, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers （Part 1)"}, "comment": {"value": "*We express our gratitude to all the reviewers for their perceptive comments aimed at enhancing the quality of our work.*\n\n## 1. Our Contributions\n\nOur primary contribution is introducing an efficient approach for integrating a physics engine into the scene layout generation agent, together with a comprehensive feedback mechanism that enhances the agent’s ability to perceive and reason about its environment. \nThis design address a core limitation of prior scene-generation methods—their inability to produce complex arrangements that are physically accurate. \nBy enabling reliable generation of scenarios such as tabletop arrangements, shelf organization, and box-packing layouts with assured physical fidelity, our method substantially advances the capabilities of existing systems.\n\n## 2. Additional Experiments\n\n**2.1. LayoutVLM Baseline**\nLayoutVLM[1] adopts a similar method for 3D scene-level layout generation, using LLMs to generate proposals for object positions and relationships, followed by a differentiable solver to determine the final layouts. We present additional quantitative results in the following table. In Appendix A.3.1, we further present the implementation details, qualitative results, and detailed analysis for this baseline.\n\n\n| Method | VQA Score ↑ | GPT Ranking ↓ | Settle Distance ↓ |\n| :--- | :---: | :---: | :---: |\n| Architect | 0.493 ± 0.392 | 3.250 ± 0.947 | 0.405 ± 0.471 |\n| 3D-Generalist | 0.578 ± 0.399 | 2.393 ± 0.899 | 0.033 ± 0.048 |\n| LayoutVLM | 0.648 ± 0.446 | 2.643 ± 1.109 | 0.115 ± 0.238 |\n| Ours | **0.704 ± 0.425** | **1.714 ± 0.920** | **0.003 ± 0.008** |\n\n*Table: Additional quantitative comparison of PhyScensis with baselines, including LayoutVLM.*\n\n\n**2.2. ClutterGen Baseline**\nClutterGen[2] addresses a related problem of generating complex object arrangements on a supporting surface of a specified size. Their approach employs reinforcement learning to optimize a policy for placing a fixed set of assets, but it does not consider semantic coherence. \nThus, we compare the maximum number of objects that can be placed in a scene using the same asset library. Results show that ClutterGen places an average of only 3.57 objects per scene, whereas our method places an average of 14.77 objects without stacking and 22.30 objects with stacking. For additional details and qualitative examples, please refer to Appendix Section A.3.2 and Figure 14.\n\n\n**2.3. User Study**\nWe conducted a user study comparing our method against two main baselines: Architect and 3D-Generalist. Specifically, we selected 6 prompts and randomly sampled one result for each of the three methods, creating a total of 18 evaluation cases. For each case, we asked users to rate the generated scene on a scale of 1-5 across three dimensions: text alignment, naturalness & physical plausibility, and complexity. We collected responses from 20 participants; the results are as follows:\n\n\n| Baseline | Match with Text ↑ | Naturalness & Physics Plausibility ↑ | Complexity ↑ |\n| :--- | :--- | :--- | :--- |\n| **Architect** | 2.68 | 2.65 | 2.69 |\n| **3D-Generalist** | 2.54 | 2.72 | 3.04 |\n| **ours** | 4.04 | 3.98 | 3.82 |\n\nOur method significantly outperforms the other two baselines, which is consistent with our quantitative metrics. This result also aligns with our qualitative observations: Architect estimates object layout directly from inpainted images, often leading to object penetrations and implausible physics. Meanwhile, 3D-Generalist employs a VLM to output placement positions in pixel space. However, the VLM's limited spatial reasoning capability fails to yield reasonable layouts for complex prompts, resulting in this baseline receiving the lowest scores for text alignment.\n\n\n**2.4. Run Time Analysis**\n\nWe calculated the average runtime for generating 50 scenes. To ensure fairness, we configured 3D-Generalist and PhyScensis to generate a similar number of objects. Architect cannot control the specific number of generated objects because it utilizes an inpainting-based pipeline. Additionally, we excluded rendering time for baselines, as ray-tracing renderer is computationally intensive and independent of the generation process. The results are as follows:\n\n| Baseline | Run Time | Number of Objects |\n| :--- | :--- | :--- |\n| **Architect** | 201.32 ± 2.47| 5.22 |\n| **3D-Generalist** | 177.20 ± 21.57 | 12.15 |\n| **ours** | 95.95 ± 30.23 | 11.82 |\n\nOur method is the fastest of the three. Architect requires multiple rounds of inpainting, diffusion based depth prediction, feature extraction, and solving, resulting in a complex and heavy pipeline. 3D-Generalist requires a VLM call and a rendering step for each object placement, which limits its scalability. In contrast, our method resolves a batch of objects after a single LLM call, making it the most efficient. However, because our solver may occasionally fail—requiring the LLM to iteratively propose new predicates—there is a larger standard deviation in runtime."}}, "id": "hBnhd0qrt5", "forum": "aCVfhY4Qen", "replyto": "aCVfhY4Qen", "signatures": ["ICLR.cc/2026/Conference/Submission9407/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9407/Authors"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9407/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763742992361, "cdate": 1763742992361, "tmdate": 1763743095301, "mdate": 1763743095301, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (Part 2)"}, "comment": {"value": "## 3. Additional Implementation Details\n\n**3.1 Quantitative Metrics Implementation Details**\n\nWe primarily adopt three quantitative evaluation metrics: VQA Score, GPT Ranking, and Settle Distance. Prior to evaluation, we re-render all scenes generated by different baselines to ensure consistent lighting conditions, viewing angles, and rendering parameters. For the **VQA Score**, we input the rendered scene image into GPT-4o with the prompt ``Does this image show {scene_prompt}? Please answer with Yes or No.`` and measure the probability of the ``Yes`` token in the response. For **GPT Ranking**, for each prompt, we aggregate the generated examples from all baselines and feed them to GPT-4o, requesting a ranking based on scene quality and alignment with the prompt. For **Settle Distance**, we initialize objects in a physics simulation with standard gravity according to their generated configurations. We run the simulation for 400 steps and calculate the displacement of each object between its initial and final positions. The displacement distance is clamped to a maximum of 1 (to account for objects falling off the table), and we report the average across all scenes.\n\n\n**3.2 Test Set Prompt Generation** \n\nWe prompt LLM to generate a diverse set of 50 scene descriptions for testing.\n\nHere's a sample of 10 of them:\n```\n\"rustic_still_life\": \"A wooden table with fruits, a candle, and old books arranged in a painterly still-life composition.\",\n\"sunflower_artist_desk\": \"An artist’s desk with a vase of sunflowers, brushes, paints, and a sketchpad.\",\n\"romantic_dinner\": \"A candlelit dinner setup for two with plates, glasses, and a warm atmosphere.\",\n\"messy_breakfast\": \"A cluttered breakfast table with food, spilled drinks, and a phone left on the side.\",\n\"fantasy_board_game\": \"A game table mid-session with cards, dice, miniatures, and snacks around.\",\n\"model_train_workbench\": \"A hobbyist’s desk with train parts, tools, paints, and in-progress assembly.\",\n\"traveler_memento_table\": \"A table with maps, postcards, foreign coins, and a globe—suggesting travel memories.\",\n\"explorer_desk\": \"An explorer’s writing table with a compass, sketches, and an old map spread out.\",\n\"science_experiment\": \"A chemistry experiment scene with beakers, colored liquids, and handwritten notes.\",\n\"space_project\": \"A tabletop with star maps, a model rocket, small tools, and technical components.\"\n```\nThen we generate 10 scenes for each prompt for quantitative evaluation.\n\n\n[1] LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models. Fan-Yun Sun, et al. CVPR 2025\n[2] Jia, Yinsen, and Boyuan Chen. \"Cluttergen: A cluttered scene generator for robot learning.\" 8th Annual Conference on Robot Learning. 2024."}}, "id": "oFcL5at5kh", "forum": "aCVfhY4Qen", "replyto": "aCVfhY4Qen", "signatures": ["ICLR.cc/2026/Conference/Submission9407/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9407/Authors"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission9407/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763743054870, "cdate": 1763743054870, "tmdate": 1763743054870, "mdate": 1763743054870, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}