{"id": "v2mfraZriO", "number": 10739, "cdate": 1758180800634, "mdate": 1759897632318, "content": {"title": "Fidelity-Aware Data Composition for Robust Robot Generalization", "abstract": "Generalist robot policies trained on large-scale, visually homogeneous datasets can be susceptible to shortcut learning, which impairs their out-of-distribution (OOD) generalization. While generative data augmentation is a common approach to introduce diversity, it presents a subtle challenge: data composition. Naively mixing real and synthetic data can corrupt the learning signal, as this process often prioritizes visual diversity at the expense of information fidelity. This paper suggests that robust generalization depends on principled, fidelity-aware data composition. We introduce Coherent Information Fidelity Tuning (CIFT), a framework that treats data composition as an optimization problem. CIFT uses a practical proxy for Information Fidelity based on the feature-space geometry of a dataset. This enables the identification of a phase transition, termed the Decoherence Point, where training stability degrades. The framework includes a generative engine, Multi-View Video Augmentation (MVAug), to synthesize a causally disentangled data spectrum for this tuning process. Applying CIFT to policy architectures such as $\\pi_0$ and Diffusion Policy improves OOD success rates by over 54\\%. These results indicate that fidelity-aware composition, beyond data synthesis alone, is an important component for developing robust, general-purpose robots.", "tldr": "We introduce CIFT, a framework that pairs our multi-view synthesis engine, MVAug, with a novel Information Fidelity metric to optimally compose augmented data, mitigating shortcut learning in robot policies.", "keywords": ["robot learning", "shortcut learning", "OOD generalization", "data augmentation", "generative models"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/480052715cf6d7bda130067432e83c7c22d47f7a.pdf", "supplementary_material": "/attachment/63c82e89495a1714962b92212c7cc1014d1357dc.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the problem of \"shortcut learning\" in robot policies, where models trained on visually homogeneous data fail to generalize to out-of-distribution (OOD) environments. The authors posit that the solution lies not just in generative data augmentation but in the principled composition of real and synthetic data. They introduce a framework, Coherent Information Fidelity Tuning (CIFT), which formalizes this composition as an optimization problem. The framework consists of two main components: 1) MVAug, a multi-view video-to-video generative model for creating a diverse spectrum of disentangled data, and 2) a composition algorithm that uses a feature-space Signal-to-Noise Ratio (SNR) as a proxy for \"Information Fidelity.\" By analyzing this SNR, CIFT aims to identify an optimal mixing ratio (λ) of real-to-synthetic data that maximizes robustness while avoiding a \"Decoherence Point\" where training stability collapses. The authors apply CIFT to train π₀ and Diffusion Policy, reporting significant improvements (over 54%) in OOD task success rates on physical robots."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper correctly identifies a critical and subtle challenge in modern robot learning. Moving the focus from just data synthesis to principled data composition is a valuable contribution to the field. The concept of a \"Diversity-Information Fidelity trade-off\" is insightful and well-articulated.\n- The attempt to create a formal, data-driven framework (CIFT) to select the data mixing ratio is commendable. It moves beyond the common practice of ad-hoc hyperparameter tuning and seeks a more systematic solution.\n- The authors present a complete system, from a sophisticated generative model (MVAug) to a composition algorithm, and validate it with closed-loop experiments on physical robots. The reported OOD performance improvements are substantial and demonstrate the potential of the underlying idea."}, "weaknesses": {"value": "Despite its promising direction, the paper is undermined by significant methodological flaws, a lack of critical experimental validation, and poor clarity in key areas. The claims made are not sufficiently supported by the evidence provided.\n- The central hypothesis is that CIFT's SNR proxy can identify an optimal mixing ratio λ* for closed-loop policy performance. However, the experiments fail to validate this.\n1. The validation of the SNR proxy is done entirely in an open-loop setting (Section 5.2, Figure 5), where the metric of success is the \"Robustness Score (RS),\" an unvalidated metric based on action prediction MSE. It is well-known in robotics that open-loop performance (i.e., imitation accuracy on a static dataset) is a poor predictor of closed-loop performance, where compounding errors and unexpected states dominate.\n2. The on-robot, closed-loop experiments (Table 3) only compare the baseline policy (λ=0, \"w/o CIFT\") against the policy trained with the CIFT-selected ratio (λ=λ*, \"w/ CIFT\"). Crucially, they do not evaluate other mixing ratios (e.g., λ values around the supposed \"Decoherence Point\") in the closed-loop setting. Without this comparison, the authors have only shown that their specific data augmentation is beneficial. They have provided no evidence that their CIFT selection procedure finds a better λ than any other randomly chosen λ, or that the \"Decoherence Point\" identified via open-loop analysis corresponds to a real collapse in closed-loop task success. This is the single most critical experiment needed to validate the paper's core contribution, and its absence is a fatal flaw.\n\n- The paper is heavily motivated by the goal of solving shortcut learning (i.e., reliance on spurious features). However, it provides no direct evidence that the method actually reduces this phenomenon. Showing improved OOD generalization is not direct proof of it. The authors should have included experiments to investigate what the baseline vs. CIFT-trained policies are paying attention to in ID vs. OOD settings. Alternatively, they could have constructed controlled experiments where a specific spurious cue is present or absent to directly measure the policy's reliance on it. Without this, the claims about how the method works are purely speculative.\n\n- Weak justification and opacity of proposed methods:\n1. The paper provides insufficient detail on how the MVAug video-to-video model was trained (Appendix B). Crucially, it omits the dataset used for fine-tuning the Cosmos-Predict2-2B-Video2World foundation model. What data was used for paired multi-view video data? Without this information, the results are not reproducible, and it is impossible to assess whether the generative model's quality unfairly advantages their method over baselines.\n2. The justification for using the SNR of the first principal component of features from a generic, pre-trained Inception-v3 model is weak and poorly explained. The text simply states, \"Analysis shows a non-monotonic relationship...\" but provides no citation or pointer to where this analysis is. Why should the axis of maximum variance in a general-purpose feature space be the most informative signal for the training dynamics of a specific robot policy? It is an unsubstantiated leap of faith.\n\n- The authors report that training a Diffusion Policy required \"approximately 80 hours on 16 H100 GPUs.\" This is an astronomical amount of computation for fine-tuning on a small dataset of 200 real episodes. This implies that the CIFT-composed dataset is enormous, meaning MVAug must generate a massive volume of synthetic data. This is not just a resource issue; it fundamentally questions the method's efficiency and practicality. It suggests the policy's robustness comes from brute-force exposure to an immense quantity of generated data, rather than a finely-tuned \"optimal\" composition. The paper should be more transparent about the scale of data generation required."}, "questions": {"value": "- The core validation experiment (Figure 5, Table 1) shows that the peak Robustness Score (RS) occurs at a 100:200 ratio, while the CIFT method selects the 100:100 ratio based on peak SNR. Why should one trust a proxy (SNR) that selects a demonstrably sub-optimal point according to the open-loop evaluation metric (RS)?\n- Could you elaborate on the justification for using the first principal component of Inception-v3 features as a proxy for information fidelity? A policy's learned representation is task- and architecture-specific. Why would the primary axis of variance from a generic, pre-trained image classifier be a reliable indicator of the complex learning dynamics and gradient alignment for a visuomotor policy like π₀?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lj7yYkir5m", "forum": "v2mfraZriO", "replyto": "v2mfraZriO", "signatures": ["ICLR.cc/2026/Conference/Submission10739/Reviewer_j786"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10739/Reviewer_j786"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760609797142, "cdate": 1760609797142, "tmdate": 1762921966080, "mdate": 1762921966080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **Coherent Information Fidelity Tuning (CIFT)**, a new framework to optimize the composition of real and synthetic data for robot policy training. CIFT consists of two major components: Multi-View Video Augmentation (MVAug) — a latent diffusion transformer that generates multi-view, causally consistent video demonstrations; Information Fidelity Metric — a quantitative proxy for data quality that measures coherence between real and synthetic data using the feature-space signal-to-noise ratio. Empirical evaluations on real-world robotic manipulation tasks show that CIFT achieves superior performance over existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Using SNR to quantitatively measures the coherence is both interesting and novel.\n- The authors conduct comprehensive experiments, including extensive ablation studies, demonstrating the effectiveness of CIFT.\n- The Robustness Score of MVAug surpasses that of the baseline methods by a large margin across all mixing ratios, demonstrating the superiority of the generated data.\n- The proposed method can be integrated with various pretrained backbones or downstream algorithms, making it broadly applicable across different robot learning setups."}, "weaknesses": {"value": "- The Feature SNR used as a proxy for robustness does not appear to align well with the actual robustness score, as shown in Figure 5(a).\n- The authors state that all code and datasets have been made publicly available in an anonymous repository. However, I was unable to locate or access this repository. I only found that some video demonstrations are included in the supplementary material."}, "questions": {"value": "- It is interesting to observe that quite different pretrained backbones (Inception-v3, CLIP, DINO-v2) yield exactly the same Decoherence Point. Could the authors provide an explanation for why this happens?\n- Could the authors clarify how to access the code and dataset mentioned in the paper?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "hirYp72FcS", "forum": "v2mfraZriO", "replyto": "v2mfraZriO", "signatures": ["ICLR.cc/2026/Conference/Submission10739/Reviewer_X2Vq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10739/Reviewer_X2Vq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761767261516, "cdate": 1761767261516, "tmdate": 1762921965683, "mdate": 1762921965683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified framework that balances data diversity and realism in robot learning by combining Multi-View Video Augmentation (MVAug) and Coherent Information Fidelity Tuning (CIFT). MVAug is a latent diffusion-based generator that produces physically consistent and multi-view coherent augmented videos through cross-view attention and structure/appearance conditioning, enriching visual diversity without breaking causality. CIFT then analyzes the feature-space Signal-to-Noise Ratio (SNR) to quantify data fidelity and identify the optimal real-to-synthetic mixing ratio. Experiments on simulated and real robots demonstrate that the fidelity-aware composition strategy enhances policy robustness and generalization under unseen visual conditions, though its scalability and reliance on generative model quality remain limitations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "1. The paper proposes a well-founded framework CIFT that balances data diversity and realism. It also helps researchers predict the optimal real/synthetic data ratio without post-training, improving policy generalization and lowering computational cost.\n2. This paper establishes a measurable relationship between feature-space SNR and robustness score (RS). SNR provides an interpretable and computationally efficient proxy for assessing data quality before training.\n3. The latent diffusion transformer with periodic cross-view attention and structure/appearance conditioning achieves impressive results in maintaining geometric and temporal consistency across views."}, "weaknesses": {"value": "1. The “decoherence point” $\\lambda_{dc}$ is detected empirically from SNR minima, but this paper does not provide a rigorous mathematical criterion or theoretical justification for its general validity.\n2. The experimental results are based on a single checkpoint with 20 trials and limited task types, making the improvement in Table 3 less reliable. Additional policy testing results from tasks in MVAug synthesis examples (in Appendix) and details on settings, such as the object's initial position range, should be provided."}, "questions": {"value": "1. What is the computational overhead (training/inference time) for generating augmented datasets using MVAug?\n2. A minor suggestion: in Figure 23, please consider changing “bottom” and “top” to “left” and “right” for clarity."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "X9uf21JEH6", "forum": "v2mfraZriO", "replyto": "v2mfraZriO", "signatures": ["ICLR.cc/2026/Conference/Submission10739/Reviewer_4SFU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10739/Reviewer_4SFU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761894369481, "cdate": 1761894369481, "tmdate": 1762921965031, "mdate": 1762921965031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a multi-view data augmentation approach to achieve better policy performance when finetuned on a target domain and when tested especially under distribution shift. The author propose a multi-view consistent diffusion model that augments the robot demonstration data with different backgrounds, distractors, relighting and foreground objects etc. They propose a metric which guides how much to mix the original data with synthetic data. Both open and close loop experiments are performed with existing foundation models i.e. pi-0 as the base model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "In my opinion, here are the strengths of the paper:\n\n1. A systematic study for robot learning foundation models on finding the feature space SNR correlates well with final post training policy stability.  \n\n2. Significant performance improvement vs the base model finetuning using the augmented dataset proposed by the paper. \n\n3. The paper is clearly written and easy to follow, and figures complement the text nicely."}, "weaknesses": {"value": "In my opinion, below are the weaknesses of the paper:\n\n1. While it is great to study open-loop correlation, I am curious if the same correlation holds with the closed-loop SR. It is also not clear if open-loop RS score even correlates well with closed-loop SR; which makes the decisions on mixing ratio being calculated on open-loop RS seems a bit questionable.\n\n2. While it is great to see qualitative results with baseline, I am wondering will the baselines i.e. RoboTransfer etc. would also result in the same performance improvements as the authors see in Table 3 with their proposed augmentations?\n\n3. No statistical error bars are presented, which doesn't show the full variance b/w runs in these generative policies [1]. \n\n4. The qualitative results for RoboEngine are presented in a bit too extreme case where in every frame the background etc changes, is there a more principled baseline where one could take masked robot embodiment from RoboEngine and applies let's say same lightning to all frames or same color augmentation to foreground objects like shirt in all frames, where the results could look a lot better and cleaner for the baseline than currently presented. \n\n5. Does the same augmentation strategy hold for pretraining the foundation model as opposed to finetuning where still 200 real-world demos are collected which is a large number. \n\n\n[1] TRI LBM Team, A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation, arXiv 2025"}, "questions": {"value": "Please see questions in the weakness section. I look forward to seeing the author's response in rebuttal."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7tqc09DLdS", "forum": "v2mfraZriO", "replyto": "v2mfraZriO", "signatures": ["ICLR.cc/2026/Conference/Submission10739/Reviewer_n7iA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10739/Reviewer_n7iA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10739/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762853046605, "cdate": 1762853046605, "tmdate": 1762921964605, "mdate": 1762921964605, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}