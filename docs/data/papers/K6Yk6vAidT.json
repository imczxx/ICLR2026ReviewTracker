{"id": "K6Yk6vAidT", "number": 8261, "cdate": 1758076731952, "mdate": 1759897795721, "content": {"title": "LG-TOM: Language Grounded Theory of Mind Modeling for Multi-agent Communication", "abstract": "Emergent communication refers to the process by which multiple agents learn to develop efficient protocols for sharing information in collaborative tasks. Agents typically learn through interaction with the environment, using reinforcement learning to optimize protocols for task completion. However, the sparse task rewards can lead to unstable training and poor generalization, especially in partially observable environments and decentralized training setups. To address these challenges, we propose \\textbf{LG-TOM}, a novel framework that enables agents to learn from social interactions via \\textbf{Theory of Mind (ToM) modeling with language grounding}. Specifically, we design a belief estimation network that leverages priors from large language models (LLMs), allowing agents to reason about how their communication influences the belief states of others. We then compute social influence as an intrinsic motivation reward, encouraging agents to share information that positively impacts teammates. Experimental results demonstrate that LG-TOM improves communication effectiveness over baselines in multi-agent collaborative tasks.", "tldr": "", "keywords": ["emergent communication", "multi-agent reinforcement learning", "social reasoning", "large language models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/92b62495f3d4fd98e61311fd0023412a1a4751cf.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LG-TOM, a framework for multi-agent communication that incorporates language-grounded Theory of Mind to improve social learning. A belief estimation network, guided by large language model priors, enables agents to infer and influence teammates’ beliefs, while a counterfactual reward promotes belief-shaping communication. Experiments on collaborative tasks show that LG-TOM enhances both performance and communication efficiency, with ablations validating the roles of language grounding and ToM-based rewards."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Pioneering integration of language-grounded ToM with multi-agent reinforcement learning, introducing a novel social reasoning paradigm.\n2. Technically sound framework that effectively addresses training instability via offline LLM supervision, enabling robust belief estimation.\n3. Comprehensive experimental validation across collaborative tasks, with rigorous ablation studies and information-theoretic analysis consistently demonstrating effectiveness."}, "weaknesses": {"value": "1. The experimental validation is critically insufficient. The limited scope—only two environments with a handful of agents and a lack of strong, modern baselines—fails to demonstrate the method's generality, scalability, or true competitive advantage.  \n2. The source of performance gains remains fundamentally ambiguous. The paper does not disentangle the contribution of its novel framework from the mere injection of knowledge from the powerful GPT-4 model, leaving its core intellectual contribution unproven.  \n3. This work ignores the practical implications of its approach. The substantial computational and financial costs of using a commercial LLM like GPT-4 for data generation are not discussed, creating a significant barrier to adoption and reproducibility for the wider research community."}, "questions": {"value": "1. Could you please provide a detailed description of the architecture and the specific update rules for the ToM network depicted in Figure 1?\n2. What information does an agent's state/belief vector b_k explicitly contain, and what is the precise procedure for constructing it from the observation and communication history?\n3. Will you release the exact prompts used for the LLM agents to generate the belief-state ground truth? This is critical for reproducibility.\n4. Does the proposed method fundamentally rely on a discrete, symbolic observation and action space, as suggested in the abstract? Please clarify its assumptions and limitations regarding the state-action representation.\n5. Why were the chosen baseline methods selected, and can you justify that they represent strong, contemporary benchmarks? The performance claims would be more convincing if compared against known state-of-the-art methods in emergent communication."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "o7DFmBOlE2", "forum": "K6Yk6vAidT", "replyto": "K6Yk6vAidT", "signatures": ["ICLR.cc/2026/Conference/Submission8261/Reviewer_QgbJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8261/Reviewer_QgbJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762008477921, "cdate": 1762008477921, "tmdate": 1762920201979, "mdate": 1762920201979, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "LG-TOM introduces a novel framework that combines LLM-based social reasoning with belief model (Theory of Mind) that has been trained with an offline supervised dataset with GPT-4 expert trajectories + textual representations. ToM model and communication policy loss are then updated with a combined standard task reward and an intrinsic social influence reward to promote communication. LG-TOM demonstrates improved performance, sample efficiency, and stability compared to baselines with different communication frameworks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel emergent behaviors can be achieved using language space embedding and potential for quicker experimentation time, with the use of large-LLM based expert trajectories.\n2. I particularly like the choice of baselines that cover a variety of different frameworks for better coverage of agent communication behaviors.\n3. Improved stability with the use of an offline trained ToM network using expert trajectories."}, "weaknesses": {"value": "1. Currently lacks evaluation or discussions on generalization capabilities of the agents to novel contexts. \n2. Discussions on scalability of the system to larger multi-agents systems has not been discussed in the paper."}, "questions": {"value": "1. How robust is LG-TOM to domain shift? Do you foresee cases where task dynamics do not allow an adaptation? \n2. How do you evaluate the belief state shifts correspond to meaningful change in team knowledge or intent? A comparison with actual environment states along with an interpretability analysis would be helpful in understanding the behavior.\n3. What is the computation costs associated with LG-TOM as compared to other models? Training steps per second would be a good metric."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sCunAi8hiq", "forum": "K6Yk6vAidT", "replyto": "K6Yk6vAidT", "signatures": ["ICLR.cc/2026/Conference/Submission8261/Reviewer_tBq9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8261/Reviewer_tBq9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762010009961, "cdate": 1762010009961, "tmdate": 1762920201667, "mdate": 1762920201667, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a key challenge in multi-agent reinforcement learning (MARL): learning effective communication protocols in environments with sparse task rewards. The authors propose a novel framework, LG-ToM (Language Grounded Theory of Mind), which equips agents with a social learning mechanism to generate dense, intrinsic rewards.\nThe core idea is to enable agents to reason about how their messages influence the belief states of their teammates. This \"social influence\" is then used as an intrinsic reward to encourage more informative communication. The main contribution and novelty lie in how this Theory of Mind (ToM) model is trained. Instead of learning from other agents online—which suffers from instability (the \"moving target\" problem)—LG-ToM uses a ToM network supervised by a static, offline dataset of belief states. This dataset is generated by Large Language Model (LLM) agents performing the task, grounding the concept of \"belief\" in natural language descriptions.The primary contributions of the paper are:\n1.\tThe introduction of the LG-ToM framework, which integrates a language-grounded ToM model with an intrinsic social influence reward to guide communication learning.\n\n2.\tA novel methodology for training the ToM model using an offline, LLM-generated dataset, which provides a stable learning signal and avoids the need for privileged access to other agents' internal states during training.\n\n3.\tEmpirical results on two collaborative tasks (Predator-Prey and USAR) demonstrating that LG-ToM outperforms state-of-the-art baselines, leading to more effective and efficient communication protocols that are shown to be more \"teammate-aware\" through information-theoretic analysis."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "This is a well-executed and insightful paper that makes a valuable contribution to the field of multi-agent communication. The strengths are evident across all dimensions: originality, quality, clarity, and significance.\n\tOriginality: The paper's primary novelty lies in its creative and effective solution to the \"moving target\" problem when training Theory of Mind (ToM) models in MARL. While the concepts of social influence rewards and ToM modeling are not new in themselves, the specific methodology of using a stable, offline dataset generated by LLMs to supervise the ToM network is highly original. This approach cleverly circumvents the instabilities of online learning while grounding the agents' abstract belief representations in the rich semantic space of natural language. It presents a novel synthesis of ideas from MARL, intrinsic motivation, and large language models.\n\n1. Quality: The technical quality of the work is high. The methodology is sound and well-motivated. The experimental setup is rigorous, employing standard benchmarks and relevant, state-of-the-art baselines. The ablation study is particularly strong; it systematically dismantles the proposed framework to isolate the contribution of each component. The negative result showing that a naively-trained online ToM model degrades performance is a crucial and convincing finding that strongly justifies the paper's core thesis. Furthermore, the inclusion of an information-theoretic analysis to quantify why the learned communication is better (i.e., more teammate-aware) adds a layer of depth that goes beyond simply reporting task-level scores.\n\n2. Clarity: The paper is exceptionally well-written and easy to follow. The authors do an excellent job of motivating the problem, clearly situating their work within the existing literature, and explaining their proposed method step-by-step. Figures 1 and 2 are clear and provide an excellent overview of the model architecture and data generation pipeline. The narrative of the results, particularly in the ablation section, is compelling and effectively guides the reader through the authors' reasoning and conclusions.\n\n3. Significance: The paper addresses a significant and long-standing challenge in MARL: designing learning signals that promote effective coordination. The findings have important implications. Firstly, it serves as a cautionary tale, demonstrating that simply adding a social reasoning module is not a panacea and can be detrimental if not trained properly. Secondly, and more importantly, it proposes a powerful and promising new paradigm: using large, pre-trained models to provide the semantic grounding and stable supervision needed for smaller, specialized agents to learn complex social behaviors. This work could inspire a new line of research into how the world knowledge of LLMs can be \"distilled\" to bootstrap more robust and efficient learning in multi-agent systems."}, "weaknesses": {"value": "While the paper is strong, there are several areas where it could be improved. The weaknesses are primarily related to the scalability of the proposed method and the scope of the experimental validation.\n\n1. Dependency on LLM-based Data Generation: The entire method hinges on the costly, environment-specific process of generating an offline dataset with LLM agents. This raises several practical concerns that are not fully addressed:\n\n2. Scalability and Cost: How much data is required, and what is the computational/financial cost of generating it? Does this process need to be repeated for every new task, or even for minor variations in the environment's dynamics or objectives? This dependency could significantly limit the method's applicability in practice.\n\n3. Quality and Bias of \"Ground Truth\": The paper treats the LLM-generated belief descriptions as ground truth. However, the quality of this data is entirely dependent on the capability of the chosen LLM and the engineering of its prompts. An LLM might produce suboptimal, biased, or even incorrect belief states, which would mean the MARL agents are being trained to model a flawed reasoner. The paper would be strengthened by a discussion of this limitation and the sensitivity of the results to the quality of this offline data.\n\n4. Complexity of Experimental Environments: The Predator-Prey and USAR tasks are good standard benchmarks, but they are relatively simple grid-world environments. The communication required is largely observational (\"prey is at X\") or direct (\"need help at Y\"). It is unclear if the significant overhead of the LG-ToM framework would provide the same level of benefit in more strategically complex environments (e.g., negotiation games, real-time strategy games) where beliefs and intentions are far more abstract and nuanced. Testing the method on a more challenging domain would make the claims about its effectiveness more robust.\n\n5. Approximation of Belief Representation: The method supervises the continuous belief vector b by comparing its cosine similarity to the embedding of a full natural language sentence from the LLM. This is a very lossy compression. It is an open question how well a single dense vector can capture the rich, compositional semantics of a sentence describing a belief state. While this is a common challenge in the field, the paper could benefit from acknowledging and briefly discussing the limitations of this representational choice.\n\n6. Missing Baseline Comparison: The core claim is that using an offline LLM dataset for supervision is key. This claim could be further substantiated by comparing against a baseline that uses an LLM in an online fashion—for example, as a \"coach\" that provides feedback or suggestions to the agents during training. This would create a more direct comparison between the \"distillation\" approach proposed here and other contemporary methods for LLM-guided MARL."}, "questions": {"value": "I would appreciate it if the authors could address the following questions in their rebuttal.\n\n1.\tOn the Practicality of Data Collection: Could you provide more details on the data collection process? Specifically: (a) How many interaction trajectories were needed to form the offline dataset D for each environment? (b) What was the approximate cost (in terms of tokens/API calls) and human effort (prompt engineering) involved? (c) How sensitive is the final performance of LG-ToM to the size and quality of this dataset, and to the choice of the underlying LLM (e.g., GPT-4 vs. a smaller open-source model)?\n\n2.\tOn the Nature of \"Belief\" Representation: The belief state b_t is a continuous vector, and similarity is measured by cosine distance. Have you conducted any qualitative analysis to verify that these learned belief embeddings capture semantically meaningful information? For instance, do belief vectors cluster in an interpretable way (e.g., via t-SNE visualization) based on the agent's situation (e.g., \"I see the prey,\" \"I am lost,\" \"I need help from teammate X\")? This would strengthen the claim that the model is truly learning a language-grounded belief space.\n\n3.\tRegarding Generalization Claims: Could you clarify the scope of the generalization benefit you claim? Does your framework improve generalization to new tasks, unseen environment configurations, or novel teammates? While I understand that new experiments may not be feasible for the rebuttal, could you point to specific evidence from your existing results or provide a more detailed theoretical argument for why language-grounded supervision should lead to better generalization compared to the baselines?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hGbls87N7W", "forum": "K6Yk6vAidT", "replyto": "K6Yk6vAidT", "signatures": ["ICLR.cc/2026/Conference/Submission8261/Reviewer_8Dmh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8261/Reviewer_8Dmh"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053060659, "cdate": 1762053060659, "tmdate": 1762920201299, "mdate": 1762920201299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Lg-TOM, Language-Guided Theory of Mind, which is a framework for training agents in Dec-POMDP settings. In their approach, each agent is allowed to communicate with other agents, using the communication from the previous time-step + the observation at the current time-step to produce an action. During training, the agents generate a belief of other agents, given their communicated message, and they define an intrinsic reward that encourages messages that maximally change the beliefs of other agents. They utilize social-intrinsic reward to update the communication and theory-of-mind networks, while using environment reward to update the action-selection network. They test several variants of their models and show that Lg-TOM achieves the highest rewards and conditional mutual information."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper takes an interesting approach to multi-agent communication, using the power of language models and their ability to communicate with language, to train a belief network. This is advantageous because ungrounded communication, at the vector-level, has a much higher dimensionality and differs between tasks.\n\n2. Ablations - I think the choices of ablation were comprehensive to asserting claims about LG-TOM's strengths. Additionally the conditional mutual information analysis was insightful."}, "weaknesses": {"value": "1. Environment Complexity - Predator Prey is a simple environment that can easily be solved by centralized training approaches like CommNet[1]. The USAR is far more interesting as the reward can only be achieved with coordination between team-mates, but two experiments feels limiting for the evaluation and justification of this method. I feel evaluation on more complex environments like Starcraft (per InfoPG[2]) or even the sequential social dilemmas in Social Influence, would be more convincing as to the method's success.\n\n2. Method Clarity - I find that the presentation of the method itself is not clear enough. In section 4, the authors jump from how the actions, communication vectors, and belief-predictions are generated, but the objective function is confusing because they mention an offline dataset of grounded belief vectors from an LLM. Are we mixing online RL training of the policy with an offline dataset -- if so, how is that valid? I think the method could be much clearer with some pseudocode at least in the appendix. Because of this, I find the interaction between the LLM belief component and the training of this method confusing.\n\n3. Missing References - InfoPG[2] asserts performance improvements to Social Influence MOA across multiple environments, including Starcraft. Also it deals with the same decentralized, multi-agent paradigm and considers a theory of mind approach with cognitive k-level. Perhaps comparison or discussion is relevant (in addition to other baselines like PR2-AC[3]).\n\n[1] https://papers.nips.cc/paper_files/paper/2016/hash/55b1927fdafef39c48e5b73b5d61ea60-Abstract.html\n[2] https://arxiv.org/pdf/2201.08484\n[3] https://arxiv.org/abs/1901.09207"}, "questions": {"value": "1. Distinction to Centralized Training - The authors mention \"In this work, we focus on a communication channel with a limited number of discrete tokens. This constraint creates pressure for emergent communication to be efficient, meaningful, and potentially generalizable to foundation language models\" as a distinction to previous models that approach multi-agent systems as an end-to-end differentiable model. However, with the communication vectors that are passed, is the system constrained such that gradient flow *does not* occur when the initial belief is generated (taking communication from the past + observation of the present -> feature vector for belief, theory of mind networks)?\n\n2. How did you measure conditional mutual information? - In the original definition of intrinsic reward, you define a distance between two probability distributions (of the belief conditioned w/ and w/o the communication). Social Influence (Jacques et. al), showed that this relates to mutual information. However, in this case, you aren't modeling a probability distribution anymore, but instead a dense vector (unless I am mistaken). so how was mutual information measured."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T2E32aCCCB", "forum": "K6Yk6vAidT", "replyto": "K6Yk6vAidT", "signatures": ["ICLR.cc/2026/Conference/Submission8261/Reviewer_u2Sh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8261/Reviewer_u2Sh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8261/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762358291594, "cdate": 1762358291594, "tmdate": 1762920200871, "mdate": 1762920200871, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}