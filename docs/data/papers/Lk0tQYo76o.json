{"id": "Lk0tQYo76o", "number": 15147, "cdate": 1758248282447, "mdate": 1759897325138, "content": {"title": "Improved Analysis for Sign-based Methods with Momentum Updates", "abstract": "This paper presents enhanced analysis for sign-based optimization algorithms with momentum updates. Traditional sign-based methods obtain a convergence rate of $\\mathcal{O}(T^{-1/4})$ under the separable smoothness assumption, but they typically require large batch sizes or assume unimodal symmetric stochastic noise. To address these limitations, we demonstrate that signSGD with momentum can achieve the same convergence rate using constant batch sizes without additional assumptions. We also establish a convergence rate under the $l_2$-smoothness condition, improving upon the result of the prior momentum-based signSGD variant by a factor of $\\mathcal{O}(d^{1/2})$, where $d$ is the problem dimension. Furthermore, we explore sign-based methods with majority vote in distributed settings and show that the proposed momentum-based method yields convergence rates of $\\mathcal{O}\\left( d^{1/2}T^{-1/2} + dn^{-1/2} \\right)$ and $\\mathcal{O}\\left( \\max \\\\{ d^{1/4}T^{-1/4}, d^{1/10}T^{-1/5} \\\\} \\right)$, which outperform the previous results of $\\mathcal{O}\\left( dT^{-1/4} + dn^{-1/2} \\right)$ and $\\mathcal{O}\\left( d^{3/8}T^{-1/8} \\right)$, respectively. Numerical experiments also validate the effectiveness of the proposed methods.", "tldr": "", "keywords": ["Stochastic optimization", "non-convex optimization", "sign-based methods", "convergence guarantees"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a817c99b0c3a22b576ef08730fdb3c3b63a4b10d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents a new analysis for sign-momentum. First, it presents a convergence rate of $O(\\frac{\\sqrt{\\|\\mathbf{L}\\|_1+\\Delta}+\\|\\mathbf{\\sigma}\\|_1}{T^{1/4}})$ for the centralized case under a separable smoothness assumption. Next, for the standard smoothness assumption, it presents a convergence rate of  $O(\\frac{d^{1/2}(L+\\Delta+\\sigma)}{T^{1/4}})$. In the distributed setup, it achieves better results than (Sun et al., 2023; Jin et al., 2021). The paper provides experimental results on CIFAR-10 for the proposed approaches against existing baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* The theoretical results appear to improve upon existing work, although I am less familiar with this literature.\n* The results on CIFAR-10 look good."}, "weaknesses": {"value": "* The $S_G$ procedure depends on unknown parameters. It is also unclear how the authors implemented $S_G$ and which version was used in the distributed setup. Could the authors provide more details? It would also be helpful if they compared results across the proposed versions (v1 and v2) to support the theoretical claims better.\n* Regarding Theorem 5, the idea of applying an unbiased estimation twice is interesting. However, in this case, shouldn’t we also observe a reduction in stochastic variance with the number of workers and, consequently, accelerated convergence, as is typically seen in standard distributed learning?\n* The authors did not provide any supplementary material. Since no code for the empirical evaluation is available, it is difficult for the community to verify and assess the contribution of the proposed approach."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XDPg1gN31D", "forum": "Lk0tQYo76o", "replyto": "Lk0tQYo76o", "signatures": ["ICLR.cc/2026/Conference/Submission15147/Reviewer_fqxZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15147/Reviewer_fqxZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760516019044, "cdate": 1760516019044, "tmdate": 1762925463194, "mdate": 1762925463194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper shows that signSGD with momentum can achieve a convergence rate of $\\mathcal{O}(T^{-1/4})$ using constant batch sizes without additional assumptions such as large batch sizes or unimodal symmetric stochastic noise. Experimental results are shown on CIFAR to demonstrate convergence rates."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Original tighter error analysis: \nBounds $\\sum_i |[\\nabla f(x_t)]_i| \\cdot \\mathbb{P}(\\text{sign mismatch})$ directly by $\\|\\nabla f(x_t) - v_t\\|_1$ instead of probability inequalities requiring $\\mathcal{O}(\\sqrt{T})$ batches or symmetric noise assumptions\n\nRemoves assumptions: \nNo $\\mathcal{O}(\\sqrt{T})$ batches or symmetric noise for $\\mathcal{O}(T^{-1/4})$ rate that are required for prior analysis\n\nExperimental validation: \nExperiments with ResNet on CIFAR dataset show faster gradient norm decay that matches theoretical results"}, "weaknesses": {"value": "Incremental improvements:\nThe improvements are incremental rather than paradigm shifting. The improved bound is useful but may not have a large practical impact\n\nWeak experimental results:\nThe experimental results are with ResNet on CIFAR datasets. These do not reflect modern uses cases of Sign-based methods. Experimental results on large models would make the paper stronger."}, "questions": {"value": "How would the method perform on a larger model and more realistic datasets?\n\nAssumption 8 $\\sup_x \\|\\nabla f_j(x;\\xi)\\|_\\infty \\leq G$ is strong for neural networks, even though the authors cite previous works that use similar assumptions. Do your experiments satisfy this? What are observed \\max_{t,j} \\|\\nabla f_j(x_t;\\xi)\\|_\\infty values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "buinjClkNm", "forum": "Lk0tQYo76o", "replyto": "Lk0tQYo76o", "signatures": ["ICLR.cc/2026/Conference/Submission15147/Reviewer_2GAT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15147/Reviewer_2GAT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945635824, "cdate": 1761945635824, "tmdate": 1762925462869, "mdate": 1762925462869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper revisits sign-based stochastic optimization with momentum (a.k.a. Signum) and provides tighter convergence analyses. First, under separable smoothness and separable bounded noise, the authors show that signSGD with momentum achieves the classical non-convex rate $O(T^{-1/4})$ without the large-batch or unimodal-symmetric-noise assumptions used in earlier work. Under the standard $\\ell_2$-smoothness and bounded-variance assumptions, they further prove an $O(d^{1/2}T^{-1/4})$ rate, improving the prior $O(dT^{-1/4})$ dependence for momentum-based sign methods. For distributed majority-vote settings, they propose an unbiased server-side sign operator and derive improved rates, e.g., $O(d^{1/2}T^{-1/2}+dn^{-1/2})$ and $O(\\max(d^{1/4}T^{-1/4}, d^{1/10}T^{-1/5}))$, that outperform prior results in both $d$ and $T$. Empirically, CIFAR-10 (centralized) and CIFAR-100 (distributed, 8 nodes) experiments with 10 seeds support the claims."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper delivers a theoretical tightening for sign-based momentum methods in both centralized and distributed settings. On the centralized side, it attains the classical non-convex rate $O(T^{-1/4})$ for Signum under separable smoothness and bounded noise without resorting to large-batch or restrictive noise-shape assumptions, and under standard $\\ell_2$-smoothness it improves the dimension dependence from $d$ to $d^{1/2}$. On the distributed side, introducing an unbiased server-side sign operator leads to sharper dependencies on $d,T$, and the number of workers $n$, strengthening the case for majority-vote style aggregation. \n\n2. The technical pivot, controlling sign error through a cleaner estimator-error recurrence, reads as careful and broadly applicable, and the presentation is clear, with assumptions and algorithms spelled out and tables that position the results against prior analyses. \n\n3. Empirically, though modest in scale, the CIFAR-10/100 studies with multiple seeds are consistent with the theoretical story and demonstrate that the proposed analyses map to competitive performance in practice."}, "weaknesses": {"value": "1. The empirical scope is narrow: results focus on CIFAR-10 (centralized) and CIFAR-100 with eight nodes (distributed), leaving open how the methods behave in larger-scale, highly heterogeneous, or bandwidth-constrained regimes. \n\n2. The theory relies on specific schedules for step size and momentum, yet the experiments use grid-tuned constants without ablations that test sensitivity to the prescribed schedules, which blurs the link between bounds and practice. \n\n3. The comparative breadth could be stronger: beyond SGDM/AdamW and selected sign baselines, results omit contenders such as error-feedback compressors or recent variance-reduced sign methods that would sharpen the empirical positioning."}, "questions": {"value": "See Weaknesses. In particular:\n1. Can you extend the evaluation beyond CIFAR-10 (centralized) and CIFAR-100 with eight nodes (distributed), e.g., to larger-scale, in order to assess how the methods behave in those settings?\n\n2. Can you add ablation testing sensitivity to the theorem-prescribed step-size and momentum schedules versus the grid-tuned constants, to clarify the link between the bounds and practice?\n\n3. Could you include additional baselines, such as error-feedback compressors or recent variance-reduced sign methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "adV41tP7sm", "forum": "Lk0tQYo76o", "replyto": "Lk0tQYo76o", "signatures": ["ICLR.cc/2026/Conference/Submission15147/Reviewer_gFkG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15147/Reviewer_gFkG"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761965347937, "cdate": 1761965347937, "tmdate": 1762925462046, "mdate": 1762925462046, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper establishes theoretical convergence guarantees for a class of stochastic optimization algorithms that employ gradient-sign updates (signSGD) augmented with momentum. \n\nThe analysis primarily focuses on the **Signum** algorithm in centralized settings and its distributed variants based on majority voting. \n\nThe main motivation is to address limitations of prior analyses, which required either large batch sizes or restrictive noise assumptions to attain the optimal convergence rate of (O(T^{-1/4})). \n\nEmpirical results on image classification tasks (CIFAR-10/100) show fast convergence compared to several established baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper shows that their algorithm reduces the large-batch requirements and improves the dimension dependency in the theory of sign-based optimization.\n\n2. The analysis is conducted under multiple standard assumptions, making the results robust and widely applicable, and the distributed analysis also accounts for heterogeneous data settings.\n\n3. The experimental results demonstrate superior performance in both centralized and distributed environments."}, "weaknesses": {"value": "1. It remains unclear how the proposed algorithm compares with Ref. [1]. The method in Ref. [1] uses a fixed mini-batch size and imposes no noise assumptions, yet achieves an O(T^{-1/3}) complexity, whereas the present work reports only O(T^{-1/4}). Although the authors note that Ref. [1] assumes component-wise smoothness while this paper assumes global smoothness, both assumptions appear mild.\n\n2. While the theory significantly improves the dependence on the dimension $d$, the experiments do not explicitly validate this. Performance gains are demonstrated on fixed-dimension tasks (e.g., CIFAR), but an ablation study varying $d$ would have provided stronger empirical support for the theoretical claims.\n\n3. The related work section mentions error-feedback (Karimireddy et al., 2019) but does not discuss in depth how the proposed methods compare to these techniques in theory or practice, particularly regarding robustness and performance when the sign operation introduces significant bias.\n\nRef [1]. Wei Jiang, Sifan Yang, Wenhao Yang, and Lijun Zhang, Efficient Sign-Based Optimization: Accelerating Convergence via Variance Reduction, NeurIPS 37, pp. 33891–33932, 2024."}, "questions": {"value": "We already have the Signum algorithm—why is there a need to introduce MVSM? Could you better illustrate the motivation behind MVSM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "PqvwEmRCGP", "forum": "Lk0tQYo76o", "replyto": "Lk0tQYo76o", "signatures": ["ICLR.cc/2026/Conference/Submission15147/Reviewer_pMhc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15147/Reviewer_pMhc"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15147/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762011796207, "cdate": 1762011796207, "tmdate": 1762925461365, "mdate": 1762925461365, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}