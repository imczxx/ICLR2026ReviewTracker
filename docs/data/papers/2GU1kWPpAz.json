{"id": "2GU1kWPpAz", "number": 21686, "cdate": 1758320522410, "mdate": 1759896908718, "content": {"title": "Generalized Inference Time Unlearning --- Effective for A Fraction of the Cost", "abstract": "Large Language Models (LLMs) can memorize and regurgitate sensitive training data, creating significant privacy and safety risks. While existing unlearning aim to address these risks, current methods are often computationally prohibitive and/or significantly degrade model utility. We introduce a framework for Inference-Time Unlearning, a new paradigm that steers an LLM's output at inference time using small secondary models, without altering the base model's weights. Through extensive experiments with LLMs we demonstrate that our method is highly effective at removing targeted verbatim and semantic knowledge, is orders of magnitude more computationally efficient than traditional approaches, and fully preserves the base model's general capabilities. We then explore efficacy in unlearning visual semantics in generative image models and find similar evidence of effectiveness. Finally, we introduce a new benchmark focused on unlearning time-dependent information. Collectively, the framework offers a practical, scalable, and low-cost solution for selective forgetting, enabling more responsible and adaptable model deployment. All code to reproduce this work is available at the following anonymous link: https://anonymous.4open.science/r/inference-time-unlearning-iclr2026/", "tldr": "Using two small models to guide generation from a large model, we achieve comparable unlearning performance across benchmarks without sacrificing utility.", "keywords": ["Unlearning", "memorization", "privacy", "safety", "generative models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c624ba7636511649e469cdc4e803a1c84090770f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposed an inference-based unlearning method, reducing the overhead of fine-tuning the LLM. The method based on divergence decoding, which two distributions are built to guide the LLM token decoding. The experimental results shows its effectiveness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper proposed time-based unlearning benchmark, enabling more diverse evaluation. \n\n2. The proposed method is applicable to different data types, including text and image."}, "weaknesses": {"value": "1. As pointed by the limitation 2 in the paper, as this method does not really change the internal representation of a model, it is more like guardrails instead of unlearning. Moreover, could the authors provide a concrete scenario how the proposed method would be used?\n\n2. The inference time cost analysis miss the important factor: run time latency, as the proposed method will have to recompute the distribution every token, could the authors provide the total runtime before and after applying the proposed unlearning approach? Unlike other approaches, like GA, it could be costly during the fine-tuning but it should work as is during the inference."}, "questions": {"value": "1. I wonder what is the connection of the proposed method to the LLM watermarking, which is also tiling the distribution in a certain way.\n\n2. For time-based unlearning benchmark, what is the difference if we simply categorize those to-be-forgotten timeline into forget set and remaining go to retain set?\n\n3. What is the effect of the ratio of model size of p and q (not absolute size)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JdBzmP995q", "forum": "2GU1kWPpAz", "replyto": "2GU1kWPpAz", "signatures": ["ICLR.cc/2026/Conference/Submission21686/Reviewer_CrGY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21686/Reviewer_CrGY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761959596992, "cdate": 1761959596992, "tmdate": 1762941890069, "mdate": 1762941890069, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Divergence Decoding (DD), an inference-time technique for unlearning. The technique involves finetuning 2 smaller models: 1 on a \"retain set\" that includes knowledge that should not be forgotten, and 1 on the \"forget\" set contains information on the concept that should be forgotten. The logits of the base model are then adjusted by the difference in logits of the forget and retain models. The authors propose a linear method of DD, as well as a rank-based method. They also introduce a new time-unlearning benchmark"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The problem of Unlearning is important and timely \n- Inference-time unlearning techniques, such as the proposed approach, are needed as finetuning is costly and prone to harming generalizability. \n- The technique of using two proxy models to adjust the logts of the base model is clever\n- The proposed approach is somewhat backed by theory, as the authors relate it to Product of Experts and importance sampling."}, "weaknesses": {"value": "- The experimental results lack error bars / confidence intervals. \n- Unclear whether DD outperforms NP on verbatim knowledge of the forget set on MUSE (figure 1)\n- The biggest weakness in my eyes is the time-unlearning benchmark. I may be missing something, but frankly I do not see how this fits in with the rest of the paper. No unlearning methods, DD or otherwise, are evaluated on the proposed Time Unlearning dataset in the main paper. I also don't understand how this dataset relates to unlearning, since it focuses on lookahead bias rather than data removal per se. The paper would be much stronger if it removed this dataset and instead performed a more comprehensive analysis on standard unlearning benchmarks."}, "questions": {"value": "- How does the Time Unlearn dataset relate to Unlearning? Why are no Unlearning methods evaluated on it? \n- I don't understand Figure 3. Can you explain in more detail what each axis represents? Why is a flat line desirable? Is DD performing poorly compared to NPO and SimNPO?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Oa3QL1IqVO", "forum": "2GU1kWPpAz", "replyto": "2GU1kWPpAz", "signatures": ["ICLR.cc/2026/Conference/Submission21686/Reviewer_UHuj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21686/Reviewer_UHuj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033797693, "cdate": 1762033797693, "tmdate": 1762941889848, "mdate": 1762941889848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new paradigm, namely Inference-Time Unlearning (ITU), which aims to remove unwanted knowledge generation from large language models without modifying their weights. The approach trains two much smaller auxiliary models — a forget expert fine-tuned on the forget set, and a retain expert fine-tuned on the retain set. During inference, these experts adjust the logits of the base model using a method called Divergence Decoding (DD), which steers the output distribution away from tokens upweighted by the forget expert and toward those upweighted by the retain expert. Two variants are proposed: a linear logit adjustment and a rank-based token suppression. This inference-time mechanism achieves effects that are similar to “unlearning” of targeted content while being orders of magnitude more computationally efficient than gradient-based methods and preserving general utility across MUSE and TOFU benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- **New paradigm on LLM output steering.** The proposed method aims to efficiently approximate the data distribution of a target large model ($\\hat Q$) using two introduced much smaller fine-tuned experts ($p,q$) plus the original model ($P$), achieving substantial computational savings while maintaining controllability.\n\n\n- **The formulation is concise.** The derivation connecting Divergence Decoding to Product of Experts and importance sampling is mathematically coherent and provides interpretability. \n\n- **Extensive empirical verification.** The author demonstrates the effectiveness of proposed methods on multiple benchmarks and visual distributions: MUSE, TOFU, image distribution and the introduced “Time-unlearning benchmark”"}, "weaknesses": {"value": "- **Dependence on auxiliary models.** The method assumes well-trained retain and forget experts but gives little detail on how to build them when data are limited or noisy, leaving room for bias or domain overlap artifacts.\n\n- **Evaluation gaps in image generative domains.** The image unlearning experiment relies solely on FID scores, which are inadequate for assessing semantic forgetting. The work would be beneficial to employ automated analysis verifying that specific visual concepts are unlearned while others remain intact, or include visual examples for qualitative evaluation. \n\n- **Conceptual ambiguity.** The method steers outputs but does not alter the model’s internal representations or parameters. Hence, it does not “remove” knowledge but suppresses its expression on targeted contents. This is better categorized as inference-time filtering or redirection, not unlearning in the formal sense.\n\n- **Missing relevant work discussion.** Training-free steering-based approach for controlled model generation is not a completely new paradigm. While this work focuses on logit space steering, prior work on activation space steering [1][2] needs to be discussed, and potentially, compared efficiency, as steering-based methods all claim benefits on computational efficiency and utility preservation.\n\n[1] Steering Language Models with Activation Engineering\n\n[2] Programming Refusal with Conditional Activation Steering"}, "questions": {"value": "- **Clarification on experimental setup.** Are there any specific retain/forget set curation involved?\n\n- Could you explicitly explain how proposed method handle non-targeted content generation?\n\n- See weaknesses for other questions/suggestions"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "kD9lNoBEEa", "forum": "2GU1kWPpAz", "replyto": "2GU1kWPpAz", "signatures": ["ICLR.cc/2026/Conference/Submission21686/Reviewer_tTAn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21686/Reviewer_tTAn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21686/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762225273840, "cdate": 1762225273840, "tmdate": 1762941889603, "mdate": 1762941889603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}