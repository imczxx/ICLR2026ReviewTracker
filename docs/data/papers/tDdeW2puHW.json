{"id": "tDdeW2puHW", "number": 8245, "cdate": 1758076119108, "mdate": 1759897796533, "content": {"title": "From Real to Synthetic: A Fine-grained Dataset and High-fidelity Biomechanical Model for Animal Behavior Understanding", "abstract": "Rat behavior research contributes to the exploration of human disease mechanisms. However, existing datasets are scarce and cover limited behavior types, hindering the analysis and modeling of complex behavior patterns. We constructed ActionRat, a new multi-view rat behavior dataset that, for the first time, captures diverse actions during free exploration and brain-computer interface (BCI) control. It combines real and synthetic sequences with fine-grained keypoint annotations and atomic action sequences, supporting broader behavior analysis tasks. To efficiently generate synthetic data for dataset expansion, we developed OpenRatEngine, a high-fidelity 3D virtual biomechanical model. This model integrates anatomical priors from computed tomography (CT) scans, kinematic constraints, and lifelike appearance, reducing the domain gap between synthetic and real data. Equipped with pose control, OpenRatEngine generates synthetic sequences with accurate 3D keypoint annotations. We evaluated behavioral uncertainty quantification and animal pose estimation tasks on the ActionRat dataset, and demonstrated the outstanding synthetic data generation capability and realism of OpenRatEngine. Extensive experiments across deep learning models confirmed the effectiveness and value of both real and synthetic data.", "tldr": "A Fine-grained Benchmark Dataset and High-fidelity Biomechanical Model for Animal Behavior Understanding", "keywords": ["Animal dataset", "Biomechanical model", "Synthetic data generation", "Behavioral uncertainty quantification", "Video understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/662f6f7672435e5bae0b049e5ab22a2629dd53aa.pdf", "supplementary_material": "/attachment/9a03b58a852188da3f20c411a8d9beb9a1d1c313.zip"}, "replies": [{"content": {"summary": {"value": "The authors present ActionRat, an open source dataset comprised of 3D keypoints and action segmentation labels for rat behavior during free exploration and brain stimulation. They also present OpenRatEngine, a biomechanical rat model that is capable of producing realistic synthetic rat behavior data including 3D keypoint trajectories, meshes, and 2D projections onto static camera views. The authors benchmark both the ActionRat dataset and the OpenRatEngine trajectories with several experiments."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The ActionRat dataset is a valuable asset for the computer vision and behavioral quantification communities. Including a range of atypical and pathological behaviors is essential for capturing a wider range of behaviors that is crucial for training more robust and generalizable models.\n\nThe OpenRatEngine produces highly realistic looking behaviors, and serves as a template for creating similar simulators for other species. The authors have done a good job creating a model that is properly biophysically grounded and visually similar to experimental data, an impressive feat in and of itself."}, "weaknesses": {"value": "The main weakness of this paper are the experiments. It's not clear to me how these properly highlight the benefits of the ActionRat dataset or the OpenRatEngine.\n\nFirst off, the \"Behavioral Uncertainty Quantification\" task is never explicitly defined - what is this, and what is it supposed to be testing? - I'm also confused as to why additional datasets are included here, this just feels like a comparison of the different baseline models and doesn't at all focus on ActionRat; their inclusion distracts from the main point of the paper.\n- The text says all models take keypoint sequences from the ActionRat V1 dataset as input - where did these keypoints come from? They aren't mentioned previously. Are these from DLC? If so, was DLC trained with real data, synthetic data, or both?\n- How am I supposed to interpret Table 2? Is the point that the numbers across datasets are similar? I'm not sure that tells me much about about the quality of the ActionRat dataset.\n- L358: \"action diversity predicted using the ActionRat dataset is higher than that of Animal Kingdom\" - Animal Kingdom contains data from a wide range of species, these numbers will not be directly comparable. In my opinion the more interesting question, that actually speaks to the strengths of this dataset, is \"how does action diversity compare when a model is trained on spontaneous vs spontaneous+stimulated behavior?\" This at least can help with the argument that freely moving behavior on its own is not sufficient.\n- L359: \"CCVAE and MCENET...confirming their strong capacity to model motion uncertainty\" - the experiments should focus on demonstrating the strengths of the ActionRat dataset, not comparing model architectures.\n\nFor 3D pose estimation, I am also unclear exactly what the experimental setup is. \n- DeepLabCut models seem straightforward: 2D pose estimators are trained (on all views together? different network per view?) using human annotations, triangulation is run on the stereo views, and then reprojected to the camera 1 image plane (why not compute MPJPE in 3D space?). \n- For OpenRatEngine, how are the synthetic data created? This relates to an earlier question I had. If there are ground-truth annotations for frame t in video v, are other ground truth annotations before and after time t used for the data generation process, and time t represents an interpolated time point? I think I'm missing something important here.\n- L407: \"OpenRatEngine achieves lower errors on most keypoints\" - seems like the ratio is closer to 50/50?\n- L412: \"Results validate...the advantages of OpenRatEngine-generated synthetic data in improving accuracy and robustness\" I see the value of this dataset differently - I think it provides a lot of synthetic data to train pose estimation models that will themselves then be more robust. An experiment that test this would be the following: imagine you have 500 human labeled frames. You train a DLC model, then evaluate it on held-out data (importantly, I think the *animals* themselves should be held out to properly address generalizability and robustness, i.e. train a model on R1-R4 and test on R5 and R6). Then train another DLC model using the same 500 (or whatever) frames from before, plus another 1k or 2k synthetic labels from OpenRatEngine. THe performance on the held-out data should be much better in this case, indicating your synthetic data has been useful for training a better pose estimation model.\n- again, one of the strengths of your dataset is the brain stimulation that results in a more diverse range of poses. You can dig into this more deeply by training on human annotations during non-stimulated periods, then testing on human annotations during both stimulated and non-stimulated periods. If you look at performance split by period I bet it will be much worse during the stimulated period where there are more novel poses. Then youc an train a model on human annotations from both periods, and test on both periods (maybe controlling for the number of training frames), and ideally see reduced errors in the stimulated period, indicating more robustness. then you can repeat this type of experiment using both real and synthetic data. I think there are lots of permutations here, each making their own subtle point.\n\nLack of clarity in some of the writing\n- L34: \"tracking the positions of gait\" doesn't make sense, gait is the tracking of limbs over time\n- L139: SLEAP is mentioned in the middle of a list of datasets but is not itself a dataset\n- the final ActionRat dataset has 8679 segments - does each segment contain just a single behavior? if not, is every frame in the segment separately labeled?"}, "questions": {"value": "L75 - should the Meijer reference actually be Bolanos et al 2021? at the very least, the Bolanos reference should be included here\n\nThe authors state that BCI interventions \"enhance behavioral diversity and achieves comprehensive coverage of motion patterns\", but this is never quantitatively verified. A simple way to do this would be to take 3D poses during non-stimulated periods and compute PCA on the poses (after doing egocentric alignment to remove uninteresting factors of variation). Then repeat with stimulated plus non-stimulated periods (perhaps taking an equal number of frames from each category, or considering other forms of controls). Plotting variance explained versus number of PCs should show much higher dimensionality for the full dataset. Of course there are other ways to this, this is just a simple suggestion.\n\ntypo L104: rodennt -> rodent\n\nThe authors suggest that fine-grained variations like sniffing and micro-movements are captured in their synthetic data, but I fail to see how this is possible given the interpolation between sparse keyframe methodology. Am I missing something here?\n\nRelated: it is not clear to me exactly what the pipeline for generating a behavioral sequence looks like, and a brief description of this, at the beginning of section 3.2, would help. From what I understand\n1. a (random?) set of sparse keyframes are generated. how are they generated? are these taken from the labeled data? if so, how are 12 labeled keypoints translated to the 60 synthetic keypoints? if they are not taken from the labeled data, how are they constrained to be plausible poses? how \"sparse\" are they in time? if these are\n2. Interpolation is applied to the 3D keypoints. What kind of interpolation? Are there instances where smooth interpolation would actually lead to implausible poses? Does smooth interpolation mean the synthetic dataset has no abrupt movements?\n3. A mesh is created on each frame using the interpolated 3D poses\n4. The mesh is projected into each 2D view\n5. Fur is rendered(?) in each 2D view\n6. Other visual features are added like noise and lighting\n\nL182: Victor et al. should be Lobato-Rios et al\n\nSection 4.3 is more along the lines of the kind of evaluation I was expecting. It might make sense to put this experiment first, demonstrating the consistency between real and synthetic data. Then the following experiments can move beyond that and show how a large amount of realistic synthetic data can lead to improved behavioral models.\n\nTable 4: Is it possible the consistency between real->real and syn->real is less about the data and more about the model architecture saturating performance (or something else)? What is a control experiment that could rule out this option?\n\nTable 5: I'm not sure how to interpret this table/analysis. I thought at first these metrics are being computed between real and synthetic trajectories (one frame at a time) but the caption says \"adjacent frames\" and the text says \"temporal consistency\". So are these values computed between times t and t+1? for which traces? It would be helpful to clarify the relationship bewteen real predictions, synthetic predictions, and time here."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "woISoqSc9K", "forum": "tDdeW2puHW", "replyto": "tDdeW2puHW", "signatures": ["ICLR.cc/2026/Conference/Submission8245/Reviewer_pF94"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8245/Reviewer_pF94"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659891635, "cdate": 1761659891635, "tmdate": 1762920189226, "mdate": 1762920189226, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents ActionRat, a multi-view video dataset of rat behavior, and OpenRatEngine, a synthetic rat animation and rendering framework designed to generate realistic pose sequences. The dataset contains ~609K annotated frames across seven behavioral categories (including some BCI-evoked actions), while OpenRatEngine reconstructs 3D rat meshes from CT scans, applies inverse kinematics (IK) control, and uses a contour-based optimization for pose alignment. The authors evaluate synthetic–real correspondence using keypoint reconstruction and motion metrics, and report a small domain gap between real and synthetic data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The pipeline for CT-derived modeling, IK-based control, and rendering is well executed and clearly described.\n- Combining real multi-view recordings with synthetic renderings in a unified dataset is a useful step toward obtaining better correspondence between simulation and behavior\n- The dataset includes stimulation-induced actions, which could open opportunities for modeling causal intervention or neural-behavioral decoding"}, "weaknesses": {"value": "- The ActionRat dataset (609K frames) is significantly smaller than existing benchmarks such as Rat7M or PAIR-R24 and does not introduce new behavioral contexts, species, or task diversity. The listed seven behaviors are standard (e.g. rearing, grooming, walking), and diversity is asserted but not quantified.\n- The only nominal addition, BCI-evoked behaviors, is underdeveloped, as no downstream applications (e.g. stimulation decoding, closed-loop control) are demonstrated.\n- OpenRatEngine combines standard components: CT-derived skeletons, mesh rigging, inverse kinematics, and contour-based fitting. Similar pipelines exist (e.g. Rat7M synthetic, Animal3D, RatSim), and the paper does not demonstrate a quantitative or methodological advance over them.\n- The evaluations reproduce existing pose-estimation benchmarks (MPJPE/MPJVE) rather than defining new tasks that exploit the unique BCI metadata or synthetic flexibility. Without a concrete downstream problem, the practical value of ActionRat remains unclear.\n- The authors claim that the synthetic-to-real domain gap is small, yet no systematic tests support this. Results are reported on the same rats and camera setups used for training. It remains unclear whether models trained with synthetic data generalize to unseen rats, new recording sessions, or unseen viewpoints.\n- Diversity is neither quantitatively defined nor contextualized against other datasets. All subjects are male, of one strain, and recorded in a fixed apparatus. Hence, diversity appears limited to modest variation in BCI conditions."}, "questions": {"value": "1. How is behavioral diversity measured? Please provide a comparison to Rat7M or other datasets\n2. What practical tasks can leverage the BCI metadata? Could this dataset enable learning of stimulation-to-behavior mappings or causal behavior prediction? An illustrative example would clarify its relevance\n3. Are models evaluated per rat or across subjects? A leave-one-subject-out test would show whether the dataset generalizes beyond individual-specific idiosyncrasies\n4. How does stimulation parameters map to specific behavioral classes or motion trajectories?\n5. Beyond pose estimation, what tasks can exploit both real and synthetic modalities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "uZLMB42ntD", "forum": "tDdeW2puHW", "replyto": "tDdeW2puHW", "signatures": ["ICLR.cc/2026/Conference/Submission8245/Reviewer_ZMKX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8245/Reviewer_ZMKX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716601623, "cdate": 1761716601623, "tmdate": 1762920188706, "mdate": 1762920188706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper present a multi-view rat behavior dataset named ActionRat, which captures diverse actions during free-exploration and brain-computer interface control. Despite real-captured data, ActionRat also contains expanded synthetic data. To generate synthetic data, the authors further developed OpenRatEngine, which is a 3d virtual biomechanical model with lifelike appearance. OpenRatEngine could generate accurate 3d keypoint annotations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tThis paper is clearly written and well motivated. Action recognition and motion capture are important for understanding the behavior of rodents (e.g. rats). This paper presents a new dataset featuring diverse behaviors and high quality annotations. \n\n2.\tPreviously, recordings about the abnormal behaviors of rats are rarely seen. I believe this dataset may play an important role for the community to understand the abnormal behaviors of rats."}, "weaknesses": {"value": "0. The main weakness is lack of technical contributions. The technologies  used in this paper have been  well explored in the past. \n\n1.\tAs recording abnormal behaviors is one of the feature of the video dataset, I would be better to show some real-captured video cases of such video footage in supp video. Existing video only shows the openratengine virtual renderings. \n\n2.\tThe appearance of virtual rat seems limited. Only a white rat appearance was employed to generate the dataset, raising some issues about generation to other kinds of rats."}, "questions": {"value": "I do not see critical flaws of the paper. The paper is self-contained with limited technical contributions. However, the problem itself and the data provided are interesting. Collecting such data requires heavy efforts, I do believe technical tricks are not the only criteria for publication. This is why I give a relatively positive rating. \n\nSome minor questions about techniques: \n\n3.\tAt L. 298, how was the weights iteratively refined? Automatically or manually? \n\n4.\tHow was the contours discrepancy metric defined? Were multi-view contours enough to control the detailed motion of rat? \n\n5. L. 300, “coherent” -> “coherence”."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "kmjgAWsIRM", "forum": "tDdeW2puHW", "replyto": "tDdeW2puHW", "signatures": ["ICLR.cc/2026/Conference/Submission8245/Reviewer_JWJZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8245/Reviewer_JWJZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761911154345, "cdate": 1761911154345, "tmdate": 1762920188207, "mdate": 1762920188207, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper claims two main contributions:\n\nActionRat Dataset: A multi-camera (three-camera) recording dataset of rat behavior with detailed annotations, including 2D keypoints with and without brain stimulation, and a subset of segmented clips labeled by action category. The authors report that the distribution of action categories differs from freely exploring conditions.\n\nOpenRatEngine: A biomechanical rat model derived from CT scans, used to simulate action sequences. The virtual rat is manually registered to selected keyframes from real videos, and Blender interpolation is used to generate continuous motion time series."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The inclusion of brain stimulation perturbations adds an interesting causal-intervention dimension that could be valuable for neuroscience and behavior modeling research.\n\nThe dataset includes rich annotations of animal behavior categories, which may support downstream supervised or semi-supervised learning studies.\n\nThe CT-based rigging and virtual modeling pipeline are described transparently and could serve as a reference for other labs interested in synthetic animal data."}, "weaknesses": {"value": "The paper does not convincingly argue the need for this dataset from a machine learning perspective. The dataset is relatively small and of lower video quality compared to existing open datasets (for instance, Rat7M Dunn et al.). To strengthen the machine learning contribution, the authors should provide evidence that the dataset exposes failure modes or limitations of current methods, or it enables learning under novel condition. \n\nThe OpenRatEngine rigging and interpolation rely on existing software (manual alignment and Blender interpolation). The authors did not demonstrate applications or downstream tasks that showcase the usefulness or superiority of the OpenRatEngine. For example, evaluating how simulated sequences improve behavior classification, pose estimation for large data."}, "questions": {"value": "1. The introduction mentions stimulation-specific behaviors (e.g., spasms and other unique responses). Why aren't these represented as new action categories in the dataset?\n\n2. Is the OpenRatEngine manually registered to the keyframes, or does it involve any machine learning methods?\n\n3. In Table 2, the best model achieves comparable performance on the ActionRat dataset relative to other datasets. To better support the claimed contribution, could the authors compare model performance separately for freely moving vs. stimulation conditions, and show whether including the BMI data improves learned priors for behavior prediction?\n\n4. In Table 3, the 3D pose estimation using DeepLabCut appears to rely on binocular cameras, with reprojection to a monocular view, while the synthetic data are generated using all three cameras (Eq. 5). Is this a fair comparison, given the different numbers of input views? Please also report variance, and the mean and variance across keypoints to make the improvements clearer.\n\n5. For Figure 3, could the authors provide quantitative evaluation metrics beyond qualitative examples? For instance, applying both reconstruction methods to unseen data and comparing with real animal trajectories."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ydhxEBzYr4", "forum": "tDdeW2puHW", "replyto": "tDdeW2puHW", "signatures": ["ICLR.cc/2026/Conference/Submission8245/Reviewer_Wxag"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8245/Reviewer_Wxag"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762146070016, "cdate": 1762146070016, "tmdate": 1762920187834, "mdate": 1762920187834, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the OpenRatEngine model and the Action rat dataset. The OpenRatEngine model is a biomechanical model of a rat with bone lengths and positions estimated from CT scans of 5 rats. The ActionRat dataset is a mix of a 3 camera recording of real rat movements with various neural stimulation to trigger diverse actions as well as a synthetic dataset generated from the OpenRatEngine model. The pose in the real recording is annotated with a DeepLabCut model training on 1500 real images.\n\nThe authors benchmark various algorithms for predicting the pose temporal trajectory and compare the OpenRatEngine model to DeepLabCut for 3D pose estimation. Finally they compare the kinematics of the synthetic data to the real data by testing how temporal prediction models generalize across the two datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "Both the ActionRat dataset and the OpenRatEngine model are novel contribution to the biomechanical modeling of rat behavior. The ActionRat dataset has a diverse set of rat behaviors in an open field, and complements Rat7M, the only other 3D rat dataset currently available. The OpenRatEngine models 60 joints on the rat body, which is indeed more than the 38 actuators modeled by Aldorondo et al, 2024. \n\nThe benchmarks of the temporal models will be really useful for the development of future similar models on the ActionRat dataset. With these benchmarks I think researchers may really develop more models to predict animal motion, which is exciting."}, "weaknesses": {"value": "The 3D pose estimation evaluation felt quite weak to me, both in terms of methodology description and results. As I understand, the authors compare a DeepLabCut estimator to the OpenRatEngine for estimating the 3D pose of the animal. \nThe test data is not properly specified. How many ground truth annotations for evaluation are there? Figure 2 says that there are 6558 annotated frames, which presumably is 1500 real images for DeepLabCut (as detailed in section 3.3) and 5058 frames fit by the OpenRatEngine from contours. If they do not overlap, what is the evaluation done on?\nBesides this, the evaluation results seem to show that the OpenRatEngine is really quite comparable to DeepLabCut, whereas the qualitative comparison in Figure 3 really shows how much more detailed OpenRatEngine is. It's unclear whether the poor quantitative performance of OpenRatEngine is due to poor fitting of OpenRatEngine model to the rat contours, due to some quirk of the evaluation data, or something else. There really should be more details on the model fitting to data and on the evaluation data.\n\nOn the dataset itself, it's unclear how much automatically annotated data it actually contains. Out 609K frames, there are only 6558 annotated frames. Are the remaining 602K frames annotated automatically (perhaps with DeepLabCut) so that they can be useful for temporal prediction? \n\nCompared to Rat7M, this dataset also does have much more occlusions due to having fewer cameras. This should be noted in the limitations perhaps.\n\nSome small typos:\nLine 104 - rodennt should be rodent\nLine 182  - Should be Lobato-Rios et al 2022 simply, no Victor"}, "questions": {"value": "See questions in Weaknesses\n\n- Why not calibrate all 3 cameras and use them all for triangulation? The 3D tracking would improve quite a bit. \n- Why are the two ears not modeled in the OpenRatEngine model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FHoGNf4wbM", "forum": "tDdeW2puHW", "replyto": "tDdeW2puHW", "signatures": ["ICLR.cc/2026/Conference/Submission8245/Reviewer_gBuZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8245/Reviewer_gBuZ"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission8245/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762190374880, "cdate": 1762190374880, "tmdate": 1762920186622, "mdate": 1762920186622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}