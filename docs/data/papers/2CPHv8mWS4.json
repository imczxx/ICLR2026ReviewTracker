{"id": "2CPHv8mWS4", "number": 6306, "cdate": 1757966371305, "mdate": 1759897923306, "content": {"title": "From Supervision to Exploration: What Does Protein Language Model Learn During Reinforcement Learning?", "abstract": "Protein Language Models (PLMs) have achieved significant breakthroughs in computational protein science through pre-training on large-scale sequence databases and leveraging scalable network architectures. Concurrently, Reinforcement Learning (RL) has demonstrated substantial progress across multiple protein design tasks by enabling expanded exploration capabilities and precise multi-objective optimization. While RL has shown transformative potential in natural language processing by enabling models to discover emergent capabilities beyond their training distributions, its capacity to unlock latent functional patterns within protein sequence space remains underexplored. In this study, we investigate whether RL-enhanced PLMs can transcend their pre-training limitations and identify implicit sequence-structure-function relationships not explicitly encoded in foundational datasets. Through systematic evaluation across four critical protein design domains—antimicrobial peptide (AMP) design, kinase optimization, antibody engineering, andinverse folding—we employ diverse RL algorithms and model architectures to address this fundamental question. Our comprehensive analysis demonstrates that RL reliably improves sampling efficiency across domains and, more importantly, that its effectiveness is governed by a three-factor interaction: task difficulty, reward model accuracy, and policy capacity. Gains scale when rewards are accurate and informative, policies have sufficient capacity to realize the signal, and tasks present headroom beyond supervised learning; conversely, noisy rewards or capacity bottlenecks cap improvements despite exploration. This principled view offers practical guidance for RL in protein design: prioritize reward refinement before scaling policy size, match RL algorithms and regularization strength to task difficulty, and allocate capacity where marginal gains are largest.", "tldr": "", "keywords": ["Deep Learning", "Protein Design", "Reinforcement Learning", "Protein Language Model"], "primary_area": "applications to physical sciences (physics, chemistry, biology, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cf987207bc3b69f39009687ad00dd456071a29f8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents a comprehensive study investigating what reinforcement learning (RL) teaches Protein Language Models (PLMs) during the fine-tuning process for protein design. The central question is whether RL enables PLMs to discover new, emergent capabilities beyond their pre-training data, or simply amplifies existing patterns. Through an analysis of two leading PLM architectures, three RL algorithms, and four prominent experimental systems, this paper concluded that RL enables more efficient sampling of high-reward regions. However, RL’s ability to learn new patterns and optimize high-reward distributions comes with trade-offs. These effects are influenced by factors such as the initialization capacity of the policy model (base model), reward accuracy, and task complexity."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper is trying to answer a critical and timely question in the field: \" Do new emergent capabilities arise during the RL fine-tuning process of PLMs?\"\n2. This work systematically evaluates RL-enhanced PLMs across multiple biological systems and reports consistent improvements in sampling efficiency and optimization performance. \n3. This paper provides a clear guidance. The performance is influenced by three factors such as the initialization capacity of the policy model (base model), reward accuracy, and task complexity."}, "weaknesses": {"value": "1. The exploration of policy capacity is relatively shallow. Policy capacity is described as a combination of model size and representational power, but it's not systematically quantified or varied. The study uses different base PLMs for different tasks, making it difficult to isolate the effect of capacity alone. A more rigorous test would involve using the same RL algorithm and reward on the same task while systematically scaling the size of the policy model (e.g., from 100M to 7B parameters) to directly show how gains scale with capacity.\n2. The paper answers what RL does (efficient sampling) but provides less insight into how it does it or what specific knowledge is encoded. For example, while latent space visualizations (UMAP) show distributional shifts, they don't explain which sequence-structure-function rules the model has internalized. Does it learn physic-chemical principles, conserved motifs, or folding constraints? The study identifies the effect (prioritizing high-TM-score regions) but not the mechanism. A deeper analysis linking the RL-guided changes to specific, interpretable biological features (e.g., changes in hydrophobicity, charge distribution, or known functional sites) would be more compelling.\n3. It's unclear if the framework holds for more complex, multi-objective tasks (e.g., design a stable, potent, and non-toxic AMP) where reward shaping becomes even more critical."}, "questions": {"value": "1. The paper states that the policy model's initial capacity is a key factor. To what extent are your results simply a reflection of the knowledge already present in the pre-trained base models? Is RL primarily \"unlocking\" or \"focusing\" the base model's latent knowledge, rather than teaching it anything fundamentally new about protein biophysics?\n2. The paper concludes RL is an \"efficient sampler\" and does not create \"new emergent capabilities.\" Could this be a matter of definition? If RL successfully guides the model to a functionally novel region of sequence space that the base model could technically reach but whose functional value it did not appreciate, isn't that a form of learning a new \"sequence-function relationship\"?\n3. It seems \"Task Difficulty\" is often defined by the accuracy of the reward signal and the complexity a policy must capture. Could this be reformulated as a two-factor model: \"Signal Quality\" (reward accuracy) and \"Model Capability\" (policy capacity to exploit the signal given the task's complexity)?\n4. In practice, creating a highly accurate reward model (e.g., with a Spearman >0.9) is often the most expensive part of the process. How should a practitioner decide on the sufficient level of reward accuracy before investing in larger policy models, given diminishing returns?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "t6MF9KPpSu", "forum": "2CPHv8mWS4", "replyto": "2CPHv8mWS4", "signatures": ["ICLR.cc/2026/Conference/Submission6306/Reviewer_k7Wy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6306/Reviewer_k7Wy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761814700630, "cdate": 1761814700630, "tmdate": 1762918607291, "mdate": 1762918607291, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors evaluate the application of three RL algorithms, PPO, DPO, and GRPO, for four protein design tasks: inverse folding, antimicrobial peptide design, kinase mutation, and antibody optimization. Their results show that applying RL can lead to improved performance in these tasks, but also impact on other metrics such as diversity and a “shrinkage” metric."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors apply RL to refine pLMs for four different protein design tasks.\n2. Results show the improved performance of RL methods compared to the base pLM model.\n3. The research question is interesting and timely."}, "weaknesses": {"value": "1. The authors mention that their framework offers a principled way to measure RL-PLM systems, but their evaluation is limited to a descriptive analysis, missing any theoretical analysis regarding these systems.\n2. For the 4 tasks in Section 2, it is unclear which methods were implemented by the authors and which are the changes from the main references.\n3. The quality of the Figures makes it hard to evaluate Section 3."}, "questions": {"value": "My initial recommendation is rejection based on the weaknesses mentioned above. My detailed comments are as follows.\n\nComments:\n\n1. (lines 103-106) In the Introduction section, three key factors are mentioned: task difficulty, reward model accuracy, and policy model capacity. However, limited discussions are made in Section 3 to justify how these factors are chosen.\n2. (lines 111-113) The authors mention a “principled way to measure the current RL-PLM systems” but the results are analyzed in a descriptive manner. Which methodology differences are made in the manuscript evaluation that support this sentence and that can be used for future RL-PLM systems?\n3. Section 2: It is unclear which parts are taken from references, which methods are re-implemented, and the different characteristics of rewards and surrogates between different tasks, so that it is hard to draw conclusions about the performance of different methods in Section 3.\n4. RL methods with different characteristics are evaluated: PPO, DDPO, GRPO. Their different characteristics influence the rewards that can be used, their training methodology, etc. More discussion regarding their differences is needed for a principled evaluation approach.\n5. What does the axis mean in Fig. 3?\n6. The quality of figures should be improved. The readability of Figures 4 and 5 is limited, which makes it hard to evaluate the results described in Section 3.\n7. How can the results in Table 1 be interpreted? Are they averages for different RL methods?\n8. For the inverse folding section 3.2, is ESMFold used both as a reward function and as the structure predictor used for evaluation?\n\nMinor Comments (that did not impact the score):\n\n1. Missing the definition of the acronym RL in line 81.\n2. The writing quality of the manuscript is weak, with arguable terms like “transcend” and “gains additional power”.\n3. Code is not available.\n4. The paragraphs in lines 426-431 and lines 432-439 seem redundant."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4CZqylmILd", "forum": "2CPHv8mWS4", "replyto": "2CPHv8mWS4", "signatures": ["ICLR.cc/2026/Conference/Submission6306/Reviewer_SMxQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6306/Reviewer_SMxQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902811476, "cdate": 1761902811476, "tmdate": 1762918606906, "mdate": 1762918606906, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper evaluates how reinforcement learning post-training strategies affect protein language models in four protein design tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- First work to systematically evaluate the RL post-training in protein language model regime, comparing DPO, PPO, and GRPO.\n- Introduce and report a set of insightful metrics, such as ESR, diversity, and novelty.\n- Robust evaluation: evaluate across four different biological benchmarks of different flavor"}, "weaknesses": {"value": "1. **Lack of baselines**. It’s really hard to gauge how good the performance **improvement** is without the baseline performance. This is critical since many works in this field rely on task-specific models, and I think it’s important to answer the question of \"can RL fine-tuning on foundation model surpass or comparable to task-specific models?\"\n    - For example of inverse folding, I strongly believe authors should have included ProteinMPNN and/or ESM-IF results.\n2. The use of **Pass@k** as a main evaluation metric seems ill-suited for biological tasks. While it makes sense in NLP contexts (where users can verify multiple generations), it does not translate well to protein modeling, where humans cannot practically validate multiple sequences. For example, what does Pass@16 even mean for inverse folding?\n3. Moreover, the four tasks considered are inherently **continuous measures,** so converting them into binary success thresholds (for Pass@k) discards valuable information. Reporting raw or normalized reward values would provide a more faithful representation of model quality. Although the paper does show score distributions (e.g., Fig. 4C, 4G, 5C, 5G), the main results in Fig. 3 still rely on arbitrary cutoffs.\n4. **Writing.** Several parts of the paper are unclear.\n    - Line 19: \"Unlock latent functional patterns within protein sequence space\" it’s unclear what this actually means.\n    - Line 74: \"Complex, non-differentiable biological objectives such as TM-score\" TM-score may not be the best example of a truly non-differentiable objective; there exist differentiable functions for structure accuracy (e.g. FAPE). A better example would strengthen this point.\n    - Figure 1 and metaphor of hill climbing isn’t very informative or even confusing. Authors mention “task difficulty sets the height of the summit to be scaled, reward accuracy determines the climbing direction, and policy-model capacity fixes the starting altitude.” (line 107-108) then say “These factors jointly shape whether RL can climb towards subspaces with stronger task alignment or stall in suboptimal plateaus.” (line 109-110) None of these factors actually shape the landscape itself, so the reference to “plateaus” becomes unclear.\n5. **Lack of holistic analysis.** The discussion section doesn’t synthesize the results across all experiments. There’s little reflection on how task difficulty, reward model accuracy, and policy capacity jointly influence post-training outcomes or how these findings might generalize to new biological tasks.\n6. **Unanswered framing question.** Line 97 asks, “Do new emergent capabilities arise during the RL fine-tuning process of PLMs?” but the paper doesn’t substantively address this. **Demonstrating that a model performs tasks it was trained for isn’t evidence of emergence. A deeper analysis of novel functional behaviors or unseen protein designs would make this claim more convincing.**"}, "questions": {"value": "- 3.1 Datasets (Line 232) For other three datasets train/test split looks reasonable, but what is the train/test split for kinase mutation? How do you prevent leakage between train and test?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F50iWUBOwi", "forum": "2CPHv8mWS4", "replyto": "2CPHv8mWS4", "signatures": ["ICLR.cc/2026/Conference/Submission6306/Reviewer_vAS6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6306/Reviewer_vAS6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6306/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761928510804, "cdate": 1761928510804, "tmdate": 1762918606500, "mdate": 1762918606500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}