{"id": "aiPC7jAd2f", "number": 938, "cdate": 1756824378080, "mdate": 1759898234715, "content": {"title": "Confidence Calibration in Vision-Language-Action Models", "abstract": "Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed.  To this end, we present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural language instructions to low-level robot motor commands.  We examine how task success relates to calibration error and how calibration evolves over time, and introduce two lightweight fixes to remedy the miscalibration we observe: prompt ensembles and action-wise Platt scaling.  Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification.", "tldr": "We present the first systematic study of confidence calibration in vision-language-action (VLA) foundation models, to develop the tools and conceptual understanding necessary to render VLAs trustworthy via reliable uncertainty quantification.", "keywords": ["robotics", "VLA", "uncertainty quantification", "calibration"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/897d74315382538d826908863b07aef02e453c45.pdf", "supplementary_material": "/attachment/2a145d1d660742d994691b2cabd617ccaffc4afe.zip"}, "replies": [{"content": {"summary": {"value": "This paper is about calibration on vision-language-action models (VLAs). The authors explore the limitations of standard calibration/uncertainty techniques when applied to VLAs. Calibration has not really been explored for VLAs, making this the first work to touch the topic, and the paper makes contributions aiming to enable future research on VLA calibration.\n\nThe contributions are:\n- An evaluation of success rate vs calibration error, finding that better task performance leads to better calibration error.\n- Introduction of prompt ensembling for VLAs by making semantic modifications to the input prompt and taking average confidence.\n- Extension of platt scaling to VLAs by applying platt scaling for each action dimension, and results showing that calibration does vary across action dimensions.\n- Analysis of calibration over task time, like a time series, with interesting conclusions."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 4}, "strengths": {"value": "- The paper is well written and easy to follow.\n- Calibration is an open problem in general for ML models and specifically for VLAs, I believe this combination has not been explored before and this paper makes novel contributions to enable future research on VLAs and uncertainty/calibration.\n- The paper explores multiple aspects of calibration in VLAs, (1) defining calibration for VLAs as based on the confidence vs success rate, and then evaluating if success rate is related with calibration error. (2) Prompt ensembling for VLAs, by making semantic variations of the prompt, passing them through the model and then combining the predictions, this method is very well known in LLMs but this is the first time I see it applied to VLAs, and it does improve calibration error. (3) An extension of platt scaling for VLAs, noting that since a VLA outputs multiple action dimensions, the each dimension should be calibrated separately and this is supported by the experiments. (4) Analysis of calibration over task time, which treats the problem as a sequence of actions, each with a confidence score, and there are interesting insights here on how calibration error behaves across sequence points (what the authors call task completion).\n- The experimental setup and experiments seem appropriate to me. The selection of baselines is also good (token probabilities) as well as using more modern uncertainty estimation methods like ensembles or platt scaling. The paper makes experiments on the LIBERO suite of tasks which is perfect for VLAs.\n- Overall the contributions of this paper are strong and they open new venues for future research on VLA calibration."}, "weaknesses": {"value": "- Some figures should be explained more clearly. Figure 4 in particular, the calibration plots in the two lower rows are not clear to me, what is the meaning of the dashed line? Since its a calibration plot I would expect the diagonal line to be perfect calibration but the dashed line does not match this expectation. Also the text in Sec 3.3, the text refers to figures using unclear terminology (\"top row\", etc), I think it is much better and readable to use figure numbers + letters to refer to subplots.\n- Additionally about Figure 4, this result is about the spatial task suite, but this contains multiple instances of the spatial task, so its not clear what the plot represents, is it average task performance over task completion? Or median or another aggregation metric? I expect this to be clarified.\n- Figure 6 makes comparisons between several methods but there is no comparison to the baseline method (token probabilities). This comparison should be made to put the ECE values into context.\n- Overall the captions in this paper are not informative enough for the reader and should be improved, for example Figure 3 just mentions a comparison, but how should the reader interpret these plots? They compare different metrics between baseline and reprompt ensemble, which means values above the diagonal are improvements, but this is implicit and should be mentioned in the caption.\n\nOne minor weakness, I consider ECE to be mostly ECE_1, there is really no value in this paper on presenting ECE_2, it does not add any relevant information and it should be skipped."}, "questions": {"value": "I only have one question, about the results about calibration on task time, it is not clear to me if results on Fig 4 generalize to all the task realizations inside the LIBERO spatial task suite? Could the authors clarify"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "29013F2Sj5", "forum": "aiPC7jAd2f", "replyto": "aiPC7jAd2f", "signatures": ["ICLR.cc/2026/Conference/Submission938/Reviewer_tjhn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission938/Reviewer_tjhn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761515397234, "cdate": 1761515397234, "tmdate": 1762915644230, "mdate": 1762915644230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on the problem of calibration in vision-language-action models (VLAs). VLAs are gaining popularity in the robotics research community due to multimodality. The paper seeks to measure the confidence of action predictions of VLA models.\n\nThe main contributions of this work are:\n\n- evaluation to measure the relationship between task success and calibration error across multiple datasets and variants of the OpenVLA model.\n- a lightweight, Bayesian-style method that averages a VLA’s confidence across semantically-similar rephrasings of prompts.\n- an analysis of calibration over task time.\n- The findings show that lower task performance is correlated with lower calibration scores."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Originality: From my understanding of the literature, there are no previous works that studied the problem of calibration in VLA models. The authors focus on this problem, which has not been previously studied.\n- Quality: Overall, the paper is decently presented with high-quality results for calibration scores in the OpenVLA setting. Looking at the code, it seems to be cleaned up well and is transparently released for reproducibility.\n- Clarity: Overall, the paper is clear, but needs improvement around the claims around the comprehensiveness of the study (see the weaknesses here).\n- Significance: I think this work has the potential to impact how VLA models are developed and improved in the community. The finding that task performance and calibration score are correlated is not all too surprising, but the impact of this finding is notable, especially as one might attempt a more targeted data collection."}, "weaknesses": {"value": "- The authors claim to have a comprehensive evaluation, but this paper focuses exclusively on OpenVLA. Over the past year or so, various other VLA models have been made open-source, so evaluating on other open-source VLA models (Octo, Pi-0, etc) would significantly strengthen the claims in this work.\n- As the authors acknowledge, all evaluation is performed in simulation, leaving out other sources of potential confidence miscalibration, including sensor noise, etc."}, "questions": {"value": "- What is Figure 2 displaying? Which tasks are evaluated?\n- The authors claim that the work is a “comprehensive evaluation to measure the\nrelationship between task success and calibration error across multiple datasets and VLA variants.” But what are all the VLA variants studied? Unless I misunderstood, this is referring to just the OpenVLA and its finetuned models, but I was expecting evaluation on other VLA models as well.\n- Have the authors considered studying the benefits in calibration score measurement for targeted data collection and finetuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HWSPXfs3A4", "forum": "aiPC7jAd2f", "replyto": "aiPC7jAd2f", "signatures": ["ICLR.cc/2026/Conference/Submission938/Reviewer_5yfq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission938/Reviewer_5yfq"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761517662631, "cdate": 1761517662631, "tmdate": 1762915644119, "mdate": 1762915644119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies confidence calibration in Vision-Language-Action (VLA) models, a new class of large multimodal foundation models that connect visual perception, natural language, and control. The authors identify that while these models are becoming central to embodied AI, their confidence outputs are not well understood. They present the first systematic calibration analysis for VLAs using OpenVLA on the LIBERO benchmark and explore how calibration relates to task success, how it evolves during execution, and how to improve it through simple post-hoc methods. They propose three lightweight techniques and show that these substantially improve calibration without retraining. The study is novel and relevant; the methodology is clear, and the metrics are well-documented."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper is clear, well-structured, and grounded in the calibration literature. The experimental design is sound, using multiple metrics (ECE, Brier, NLL) and analyzing calibration across tasks, time, and model precision. The results are consistent and interpretable. For example, better-performing models are also better calibrated, and early overconfidence decreases over time. The proposed fixes are simple but effective and practical for real-world systems."}, "weaknesses": {"value": "- Methods are empirical adaptations rather than theoretical advances.\n- All experiments are simulation-only; no real-robot validation. More generally, it is not clear what role robotics plays in this framework. \n- Only one model family (OpenVLA) and one benchmark were tested.\n- Missing comparisons to other post-hoc calibration baselines.\n- Discussion of broader implications (safety, planning) could be deeper.\n- The evaluation is entirely simulation-based and limited to OpenVLA on LIBERO tasks, which restricts how confidently the conclusions can be generalized. The authors should demonstrate calibration performance on at least one real-robot setup or different VLA architecture (e.g., RT-2, π₀) to validate the robustness of their approach.\n-  The reliance on one benchmark makes it unclear whether the observed improvements hold under sensor noise, dynamic lighting, or embodiment variation, all critical for real deployment. \n- The study also lacks ablation or sensitivity analysis on the number of prompt rephrasings, calibration-set size, and hyperparameters of the scaling functions, which could expose overfitting or instability. \n- Finally, stronger comparisons against standard post-hoc baselines would make the results more convincing. Without these additions, the empirical evaluation, while neat and well-presented, feels too narrow and over-controlled for a paper claiming to improve confidence calibration for “real-world embodied systems.”\n\n\nMinor comments:\n--Include error bars for ECE and Brier results.\n--Clarify binning strategy for ECE.\n--Discuss briefly how calibration could be integrated during training.\n--Expand limitations to connect better with real-world deployment."}, "questions": {"value": "- What is the role the fact that this is a robotics system plays in the suggested approach? It seems to be a general approach that was tested on robotics.\n\n- In the examined setting, does it make sense to only consider the most likely action and not consider a more robust version?\n\n- What do you expect to be the core sim-to-real hurdles this framework would address (beyond the cost of evaluating real-world robotic systems)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "gYGMh60TaC", "forum": "aiPC7jAd2f", "replyto": "aiPC7jAd2f", "signatures": ["ICLR.cc/2026/Conference/Submission938/Reviewer_EuHf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission938/Reviewer_EuHf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761586310565, "cdate": 1761586310565, "tmdate": 1762915644000, "mdate": 1762915644000, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents a study of confidence calibration of vision-language action models. It analyzes the correlation between model confidence and task success rate, as well as model confidence across different perturbed language instructions. Experiments on LIBERO with OpenVLA demonstrate that calibration error decreases with higher task success rates and longer trajectories."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well-motivated. The problem of confidence calibration is highly relevant for deploying trustworthy robotic systems in high-stakes, real-world environments"}, "weaknesses": {"value": "- The paper's technical novelty is limited. The proposed methods are largely simple applications of existing techniques to an existing VLA.\n### The claim of a systematic study is not well-supported by the experiments:\n- All experiments are conducted on a single VLA, OpenVLA. This is not enough to claim a systematic study of calibration errors for VLAs. Diffusion-based VLAs might exhibit different calibration behavior.\n- All experiments are conducted in the LIBERO simulation. For a study on this topic, the lack of any diverse simulation (or real-world) experiments is a significant weakness.\n- The paper's default choice of using the pre-action confidence (ci,1​) as the primary metric is not well-justified. OpenVLA is only trained on single-image observations and produces a single action. It has no explicit understanding of task progress. Thus, it is unclear if the confidence at the very first step is representative of the entire trajectory's confidence. Further ablations are required to validate this choice.\n- No experiments on zero-shot or out-of-domain environments, and the relation to model confidence\n- Other aggregation methods are not evaluated or ablated\n\nIn its current state, I think the paper is not ready for publication at ICLR. While the problem of VLA calibration is highly relevant to robotics and underexplored, the paper's technical novelty is somewhat limited. More importantly, the experimental scope is not sufficient to support the claims of a systematic study."}, "questions": {"value": "- What is the reasoning behind choosing the first frame's confidence as the default metric? Did you experiment with other aggregation metrics?\n- What does the calibration look like in OOD scenarios? e.g., with new objects or unseen settings?\n- What does the confidence look like without model fine-tuning?\n- Why do you compare against quantized versions of the same model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MxqebC1JsI", "forum": "aiPC7jAd2f", "replyto": "aiPC7jAd2f", "signatures": ["ICLR.cc/2026/Conference/Submission938/Reviewer_Cqir"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission938/Reviewer_Cqir"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission938/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924795168, "cdate": 1761924795168, "tmdate": 1762915643888, "mdate": 1762915643888, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}