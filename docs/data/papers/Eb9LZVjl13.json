{"id": "Eb9LZVjl13", "number": 20987, "cdate": 1758312403872, "mdate": 1762996828827, "content": {"title": "RADAR and PFIM: Content-Prior Patch-Space Modulation for Efficient Vision Transformers", "abstract": "Positional encodings in Vision Transformers, relative (iRPE, ROPE) or otherwise, help to reason about space but remain content-agnostic. We introduce a lightweight, content-aware patch modulation that injects a quasi-positional prior computed from pre-trained patch embeddings. We present two light weight, drop-in pre-MHSA modules: RADAR (anchor-conditioned distance priors that modulate tokens) and PFIM (parameter-free importance scaling with no new trainable parameters beyond the logit layer). Both keep the ViT backbone frozen, preserve the attention kernel, and add negligible to no overhead.\n\nOn CIFAR-100 with absolute positional encoding, RADAR boosts Top-1 accuracy by +7.5 pp and Top-5 by +3.3 pp over vanilla ViT, and by +4.1 pp / +1.6 pp over a strong single-CPE baseline. PFIM improves vanilla ViT by +2.0 pp(Top-1) and +1.1 pp (Top-5), performing on par with Single-PEG within a small margin. Improvements are statistically significant across seeds (paired t-test, 95% CI). RADAR contains 56% and PFIM 88% , fewer trainable params compared to Single-PEG on CIFAR100. By turning latent patch geometry into content-aware priors, our approach reallocates attention to semantically relevant regions, offering parameter-efficient gains ideal for low-budget training.", "tldr": "We changed Patch Embeddings from Pre-trained ViTs using their relative distances from each other to improve performance on downstream tasks like classification.", "keywords": ["Vision Transformers", "Computer Vision"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/015052235c29141c80d2bfb721d4d2a40d9d864c.pdf", "supplementary_material": ""}, "replies": [{"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "QtgLchHFLQ", "forum": "Eb9LZVjl13", "replyto": "Eb9LZVjl13", "signatures": ["ICLR.cc/2026/Conference/Submission20987/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20987/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762996827971, "cdate": 1762996827971, "tmdate": 1762996827971, "mdate": 1762996827971, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two lightweight, content-aware patch modulation techniques: RADAR (Relational Anchor-Distance Attentional Re-weighting) and PFIM (Parameter-Free Importance Modulation), for enhancing the efficiency and performance of Vision Transformers (ViTs).\nBoth modules operate pre-MHSA (Multi-Head Self-Attention), injecting content-aligned priors computed directly from pretrained patch embeddings, without modifying the ViT backbone or adding significant parameters.\n\nExperiments on CIFAR-10 and CIFAR-100 demonstrate that:\n\nRADAR boosts Top-1 accuracy by +7.5 pp over vanilla ViT and +4.1 pp over Single-PEG with 56% fewer parameters.\n\nPFIM achieves +2.0 pp Top-1 and +1.1 pp Top-5 improvements with up to 98% parameter reduction relative to Single-PEG.\nBoth approaches yield statistically significant gains under paired t-tests and equivalence tests across multiple random seeds."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. novel concept: introduces content-prior modulation in patch space, distinct from standard positional encoding.\n2. lightweight and modular: both RADAR and PFIM are drop-in additions requiring minimal computation or parameter cost.\n3. Transparent reporting: FLOPs, parameter counts, and reproducibility instructions are explicitly detailed in the appendix.\n4. Interpretable design: distance-based relational priors and entropy-based scaling have clear intuitive grounding."}, "weaknesses": {"value": "1. limited dataset scope: only CIFAR-10/100 experiments are presented. Evaluating on larger datasets would confirm scalability.\n2. Minor writing inconsistencies: some notation, spacing, and punctuation reduce readability.\n3. Underexplored visualization: the paper could include attention heatmaps or token-importance visualizations to illustrate how modulation changes model behavior.\n4. The teaser figure needs some improvement."}, "questions": {"value": "1. Could PFIM be integrated into dynamic token pruning or distillation pipelines to further reduce inference cost or not?\n2. How sensitive are both methods to $\\alpha$, $\\beta$ initialization and mix% hyperparameters?\n3. Have you tested RADAR/PFIM on larger backbones (e.g., ViT-B/32) or datasets like ImageNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eRJpsYJLjP", "forum": "Eb9LZVjl13", "replyto": "Eb9LZVjl13", "signatures": ["ICLR.cc/2026/Conference/Submission20987/Reviewer_uj3p"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20987/Reviewer_uj3p"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761436968615, "cdate": 1761436968615, "tmdate": 1763000001741, "mdate": 1763000001741, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two lightweight pre-MHSA modules — **RADAR** and **PFIM** — designed to provide content-aware modulation for Vision Transformers (ViTs). RADAR introduces anchor-conditioned distance priors, while PFIM applies parameter-free importance scaling based on patch entropy. The methods target parameter-efficient gains on CIFAR-10/100 with frozen ViT backbones."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Reasonable motivation for pre-attention content priors.  \n- Lightweight implementation with negligible parameter cost.  \n- Shows modest gains over vanilla ViT."}, "weaknesses": {"value": "1. **Very limited experiments.**  \n   Only evaluated on CIFAR-10/100; no ImageNet or larger models. Reported gains are minor and dataset scale is insufficient to claim significance.\n\n2. **Poor figure and presentation quality.**  \n   Figures are unclear and inconsistent; architecture diagrams fail to illustrate the design clearly.\n\n3. **Unconvincing ablation and analysis.**  \n   Statistical tests and significance claims are weak; experiments are run on too few seeds.\n\n4. **Missing comparisons.**  \n   Absent evaluation against contemporary efficient adaptation methods (LoRA, AdaptFormer, VPT, etc.).\n\n5. **Weak writing quality.**  \n   Numerous grammatical issues, inconsistent structure, and overcomplicated pseudo-math."}, "questions": {"value": "Please refer to the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "RY3Wu2AA6r", "forum": "Eb9LZVjl13", "replyto": "Eb9LZVjl13", "signatures": ["ICLR.cc/2026/Conference/Submission20987/Reviewer_myTj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20987/Reviewer_myTj"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856932452, "cdate": 1761856932452, "tmdate": 1763000002193, "mdate": 1763000002193, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces RADAR and PFIM, two lightweight methods designed to bias a transformer's attention toward task-relevant tokens. Both methods achieve this by modifying patch embeddings based on global context before they are processed by the transformer layers.\n\nRADAR uses pretrained patch embeddings to compute \"anchor tokens\" and then uses the resulting anchor distance offsets to modulate each patch's features. PFIM, in contrast, scales patch embeddings based on an \"importance\" score, which is computed using token-wise, task-specific entropy. The authors show that both RADAR and PFIM outperform Vanilla ViTs and Single PEG on CIFAR-10 and CIFAR-100 classification."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "* Simple and Practical: Both RADAR and PFIM are easy to add to existing ViTs, and do not require major architecture changes. \n* Performance: Both methods show improved performance compared to vanilla ViT and Single-PEG on CIFAR classification"}, "weaknesses": {"value": "1. The experiments are confined to low-resolution (32x32) CIFAR datasets. To demonstrate practical relevance and scalability, the authors should benchmark RADAR and PFIM on ImageNet, which is the standard for vision classification models. It also makes less sense to benchmark on CIFAR when using tokens from models pretrained on ImageNet.\n2. The ablation studies are limited, as they only test specific hyperparameter combinations. A more rigorous approach would involve systematic hyperparameter sweeps (varying one while holding others fixed) to isolate the impact of each. Furthermore, critical experimental details are omitted, including learning rate, batch size, and cosine scheduling parameters.\n3. If pretrained tokens are already required, then the authors should use direct feeding of these tokens (with regular PEs) as a baseline. The pretrained model's performance (instead of just Vanilla ViT) should also be measured.\n4. All experiments are conducted only on classification tasks. Can the method be applied to object detection, and semantic segmentation?\n5. Both methods depend on pretrained representations, yet they are only tested with a single pretrained model. Can the authors test robustness using other pretrained models (e.g., DINO), which might offer more semantically rich token representations?\n\n**6. Poor Clarity and Organization**\n\nThe paper's organization and writing are unclear, making it extremely difficult to follow.\n\n**Figures**: Figures 1 and 2 are small, disorganized, and lack informative captions. They are not referenced until late in the paper (Section 4.2) and waste space depicting the standard transformer architecture instead of clearly illustrating the novel RADAR and PFIM components.\n\n**Notation**: The mathematical notation is poor. It is inconsistent (e.g., $s$_$j$ on line 141 vs. $s_j$ on line 056; $X_j$ in Algorithm 1 vs. $x_j$ elsewhere) and unclear, with many variables introduced but never defined (e.g., $x_j, \\alpha, \\beta, s_j, b_j$ on line 056).\n\n**Missing Context and Citations**: Key related methods (e.g., FiLM) are poorly explained, while many others (T5, ALiBi, RoPE, Swin, LoRA) are mentioned without references. Furthermore, numerous acronyms (PEG, SSA, LOOSA, APE) are used without ever being introduced.\n\nOrganization and clarity are sufficient reasons for rejection for me; the paper is not in a state to be a published as-is."}, "questions": {"value": "1. How are vector_values obtained in Algorithm 1? Are they also used in Algorithm 2 or is this a typo?\n2. It is not clear to me what the 'entropy' score of a token is for PFIM. If $X_j$ are tokens, then how is $\\text{Pr}(X_j)$ computed, and what does it represent? It also not clear what is being summed in the expression for $H_j$. I'm skeptical this would be a good measure of importance, as opposed to other saliency measures. It would be nice if the authors could try using aggregated attention scores (e.g. average of CLS to patches, across layers/heads) from powerful SSL models such as DINO.\n\nPlease also refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bfT9mGiMlw", "forum": "Eb9LZVjl13", "replyto": "Eb9LZVjl13", "signatures": ["ICLR.cc/2026/Conference/Submission20987/Reviewer_9X5f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20987/Reviewer_9X5f"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761857436974, "cdate": 1761857436974, "tmdate": 1763000001593, "mdate": 1763000001593, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes two modules (RADAR and PFIM) that adapt a frozen ViT by injecting “content-aware quasi-positional priors” into patch embeddings before MHSA, using anchor-based distance features or entropy-based scaling. Experiments are conducted on CIFAR-10/100 using ViT-Base pretrained on ImageNet."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- The paper introduces a parameter-efficient ViT adaptation approach, which is shown to work on CIFAR10 and CIFAR 100.\n- The proposed methods seem reasonable."}, "weaknesses": {"value": "- Weak empirical evaluation: The experiments are limited to CIFAR-10/100, which are small and outdated for evaluating ViTs. The backbone used is ViT-Base (86M parameters) fine-tuned on CIFAR—a setting that is not meaningful, as small CNNs or ViT-Tiny models trained from scratch can achieve comparable performance. Despite broad claims, no additional benchmarks are included. Moreover, the paper positions itself as a parameter-efficient adaptation method but omits standard PEFT baselines such as LoRA, VPT, AdaptFormer, adapters. The only comparison is with Single-PEG, which is insufficient.\n\n- Misleading compute claims: RADAR increases FLOPs by 67% compared to the baseline and requires more epochs to converge, contradicting the claim of “negligible overhead.” PFIM’s efficiency gains do not justify the combined claims for both modules.\n\n- Limited novelty and conceptual clarity: The proposed method closely resembles existing techniques, such as ConViT, content-aware positional embeddings, and token-importance weighting. PFIM’s entropy scaling appears ad-hoc, and the paper’s claims of novelty are overstated."}, "questions": {"value": "Could the authors evaluate robustness to different pretrained checkpoints, patch sizes, training hyperparameters, and model architectures? This would help confirm that the method is not tuned to a single CIFAR training recipe."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U8T690dt7W", "forum": "Eb9LZVjl13", "replyto": "Eb9LZVjl13", "signatures": ["ICLR.cc/2026/Conference/Submission20987/Reviewer_6vGn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20987/Reviewer_6vGn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761886148953, "cdate": 1761886148953, "tmdate": 1762939562470, "mdate": 1762939562470, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}