{"id": "7HNRYT4V44", "number": 3838, "cdate": 1757547751949, "mdate": 1759898067325, "content": {"title": "RL's Razor: Why Online Reinforcement Learning Forgets Less", "abstract": "Comparison of fine-tuning models with reinforcement learning (RL) and supervised fine-tuning (SFT) reveals that, despite similar performance at a new task, RL preserves prior knowledge and capabilities significantly better. We find that the degree of forgetting is determined by the distributional shift, measured as the KL-divergence between the fine-tuned and base policy evaluated on the new task. Our analysis reveals that on-policy RL is implicitly biased towards KL-minimal solutions among the many that solve the new task, whereas SFT can converge to distributions arbitrarily far from the base model. We validate these findings through experiments with large language models and robotic foundation models and further provide theoretical justification for why on-policy RL updates lead to a smaller KL change. We term this principle $\\textit{RL’s Razor}$: among all ways to solve a new task, RL prefers those closest in KL to the original model.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Catastrophic Forgetting"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a9b3d93564c9b0142c39424d0e42b5a20fd13484.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper finds that fine-tuning base model by RL on new tasks can better resist catastrophic forgetting compared to fine-tuning by SFT. The authors find that the KL divergence between fine-tuned model and base model can be a strong predictor of catastrophic forgetting. RL forgets less because on-policy sampling inherently biases the model toward KL-minimal solutionns, while SFT may drift arbitrarily far to adapt to new tasks. Their empirical evidence and theoretical justifications support the claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The cor insight is simple but efficient: the extent of catastrophic forgetting is determined by KL divergence. That RL does better than SFT is because the on-policy nature which minimizes the KL divergence. This insight can bring more insights of RLHF and continual post-training research\n1. The takeaway messages are consice and clear, directly conveying the most important conclusions.\n1. The experiments including LLM and robotics control are comprehensive to illustrate the effectiveness and generality of the KL-based explanation, demonstrating consistent forgetting behavior and RL's advantage."}, "weaknesses": {"value": "1. The analysis of KL divergence in catastrophic forgetting is still high level:\n    1. The sampling distribution and KL divergence is a correlation analysis but not a causality explanation. A mathematics proof should be more helpful to support this finding\n    1. The paper does not provide a deeper examination of how KL drift translates into the loss on prior tasks. For instance, it is unclear whether the degradation arises from representational collapse on past tasks, or from changes in critical neurons that significantly affect previously acquired skills. A more fine-grained analysis would greatly strengthen the causal argument behind the proposed explanation.\n1. As KL divergence is an indicator to forgetting, it would be meaningful to add KL-regularized online and offline methods as baselines, e.g.,[1,2,3], which also extends the scope if minimizing KL divergence directly can mitigate forgetting.\n1. The experiments in the paper indicate that the forgetting is substantial in toy experiments, but noticeably milder in LLM-based tasks. However,Fig. 2 and Fig. 4 exaggerate the performance loss by compressing the y-axis. This plotting is misleading and give readers an inflated perception of catastrophic forgetting, overstating the empirical impact of the proposed phenomenon.\n\n[1] Azar, Mohammad Gheshlaghi, et al. \"A general theoretical paradigm to understand learning from human preferences.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2024.\n\n[2] Ethayarajh, Kawin, et al. \"Kto: Model alignment as prospect theoretic optimization, 2024.\" URL https://arxiv. org/abs/2402.01306.\n\n[3] Wu, Yifan, George Tucker, and Ofir Nachum. \"Behavior regularized offline reinforcement learning.\" arXiv preprint arXiv:1911.11361 (2019)."}, "questions": {"value": "1. In the toy experiment, forgetting is significant, but in LLM tasks, the absolute value of forgetting is small. Why is there such a big gap between the two? Is it due to differences in model size, optimization robustness, task overlap, or KL sensitivity across models of different sizes?\n1. The compression in Fig. 2 and Fig. 4 visually makes forgetting look more severe than the actual numerical value.  Would the author be willing to provide an absolute difference or a version with a uniform scale?\n1. The authors test alternative indicators in distribution change, weight change, and activation change in Sec. 6. Have the authors tested other distribution-level indicators, like JS divergence and Wasserstein distnace? Will they perform better?\n1. Is this finding only suitable for on-policy RL, what about off-policy RL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k25khJD4ho", "forum": "7HNRYT4V44", "replyto": "7HNRYT4V44", "signatures": ["ICLR.cc/2026/Conference/Submission3838/Reviewer_xH9f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3838/Reviewer_xH9f"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761461730231, "cdate": 1761461730231, "tmdate": 1762917059194, "mdate": 1762917059194, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates why reinforcement learning (RL) fine-tuning tends to forget less than supervised fine-tuning (SFT) when adapting foundation models to new tasks. Across LLM and robotics benchmarks (and a controlled ParityMNIST toy experiment), the authors show an empirical forgetting law: the degree of catastrophic forgetting is well predicted by the KL shift measured on the new-task inputs, $\\mathbb{E}_{x\\sim\\tau}\\big[ \\mathrm{KL}(\\pi_0\\|\\pi)\\big]$. They argue (and empirically validate) that on-policy RL is implicitly biased toward KL-minimal solutions among all policies that solve the new task — because RL samples from and reweights the model’s own outputs — while SFT can converge to arbitrarily distant solutions given the annotator distribution.\n\nThe paper’s contributions are threefold: (1) empirical demonstration that RL fine-tuning preserves prior capabilities better than SFT at matched new-task performance, (2) the KL–forgetting law showing forward (and backward) KL on the new-task distribution predicts forgetting, and (3) theoretical and experimental evidence for “RL’s Razor” — a proof that policy-gradient on-policy methods converge to the optimal representable policy closest in KL to initialization, plus ablations (oracle SFT, KL regularization, different objectives) that validate the mechanism."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The paper presents solid evidence for a novel and significant claim that online RL finetuning preserves the base model's knowledge better than SFT finetuning. This claim has far-reaching consequences for fine-tuning pipelines in foundational models, both in LLMs and Robotics (VLAs and Behavioral Foundation Models) space. In addition to an extensive empirical evaluation of SFT and RL finetuning on Qwen 2.5 models and OpenVLA, the paper also presents the forgetting phenomenon during fine-tuning from a theoretical perspective. I find the provided toy experiment on ParityMNIST to be illustrative and instructive for gaining intuition. The soundness of the experiment section is high-level, and the presentation is clear and of high quality."}, "weaknesses": {"value": "The paper does not present any obvious weaknesses."}, "questions": {"value": "One clarity remark - I am not sure how dist. 1 and dist. 2 differ - it's not easy to find in the text or in the Figure caption.\nAdditionally, I would like to know whether the authors plan to conduct any further experiments with VLA, as the paper currently only presents a single task: Pick and Place. I'm wondering whether the trend of catastrophic forgetting with SFT is more severe across some family of manipulation tasks and maybe less severe in some others? Can authors provide any intuition about that?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E8UI9UrshH", "forum": "7HNRYT4V44", "replyto": "7HNRYT4V44", "signatures": ["ICLR.cc/2026/Conference/Submission3838/Reviewer_muvc"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3838/Reviewer_muvc"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761581157133, "cdate": 1761581157133, "tmdate": 1762917058818, "mdate": 1762917058818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes the hypothesis that RL finetuning, compared to supervised finetuning leads to a minimal change (in the KL sense) in the model distribution, therefore minimizes forgetting. They partially attribute this behavior to the success of RLHF for large LLM finetuning. \nThe authors provide both empirical evidence on LLMs (Math reasoning, Science Q&A, Tool use) and a robotic task (Pick and Place) as well as on a toy task build out of MNIST and FashionMNIST that they call parityMNIST. Finally they formalize and provide some theoretical justification of their hypothesis. \n\nOverall I think the hypothesis is interesting. On the other hand I feel the paper is quite liberal in the claims that it makes and potentially (and most likely not intentionally) misinterprets some of the previous work."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "To me the main strength of the work is the hypothesis proposed. I think it could be the basis of a potentially a novel perspective, and provides an angle on the discussion between RLFT and SFT that I have not seen yet. I'm not aware of work looking at whether on-policyness affects catastrophic forgetting.  The paper provides some decent arguments in favor of their perspective, both empirically and theoretically, though I'm not sure if these arguments are sufficient."}, "weaknesses": {"value": "My main concern is probably with the generality of the claims (and potentially the lack of ablation to be sufficiently convincing). I would have preferred everything to be a bit more contextualised. Maybe the RL Razor hypothesis (not sure if it is the best name for the hypothesis) is fine -- which I understand it as the source of less forgetting being on-policy behaviour of RL compared to SFT. \n\nBut the empirical results are considerably more specific (e.g. it is about comparing GRPO in the RL context with AdamW for the SL context). Generally I think the main body of the paper is light on technical details (pushing many of these to the appendix), to a point that is harming the scholarly understanding of the work, brushing away some of the nuance of the results. I also think that the paper has not done sufficient work to isolate or ablate in order to show that it is the on-policyness that leads to this behaviour rather than other aspects (e.g. the regularization that is part of the GRPO objective which are also typical for certain optimization algorithms in SL).   \n\nIn terms of generality of the claims:  the choice of optimizer will greatly affect how far away you travel from the starting point. Is not clear to me that a different choice (e.g. natural gradient that includes a KL constraint not that different from GRPO/PPO but enforced more properly) would not lead to a similar minimal change in policy. Of course the approximation made to make the algorithm run efficiently can drastically impact things, and I can not tell if it would be equivalent or not. But that is the problem. The paper makes a broad argument about supervised learning, implying that this will be true for any choice of optimizer.  The same on the RL side, change in objective, change on the optimizer used in the RL setting will of course drastically change how much the model will traverse in functional space making it impossible to predict if the two settings will rank the way the authors are saying. So please be specific in claims.\n\nIn terms of ablations: GRPO contains a regularization as well as a clipping component (beside maximizing the likelihood of the action that gives bigger reward). Is not clear to me to what extent these components are the one driving the observed behaviour, compared to the argument made by the authors that it is because things are on policy. Specifically we know that PPO for example can diverge in RL settings, which to me is the opposite of doing minimal change in the policy in the KL sense. Actually a lot of RL algorithms suffer from such instabilities. How are these behaviours integrated in this hypothesis? \n\nTheory: While I understand the argument made for the RL algorithm, I'm not sure enough work has been put in the counterpart argument, that SFT does not reach a minimal change in the KL sense. Am I misunderstanding this? There does not seem to be any Lemma in the main paper saying that SFT does not do the same as RL, and is not clear to me that bar the optimizer used, SFT would not do the same as RLFT.\n\nI have two main issues with the theory: \n (1) It does not take into account the function approximator (i.e. the neural network) that I think can easily invalidate the argument. I understand that the neural network makes everything non-linear and hence any explicit/concrete mathematical argument is hard to make. But at least acknowledge this. Obviously due to shared parametrization, when the loss is minimized on a particular set of observation, this can change in arbitrary ways the behaviour of the model on not observed data (which is the main point of catastrophic forgetting). So while I appreciate the attempt of adding some theoretical results, I'm not sure to what extent they speak in favor of the hypothesis. \n\nAs a said note, a lot of continual learning work argues that is specifically the neural network (and its non-linear nature) that makes catastrophic forgetting interesting. \n\n(2) Maybe related to the above point. If you consider the tabular case. I do not see who being on policy vs not (in the case where the optimal policy is achievable) leads to different solutions. Maybe there is something to the argument that I'm missing."}, "questions": {"value": "1. Can you justify why not the KL constraint embedding in RL objective to stay close to the previous policy is not the main reason why these methods converge to a solution that has a minimal KL change? E.g. jumping to the appendix (lines 805-809) you say that for SFT equals the minimum-KL optimum policy. Why is that the case? What is the technical argument? In my view it should lead to a minimum -KL policy if that is representable by pi for any beta, no? Actually where is the proof of theorem A.1? \n\n2. There is no justification of why the KL computed on the new task data is sufficient to measure forgetness. Can you say more about this (particularly as I feel that implicitly in the later part of the paper you are implicitly making the claim that the fact that that CL algorithms look at the KL on the old data is somewhat problematic -- if that is not the intent, could you have a discussion of this in the paper and make your view explicit). I think it is easy to provide an argument of why KL on new data does not have to correlate with forgetness. E.g. if the old task and the new task have different supports, this metric will say very little about how much you forget (at least under a standard definition of forgetting). So it seems is more of an instance that the old task and new task are sufficiently similar (for an unknown definition of similar). Please also see the connection of this choice and algorithms like Learning without forgetting: https://arxiv.org/abs/1606.09282 which is not cited in current work.  In LwF paper the choice of computing on new data is justified from being cheaper (requiring not to store data) but no claim is being made that is more justified. But I think the predominant view is that the KL computed in the old data tracks the quantity we care, so if the paper tries to argue against this, it should be made more explicit and there should be more evidence for the it.\n\n3. I would rephrase a bit section 6. Actually similar claims are made in the intro. E.g. Kirkpatrick et al do not aim at minimizing change in parameter space. The objective of EWC is explicitly in functional space and talks about the KL term similar to the one you mention (except computed on the old data, so if the claim is that the particular choice of data is wrong please clarify and explain why). Most of CL works do this.  The approximation used to instantiate the algorithm leads to a parameter level regularization due to taking a second order taylor expansion of the KL term (and then approximating the Fisher by a diagonal). But in spirit most of CL algorithms explicitly target minimizing the change in functional space. It is just that minimizing this quantity it needs to be approximated by something cheaper. The framing currently sounds as if these works are minimizing the change in parameters hoping that will lead to less forgetting without connecting to the KL term or acknowledging it.  I would also argue that the claim that these metrics correlate weakly with the KL change is a bit misleading. They are direct approximations of the KL, and for sufficiently large models (where second order Taylor holds), they correlate quite well. Please provide evidence otherwise (and extend the technical description of what exactly is being run for e.g. for table 1). \n\n4. Can you actually have comparisons with at least some basic CL methods? At least replay, which is sometimes used with SFT. Or with SFT + KL regularization?\n\n5. Can you ablate further the RL algorithm used (e.g. PPO, reinforce) or rather remove terms of the GRPO objective to ablate what part of the algorithm empirically leads to small KL term (is the on-policyness or sampling from the model distribution, is it the clipping or KL regularization).  \n\n6.  Can you provide some comments regarding the theory part (outlined above)? I.e. discussing the tabular case, or at least discussing the role of the function approximator in this."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "I do not see the need of an ethic review for this work."}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rnUXcZUp3v", "forum": "7HNRYT4V44", "replyto": "7HNRYT4V44", "signatures": ["ICLR.cc/2026/Conference/Submission3838/Reviewer_8HAS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3838/Reviewer_8HAS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762005787597, "cdate": 1762005787597, "tmdate": 1762917058576, "mdate": 1762917058576, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows the effectiveness of using RL in post training in terms of the catastrophic forgetting. Especially, compared to using RL, post-training the model using SFT suffers from forgetting when the model tries to adapt to new tasks. Through extensive experiments, the authors show that using on-policy RL makes the model converge to much closer point from the base model, and using SFT can converge to the points far from the base model. In the experiments on analyzing the contributions of sampling distribution and negative examples to forgetting, the interesting point is that on-policy sampling mainly contributes to preventing the model from being apart from the base model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The main contribution on analyzing the RL in terms of the catastrophic forgetting is quite novel. Most of the experiment results are well-aligned to support the claim on the effectiveness of RL, and proposing the quantitative metric on measuring the forgetting has significant contribution."}, "weaknesses": {"value": "I think this paper is well-written, and cannot find the significant weaknesses.\n\n1. It would be better to specify that all of the RL approaches (e.g. RLVR or RLHF) are robust to the forgetting, and such robustness is still occurs when the model hacks the reward during RL training.\n\n2. It would be better to show the results in terms of the model sizes. I know training much larger LLMs takes enormous resources even though using LoRA tuning, but I still wonder the larger model is also robust to the forgetting."}, "questions": {"value": "1. Does this phenomenon also occurs in multi-trun RL? (e.g interaction with environments in agentic RL)\n\n2. Does the robustness come from the on-policy sampling or clipping in PPO-like on-policy methods?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "y2vBghMG8z", "forum": "7HNRYT4V44", "replyto": "7HNRYT4V44", "signatures": ["ICLR.cc/2026/Conference/Submission3838/Reviewer_Qf7W"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3838/Reviewer_Qf7W"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3838/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762148123582, "cdate": 1762148123582, "tmdate": 1762917058202, "mdate": 1762917058202, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}