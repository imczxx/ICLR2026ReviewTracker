{"id": "DwrwGl8BNE", "number": 19779, "cdate": 1758299260467, "mdate": 1759897020024, "content": {"title": "Spectral Neural Graph Sparsification", "abstract": "Graphs are central to modeling complex systems in domains such as social networks, molecular chemistry, and neuroscience. While Graph Neural Networks, particularly Graph Convolutional Networks, have become standard tools for graph learning, they remain constrained by reliance on fixed structures and susceptibility to over-smoothing. We propose the Spectral Preservation Network, a new framework for graph representation learning that generates reduced graphs serving as faithful proxies of the original, enabling downstream tasks such as community detection, influence propagation, and information diffusion at a reduced computational cost. The Spectral Preservation Network introduces two key components: the Joint Graph Evolution layer and the Spectral Concordance loss. The former jointly transforms both the graph topology and the node feature matrix, allowing the structure and attributes to evolve adaptively across layers and overcoming the rigidity of static neighborhood aggregation. The latter regularizes these transformations by enforcing consistency in both the spectral properties of the graph and the feature vectors of the nodes. We evaluate the effectiveness of Spectral Preservation Network on node-level sparsification by analyzing well-established metrics and benchmarking against state-of-the-art methods. The experimental results demonstrate the superior performance and clear advantages of our approach.", "tldr": "", "keywords": ["Graph Neural Networks", "Graph representation learning", "Graph sparsification", "Spectral Preservation Network", "Generative AI"], "primary_area": "learning on graphs and other geometries & topologies", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/9071abc9eaf79ea9dec80e12efaf18414fdf5231.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes a spectral neural architecture (SpecNet) that jointly learns graph structure and node features via a “Joint Graph Evolution” layer and then performs node-level sparsification guided by a spectral alignment loss, with the goal of producing a smaller graph that preserves the original graph’s Laplacian spectrum and feature geometry."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. The paper attempts to couple structure learning and sparsification: it proposes a Joint Graph Evolution (JGE) layer that is intended to update both the graph connectivity ($Q_{t+1}$) and node features ($H_{t+1}$), together with a Spectral Concordance loss meant to preserve spectral properties (e.g., Laplacian behavior, global structure) after pruning nodes.\n\n2. The experiments at least try to evaluate sparsification not only by edge count but also by higher-level structural indicators (largest connected component size, modularity, epidemic threshold, MASS)"}, "weaknesses": {"value": "The paper would benefit from a substantial revision under the guidance of a more senior researcher. In its current form, it’s difficult to evaluate. The abstract and introduction together span only about one page, are very high-level, and begin describing the method almost immediately. There is no sufficiently developed background, related work section or problem formulation, and the motivation is unclear. As a result, it’s not clear what specific task the paper is targeting or what the concrete contributions are.\n\n-------\n\nThe presentation is not professional. The paper spends nearly a full page of the main text re-explaining basic graph preliminaries such as adjacency matrices, incidence matrices, and in-/out-degree. While these concepts are relevant, they are undergraduate-level material and could be summarized more concisely or moved to the appendix. At the same time, Tables 1–3 are dense, visually cramped, and placed in layouts with large unused whitespace, which makes them difficult to read.\n\n-------\n\nIn Section 2.1, the proposed Joint Graph Evolution (JGE) layer is defined to output updated graph structure $Q_{t+1} \\in \\mathbb{R}^{r_{t+1} \\times r_{t+1}}$ and node features $H_{t+1} \\in \\mathbb{R}^{r_{t+1} \\times p_{t+1}}$.  \n   However, the intermediate matrix $J_{t+1}$ is stated to be in $\\mathbb{R}^{p_t \\times p_t}$, and then the paper defines  \n   $Q_{t+1} = \\sigma_1(J_{t+1} \\Phi_t)$ with $\\Phi_t \\in \\mathbb{R}^{p_t \\times r_{t+1}}$.  \n   Multiplying a $(p_t \\times p_t)$ matrix by a $(p_t \\times r_{t+1})$ matrix gives shape $(p_t \\times r_{t+1})$, not $(r_{t+1} \\times r_{t+1})$ as claimed for $Q_{t+1}$.  \n   The same mismatch appears again for $H_{t+1}$.  This is a basic correctness issue in the model specification.\n\n-------\n\nThe paper repeatedly refers to $Q_{t+1}$ as “a transformed graph topology with updated edge weights and a redefined node set,” implying that it is a learned adjacency for the next layer. But there is no constraint that $Q_{t+1}$ be a valid adjacency-like operator (e.g., nonnegative, sparse, symmetric, or even well-conditioned). It is simply $Q_{t+1} = \\sigma_1(J_{t+1}\\Phi_t)$, i.e. an arbitrary dense learned matrix. The paper also claims stability by normalizing $Q_t$ via diagonal scalings $U_t$ and $V_t$ so that $U_t Q_t V_t$ is “non-expansive,” but that argument only applies to the intermediate product, not to the final $Q_{t+1}$ after additional learned projections and nonlinearities. In other words, neither the “this is a new adjacency” claim nor the “this is stable” claim is actually enforced by the formulation given.\n\n---------\n\nOnly a limited set of baselines is considered, and they are largely standard sparsifiers or older heuristics. Authors should also consider modern deep learning-based baselines."}, "questions": {"value": "N/A."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "GlNe3shfrq", "forum": "DwrwGl8BNE", "replyto": "DwrwGl8BNE", "signatures": ["ICLR.cc/2026/Conference/Submission19779/Reviewer_bM5E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19779/Reviewer_bM5E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761633377378, "cdate": 1761633377378, "tmdate": 1762931627056, "mdate": 1762931627056, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper attempts to evolve the graph during training adaptively and also sparsify it to address the reliance on a fixed graph structure and oversmoothing. The paper proposes a new architecture, the Joint Graph Evolution layer (JGE layer), and a spectral concordance loss to sparsify the graph. Empirical results on robust structural and spectral properties after sparsification and a high MASS value, which indicates that the method maintains the global properties of a graph."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper explores the idea of evolving the graph structure during training, a direction that has been relatively underexplored in existing research, highlighting its novel contribution.\n2. The proposed method achieves superior performance compared to established baselines, demonstrating its effectiveness and supporting the originality of the approach."}, "weaknesses": {"value": "1. The paper would benefit from additional comparisons with related works. For instance, graph pooling methods also dynamically compress graph structures for improved representation, often employing similar equations. A more apparent distinction and comparison with such approaches would strengthen the contribution.\n2. Some key experiments, particularly an ablation study on the loss functions, are missing and should be included to clarify the contribution of each component.\n3. Even though the authors stated that fixed graph structure can fall into scalability and expressive issues, empirical analysis of whether the proposed method mitigates these issues is underexplored.\n4. The time complexity, which grows quadratically with the number of nodes, raises concerns about the scalability of the approach to large-scale graphs."}, "questions": {"value": "- Could the authors provide a more detailed explanation of how the right-hand side of Equation (13) is derived?\n- Can $Z$ be extended to explain the crucial edges in a given input graph?\n- When conventional GNN methods are optimized using the reduced graph, do they maintain high performance when evaluated on the original graph?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cF6ygm1ODW", "forum": "DwrwGl8BNE", "replyto": "DwrwGl8BNE", "signatures": ["ICLR.cc/2026/Conference/Submission19779/Reviewer_9M5h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19779/Reviewer_9M5h"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761873538505, "cdate": 1761873538505, "tmdate": 1762931626451, "mdate": 1762931626451, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "SpecNet introduces a spectral-preserving neural sparsifier that combines a Joint Graph Evolution (JGE) layer with a Spectral Concordance (SC) loss. The JGE layer co-evolves adjacency and feature matrices through bilinear transformations, while the SC loss aligns the leading eigenvalues of Laplacian and Gram matrices to maintain spectral and feature consistency. It performs node-level sparsification preserving both structure and attributes."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Novel joint architecture that evolves structure and features instead of treating graphs as static.\n2. Spectral-theoretic objective preserves global connectivity and dynamical properties.\n3. Extensive evaluation across 5 datasets and multiple sparsity ratios"}, "weaknesses": {"value": "1. Experiments primarily assess spectral metrics (MASS, τc) rather than downstream performance.\n2. Eigen-decomposition can be computationally costly for large graphs"}, "questions": {"value": "1. How does SpecNet affect downstream GNN accuracy after sparsification?\n2. What are the guidelines for choosing k₁/k₂/β/λ? (Optional)\n3. Can JGE handle heterogeneous or non-square incidence graphs?\n4. How scalable is the eigenvalue computation in high-dimensional settings? (Main issue)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HqyEJV41Pv", "forum": "DwrwGl8BNE", "replyto": "DwrwGl8BNE", "signatures": ["ICLR.cc/2026/Conference/Submission19779/Reviewer_Nrsr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19779/Reviewer_Nrsr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761915971662, "cdate": 1761915971662, "tmdate": 1762931625862, "mdate": 1762931625862, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces the Spectral Preservation Network (SpecNet), which consists of a joint graph evolution layer to mitigate oversmoothing by adaptively reshaping the topology, and a spectral concordance loss that sparsifies the graph to remove uninformative nodes."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. Graph sparsification is indeed a practical problem when dealing with large graphs.\n2. The writing of the proposal is easy to follow, which also applies to most parts of the paper."}, "weaknesses": {"value": "1. The motivation is unclear and lacks real-world support. For instance, in line 42, the authors mention “evolve the graph.” However, if the graph encoder is already changing continuously, the extracted information from the graph would also change without needing to modify the graph structure itself. What is the deeper reason for evolving the graph structure?\n2. Dropping nodes may limit the applicability of the proposed GNN. For example, link prediction would no longer be fully feasible.\n3. Could you elaborate further on the design of the proposed single layer in Equation (4)? The intuition behind it is unclear.\n4. What are the empirical results regarding the improvement of oversmoothing? Additionally, no evaluation metrics are provided."}, "questions": {"value": "See the weaknesses listed above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ONOIn98zVk", "forum": "DwrwGl8BNE", "replyto": "DwrwGl8BNE", "signatures": ["ICLR.cc/2026/Conference/Submission19779/Reviewer_NBhJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19779/Reviewer_NBhJ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19779/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762042077030, "cdate": 1762042077030, "tmdate": 1762931625037, "mdate": 1762931625037, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}