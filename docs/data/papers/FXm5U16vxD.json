{"id": "FXm5U16vxD", "number": 22955, "cdate": 1758337435972, "mdate": 1763741135135, "content": {"title": "Generative Adversarial Post-Training Mitigates Reward Hacking in Live Human-AI Music Interaction", "abstract": "Most applications of generative AI involve a sequential interaction in which a person inputs a prompt and waits for a response, and where reaction time and adaptivity are not important factors. In contrast, live jamming is a collaborative interaction that requires real-time coordination and adaptation without access to the other player’s future moves, while preserving diversity to sustain a creative flow. Reinforcement learning post-training enables effective adaptation through on-policy interaction, yet it often reduces output diversity by exploiting coherence-based rewards. This collapse, known as ``reward hacking'', affects many RL post-training pipelines, but is especially harmful in live jamming, where musical creativity relies on dynamic variation and mutual responsiveness. In this paper, we propose a novel adversarial training method on policy-generated trajectories to mitigate reward hacking in RL post-training for melody-to-chord accompaniment. A co-evolving discriminator separates policy trajectories from the data distribution, while the policy maximizes the discriminator output in addition to coherence rewards to prevent collapse to trivial outputs. We evaluate accompaniment quality and output diversity in simulation with both fixed test melodies and learned melody agents, and we conduct a user study with the model deployed in a real-time interactive system with expert musicians. Quantitative evaluation and user feedback demonstrate improved output diversity, harmonic coherence, adaptation speed and user agency. Our results demonstrate a simple yet effective method to mitigate reward hacking in RL post-training of generative sequence models.", "tldr": "", "keywords": ["Real-time Music Accompaniment", "Music Generation", "Reinforcement Learning", "Adversarial Machine Learning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f6e3262d53e26efa87316c268ae424ccb4372bab.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes GAPT (Generative Adversarial Post-Training) to solve the problem of reward hacking in human-music interaction. The paper argues that current methods for RL post training in music related scenarios hacks the reward but leads to repetitive or low diversity music samples. To solve this, the authors add a discriminator component to distinguish between policy generated vs real-data trajectories. Since GAN based training can be unstable, they also propose training techniques to stabilize the training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important problem in live music jamming. The proposed solution of having an adversarial loss is simple and seems to work. Moreover, the authors have good evaluation setting, they evaluate on three scenarios: fixed-melody, model-model and real-time with musicians. The paper is very well written and easy to understand."}, "weaknesses": {"value": "1. The scope of experimentation is narrow. For example, the proposed algorithm is tested on only melody-chord accompaniment. Similarly, the method is limited to T<=256 frames, where each frame is a sixteenth note. There are no studies in the paper that explains what happens for greater timesteps, is the training still stable?\n\n2. The authors use note-in-chord ratio which is a very simple metric. While the qualitative results suggest that GAPT performs better, from Table 1 and 2, we see minimal improvement compared to other baselines. For instance, Table 2 shows that Online MLE performs better than GAPT in everything except for harmony in user interaction (0.448 vs 0.467 (GAPT)). The quantitative results are not convincing enough to prove the efficacy of this method."}, "questions": {"value": "1. Could you provide empirical analysis of your method's performance and training stability for sequences longer than 256 frames? This is important because real musical interactions often extend beyond this length, and understanding any degradation in performance or training stability would significantly impact the method's practical utility.\n\n2. In Table 2, Online MLE outperforms GAPT in most metrics except for harmony in user interaction. Could you explain this discrepancy and provide additional analysis to justify why GAPT should be preferred over the simpler Online MLE approach?\n\n3. How does this method work for other musical styles?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yz1jTamlDC", "forum": "FXm5U16vxD", "replyto": "FXm5U16vxD", "signatures": ["ICLR.cc/2026/Conference/Submission22955/Reviewer_wu95"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22955/Reviewer_wu95"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761949550555, "cdate": 1761949550555, "tmdate": 1762942457292, "mdate": 1762942457292, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed Generative Adversarial Post-Training (GAPT) for live melody→chord accompaniment. A discriminator is trained to prevent reward hacking, ensuring the realism of the accompaniment. The experiment results show that with GAPT, the model outputs more diverse content."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Promise to release training datasets, model checkpoints, and code.\n2. Well written, well structured, clear problem and motivation.\n3. Comprehensive experiments show the improvement of diversity, harmonic coherence, adaptation speed and user agency."}, "weaknesses": {"value": "1. The chord set of the model is not specified; note-in-chord could be inflated by choosing chords with more pitch classes.\n2. The authors did not benchmark against other RL post-training methods."}, "questions": {"value": "1. What if the discriminator conditioned on melody D(x,y)?\n2. What’s the latency budget and end-to-end timing under live use? Does GAPT change compute/latency vs RealChords?\n3. In the appendix, you mentioned that for all user study sessions, you set the tempo to 80 BPM. Does that mean when participants are improvising, the bpm should be set to 80?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Al5D22kTX1", "forum": "FXm5U16vxD", "replyto": "FXm5U16vxD", "signatures": ["ICLR.cc/2026/Conference/Submission22955/Reviewer_VfzC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22955/Reviewer_VfzC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969491537, "cdate": 1761969491537, "tmdate": 1762942456253, "mdate": 1762942456253, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed Generative Adversarial Post-Training (GAPT) for live melody→chord accompaniment. A discriminator is trained to prevent reward hacking, ensuring the realism of the accompaniment. The experiment results show that with GAPT, the model outputs more diverse content."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Promise to release training datasets, model checkpoints, and code.\n2. Well written, well structured, clear problem and motivation.\n3. Comprehensive experiments show the improvement of diversity, harmonic coherence, adaptation speed and user agency."}, "weaknesses": {"value": "1. The chord set of the model is not specified; note-in-chord could be inflated by choosing chords with more pitch classes.\n2. The authors did not benchmark against other RL post-training methods."}, "questions": {"value": "1. What if the discriminator conditioned on melody D(x,y)?\n2. What’s the latency budget and end-to-end timing under live use? Does GAPT change compute/latency vs RealChords?\n3. In the appendix, you mentioned that for all user study sessions, you set the tempo to 80 BPM. Does that mean when participants are improvising, the bpm should be set to 80?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Al5D22kTX1", "forum": "FXm5U16vxD", "replyto": "FXm5U16vxD", "signatures": ["ICLR.cc/2026/Conference/Submission22955/Reviewer_VfzC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22955/Reviewer_VfzC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969491537, "cdate": 1761969491537, "tmdate": 1763744240644, "mdate": 1763744240644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work explores the use of adversarial learning to mitigate the reward hacking problem in reinforcement learning for the task of melody-to-chord accompaniment generation.\nReward hacking in RL often arises because (1) human-defined rewards are incomplete and may lead the policy to converge to a local minimum, and (2) RL agents cannot distinguish out-of-distribution (OOD) samples, causing them to over-rely on the reward signal.\n\nWhat makes this paper interesting is how it integrates adversarial learning into the RL framework. Unlike other methods (e.g., [1]), the authors adopt a simple yet direct strategy: they add the discriminator term −log(1−y) directly to the reward. In this way, if the discriminator considers a generated sample to be “real,” the policy receives an additional (and potentially large) reward boost.\n\nExperimentally, the authors demonstrate strong results in both quantitative metrics and perceptual evaluations. As an amateur music enthusiast, I can clearly perceive the improvement brought by the adversarial reward.\n\n[1] Bukharin et al., Adversarial Training of Reward Models, 2025."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "**Technical Comments**\n\n1. Exploring adversarial learning in the context of melody-to-chord accompaniment is quite novel. Moreover, directly integrating the discriminator’s output into the reward function is a simple yet effective strategy, as demonstrated in the paper.\n\n2. The authors present several insightful analyses and results. For example, in Figure 4, they show that the adversarial reward encourages more diverse generation outputs and expands the representational space. This finding is particularly interesting, since—according to the authors—other components such as entropy regularization remain unchanged compared to the w/o-adversarial baseline.\n\n**Presentation**\n\nThe writing is clear and easy to follow, and the examples used in the comparative experiments are also quite convincing when listened to.\n\n**Experiments**\n\n1. For the main task, the authors demonstrate significant improvements over both the baseline and the w/o-adversarial-reward setting, in terms of quantitative metrics and subjective evaluation.\n2. The ablation study is also comprehensive and effectively supports the paper’s claims."}, "weaknesses": {"value": "**Technical Comments**\n\n1. Since the adversarial reward takes values in  [0,+∞) and is directly added to the original reward, does this cause any numerical instability during training? Has the author conducted experiments to verify its stability?\n\n2. Adversarial training (GAN) itself is known to be unstable. Did the author perform repeatability experiments to show the variance across different random initializations?\n\n3. What does the reward convergence curve look like during training? It seems that this information is not presented in the paper.\n\n4. Could the author show how the critic value differs between the models with and without adversarial reward? (I notice the authors use PPO instead of GRPO.)\nAdditionally, could they run an experiment using GRPO, since GRPO normalizes rewards using the group/batch average?\nGiven that the adversarial reward is always positive, it might shift the overall value distribution upward, potentially affecting learning stability and normalization.\n\n5. Is the discriminator’s training process stable? What does its learning curve look like over the course of training?\n\n\nI am very interested in this paper and I genuinely like the direction. My current score mainly reflects the fact that I would like to see a clearer and more complete picture from the authors, so that the contribution becomes more solid. If the authors can provide more thorough/deep analysis in the rebuttal or revision, I would be happy to raise my score."}, "questions": {"value": "see above"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WUVvz825O3", "forum": "FXm5U16vxD", "replyto": "FXm5U16vxD", "signatures": ["ICLR.cc/2026/Conference/Submission22955/Reviewer_SxDd"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22955/Reviewer_SxDd"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989498881, "cdate": 1761989498881, "tmdate": 1762942454145, "mdate": 1762942454145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Generative Adversarial Post-Training (GAPT) for real-time melody-to-chord accompaniment. GAPT introduces a discriminator to avoid reward hacking in RL finetuning. Evaluation shows this simple yet effective GAPT framework works to mitigate reward hacking."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Evaluation metrics are reasonable. Note-in-chord ratio allows flexibility because of the nature of real-time accompaniment, but at the same time evaluates the quality of chords.\n2. Adversarial realism reward is easy to add and conceptually orthogonal to KL/entropy regularization. The confidence-gated update is a neat stability trick.\n3. The RL post training scalar reward not only include discriminative models, but also include music rules which mitigate the bias brought by discriminative model as well.\n4. Appreciate the examples provided to have a straightforward evaluation in ears."}, "weaknesses": {"value": "1. I would appreciate that if more ablations study would be completed on GAPT itself, especially on the scalar reward in Equation (6), applying different weights to see the behavior of models would be a helpful study to provide more insights."}, "questions": {"value": "1. What could be the reason for harmonic incoherence (like in Hooktheory Example 4)? Is it because the melody has higher density of notes in beats comparing to other successful accompaniments? When GAPT underperforms harmony (e.g., slight drop on OOD), what musical patterns cause it? Maybe because of the training dataset?\n2. What are end-to-end latency numbers (ms) and GPU budget during live use? How does lookahead/commit horizon trade off with perceived responsiveness?\n3. Would you mind elaborating a bit more on applying GAPT outside real-time accompaniment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "MWzkP6rBe8", "forum": "FXm5U16vxD", "replyto": "FXm5U16vxD", "signatures": ["ICLR.cc/2026/Conference/Submission22955/Reviewer_pmqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22955/Reviewer_pmqm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22955/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998368110, "cdate": 1761998368110, "tmdate": 1762942453278, "mdate": 1762942453278, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Additional experiments and analyses added in the revised draft"}, "comment": {"value": "We sincerely thank all reviewers for their thoughtful and constructive feedback, which has greatly helped us improve the clarity, experiments, and overall presentation of the paper.\n\nHere is a brief summary of the additional experiments and analyses added in the revised draft:\n\n1. **Ablation on scalar reward weights \\(\\alpha, \\beta, \\gamma\\) and invalid penalty \\(R_{\\text{invalid}}\\)**, Appendix C.9, Table 7, line 1015. GAPT with equal weights \\(\\alpha = \\beta = \\gamma = 1\\) is robust to moderate coherence upweighting, while strongly upweighting rules or adversarial reward degrades both harmony and diversity, removing rules leads to reward hacking via invalid outputs, and keeping only an invalidity penalty partially stabilizes training but still underperforms the full rule based term.\n2. **Three seed stability study for GAPT, GAPT w/o Adv., and ReaLchords**, Appendix C.7, Table 6, line 910. Standard deviations over three RL runs are small for all models and GAPT consistently achieves higher diversity than ReaLchords and the non adversarial ablation at similar or slightly better harmony, indicating low variance and stable gains across random initializations.\n3. **Training dynamics and adversarial reward schedule curves**, Figure 8, line 938. The overall scalar reward, adversarial reward, discriminator accuracy, and discriminator loss evolve smoothly over training, and the adaptive update schedule keeps the adversarial reward in a moderate numerical range, supporting the practical stability of the adversarial post training.\n4. **Critic value comparison between GAPT and GAPT w/o Adv. and relation to adversarial reward**, Figures 9 and 10, lines 961 and 972. Critic values for GAPT are slightly higher than for GAPT w/o Adv. and the difference closely tracks the additional adversarial reward, showing that the critic successfully estimates the incremental contribution of the adversarial term without destabilizing value learning.\n5. **GRPO / Dr.GRPO as an alternative RL objective**, Appendix C.10, Table 8, line 1040. GRPO without adversarial training shows the same diversity collapse as PPO based RL post training, while adding adversarial training on top of GRPO restores diversity and slightly improves harmony on both the test set and the out of distribution dataset, confirming that the benefits of GAPT are robust across different policy gradient algorithms.\n6. **Melody conditioned discriminator \\(D_{\\psi}(x,y)\\) versus chord only discriminator \\(D_{\\psi}(y)\\)**, Appendix C.11, Table 9, line 1067. Conditioning the discriminator on both melody and chords still improves diversity over the non adversarial baseline but yields lower diversity and slightly worse held out harmony than the chord only discriminator, suggesting that conditioning on melody makes it easier for the discriminator to overfit specific training pairs and weakens its regularization effect.\n7. **Chord vocabulary specification and pitch class statistics to rule out dense chord inflation**, Appendix C.12, text at line 1080 and Table 10, line 1085. We specify a shared vocabulary of 2,821 chord symbols (triads, sevenths, extensions, inversions, and suspensions) and show that the average number of distinct pitch classes per chord for all models (3.00–3.26) is close to the dataset averages (3.28–3.54), indicating that none of the models, including GAPT, artificially inflate the note in chord ratio by predicting unusually dense chords.\n8. **Long horizon evaluation at \\(T = 512\\) with naive extension and sliding window inference**, Appendix C.13, Figures 11 and 12, lines 1117 and 1155. For both long sequences from the test set and from Wikifonia the note in chord ratio remains stable over time, and the sliding window strategy with 256 frame context achieves consistently better harmony than naive context extension, showing that training remains stable and GAPT maintains good performance beyond the RL horizon."}}, "id": "7tCitFW3jv", "forum": "FXm5U16vxD", "replyto": "FXm5U16vxD", "signatures": ["ICLR.cc/2026/Conference/Submission22955/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22955/Authors"], "number": 12, "invitations": ["ICLR.cc/2026/Conference/Submission22955/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763744775030, "cdate": 1763744775030, "tmdate": 1763761489803, "mdate": 1763761489803, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}