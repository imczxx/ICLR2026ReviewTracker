{"id": "1mj4z3ZUeZ", "number": 16617, "cdate": 1758266786637, "mdate": 1759897229304, "content": {"title": "Trigger Embeddings for Data Exfiltration in Diffusion Models", "abstract": "Diffusion models (DMs) have achieved remarkable success in image and text-to-image generation, but their rapid adoption raises concerns about training data security. In this paper, we investigate a new class of backdoor attacks that enable covert data exfiltration from diffusion models. Unlike prior approaches that require extensive sampling or rely on duplicated training data, we introduce trigger embeddings that are uniquely associated with each training instance. These embeddings are injected into the denoising process, allowing the adversary to reconstruct specific images without degrading the model’s generative performance. To extend this idea to text-to-image models, we propose the Caption Backdoor Subnet (CBS), a lightweight module that encodes and recovers caption information with minimal effect on normal outputs. Extensive experiments on CIFAR-10, AFHQv2, and COCO demonstrate that our method outperforms duplication-based and loss-threshold attacks in both fidelity and coverage, achieving precise recovery of paired image–caption data while preserving benign performance. Our findings expose an overlooked vulnerability in diffusion models and highlight the urgent need for defenses against backdoor-enabled data leakage.", "tldr": "We show how diffusion models can be implanted with trigger embeddings to exfiltrate training images and captions, revealing a critical data security vulnerability.", "keywords": ["diffusion models", "backdoor attacks", "data exfiltration", "memorization", "generative security"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e1e86412ad01bface59e9d3e8cef01a8c4f0b996.pdf", "supplementary_material": "/attachment/f22b957a2e26995f7a5cfe5514ad7f9fbfa237ca.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes a backdoor attack on diffusion models that enables reconstruction of training data after model release. During training, the attacker adds trigger embeddings and extra loss terms so each trigger corresponds to a specific training sample. At inference, providing the trigger regenerates that sample while normal generation remains unaffected. For text-to-image models, a small Caption Backdoor Subnet recovers captions. Experiments show high-fidelity data recovery with minimal performance loss, revealing serious privacy risks in diffusion model training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Technical novelty: Proposes a simple yet effective mechanism—per-instance trigger embeddings injected into conditioning (time/text) and a lightweight Caption Backdoor Subnet—to recover both images and captions with minimal architecture changes.\n\n2. Strong empirical evidence and stealth: Demonstrates high-fidelity reconstruction across unconditional and T2I settings while preserving benign generation metrics, showing the attack is both effective and hard to detect."}, "weaknesses": {"value": "1. Assumption on valuable data instances:\nThe paper assumes that the training dataset contains specific sensitive or high-value samples worth exfiltrating. However, if the dataset is large and diverse, the attacker would need a correspondingly large number of trigger embeddings, which may limit scalability. Conversely, if only a small subset of data is truly sensitive, such samples are often subject to filtering or sanitization before dataset finalization for training. Clarifying this assumption and its practical implications would strengthen the paper’s motivation.\n\n2. Clarify attacker privileges and feasibility: The paper assumes the attacker can modify the training objective to add trigger losses. Please clarify realistic threat vectors that grant such capability."}, "questions": {"value": "1. The paper lacks a quantitative analysis of storage, bookkeeping, and retrieval costs for large trigger sets. Practical deployment at scale (e.g., thousands–millions of targets) may impose nontrivial overheads; authors should quantify limits and trade-offs.\n\nWith the questions above, please further discuss about the weaknesses parts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F3486MxLd9", "forum": "1mj4z3ZUeZ", "replyto": "1mj4z3ZUeZ", "signatures": ["ICLR.cc/2026/Conference/Submission16617/Reviewer_UYwN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16617/Reviewer_UYwN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761645108964, "cdate": 1761645108964, "tmdate": 1762926687578, "mdate": 1762926687578, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors design a privacy attack method targeting data leakage of diffusion models. In this method, the trainer can control the training process but cannot extract data from zero-trust environments. By leveraging backdoor injection techniques, the attack recovers private training data (images or text) in text-to-image / unconditional diffusion models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose and design an interesting and novel attack scenario in which the trainer is able to control the training process but cannot exfiltrate data from zero-trust environments."}, "weaknesses": {"value": "(1) **Unrealistic threat model.**\n\nThe assumed capabilities of the attacker are too strong to reflect a practical scenario. Although the authors list some justifications in Section 3, if institutions truly operate under a zero-trust environment, there are simpler and more effective strategies to prevent sensitive data leakage, such as (1) applying data filters at the output level (e.g., API service), or (2) conducting strict training log audits (especially with open-source models).\n\n(2) **The experiments are not sufficient.** \n\nThe dataset used does not align well with real-world privacy leakage scenarios. One key question the authors should consider is: what data exactly counts as private in the threat setting? If private images are the target, the authors should evaluate their method on sensitive images from domains like facial or medical data, where the distribution is more concentrated and might degrade the performance of the proposed method. If private prompts are the focus, more complex text-image datasets should be tested instead of COCO, since COCO prompts are short and syntactically simple, potentially making evaluation easier.\n\n\n(3) **Comparison with related works.**\n\nFrom the perspective of image leakage, the proposed method essentially performs multi-backdoor injection. In this setting, the authors are encouraged to discuss and compare whether existing text-to-image or diffusion backdoor methods can be adapted to this task, and how their performance compares with the proposed approach."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MsD5d5u5en", "forum": "1mj4z3ZUeZ", "replyto": "1mj4z3ZUeZ", "signatures": ["ICLR.cc/2026/Conference/Submission16617/Reviewer_BWHC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16617/Reviewer_BWHC"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761987869945, "cdate": 1761987869945, "tmdate": 1762926687169, "mdate": 1762926687169, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a novel backdoor-based data exfiltration attack for diffusion models, termed Trigger Embeddings (TGF). The attacker can extract the training dataset by only backdooring the textual embeddings. Unlike prior attacks relying on duplicated data or brute-force sampling, TGF injects unique trigger embeddings into the diffusion denoising process, allowing covert reconstruction of specific training images. The paper extends this to text-to-image diffusion models using a Caption Backdoor Subnet (CBS) that learns to recover captions associated with exfiltrated images. Experiments on CIFAR-10, AFHQv2, and COCO datasets demonstrate high fidelity and stealthiness of exfiltration, with minimal impact on benign model performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Novel Threat Model: The formulation of backdoor-enabled data exfiltration in diffusion models is new and well motivated. The paper articulates realistic insider-threat scenarios under zero-trust infrastructures."}, "weaknesses": {"value": "1. Ambiguous presentation for the methodology: It's hard to understand the attack method. I would suggest to present in top-down way, which means first pointing out achiving the data exfiltration attack by memorizing out-of-dsitribution token embedding to each image.\n2. Heuristic methodology: It's unclear why selecting the parameters inside the diffusion model to achieve the prompt recovering. Additionally, use LLM to reorder the recovered token has no correctness gurantee.\n3. Lack of benign variety evaluation: The attacker might enhance the memorization issue of the diffusion model but the paper doesn't provide rigorous analysis on it."}, "questions": {"value": "1. Can you further elaborate the equation (7) and how to define a caption label $C_{j}$ and each token?  How possible does the trigger activated accidently by the users?\n2. Why not construct a new neural network to recover the token embeddings $\\mathbf{e}_{c,0}$ to $C_{p}$ ? Why only reconstuct the first token? \n3. Why use LM to re-construct the tokens? Why not just train a neural network to recover the the sequence of tokens?\n4. Does sample $\\mathbf{e}_{c}^{i}$ from uniform distribution can be memorized well? \n5. The potential most affected benign performance should be the bengin variety of generated samples. IS and CLIP score might not be good ways to evaluate the variety because duplicated training sample can achieve the best score. I would suggest adding a new experiment to evaluate the variety of the generated samples."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UnBdjIkC7u", "forum": "1mj4z3ZUeZ", "replyto": "1mj4z3ZUeZ", "signatures": ["ICLR.cc/2026/Conference/Submission16617/Reviewer_nXJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16617/Reviewer_nXJT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16617/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762189333892, "cdate": 1762189333892, "tmdate": 1762926686814, "mdate": 1762926686814, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}