{"id": "4Ewgw9M2xE", "number": 12048, "cdate": 1758205411679, "mdate": 1763293136450, "content": {"title": "R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning", "abstract": "Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a 8.4% improvement on the VL Reward-Bench and a 14.3% improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.", "tldr": "This paper introduces StableReinforce, a novel RL algorithm designed to stably and effectively train Multimodal Reward Models (MRMs), leading to the R1-Reward model which significantly outperforms previous SOTA models on key benchmarks.", "keywords": ["Multimodal Large Language Model", "Multimodal Reward Model", "Stable Reinforcement Learning", "Long-CoT Reasoning"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/24bae1071a5548e0dc9e3a20b18ee6754df3a518.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces R1-Reward, a multimodal reward model trained using a novel StableReinforce algorithm that reformulates reward modeling as a rule-based reinforcement learning task. The authors address training instability issues in existing RL algorithms through pre-clipping operations, advantage filtering, and a consistency reward mechanism that aligns reasoning with final judgments. Experimental results demonstrate significant improvements over state-of-the-art models on multiple multimodal reward modeling benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper explores using reinforcement learning to train multimodal reward models by reformulating preference evaluation as a rule-based RL task, enabling long-term reasoning capabilities beyond traditional binary classification approaches.\n2. The paper introduces several algorithmic innovations to address instability issues for MRM RL training.\n3. The approach demonstrates superior data efficiency showing the effectiveness of the RL-based training paradigm."}, "weaknesses": {"value": "1. While the paper claims novelty in applying RL to multimodal reward modeling, it lacks adequate justification for why RL is specifically beneficial for MRMs, since there have been several works (e.g. https://arxiv.org/abs/2505.02387) in text-based RL reward modeling. The paper does not clearly articulate what unique challenges multimodal data presents that necessitate the proposed RL approach, nor does it sufficiently differentiate itself from existing generative MRM methods (like MM-RLHF-Reward).\n2. The paper's main technical contributions appear to address numerical stability. However, they seem unnecessary in comparison to commonly used GRPO. (1) The claimed (log_probs - old_log_probs).exp() overflow issue is commonly handled with clipping in most RL implementations and rarely occurs. (2) Extreme advantage values after normalization are less problematic in GRPO with reasonable rollout numbers. (3) GRPO has default clipping strategies. The paper should compare against more commonly used algorithms like GRPO rather than only comparing against Reinforce++."}, "questions": {"value": "1. The 200K dataset is carefully curated (filtering samples where GPT-4o needs â‰¥2 attempts), while baselines likely use unfiltered data. How do you ensure fair comparison when comparing curated vs. raw datasets?\n2. The method requires expensive GPT-4o annotations for SFT initialization. When including GPT-4o annotation costs, is your approach truly more cost-efficient than traditional reward model training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TbaXUsw9tf", "forum": "4Ewgw9M2xE", "replyto": "4Ewgw9M2xE", "signatures": ["ICLR.cc/2026/Conference/Submission12048/Reviewer_5DDa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12048/Reviewer_5DDa"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761716874378, "cdate": 1761716874378, "tmdate": 1762923024536, "mdate": 1762923024536, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors proposed a multi-modal reward model called R1-Reward. This method is designed to solve traditional RL's problem of training instability and inconsistent results. R1-Reward introduces Pre-CLIP and Advantage Filter to enhance the model's training stability. They also introduce a staged training strategy to efficiently improve the model's capabilities. The experiments show that R1-Reward achieves competitive performance compared to other baseline models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The motivation of this paper is clear and important. The performance instability and annotation cost are a very practical application problem. The progress of alleviating the problem has direct practical application potential.\n\n2. The authors provide detailed experiments to validate the method's effectiveness. The tasks are various and cover different scales of LLMs. In general, the experiments are convincing, and the reproducibility should not be a problem.\n\n3. The authors provide detailed and clear charts and figures to clarify the method and its performance. These visualizations are easy to follow and enhance the readability of the paper."}, "weaknesses": {"value": "1. The authors choose Qwen2.5 as the judge to compute the consistency reward, but wait until the appendix to clarify the reason for choosing an LLM of this size. This setting is an important experimental detail, and mentioning it in the main text might improve the paper's integrity.\n\n2. The ensemble reward lacks a sufficient explanation. The reason why the \"consistency reward\" is introduced multiplicatively while the \"formatting reward\" is introduced additively is not systematically explained. The ablation study evaluates its effectiveness but does not provide a sufficient theoretical explanation."}, "questions": {"value": "1. The reward function's weights and structure are empirical. An open question is whether these settings can easily transfer to other tasks. The authors may need to further explore systematically and adaptively combining multiple reward signals.\n\n2. The method's performance is limited by the LLM that is applied as the judge. This strategy may fail in resource-constrained scenarios where a sufficiently strong LLM judge cannot be deployed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FD6EPaV4z0", "forum": "4Ewgw9M2xE", "replyto": "4Ewgw9M2xE", "signatures": ["ICLR.cc/2026/Conference/Submission12048/Reviewer_Pch1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12048/Reviewer_Pch1"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761977356329, "cdate": 1761977356329, "tmdate": 1762923024128, "mdate": 1762923024128, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "To address the problem of training instability or even collapse in multimodal reward models, the paper proposes StableReinforce,  which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. Specifically, StableReinforce refines the clipping operation to mitigate numerical instability caused by large updates and introduces a robust advantage normalization technique that limits the impact of outliers. This paper employs QwenVL-2.5-7B as the backbone model, and trains it using the R1-Reward-200k dataset. The experimental section demonstrates that StableReinforce outperforms existing baselines across multiple benchmarks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. StableReinforce exhibits smoother convergence in policy loss and demonstrates sustained length compression during training, which helps reduce inference overhead.\n\n2. This paper designs clear ablation experiments to precisely quantify the contribution and sensitivity of each component."}, "weaknesses": {"value": "1. Test time scaling (TTS) is limited to majority voting and can be further evaluated with approaches such as confidence-weighted sampling, early stopping, and calibrated reordering.\n\n2. While Preclip reduces variance and suppresses overflow, it alters the gradient shape of the objective function, potentially introducing optimization bias.\n\n3. This paper trains RM as a rule-based RL task focused on decision-making between 1 and 2, making it unsuitable for multi-candidate ranking or continuous scoring tasks."}, "questions": {"value": "How do the gains evolve as the consistency judge becomes increasingly noisy? Is 0.5 still the optimal weight under noise?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "McEMbk5pz7", "forum": "4Ewgw9M2xE", "replyto": "4Ewgw9M2xE", "signatures": ["ICLR.cc/2026/Conference/Submission12048/Reviewer_G94z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12048/Reviewer_G94z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762432429898, "cdate": 1762432429898, "tmdate": 1762923023709, "mdate": 1762923023709, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces R1-Reward, a novel Multimodal Reward Model (MRM) trained using a new reinforcement learning (RL) algorithm called StableReinforce. The authors found that directly applying existing RL algorithms like PPO or Reinforce++ to reward modeling leads to extreme training instability and collapse. The instability is traced to flaws in the PPO loss function when handling negative advantages and to extreme outliers created by \"Advantage Normalization\" in low-variance batches. StableReinforce solves this by introducing \"Pre-CLIP,\" which clamps log probability differences before the exponential function to prevent overflow, and an \"Advantage Filter,\" which removes outliers using the 3-sigma rule. Furthermore, to address inconsistencies where the model's reasoning and final answer contradict each other, the paper introduces a \"Consistency Reward\" evaluated by an MLLM referee. Trained on a 200K preference dataset , R1-Reward achieves state-of-the-art performance, improving accuracy by 8.4% on the VL Reward-Bench and 14.3% on the Multimodal Reward Bench."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper's main strength is being the first to successfully use reinforcement learning (RL) to train a multimodal reward model. It cleverly treats the MLLM itself as the reward model, avoiding extra parts like a reward head. The authors showed strong engineering skills by modifying existing RL algorithms to fix critical instability issues that caused them to crash on this task. This practical approach worked very well, allowing their model to achieve SOTA results."}, "weaknesses": {"value": "I think this is a good work. One weakness may be its limited novelty."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "31c1Jd0LHA", "forum": "4Ewgw9M2xE", "replyto": "4Ewgw9M2xE", "signatures": ["ICLR.cc/2026/Conference/Submission12048/Reviewer_KA5w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12048/Reviewer_KA5w"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12048/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762466830278, "cdate": 1762466830278, "tmdate": 1762923023322, "mdate": 1762923023322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}