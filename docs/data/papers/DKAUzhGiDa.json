{"id": "DKAUzhGiDa", "number": 13611, "cdate": 1758219790973, "mdate": 1759897424787, "content": {"title": "EfficientLLM: Evaluating Large Language Models Efficiency", "abstract": "Large Language Models (LLMs) have achieved remarkable advances across reasoning, generation, and problem-solving, yet their scaling comes with prohibitive training, deployment, and environmental costs. Training frontier models like GPT-3 or PaLM consumes thousands of GPU/TPU days and millions of dollars. As these costs escalate, there is a pressing need for rigorous benchmarks that quantify efficiency–performance trade-offs. However, existing evaluations remain inadequate: 1) they rely on narrow metrics such as FLOPs or latency, neglecting complementary dimensions like memory, throughput, energy, and compression, leading to mischaracterized efficiency; 2) they are often limited to small models or a single hardware setup, making conclusions difficult to generalize to billion-parameter deployments across diverse accelerators; and 3) they fragment coverage across pretraining, fine-tuning, or inference, failing to provide an end-to-end perspective on the full lifecycle of model efficiency. To address these gaps, we present \\textbf{EfficientLLM}, the first large-scale empirical benchmark that systematically quantifies efficiency–performance trade-offs across the entire lifecycle of LLMs. 1) First, to overcome missing multi-dimensional metrics, EfficientLLM unifies six orthogonal dimensions into a consistent evaluation framework. 2) Second, to address scale and hardware diversity, we evaluate over 150 model–technique pairs spanning 0.5B–72B parameters on production-class clusters with 48*GH200, 8*H200, and 8*A100 accelerators, ensuring conclusions generalize to realistic deployment conditions. 3) Third, to provide end-to-end lifecycle coverage, EfficientLLM benchmarks architectural pretraining, fine-tuning, and bit-width quantization. By systematically resolving these three limitations, EfficientLLM establishes the most comprehensive benchmark to date for evaluating efficiency in large-scale models. Our results not only highlight critical trade-offs between accuracy, cost, and sustainability but also offer actionable guidance for both academic researchers and industrial practitioners in designing, training, and deploying the next generation of foundation models. All code and datasets are released as an open-source toolkit, accessible via \\texttt{pip install efficientllm-toolkit}.", "tldr": "EfficientLLM introduces the first comprehensive benchmark systematically evaluating efficiency techniques for large language models across pretraining, fine-tuning, and bit-width quantification inference.", "keywords": ["Large Language Models (LLMs)", "Efficiency Benchmark", "Architecture Pretraining"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ad79a25dd30e6487b4c9812590a41e4f631816d9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces EfficientLLM, a large-scale empirical benchmark designed to systematically evaluate the efficiency of Large Language Models (LLMs) across three critical dimensions: architecture pretraining, fine-tuning, and inference quantization."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This benchmark is the first end-to-end empirical study of LLM efficiency across pretraining, fine-tuning and inference. \n\n- This benchmark evaluate a broad scope of model including LLM, VLM, SD."}, "weaknesses": {"value": "1. The paper attempts to address the impacts of architecture selection, pretraining, fine-tuning, and bit-width quantization on model efficiency all at once. This wide-ranging scope may result in insufficient depth of analysis for each subtopic. It is recommended that the authors focus on one or two of these aspects to provide more insightful and in-depth analysis.\n\n2. The current study only utilizes benchmarks from the medical domain, which may limit the generalizability and impact of the findings. It is advisable to include additional benchmarks that better reflect the capabilities of large language models (LLMs), such as reasoning-related benchmarks, to enhance the practical relevance and guidance of the research.\n\n3.  The authors employ the 3D parallelism strategy from Megatron-LM in the pretraining phase. However, there are various 3D parallelism configurations, each with different implications for efficiency. A more comprehensive analysis of how different configurations affect efficiency would be beneficial.\n\n4.  It is suggested to adopt widely recognized hardware efficiency metrics in the industry, such as MFU (Model FLOPs Utilization), which are more commonly used and practical for evaluating hardware utilization efficiency.\n\n5. In Section H.6.2, the efficiency scoring method presented in this section treats five factors as equally important, assigning each a weight of 0.2. However, the significance of these factors may vary across different application scenarios. It is recommended to adjust the weight assignments based on specific use cases to improve the rationality and applicability of the evaluation method.\n\n\n6. Finally, please fix the following typos.\n\n- Fix the typo in Figure2 to change quantification to quantization \n- Fix type in line 2594 and 1895\n- Fix the typo in Figure1 from vedio to video\n- Fix the typo in Table 10 about the meaningless black block."}, "questions": {"value": "see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "HVrelbedUB", "forum": "DKAUzhGiDa", "replyto": "DKAUzhGiDa", "signatures": ["ICLR.cc/2026/Conference/Submission13611/Reviewer_3TeV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13611/Reviewer_3TeV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761229955253, "cdate": 1761229955253, "tmdate": 1762924195609, "mdate": 1762924195609, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper is focusing on the efficient LLMs and trying to benchmark the efficiency of the existing LLMs. It has introduced Average Memory Utilization (AMU), Peak Compute Utilization (PCU), Average Latency (AL), Token Throughput (TT), Sample Throughput (ST), and Inference Throughput (IT) for assessing the efficiency of resource utilization for memory bandwidth, device utilization, and throughput. Authors also have computed the energy consumption rate as well as compression rate with performance for these LLM models. Authors have conducted the experiments with multiple LLM models on Medical-O1 dataset for comprehensive comparions."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ Authors have conducted really very extensive experiments for comparing the efficiency of different models. It is really helpful to evaluate these model efficiency with the same benchmark and metrics.\n\n+ Authors have packed the evaluation code and release them, making it pretty easy to be assessed with pip install for future works."}, "weaknesses": {"value": "- Some of the metrics actually varies signicantly with the devices used, such as Memory Utilization, and some of the metrics might be also capped with some other infra issues, such as  Compute Utilization. If some other new works is trying to follow up this work for producing the metrics on some newly release LLM, it might not be possible to fully reproduce the results under the same environment and the I/O speed between the GPU/TPU and memory might also be different. Wonder whether is it possible to introduce some balancing terms to have these better crafted so that it can be better used in the future.\n\n- Some of the metrics might be significantly different from case to case, e.g, Compute Utilization. It may be different from two runs with different cache and memory situation. It would be great if authors are able to produce the STD value for these metrics with multiple experiments. \n\n- Please consider including all the numerical numbers in addition to the diagrams so that the future work is able to follow and use the number directly for comparison. It can be provided either next to the bars in the diagram or use as a table in the supplementary material."}, "questions": {"value": "Please see the weakness. For some of the metrics, if it can be used more widely, it is easier for the future work to follow the development and make the benchmark more useful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "60ar2Pmz2x", "forum": "DKAUzhGiDa", "replyto": "DKAUzhGiDa", "signatures": ["ICLR.cc/2026/Conference/Submission13611/Reviewer_nU3j"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13611/Reviewer_nU3j"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761603163824, "cdate": 1761603163824, "tmdate": 1762924195139, "mdate": 1762924195139, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EfficientLLM, a comprehensive benchmark designed to evaluate the efficiency of large language models (LLMs). The authors identify key limitations in existing evaluations, such as a lack of multi-dimensional metrics, insufficient scale, and fragmented lifecycle coverage. To address these gaps, EfficientLLM systematically assesses over 100 model-technique combinations across six efficiency dimensions including memory utilization, computational throughput, latency, and energy consumption spanning the full model lifecycle from pre-training to inference. Executed on a large-scale production cluster, the benchmark provides empirical insights into trade-offs for various architectural choices, fine-tuning methods, and quantization strategies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "●\tThe authors evaluate efficiency across pretraining, fine-tuning, and quantization, providing a full lifecycle perspective rarely seen in prior work.\n●\tThe paper introduces six complementary efficiency dimensions (AMU, PCU, AL, TT, AEC, MCR) for a more holistic evaluation beyond FLOPs or latency.\n●\tThe authors benchmark over 150 model–technique pairs (0.5B–72B parameters) on production-class hardware, demonstrating strong experimental coverage.\n●\tThis paper offers clear, actionable recommendations (e.g., RSLoRA for production, Freeze for academia, FP8/INT4 for deployment) useful for real-world practitioners."}, "weaknesses": {"value": "●\tThe work mainly integrates existing benchmarks (e.g., MLPerf, LLMPerf, EfficiencyBench) and metrics without introducing a fundamentally new framework or methodology.\n●\tThe paper repeatedly references an \"Efficiency Score\" (e.g., in Figure 4 and Section H.6.2) that is used to rank methods, but provides only a high-level description (“weighted harmonic combination of normalized resource metrics”). The exact weights, normalization procedure, and aggregation function are not specified in the main text or appendices. This makes it impossible to reproduce or critically evaluate the central ranking results that underpin many of the paper’s key claims and recommendations.\n●\tThe paper emphasizes energy efficiency and sustainability but fails to present actual energy or carbon emission data. “Average Energy Consumption” (AEC) is introduced but not supported by verifiable measurement methods or units, weakening one of the paper’s central claims.\n●\tThe paper does not isolate the impact of individual components. For example, when a model uses both GQA and RoPE, how much of the efficiency gain is attributable to each? Without ablation studies, the specific contribution of each \"efficient\" technique remains ambiguous."}, "questions": {"value": "●\tHow exactly is the “Efficiency Score” computed? What are the weighting factors among the six dimensions, and how were they determined or validated?\n●\tWhen you say metrics are “normalized across all models,” what is the reference baseline or normalization formula used?\n●\tHow was the “Average Energy Consumption (AEC)” measured? Did you use any specific monitoring tool (e.g., NVIDIA-SMI, PowerAPI, Carbontracker), and were measurements averaged over multiple runs?\n●\tWere experiments repeated with different random seeds or runs? If not, how do you ensure that the reported results are not due to measurement noise or hardware fluctuations?\n●\tCan you provide quantitative estimates (e.g., total energy in kWh or CO₂ equivalent) to substantiate the sustainability claims?\n●\tDo the metrics or rankings change significantly under different GPU architectures?\n●\tThe paper mentions “Pareto-optimized efficiency,” but no Pareto frontier plots or theoretical analysis are shown. How is Pareto optimality determined or visualized?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jbeQOEJrWn", "forum": "DKAUzhGiDa", "replyto": "DKAUzhGiDa", "signatures": ["ICLR.cc/2026/Conference/Submission13611/Reviewer_UM3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13611/Reviewer_UM3U"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761943001188, "cdate": 1761943001188, "tmdate": 1762924194659, "mdate": 1762924194659, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "**Overview**  \nLarge Language Models (LLMs) like GPT-3 and PaLM have achieved major advances in reasoning and generation but incur enormous **training, deployment, and environmental costs**. Current efficiency evaluations are limited—they often focus on narrow metrics such as FLOPs or latency, lack hardware diversity, and fail to capture the **end-to-end lifecycle** of model training and deployment.  \n\nTo address these gaps, the authors introduce **EfficientLLM**, a large-scale benchmark designed to systematically evaluate **efficiency–performance trade-offs** across LLMs. EfficientLLM unifies **six dimensions of efficiency** (computation, memory, throughput, latency, energy, and compression) within a consistent framework. It evaluates over **150 model–technique pairs** ranging from **0.5B to 72B parameters** on diverse hardware platforms (GH200, H200, A100 clusters). The benchmark covers **pretraining, fine-tuning, and quantization**, providing a comprehensive view of model efficiency across the full lifecycle.  \n\nThe results highlight crucial **trade-offs between accuracy, cost, and sustainability**, offering practical guidance for both researchers and practitioners aiming to design and deploy more efficient foundation models. All code and datasets are open-sourced."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The benchmark is very well motivated to tackle three gaps in current benchmarks (a) incorporate multi-dimensional metrics (b) evaluation across multiple scales and hardwares (c) covering the full llm life cycle (architecture pretraining, fine-tuning and quantization)\n- The insights derived from the creation of the benchmark in Section 1.1 would be of general interest to the ICLR community\n- The paper is very well structured and written clearly in most parts\n- Performance metrics considered for different attention types are thorough and explained clearly\n- The benchmark also considers architectural variations such as MoEs and Mamba in addition to different attention variants in transformers and provides a very thorough architecture coverage."}, "weaknesses": {"value": "- I find that the post-training benchmarking (eg: quantization) lacks several crucial details about type of quantization used (vector or scalar), incoherence processing used (ie rotations), etc. \n- Recently knowledge distillation (KD), pruning in addition to quantization have been used in development of Gemma [1] and llama-3.2 [2] models. I think studies on KD and Pruning would be very useful as these depict crucial parts of the llm life cycle. \n- I find the model scales limited in general. Could the authors add one model of scale in the range on 70B? (eg: llama-3.1-70B)\n- Check questions\n\n[1] Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., Ramé, A., Rivière, M. and Rouillard, L., 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786.\n\n[2] https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/"}, "questions": {"value": "- Could the authors justify the choice of fineweb-edu as the pretraining dataset? \n- Could the authors evaluate on math tasks which require generation such as gsm8k?\n- Could the authors also benchmark models using MHA (multi-head attention) eg: Pythia[1] suite?\n- Could the authors also benchmark models with positional encodings (eg: each position represented by a (learnable) vector)?\n- In the case of MoEs since each token is processed differently (by different experts), how are the efficiency metrics computed here?\n- How does efficiency of PEFT methods vary across types of hardwares used? Is the most/least efficient PEFT method consistent across hardware types?\n- Could the authors elaborate on the type of quantization methods studied? Did the authors use outlier processing with rotations [2], before quantization?\n\n[1] Biderman, S., Schoelkopf, H., Anthony, Q.G., Bradley, H., O’Brien, K., Hallahan, E., Khan, M.A., Purohit, S., Prashanth, U.S., Raff, E. and Skowron, A., 2023, July. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning (pp. 2397-2430). PMLR.\n\n[2] Ashkboos, S., Mohtashami, A., Croci, M.L., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T. and Hensman, J., 2024. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37, pp.100213-100240."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ilhluXn5eQ", "forum": "DKAUzhGiDa", "replyto": "DKAUzhGiDa", "signatures": ["ICLR.cc/2026/Conference/Submission13611/Reviewer_Azzo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13611/Reviewer_Azzo"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13611/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762733136148, "cdate": 1762733136148, "tmdate": 1762924193818, "mdate": 1762924193818, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}