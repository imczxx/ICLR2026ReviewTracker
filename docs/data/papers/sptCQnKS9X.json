{"id": "sptCQnKS9X", "number": 20517, "cdate": 1758306987876, "mdate": 1759896973710, "content": {"title": "When Flatness Does (Not) Guarantee Adversarial Robustness", "abstract": "Despite their empirical success, neural networks remain vulnerable to small, adversarial perturbations. A longstanding hypothesis suggests that flat minima, regions of low curvature in the loss landscape, offer increased robustness. While intuitive, this connection has remained largely informal and incomplete. By rigorously formalizing the relationship, we show this intuition is only partially correct: flatness implies *local* but not *global* adversarial robustness. To arrive at this result, we first derive a closed-form expression for relative flatness in the penultimate layer, and then show we can use this to constrain the variation of the loss in input space. This allows us to formally analyze the adversarial robustness of the entire network. We then show that to maintain robustness beyond a local neighborhood, the loss needs to curve *sharply* away from the data manifold.\nWe validate our theoretical predictions empirically across architectures and datasets, uncovering the geometric structure that governs adversarial vulnerability, and linking flatness to model confidence: adversarial examples often lie in large, flat regions where the model is confidently wrong. Our results challenge simplified views of flatness and provide a nuanced understanding of its role in robustness.", "tldr": "", "keywords": ["Flatness", "Adversarial Robustness"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0164c0363f6456a5dd4ec7b7c64db6a7a6a5eb72.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper provides a formal analysis of the relationship between flatness in the loss landscape and adversarial robustness in neural networks. While flat minima are often believed to enhance robustness, the authors demonstrate that this connection holds only locally, not globally. They derive a closed-form measure of relative flatness in the penultimate layer and use it to constrain input-space loss variation, enabling a theoretical assessment of network robustness. Empirical results across models and datasets support these findings, showing that adversarial examples often occupy flat regions where models are confidently incorrect."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This work presents a framework for analyzing adversarial robustness through relative flatness, a concept introduced in prior studies.\n2. While the conclusion that “flatness implies local but not global adversarial robustness” is not surprising, formally establishing this insight contributes meaningfully to the theoretical understanding of adversarial robustness.\n3. The finding that adversarial examples often lie in large, flat regions is intriguing."}, "weaknesses": {"value": "1. The theoretical analysis appears to overlook the generalization from training to unseen test data. Even if the connection between relative flatness and adversarial robustness holds on the training data, it remains unclear how flatness measured on training examples translates to robustness on unseen test inputs.\n2. The paper lacks actionable insights for improving adversarial robustness, limiting its practical impact despite its theoretical contributions."}, "questions": {"value": "1. Given the similarity between the finding that adversarial examples often lie in large flat regions and the observation of a downward trend in the input loss landscape slope (as shown by IG in Fig. 1 of [1]), it would be valuable to explore whether the theoretical framework presented in this work can explain or align with that trend. Establishing such a connection could strengthen the theoretical grounding and unify observations across studies.\n\n2. Since adversarial training is known to significantly enhance adversarial robustness, it is important to examine how the proposed analysis interacts with or adapts to models trained adversarially. Specifically, how does relative flatness behave under adversarial training, and does the established relationship between flatness and local/global robustness still hold? Addressing this would clarify the scope and applicability of the theoretical insights.\n\n[1] Li, Lin, and Michael Spratling. \"Understanding and combating robust overfitting via input loss landscape analysis and regularization.\" Pattern recognition 136 (2023): 109229."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fQxPQReG9u", "forum": "sptCQnKS9X", "replyto": "sptCQnKS9X", "signatures": ["ICLR.cc/2026/Conference/Submission20517/Reviewer_eTYw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20517/Reviewer_eTYw"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761081663953, "cdate": 1761081663953, "tmdate": 1762933941524, "mdate": 1762933941524, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper re-assesses prior claims about the relationship between the flatness of the loss landscape and adversarial robustness. Consistent with previous claims the paper finds that flatness enhances robustness around particular training samples. However, in contrast to previous claims the current paper suggests that adversarial examples can also lie in flat regions. Hence, increasing flatness does not necessarily improve robustness."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well structured and generally clearly written.\n\nThe paper combines both theoretical and empirical research."}, "weaknesses": {"value": "All the analysis in the paper is based on characterizing successful adversarial attacks through changes in loss rather than changes in predicted label (section 3.2). Specifically, it is claimed that an adversarial perturbation will increases the loss beyond a threshold. However, this is not true for cross-entropy loss. For example, consider a neural network that performs a 3-way classification task. If the true label of a sample is 0 and this network outputs logits [0.6, 0.1, 0.1], then the sample is classified correctly and the cross-entropy loss is 0.7944. If the sample is perturbed in such a way as to produce logits [0.6, 0.7, -5], then the predicted class label is wrong (i.e. the perturbation constitutes a successful adversarial attack), yet the loss decreases to 0.7462. The reverse is also the case: a large increase in loss does not necessarily indicate a successful attack. For example, if the same sample was perturbed so that the network produced logits [0.6, 0.5, 0.5], then the attack would be unsuccessful, but the loss would be increased to 1.0331."}, "questions": {"value": "Given the lack of correspondence between classification accuracy and cross-entropy loss described above, which of the claims/results in the paper are still valid?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qYpAkHPEpM", "forum": "sptCQnKS9X", "replyto": "sptCQnKS9X", "signatures": ["ICLR.cc/2026/Conference/Submission20517/Reviewer_zHuA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20517/Reviewer_zHuA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761303846715, "cdate": 1761303846715, "tmdate": 1762933940949, "mdate": 1762933940949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the long-standing hypothesis that flat minima in the loss landscape imply increased adversarial robustness. Through a rigorous theoretical formulation, the authors show that flatness guarantees only local robustness rather than global robustness. Empirical evaluations across different architectures further support this finding, revealing that adversarial examples often reside in large, flat regions of the loss landscape."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The authors rigorously extend Petzka et al. (2021)’s notion of relative flatness to the setting of adversarial robustness, providing formal derivations that are technically sound and mathematically non-trivial. The paper presents elegant formulations that establish a clear analytical link between adversarial robustness and the Hessian of the loss."}, "weaknesses": {"value": "1. While overall well-written, the paper is dense and somewhat derivative-heavy. Key ideas, such as the geometric mapping between feature-space and input-space curvature, could be illustrated more clearly using diagrams or intuitive explanations. A more intuitive introduction or motivating example would help readers better grasp the high-level ideas before delving into the detailed derivations. In particular, the terms “relative flatness” and “relative sharpness” are used somewhat inconsistently, which may confuse readers.\n\n2. The empirical evaluation is somewhat limited. While the experiments illustrate the theoretical claims qualitatively, they rely on relatively weak PGD attacks (PGD-$l_2$ with $\\epsilon = 0.025$ in page 8) and lack comparisons with adversarially trained baselines (e.g., TRADES, AWP, or SAM-trained models). As a result, it remains unclear whether the observed relationships between sharpness and robustness persist under stronger or more realistic adversarial conditions."}, "questions": {"value": "1. I think the first paragraph on page 1 could be divided into multiple shorter paragraphs to improve clarity. Also, in Figure 1 (mentioned on the first page), the notation $\\phi^{-1}$ is a bit confusing — it’s not immediately clear how the authors relate the feature space to the input space. It would be better to explain this mapping more intuitively in the introduction.\n\n2. On page 2, lines 90–93, the first and second listed contributions appear somewhat redundant. In the first contribution, you state that you theoretically establish the link between flatness and adversarial robustness, while in the second, you restate this point with more detail, specifying that the link is derived through the penultimate layer. I think these two points should be combined into a single, unified contribution for clarity.\n\n3. On page 4, in Definition 4 (Loss-change adversarial example), you define adversarial examples as those that increase the loss by more than $\\epsilon$. Then, in line 183, you state that “by using a conservative $\\epsilon > \\log(k)$ for cross-entropy loss, we can ensure a prediction flip.” However, in practice, when the number of classes $k$ is large, this threshold can correspond to very high loss values. Such cases may represent “strong” adversarial examples (in terms of loss magnitude). Moreover, the reverse implication is not guaranteed, when $l(f(x), y)$ is not close to zero, a prediction flip might occur with much smaller loss changes. Therefore, the relationship you establish between loss increase and prediction change may not hold universally. Could you provide more discussion or clarification on this limitation?\n\n4. On page 7, line 363, and page 8, line 378, there are two separate “Setup” paragraphs in Section 6 From Theory to Practice—one ending with a full stop and another continuing without. It would be clearer to merge them into a single, continuous setup description, as the current separation is somewhat confusing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6l3z2OB324", "forum": "sptCQnKS9X", "replyto": "sptCQnKS9X", "signatures": ["ICLR.cc/2026/Conference/Submission20517/Reviewer_HZEM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20517/Reviewer_HZEM"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761756504590, "cdate": 1761756504590, "tmdate": 1762933940564, "mdate": 1762933940564, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper challenges the theoretical hypothesis that flatness increases the robustness of neural networks and discovered that flatness implies local but not global adversarial robustness. Flatness tends to emerge in regions where the model is highly confident."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The Uncanny Valley analysis helps explain why adversarial examples can appear deceptively safe, because they often lie in flat, vast, high-confidence region.\n2. The paper is generally well-written and well-presented.\n3. The relation between relative sharpness and adversarial robustness is clearly explained, and a precise robustness bound is given"}, "weaknesses": {"value": "1. A large part of the paper is used to justify that relative flatness at the penultimate layer is sufficient, which seems to have been stated in Petzka (2021).\n2. The entire analysis was built on the local flatness at the penultimate layer; did the authors rule out the effect of the geometry of the input space on the adversarial robustness?"}, "questions": {"value": "1. Does the metric relative flatness possibly ignore the correlated curvature directions across layers?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "1g21HSjBnV", "forum": "sptCQnKS9X", "replyto": "sptCQnKS9X", "signatures": ["ICLR.cc/2026/Conference/Submission20517/Reviewer_qEqm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20517/Reviewer_qEqm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20517/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982977124, "cdate": 1761982977124, "tmdate": 1762933939854, "mdate": 1762933939854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}