{"id": "fSRmZ7P1kb", "number": 17857, "cdate": 1758281322052, "mdate": 1759897149771, "content": {"title": "Log-Bit Distributed Learning with Harmonic Modulation", "abstract": "We consider distributed learning over a communication graph where decentralized clients, as local data owners, exchange information only with their neighbors to train a system-level model,  making communication complexity a critical factor. To mitigate this complexity, we introduce a communication quantization scheme based on Harmonic Modulation, in which high-dimensional vectors are compressed and quantized prior to transmission, thereby substantially reducing communication overhead. Building on this idea, we propose Log-Bit Gradient Descent with Harmonic Modulation, where each sender compresses a $d$-dimensional vector into a single scalar, quantizes it into an $m$-bit binary code, and transmits it to the receivers for decoding. Under a sufficient condition, our method achieves an $\\mathcal{O}(1/t)$ convergence rate, where $t$ denotes the number of iterations. Moreover, we establish a conservative lower bound showing that only $\\log_2(\\mathcal{O}(d))$ bits per communication are required, with $d$ representing the vector dimension. Experimental results on synthetic quadratic optimization and logistic regression validate the effectiveness of our approach. In particular, for logistic regression, our method reaches the same target accuracy while using nearly 800× fewer bits per iteration and almost two orders of magnitude less total communication compared to baseline methods.", "tldr": "We propose Log-Bit Distributed Learning with Harmonic Modulation, which compresses high-dimensional updates into log-bit transmissions, enabling provable convergence and drastically reduced communication.", "keywords": ["Federated Learning", "Decentralized Optimization", "Communication Compression", "Quantization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/90201fc02515a93cde235c424b4cacbec142fc1b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes Log-Bit Gradient Descent with Harmonic Modulation, a distributed optimization method that dramatically reduces communication cost by compressing each high-dimensional update vector to a single scalar per iteration. Each client projects its local gradient or model difference onto a deterministic sinusoidal “harmonic modulation” direction, quantizes that scalar into a few bits, and transmits it to its neighbors. The authors provide a convergence proof (under certain) assumptions and claim that only O(log d) bits per round are required, where d is the model dimension. Experiments on synthetic problems show comparable accuracy to existing methods while using fewer transmitted bits per iteration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses an important bottleneck in distributed and federated optimization, communication cost, and proposes an original compression/quantization framework that is mathematically interesting.\n\nThe theoretical analysis is nontrivial and provides an explicit convergence guarantee for an aggressively quantized communication scheme.\n\nExperiments are presented, and comparisons include representative baselines."}, "weaknesses": {"value": "The premise that projecting each d-dimensional update to one scalar suffices effectively requires the projection directions to span all dimensions over time. However, this coverage occurs only asymptotically, over extremely long horizons -- in the Appendix the derivation suggests that this is on the order of d! . It's not clear to me from this analysis that this would be a useful scheme in practice, and I don't think the experiments really get to this point.  \n\nRelatedly, while the paper focuses on metrics such as bits per iteration and total bits, this really doesn't seem like the right metric.  Recent work on compression for distributed and federated optimization have highlighted that a more relevant metric in practice is total time until convergence. If the algorithm requires significantly more iterations to reach the same accuracy, or more computation time, this round-level bit metric seems misleading. No timing or iteration-to-accuracy trade-offs are provided, leaving it unclear what the impact would be here beyond the theoretical."}, "questions": {"value": "Please clarify the realistic implications of the “persistent excitation” assumption and whether the harmonic modulation sequence can meaningfully explore the necessary number of dimensions in a reasonable time horizon.  (2d-1)! doesn't seem feasible to me.  Experiments with d=8 are not particularly interesting;  it makes me concerned you can't manage d=11.  At such dimensions, the bits per round don't seem like an actual constraint.  \n(If this is just meant to be an interesting theoretical paper -- and perhaps judged on that basis it's interesting -- but that doesn't seem to have been the pitch made here.)  \n\nAgain, if practice is a motivation, please provide empirical comparisons of iteration counts and total runtime to demonstrate real communication efficiency.  It seems also relevant in considering a full system to explain how encoding/decoding overhead and synchronization would affect performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9uPEF8JPDu", "forum": "fSRmZ7P1kb", "replyto": "fSRmZ7P1kb", "signatures": ["ICLR.cc/2026/Conference/Submission17857/Reviewer_ZfUi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17857/Reviewer_ZfUi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761085930671, "cdate": 1761085930671, "tmdate": 1762927686929, "mdate": 1762927686929, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a distributed learning algorithm, which aims to reduce communication costs. The gradient vector is sent from one node to all its neighbor nodes after compression, meaning a projection of a $d$ dimensional vector to a scalar, and (uniform) quantization, meaning representing this scalar with a finite number of bits. The main proposal of the paper is a choice of the projection direction, based on an harmonic vector, which is chosen in advance. The paper demonstrates the performance of the algorithm on two convex problems, showing a significant reduction in communication costs, while maintaining the accuracy."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1) The paper is in general easy to follow, and the motivation is clear.\n\n2) The proposed algorithm is rather elegant.\n\n3) The second experiment (logistic regression) shows an impressive savings in communication compared to other algorithms"}, "weaknesses": {"value": "1) The algorithm is rather simple, as random projections can recover high dimensional vectors, and quantization with sufficient number of bits will allow the corresponding scalar to be reproduced. It should be discussed, how this algorithm is significantly different from the current state-of-the-art (SOTA) algorithm, and what is the new mechanism that allows to improve. \n\n2) The choice of this specific harmonic vector is stated as a main aspect of the method proposed in the paper. However, how this choice of vector as a harmonic vector affects performance. For example, what will be the performance in case the projection vector is one-hot vector, where the location of the $1$ changes with time? What is the performance for pseudo-random vectors (chosen with some seed) from an isotropic Gaussian? More deeply, this choice is heuristic, and if at all, it is of interest to explain how the choice of the projection vectors is suitable to classes of problems.\n\n3) Algorithm 1, the main algorithm of the paper is neither explained nor discussed. Specifically, lines 6-7 and then 16-18, which are the main updates of the algorithm (beyond the compression-quantization part) are not explained. \n\n4) Theorem 4.1 (the main and only theorem of the paper) states that $O(\\log(d))$ bits suffice for an error of $O(1/t)$. Thus the dependence on the number of bits, network topology, step sizes and other parameters of the algorithm is unspecified. In other words, since the dependence on $d$ is unspecified in the convergence rate, one can just propose a round-robin algorithm, where in each step a different coordinate is quantized. It is also difficult to distill a more accurate result from the appendix. \n\n5) The other main aspect of the setting considered in the paper, to wit, distribution of computation across a network, is hardly discussed in the paper: The theoretical result does not display any dependence on the network, and only a single graph in the first experiment shows that the network topology does not significantly affects the convergence (which obviously could be challenged with very difficult topologies). \n\n6) The experiments are only on convex problems. Even if the theoretical guarantees are for convex functions, the algorithm can still be tested on non-convex ones, such as deep neural networks.\n\n7) Since the algorithm is rather simple, the origin of a two-order of improvement achieved in the second case should be discussed in detail. There are many algorithms for federated and distributed learning. Are the baselines represent the SOTA ?"}, "questions": {"value": "1) Equation (2) is a rather formulation of the problem. On the left-hand side there is a single variable $x$, and on the right-hand side $n$ variables $x_i$. Why is this necessary?\n\n2) The “persistent excitation” condition in Lemma 3.1 is achieved with rather large constants, on the order of $O((2d)!)$. It is not clear from the results of the paper how these constants affect the performance, but anyway, an exponential dependence on $d\\log(d)$ does not scale well.\n\n3) On page 5, the Laplacian is defined with constants $a_{ij}$. I could not find the use of these constants in the algorithm.\n\n4) Line 365 - typo “a'rara”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LAQ2ukai45", "forum": "fSRmZ7P1kb", "replyto": "fSRmZ7P1kb", "signatures": ["ICLR.cc/2026/Conference/Submission17857/Reviewer_wAuE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17857/Reviewer_wAuE"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761421270354, "cdate": 1761421270354, "tmdate": 1762927686497, "mdate": 1762927686497, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel communication-efficient algorithm for decentralized distributed learning called Log-Bit Gradient Descent with Harmonic Modulation. The core problem addressed is the high communication complexity in distributed learning, where agents must exchange high-dimensional vectors. The key innovation is Harmonic Modulation , a compression scheme where each $d$-dimensional vector is compressed into a single scalar using a time-varying, deterministic harmonic projection sequence. This scalar is then quantized into an $m$-bit binary code for transmission. The authors prove that this method achieves an optimal $\\mathcal{O}(1/t)$ convergence rate for strongly convex problems, while theoretically only requiring $m \\geq \\mathcal{O}(\\log_2(d))$ bits per communication. Experiments on quadratic optimization and logistic regression validate the approach, showing that LBGd-HarMo achieves comparable accuracy to baselines (like DSGD and CHOCO) while reducing communication by up to 800x-4000x bits per iteration."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The primary strength is the originality of the Harmonic Modulation scheme. Compressing a $d$-dimensional vector to a single $m$-bit scalar 31via a deterministic, time-varying projection 32 is a fundamentally new approach to communication compression in this field. It breaks from the standard paradigms of sparsification or $d$-dimensional quantization.\n\n2.Strong and interpretable theory: an explicit $\\mathcal{O}(1/t)$ rate under standard convexity and graph assumptions with a sufficient condition $m = \\Omega(\\log_2 d)$ that formalizes log-bit communication sufficiency for convergence.\n\n3.The paper is a model of clarity. Figure 1 perfectly illustrates the method. The experiments are thorough, testing robustness to client count, network topology, and quantization precision (Figure 2). The clear separation of performance vs. iterations (Figure 4a) and vs. communicated bits (Figure 4b) makes the contribution unambiguous."}, "weaknesses": {"value": "1.The proof of the PE condition (Lemma 3.1, Appendix B) derives a period $N = (2d - 1)!$. This is an astronomically large number. While this appears to be a sufficient period for the proof and does not seem to affect the final $\\mathcal{O}(1/t)$ rate, it is a somewhat jarring theoretical artifact. It's unclear if this has any practical implications or if it's purely a limitation of the current analysis. The more important result is the $m \\geq \\mathcal{O}(\\log_2(d))$ bit-rate, which is beautifully supported by the experiment in Figure 2c.\n\n\n2.Communication accounting lacks clarity. The reported communication volume (in KB) is aggregated across links and nodes in a way that does not clearly align with the stated per-message costs for $Top-\\alpha$ and Sign. A precise definition is needed to specify whether communication is measured per edge, per node, or as a network total, along with consistent measurement across all methods.\n\n3.Tuning complexity and stability margins. The algorithm introduces several hyperparameters ($\\kappa, \\kappa_0, \\eta, \\alpha, g_0, \\gamma$), and while tuned fairly, guidance on principled selection to mis-specification would improve usability."}, "questions": {"value": "1.Could you comment on the computational overhead of the HarMo encoder/decoder? As noted in the weaknesses, this involves $d$ $\\sin$ operations and two $d$-dimensional vector operations at each step $t$. How does this computational cost scale, and how does it compare to the cost of the local gradient computation? Is there a point (a very large $d$) where this computation is no longer negligible?\n\n2.How sensitive is convergence to the choice of the harmonic schedule; e.g., would randomized orthogonal directions or structured Hadamard projections yield similar guarantees with better constants than the factorial PE window?\n\n3.Could you add experiments on time-varying graphs or mild nonconvex objectives (e.g., two-layer networks) to assess whether the log-bit mechanism and error-feedback variables $\\sigma\\$ and $z$ remain effective beyond the strongly convex regime?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pe2k9QpsoX", "forum": "fSRmZ7P1kb", "replyto": "fSRmZ7P1kb", "signatures": ["ICLR.cc/2026/Conference/Submission17857/Reviewer_gMoS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17857/Reviewer_gMoS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761710481611, "cdate": 1761710481611, "tmdate": 1762927685958, "mdate": 1762927685958, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Log-Bit Gradient Descent with Harmonic Modulation (LBGD-HarMo), a decentralized distributed learning framework that compresses high-dimensional updates into single scalars via a deterministic harmonic sequence, quantizes them into m-bit codes, and reconstructs them at receivers. Under standard convexity and graph-connectivity assumptions, it achieves an O(1/t) convergence rate while using only O(log d) bits per iteration, a near-optimal communication bound. Theoretical analysis proves persistent excitation of the harmonic projections and sublinear convergence; a conservative lower bound shows log₂(d) bits suffice. Empirical evaluation on synthetic quadratic problems and logistic regression (εpsilon dataset) demonstrates that LBGD-HarMo matches the accuracy of DSGD, CHOCO, MoTEF, and Sign-SGD with up to two orders of magnitude less communication. Robustness across network sizes, topologies, quantization levels, and data heterogeneity (IID vs. Non-IID) is shown."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The harmonic modulation compresses d-dimensional vectors to scalars deterministically, avoiding random sketches and enabling error-feedback-free updates.\n2. Proofs establish persistent excitation of HarMo, O(1/t) convergence, and log₂(d) bit complexity, filling a gap in communication-efficient decentralized learning.\n3. Extensive experiments (varying client count, topologies, quantization bits, data distributions) on both synthetic and real benchmarks confirm substantial communication reduction without sacrificing convergence."}, "weaknesses": {"value": "1. The introduction briefly contrasts compression and quantization but lacks discussion of practical scenarios (e.g., wireless sensor networks) where single-scalar exchanges outperform existing sparse/sketched schemes.\n2. Computing ψHarMo(t) involves sinusoids of increasing frequency up to πd/(d+1)t; costs and numerical stability for large d or t are not analyzed.\n3.  Though quantization error bounds ∥qm(a)−a∥∞≤l/2, the impact of reconstructing a full vector ψHarMo(t)·qm(ψ⊤b) on consensus bias is not empirically quantified.\n4. Convergence proof and experiments rely on many hyperparameters (κ, κ₀, η, g₀, γ, m); guidance on selecting or adapting them in practice is missing.\n5. CHOCO and MoTEF use Top-α with α=0.125, but LBGD-HarMo uses m=8 bits; a sensitivity analysis matching communication budgets across methods is absent.\n6. The framework is limited to strongly convex objectives; no preliminary experiments on non-convex tasks or discussion of potential obstacles (e.g., PE condition violation).\n7.  Logistic regression uses d=2 000 features, but deep learning models feature millions of parameters; the paper omits runtime/memory benchmarks on large-scale settings.\n8. Only bits transmitted are measured; actual round-trip latency, synchronization overhead, and impact of asynchrony are not considered."}, "questions": {"value": "please see the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vX8VQXzVgp", "forum": "fSRmZ7P1kb", "replyto": "fSRmZ7P1kb", "signatures": ["ICLR.cc/2026/Conference/Submission17857/Reviewer_bf5v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17857/Reviewer_bf5v"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17857/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993307710, "cdate": 1761993307710, "tmdate": 1762927685533, "mdate": 1762927685533, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}