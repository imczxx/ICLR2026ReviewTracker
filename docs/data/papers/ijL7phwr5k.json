{"id": "ijL7phwr5k", "number": 17137, "cdate": 1758272623839, "mdate": 1759897194449, "content": {"title": "IMLP: An Energy-Efficient Continual Learning Method for Tabular Data Streams", "abstract": "Tabular data streams are rapidly emerging as a dominant modality for real-time decision-making in healthcare, finance, and the Internet of Things (IoT). These applications commonly run on edge and mobile devices, where energy budgets, memory, and compute are strictly limited. Continual learning (CL) addresses such dynamics by training models sequentially on task streams while preserving prior knowledge and consolidating new knowledge. While recent CL work has advanced in mitigating catastrophic forgetting and improving knowledge transfer, the practical requirements of energy and memory efficiency for tabular data streams remain underexplored. In particular, existing CL solutions mostly depend on replay mechanisms whose buffers grow over time and exacerbate resource costs.\n\nWe propose a \\textit{context-aware incremental Multi-Layer Perceptron (IMLP)}, a compact continual learner for tabular data streams. IMLP incorporates a windowed scaled dot-product attention over a sliding latent feature buffer, enabling constant-size memory and avoiding storing raw data. The attended context is concatenated with current features and processed by shared feed-forward layers, yielding lightweight per-segment updates.\nTo assess practical deployability, we introduce \\textit{NetScore-T}, a tunable metric coupling balanced accuracy with energy for Pareto-aware comparison across models and datasets.  \nIMLP achieves up to \\textbf{$27.6\\times$} higher energy efficiency than TabNet and \\textbf{$85.5\\times$} higher than TabPFN, while maintaining competitive average accuracy. \nOverall, IMLP provides an easy-to-deploy, energy-efficient alternative to full retraining for tabular data streams.", "tldr": "", "keywords": ["Continuous learning", "energy efficiency", "tabular data streams"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/85a34d926e21d5322c6b8c9423e460720fe98892.pdf", "supplementary_material": "/attachment/7ae35e530e3545c613a280622982ee6aad41ef74.zip"}, "replies": [{"content": {"summary": {"value": "This paper aims to address an important and practical problem in the field of continual learning for tabular data streams: how to achieve energy-efficient learning on resource-constrained devices. The authors propose an incremental Multi-Layer Perceptron (IMLP) model that utilizes a fixed-size sliding window storing latent features, combined with an attention mechanism, to avoid storing raw data and prevent unbounded memory overhead. Furthermore, the authors introduce a new evaluation metric, NetScore-T, to quantify the trade-off between a model's accuracy and its energy consumption."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The task setting is very interesting.\n2. The data processing and experimental details are thoroughly described in the appendix, which enhances the reproducibility and credibility of the study. In addition, the experimental metrics are comprehensively designed, and the comparison methods cover both recent popular paradigms and classical models in tabular learning, providing a solid basis for the objectivity of the results."}, "weaknesses": {"value": "1. The paper's core method is a combination of existing, mature techniques such as sliding windows, attention mechanisms, and latent replay, rather than proposing a fundamentally new algorithm or theory. Therefore, its innovative academic contribution is limited.\n2. Comparing the energy efficiency of the incrementally-trained IMLP against baseline models forced into a cumulative retraining setting leads to an obvious conclusion (that incremental updates are more efficient than retraining). This comparison fails to effectively demonstrate IMLP's superiority over other **continual learning** algorithms.\n3. The paper fails to quantify the actual contribution of its key components (e.g., the attention mechanism) or analyze the impact of core hyperparameters (e.g., window size) through ablation studies, leaving the effectiveness and rationale of the model's design unverified.\n4. Simulating a data stream by partitioning a static dataset ignores complex, real-world dynamics such as \"gradual concept drift.\" This makes it questionable whether the model's performance in controlled experiments can be generalized to practical applications.\n5. The caption in figure2 is too small.\n6. As the authors mention at the beginning of the experimental section, most baseline models are not designed for streaming learning. To ensure a fair comparison, a “best-effort comparison” strategy is adopted. However, under this setting, IMLP performs incremental updates, while other models need to be retrained from scratch for each segment, which may introduce bias in the energy consumption results. Is such a comparison truly fair?\n7. A strong motivation of this paper is to maintain comparable accuracy while improving energy efficiency. However, could the FIFO mechanism cause highly discriminative early features to be discarded too early, leading to catastrophic forgetting and affecting the model’s long-term stability? It would be helpful to include comparison experiments with different buffer strategies."}, "questions": {"value": "Overall, I think this is a very interesting topic, but current version needs to be polished."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tr4wwHIqzc", "forum": "ijL7phwr5k", "replyto": "ijL7phwr5k", "signatures": ["ICLR.cc/2026/Conference/Submission17137/Reviewer_kJho"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17137/Reviewer_kJho"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761432007160, "cdate": 1761432007160, "tmdate": 1762927130775, "mdate": 1762927130775, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses energy-efficient continual learning (EECL) for tabular data streams, a crucial yet\nunderexplored direction in Green AI and Edge Learning. The authors propose IMLP, a lightweight\nadaptation of MLP for online tabular learning under energy and memory constraints. By integrating\nwindowed self-attention and a sliding latent buffer, the model achieves fixed-memory continual adaptation\nwithout sample replay. A new metric, NetScore-T, is introduced to jointly assess accuracy and energy\nconsumption. Experiments on 36 TabZilla datasets show consistent performance-energy trade-offs, with\nempirical results supported by Pareto-front analysis using real hardware power measurements. Overall, the\npaper contributes a practically meaningful and methodologically coherent approach for sustainable tabular\ncontinual learning"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1) Timely and relevant topic: The work targets energy-efficient continual learning on tabular data, aligning with sustainability and edge-computing trends while filling a clear research gap beyond vision and NLP.\n\n 2) Methodological coherence: The paper presents a well-defined logical chain — from problem formulation to mechanism design, model implementation, and empirical validation — with a consistent focus on energy-accuracy trade-offs.\n\n3) Technical design: The “windowed attention + latent feature buffer” structure achieves constant memory and low energy without sacrificing performance, balancing efficiency, privacy, and adaptability. \n\n4) Evaluation rigor: Experiments are extensive and diverse, covering 36 datasets, multiple baselines, and real hardware\nenergy measurements. The use of NetScore-T and Pareto analysis provides a meaningful and interpretable evaluation\nframework."}, "weaknesses": {"value": "1) ) Innovation boundary: While practical, the “windowed attention” and FIFO buffer design appear incremental and largely engineering-driven, with limited conceptual novelty over prior latent replay or SAINT methods. \n\n2) Experimental limitations: The experiments are primarily conducted under presegmented data streams, missing evaluations on abrupt drifts, class expansion, and noisy streams. Statistical significance tests and ablation analyses are insufficient. \n\n3) Deployment and robustness gaps: No validation on real edge devices (e.g., latency, battery life), and feature dimensionality sensitivity is not explored. Pareto analysis lacks quantitative AUC comparisons."}, "questions": {"value": "1) Can the authors provide theoretical support for the energy-accuracy trade-off, such as energy complexity bounds or convergence guarantees under non-stationary distributions?\n2) How is the NetScore-T metric normalized across different hardware settings? Is it stable and comparable across devices?\n3) Could the latent buffer be made adaptive to distributional drift (e.g., through feature validity scoring)?\n4) How would IMLP perform on real-world streaming or edge environments where drift and noise are dynamic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "IsHk5JyRjE", "forum": "ijL7phwr5k", "replyto": "ijL7phwr5k", "signatures": ["ICLR.cc/2026/Conference/Submission17137/Reviewer_vdbK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17137/Reviewer_vdbK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918991042, "cdate": 1761918991042, "tmdate": 1762927129956, "mdate": 1762927129956, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces IMLP (Incremental Multi-Layer Perceptron), a continual learning approach designed for tabular data streams under energy and memory constraints. IMLP replaces replay buffers with a windowed scaled dot-product attention mechanism that operates over a fixed-size latent feature buffer, avoiding raw data storage while maintaining context. The authors also propose NetScore-T, a new metric that balances accuracy and energy consumption for fair comparison across models and datasets."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Addresses the underexplored area of energy-efficient continual learning for tabular data.\n2. Uses a lightweight MLP with feature-level attention instead of complex replay mechanisms.\n3. Evaluated on diverse datasets, with clear statistical analysis (Friedman/Wilcoxon tests) and real hardware-level energy measurements."}, "weaknesses": {"value": "1. The paper lacks formal justification or convergence analysis of the proposed attention-based feature memory.\n2. Most baselines (e.g., TabPFN, TabNet) are retrained from scratch rather than adapted for continual settings, which could inflate IMLP’s relative advantage.\n3. No detailed study on how window size (W), feature dimension, or attention mechanism choices affect performance or energy.\n4. While energy efficiency is showcased, the performance loss (~5% in accuracy and ~15% in log loss) might matter for high-stakes domains.\n5. NetScore-T unfairly benefits the method proposed - The method is explicitly designed for low energy consumption, not necessarily for maximal accuracy. Since NetScore-T rewards small energy values non-linearly (via log scaling), it amplifies IMLP’s comparative advantage.\n6. Disproportionate penalty for high-energy models: Because the denominator grows slowly (log scale), an order-of-magnitude energy reduction can more than offset noticeable accuracy drops."}, "questions": {"value": "1. How do results change if baselines are trained incrementally rather than retrained from scratch?\n2. What is the actual storage overhead (in MBs) of the latent feature buffer for typical settings?\n3. Could IMLP handle concept drift or non-stationary distributions beyond the fixed task segmentation in TabZilla?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "A2KFTu9p7B", "forum": "ijL7phwr5k", "replyto": "ijL7phwr5k", "signatures": ["ICLR.cc/2026/Conference/Submission17137/Reviewer_xd8r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17137/Reviewer_xd8r"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17137/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932326482, "cdate": 1761932326482, "tmdate": 1762927129036, "mdate": 1762927129036, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}