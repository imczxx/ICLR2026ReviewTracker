{"id": "O33LAUliUF", "number": 20023, "cdate": 1758301622852, "mdate": 1759897005930, "content": {"title": "On the Interpolation Effect of Score Smoothing in Diffusion Models", "abstract": "Score-based diffusion models have achieved remarkable progress in various domains with the ability to generate new data samples that do not exist in the training set. In this work, we study the hypothesis that such creativity arises from an interpolation effect caused by a smoothing of the empirical score function. Focusing on settings where the training set lies uniformly in a one-dimensional subspace, we probe the interplay between score smoothing and the denoising dynamics with analytical solutions and numerical experiments. In particular, we demonstrate how a smoothed score function can lead to the generation of samples that interpolate among the training data within their subspace while avoiding full memorization. Moreover, we present theoretical and empirical evidence that learning score functions with regularized neural networks can have a similar effect as score smoothing, including in simple nonlinear settings.", "tldr": "", "keywords": ["score-based diffusion model", "empirical score function", "norm-bounded neural network", "barron norm", "function smoothing", "subspace recovery"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/ab0f43f0579886050c21bc4235aa0f791abf975a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies the effect of score smoothing on the ability of a diffusion model to generate samples outside the training data set. Most of the analysis revolves around a simply toy problem with evenly-spaced data points, where the analytic score functions (ESF) are known exactly.  It is shown that the scores learned by a regularized two-layer NN trained on this dataset is closely approximated by a smoothed version of the ESF. In the 1D case, and when the data manifold is a 1D subspace of a larger data space, the smoothed score (which models the NN score) ascribes some probability to regions on the data manifold that lie between the training data samples. At least in this limited setup, smoothed score functions offer a compelling explanation to 'generalization'."}, "soundness": {"value": 4}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. It is always great to have an analytic model that closely approximates the NN behavior. The authors have done a great job in picking a very minimal setup, modeling the score in closed form, and analyzing it thoroughly to draw their conclusions.\n\n2. The problem being studied is pertinent, and the paper makes clear progress albeit with a simple example.\n\n3. The claims made in the paper are backed up with rigorous proofs."}, "weaknesses": {"value": "1. The core arguments in the paper, which are contained in Sec. 3 and 4, were a little difficult to get through for me. Part of the the problem was that a great deal of notation was introduced in the course of developing the argument that I missed the forest for the trees. I think it would greatly help the paper to have a sketch of the content of these sections precede the more detailed discussion. On a more minor note, making Fig. 2 wider, and stating explicitly that $\\lambda$ is the weight decay coefficient around line 161 would also help.\n\n2. The authors use the fact that regularizing the weight norm penalizes non-smooth scores. Later, in Sec. 6.2 they show an experiment where the NN scores smoothly interpolate a circular dataset, even _without weight decay_. This is attributed vaguely to the optimization dynamics. But does this not undercut your main thesis? That is, can you still claim that your smoothed PL ESF reflects the NN scores in more general scenarios where the latter is smoothed by mechanisms other than weight decay? In particular, is there reason to believe that such smoothing will produce a score with smoothness close to that of $r^{*}$? To be clear, this is a question about the _relevance_ of the smoothed PL ESF, rather than the soundness. If there is an answer in the paper that I missed please let me know. Otherwise, it appears to me that this is the primary weakness of the paper.\n\n3. In the extension to higher-dimensions, it would have been interesting to see a data subspace with some curvature, even a mild one. Would some sort of 'locally-flat' assumption help generalize your arguments to such a subspace?"}, "questions": {"value": "In proposition 1, if I understood correctly, the function $F^{-1}(\\epsilon)$ grows as $\\epsilon \\to 0$. That means $\\kappa$ becomes larger at small $\\epsilon$. So $\\delta_t = \\kappa \\sqrt{t}$ is small only when $t$ is very small. But we also know that the actual diffusion model is trained by evaluating the denoising score-matching objective from $t_{\\rm min}$ to $T$. If the smoothed PL ESF in Eq. (9) is a 'proxy' for the NN score in your analysis, should you not restrict yourself to using this score only till $t_{\\rm min}$. Then, is $\\delta_{t_{\\rm min}} = \\kappa \\sqrt{t_{\\rm min}}$ necessarily small?\n\nI have given a tentative score based on my current understanding of the paper. I'm willing to update the score if my questions are addressed satisfactorily."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RN9itfijPa", "forum": "O33LAUliUF", "replyto": "O33LAUliUF", "signatures": ["ICLR.cc/2026/Conference/Submission20023/Reviewer_8eUJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20023/Reviewer_8eUJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761349101861, "cdate": 1761349101861, "tmdate": 1762932921844, "mdate": 1762932921844, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies how NN-based score predictor can help alleviate the exact memorization phenomenon when an exact score function is used. Specifically, the authors study the score function learned via 2-layer ReLU NN, and show that the learned score function has a smoothing effect that helps creativity."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The paper is well-organized and strongly motivated. Studying NN-learned score function is important in understanding the creativity of diffusion model. The theory is rigorous and the authors carry out well-targeted experiments to verify their theoretic results."}, "weaknesses": {"value": "1. missing highly-related prior work [1]\n\nSpecifically, in the current paper, the authors mention \"while it is difficult to predict exactly what function an NN will learn due to the nonlinearity and stochasticity of the training dynamics\" and seeks to find the NN prediction by converting weight decay as a smoothness penalty, while in work [1], the authors already converted the non-convex two-layer NN to a convex program and solved it to optimality, where they showed the optimal function is piecewise linear.\n\nIndeed, Figure 2 in current paper is well-aligned with Figure 1 in [1], where the authors in [1] already theoretically showed why 2-layer ReLU will produce results as Figure 2 in current paper. The 2d circle experiment in current paper was also done in Section 5.2 in [1]. Also, formula (9) in current paper looks similar to formula (7) in [1].\n\nThe highly-correlated results and scientific problems being studied should be paid attention.\n\n2. the setting is limited to 2 layer ReLU, and is far from what's being used in real diffusion model\n\n\n[1] Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization: https://arxiv.org/abs/2402.01965"}, "questions": {"value": "1. is there any clue how to extend the current analysis framework to deeper NN architecture?\n\n2. if it's hard to generalize current result to modern architecture, is there at least any clue that whether modern diffusion models' effectiveness also comes from such score smoothing mechanism? \n\n3. if the authors take a look at the prior work mentioned above, there is some deeper connections between these two (which might also be superficial), like the current work has sign(x) for 1d data, and the derived convex program also has sign(x) (see the paragraph below Theorem 3.1). Is this coincidence?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pv0mwqYQeL", "forum": "O33LAUliUF", "replyto": "O33LAUliUF", "signatures": ["ICLR.cc/2026/Conference/Submission20023/Reviewer_GL3U"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20023/Reviewer_GL3U"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761548287013, "cdate": 1761548287013, "tmdate": 1762932921166, "mdate": 1762932921166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies why score-based diffusion models (DMs) with neural networks can generate new samples instead of simply memorizing their training data.\nBesides, the authors propose a mathematically tractable Smoothed Piecewise-Linear Empirical Score Function and show that this smoothing prevents the reverse-time denoising process from collapsing to training points.\nThrough analytical results and simple experiments, they demonstrate that both smoothed and NN-learned score functions produce smooth interpolations rather than pure memorization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper gives a clear and rigorous mathematical formulation of how score smoothing prevents collapse and enables interpolation in diffusion models.\n2. It provides elegant geometric insight and connects explicit smoothing with the implicit regularization effect of neural network training."}, "weaknesses": {"value": "1. The paper does not theoretically prove that neural network training truly realizes the same smoothing mechanism.\n2. All experiments are conducted on low-dimensional toy cases, and it remains uncertain whether the proposed mechanism can actually help reduce memorization in diffusion models trained on limited real-world data."}, "questions": {"value": "See Weakness. Further:\n  1. Can the proposed smoothing framework be extended or tested on high-dimensional real data ï¼Ÿ"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zhATEAsFfM", "forum": "O33LAUliUF", "replyto": "O33LAUliUF", "signatures": ["ICLR.cc/2026/Conference/Submission20023/Reviewer_1ab2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20023/Reviewer_1ab2"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924892007, "cdate": 1761924892007, "tmdate": 1762932920706, "mdate": 1762932920706, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the interpolation effect of NN-learned score estimator in diffusion models. Numerical experiments are conducted to support theoretical analysis."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. By using a two-point 1-d example, the authors analyze properties of a smoothed piece-wise linear score estimator.\n2. The authors investigates the evolution of marginal distribution in sampling using the above estimator\n3. For multi-point case, the theory is also established.\n4. Numerical experiments are conducted for this special estimator with comparison to neural networks."}, "weaknesses": {"value": "I have several major concerns:\n1. Essentially, the authors constructed a special score estimator and argues diffusion model sampling benefits from this estimator. However, it is not clear to me how this estimator is connected to neural networks. Even in 1-d two-point case, such a connection is missing.\n2. Seemingly the authors propose a new regularized score estimator to make it smoother and thus benefiting the sampling process. However, in practice, the regularized training objective (10) and (20) seem to be hard to compute. Could you elaborate this point further?\n3. In terms of experiments, the NN-learned estimator indeed behave similarly to the one proposed by authors. Is it because that NN learns exactly this piece-wise linear form or NN estimator enjoys some other smoothing properties?"}, "questions": {"value": "I have a minor question:\n1. Although maybe not easy, I would appreciate it if authors could show numerical results for high-dimensional data."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fQxoHxmAnE", "forum": "O33LAUliUF", "replyto": "O33LAUliUF", "signatures": ["ICLR.cc/2026/Conference/Submission20023/Reviewer_PEv5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20023/Reviewer_PEv5"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20023/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762217638497, "cdate": 1762217638497, "tmdate": 1762932920260, "mdate": 1762932920260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}