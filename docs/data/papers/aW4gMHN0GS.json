{"id": "aW4gMHN0GS", "number": 8698, "cdate": 1758095345810, "mdate": 1759897769042, "content": {"title": "A Task-Aware Dynamic Expansion Network for Continual Reinforcement Learning", "abstract": "Reinforcement learning has been widely applied in domains such as gaming and robotic control. However, CRL methods that rely on a single network architecture often struggle to preserve previously learned skills when they are trained on substantially different new tasks. To address this challenge, we propose a Task-Aware Dynamic Expansion Network (TADEN), which features a task-aware expansion strategy. This approach collects sequential environment states to measure task similarity, which reflects the suitability of the existing policy to a new task. Then, the task similarity score is utilized to determine whether to expand the actor-critic architecture or reuse existing modules. When expanding the network, our method leverages prior knowledge while preserving adaptability by initializing new modules through the reuse of lower layers of existing modules. We evaluate our method on the MiniHack and Atari environments. The experimental results demonstrated that TADEN achieved significantly better performance and mitigated catastrophic forgetting compared to existing methods.", "tldr": "", "keywords": ["continual learning", "reinforcement learning"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5be74d6bc164670a516015d5aef2f5cbccb955cb.pdf", "supplementary_material": "/attachment/5904dcb49ceeb52f215084b3717ab90ac77a1383.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces the Task-Aware Dynamic Expansion Network (TADEN), a novel framework designed to address a critical challenge in Continual Reinforcement Learning (CRL): catastrophic forgetting. The TADEN framework is built on several key components:\n\n**Task-Specific Representation and Similarity Measurement:** The core idea is to define a task's \"identity\" by the statistical distribution of environment states an agent encounters while performing that task. Before fully training on a new task, TADEN uses a reinforcement learning policy (PPO) for a few steps to collect a sequence of states. This sequence is passed through a feature extractor (a pre-trained ResNet18) to create a feature distribution that represents the task. This representation is then compared to the representations of previously learned tasks (stored in a memory bank) using the Wasserstein distance, a metric robust for comparing distributions that may not overlap.   \n\n**Task-Aware Expansion and Reuse:**The calculated task similarity score serves as the basis for an architectural decision. If the similarity score for a new task is low compared to all previous tasks (i.e., the minimum Wasserstein distance is above a set threshold), the new task is deemed dissimilar. In this case, TADEN expands the network by creating a new, separate actor-critic module to prevent interference and protect existing knowledge. Conversely, if the task is found to be similar to a previous one, the framework reuses the existing module associated with the most similar task, promoting knowledge sharing and limiting model growth.   \n\n**Dual-Mode Initialization:** When a new module is created, TADEN employs a hybrid initialization strategy to balance knowledge transfer and adaptability. The lower layers of the new module, which are responsible for feature extraction, inherit their parameters from the most similar existing module. This facilitates the transfer of general, low-level knowledge. The top layer, or \"policy head,\" which is responsible for final decision-making, is randomly initialized. This ensures the module has the plasticity required to adapt to the unique demands of the new task.   \n\n**Task Inference at Test Time:** A crucial feature of TADEN is its ability to operate without explicit task labels during testing. When faced with a test environment, the agent collects a small number of observations, computes the task representation, and compares it to the representations in its memory bank. The module corresponding to the most similar task is then activated to perform the task, enabling the agent to autonomously select the correct skill for the job."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Clear Writing\n\n2. Clear Motivation\n\n3. Outperforming Performance"}, "weaknesses": {"value": "1. Insufficient Reference Check: There are some context-aware methods in reinforcement learning, e.g., [1]. Although they did not focus on continual RL, it is necessary to explain previous context-aware methods.\n\n2. Lack of comparison with state-of-the-art methods: For instance, although the authors mentioned that the public code in [1] is not available, but I found that they released a public code. \n\n---\n\n\n[1]  Lee, Kimin, et al. \"Context-aware dynamics model for generalization in model-based reinforcement learning.\" International Conference on Machine Learning. PMLR, 2020.\n\n[2]  Ahn, Hongjoon, et al. \"Prevalence of negative transfer in continual reinforcement learning: Analyses and a simple baseline.\" The Thirteenth International Conference on Learning Representations. 2025."}, "questions": {"value": "1. It seems that similar task-aware dynamic approaches have already been developed, e.g., [1]. I suggest that the authors include a discussion of these prior methods and clarify what specifically distinguishes their proposed approach.\n\n2. The baselines appear to be rather classical. The MoE method is not tailored for continual reinforcement learning but is instead a standard continual learning approach. Moreover, BC-based methods are not included in the comparison. Additionally, although the authors mentioned that the public code in [2] is unavailable, I found that it has actually been released. I recommend that the authors explicitly clarify this point and demonstrate that their proposed method remains effective even when compared to other relevant approaches not covered here.\n\n3. The authors state that previous network expansion methods incur high memory and computational costs. Could the authors elaborate on how their method addresses these issues in terms of network expansion efficiency? Although Section D provides a partial explanation, it would be very helpful if the authors could also discuss the general tendencies of other architecture-based methods for comparison.\n\n\n[1]  Lee, Kimin, et al. \"Context-aware dynamics model for generalization in model-based reinforcement learning.\" International Conference on Machine Learning. PMLR, 2020.\n\n[2]  Ahn, Hongjoon, et al. \"Prevalence of negative transfer in continual reinforcement learning: Analyses and a simple baseline.\" The Thirteenth International Conference on Learning Representations. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "oXPCr9EaYV", "forum": "aW4gMHN0GS", "replyto": "aW4gMHN0GS", "signatures": ["ICLR.cc/2026/Conference/Submission8698/Reviewer_n2uM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8698/Reviewer_n2uM"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760809775336, "cdate": 1760809775336, "tmdate": 1762920505848, "mdate": 1762920505848, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TADEN (Task-Aware Dynamic Expansion Network) for continual reinforcement learning. The method keeps a memory bank of task representations built from short sequences of observations collected by briefly running a policy in each new environment and encoding them with a fixed ResNet-18. For a new task, it compares the representation to stored ones using the Wasserstein distance to decide whether to reuse an existing module or create a new one. New modules reuse lower layers from the closest module while randomly initializing the policy head for adaptation. At test time, the model identifies the right module by comparing a few new observations with the memory bank. Experiments on MiniHack and Atari show that TADEN outperforms previous continual reinforcement learning baselines, with ablations confirming the value of its expansion rule and initialization design."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Easy to follow and read.\n- Empirical results on the Atari benchmark show that TADEN effectively prevents forgetting during continual learning."}, "weaknesses": {"value": "- The framework combines known ideas, such as Meta-RL style task embedding construction [1,2,3] for the memory bank and model expansion [4,5], but it does not introduce a fundamentally new learning principle.\n- Ambiguity in the paper’s claim and problem formulation. For example, the abstract states that “...struggle to preserve previously learned skills...,” but the actual method operates based on task-level actor critic moduleestimation. There is a lack of concrete evidence of how skills are preserved in practice .The paper should report an analysis of the correlation between task similarity and correct actor critic moduleretrieval, ideally by comparing the retrieved model context with the true task context. as the knowledge sharing occurs across experts or actual skills.\n- Lack of fairness in comparison. One of the easiest ways to address the stability–plasticity trade-off is to use more memory (e.g., replay buffer or model parameters). At minimum, evaluations should be conducted under the same memory budget. In the current context, TADEN uses both but does not justify this choice. To support its claims, there must be evidence that TADEN is efficient even under this setup, and such evidence should be organically connected to the main argument in the introduction. The ablation in Appendix E(parameters analysis) alone is insufficient to evaluate this point.\n- Limited generalization and restrictive assumptions in experiments. The reported improvements primarily appear in Atari tasks, where task similarity is visually distinguishable. However, Appendix D, Figure S4(b), shows that new tasks are almost perfectly isolated during the first epoch (first loop of six tasks). For instance, SpaceInvaders-1 exhibits performance gains only between 0M–10M and 60M–70M steps during the first epoch, while remaining stable otherwise.\n- The paper should analyze how well task similarity aligns with correct actor critic moduleretrieval, ideally by examining whether the retrieved model truly matches the underlying task context. It should also clarify whether the performance mainly depends on task retrieval accuracy or actual knowledge sharing.\n- Figure 4 shows that performance improvement occurs only at the points where each task is being trained during the repeated continual scenario (epochs). The authors should transparently report how close this performance is to the oracle architecture that uses true task IDs.\n\n[1] Rakelly, Kate, et al. \"Efficient off-policy meta-reinforcement learning via probabilistic context variables.\" International conference on machine learning. PMLR, 2019.\n\n[2] Zintgraf, Luisa, et al. \"Varibad: Variational bayes-adaptive deep rl via meta-learning.\" Journal of Machine Learning Research 22.289 (2021): 1-39.\n\n[3] Duan, Yan, et al. \"Rl $^ 2$: Fast reinforcement learning via slow reinforcement learning.\" arXiv preprint arXiv:1611.02779 (2016).\n\n[4]  Aljundi, Rahaf, Punarjay Chakravarty, and Tinne Tuytelaars. “Expert Gate: Lifelong Learning with a Network of Experts.” *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2017\n\n[5] Yoon, Jaehong, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. *“Lifelong Learning with Dynamically Expandable Networks.”* *International Conference on Learning Representations (ICLR)*, 2018."}, "questions": {"value": "- It appears that the evaluation uses an action head customized for each environment. In this case, the evaluator effectively knows which environment is being tested, which also indicates the corresponding task head. Could the authors clarify why the task ID is not explicitly used during evaluation?\n- Do common skills genuinely transfer across tasks? It would be helpful if the authors could provide supporting evidence, perhaps from the MiniHack experiments, demonstrating that meaningful knowledge sharing takes place.\n- TADEN uniquely collects additional RL trajectories solely for constructing its task embeddings(memory bank). Is it fair to compare its performance with other methods that do not perform such extra data collection?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "AAqBulOjMH", "forum": "aW4gMHN0GS", "replyto": "aW4gMHN0GS", "signatures": ["ICLR.cc/2026/Conference/Submission8698/Reviewer_mDhk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8698/Reviewer_mDhk"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760818528454, "cdate": 1760818528454, "tmdate": 1762920505494, "mdate": 1762920505494, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the Task-Aware Dynamic Expansion Network (TADEN), a continual learning framework that first estimates inter-task similarity from collected state representations and then uses this estimate to decide whether to expand model capacity. When expansion is warranted, TADEN leverages prior knowledge while preserving adaptability by initializing new modules with reused lower-layer parameters from existing modules. Empirical evaluations on the MiniHack and Atari benchmarks demonstrate the effectiveness of the approach."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe manuscript is clearly written and well structured.\n2.\tThe proposed method is easy to understand.\n3.\tThe experimental results are promising."}, "weaknesses": {"value": "1.\tThe current set of baselines appears outdated relative to the paper’s emphasis on architectural expansion. Given that TADEN is an expansion-based approach, the evaluation should foreground contemporaneous (2024–2025) capacity-growth and modular-expansion methods. Relying primarily on SANE (2022) and a 2025 MoE variant that combines EWC with MoE may not sufficiently represent the current state of the art.\n2.\tOn Atari, random initialization outperforms the proposed knowledge-transfer initialization, which calls into question the claimed benefits of transfer—the paper’s central motivation. What’s more, a more complete ablation would compare: (i) random initialization; (ii) initialization from the most similar existing module; (iii) initialization from a randomly chosen prior task/module. \n3.\tThe empirical difference between Reuse and TADEN requires deeper analysis, as their architectures differ only in the initialization of the top layer. If, as the authors suggest, the inferior performance of Reuse stems from “convergence restricting the model’s flexibility,” this could potentially be mitigated by tuning the learning rate schedule or employing adaptive optimizers. A systematic study of these factors would strengthen the causal interpretation.\n4.\tThe manuscript does not clearly articulate how previously acquired knowledge is maintained or utilized during subsequent training phases. It remains unclear whether any explicit regularization mechanisms are employed to prevent catastrophic forgetting, such as constraining the update magnitude of low-level feature representations.\n5.\tThe approach’s behavior when facing dissimilar or non-transferable tasks is insufficiently analyzed. In such scenarios, does the framework degenerate into training an independent network per task, with the memory bank serving merely as a task identifier?\n6.\tWhile the paper focuses on continual RL, the proposed components—namely the memory bank and model expansion—could also apply to general continual learning settings. It remains unclear whether any module is specifically tailored for the RL context."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gk9xdH4NaF", "forum": "aW4gMHN0GS", "replyto": "aW4gMHN0GS", "signatures": ["ICLR.cc/2026/Conference/Submission8698/Reviewer_z4qT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8698/Reviewer_z4qT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760936319900, "cdate": 1760936319900, "tmdate": 1762920504676, "mdate": 1762920504676, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TADEN, a **Task-Aware Dynamic Expansion Network** designed to alleviate catastrophic forgetting in Continual Reinforcement Learning (CRL). The key innovation is a task similarity-driven network expansion mechanism: the agent measures similarity between tasks using *Wasserstein distance* on environment state embeddings and decides whether to reuse existing actor–critic modules or create new ones. When expanding, dual-mode initialization reuses low-level features from the most similar prior module and randomly initializes higher layers for plasticity. TADEN is evaluated on MiniHack and Atari environments, outperforming major baselines like EWC, CLEAR, P&C, MoE, and SANE."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "* The task-aware expansion using Wasserstein distance between task-specific feature representations is a strong and well-motivated contribution.\n* Across both MiniHack and Atari benchmarks, TADEN achieves top performance in average reward and forgetting mitigation with fewer parameters than other architecture-based CRL methods.\n* The paper is well-organized, logically progressing from motivation to design and experimentation."}, "weaknesses": {"value": "* While the paper mentions dynamic expansion, the number of modules still grows with task diversity. This may become infeasible for long task sequences.\n* Using Wasserstein distance on visual embeddings may not generalize across tasks with abstract (non-visual) differences. Some theoretical justification or sensitivity analysis is missing.\n* Small spelling mistakes e.g. in Chapter 5 first sentence:  \"DATEN\"\n* Although major CRL methods are included, recent large-scale continual RL frameworks (e.g., Continual-World benchmarks) are absent. (not required, but maybe why a sentance or two on why it was not considered)"}, "questions": {"value": "Your method dynamically expands actor-critic modules based on task similarity, but as task diversity grows, the number of modules, and thus memory and compute, may become large.\n\nQ: Have you considered mechanisms to constrain or merge modules over time?\n\nQ: Do you have ideas for environments that require a very large task diversity?\n\nQ: How robust are Wasserstein distence metrics to non-visual or partially observable tasks, and have you compared it with alternative similarity measures (e.g. Feature-Clustering)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "KTcMhPzIO5", "forum": "aW4gMHN0GS", "replyto": "aW4gMHN0GS", "signatures": ["ICLR.cc/2026/Conference/Submission8698/Reviewer_KJZp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8698/Reviewer_KJZp"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8698/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761908157527, "cdate": 1761908157527, "tmdate": 1762920504183, "mdate": 1762920504183, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}