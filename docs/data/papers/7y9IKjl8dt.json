{"id": "7y9IKjl8dt", "number": 14538, "cdate": 1758238391090, "mdate": 1763713550776, "content": {"title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning", "abstract": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK--a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision—achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4\\% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9\\%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.", "tldr": "We train process reward models without ground truth by aggregating multiple verification attempts through inference-time scaling, achieving better performance than ground-truth-based approaches.", "keywords": ["Process Reward Models", "Inference-time Scaling", "Reference-free Reinforcement Learning", "Mathematical Reasoning", "Synthetic Verification"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2f53187b981a135df7d705faf7bc3f330c01d85.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SPARK, a three-stage framework to train generative process reward models (PRMs) for reinforcement learning without requiring any ground-truth references or human annotations. In Stage I, it generates a synthetic dataset by using a generator model to create solutions and a more powerful verifier model to evaluate them using inference-time scaling techniques like self-consistency and meta-critique. In Stage II, this synthetic data is used to fine-tune generative PRMs. In Stage III, this reference-free PRM-CoT is used as the reward signal to train a policy model via RL, achieving state-of-the-art results on math reasoning benchmarks, even outperforming baselines trained with ground-truth outcome verification."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The primary strength of this work is its novel and effective framework for training process reward models (PRMs) without access to ground-truth references. The reliance on expensive, step-level human annotations or gold solutions is a major bottleneck for scaling process-based feedback, and this paper offers a viable, reference-free alternative. The core idea of using inference-time scaling methods (like self-consistency and meta-critique) to generate high-quality synthetic verification data is a solid contribution.\n\nThe empirical results for the PRM itself are strong. The paper shows that a PRM trained on this synthetically generated data (specifically using step-level consistency) achieves a 67.5 F1 on ProcessBench. This result is impressive not only because it outperforms strong LLM critics like GPT-4o, but also because it surpasses a baseline PRM trained with access to ground-truth outcomes, which suggests that aggregating multiple noisy, reference-free process verifications has the potential to provide a richer training signal than verifying against a single ground-truth final answer.\n\nThe paper also demonstrates the downstream utility of this reference-free PRM in a practical RL setting and also provides a valuable analysis of reward hacking patterns, and introduces format constraints to mitigate them."}, "weaknesses": {"value": "There is a mismatch between the paper's core motivation and its experimental validation. The method is motivated as a solution for domains where ground truth is \"unavailable,\" \"subjective,\" or \"lacks clear verification criteria,\" such as creative writing or complex planning. However, all experiments are conducted exclusively on mathematical reasoning, a domain defined by objective, verifiable ground truth. \n\nThe computational cost of the Stage 1 synthetic data generation pipeline appears to be enormous and is not analyzed. To generate the \"Step Consistency\" dataset, the framework must run $N=16$ independent verifications for each of the $M=8$ solutions generated for each problem. This implies over 100 verifier passes per problem just to create a single training data point. This massive offline inference cost is not compared against the cost of alternative methods."}, "questions": {"value": "The paper opts for a generative PRM (Gen-PRM) over a discriminative one (Disc-PRM). However, the paper dedicate substantial analysis to reward hacking issues (e.g., solution appending, step inflation) that are unique to this generative approach . Given that Gen-PRMs introduce these complex new failure modes, what is their fundamental advantage over simpler discriminative PRMs that justifies this trade-off?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "The paper does not appear to raise any ethical concerns. It does not involve human subjects, sensitive data, or potentially harmful applications."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "E3BNCYA1t7", "forum": "7y9IKjl8dt", "replyto": "7y9IKjl8dt", "signatures": ["ICLR.cc/2026/Conference/Submission14538/Reviewer_QmP1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14538/Reviewer_QmP1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761498165603, "cdate": 1761498165603, "tmdate": 1762924929178, "mdate": 1762924929178, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces SPARK, a framework to train process reward models (PRMs) for reinforcement learning (RL) entirely without ground-truth references. The method uses a \"generator-verifier\" system with inference-time scaling (like self-consistency and meta-critique) to create high-quality synthetic step-level verification data. This data is then used to train a generative PRM (PRM-CoT), which subsequently provides reward signals for RL training. SPARK enables reference-free training that outperforms ground-truth methods. On the ProcessBench evaluation, the SPARK-trained PRM achieved 67.5 F1, surpassing the reference-guided (ground-truth) model's 66.4 F1. In RL experiments, SPARK's PRM-CoT led to 47.4% average accuracy, exceeding the ground-truth-based RLVR's 43.9%."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The design of SPARK is intuitive to me.\n\n1. Instead of relying on a static, expensive ground-truth dataset, SPARK uses a dynamic generator-verifier framework. It leverages inference-time scaling techniques (like self-consistency and meta-critique) to aggregate multiple verification attempts, effectively bootstrapping a high-quality, step-level training dataset from the model's own reasoning capabilities.\n\n1. When used in RL training, SPARK's generative PRM enables the policy model to achieve 47.4% average accuracy on math benchmarks. This result exceeds the performance of the ground-truth-based method, RLVR, which achieved 43.9%.\n\n1. This paper also systematically analyzes reward hacking patterns in process reward-based RL."}, "weaknesses": {"value": "1. The method is motivated by the need to apply RL to subjective domains without ground truth (e.g., creative writing, ethical reasoning). However, all experiments are conducted exclusively in mathematical reasoning, a domain where objective ground truth does exist. This creates a mismatch between the problem the method claims to solve and the domain in which it is actually validated.\n\n1. The paper provides a systematic analysis of reward hacking patterns. However, the identified patterns (e.g., solution appending, step inflation, and step reduction) and their solutions are specific to the highly structured format of mathematical problem-solving, which also deviates from the motivation of applying RL to subjective domains without ground truth. The experimental design does not demonstrate whether these findings or solutions are transferable to the unstructured, open-ended tasks that are the method's ultimate target.\n\n1. The contribution of this paper is marginal. To my understanding, SPARK just combines self-consistency and meta-critique for auto annotation. Besides, there is no quality evaluation to show how accurate the SPARK-generated annotations are."}, "questions": {"value": "1. As mentioned in Weaknesses, how can SPARK be reliably generalized to the very domains the authors use for motivation (like creative writing or ethical reasoning), where an objective verifier doesn't exist and the verifier's critique is just as subjective and unverifiable as the generator's output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UGIiqJOc2C", "forum": "7y9IKjl8dt", "replyto": "7y9IKjl8dt", "signatures": ["ICLR.cc/2026/Conference/Submission14538/Reviewer_9QxJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14538/Reviewer_9QxJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970433927, "cdate": 1761970433927, "tmdate": 1762924928710, "mdate": 1762924928710, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a new method for generating a verification dataset for training PRMs based on scaling a step-wise verification process parallely and sequentially. This synthetic dataset, which comprises step-level judgment with rationales, is used to train these PRMs that later on serves as reward models for RL training. The work presents evaluation on two setups: first, on ProcessBench, claiming that the proposed method for PRM training surpasses GPT-4o and reference-guided verification; and second, on RL training, claiming a performance that matches or exceeds ground truth performance."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The method is conceptually simple and the paper is easy to follow.\n\n- The comparison against GPT-4o and Reference-Guided verification in ProcessBench suggests that the employed methodology is promising, since it does not rely on ground-truth nor on a frontier model."}, "weaknesses": {"value": "- The main concern is the lack of evidence to assess statistical significance in the results. The paper does not mention how many experimental seeds were used (I assume it is a single one), and no results in the paper brings confidence intervals. A well known fact supported by prior literature is that RL training is extremely stochastic [1, 2], which is also observed in math reasoning benchmarking [3], so it is unclear whether the reported takeaways are meaningful or just observation noise. This is particularly necessary to the RL training results (Figs 4-7 at least) but also necessary for the PRM training, as different seeds may lead to different verification performances.\n\n- As the paper states in Section 4.4, one of the limitations from the developed PRMs is that they are still vulnerable to reward hacking, which is one of the main reasons why RLVR is still the standard choice over PRMs. The reward hacking issue is very much present in the “Step-Augmented Process Rewards”, which is the method that strongly relied on step-level rewards.\n\n- The evaluation methodology in Figure 5 is flawed. The Figure highlights a 7.6% difference between PRM-CoT and the other methods, but ignores the fact that prior checkpoints present considerably better performance for ORM. Besides the issue of reporting a single seed, the work also does not provide a methodology on checkpoint selection, and the evaluation on t = 300 is arbitrary. \n\n- The paper does not bring a comparison of the computational cost involved in (1) generating the training dataset; and, more importantly, (2) the cost involved in performing inference in the trained PRMs. From the paper, it is unclear if the proposed PRMs use variable test-time computation, and it would be extremely important to ensure fairness in computation during inference.\n\n- There are other methods to train RL without verifiable rewards, e.g., [4]. It would be nice to compare against them.\n\nOverall, while I see the method as a simple yet interesting direction for generating PRM training datasets, the experimental methodology of the paper is currently flawed which makes the provided evidence weak/questionable. I also believe the main claims of “enabling RL to scale beyond verifiable domains” is somewhat too strong and not supported, especially given that PRMs in general (including the ones in this work) are generally vulnerable to reward hacking, and the proposed method does not address this problem."}, "questions": {"value": "- During verifier inference, are the inference scaling methods also used? Or are they used solely during dataset generation?\n\n- The paper mentions that the datasets contain 63k examples after filtering. Which filtering is that?\n\n\nReferences\n\n\n[1[ Henderson et. al. Deep Reinforcement Learning that Matters. AAAI, 2018.\n\n[2] Agarwal et. al. Deep Reinforcement Learning at the Edge of the Statistical Precipice. NeurIPS, 2021.\n\n[3] Hochlehnert et. al. A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility. COLM, 2025. \n\n[4] Zhao et. al. Learning to Reason without External Rewards, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "87ws3DfBoV", "forum": "7y9IKjl8dt", "replyto": "7y9IKjl8dt", "signatures": ["ICLR.cc/2026/Conference/Submission14538/Reviewer_YhFA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14538/Reviewer_YhFA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14538/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206186816, "cdate": 1762206186816, "tmdate": 1762924928309, "mdate": 1762924928309, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}