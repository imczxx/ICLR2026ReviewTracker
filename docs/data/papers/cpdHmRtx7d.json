{"id": "cpdHmRtx7d", "number": 4118, "cdate": 1757604941988, "mdate": 1759898051901, "content": {"title": "One step further with Monte-Carlo sampler to guide diffusion better", "abstract": "Stochastic differential equation (SDE)-based generative models have achieved\nsubstantial progress in conditional generation via training-free differentiable\nloss-guided approaches. However, existing methodologies utilizing posterior sam-\npling typically confront a substantial estimation error, which results in inaccurate\ngradients for guidance and leading to inconsistent generation results. To mitigate\nthis issue, we propose that performing an additional backward denoising step and\nMonte-Carlo sampling (ABMS) can achieve better guided diffusion, which is a\nplug-and-play adjustment strategy. To verify the effectiveness of our method, we\nprovide theoretical analysis and propose the adoption of a dual-evaluation frame-\nwork, which further serves to highlight the critical problem of cross-condition\ninterference prevalent in existing approaches. We conduct experiments across var-\nious task settings and data types, mainly including conditional online handwritten\ntrajectory generation, image inverse problems (inpainting, super resolution and\ngaussian deblurring), and molecular inverse design. Experimental results demon-\nstrate that our approach consistently improves the quality of generation samples\nacross all the different scenarios.", "tldr": "", "keywords": ["Conditional Generation; Diffusion Model; Training-free Guidance"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e7a5a5a4b13247dd099a783b0327c7b5a0427ecc.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes ABMS for solving inverse problems using diffusion models.\nIt identifies and addresses errors arising from the imprecise gradient guidance in a prominent baseline method, DPS.\nTo resolve this, ABMS computes the gradient guidance by drawing multiple samples at each denoising step and using the diffusion model's predictions for those samples.\nThe proposed method shows enhanced performance on a variety of tasks, such as character generation, image restoration, and monocular property prediction."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- It highlights that many existing training-free inverse problem methodologies including DPS rely on numerous assumptions and approximations, which often leads to suboptimal results.\n\n- It makes a significant contribution by demonstrating the trade-off inherent in using gradient guidance through a dual-focus evaluation."}, "weaknesses": {"value": "- There are concerns regarding the practical applicability of the proposed method. ABMS is computationally heavy as it requires M diffusion model operations at each step, and 1000 sampling steps.\n\n- Most of the demonstrated tasks have limited practicality; for instance, the inpainting task only uses very small masks instead of large ones.\n\n- The results are shown on pixel-space diffusion models. However, state-of-the-art diffusion models like Stable Diffusion or Flux-dev operate in latent space. Including results on these models would broaden the paper's scope."}, "questions": {"value": "Is the proposed methodology applicable to recent state-of-the-art diffusion models such as Flux-dev?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WSmP4DhRsZ", "forum": "cpdHmRtx7d", "replyto": "cpdHmRtx7d", "signatures": ["ICLR.cc/2026/Conference/Submission4118/Reviewer_Daim"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4118/Reviewer_Daim"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760816225651, "cdate": 1760816225651, "tmdate": 1762917184847, "mdate": 1762917184847, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a training-free guidance method to improve the inverse problem in diffusion models. The authors argue that the existing method, DPS, suffers from a biased gradient. To mitigate this, ABMS leverages the gradient of the averaged multiple backward predictions of diffusion models. With theoretical justification, ABMS shows improved results compared to existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-Motivated and Simple Solution**: The proposed ABMS method is intuitive, well-motivated by the law of total expectation, and directly targets the identified source of bias (single-point estimation). It seems to be easily adapted to the existing codebase due to its simplicity and can be widely utilized as it does not require any additional conditions.\n\n- **Comprehensive Experiments**: The method's effectiveness is demonstrated across three different domains. The consistent improvements across all tasks support the claims.\n\n- **Theoretical Analysis**: The estimation error analysis showing that ABMS's error bound under the given assumptions avoids the $\\delta_{f}(x_t)$, which plagues the DPS bound, provides a theoretical justification for the effectiveness of the ABMS."}, "weaknesses": {"value": "- **Computational Overhead**: The most significant weakness is the increased computational cost. ABMS requires $M$ denoising network evaluations in addition to the original denoising steps. While it can be parallelized, the memory consumption can grow rapidly as it also requires additional gradient calculations. For a more comprehensive analysis, the additional computational time and memory consumption for ABMS should be reported.\n\n- **Novelty in Context**: The idea of using Monte Carlo sampling to get better estimates in diffusion guidance is not entirely new (e.g., LGD-MC). The paper's novelty lies in the one-step-back formulation. The paper could be stronger if it more directly compared against a simpler MC-DPS baseline (i.e., averaging $M$ estimates from $x_t$, not $x_{t-1}$) to isolate the benefit of the backward step from the benefit of MC sampling. Furthermore, while the authors argue that LGD-MC incurs high computational costs, the proposed ABMS is a more computationally intensive method as it requires multiple diffusion model calls with gradient calculation, while LGD-MC requires only one diffusion model call.\n\n- **Limited Scope of Samplers**: While the authors acknowledge this limitation, it is unclear how ABMS, which relies on the SDE-based one-step transition, would be adapted to faster ODE-based or higher-order solvers (like DPM-Solver++, etc.) that are now state-of-the-art for fast sampling. This potentially limits the method's practical application."}, "questions": {"value": "- **Clarification on Computational Cost**: Please refer to Weakness 1.\n\n- **Ablation Study Without DSG**: ABMS opts for the DSG for the stability of the algorithm. It could be helpful if the base performance of ABMS without DSG is analyzed compared to DPS."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethical concerns are raised."}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "oovp7ezQty", "forum": "cpdHmRtx7d", "replyto": "cpdHmRtx7d", "signatures": ["ICLR.cc/2026/Conference/Submission4118/Reviewer_BtcN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4118/Reviewer_BtcN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761704700615, "cdate": 1761704700615, "tmdate": 1762917184523, "mdate": 1762917184523, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets bias in training-free diffusion posterior sampling (DPS) for conditional generation and inverse problems. DPS approximates the posterior p(x0∣y)∝p(x0)exp⁡[−L(x0;y)]p(x_0|y) through two linearizations: (1) moving the expectation inside the loss, and (2) replacing the conditional mean with the denoiser output via Tweedie’s formula. The authors argue these lead to biased gradients and propose ABMS (Additional Backward step with Monte-Carlo Sampling): before estimating guidance at step ttt, it samples xt−1∼ p(xt−1∣xt) multiple times, evaluates the loss on each denoised x^0(xt−1), and averages the results to reduce bias. A dual-focus evaluation (alignment vs. global quality) is introduced. Experiments on classifier-guided digit synthesis, image inverse problems (SR, inpainting, deblurring), and molecular property conditioning show modest but consistent gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem—bias in loss-guided diffusion—is relevant and well-motivated.\n\nThe proposed method (MC sampling one step earlier) is simple, plug-and-play, and compatible with existing samplers. Also parallelization of MC sampler justifies the added computations. \n\nEvaluation across multiple domains, with clear quantitative metrics."}, "weaknesses": {"value": "The “unbiased” claim is overstated: ABMS still produces a lower-bias approximation but not a provably unbiased gradient of the tilted posterior. No formal unbiasedness proof or convergence result is provided.\n\nThe theoretical bound (Sec. 4.2) only compares error upper bounds under strong assumptions (Lipschitz fff, monotone denoiser accuracy). Variance of the stochastic gradient is unaddressed.\n\nScope limited to DPS. Extensions to other plug-and-play or variational samplers (e.g., RED-Diff, flow-matching, ODE solvers) are not discussed experimentally.\n\nThe empirical improvements, though consistent, are incremental; figures often lack statistical significance or ablations isolating the MC vs. scaling effects."}, "questions": {"value": "Can the authors clarify whether the proposed MC sampling yields an unbiased estimator of gradients? If not, what assumptions make the bias negligible?\n\nDoes the ABMS correction still hold under deterministic DDIM/flow samplers?\n\nHow does the method compare to re-scoring or re-noise-based variance-reduction schemes such as RED-Diff or Score-DPO?\n\nAre there ablations showing the effect of the hypersphere projection alone versus MC averaging alone?\n\nFor molecular tasks, how sensitive is performance to the sampling count MMM?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UE3Ug4GuwV", "forum": "cpdHmRtx7d", "replyto": "cpdHmRtx7d", "signatures": ["ICLR.cc/2026/Conference/Submission4118/Reviewer_93q3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4118/Reviewer_93q3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762127130153, "cdate": 1762127130153, "tmdate": 1762917184322, "mdate": 1762917184322, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical and widely recognized problem in training-free guided diffusion models: the degradation of sample quality (e.g., FID) under strong conditional guidance. The authors convincingly argue that this issue stems from a systematic estimation error and bias in the guidance gradient, which is typically derived from a single, noisy denoising step. To mitigate this, they propose ABMS, a plug-and-play Monte-Carlo sampling strategy. At each reverse diffusion step, instead of relying on a single estimation path, ABMS explores multiple (M) potential predecessor states, denoises each one to predict a clean output, and then averages the guidance function evaluations across these M predictions. This Monte-Carlo averaging yields a more stable and accurate estimate of the true guidance gradient. The paper provides a theoretical justification (Proposition 1) showing that ABMS achieves a lower estimation error bound compared to the standard DPS approach. The effectiveness of ABMS is demonstrated empirically across a diverse set of tasks, including stylized handwriting generation, standard image inverse problems (inpainting, super-resolution, deblurring) on ImageNet, and molecular property prediction. The results consistently show that ABMS allows for stronger guidance without the typical collapse in sample quality, outperforming existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper tackles a fundamental challenge in guided diffusion. The \"dual-focus evaluation\" paradigm, which explicitly calls for balancing task-specific performance (e.g., reconstruction error) with general sample quality (e.g., FID), is an excellent and necessary framing for this problem area. The authors clearly articulate why strong guidance often leads to poor results, providing strong motivation for their work.\n2. The proposed ABMS method is simple, well-motivated, and elegant. The core idea of using Monte-Carlo averaging to obtain a more robust estimate of an expectation is a classic statistical principle applied very effectively here. Its \"plug-and-play\" nature makes it broadly applicable to various diffusion frameworks and tasks without requiring model retraining, which is a significant practical advantage.\n2.1 While not a deep theoretical paper, the inclusion of the estimation error analysis and Proposition 1 provides a solid theoretical grounding for why ABMS should be expected to work better than single-path estimators. It connects the intuitive idea of averaging to the mathematical problem of reducing the bias introduced by Jensen's inequality.\n\n3. The experimental section is a major strength of this paper.\n  - Diversity of Tasks: Testing the method on stylized text, natural images, and molecular data convincingly demonstrates its generality.\n  - Rigorous Evaluation: The use of performance curves (Figure 3) that plot task-specific distance against FID is particularly effective. These plots provide a clear and powerful visualization of the core contribution, showing that ABMS dominates other methods by achieving a better trade-off frontier."}, "weaknesses": {"value": "1. The primary drawback of ABMS is the increased computational cost, which scales with the number of Monte-Carlo samples, `M`. The paper demonstrates the effectiveness of `M=3` and `M=5` but never explicitly analyzes or reports the trade-off between performance and inference time/FLOPs. For a sampling method, this performance-cost analysis is crucial for researchers and practitioners to assess its viability. While Figure 3 implicitly shows the performance gain for different `M`, the associated cost is not quantified.\n2. While the use of ImageNet 256x256 is a standard and respectable benchmark, the field of generative models is rapidly moving towards much higher resolutions (512x512, 1024x1024) and significantly larger models (e.g., Stable Diffusion). The paper does not demonstrate whether the benefits of ABMS hold or are even more critical at this larger scale, where guidance is often essential. Demonstrating scalability to at least one high-resolution setting would substantially increase the paper's impact.\n3. The paper would benefit from a more detailed discussion of how ABMS relates to other methods that also aim to improve guided sampling by investing more computation. A key missing comparison is with \"Restart Sampling\"[1]. Both methods address quality degradation under strong guidance but seem to operate on different principles. A discussion clarifying these differences would better situate the paper's contribution within the current literature.\n\nRefrence:\n[1] Xu, Yilun, et al. \"Restart sampling for improving generative processes.\" Advances in Neural Information Processing Systems 36 (2023): 76806-76838."}, "questions": {"value": "See the weakness.\nWhile the Weakness.3 is the most important, which affects the innovation of this paper, with an excellent explanation, I will increase the score, while a bad explanation, I will decrease the score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4IHDCT1lEZ", "forum": "cpdHmRtx7d", "replyto": "cpdHmRtx7d", "signatures": ["ICLR.cc/2026/Conference/Submission4118/Reviewer_vUip"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4118/Reviewer_vUip"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4118/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762834859081, "cdate": 1762834859081, "tmdate": 1762917184093, "mdate": 1762917184093, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}