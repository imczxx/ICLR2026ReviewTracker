{"id": "Td9Oi4Hhwg", "number": 12980, "cdate": 1758212401195, "mdate": 1759897472788, "content": {"title": "Is Privacy Always Prioritized over Learning? Probing LLMs' Value Priority Belief under External Perturbations", "abstract": "The value alignment of Large Language Models (LLMs) is critical because value is the foundation of LLM decision-making and behavior. Some recent work show that LLMs have similar value rankings. However, little is known about how susceptible LLM value rankings are to external influence and how different values are correlated with each other. In this work, we investigate the plasticity of LLM value systems by examining how their value rankings are influenced by different prompting strategies and exploring the intrinsic relationships between values. To this end, we design 6 different value transformation prompting methods including direct instruction, rubrics, in-context learning, scenario, persuasion, and persona, and benchmark the effectiveness of these methods on 3 different families and totally 8 LLMs. Our main findings include that the value rankings in large LLMs are much more susceptible to external influence than small LLMs, and there are intrinsic correlations between certain values (e.g., Privacy and Respect). Besides, through detailed correlation analysis, we find that the value correlations are more similar between large LLMs of different families than small LLMs of the same family. We also identify that scenario method is the strongest persuader and can help entrench the value rankings.", "tldr": "", "keywords": ["LLM", "Benchmark", "Evaluation", "Psychometrics", "Value", "Alignment"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4151cfe54f4b12a313e319f3cf99f97e5b91f6d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how large language models’ (LLMs) value rankings can be influenced through different prompting strategies. The authors experiment with six prompting methods to examine how emphasizing or reducing specific values affects a model’s internal value hierarchy, which is derived using pairwise “value battles.” The study evaluates these value rankings using the dataset and methodology introduced in LitmusValues (Chiu et al., 2025). The results show that the “Scenario” prompting method notably shifts a model’s default value orientation, with larger models exhibiting greater sensitivity to prompt-based value changes. Additionally, the results show correlations among different values, suggesting interconnected value structures."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "1. This paper explores how LLMs can change their value rankings given steering prompts. \n2. This paper shows that the steerability of values depends on the input prompt."}, "weaknesses": {"value": "# 1. Novelty\n- While it is encouraged to build upon prior work, a huge portion of this paper closely overlaps with or is derived from the following studies [1,2]. The Elo rating system and dataset are from [1], and even the figure design appears highly similar (e.g., Figure 5 from [1] and Figure 4 from this paper). The primary novel contribution of this paper is in introducing six prompt techniques to modify LLMs’ value rankings.\n\n- One of the main findings of this paper is that models of similar sizes exhibit similar value ranking patterns. However, as noted in lines 430–431, a similar pattern has already been observed in prior work [3], which diminishes the novelty of the current study.\n\n[1] Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, and Evan Hubinger. Will ai tell lies to save sick children? litmus-testing ai values prioritization with airiskdilemmas. arXiv preprint 2025.  \n[2] Yu Ying Chiu, Liwei Jiang, and Yejin Choi. Dailydilemmas: Revealing value preferences of llms with quandaries of daily life. In The Thirteenth International Conference on Learning Representations, 2025.  \n[3] Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. The platonic representation hypothesis. In The International Conference on Machine Learning, 2024.\n\n\n# 2. Conceptual and Interpretive Issues\n- The claim that larger models’ values are more susceptible to manipulation (lines 82–84) appears overstated. The ability to adjust value priorities through prompting is not inherently problematic; rather, it reflects proper instruction following.\n\n- The interpretation of Figure 5 (lines 355–357) and Figure 7 (lines 404–409) lacks sufficient justification or explanation. For Figure 5, further explanation is needed why the Persona method is better than the Scenario method in the “Reduce” configuration. For Figure 7, the high correlation observed within the two value sets may reflect two distinct concepts: (Privacy, Justice, Respect, Truth, Freedom) as “moral principles” and (Adaptability, Creativity, Care, Cooperation, Learning, Sustainability, Wisdom) as “growth-oriented” values. A discussion of this possibility would be beneficial.\n\n- The experiment addressing Research Question 3 (value entrenchment) does not convincingly support the stated conclusions. While the intention to demonstrate the usefulness of the Scenario prompting method is reasonable, to validly support the claim that this method can be used for “value entrenchment”, additional baseline experiments should be conducted using existing steering approaches, such as activation engineering methods.\n\n\n# 3. Empirical and Analytical Limitations\n- The analysis of Figure 4 appears selectively interpreted. The authors generalize from a single sample case (lines 347–349), overlooking internal inconsistencies—for example, GPT-4.1-nano shows larger rank shifts within the same value dimension (adaptability).\n\n\n- When presenting correlation results, it would be better if the corresponding p-values are also shared.\n\n- Figure 9 lacks a clear explanation of the y-axis and is difficult to interpret.\n\n\n\n# 4. Clarity and Presentation\n- Figure 4 is visually difficult to interpret. Improving the design (e.g., clearer labeling, consistent color mapping) would enhance readability. Also, there is a typo in the name of the model (e.g., LLaMA3-7B). Figure 9 also requires visual and explanatory refinement.\n\n\n- The interpretation sections would benefit from concise summaries and better linkage between visualizations and textual analysis.\n\n\n# 5. Writing and Formatting Issues\n- Multiple typographical and grammatical errors hinder readability (e.g., line 71: “qustions”, line 341: “nder”).\n\n\n- There are typos, unpunctuated sentences, and incomplete sentences (e.g., lines 128, 229, 356–357).\n\n- Multiple references to the following paper: “Do llms have consistent values?”. Citing only a single version of the paper is recommended (cite the ICLR 2025 version).\n\n- It’s acceptable to cite the arXiv version of a paper, but if the paper has been officially published, it would be better to use the BibTeX entry of the published version instead.\n\n- Although the appendix serves as supplementary material, there are several citation errors (e.g., lines 987 and 991). In addition, there are multiple instances where citep is used instead of citet (e.g., line 1041).\n\n- The caption of Table 9 is below the table."}, "questions": {"value": "1. Would it not be desirable for larger models to exhibit a greater ability to adjust their value orientations in response to given instructions? Given that larger models typically possess stronger instruction-following capabilities, shifts in value rankings based on input prompts are a natural phenomenon. Moreover, such adaptability would be advantageous in applications like Persona Prompting (e.g., assigning specific personas to agents).\n\n2. Do you believe that Research Question 3 is sufficiently supported by the experimental results presented in Figure 9?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "lF3PAb7M9x", "forum": "Td9Oi4Hhwg", "replyto": "Td9Oi4Hhwg", "signatures": ["ICLR.cc/2026/Conference/Submission12980/Reviewer_o4ni"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12980/Reviewer_o4ni"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761468731862, "cdate": 1761468731862, "tmdate": 1762923735490, "mdate": 1762923735490, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the plasticity and robustness of LLM value systems. Specifically, the authors examine how susceptible LLM value rankings are to external perturbations introduced via different prompting methodologies (e.g., direct instruction, persuasion, persona-based prompts). The study systematically explores the resulting changes in value correlation and the overall extent of value ranking manipulation across various model scales. The key findings demonstrate a non-trivial relationship between model scale and both the inherent correlation among values and the manipulability (plasticity) of the value hierarchy under external influence."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "(1) The core research question—the degree to which LLM value rankings can be altered or influenced by various prompting strategies—is significant. \n\n(2) The work provides valuable empirical evidence regarding the relationship between model scale and the intrinsic correlation among different values. \n\n(3) The paper establishes an experimental relationship between model scale and the manipulability (plasticity) of the LLM's value ranking."}, "weaknesses": {"value": "(1) The analysis of value correlation changes, while present, appears to overlook deeper insights. Specifically, previous work (e.g., Kang et al., 2025) has explored correlations that transcend simple lexical semantics of the value terms themselves, revealing more profound, structural relationships within the LLM's value space. This paper did not ablate these lexical correlations.\n\n(2) The study relies on six distinct prompting methods to perturb the value rankings. However, the manuscript does not adequately justify why these six methods are sufficiently representative of the entire possible space of value-based prompts. If there exist other common or powerful forms of value prompting that are not covered, the conclusion that the observed manipulability accurately represents the model's general resistance or plasticity could be weakened. \n\n(3) The current evaluation is exclusively performed on a value dilemma dataset. While this setup is crucial for measuring ethical conflict resolution, the conclusions drawn about LLM value plasticity and robustness might not fully generalize to other forms of value-related tasks. Testing on a wider range of ethical judgment tasks—such as value-laden generation, ethical story completion, or direct value assessment without a forced conflict scenario—would significantly enhance the robustness and applicability of the findings."}, "questions": {"value": "In Line 69 (based on the reviewer's reference), the authors mention that LLMs must \"persist some value rankings, like it must obey human orders\". This creates a conceptual conflict with the entire experimental setup: \n\nThe paper's finding that LLMs' value rankings can be altered by human instructions, from the perspective of value alignment, is this fundamentally a desired feature (e.g., enabling contextualization or persona adoption) or a potential vulnerability (e.g., susceptibility to malicious or accidental manipulation)? Given that human instructions themselves are often intended to change the LLM's value hierarchy, the authors should clarify the tension between this plasticity and the model's fundamental meta-value of 'obeying instructions.'"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "T3uVF4KIme", "forum": "Td9Oi4Hhwg", "replyto": "Td9Oi4Hhwg", "signatures": ["ICLR.cc/2026/Conference/Submission12980/Reviewer_Md6f"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12980/Reviewer_Md6f"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761824686993, "cdate": 1761824686993, "tmdate": 1762923734471, "mdate": 1762923734471, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Though existing works have shown that LLMs have similar value rankings, few studied how LLMs’ value rankings are influenced by different prompts, but the persistence of value rankings within LLMs is crucial under some scenarios. \n\nInspired by this, this paper studies the following question: How LLMs’ value rankings are influenced by different prompts? What is the relationship between different values? How to entrench LLM values with prompt settings?\n\nThe authors design 6 different value transformation prompting method to study this question. With experiments on 3 different families and totally 8 LLMs, they present 5 main findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The task of studying the stability of value rankings within LLMs is important.\n2. To investigate this task, this paper proposes 6 different prompting strategies.\n3. Five findings are empirically discovered."}, "weaknesses": {"value": "1. The evaluation excludes recently released and more advanced reasoning models (e.g., OpenAI o-series, GPT-5, DeepSeek). Including such models would strengthen the conclusions and improve the paper’s relevance.\n2. Some experimental settings should be clarified.\n- The rationale for choosing the 16 value categories is unclear. Theoretical foundations, definitions, and interrelationships among these categories should be explicitly described.\n- This paper utilizes Elo Rating score as the metric to obtain the relative ranks of all value dimensions. However, it is computed on local pairwise value battles. How to transfer such local battle score into the global rankings across all 16 values should be clarified.\nBesides, different samples in the evaluation dilemma dataset involve different value dimensions, which could impact the computation of Elo-Rating score. You should clarify the distribution and statistics of the evaluation dataset. A biased dataset is hard to compare all values fairly.\n- Each dilemma could involve either two or more value dimensions. If it reflects more than two values, how to compute the Elo rating score for each dimension?\n3. Scenario-based prompting achieves the strongest manipulation effect, but constructing such prompts (e.g., jailbreak-like setups) can be non-trivial. The algorithmic or procedural approach for generating these scenarios should be explained.\n4. For value correlation, you mainly analyze the relation among LLMs, how about the changing tendency and relationship between value dimensions? Is the correlation explainable?\n5. Generalizability to more value dimensions and evaluation datasets would be better."}, "questions": {"value": "1. There are still some typos.\n- line 340, “finegrained” –> fine-grained ?\n- Line 341, “four models nder various promoting methods …”\n- Line 172, “reflecting its aggregate importance…”"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "NEykB6CUZ0", "forum": "Td9Oi4Hhwg", "replyto": "Td9Oi4Hhwg", "signatures": ["ICLR.cc/2026/Conference/Submission12980/Reviewer_uzNs"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12980/Reviewer_uzNs"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761896787708, "cdate": 1761896787708, "tmdate": 1762923733783, "mdate": 1762923733783, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies how operational value priorities of LLMs change under different prompt “perturbations.” Building on LitmusValues/AIRiskDilemmas, the authors compute Elo rankings over 16 values from pairwise “value battles,” then apply six value‑steering prompt families—Direct instruction, Rubrics, In‑Context Learning (ICL), Scenario, Persuasion, and Persona—to see how those rankings move across eight models (GPT‑4.1 family; Llama‑3 8B/70B; Qwen 2.5 7B/32B/72B). Major empirical claims are: (1) scenario prompts are the strongest “persuaders” (largest ΔRank/ΔElo), (2) larger models show greater plasticity than smaller ones, (3) some values co‑move (e.g., Privacy with Respect), and (4) value‑correlation structures become more similar across families as model size increases. The paper also explores “entrenchment”: preconditioning with scenarios to make later persona prompts less able to move rankings."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. **Entrenchment experiment**. The two‑stage “scenario→persona” setup (Fig. 9, p. 9) is a nice touch to test whether pre‑context can harden downstream behavior.\n2. **Correlation view of values**. Treating values as an interdependent system rather than isolated dimensions is a good instinct; the correlation heatmaps (Fig. 7, p. 8) and matrix‑distance comparison (Fig. 8, p. 8) are helpful visual summaries."}, "weaknesses": {"value": "1. **Writing/clarity & presentation**: Several passages are hard to parse or appear unfinished (p. 3, Sec. 3.1). Figure/table presentation also needs work (e.g., caption too close to the table, p. 4 Table. 1). \n2. The criteria for constructing the prompts are underspecified.\n3. **Novelty and framing**: Much of the pipeline—dataset (AIRiskDilemmas), evaluation (pairwise Elo), and even some motivation—closely follows prior work, with the new element primarily a set of prompt wrappers around the same evaluation. \n4. The title foregrounds Privacy vs. Learning, but the body treats 16 values uniformly; if Privacy/Learning is a special case, the paper should analyze it directly (e.g., targeted ablations), or retitle to match scope.\n5. **Central result may conflate instruction-following with “values.”** Larger models’ greater ΔRank could simply reflect stronger instruction‑following, not deeper value plasticity. A manipulation check is missing (e.g., probe stated preferences, free‑form rationales, or refusal rates alongside the forced choice).\n6. The conclusion that “model scale, rather than family lineage, drives value‑correlation alignment,” and its tie‑in to the Platonic Representation Hypothesis (Fig. 8, p. 8) are suggestive but **not rigorous**: no statistical tests are tested."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "HFcddmmlb5", "forum": "Td9Oi4Hhwg", "replyto": "Td9Oi4Hhwg", "signatures": ["ICLR.cc/2026/Conference/Submission12980/Reviewer_KBYh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12980/Reviewer_KBYh"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission12980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981304517, "cdate": 1761981304517, "tmdate": 1762923733339, "mdate": 1762923733339, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors test several prompt variations to see whether they change LLM behavior on the LitmusValues benchmark, a set of hypothetical ethical dilemmas intended to measure “value” patterns in LLM behavior. They develop a taxonomy of different prompting strategies and show that these strategies can be used to manipulate “value” patterns in LLM responses to ethical dilemmas. They also find apparent correlations between prompt effectiveness and model size and similarities in inter-value correlations between LLM models."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "*Quality*: I like the various jailbreaking strategies proposed in Table 1 — this seems like a useful taxonomy. The Appendix contains a nicely detailed history of related work.\n\n*Significance*: The finding that models with more parameters change responses more is very interesting! (Does this line up with any practical observations about ease of jailbreaking?)\n\n*Clarity:* The paper is very well written and for the most part well-presented.\n\n*Originality:* The paper introduces some neat new ideas about ways to jailbreak LLMs, including new variations of known “scenario” strategies. I'd be excited to see more ideas like these."}, "weaknesses": {"value": "Generally speaking, the intervention strategy here seems very well-motivated and resembles the kinds of strategies used to jailbreak models in practice. The main weakness of the paper is in the estimation of resulting outcomes. As I detail below, it’s not clear what LLM “values” are, why this test is relevant to real-world LLM use, and whether the variations observed are due to random noise. Findings 3 and 5 in particular seem questionable.\n\nIn addition to addressing the internal validity issues mentioned below, I would encourage the authors to clarify the conceptual grounds for this approach: What exactly are LLM “values” (patterns in mathematical representations, patterns in responses to ethical dilemmas)? How do they translate to real-world outcomes and harms? Is LitmusValues a meaningful test for this concept and these outcomes? Are there other well-validated tests that could be included to back up these results?\n\nI would be very interested in a revision of this paper that focuses closely on the effectiveness of various jailbreaking strategies on concrete LLM behaviors in real-world use cases.\n\n*Clarity*\n\n1. **[Major]** LLM “values” are not clearly defined.\n    1. L043 jumps straight to how LLM “values” are measured without defining what they are. Reverse engineering, is an LLM “value” simply the way the system tends to respond to particular survey prompts or ethical hypotheticals? L108-110 also seems to imply this behavioral definition.\n    2. But, Platonic Representation Hypothesis (L090) refers to mathematical representations—not observed behaviors. Is an LLM “value” some type of internal representation, then?\n    3. Similarly L101 lists several findings related to LLM “values” but only provides a definition for human values (”abstract goals influencing human perception”). It’s not clear how that applies to LLMs. Just because human values motivate decisions and behaviors, why should we expect a similar model to describe LLMs?\n    4. Similarly, what is a “value ranking” (L053)? What is a “value correlation” (L085)?\n2. **[Major]** Fig. 9 and Section 5.3 (Finding 5) don’t make sense to me. Is this figure showing that, for large models, the scenario prompts changed “values” *less* than the persona prompts? This seems to contradict the overall trend in Fig. 5, right? And it definitely doesn’t mean that the models “resisted” perturbation (L456), since there was still a positive effect, right? More clarity in this Figure and Sec. 5.3 would be very helpful.\n\n*Significance*\n\n1. **[Major]** It’s not very clear how these results correlate to concrete harms and outcomes associated with real-world use of LLM systems.\n    1. L039-041: What do LLM “values” have to do with “biased outputs or harmful responses”? The citations referenced don’t provide any evidence establishing this relationship. (I am not familiar enough with the notion of LLM values to know if this evidence exists, but it seems critical to the argument here.)\n    2. The authors say that LitmusValues tests risky dilemmas that future AI models might encounter (L134). That leap of logic is not clear to me. As far as I can tell, LitmusValues dilemmas are hypothetical role-playing scenarios. How can we be sure this type of measurement has any correlation with downstream outcomes associated with actual LLM use? Is there any evidence that users are employing LLMs to answer contrived questions like these?\n    3. One possible improvement would be to use one of the many fairness benchmarks intended to measure these kinds of harmful outcomes (e.g., in resume screening), and test for correlation with the “value” measurements.\n    4. Still, at a basic level, these findings are predictable: LLM responses to hypothetical dilemmas are different when the parameters of those hypothetical dilemmas are changed. But what does this have to do real-world uses and harms? What is the use case imagined here? What is the threat model?\n2. **[Minor]** L085, L405: Aren’t the value correlations (Finding 3) simply an artifact of the benchmark construction? By design, the benchmark systematically pits “values” against one another (choosing value A precludes choosing value B), so we would expect the correlation matrices to look similar across models, right? For example, in the Fig. 3 example, choosing for “care”, or “justice” means choosing against “sustainability”. How do we account for the correlation structures that already exist in the benchmark? (What are they?) I could be missing something here but it’s not clear to me why this result is meaningful.\n\n*Quality/Soundness*\n\n1. **[Major]** How can we distinguish the observed effects from random chance?\n    1. Another explanation for these results is simply that the hypothetical questions are very noisy. Fig. 9 could actually be interpreted as supporting this hypothesis—the delta rank values seem to vary depending on the choice of movie (L460), which seems like it could be irrelevant to the “value” rank. (How exactly are the movie’s “values” expressed in the prompt? This was not clear.)\n    2. To test this, I would suggest including several “placebo” controls (random statements or paragraphs such as “the sky is blue…”) to establish a baseline for variance when the prompt is changed in some theoretically irrelevant way.\n    3. Likewise, did the authors run multiple trials/epochs for each benchmark question? It seems important to know how consistent model responses are across random seeds.\n2. **[Major]** Without a sense of the uncertainty in these measures, it’s difficult to tell if the claims are really supported by these findings. (For example, the authors claim that “scenario has the strongest persuasioness [sic]”, but the average delta rank for “persona” is also pretty high. How do we know these are meaningfully different—or any of the other methods, for that matter?)\n    1. Fig. 4 could include confidence intervals or reliability scores (Bradley-Terry scores might be a better choice).\n    2. Fig. 5 (the delta measures) could include statistical tests of difference from zero. Cells which are not statistically different from zero can be left blank.\n    3. Fig. 6 is an average and could include error bars (are these deltas different from zero?).\n3. **[Minor]** L430: The authors claim that Fig. 8 supports the Platonic Representation Hypothesis that AI models are converging on a shared statistical model. But doesn’t Fig. 8 describe patterns in responses to prompts, not underlying mathematical representations? Reviewing the Platonic Representation paper, it seems to be more focused on internal representations rather than prompt-response behavior.  \n4. **[Minor]** Asimov’s (fictional) Three Laws of Robotics (written in 1950) is not a serious source for ethical guidelines for LLM system behavior. (They also do not describe values as I see them typically defined in, e.g., virtue ethics approaches; the Three Laws are deontological, in that they describe rules for behavior.) I would encourage the authors to dig deeper into the large body of recent work by ethicists and legal scholars on desirable properties for LLM systems."}, "questions": {"value": "In addition to the questions and suggestions above:\n\n**[Minor]** What separates this work from the papers cited in Related Work (particularly L117)? Are all 5 findings new? (It seems like this paper does some fine-grained analysis into how different jailbreaking strategies may influence model behavior.) A bit more detail here would be helpful."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ZwsgZngLtm", "forum": "Td9Oi4Hhwg", "replyto": "Td9Oi4Hhwg", "signatures": ["ICLR.cc/2026/Conference/Submission12980/Reviewer_ikzW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12980/Reviewer_ikzW"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission12980/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762553276242, "cdate": 1762553276242, "tmdate": 1762923732889, "mdate": 1762923732889, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}