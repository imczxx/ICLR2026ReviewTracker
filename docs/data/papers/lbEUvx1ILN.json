{"id": "lbEUvx1ILN", "number": 25470, "cdate": 1758368369662, "mdate": 1759896719913, "content": {"title": "OVA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data", "abstract": "Federated fine-tuning (FFT) adapts foundation models to decentralized data but remains fragile under heterogeneous client distributions due to local drift, i.e., client-level update divergences that induce systematic bias and amplified variance in the global model. Existing aggregation and personalization methods largely correct drift post hoc, which proves brittle under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework that is, to our knowledge, the first explicitly designed to suppress drift at its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing on a frozen encoder with a one-vs-all head and a simple two-stage procedure, preserving pretrained feature geometry and decoupling logits to prevent the mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over shard-1, shard-2, and Bernoulli--Dirichlet partitions, OvA-LP retains 95.9\\% of its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1\\% (PFPT) and 34.5\\% (FFT-MoE) under the same conditions. OvA-LP further maintains resilience under both symmetric and asymmetric label noise. In addition, precomputing encoder features makes per-round cost nearly independent of encoder size. Together, these results demonstrate that OvA-LP provides a principled and efficient basis for robust FFT under heterogeneity.", "tldr": "", "keywords": ["federated learning", "non-iid", "noisy labels", "One-vs-All", "Linear Probing"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/768ab6e2fcc967049e35301c340813458461406d.pdf", "supplementary_material": "/attachment/a3122d2fdeb40e8b238eb021002578ed9e2a78f8.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces OvA-LP, a framework that combines linear probing, one-vs-all (OvA) heads, and a two-stage training schedule to mitigate client drift in Federated Fine-Tuning (FFT) of foundation models on non-IID data."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Strong Empirical Performance**.\n- **High Efficiency**.\n- **Simplicity and Modularity**."}, "weaknesses": {"value": "1. **Incremental Contribution:** The core components (linear probing, OvA heads, two-stage training) are all established techniques. The paper fails to demonstrate novel synergistic mechanisms beyond their combination.\n2. **Superficial Theoretical Analysis:** The bias-variance decomposition remains qualitative. Lacks formal proofs or bounds to substantiate claims about bias reduction or variance control.\n3. **Limited Experimental Validation:**\n    - Scope is narrow (only vision tasks, ViT encoders). No results on NLP/time-series or with CNN architectures.\n    - Baseline comparisons are incomplete and potentially unfair (e.g., unequal participation rates vs. PFPT).\n    - Missing sensitivity analysis (hyperparameters, non-IID severity).\n4. **Poor Practicality:**\n    - Unrealistic 100% client participation assumption. No solutions offered for partial participation.\n    - Incomplete cost analysis (no communication costs across encoder sizes or edge device deployment)."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7TyOapxGtw", "forum": "lbEUvx1ILN", "replyto": "lbEUvx1ILN", "signatures": ["ICLR.cc/2026/Conference/Submission25470/Reviewer_4AKB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25470/Reviewer_4AKB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761533739322, "cdate": 1761533739322, "tmdate": 1762943446055, "mdate": 1762943446055, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies the problem of federated fine-tuning on non-IID client data, where models drift due to data heterogeneity. It introduces **OvA-LP**, a simple framework that prevents this drift **at its source** rather than correcting it afterward. OvA-LP freezes the pretrained encoder for stable features, replaces softmax with one-vs-all (OvA) heads to remove label bias, and uses a two-stage training process to reduce variance. Experiments show that OvA-LP achieves accuracy close to the IID setting while remaining efficient and resilient under various forms of label noise."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "**1. Source-Level Philosophy:**\nThe paper introduces a “source-level” perspective on preventing client drift. Instead of adjusting aggregation or adding personalization after the fact, OvA-LP stops drift before it begins. This proactive approach is supported by a clear bias–variance framing that connects theory with observed results.\n\n**2. Strong Theoretical Motivation:**\nOvA-LP is grounded in a bias–variance decomposition of federated gradients, showing how each component addresses a specific cause of drift. The frozen encoder reduces feature-skew bias, the OvA head removes label-skew bias and variance, and the two-stage training schedule manages optimization variance. This structure gives a clear and convincing rationale for why the method works.\n\n**3. Minimal Yet Effective Design:**\nThe framework stands out for its simplicity and practicality. By combining a frozen encoder, linear probing, and OvA heads, it achieves strong results without the complexity of mixture-of-experts or prompt-tuning models. Its modular design also allows easy integration with existing federated learning schemes.\n\n**4. Ablation Studies:**\nAblation studies show that each component contributes meaningfully to performance gains. Moreover, the inclusion of label noise experiments demonstrates the robustness of OvA-LP under real-world conditions."}, "weaknesses": {"value": "**1. Experimental Setup Clarification (Major):**\nIt is unclear whether the comparisons between OvA-LP and the baselines (FFT-MoE, PFPT) are conducted on the same dataset and under identical settings. Although the paper mentions that baseline methods are reproduced using their original architectures and training protocols, Table 7 shows different datasets and client counts for OvA-LP compared to the baselines. Moreover, Table 8 reports an active client ratio of 10% for PFPT, while OvA-LP uses 100% participation. This discrepancy raises concerns about the fairness of the comparison, especially given that OvA-LP’s reported advantage (95.9% vs. 10.1% and 34.5%) could partly stem from this difference. Clarification on whether all methods were evaluated under identical conditions would strengthen the paper’s claims.\n\n**2. Limited Generality:**\nThe paper claims that OvA-LP’s minimalist design is modular and compatible with aggregation and personalization frameworks, suggesting potential use across diverse FFT pipelines. However, its comparisons are limited to personalization-based methods like FFT-MoE and PFPT. Extending the approach to PEFT settings, such as FLoRA, FedSA-LoRA, or FRLoRA, would strengthen its generality and applicability beyond vision tasks.\n\n**3. Potential Overclaim in Efficiency:**\nThe claim that OvA-LP achieves substantially faster convergence could partially stem from using a frozen encoder (no backprop through large layers). A fair comparison should equalize encoder freezing across baselines or report separate encoder vs. head timings.\n\n**4. Notational Inconsistency (Minor):**\nIn Sections 3.1 and 3.2, the notation for $(x, y)$ is inconsistent. In Section 3.1, $x$ and $y$ represent input and label pairs from the data distribution, while in Section 3.2, $y$ denotes a second input sample in the alignment term, making the notation unclear."}, "questions": {"value": "Can you provide any analytical result or empirical variance plots that quantify the “variance suppression” beyond accuracy curves?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YyMC3dqXOm", "forum": "lbEUvx1ILN", "replyto": "lbEUvx1ILN", "signatures": ["ICLR.cc/2026/Conference/Submission25470/Reviewer_PhgA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25470/Reviewer_PhgA"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761585269566, "cdate": 1761585269566, "tmdate": 1762943445553, "mdate": 1762943445553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces OvA-LP, a framework for federated fine-tuning (FFT) on non-IID data. It proposes to suppress local drift \"at its source\" by combining three components: (1) linear probing (LP) on a completely frozen encoder; (2) a one-vs-all (OvA) classification head to decouple logits ; and (3) a two-stage (positive-only, then positive+negative) training schedule. The proposed method is simple, efficient , and demonstrates exceptionally strong empirical results in its primary setting (e.g., 95.9% IID accuracy retention vs. 10.1%-34.5% for SOTA baselines). The clarity of the bias-variance motivation is also a strength."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Clear Motivation:** The design is well-grounded in a bias-variance decomposition. Each component is clearly justified as targeting a specific source of drift: linear probing limits feature-skew bias , OvA heads eliminate label-skew bias , and the two-stage training controls variance.\n\n2. **Methodological Simplicity**: The paper presents a \"minimalist\" framework that combines three relatively simple components: linear probing on a frozen encoder, one-vs-all (OvA) heads, and a two-stage training schedule.\n\n3. **High Efficiency and Fast Convergence**: The framework is computationally efficient. By precomputing encoder features, the per-round training cost is nearly independent of the encoder's size. Furthermore, it converges significantly faster than baselines, often reaching near-IID performance in just 1-3 rounds.\n\n4. **Very Nice Reproducibility:** The code implementation fully covers all the core methods, experimental settings, ablation studies, and baseline comparisons described in the paper. The codebase structure is clear and corresponds one-to-one with the arguments in the paper. This also helps in understanding the technical details in the paper. I personally recommend that other reviewers take this into consideration when evaluating the paper."}, "weaknesses": {"value": "1. **Unfair Baseline Comparison**: This is the most critical issue. The paper compares OvA-LP- which only trains a linear head on a frozen encoder - against SOTA baselines like PFPT and FFT-MoE. These baselines are Parameter-Efficient Fine-Tuning (PEFT) methods that adapt the model (e.g., via prompt-tuning or MoE layers).\nThe \"local drift\" that the paper claims to solve is a phenomenon that arises precisely because the baselines are fine-tuning the shared encoder on heterogeneous local data. OvA-LP does not solve this drift; it avoids the problem entirely by not fine-tuning the encoder at all.\nTherefore, the comparison in Figure 4 is an unfair comparison between a non-adapting model (OvA-LP) and adapting models (PFPT, FFT-MoE). The conclusion that OvA-LP is a superior framework for \"federated fine-tuning\" is unsupported by this evidence.\nThe paper's own ablation study (Figure 3) demonstrates this issue. The \"LP-Softmax\" baseline (frozen encoder + standard head) achieves 56.3% relative performance. This frozen-encoder baseline already massively outperforms the fine-tuning SOTA baselines (PFPT at 34.5%, FFT-MoE at 10.1%). The paper even states, \"even LP-softmax... already surpasses post-hoc baselines\".\nThis strongly implies that the primary reason for the dramatic performance gain (from 10-35% to 56%+) is freezing the encoder, not the specific OvA-LP design. While the OvA-LP components provide a further, significant boost (from 56.3% to 95.9%), the paper's primary comparison against fine-tuning methods is misleading. The authors must re-frame the contribution: OvA-LP is a superior linear probing method, not a superior fine-tuning method.\n\n2. **Mismatch in Claims and Methodology**: The paper positions itself within the \"PEFT-based FFT paradigm\". Linear probing is the most trivial form of PEFT and is functionally distinct from methods like LoRA, prompt tuning (like PFPT), or adapters, which are designed to adapt the foundation model. Calling this \"federated fine-tuning\" is an overclaim. This is \"federated linear probing.\" This distinction is critical because the method sacrifices model adaptation to gain robustness. This trade-off is not adequately discussed.\n\n3. **Practicality of Experimental Assumptions**: The main results rely on 100% client participation , which is an unrealistic assumption for most practical FL scenarios. The paper acknowledges this in the limitations and Appendix A.3 , where it is shown that \"Lower participation ratios lead to slower convergence\". This weakens the main paper's claims of extreme efficiency and rapid convergence (e.g., \"1 round\" to Acc@95 in Table 3 ). Results under partial participation shall be moved to the main paper and analyzed thoroughly."}, "questions": {"value": "1. **Revise Baseline Comparisons**: To make a fair comparison, the authors must compare OvA-LP to other frozen-encoder baselines. The current SOTA baselines (PFPT, FFT-MoE) must be re-run in a configuration where their encoders are also frozen, to isolate the benefit of their respective heads/adapters. Alternatively, the authors must compare OvA-LP against a true SOTA federated fine-tuning method (e.g., Fed-LoRA) and honestly discuss the trade-off (i.e., OvA-LP gains non-IID robustness by losing the ability to adapt the encoder).\n\n2. **Discuss Scope of Applicability**: The method inherently relies on a high-quality pretrained encoder whose \"feature geometry\" is already sufficient for linear separation. This is a major assumption. The limitations section should be expanded to more critically discuss what happens when the downstream task is not linearly separable from the pretrained features and requires encoder adaptation (a scenario where OvA-LP would presumably fail completely)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "No ethics review needed."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6EPtdgcB1V", "forum": "lbEUvx1ILN", "replyto": "lbEUvx1ILN", "signatures": ["ICLR.cc/2026/Conference/Submission25470/Reviewer_2VP7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25470/Reviewer_2VP7"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25470/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327562768, "cdate": 1762327562768, "tmdate": 1762943444955, "mdate": 1762943444955, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}