{"id": "uylBfVAAsV", "number": 15388, "cdate": 1758250867075, "mdate": 1759897310087, "content": {"title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation", "abstract": "Recent advances in AI-generated content (AIGC) have significantly accelerated animation production. To produce engaging animations, it is essential to generate coherent multi-shot video clips with narrative scripts and character references. However, existing public datasets primarily focus on real-world scenarios with global descriptions, and lack reference images for consistent character guidance. To bridge this gap, we present AnimeShooter, a reference-guided multi-shot animation dataset. AnimeShooter features comprehensive hierarchical annotations and strong visual consistency across shots through an automated pipeline. Story-level annotations provide an overview of the narrative, including the storyline, key scenes, and main character profiles with reference images, while shot-level annotations decompose the story into consecutive shots, each annotated with scene, characters, and both narrative and descriptive visual captions. Additionally, a dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each shot, along with audio descriptions and sound sources. To demonstrate the effectiveness of AnimeShooter and establish a baseline for the reference-guided multi-shot video generation task, we introduce AnimeShooterGen, which leverages Multimodal Large Language Models (MLLMs) and video diffusion models. The reference image and previously generated shots are first processed by MLLM to produce representations aware of both reference and context, which are then used as the condition for the diffusion model to decode the subsequent shot. Experimental results show that the model trained on AnimeShooter achieves superior cross-shot visual consistency and adherence to reference visual guidance, which highlight the value of our dataset for coherent animated video generation.", "tldr": "", "keywords": ["Multi-shot video generation", "Animation generation", "MLLM"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1000fa734fe392e2503bbc5dbc27f8214eff6750.pdf", "supplementary_material": "/attachment/5b0c80860b79f08343e9daa90f5be00a342a8255.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces AnimeShooter, a new dataset designed to address the task of reference-guided, multi-shot animation generation. The AnimeShooter dataset is constructed through an automated pipeline that collects animation videos from YouTube, uses Gemini to generate hierarchical story scripts including story-level and shot-level annotations, and leverages models like Sa2VA and InternVL to extract and filter high-quality character reference images.\nTo validate the dataset's effectiveness, the authors propose a baseline model, AnimeShooterGen, which uses MLLM and video diffusion models to generate subsequent shot according to the reference and context. Experimental results demonstrate the effectiveness of the AnimeShooter dataset and AnimeshooterGen."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The primary contribution is a large-scale dataset for the animation domain that addresses a clear and important research gap in multi-shot, reference-guided video generation. Its scale and rich hierarchical annotations make it a valuable resource for the community.\n- The authors employ a highly automated pipeline to process data. And the data is carefully designed.\n- To demonstrate the dataset's effectiveness and practical utility, the authors also introduce AnimeShooterGen. This model serves as a strong baseline and effectively validates that the proposed dataset can be used to train robust models for this complex task."}, "weaknesses": {"value": "- The baseline model's strong performance heavily relies on the fourth stage, \"LoRA Enhancement,\" which is essentially test-time finetuning on a few video clips of a specific IP. This makes the model more of a few-shot IP customization method rather than a general reference-guided generator. Can the authors provide a quantitative ablation study comparing AnimeShooterGen's performance on all metrics with and without the LoRA Enhancement stage?\n- How does the model perform in a zero-shot setting? That is, given a reference image for an IP that was unseen during all training stages (including LoRA enhancement), how does AnimeShooterGen compare to baselines like IP-Adapter?\n- Why was the decision made to use only the last frame of the previous shot as visual context? Were other representations (e.g., first and last frames, multiple frames, pooled video features) explored?\n- The dataset provides both a narrative caption and a descriptive caption for each shot. How were these two caption types used when training the baseline model? Were they concatenated, or was only one type (e.g., the descriptive caption) used?"}, "questions": {"value": "Please See Weaknesses"}, "flag_for_ethics_review": {"value": ["Yes, Potentially harmful insights, methodologies and applications"]}, "details_of_ethics_concerns": {"value": "As the authors correctly note in the paper, while generation models democratize content creation, they also risk enabling malicious applications such as generating deepfakes for disinformation and producing harmful content."}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hBt2hokVfa", "forum": "uylBfVAAsV", "replyto": "uylBfVAAsV", "signatures": ["ICLR.cc/2026/Conference/Submission15388/Reviewer_Kftk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15388/Reviewer_Kftk"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761045479006, "cdate": 1761045479006, "tmdate": 1762925670316, "mdate": 1762925670316, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces AnimeShooter, a large animation‑focused dataset aimed at reference‑guided, multi‑shot video generation. Each roughly one‑minute “story” includes (i) story‑level annotations (storyline, 1–3 main characters with reference images, and main scenes) and (ii) shot‑level annotations (ordered shots with scene, characters, and both narrative and descriptive captions). A smaller AnimeShooter‑audio subset adds synchronized shot‑level audio descriptions and sources. To demonstrate utility, the authors propose AnimeShooterGen, an autoregressive pipeline that conditions a video diffusion model on: (a) a user reference image, (b) the last frames of previously generated shots, and (c) the shot text. An MLLM backbone produces a conditioning embedding (via a Q‑Former adapter) for a DiT‑based video generator; LoRA layers enable light test‑time adaptation. On a custom multi‑IP evaluation set, AnimeShooterGen outperforms existing baselines on CLIP similarity and DreamSim (shot‑ and story‑level), and in MLLM and user studies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper isolates a genuinely under‑served setting: reference‑guided multi‑shot animation generation with cross‑shot character/style consistency, rather than single‑shot real‑world videos or global captions.\n2. The combination of story‑level elements (storyline, scenes, character cards + reference images) and fine‑grained shot‑level captions is valuable for autoregressive modeling and evaluation.\n3. Conditioning on a reference image + prior last frames + text through an MLLM + Q‑Former adapter is technically sound and well motivated.\n4. Good empirical results on the proposed evaluation benchmark compared with baselines."}, "weaknesses": {"value": "1. The compared baselines are relatively weaker baselines (e.g., IP‑Adapter+I2V and CogVideo‑LoRA). Stronger baseline models with MLLM might also be considered.\n2. The evaluation metrics with CLIP and DreamSim cannot capture some video quality aspects like motion smoothness. Better automatic evaluation metrics for these categories should be investigated (e.g., like in VBench). \n3. More qualitative examples/analysis should be included for generalization to longer shots (e.g., 15 shots) to support the claim of \"AnimeShooterGen generalizes robustly to longer sequences during testing.\""}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WNBjwwRbH3", "forum": "uylBfVAAsV", "replyto": "uylBfVAAsV", "signatures": ["ICLR.cc/2026/Conference/Submission15388/Reviewer_pjXJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15388/Reviewer_pjXJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982537193, "cdate": 1761982537193, "tmdate": 1762925669481, "mdate": 1762925669481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AnimeShooter to address the current limitations in multi-shot datasets, which mainly focus on real-world scenarios and lack reference images. AnimeShooter is a reference-guided multi-shot animation dataset. For each shot, the dataset provides annotations of the scene and characters, as well as visual descriptions in both narrative and descriptive forms. Additionally, a subset with synchronized audio annotations, AnimeShooter-audio, is provided. Moreover, this paper introduces a reference-image-guided multi-shot video generation model based on MLLMs and diffusion models. The effectiveness of the proposed model is validated through both qualitative and quantitative experiments."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a multi-shot video dataset in the anime domain, along with a subset containing audio data, laying a foundation for advancing research in animation storytelling.\n2. The paper is well-organized and highly readable, with figures concisely illustrating the workflow, data structure, and qualitative comparisons."}, "weaknesses": {"value": "1. In the proposed method, visual information such as reference images and different shot contexts is encoded by the MLLM and aligned with the text embeddings of the diffusion model. This can lead to the loss of fine details from the reference images or shot scenes. As shown in Figure 5, without LoRA enhancement, the consistency of details is not satisfactory. However, many real-world workflows prefer models that do not require additional fine-tuning.\n\n2. The baseline methods compared in this paper are not specifically designed for multi-shot video generation. Since they lack any cross-shot perception capability, it is unsurprising that the proposed method shows improvements over these baselines.\n\n3. The videos generated in this paper exhibit weak storytelling and scene transitions across different shots. In my view, they resemble a combination of multiple videos rather than multiple shots of a single coherent video.\n\n4. The paper lacks more ablation studies to demonstrate the effectiveness of the proposed model architecture."}, "questions": {"value": "The proposed dataset contains multi-character shots, but the results presented in the paper are all single-character. Is the method capable of generating multi-character videos?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TZgHUHNyj3", "forum": "uylBfVAAsV", "replyto": "uylBfVAAsV", "signatures": ["ICLR.cc/2026/Conference/Submission15388/Reviewer_ZmJj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15388/Reviewer_ZmJj"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988706470, "cdate": 1761988706470, "tmdate": 1762925668849, "mdate": 1762925668849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces AnimeShooter, a large-scale, reference-guided multi-shot animation dataset featuring hierarchical story- and shot-level annotations, synchronized audio tracks, and strong cross-shot visual consistency. To demonstrate its utility, the authors propose AnimeShooterGen, a baseline framework that integrates a Multimodal Large Language Model (MLLM) with a video diffusion model for generating coherent multi-shot animations."}, "soundness": {"value": 1}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "AnimeShooter provides structured, story-aware, and reference-guided annotations, filling a significant gap in current video-generation datasets. Clear separation between story-level and shot-level elements enables both global narrative control and local visual coherence.\nThe open release of both dataset and baseline has high potential to become a standard benchmark for multi-shot animation generation."}, "weaknesses": {"value": "1.Hierarchical captioning reduces drift but lacks visual grounding, leaving potential hallucination issues.\n2.Only basic normalization is used; no explicit domain alignment across different animation styles and for object segmentation methods, fine-tuning on ~500 frames offers limited adaptation from real-world to animated content.\n3.For keyframe-selection, real-video heuristics are applied; not well-suited for low frame-rate animation.\n4.No explicit loss or alignment; character consistency relies on semantic coincidence. Lacks global temporal structure, leading to potential drift over long multi-shot sequences.\n5.The MLLM–diffusion pipeline is computationally heavy, with no reported efficiency metrics or optimization strategy."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dRffi8QKA5", "forum": "uylBfVAAsV", "replyto": "uylBfVAAsV", "signatures": ["ICLR.cc/2026/Conference/Submission15388/Reviewer_pA6S"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15388/Reviewer_pA6S"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15388/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103126091, "cdate": 1762103126091, "tmdate": 1762925668418, "mdate": 1762925668418, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}