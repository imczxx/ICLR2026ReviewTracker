{"id": "CtwyBsbvOA", "number": 16558, "cdate": 1758266019876, "mdate": 1759897232756, "content": {"title": "Transformers as Multi-task Learners: Decoupling Features in Hidden Markov Models", "abstract": "Transformer-based models have shown remarkable capabilities in sequence learning across a wide range of tasks, often performing well on specific task by leveraging input-output examples. Understanding the mechanisms by which these models capture and transfer information is important for driving model understanding progress, as well as guiding the design of more effective and efficient algorithms.\nHowever, despite their empirical success, a comprehensive theoretical understanding on it remains limited. In this work, we investigate the layerwise behavior of Transformers to uncover the mechanisms underlying their multi-task generalization ability. Taking explorations on a typical sequence model—Hidden Markov Models (HMMs), which are fundamental to many language tasks, we observe that:\n(i) lower layers of Transformers focus on extracting feature representations, primarily influenced by neighboring tokens;\n(ii) on the upper layers, features become decoupled, exhibiting a high degree of time disentanglement.\nBuilding on these empirical insights, we provide theoretical analysis for the expressiveness power of Transformers. Our explicit constructions align closely with empirical observations, providing theoretical support for the Transformer’s effectiveness and efficiency on sequence learning across diverse tasks.", "tldr": "", "keywords": ["Transformer", "expressiveness power", "hidden Markov model"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a1dfddc63985d8fda8f4904124ab32dbb6c95634.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper studies how (well) transformers learn Hidden Markov Models (HMMs).  They do so by starting with experimental observations of the same setup, i.e., learning HMMs using transformers, wherein they observe that the state is learned before the overall task.  They use these observations to come up with an architecture that learns transformers and obtain bounds on the expressive power of transformers (in learning HMMs)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.  The paper attempts to provide a strong theoretical foundation to how transformers learn --- they do so by studying how transformers learn an HMM, providing a significant step up in generality than simple Markov chains, which seems to have been standard so far.\n\n2. The theoretical analysis is done correctly and reasonably clearly, there are no mistakes as far as I can see."}, "weaknesses": {"value": "1. The notation is not very clear and this makes the paper rather hard to read, at least on the first pass.  Here are my notation-specific questions, but some suggestions are in Minor comments under Questions below.\n      What does \\mathcal{O} represent?  My guess is that it represents the observation space, the set from which the observations are drawn.  Then what is \\mathcal{O}(\\mathcal{H}) (typo, should be \\Delta?)?  And what does the \"emission operator\" mean? What does the * do, and why does the * not appear when \\mathbb{T} is referred to again (in, say Assumption 2)? (I could not find any explanation in the paper, but if it is explained somewhere a reference would help.)\n\n2. The natural question is what this tells us about more realistic models.  Theoretical analyses are useful, but the goal is never to keep increasing the complexity of models we study till we can analyze practical models.  The goal is to obtain some useful insights that extend to large models and guide intuition.  It is not clear to me that this paper has any such insights (though the analysis is certainly appreciated).\n\nAs such, I recommend acceptance, but not strongly.  I will be happy to increase my score if the authors can convince me that there are potential useful insights to be obtained.  I would also like to ask the authors to improve some of the presentation to make it easier to read (as in the Weakness above and Questions below)."}, "questions": {"value": "1. Section 2: Figure 2 is supposed to show that the features become decoupled at deeper layers, but there seems to be some weird phenomena happening at layers 13--14.  Comments?  What exactly is the experiment doing --- what does the random shuffling of positions represent?\n\n2. I appreciate that Assumptions 1, 2, 3 are reasonable analytically and have been made by previous works, but it would be useful to know if these assumptions hold in practice, at least approximately.  Could you comment on this? \n\n3. Why does the low-rank structure in Assumption 1 lead to the \"low-rank\" transition at line 270 (top of page 6)?  I suspect it might follow from elementary probability, but I was unable to convince myself of this, could you give a proof?\n\n4. Remark 1 seems rather superfluous: it should not be surprising that your explicit construction \"aligns closely\" with the empirical observations --- if anything, the empirical observations likely helped with the explicit construction?  And then yes, the empirical observation is valid and interesting, and so is the explicit theoretical construction, but the \"connection\" just seems forced when it is causal, not a correlation.\n\nMinor comments:\n1. Section 3.2 is a little hard to read, mainly because of all the notation.  It might be easier to follow if the dimensions of s, v, o are written before they are introduced in the equation with the matrices M_{0,i} and M_{0,test}.\n2. Section 3.2: why do you need D > 2p^2 L?  It would be helpful to refer to the part of the analysis where this constraint shows up.  Is this condition normally met in practice?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "efhWdUIyFs", "forum": "CtwyBsbvOA", "replyto": "CtwyBsbvOA", "signatures": ["ICLR.cc/2026/Conference/Submission16558/Reviewer_4m3J"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16558/Reviewer_4m3J"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761856429170, "cdate": 1761856429170, "tmdate": 1762926639666, "mdate": 1762926639666, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper aims to understand the mechanisms by which transformers learn structured sequence tasks, using Hidden Markov Models (HMMs) as a controlled synthetic setting.\n\nEmpirically, the authors analyze how layer outputs depend on the ordering of input sequences, revealing distinct functional roles across lower and upper layers. Through probing experiments, they further examine what types of information, task information or hidden-states, are accessible at different layers.\n\nTheoretically, the paper proves that under a low-rank assumption on the HMM, there exists a transformer with a number of layers growing logarithmically in the sequence length that can approximate the transition probability of the underlying HMM."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "Using a structured synthetic setup like Hidden Markov Models to study how transformers learn the task structure is a reasonable and well-motivated approach. The approach of pairing empirical observations about layer-wise behavior with a theoretical characterization of how transformers can approximate HMM distributions has the potential to provide complementary insights."}, "weaknesses": {"value": "The overall presentation lacks clarity and organization, which makes it difficult to follow the results, and the paper would benefit from a thorough revision. The organization is confusing. For instance, many details about the experiments in Section 2 are either missing or only defined later (such as the data format introduced in Section 3), and even there, not clearly. The probing experiment starting around line 126 is not clearly described and the discussions are not concrete enough. The metrics and evaluation methods are also not well-explained, making it hard to interpret the reported results. \n\nThe connection between the empirical and theoretical parts is similarly unclear; the two sections are not well tied together, and the relationship between their findings is not clearly articulated. In addition, some statements made at the beginning of the paper are not sufficiently supported or discussed in the main text.\n\nIn its current form, it is difficult to parse the main results and contributions. I recommend a revision before the paper can be considered ready for publication. \n\nSee Questions for details."}, "questions": {"value": "1. **Sec 2 -- Experiments**: \n\n    (a) Does the sequence format in the experiments follow the one described later in Sec 3 (around line 200)? What is the sequence length of the HMMs used in the experiments? There is no mention of low-rankness in this section; does the data follow the low-rank transition structure later described in Sec 3?\n\n    (b) The Appendix mentions using approximately ~8k HMMs for data generation. Is this what you refer to as the multi-task setup? Are these HMMs used only for training sequences, or are test sequences also drawn from this 8k set? Basically, could you clarify what exactly constitutes the *in-context learning* part of the task? Also, are you training the model on only last token, or auto-regressive?\n\n    (c)  Given 64 steps per epoch and a batch size of 32, total training sequences should be 2048. How is the data size 131K? \n\n    (d) What's accuracy in Fig 1? \n\n    (e) Fig 2: What do you mean by \"shuffling positions\" in this experiment? What exactly are the logits at each layer: are they the embeddings after each layer, or the outputs of the softmax on key–query scores? and for the metric, how is the std and mean computed? Since logits are vectors, is the standard deviation taken over both the position shuffles and the logit dimensions? Otherwise, the mean and standard deviation should themselves be vectors, not scalars suitable for a heatmap.\n\n2. **Sec 3 -- Theory**: The notation and definitions in this section are quite unclear, and there appear to be multiple typos or inconsistencies. For instance,  (Line 213) $p$, (Theorem 1) $T$, (Line 239) $\\mathcal{O}(\\mathcal{H})$, (line 270) $\\mu,\\xi$ are undefined. (Line 258) $\\mathbb{T}$ should be $\\mathbb{T}*$. (Line 202) There's a dimension mismatch between $M_0$ and $M_{0,i}$. (Line 289) Dimensions should be $n(L-1)\\times p$. (Line 419) Feature dimension of the output matrix seems to be $L(p+3)$, but which part of the transformer has this dimension? \n    \n     Beyond these issues:\n\n    (a) Should there not be a condition on $d$ to ensure the induced low-rankness in Assumption 1?\n\n    (b) what role does $v_{pos}$ play? Why are you considering these extra dimensions that are always 1 or 0 in for all tokens?\n\n    (c) In remark 1, from the discussions up to this point, how do we see the connections to the results in Sec 2? And more importantly, how do we see the *decoupling* of features in Theorem 1?\n\n    (d) What do you mean by future observations $F_r$ (also mentioned in line 78)? Don't you use casual masking in your setup? \n\n3. Line 57: \"Such feature decoupling phenomenon also has practical implications, such as assigning different tasks to different layers in multi-task learning, or masking position-related features in higher layers to improve inference efficiency\". Reading the paper, I did not find it clear what *feature decoupling* is referring to concretely, and even setting that aside, its connection to these practical implications is not discussed anywhere."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "s3RmgwpRgb", "forum": "CtwyBsbvOA", "replyto": "CtwyBsbvOA", "signatures": ["ICLR.cc/2026/Conference/Submission16558/Reviewer_wD2R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16558/Reviewer_wD2R"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762065038522, "cdate": 1762065038522, "tmdate": 1762926638860, "mdate": 1762926638860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper addresses a highly interesting and fundamental problem: understanding how transformers can learn hidden Markov models (HMMs)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper attempts to address an extremely interesting and important problem.\n\n2. The authors aim to understand this phenomenon from both theoretical and practical perspectives."}, "weaknesses": {"value": "1. I spent a considerable amount of time on this paper, but still found it difficult to parse and follow. A thorough rewrite could significantly improve its clarity and readability.\n\t\n2. Several figures are also difficult to interpret and would benefit from clearer labeling or improved presentation.\n\n3. Additionally, some relevant references appear to be missing and should be included to provide appropriate context and attribution."}, "questions": {"value": "1. It seems that several relevant references ([A]–[I]) have been omitted. Including these works and discussing how the proposed approach compares to them would strengthen the paper’s positioning within the existing literature and clarify its unique contributions.\n\t\n2. I found the paper challenging to follow, despite multiple careful readings. It would be very helpful if the authors could provide a more intuitive explanation of the transformer construction. In particular, clarification on the role of W in Equation (2), how it is optimized, and the rationale behind performing gradient descent per layer (lines 420–431) would be valuable. Is the optimization applied only to the W matrices, or does it also involve the query, key, and other parameters?\n\n3. Given the complexity of the presentation, a clearer description of the data setting would be beneficial. Specifically, is the training performed in an in-context learning setup, or are the samples drawn from the same underlying HMM? Additional details on the experimental setup, especially regarding data generation, would help readers better understand the empirical evaluation and its implications.\n\n4. The figures are difficult to interpret in their current form. I would appreciate further clarification on how Figures 1 and 2 were generated and what specific aspects they aim to illustrate. Moreover, in Figure 3b, the text mentions that accuracy decreases, but the plot appears largely constant. Could the authors clarify this apparent discrepancy? Finally, please explain how accuracy is computed in these experiments.\n\n5.\tThe reference to “G-optimal design” in lines 1466–1467 is unclear. A brief explanation or citation would help situate this concept for readers who may not be familiar with it.\n\n**Note**: At this stage, I have assigned a relatively low score because I do not yet fully understand the paper’s main contributions. However, I am open to revising my assessment after the author–reviewer discussion, should the authors be able to clarify these points and improve the presentation.\n\n---\n\n**Refereces**\n\n\n[A] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of a Transformer: A Memory Viewpoint. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\n\n[B] Ezra Edelman, Nikolaos Tsilivis, Benjamin Edelman, Eran Malach, and Surbhi Goel. The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains. In Advances in Neural Information Processing Systems, 37:64273–64311, 2024.\n\n[C] Eshaan Nichani, Alex Damian, and Jason D. Lee. How Transformers Learn Causal Structure with Gradient Descent. In Forty-first International Conference on Machine Learning, 2024. URL\n\n[D] Nived Rajaraman, Marco Bondaschi, Ashok Vardhan Makkuva, Kannan Ramchandran, and Michael Gastpar. Transformers on Markov Data: Constant Depth Suffices. In Advances in Neural Information Processing Systems, 37:137521–137556, 2024.\n\n[E] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Martin Jaggi, Hyeji Kim, and Michael Gastpar. Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains. arXiv preprint arXiv:2402.04161, 2024.\n\n[F] Ruifeng Ren and Yong Liu. Towards Understanding How Transformers Learn In-Context Through a Representation Learning Lens. In Advances in Neural Information Processing Systems, 37:892–933, 2024.\n\n[G] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding. In International Conference on Machine Learning, pages 19689–19729. PMLR, 2023.\n\n[H] Ashok Vardhan Makkuva, Marco Bondaschi, Adway Girish, Alliot Nagle, Hyeji Kim, Michael Gastpar, and Chanakya Ekbote. Local to Global: Learning Dynamics and Effect of Initialization for Transformers. In Advances in Neural Information Processing Systems, 37:86243–86308, 2024.\n\n[I] Ekbote, C., Makkuva, A. V., Bondaschi, M., Rajaraman, N., Gastpar, M., Lee, J. D., & Liang, P. P. (2025). What one cannot, two can: Two-layer transformers provably represent induction heads on any-order Markov chains. In Proceedings of the 39th Annual Conference on Neural Information Processing Systems (NeurIPS 2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "4lTaKUH3kt", "forum": "CtwyBsbvOA", "replyto": "CtwyBsbvOA", "signatures": ["ICLR.cc/2026/Conference/Submission16558/Reviewer_wqke"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16558/Reviewer_wqke"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762209534737, "cdate": 1762209534737, "tmdate": 1762926638495, "mdate": 1762926638495, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "In this paper, the authors study the representation capability of transformers to estimate a HMM in-context from a batch of sequences of observations. Their theoretical construction is inspired by empirical findings that show how early layers in a transformer architecture model short-term dependencies while deeper ones model more long-term, abstract ones."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The paper is an interesting extension of previous works on the ability of transformers to learn Markov data to the more general HMMs. Filling this gap is of theoretical interest, especially due to the proof tools employed, leveraging previous literature on HMMs and ML architectures, and some of the theoretical assumptions used to simplify the theoretical analysis, such as the low-rank structure of the Markov transition kernel."}, "weaknesses": {"value": "My main concerns about the paper is the lack of clarity and details of the experiments in Section 2. I think this Section need extensive rewriting. The dataset construction described in Section 2.1 is very badly explained and not clear at all. A bit better is the description provided in Appendix B.1. Figures 1-2-3 are not described in sufficient detail and are very confusing and hard to understand. In particular, it is not clear how Figure 3 is generated.\n\nThe Related Work section is also way too short, missing a lot of references on representation results for Transformers on Markov data. I recommend the authors to carry out a more extensive literature research and include the important missing works. A few essentials, among others:\n- Edelman et al., \"The evolution of statistical induction heads: In-context learning markov chains\", 2024.\n- Makkuva et al., \"Attention with markov: A framework for principled analysis of transformers via markov chains\", 2024.\n- Bietti et al., \"Birth of a transformer: A memory viewpoint\", 2023.\n- Zhou et al., \"Transformers learn variable-order Markov chains in-context\", 2024.\n\nI also advise the authors to revise the whole manuscript once more, as it is littered with typos and English language mistakes."}, "questions": {"value": "I would like the authors to explain better how the data generation works in Section 2. I would also like a detailed explanation on how Figures 1-2-3 are generated and what they represent."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BFkmdAQfdk", "forum": "CtwyBsbvOA", "replyto": "CtwyBsbvOA", "signatures": ["ICLR.cc/2026/Conference/Submission16558/Reviewer_dU2H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16558/Reviewer_dU2H"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16558/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762240128397, "cdate": 1762240128397, "tmdate": 1762926637713, "mdate": 1762926637713, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}