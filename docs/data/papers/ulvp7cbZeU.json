{"id": "ulvp7cbZeU", "number": 20563, "cdate": 1758307426783, "mdate": 1763621717737, "content": {"title": "Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability", "abstract": "Language model post-training has enhanced instruction-following and performance on many downstream tasks, but also comes with an often-overlooked cost on tasks with many possible valid answers. We characterize three desiderata: in-context steerability, valid output space coverage, and distributional alignment, and document across three model families how post-training can reduce these properties. In particular, we disambiguate between two kinds of in-context learning: ICL for eliciting existing underlying knowledge or capabilities, and in-context steerability, where a model must use in-context information to override its priors and steer to a novel data generating distribution. To better evaluate and improve these desiderata, we introduce Spectrum Suite, a large-scale resource compiled from $>40$ data sources and spanning $>90$ tasks requiring models to steer to and match diverse distributions. We find that while instruction-tuning helps elicit underlying capabilities and models, it hurts a model’s ability to flexibly steer in-context. To mitigate these issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite to improve steerability and distributional coverage. We find that Spectrum\nTuning often improves over pretrained models and their instruction-tuned counterparts, enhancing steerability, spanning more of the out-\nput space, and improving distributional alignment on held-out datasets.", "tldr": "We contribute a novel dataset and post-training method to improve in-context steerability and distributional alignment and coverage, and characterize weaknesses to current post-training techniques along these desiderata.", "keywords": ["post-training", "language models", "distributional learning", "alignment", "pluralistic alignment", "uncertainty estimation"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/83dae456a4ccd07f9c173c0a594840c912d7f8e1.pdf", "supplementary_material": "/attachment/76d306ed5d4ba4e1c0afaf1a7ea9434f8381f4ad.pdf"}, "replies": [{"content": {"summary": {"value": "The paper investigates how the post-training of models (through instruction-tuning) negatively affects a specific set of desiderata, such as the ability of the in-context examples to steer the large language model towards a specific perspective, or diversity in the outputs for tasks with many valid responses. The authors prepare a dataset of tasks, called Spectrum Suite, to evaluate these desiderata and compare them between base and instruction-tuned models, finding that instruction-tuning breaks the steerability of LLMs for the generation tasks (while increasing performance for classification tasks or tasks with one/few specific valid answers). Building on this dataset, the authors introduce Spectrum Tuning, a post-training approach that provides the benefit of instruction-tuning withouth breaking the LLM desiderata (i.e., steerability and diversity of outputs)."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "The paper deals with important problem and I believe it can lead to many possibilities for intersting discussions and future work not only in post-training, but also in areas of interpretability.\n\nThe authors run an extensive set of experiments, providing a comprehensive evaluation on \"failure\" modes of instruction-tuned models, while proposing a solution for mitigating it. In addition, authors provide important resources (Spectrum Suite dataset)."}, "weaknesses": {"value": "**Paper structure/writing**\n\nMy biggest and only issue with the paper is how it is structured and the overall presentation. Overall, the paper is not really well put together -- although all the information is there, it is kind of \"all over the place\" and there are many explanations of motivation, connections between ideas, and details missing.\n\nFirst of all, the abstract and introduction do not really introduce or motivate the problem and it is not really clear why it is important to deal with it -- only 2 sentences are dedicated to this (Liens 33-37) and afterwards the contributions and the description of the desiderata is introduced. I would suggest expanding the motivation in introduction and abstract as it would improve the paper quite a lot. In addition, I would suggest having a dedicated section for the Desiderata and providing more in-depth description or discussion on them. Similar problems are in the dataset and method description, which are quite short and deserve more detailed description and explaining the motivation behind them.\n\nFor the experiments section, the details for how the individual metrics are calculated are missing -- for example, there are lot of references to \"yield\" in Section 4, which is not explained and makes it more problematic to understand the results. Similar case is with \"diversity\" or \"calibration\" as it is not clear how these metrics were calculated. In addition, when performing Spectrum tuning and comparing with instruction-tuned models, it is not clear whether the evaluation is done on a completely different set of tasks or not.\n\nFurthermore, almost all of the figures are quite small and full of information, making them hard to read and easily understand -- for example, in Figure 4 the legend (what the colours mean) is small and very well hidden.\n\n(minor) \n\nLines 89-90 \"see 1 above\" -- it is not really clear what this is referring to; is it the \"exhibit natural person-to-person variation\" from the list at the beginning of the section?\n\nAppendix D seems to be empty, or at least does not refer to any additional figures\n\n\n\n**I acknowledge that many of the misunderstandings and difficulties understanding the paper are a result of limited space and can be easily fixed during rebuttal period, and will update my score if addressed**"}, "questions": {"value": "See weaknesses for details, but mostly relate to details:\n\nHow are the metrics, such as \"yield\", \"calibration\", and \"diversity\", calculated?\n\nIs the Spectrum tuning trained on one set of tasks and then evaluated (and compared with instruction-tuning) on a separate, unseen set of tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XRTzedwtGY", "forum": "ulvp7cbZeU", "replyto": "ulvp7cbZeU", "signatures": ["ICLR.cc/2026/Conference/Submission20563/Reviewer_nA29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20563/Reviewer_nA29"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760692344545, "cdate": 1760692344545, "tmdate": 1762933977252, "mdate": 1762933977252, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper investigates how the post-training of models (through instruction-tuning) negatively affects a specific set of desiderata, such as the ability of the in-context examples to steer the large language model towards a specific perspective, or diversity in the outputs for tasks with many valid responses. The authors prepare a dataset of tasks, called Spectrum Suite, to evaluate these desiderata and compare them between base and instruction-tuned models, finding that instruction-tuning breaks the steerability of LLMs for the generation tasks (while increasing performance for classification tasks or tasks with one/few specific valid answers). Building on this dataset, the authors introduce Spectrum Tuning, a post-training approach that provides the benefit of instruction-tuning withouth breaking the LLM desiderata (i.e., steerability and diversity of outputs)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper deals with important problem and I believe it can lead to many possibilities for intersting discussions and future work not only in post-training, but also in areas of interpretability.\n\nThe authors run an extensive set of experiments, providing a comprehensive evaluation on \"failure\" modes of instruction-tuned models, while proposing a solution for mitigating it. In addition, authors provide important resources (Spectrum Suite dataset)."}, "weaknesses": {"value": "**Paper structure/writing**\n\nMy biggest and only issue with the paper is how it is structured and the overall presentation. Overall, the paper is not really well put together -- although all the information is there, it is kind of \"all over the place\" and there are many explanations of motivation, connections between ideas, and details missing.\n\nFirst of all, the abstract and introduction do not really introduce or motivate the problem and it is not really clear why it is important to deal with it -- only 2 sentences are dedicated to this (Liens 33-37) and afterwards the contributions and the description of the desiderata is introduced. I would suggest expanding the motivation in introduction and abstract as it would improve the paper quite a lot. In addition, I would suggest having a dedicated section for the Desiderata and providing more in-depth description or discussion on them. Similar problems are in the dataset and method description, which are quite short and deserve more detailed description and explaining the motivation behind them.\n\nFor the experiments section, the details for how the individual metrics are calculated are missing -- for example, there are lot of references to \"yield\" in Section 4, which is not explained and makes it more problematic to understand the results. Similar case is with \"diversity\" or \"calibration\" as it is not clear how these metrics were calculated. In addition, when performing Spectrum tuning and comparing with instruction-tuned models, it is not clear whether the evaluation is done on a completely different set of tasks or not.\n\nFurthermore, almost all of the figures are quite small and full of information, making them hard to read and easily understand -- for example, in Figure 4 the legend (what the colours mean) is small and very well hidden.\n\n(minor) \n\nLines 89-90 \"see 1 above\" -- it is not really clear what this is referring to; is it the \"exhibit natural person-to-person variation\" from the list at the beginning of the section?\n\nAppendix D seems to be empty, or at least does not refer to any additional figures\n\n\n\n**I acknowledge that many of the misunderstandings and difficulties understanding the paper are a result of limited space and can be easily fixed during rebuttal period, and will update my score if addressed**\n\n\n**UPDATE:** During the rebuttal process, the authors have addressed all of the above-raised concerns, significantly improving the readability and overall presentation of the paper. As a result, I am increasing the presentation and overall score, as I believe it provides interesting findings that will lead to fruitful discussion and future work."}, "questions": {"value": "See weaknesses for details, but mostly relate to details:\n\nHow are the metrics, such as \"yield\", \"calibration\", and \"diversity\", calculated?\n\nIs the Spectrum tuning trained on one set of tasks and then evaluated (and compared with instruction-tuning) on a separate, unseen set of tasks?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XRTzedwtGY", "forum": "ulvp7cbZeU", "replyto": "ulvp7cbZeU", "signatures": ["ICLR.cc/2026/Conference/Submission20563/Reviewer_nA29"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20563/Reviewer_nA29"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760692344545, "cdate": 1760692344545, "tmdate": 1763645185051, "mdate": 1763645185051, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the scenario where questions in a task have no strict correct answers, and a model needs to give an answer that matches the distribution of a series of examples. The experiments show that instruction-tuned models perform worse than the pretrained models under this setting, and that a model specifically trained for this task performs better than pretrained models. They also find that the trained model achieves a better diversity-quality trade-off in out-of-domain questions that have multiple possible answers."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. It is the first paper that considers the in-context steerability tasks\n2. It provides insightful results about the degradation of IT models on this specific task\n3. It provides a large-scale dataset for the task"}, "weaknesses": {"value": "1. Experiment setting seems over-complicated. It does not make sense to me to evaluate on the first few outputs, if the purpose is to evaluate in-context steerability.\n2. Section 4 is missing an ablation study on temperature. It is claimed that the spectrum tuned models is Pareto optimum comparing to IT tuned models with temperature 1. But it would clearly make more sense if you use a couple of different temperature values for the IT models and see whether spectrum tuned model with a specific temperature is better than all of them, or plot a curve for both methods if the advantage is not that obvious.\n3. Missing a baseline that instruction-tunes the model on the spectrum-suite training set. The paper only shows that a model, trained on the spectrum-suite training set with spectrum tuning, performs better on the spectrum-suite test set, comparing to models that are not trained on this dataset (PT and IT). This does not demonstrate the effectiveness of spectrum tuning."}, "questions": {"value": "1. Are the instruction-tuned models taken from some existing checkpoints?\n2. In the main experiment, how does the spectrum-tuned model perform under zero-shot settings? (i.e., we only evaluate on the first output)\n3. What is the rationale in changing system/user/assistant to description/input/output?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "po9mtNpZST", "forum": "ulvp7cbZeU", "replyto": "ulvp7cbZeU", "signatures": ["ICLR.cc/2026/Conference/Submission20563/Reviewer_HCf6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20563/Reviewer_HCf6"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855590941, "cdate": 1761855590941, "tmdate": 1762933976860, "mdate": 1762933976860, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper argues that standard instruction-tuning improves instruction following but harms three properties that matter when many outputs are valid: (i) in-context steerability, (ii) diversity and coverage of the valid output space, and (iii) distributional alignment. It introduces SPECTRUM SUITE (>40 sources, >90 tasks) formatted as description/input/output sequences, and SPECTRUM TUNING, a simple SFT variant that incoperates task description and ICL examples into the training process. Empirically, instruction-tuned (IT) models are strong on validity but collapse in diversity, while pretrained (PT) models are diverse but under-valid; Spectrum-tuned models raise yield and often give Pareto-style gains on diversity–validity and improve calibration and JS-divergence vs. PT on several held-out datasets, without degrading standard capabilities."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper tackles fundamental, challenging questions in the LLM post-training stage, most notably the loss of diversity after instruction tuning.\n\n2. The experimental evaluation is comprehensive, incorporating human judgments in key sections; several empirical observations are novel and likely of broad interest.\n\n3. The writing and presentation are clear, crisp, and well-structured.\n\n4. Spectrum Tuning delivers notable gains over instruction tuning. While the source of improvement (training paradigm vs. dataset) is not fully disentangled, the work meaningfully advances the SFT stage."}, "weaknesses": {"value": "**General problems**\n\n1. Technical and implementation details are insufficient. Please specify training configurations, the datasets used, and key statistics. **Most importantly, did instruction-tuning and Spectrum Tuning use the same dataset, or datasets of comparable scale?** Otherwise, the gains could be due to a better dataset rather than Spectrum Tuning itself.\n\n2. While the paper addresses important problems and paints a broad picture, many of the issues are largely orthogonal. A unified, principled analysis is missing to explain why these seemingly orthogonal issues can be resolved by a single approach.\n\n**In-context learning**\n\n1. ICL provides training-free adaptation to downstream tasks, often to enforce output format without SFT. If the model is already fine-tuned for the target task, is ICL still necessary? A fine-tuned model should already know the required format and task specifics. I recommend authors to look for more justifications (through existing studies perhaps) for this aspect.\n\n2. The baseline results are puzzling: the instruction-tuned LLM consistently underperforms the raw pre-trained model. This contradicts prior studies and common community expectations.\n\n**Diversity and Space Coverage**\n\n3. The diversity concerns of conventional SFT are valid, but substantial prior work has addressed this area. The paper does not adequately cite, discuss, or compare with approaches such as rejection fine-tuning (RFT) [1] and entropic distribution matching [2], among others.\n\n**Distributional alignment**\n\n4. This section reports results without explaining why Spectrum Tuning improves distributional alignment. Is the effect due to broader data coverage, the addition of task descriptions and ICL examples, or something else? If the latter, why would the instruction-tuned model’s probability distribution be spikier than Spectrum Tuning’s?\n\n[1] Scaling Relationship on Learning Mathematical Reasoning with Large Language Models\n\n[2] Entropic distribution matching for supervised fine-tuning of LLMs: Less overfitting and better diversity, ICLR 2025."}, "questions": {"value": "1. It is not very straight forward for me why adding description embedding before the instruction can improve the diversity of the generation results."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XgX7iHcb39", "forum": "ulvp7cbZeU", "replyto": "ulvp7cbZeU", "signatures": ["ICLR.cc/2026/Conference/Submission20563/Reviewer_suLi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20563/Reviewer_suLi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997040635, "cdate": 1761997040635, "tmdate": 1762933976479, "mdate": 1762933976479, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses three important properties of language models for adaptable inference. (1) In-context steerability: the ability to adapt to new distributions given ICL examples. (2) Valid output space coverage: generating diverse yet valid responses, and (3) Distributional alignment: matching a target output distribution (Calibration). To test these three properties (especially in-context steerability), the author first constructs the datasets Spectrum Suite, which contains data that includes natural person-to-person variations that requires the model to adapt to certain distribution given ICL examples. Using this dataset, the author found that instruction tuning, while gives good ICL elicitation and with high valid response rate, will hurt the in-context steerability and output diversity. The author further proposed the Spectrum Tuning paradigm, which let the model learn via predicting each of the sequential ICL outputs to achieve better generalizbility. The proposed methods shows improvement on all of the three properties, showing potential of increasing inference adaptability of language models with this new training paradigm."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The problem definition of this paper is interesting. While most papers focus on direct comparison of accuracy, the authors proposed the three properties of output that largely impact the user experience in real-world, which is often lack discussed in the benchmark results.\n2. The authors proposed the ICL steerability and elicitation with clear definition. The motivation of why ICL steerability is important for inference adaptation is also clear.\n3. The Spectrum Tuning method is extremely simple yet effective following the experiment results. Showing improvement on all diversity, ICL steerability and calibration."}, "weaknesses": {"value": "Overall I think the paper well demonstrated the effectiveness of the Spectrum Tuning method. However, there're certain points about the experiment that lack clarification.\n\n1. Spectrum Suite is an important dataset for this paper, since the author use it to evalaute the three properties. However, how this dataset is constructed is not very clear, which can hurt the soundness of the experiment results. Specifically, at line 78 to 86, how the author identified those subjective tasks is unclear. This is important in the sense that the author uses these tasks to evaluate the ICL steerability.\n2. While the reason how Spectrum tuning shows improvement on ICL steerability is trivial, how it help with better diversity and calibration is unclear to me. Can the author proposed some justification about this?\n3. Following 2, while Spectrum Tuning models show better diversity compare to Instruction tuning in Figure 4, the validity is lower than instruction tuning. This indicates that Spectrum tuning might not overall be a superior tuning mechanism for the valid-diveristy rate, since it can just be a model that is underfit and has higher diversity compared to instruction tuning."}, "questions": {"value": "1. Transition from line 53 to 54 is abrupt. Shouls add a few sentences to talk about the motivation of choosing to investigate these abilities instead of jumping right into it."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BChozTUWUX", "forum": "ulvp7cbZeU", "replyto": "ulvp7cbZeU", "signatures": ["ICLR.cc/2026/Conference/Submission20563/Reviewer_zhPy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20563/Reviewer_zhPy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20563/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044522504, "cdate": 1762044522504, "tmdate": 1762933976125, "mdate": 1762933976125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}