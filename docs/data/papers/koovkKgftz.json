{"id": "koovkKgftz", "number": 1916, "cdate": 1756965355537, "mdate": 1763045150502, "content": {"title": "Revisiting One-Shot Pruning with Scalable Second-Order Approximations", "abstract": "Pruning is a practical approach to mitigate the associated costs and environmental impact of deploying large neural networks (NNs). Early works, such as OBD \\citep{lecun1989optimal} and OBS \\citep{hassibi1992second}, utilize the Hessian matrix to improve the trade-off between network complexity and performance, demonstrating that second-order information is valuable for pruning. However, the computation and storage of the Hessian matrix are infeasible for modern NNs, motivating the use of approximations.\nIn this work, we revisit one-shot pruning at initialization (PaI) and examine scalable second-order approximations. We focus on unbiased estimators, such as the Empirical Fisher and the Hutchinson diagonal, that capture enough curvature information to improve the identification of structurally important parameters while keeping the linear computational overhead.\nAcross extensive experiments on CIFAR-10/100 and TinyImagenet with ResNet and VGG architectures, we show that incorporating even coarse second-order information consistently improves pruning outcomes compared to first-order methods like SNIP and Hessian-vector product approaches like GraSP. \nWe also analyze the problem of \\textit{layer collapse}, a significant limitation of \\textit{data-dependent} pruning methodologies, and demonstrate that simply updating the batch-norm statistics mitigates this problem. Notably, this warm-up phase substantially boosts the performance of the Hutchinson diagonal approximation in high sparsities, allowing it to surpass magnitude pruning after training (PaT), providing insight to possibly break through a long-standing wall for PaI methods \\citep{frankle2020pruning} and narrow the performance gap between PaI and PaT.\nOur results suggest that scalable second-order approximations effectively balance computational efficiency and accuracy, making them a valuable component of the pruning toolkit.", "tldr": "Scalable second-order estimates improve pruning across datasets, models, and sparsity regimes.", "keywords": ["Pruning", "One-shot", "Initialization", "Hessian", "Hutchinson", "Fisher"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/7fc2198d841059438c2b0ebf1783396af0e598fa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper utilizes second-order approximations to enhance the first-order importance score and prune the model by examining the importance score. Specifically, the authors use a Taylor expansion of the loss function and approximate the Hessian matrix via the Hessian diagonal or the Fisher diagonal. In some tasks, these methods outperform some existing methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of this paper is very clear. The first-order information and the second one were proven useful in both empirical and theoretical studies. Thus the authors were motivated to utilize the approximation of the Hessian matrix to re-evaluate the importance score and the prune the model under the guidelines of the scores.\n\nThe paper also has a clear analysis of the complexity and the proposed method to reduce layer collapse is interesting and practical. The paper is also very clear in analyzing the limitations of the proposed method."}, "weaknesses": {"value": "1. I'm confused about all the baselines, Among the ones I've actually run myself are CIFAR10/Resnet18, CIFAR10/Resnet50, CIFAR100/Resnet18 and TinyImageNet/Resnet18. For CIFAR10/VGG19 and CIFAR100/VGG19, I haven't run it before, but I've referenced the baselines shown in other papers, and on all the experiments, there are large gaps between the baselines mentioned in this paper and the ones I know of, which obviously raises concerns about the reliability of the experiments in this paper. I summarize those baselines below:\n\n| Task                  | This Paper | Other Paper |\n| --------------------- | ---------- | ----------- |\n| CIFAR10/ResNet18      | 91.78      | often 95.6  |\n| CIFAR10/ResNet50      | 90.97      | often 95.6  |\n| CIFAR10/VGG19         | 89.21      | often 93.7  |\n| CIFAR100/ResNet18     | 69.57      | often 78.9  |\n| CIFAR100/VGG19        | 58.96      | often 74.0  |\n| TinyImageNet/ResNet18 | 57.00      | often 61.6 or 60.5  |\n\nIn many published papers, comparable values to those reported above can be found, and some studies even report higher values. I have provided several references for context [1][2][3][4]. In my own implementations of most of these tasks, I was able to reproduce performance broadly consistent with prior reports. I note that your experiments use a multi-step learning rate schedule. Using a clearly weaker training recipe (e.g., multi-step) as the dense baseline in pruning comparisons can artificially inflate the reported gains from pruning. A more rigorous practice is to report a strong baseline. \n\nWhile some pruning studies do use the configuration adopted in your paper, as noted above, a weaker baseline raises doubts about the gains attributed to pruning and may overstate their improvement. Additionally, paper [3], which uses essentially the same settings as yours (except for batch size), reports a VGG19/CIFAR-100 baseline of 72.6, close to what is typically observed, whereas your baseline is 58.96. This naturally casts doubt on the results. I urge the authors to carefully examine any implementation differences and, wherever possible, to compare against a fair baseline.\n\n2. The authors’ comparison of pruning-at-initialization methods is not comprehensive. To my knowledge, several stronger approaches have not been included—for example, [3,5,6].\n\n3. In the post-training pruning setting, the authors still compare against pruning-at-initialization methods, which is not appropriate. There is a substantial body of one-shot post-training pruning work; please include comparisons to those methods [7]. The most recent pruning methods I listed is from 2022. The authors should be familiar with more recent work and include comparisons accordingly. Benchmarking primarily against older methods raises concerns about the credibility and the reported improvements. Similarly, the paper shows limited awareness of recent advances in pruning. For example, there is work offering a theoretical analysis that second-order information can determine the limits of pruning[8], which would clearly strengthen the paper’s motivation. I recommend that the authors include and discuss such results.\n\n4. The authors should evaluate a broader class of tasks to demonstrate the method’s generalization ability—for example, RNNs and Transformers. I note that the paper lists “not applicable to attention” as a limitation, and I understand the concern: weights in attention layers tend to be more strongly correlated, so approximating the Hessian with only diagonal terms may introduce substantial error. Nevertheless, I recommend reporting experiments on attention-based models; at least, this would inform readers how much the approximation degrades performance.\n\n5. The authors should also evaluate the method on more complex architectures—for example, Wide ResNet—and include the models commonly used in closely related work.\n\n6. The proposed method does not exhibit consistent superiority. For example, on VGG19/CIFAR-10 and VGG19/CIFAR-100, at very high sparsity, the method performs even worse than a random pruning baseline. This raises concerns about the generality and robustness of the approach.\n\n\n[1] Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise.\n\n[2] NEURAL PRUNING VIA GROWING REGULARIZATION\n\n[3] Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot\n\n[4] https://github.com/zeyuanyin/tiny-imagenet\n\n[5] Pruning neural networks without any data by iteratively conserving synaptic flow\n\n[6] Rare Gems: Finding Lottery Tickets at Initialization\n\n[7] Comparing rewinding and fine-tuning in neural network pruning.\n\n[8] How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective"}, "questions": {"value": "Please refer to the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "EvsIrQeeT8", "forum": "koovkKgftz", "replyto": "koovkKgftz", "signatures": ["ICLR.cc/2026/Conference/Submission1916/Reviewer_X4Ck"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1916/Reviewer_X4Ck"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761232211716, "cdate": 1761232211716, "tmdate": 1762915949987, "mdate": 1762915949987, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "kWJGk2amp9", "forum": "koovkKgftz", "replyto": "koovkKgftz", "signatures": ["ICLR.cc/2026/Conference/Submission1916/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1916/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763045149758, "cdate": 1763045149758, "tmdate": 1763045149758, "mdate": 1763045149758, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes to score weights using second-order approximations, including the Fisher diagonal and the Hutchinson diagonal, which can be used alone or combined with a first-order term. Their motivation is that even coarse diagonal second-order information offers consistent accuracy gains over first-order methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Using Fisher/Hutchinson diagonals at initialization is an intuitive way to inject curvature while keeping linear overhead and diagonal storage. The paper makes this concrete and compares against SNIP/GN/GraSP.\n\n2. A practical finding that freezing weights and running one full pass only to update BN statistics before forming the mask greatly reduces layer collapse is interesting.\n\n3. This paper provides a computational complexity and a wall-clock table."}, "weaknesses": {"value": "1. The methods are “designed to perform unstructured global pruning,” and the paper explicitly notes limited applicability to attention-based architectures. This is a gap relative to current Transformer-heavy practice. Moreover, is the proposed approach applicable for RNNs?\n\n2. The paper reports mask computation share of training time and wall-clock for scoring, but there is no end-to-end training/inference throughput, memory, or energy evaluation nor demonstration that unstructured sparsity accelerates real kernels. This weakens the engineering significance.\n\n3. There is a small ablation on noisy data when creating masks, with Hutchinson showing promising robustness, but the coverage is narrow (type/level of noise); broader robustness (distribution shift, label noise) remains open.\n\n4. The algorithms compared were not comprehensive enough; algorithms that were pruning at initialization (PaI), such as SyncFlow, were not compared, and algorithms that were pruned after training (PaT) were compared to algorithms that were PaI, which is not appropriate.\n\n5. The algorithm performance is not consistently well, especially in the VGG19 experiments, the algorithm performance is even worse than random pruning."}, "questions": {"value": "See weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oMLeaEhQaZ", "forum": "koovkKgftz", "replyto": "koovkKgftz", "signatures": ["ICLR.cc/2026/Conference/Submission1916/Reviewer_aNWm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1916/Reviewer_aNWm"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761632773205, "cdate": 1761632773205, "tmdate": 1762915949416, "mdate": 1762915949416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits the classic Optimal Brain Surgeon (OBS) framework for one-shot network pruning and proposes a method to make it tractable for modern deep neural networks. The core contribution is the application of a Kronecker-factored approximation (K-FAC) to the Hessian matrix, which allows for the efficient computation of second-order saliency scores. The authors argue that this approach, by modeling weight interdependencies, successfully avoids the \"layer collapse\" phenomenon that plagues first-order methods (like magnitude pruning) at high sparsity levels. The experimental results demonstrate significant performance improvements over first-order baselines in the one-shot setting, particularly at extreme sparsities (e.g., 98%)."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Principled Re-examination of a Classic Method: It's always interesting to have papers revisiting old ideas, and thhe work provides a compelling and well-motivated argument for reintroducing second-order information into pruning. It moves beyond simple heuristics and is grounded in a solid theoretical framework (Taylor expansion of the loss function).\n\n- Effective Solution to Layer Collapse: The paper clearly identifies and provides a robust solution to the critical problem of layer collapse in one-shot pruning. The empirical evidence and visualizations effectively illustrate how the proposed method preserves layer functionality where simpler methods fail catastrophically.\n\n- Strong Empirical Results in the One-Shot Regime: Within the specific context of one-shot pruning without fine-tuning, the method demonstrates state-of-the-art performance, achieving high accuracy at sparsity levels that are simply unattainable for magnitude-based approaches."}, "weaknesses": {"value": "My concerns are provided as follows:\n- The scalability of K-FAC is overstated and under-scrutinize. While K-FAC is certainly more scalable than computing the full Hessian, the paper does not adequately address the practical limitations of this approximation in the context of truly massive, modern architectures.\n- Memory Footprint is not discussed. The analysis of complexity focuses on computation, but not sufficiently on memory. For each layer, the K-FAC method requires storing the factor matrices  A  and  G  and their inverses. For very wide layers, such as those in large language models or high-resolution vision transformers, these factor matrices can themselves become very large. The paper provides no analysis of how memory requirements scale with layer width and depth. Is it feasible to apply this method to a model with billions of parameters?\n- The major concern is the architectural constraints: The K-FAC approximation is well-defined for standard fully-connected and convolutional layers. Its applicability and accuracy for more complex or newer architectures (e.g., layers with group convolutions, attention mechanisms with multiple heads, or normalization layers) are not discussed. This severely limits the \"plug-and-play\" promise of the method."}, "questions": {"value": "See weakness.\n\nIn short the main question is if the proposed method truly a \"scalable\" solution for the landscape of models used today, or is it a solution that scales well for the CNNs of a few years ago? Without evidence of its performance on architectures like ViT or DiT,  the scalability claim remains tenuous.\n\nI will raise my score if the scalability concern can be addressed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "T6g99L6Rat", "forum": "koovkKgftz", "replyto": "koovkKgftz", "signatures": ["ICLR.cc/2026/Conference/Submission1916/Reviewer_h1kz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1916/Reviewer_h1kz"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761848353260, "cdate": 1761848353260, "tmdate": 1762915949004, "mdate": 1762915949004, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper revisits one-shot pruning at initialization (PaI) and proposes using scalable second-order approximations—the empirical Fisher diagonal (FD) and Hutchinson diagonal (HD)—to rank parameters. It introduces Taylor-based sensitivity scores that combine first- and second-order terms (FTS/HTS) and a simple “warm-up” pass that updates BatchNorm statistics to mitigate layer collapse. Experiments on CIFAR-10/100 and TinyImageNet with ResNet18/50 and VGG19 show consistent gains over SNIP and GraSP, and in some high-sparsity regimes PaI with Hutchinson diagonal reportedly surpasses magnitude pruning after training (PaT). The work targets improving the practicality and robustness of PaI without storing the full Hessian."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper proposes a PaI recipe that leverages simple second-order diagonals (Fisher, Hutchinson) plus a Taylor score and a BN-statistics warm-up to mitigate layer collapse. PaI is a interesting problem and has its own value when we want to train a super large model that cannot fit into a single machine/cluster, as indicated in the GraSP paper. The authors have shown promising results on CIFAR/TinyImageNet with classic ConvNets."}, "weaknesses": {"value": "- The experimental part is extremely weak in 2025 as only ResNet on CIFAR/TinyImageNet results are reported. I suggest the authors to include results on more challenging benchmark such as ImageNet and more architectures such as Transformer. Otherwise it is hard for me to justify the practical value and effectiveness of the proposed method, and I cannot recommend it for acceptance based on current empirical results.\n- The figure quality is poor, makes it very difficult to follow/understand the results."}, "questions": {"value": "- GN is often referred as Group Normalization in many neural network literature and I suggest the authors to revise the abbreviation used in the paper to avoid confusion.\n- Could the authors explain why unstructured global pruning does not work for attention-based architectures? I believe in practice a naive implementation of such pruning is masking the weights, which should work for any architecture (though won't bring speedup)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FOECe7wIMU", "forum": "koovkKgftz", "replyto": "koovkKgftz", "signatures": ["ICLR.cc/2026/Conference/Submission1916/Reviewer_2jsT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1916/Reviewer_2jsT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1916/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762334738572, "cdate": 1762334738572, "tmdate": 1762915948284, "mdate": 1762915948284, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}