{"id": "aY6RcnTH4v", "number": 21326, "cdate": 1758316271885, "mdate": 1759896928299, "content": {"title": "ReefNet: A Large-Scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification", "abstract": "Coral reefs are rapidly declining due to anthropogenic pressures like climate change, underscoring the urgent need for scalable, automated monitoring. We introduce ReefNet, a large public coral reef image dataset with point-label annotations mapped to the World Register of Marine Species (WoRMS). ReefNet aggregates imagery from 76 curated CoralNet sources and an additional site from Al-Wajh in the Red Sea, totaling (925 K) genus-level hard coral annotations with expert-verified labels. Unlike prior datasets, often limited by size, geography, or coarse labels and not ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global scale to WoRMS. We propose two evaluation settings: (i) a within-source benchmark that partitions each source’s images for localized evaluation, and (ii) a cross-source benchmark that withholds entire sources to test domain generalization. We analyze both supervised and zero-shot classification performance on ReefNet, and find that while supervised within-source performance is promising, supervised performance drops sharply across domains, and performance is low across the board for zero-shot models, especially for rare and visually similar genera, providing a challenging benchmark intended to catalyze advances in domain generalization and fine-grained coral classification. We will release our dataset, benchmarking code, and pretrained models to advance robust, domain-adaptive, global coral reef monitoring and conservation.", "tldr": "ReefNet introduces a large-scale, taxonomically enriched dataset and benchmark for hard coral classification, demonstrating global applicability through rigorous model benchmarking.", "keywords": ["Reef Ecology", "Scleractinian Corals", "Hard Corals", "Marine Science", "Coral Classification", "Image Dataset", "Benchmarking"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7bb7376de7f59c276f90b3bbca1c7bf86bcf4080.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces ReefNet, a coral reef image dataset that unifies data from 76 CoralNet sources and an additional Red Sea subset, containing about 920K genus-level annotations mapped to the World Register of Marine Species (WoRMS). It establishes two standardized benchmarks, i.e., within-source (in-domain) and cross-source (out-of-domain), to evaluate model generalization under domain shift and class imbalance conditions.  The paper’s contributions include dataset standardization and expert verification, the introduction of a Red Sea subset, and empirical evaluation of supervised and zero-shot models on these benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "**1.** The paper presents a coral dataset that integrates multiple data sources under a unified taxonomy aligned with the WoRMS, improving data consistency across heterogeneous origins.\n\n**2.** It establishes two standardized evaluation settings, within-source and cross-source, to assess model performance under both in-domain and domain-shift conditions in coral classification tasks.\n\n**3.** The introduction of a Red Sea subset from an underrepresented region broadens the dataset’s geographic diversity and may support further research on generalization across biogeographic contexts."}, "weaknesses": {"value": "**1. Incremental contribution within the Datasets and Benchmarks scope:** ReefNet mainly aggregates existing CoralNet sources and applies genus-level standardization using WoRMS taxonomy. The inclusion of a Red Sea subset broadens coverage but does not represent a conceptual or methodological advance compared with prior marine datasets such as BenthicNet or CoralNet.\n\n**2. Use of coarse, patch-based annotations:** As the paper itself acknowledges, ReefNet relies on sparse, patch-based point annotations, which are not precise for coral classification because a single patch can contain multiple coral species. This limits the dataset’s ability to support fine-grained visual understanding. In contrast, datasets such as CoralSCOP and CoralVOS provide mask-based annotations that capture pixel-level structure and are better suited for detailed ecological analysis.\n\n**3. Lack of analytical depth in benchmarking:** The experimental results confirm well-known trends, i.e., strong within-domain accuracy but severe degradation under domain shift and weak zero-shot generalization, without providing deeper insight into underlying causes or dataset-specific challenges. \n\n**4. Unclear advantage over existing resources:** Table 1 shows that some prior datasets already offer species-level labels and comparable or larger image volumes. The paper does not convincingly demonstrate how ReefNet enables new research directions beyond those datasets. Although standardized taxonomy and expert re-verification is valuable for ecological data management, these are engineering-level enhancements rather than conceptual advances."}, "questions": {"value": "**1.** Table 1 shows that BenthicNet already provides a larger image volume, species-level annotations, and WoRMS mapping, which suggests a higher level of taxonomic detail and scale than ReefNet. Could the authors clearly explain what advantages ReefNet offers over BenthicNet?\n\n**2.** It would be interesting to see the future work incorporating additional modalities, such as depth data, 3D reconstructions, or temporal sequences, to enhance the dataset’s relevance for representation learning.\n\n**3.** Could the authors clarify the planned release timeline and accessibility details of ReefNet, including availability of preprocessing or benchmark scripts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "hiHoHf4AY1", "forum": "aY6RcnTH4v", "replyto": "aY6RcnTH4v", "signatures": ["ICLR.cc/2026/Conference/Submission21326/Reviewer_yu4R"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21326/Reviewer_yu4R"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761644499275, "cdate": 1761644499275, "tmdate": 1762941696842, "mdate": 1762941696842, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReefNet, a large-scale, WoRMS-mapped dataset and benchmark for genus-level hard-coral classification, aggregating ≈925k point labels from 76 CoralNet sources plus a new Red Sea site. It defines within-source and cross-source splits and evaluates a suite of supervised backbones and zero-shot VLMs/MLLMs, reporting strong in-source results but sharp performance drops under domain shift. Despite useful data aggregation, the submission reads like an engineering/curation report with limited novelty, shallow evaluation, and unclear positioning vs. existing large marine datasets. The scientific contribution falls short of ICLR’s high bar for novelty, rigor, and insight. I recommend strong reject."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ The curation scale and WoRMS alignment (genus-level) are valuable to the community, and the within- vs cross-source protocols are reasonable to study domain shift.\n+ The paper documents a quality-control pipeline (expert review → filtering) and provides detailed split summaries (S1–S4, Test-W).\n+ Baseline runs show the severity of cross-site degradation; the result tables/ablations (e.g., focal vs class-balanced loss) may guide practitioners."}, "weaknesses": {"value": "+ My main concern is the insufficient novelty for ICLR main track. The paper is fundamentally a dataset/benchmark release with conventional baselines (ResNet/EfficientNet/ViTs, CLIP variants) and straightforward loss tweaks; there is no new learning method, no new DG/robustness technique, and limited analysis beyond reporting macro-recall drops. ICLR typically expects algorithmic or theoretical advances or significantly deeper analytical insights than shown here. \n+ Table 1 acknowledges BenthicNet and other resources at global scale and even notes WoRMS support, undermining the “first/most comprehensive” narrative. The manuscript does not convincingly differentiate ReefNet beyond re-mapping CoralNet labels and adding one Red Sea site. A rigorous head-to-head benchmark against BenthicNet subsets or standardized taxonomies is missing.\n+ Initial expert agreement is ~73%, later filtered to 78–92% by removing weak source/genus pairs. This heavy pruning raises questions about selection bias and external validity of the “high-confidence” subset (S2/S4), yet the paper leans on these filtered splits to claim reliability. A careful analysis of what is discarded and how this affects class/geography distributions is not provided. \n+ Macro recall is the primary metric; confidence intervals/seed variation are not reported per model/split in Table 3, and there is little analysis of calibration, per-class long-tail behavior, or OOD diagnostics beyond a brief note and a loss ablation.\n+ No modern domain generalization baselines (e.g., strong DG/DA methods, test-time adaptation, distributionally robust training) appear; conclusions that “models struggle” under shift are unsurprising without a competitive DG suite.\n+ The dataset is hierarchical, but the benchmark does not evaluate hierarchical metrics (e.g., hierarchical precision/recall, taxonomic distance penalties), missing an obvious and informative lens.\n+ I also find the zero-shot/MLLM setup as weak scientifically. The paper prompts Qwen-VL and even uses GPT-4-generated or book-summarized genus descriptions to boost zero-shot numbers. This ad-hoc retrieval/LLM prompting feels methodologically fragile, and the takeaways (BioCLIP best; others low) are not surprising given pretraining corpora. As “results,” they do not elevate the scientific contribution.\n+ I am also very concerned about the licensing/ethics and provenance, as the proofs are thin. ReefNet aggregates from 76 CoralNet sources and scanned books for text; while the paper says releases will comply with licenses, it does not detail per-source license heterogeneity, derivative-work permissions for text, long-term hosting, or update/versioning of a taxonomy that evolves (WoRMS). These are critical for a flagship benchmark."}, "questions": {"value": "+ Can you substantively differentiate ReefNet from BenthicNet and other existing resources via controlled, head-to-head experiments (same taxonomic mapping, same backbones, same splits) rather than narrative comparisons?\n+ The zero-shot section relies on GPT-generated/summarized text to prompt MLLMs. What is the scientific takeaway for ICLR beyond confirming pretraining coverage effects? Can you replace this with a reproducible retrieval method and ablate prompt sources? \n+ Please provide licensing and provenance tables per source (image/text), explicit redistribution permissions, and a versioning policy for future WoRMS taxonomic updates."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "My big ethics concern is the dataset’s licensing and provenance: ReefNet aggregates imagery from 76 CoralNet sources and external texts (incl. book scans/GPT-generated summaries) without a clear, per-source breakdown of redistribution rights, derivative-work permissions, or long-term hosting and versioning policies for a taxonomy that changes over time. This is compounded by heavy expert-filtering that alters class/site distributions without quantifying what was removed, potentially introducing selection bias while still claiming “high-confidence” labels. The zero-shot section’s reliance on GPT-produced or book-summarized genus descriptions further raises reproducibility and copyright risks, and may import unexamined biases from those sources."}, "rating": {"value": 0}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "m2enJOBaAs", "forum": "aY6RcnTH4v", "replyto": "aY6RcnTH4v", "signatures": ["ICLR.cc/2026/Conference/Submission21326/Reviewer_Es6T"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21326/Reviewer_Es6T"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761774594489, "cdate": 1761774594489, "tmdate": 1762941696407, "mdate": 1762941696407, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ReefNet, a large-scale coral reef image dataset aimed at advancing coral classification. The dataset integrates imagery from 76 curated CoralNet sources and an additional Red Sea collection from the Al-Wajh Lagoon. It includes approximately 925k genus-level hard coral point annotations, each mapped to the WoRMS and verified under the supervision of domain experts. Two benchmark settings are proposed: a within-source benchmark for assessing in-distribution performance and a cross-source benchmark for measuring out-of-distribution generalization across distinct reef sites. Experimental results show that models achieve strong within-source performance but experience substantial performance degradation across sources, underscoring the continued difficulty of achieving domain generalization in underwater image classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) Despite the dataset’s considerable ecological and practical value, its contribution to machine learning and computer vision research remains limited. The work primarily centers on dataset construction and benchmarking rather than presenting new learning methods, optimization techniques, or analytical frameworks. For a high-impact venue such as ICLR, where methodological novelty is a key expectation, this focus on resource building may restrict the paper’s broader appeal.\n\n2) Another strong aspect is the taxonomic precision and alignment with a recognized global standard (WoRMS). By offering genus-level labels rather than coarse taxonomic groups, the dataset provides a richer foundation for ecological research and biodiversity modeling."}, "weaknesses": {"value": "1) While the dataset’s ecological value is undeniable, its contribution to computer vision or machine learning is less substantial. The paper focuses on resource creation and benchmarking rather than proposing new modeling techniques or addressing fundamental algorithmic questions. For a top-tier venue like ICLR, where methodological novelty is often prioritized, this may limit the work’s appeal.\n\n2) A major concern is the dataset’s continued reliance on **point-based annotations**, which substantially limit spatial expressiveness. While such annotations are efficient to collect, they fail to capture essential structural cues, including object boundaries, surface textures, and morphological context, that are crucial for dense prediction tasks like semantic segmentation, coverage estimation, or 3D reconstruction. Although ReefNet offers a wider range of semantic labels than existing datasets such as MosaicsUCSD, CoralSCOP, and Coralscapes, which provide **dense mask annotations**, ReefNet’s point annotations inherently constrain downstream applications  and appear less aligned with current computer vision practices emphasizing spatially rich data. The paper would benefit from a clearer and more compelling justification for maintaining a point-based annotation scheme rather than transitioning toward mask- or region-level representations.\n\n3) Furthermore, the findings from the benchmark experiments lack conceptual novelty within the broader field of machine learning. The observation that models experience **performance degradation under cross-source or out-of-distribution scenarios largely reaffirms well-established phenomena**. The paper would be significantly strengthened by a deeper and more domain-specific analysis of why coral classification presents unique challenges compared to the general classification. For instance, discussing how disordered coral morphologies, overlapping growth forms, and environmental factors influence coral visual appearance and finally impact the classification accuracy could yield valuable biological and computational insights. Exploring these aspects would elevate the work beyond a dataset paper, offering a richer understanding of domain shift in ecological imagery."}, "questions": {"value": "My main question regarding this paper is about the limited methodology, superficial analysis and missing explanation of the degraded performance."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "oklZvavWtp", "forum": "aY6RcnTH4v", "replyto": "aY6RcnTH4v", "signatures": ["ICLR.cc/2026/Conference/Submission21326/Reviewer_oa4H"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21326/Reviewer_oa4H"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761801790596, "cdate": 1761801790596, "tmdate": 1762941695973, "mdate": 1762941695973, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper uses a subset of CoralNet and one set of newly collected images to produce a dataset of hard coral point annotations. They convened a team of expert coral annotators to review and refine existing labels. The dataset was split a variety of ways to test in- vs. out-of-distribution performance of a number of fine-tuned CNNs, transformers, and BioCLIP. Performance degraded in the out-of-distribution situation. The authors additionally did some zero-shot tests, noting that BioCLIP did best."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors come up with some interesting out-of-distribution test splits based on where data is collected. The following tests reveal potentially informative structure of the dataset. \n- The quality of the dataset is quite high. The authors went to a great deal of effort to develop a thorough domain expert review process with a team of coral experts. That information is largely found in the supplement and could be fore-fronted in the main text.\n- The writing is reasonably clear, though the naming conventions for the out-of-distribution tests splits are somewhat confusing. \n- The results, especially regarding label consistency of publicly available coral datasets, is an important issue for marine conservation."}, "weaknesses": {"value": "Degradation of model performance in out-of-distribution situations is a well-documented issue, especially in data poor domains. The point is made in papers like the [WILDS benchmark](https://proceedings.mlr.press/v139/koh21a) and the BioCLIP paper itself. The primary contribution of this work seems to be more domain specific, rather than revealing anything novel from a learning perspective. \n\nThe dataset is, for the moment, relatively small with ~300k images containing ~925k point annotations of 44 classes. The authors seem to intend to grow the dataset which may make some of these results more compelling."}, "questions": {"value": "- Can you elaborate on how much overlap there is between the BioCLIP label space and that of ReefNet? \n- Can you contextualize the zero-shot performance of the VLMs on ReefNet relative to other out-of-distribution datasets? This need not require new experiments, just some literature review. \n- How do non-domain specific VLMs perform when fine-tuned on the various data splits?\n- How representative of coral diversity are the 44 genera in ReefNet? This is almost certainly a complicated question, but some indication of what the complete space looks like will be helpful to contextualize how ReefNet fits in. \n- Please expand on the statement at line 482 that ReefNet-trained models can be used to pre-label common genera. What constitutes a 'common genera'? Is there a heuristic to define that in terms of label frequency across regions?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hG8PDf271L", "forum": "aY6RcnTH4v", "replyto": "aY6RcnTH4v", "signatures": ["ICLR.cc/2026/Conference/Submission21326/Reviewer_E5Ys"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21326/Reviewer_E5Ys"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21326/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761923342993, "cdate": 1761923342993, "tmdate": 1762941695161, "mdate": 1762941695161, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}