{"id": "U6z5Y1htbe", "number": 13432, "cdate": 1758217770815, "mdate": 1759897437906, "content": {"title": "Safe Continuous-time Multi-Agent Reinforcement Learning via Epigraph Form", "abstract": "Multi-agent reinforcement learning (MARL) has made significant progress in recent years, but most algorithms still rely on a discrete-time Markov Decision Process (MDP) with fixed decision intervals. This formulation is often ill-suited for complex multi-agent dynamics, particularly in high-frequency or irregular time-interval settings, leading to degraded performance and motivating the development of continuous-time MARL (CT-MARL). Existing CT-MARL methods are mainly built on Hamilton–Jacobi–Bellman (HJB) equations. However, they rarely account for safety constraints such as collision penalties, since these introduce discontinuities that make HJB-based learning difficult. To address this challenge, we propose a continuous-time constrained MDP (CT-CMDP) formulation and a novel MARL framework that transforms discrete MDPs into CT-CMDPs via an epigraph-based reformulation. We then solve this by proposing a novel PINN-based actor–critic method that enables stable and efficient optimization in continuous time. We evaluate our approach on continuous-time safe multi-particle environments (MPE) and safe multi-agent MuJoCo benchmarks. Results demonstrate smoother value approximations, more stable training, and improved performance over safe MARL baselines, validating the effectiveness and robustness of our method.", "tldr": "We propose an epigraph-based CT-MARL framework that coping with continuous-time constrainted MDP problems.", "keywords": ["Continuous-time", "safe multi-agent reinforcement learning", "epigraph"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/111caa85eb9fdd7c124113d319422e0653c13422.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper considers the continuous-time multi-agent reinforcement learning (CT-MARL) problem with safety constraints. To address the problem, the paper presents a framework that transforms discrete MDPs into CT-CMDPs via an epigraph-based reformulation. The problem is then solved with a PINN-based actor-critic method. Empirical results show that the proposed method performs better in continuous-time safe multi-particle environments and safe multi-agent MuJoCo environments."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem considered is interesting. \n\n2. The proposed method shows strong empirical results. \n\n3. Each part of the loss function is well ablated."}, "weaknesses": {"value": "1. The proposed method is not very well motivated and includes some unclear parts. Please refer to Questions. \n\n1. The metric used in the experiment section can be questionable. The paper \"designs the reward as the summation of the minus cost and constraints\", which can be sensitive to the relative scales of the cost and the constraints. I think figures like Figure 4 are clearer than Figure 3. Or the authors can also show the training curves of the cost and the constraints separately. \n\n1. It seems that all the baselines chosen in this paper are designed for discrete-time MDPs. It would be better if some continuous-time methods could be included as baselines. \n\n1. Minor: Some acronyms are not defined clearly in the paper, e.g., PINN, PDE."}, "questions": {"value": "1. How are the decision times $t_k$ selected? Are they given, or does the policy need to decide?\n\n1. Considering the motivation of the proposed method, why does the value become discontinuous when state constraints are violated? Why can this hinder the convergence? Why can the epigraph formulation address this issue? \n\n1. Why does the random sampling of $z$ introduce nonstationary noise and lead to poor convergence? Is this a drawback only in the continuous-time setting? It would be great if the authors could show this theoretically or empirically, or both. \n\n1. It seems that the value function $\\tilde V$ and the advantage function $A$ are conditioned on $z^\\*$, which, to my understanding, can change a lot during training. However, the learned policies are not conditioned on $z^*$. Therefore, it seems that the policy learning target changes constantly during training. Will this cause unstable training?\n\n1. Why $f_\\xi$ and $l_\\phi$ do not have $\\Delta_t$ as input in Equation (16)?\n\n1. Which part of the proposed method is designed specifically for *multi-agent* systems?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "60SsxMYNlu", "forum": "U6z5Y1htbe", "replyto": "U6z5Y1htbe", "signatures": ["ICLR.cc/2026/Conference/Submission13432/Reviewer_cjAT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13432/Reviewer_cjAT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761761548816, "cdate": 1761761548816, "tmdate": 1762924057166, "mdate": 1762924057166, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EPI, a method for solving safe continuous time MARL problems posed as a constrained optimal control problem. An epigraph reformulation is used to handle the state constraints, then a PINN is used to learn the continuous-time value function, where two additional losses are used to improve the PINN training. Empirical results on the MPE and Safe MA-MuJoCo environments demonstrate that the proposed EPI outperforms continuous-time variants of existing safe MARL methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- To the best of my knowledge, the use of the epigraph form for continuous-time safe MARL is novel.\n- The experimental results on MPE show that EPI has strong performance compared to existing methods. On MuJoCo EPI, it is not yet clear how the performance in terms of cost and constraint compare to existing methods."}, "weaknesses": {"value": "- Many details in the experiments section are not very clear (see questions)\n- It is not clear which parts of the proposed method are claimed as novel contributions and which ones are taken from existing methods. For example, are the concepts of the target loss and value gradients claimed to be novel, or were they proposed in previous works and then adapted to the current setting?\n- The experiments on multi-agent MuJoCo seem to only compare the reward (Figure 2). It is not clear how the different methods compare in terms of constraint satisfaction. Something similar to Figure 4 for the multi-agent MuJoCo setting would be appreciated."}, "questions": {"value": "- Line 139: The paper writes that each agent applies a decentralized policy, but each policy seems to take the entire state space as input. Later on in Line 211, the authors write that the decentralized actors “map local observations”, which is confusing.\n- Why do the networks take ∆t as input? Does ∆t remain constant or does it change?\n- What are the weights for the different terms for the PINN learning?\n- How sensitive is the method to the weighting of these terms?\n- What does “overweighting” mean for the ablation in Figure 9? How much are the weights increased by?\n- Algorithm 1, line 16: I’m assuming that z^* is solved for for every x in the rollout?\n- How are the benchmarks “adapted”? The MPE and Safe MA-MuJoCo benchmarks are originally for discrete time.\n- How does the proposed method compare to (discrete) CBF-based methods for safety in MARL, such as [A]? Though it is a discrete-time method, it should still apply to continuous-time environments, similar to how the other discrete-time methods have been adapted.\n\n[A]: Zhang, Songyuan, et al. \"Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal Control.\" ICLR 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GnBb5cRcX9", "forum": "U6z5Y1htbe", "replyto": "U6z5Y1htbe", "signatures": ["ICLR.cc/2026/Conference/Submission13432/Reviewer_qGRY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13432/Reviewer_qGRY"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771813990, "cdate": 1761771813990, "tmdate": 1762924056778, "mdate": 1762924056778, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel framework for safe continuous-time multi-agent reinforcement learning (CT-MARL) using epigraph forms. The authors address the challenge of incorporating safety constraints into CT-MARL, which often leads to discontinuities in value functions that are difficult to handle with traditional methods. They introduce an epigraph-based reformulation that converts discontinuous value functions into continuous ones by augmenting the system with an auxiliary state variable z. This reformulation enables the use of Physics-Informed Neural Networks (PINNs) for stable and efficient optimization in continuous time. The authors prove the existence and uniqueness of viscosity solutions for the epigraph-based HJB PDEs, providing theoretical support for their method. Extensive experiments on continuous-time safe multi-particle environments (MPE) and multi-agent MuJoCo benchmarks demonstrate that their approach outperforms existing safe MARL baselines in terms of both cost reduction and constraint satisfaction. This work offers a robust and effective solution for safe CT-MARL by leveraging epigraph forms and PINNs to handle discontinuities and ensure stable learning in continuous time."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation behind this work is interesting and necessary.\n2. The theoretical proofs provided in this work are comprehensive."}, "weaknesses": {"value": "1. In the section “Related work”, what are the differences between this work and [1]? I feel that this work merely extends the ideas of [1] to the continuous-time scenario. I suggest that the authors restate the contributions of this paper.\n2. In the section “Methodology”: ① The figure in Fig1 is incorrect. According to the description of Algorithm 1, at least three steps are not independent but form an integrated loop. An arrow from Learning to rollout should be added. ② I don't quite understand how equation (7) is transformed into equations (8) and (9). Some additional explanations may be needed here. ③ In the “INNER OPTIMIZATION WITH CRITIC LEARNING” section, are the three loss functions updated independently or jointly? If they are updated jointly, how are their weights defined?\n3. In the section “Experiments”: ① In 4.1 Benchmarks and baselines, the authors mention “In MuJoCo, we adapt several scenarios such as HalfCheetah and Ant into continuous-time versions and introduce randomly placed walls as obstacles.” I don't know how the continuous time is designed here. I think it is not necessary to spend too much space describing this environment. Instead, the differences between the original version and the continuous-time version should be explained. ② “We design the reward as the summation of the minus cost and constraints listed in Appendix C, which directly reflects performance under both objectives.” I don't understand why the reward is designed in this way. Why not design it as a similar independent reward and cost like MACPO? Since this paper mainly focuses on RL in continuous time, it may be helpful to highlight the contributions of this paper by separating the reward and cost. ③ Section 4.3 lists the impact of the choice of z on performance changes, but lacks a visualization of how the z value changes over time. I think it would be useful to add such an experiment to show that the changes in z do indeed affect the final decision.\n\n**Reference**\n\n[1] Zhang, Songyuan, et al. \"Solving Multi-Agent Safe Optimal Control with Distributed Epigraph Form MARL.\" arXiv preprint arXiv:2504.15425 (2025)."}, "questions": {"value": "Please refer to the “Weakness” section for related questions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "Regarding ethical review, I have no concerns."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "W5bZSYzPZM", "forum": "U6z5Y1htbe", "replyto": "U6z5Y1htbe", "signatures": ["ICLR.cc/2026/Conference/Submission13432/Reviewer_GrMo"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13432/Reviewer_GrMo"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761893678087, "cdate": 1761893678087, "tmdate": 1762924056515, "mdate": 1762924056515, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper formulates safe CT--MARL as a continuous-time CMDP and introduces an epigraph variable $z$ to smooth discontinuities, deriving an epigraph-HJB and training a PINN-based actor--critic with a revised inner/outer optimization that computes $z^\\*$ during rollouts. Results are on safe CT-MPE and CT-MA-MuJoCo with ablations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Addresses a real gap: explicit continuous-time safe MARL (state constraints) with a principled epigraph--HJB route and viscosity-solution framing. \n\n2. Clean derivations: Lem.3.1/3.2 and Thm. 3.3 tightly connect DPP to the PDE with the $\\ln\\gamma$ term.\n\n3. Practical training design:  $z^\\*$ computed during training; $z$-independent critics simplify deployment. \n\n4. Broad evaluation \\& ablations:  consistent gains on CT-MPE/MA-MuJoCo; ablations clarify each loss term’s effect (target/VGI $>$ residual)."}, "weaknesses": {"value": "1. Feasibility of the outer problem: Eq.(8) requires $\\max\\{V _{\\text{cons}}(x),V _{\\text{ret}}(x)-z\\}\\le 0$. If $V _{\\text{cons}}(x)>0$, no $z$ is feasible; the paper clips $z^\\*$ to $[z _{\\min},z _{\\max}]$ (Sec.~3.2.2), which does not restore feasibility or preserve the theory.\n\n2. Model-bias vs. safety: residual/advantage (Eqs.~10,17) use learned $f_\\xi,l_\\phi$; PDE guarantees hold for true dynamics but not for the surrogate, and no robustness link to violation rates is provided. \n\n3. “Continuous-time” evaluation clarity: Alg.~1 samples arbitrary decision times and the policy conditions on $\\Delta t$, but there is no systematic $\\Delta t$-sweep or irregular-step stress test in the main results."}, "questions": {"value": "1. When $V_{\\text{cons}}(x)>0$ along a rollout, Eq.(8) has no feasible $z$. What exact rule do you use for $z^\\*$ in this case, and why does the resulting objective still correspond to the epigraph--HJB target used in Lem.3.1/Thm.3.3? \n\n2. How is $\\partial_z\\tilde V$ computed in Eq.~(10) with $z$-independent critics? Do you use a straight-through $-{\\bf 1}$ on the return branch or a smooth approximation to $\\max$? Any stability tricks near the switching set? \n\n3. Given Eqs.~(16)--(17), can you bound the gap between model-PDE residuals and true residuals, or provide stress tests (dynamics mismatch) linking model error to violation rates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "jUMesT4JoN", "forum": "U6z5Y1htbe", "replyto": "U6z5Y1htbe", "signatures": ["ICLR.cc/2026/Conference/Submission13432/Reviewer_VtHr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13432/Reviewer_VtHr"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13432/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762886895825, "cdate": 1762886895825, "tmdate": 1762924056262, "mdate": 1762924056262, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}