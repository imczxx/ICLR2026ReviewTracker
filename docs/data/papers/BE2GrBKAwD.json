{"id": "BE2GrBKAwD", "number": 16701, "cdate": 1758267818493, "mdate": 1759897224056, "content": {"title": "FPTQuant: Function-Preserving Transforms for LLM Quantization", "abstract": "Large language models (LLMs) require substantial compute, and thus energy, at inference time. While quantizing weights and activations is effective at improving efficiency, naive quantization of LLMs can significantly degrade performance due to large magnitude outliers. This paper describes FPTQuant, which introduces three novel, lightweight, and expressive function-preserving transforms (FPTs) to facilitate quantization of transformers: (1) a mergeable pre-RoPE transform for queries and keys, (2) a mergeable transform for values, (3) a cheap, dynamic scaling transform. By leveraging the equivariances and independencies inherent to canonical transformer operation, we designed these FPTs to maintain the model’s function while shaping the intermediate activation distributions to be more quantization friendly. FPTQuant requires no custom kernels and adds virtually no overhead during inference. The FPTs are trained both locally to reduce outliers, and end-to-end such that the outputs of the quantized and full-precision models match. FPTQuant enables static INT4 quantization with minimal overhead and shows SOTA speed-up of up to $3.9\\times$ over FP. Empirically, FPTQuant has an excellent accuracy-speed trade-off—it is performing on par or exceeding most prior work and only shows slightly lower accuracy compared to a method that is up to 29% slower.", "tldr": "FPTQuant enables low bit-width quantization of LLMs using function-preserving transforms that reduce outliers without custom kernels and with marginal inference overhead. It achieves a strong accuracy-speed trade-off.", "keywords": ["Quantization", "Model Efficiency", "LLM", "Static Quantization"], "primary_area": "other topics in machine learning (i.e., none of the above)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/cecb8d8779e61a94ab2771cbfb622af7a5308c82.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper presents FPTQuant, a framework addressing LLM inference inefficiency and quantization-induced performance loss from outliers by introducing three novel lightweight function-preserving transforms (FPTs): a mergeable pre-RoPE transform for queries/keys (commuting with RoPE to fuse into projection weights), a mergeable multihead value transform (independent invertible matrices per head fused into value/output weights), and a pseudodynamic per-token residual scaling transform (normalizing residuals via recursive RMSNorm-derived scales). These FPTs are optimized via local $L_p-norm$ minimization (outlier suppression) and end-to-end student-teacher training (Jensen-Shannon Divergence alignment with full-precision models). FPTQuant enables static INT4 quantization with up to 3.9× speedup over FP models, no custom kernels, and strong accuracy-speed tradeoffs (matching/outperforming QuaRot/SpinQuant across Llama models/tasks like WikiText-2 perplexity), though it has expressivity limits in extreme low-bit settings and limited validation beyond Llama models/NVIDIA GPUs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The three core Function-Preserving Transforms (FPTs) proposed in the paper are designed around the principle of fusibility, enabling seamless integration into existing LLM weights (e.g., query/key projection weights and value/output projection weights) without requiring any custom kernels or introducing noticeable inference overhead. For instance, the Pre-RoPE transform is compatible with RoPE positional encoding and can be fused directly into the query/key weights, while the pseudo-dynamic residual scaling is recursively derived through RMSNorm, achieving zero additional computational cost.\n\n2. The method adopts a two-stage strategy of local optimization followed by end-to-end student–teacher training. In the local optimization stage, an L₄-norm minimization is employed to selectively suppress outliers introduced by different transforms, resulting in a twofold improvement in training convergence speed. The subsequent end-to-end training leverages a Jensen–Shannon divergence loss to align the probability distributions of the quantized model and its full-precision counterpart, thereby mitigating the overfitting issue observed in SpinQuant’s “next-token prediction” loss. Consequently, the proposed approach achieves a 4–5 percentage point improvement in zero-shot commonsense reasoning accuracy over SpinQuant."}, "weaknesses": {"value": "1. This work can be viewed as an extension of SpinQuant and OSTQuant, as it fundamentally focuses on equivalent transformations within Transformer architectures and the corresponding training procedures to approximate such transformations for task-specific optimization. To some extent, the methodological innovation appears limited.\n\n2. Although FPTQuant achieves favorable performance through its local optimization, it should be noted that the substantial number of introduced parameters—such as rotation transformations and affine scaling factors—still entails considerable training costs. While instability is not explicitly mentioned, empirical observations suggest that FPTQuant’s performance is, to some extent, inferior to that of FlatQuant, indicating room for improvement in training stability.\n\n3. The core advantage of FPTQuant lies in its \"mergeable\" transformations, which reduce inference overhead. However, this design inherently constrains the representational capacity of certain transformations. For instance, the Pre-RoPE transformation $T_k$ applied to queries $q$ and keys $k$ adopts a 2×2 block-diagonal rotation matrix to align with the block-diagonal structure of RoPE, thereby precluding flexible cross-channel mixing. In contrast, non-mergeable transformations—such as $R_3$ in SpinQuant and $P_h$ in FlatQuant—leverage on-the-fly computation to enable more expressive, unrestricted channel interactions. This fundamental trade-off between \"mergeability\" and \"expressiveness\" ultimately leads to a compromise in accuracy under extreme quantization scenarios."}, "questions": {"value": "1. Could a theoretical analysis be provided to explain why FlatQuant outperforms FPTQuant on certain metrics? For instance, in Table 2 (row “4-8-4”), I observe that on the Llama3.2-3B-Instruct model, both FlatQuant and QuaRot achieve better performance than FPTQuant. Does this phenomenon suggest that FPTQuant’s effectiveness is sensitive to the weight distribution induced by instruction tuning, or is it more closely related to model scale?\n\n2. Could a comparative analysis of the training costs among FPTQuant, OSTQuant, FlatQuant, and SpinQuant be provided? Specifically, given that FPTQuant incorporates a teacher–student training framework, does this lead to increased GPU memory consumption? Furthermore, does the local optimization strategy proposed in FPTQuant introduce additional hyperparameter tuning overhead during training?\n\n3. In many recent large language model (LLM) architectures, normalization layers are commonly inserted after the query and key (QK) projections to enhance training stability. Consequently, outlier values tend to emerge after the RoPE (Rotary Position Embedding) transformation rather than before it, potentially leading to overflow in the QK matrix multiplication. Has this phenomenon been taken into account—specifically, through post-RoPE processing or an equivalent transformation—to mitigate such numerical instability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H3GZda15HS", "forum": "BE2GrBKAwD", "replyto": "BE2GrBKAwD", "signatures": ["ICLR.cc/2026/Conference/Submission16701/Reviewer_GL33"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16701/Reviewer_GL33"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760847018163, "cdate": 1760847018163, "tmdate": 1762926753823, "mdate": 1762926753823, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This article achieves int4 static quantization through dynamic per-token scaling and pre-RoPE transformation.\nDetailed ablation experiments. The article conducts sufficient ablation on each type of transformation to prove the effectiveness of each component."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This article achieves int4 static quantization through dynamic per-token scaling and pre-RoPE transformation.\n2. Detailed ablation experiments. The article conducts sufficient ablation on each type of transformation to prove the effectiveness of each component."}, "weaknesses": {"value": "1. The innovation of some components in this article is limited. OSTQuant also uses per-head invertible matrices and completes supervision using the full probability labels of the teacher model.\n\n2. This article achieves a similar inference speed to SpinQuant, but its accuracy is lower than that of SOTA models such as FlatQuant, which limits its overall contribution.\n\n3. The typesetting of the paper needs improvement. For example, in Figure 1, the color scheme makes it difficult to read.\n\n4. Figure 2 shows the acceleration performance of LLama-70B, but the entire paper lacks accuracy performance on larger scales such as LLama3-70B. This raises concerns about its feasibility."}, "questions": {"value": "1. Can it report the systematic differences and accuracy performance compared with OSTQuant? Can it supplement the performance on larger scales such as 30/70B?\n2. If the time and overhead of the quantization itself are supplemented and compared with FlatQuant, SpinQuant, and OSTQuant, it will help alleviate my concern about its feasibility.\n3. Did the weight quantization use GPTQ?\n4. Why is it better than FlatQuant under the W4A8 setting, but worse than it under the more challenging W4A4 setting?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "zN0Fw2f8ng", "forum": "BE2GrBKAwD", "replyto": "BE2GrBKAwD", "signatures": ["ICLR.cc/2026/Conference/Submission16701/Reviewer_Hwow"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16701/Reviewer_Hwow"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761657077712, "cdate": 1761657077712, "tmdate": 1762926753416, "mdate": 1762926753416, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the key challenge of performance degradation in LLM quantization caused by large-magnitude outliers, proposing FPTQuant—a method leveraging function-preserving transforms (FPTs) to balance quantization efficiency and model accuracy. FPTQuant introduces three novel lightweight FPTs (mergeable pre-RoPE for queries/keys, mergeable per-head transform for values, dynamic per-token residual scaling) and integrates existing effective transforms (e.g., Hadamard, rotation). These FPTs exploit transformer equivariances/independencies to reshape activation distributions for better quantization compatibility while preserving model function; crucially, most are mergeable into existing weights, adding nearly no inference overhead and requiring no custom kernels. Empirically, FPTQuant enables static INT4 quantization with up to 3.9× speedup over FP models on NVIDIA RTX 3080 Ti. Across Llama-series models and tasks (Wikitext-2 perplexity, zero-shot CSR, MMLU), it matches or outperforms baselines (QuaRot, SpinQuant) and only slightly lags the 29%-slower FlatQuant—excelling particularly in realistic static quantization scenarios (quantizing more intermediate activations) where prior work falls short. Ablations validate individual FPT contributions, and practical guidelines for FPT selection are provided."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Minimal Inference Overhead with Mergeable Transforms. Most FPTs (e.g., pre-RoPE for queries/keys, per-head value transform) can be merged into existing model weights, avoiding extra computational cost or custom kernels during inference, which is critical for practical LLM deployment, especially on edge devices\n2. Leverages transformer equivariances/independencies and two-stage optimization, local L_p-norm minimization + end-to-end student-teacher training to reshape activation distributions, addressing the core issue of outlier-induced quantization error\n3. Enables static INT4 quantization with up to 3.9× speedup over FP models, while matching or outperforming baselines (QuaRot, SpinQuant) on tasks like Wikitext-2 perplexity and zero-shot CSR; it only slightly lags the 29%-slower FlatQuant"}, "weaknesses": {"value": "1. For very challenging setups (e.g., W4A4KV4 on Llama 2 7B), FPTQuant’s accuracy gap with FlatQuant widens, especially in zero-shot reasoning—indicating limitations in handling severe quantization pressure\n2. While inference is lightweight, FPTQuant’s two-stage optimization (local L_p-norm minimization + end-to-end student-teacher training) adds more training steps than simpler PTQ methods (e.g., RTN-opt); even with mergeable transforms, training larger models (e.g., Llama 2 7B) takes longer than baselines like QuaRot \n3. The core theoretical foundation of this paper lies in the concept of Function-Preserving Transformations (FPTs), which are designed to leave model outputs unchanged in the absence of quantization. However, the introduction of a teacher–student training framework disrupts this theoretical closure: in the distillation process, the student model (quantized + equipped with FPTs) is trained to approximate the output distribution of the full-precision teacher model. This fitting process inevitably adjusts the parameters of the FPTs—meaning that even without quantization, these updated FPTs may deviate from their originally function-preserving design. The paper does not provide theoretical or empirical evidence ensuring that the transformations remain equivalent after distillation. Consequently, it remains unclear whether the observed performance improvements stem primarily from the theoretically equivalent transformations or from the subsequent distillation training itself."}, "questions": {"value": "1. The local optimization stage is said to improve end-to-end training stability. However, the paper only provides results for p=4 (following Bondarenko et al., 2024) without ablating other p values (e.g., p=2, p=∞. Why is p=4 optimal for FPTQuant, and how sensitive is the method’s performance to this hyperparameter—especially for models with different activation distributions?\n2. While FPTQuant is tested on Llama-series models and Qwen 2.5 7B, it lacks evaluation on LLMs with distinct architectures (e.g., Mistral with sliding window attention, GPT-4 derivatives) or non-standard activation functions. Given that outlier patterns vary across architectures (as seen in Qwen’s degraded performance), how can the authors claim FPTQuant’s broader applicability without validating it on more diverse model families?\n3. Can a statistical analysis be conducted on the overall time-consuming costs of the FPTQuant method, including both the quantization and distillation training processes, and can the convergence criteria be provided? What advantages does it have over other quantization methods such as FlatQuant? Or can it achieve better convergence performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6d6pHUiJXM", "forum": "BE2GrBKAwD", "replyto": "BE2GrBKAwD", "signatures": ["ICLR.cc/2026/Conference/Submission16701/Reviewer_h3nK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16701/Reviewer_h3nK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761874457149, "cdate": 1761874457149, "tmdate": 1762926752628, "mdate": 1762926752628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a framework called FPTQuant, which improves static quantization in Large Language Models (LLMs) by introducing Function-Preserving Transforms (FPTs).\n\nThe main innovations of the paper include:\n\nPre-RoPE Transform: A mergeable 2×2 scaling-rotation matrix pair is inserted before RoPE, allowing Q/K projections to be quantized while preserving the attention output.\n\nPseudo-Dynamic Residual Scaling: The scaling coefficients of RMSNorm are extended to the residual path to balance the magnitude differences between different tokens, thereby reducing the discreteness of the activation distribution.\n\nMulti-Head Value Transform: An invertible matrix transformation is introduced along the value vector dimension, allowing each head to be trained independently and directly incorporated into the weights.\n\nThis method achieves near-state-of-the-art (SOTA) accuracy with static INT4 quantization, while boosting inference speed by up to 3.9x, without requiring custom operators or kernel modifications. Experiments show that its performance outperforms SpinQuant and QuaRot, and is close to FlatQuant in accuracy."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The pre-RoPE transformation proposed by the authors elegantly solves the merging problem of RoPE in quantization, achieving \"functional equivalence\" through-RoPE, a first in existing methods.\n\n2. Applying RMSNorm to the residual path equivalently introduces an approximate dynamic normalization mechanism, which is beneficial to quantization stability.\n\n3. A comprehensive comparison was conducted on models such as LLaMA 2, LLaMA 3, and 3B instruct, covering various quantization bit widths and static/dynamic schemes. The results consistently outperform or approach mainstream methods.\n\n4. Compared to the dynamic quantization of SpinQuant/FlatQuant (which requires calculating quantization parameters token by token), FPTQuant achieves good accuracy under completely static configuration (outperforming QuaRot and approaching FlatQuant on W4A8KV8). This is very practical for hardware deployment (NPU/Edge)."}, "weaknesses": {"value": "1. Lack of theoretical guarantees for optimization: The rotation matrix of Pre-RoPE is obtained only through local optimization by minimizing the L4 norm; the paper does not provide convergence or global optimality analysis.\n\n2. Lack of distribution validation for residual scaling: Although the authors claim that this mechanism can reduce the amplitude difference between tokens, the paper does not provide activation distribution or outlier visualization, making it difficult to intuitively understand its actual effect.\n\n3. Risk of overfitting when using scale to train quantization coefficients statically: As shown in Table 2, PPL shows significant improvement on Wikitext, but the zero-shot accuracy decreases even more, indicating that scale/zero-point may overfit to the calibration set.\n\n4. Lack of direct comparison with OSTQuant: OSTQuant also uses mergeable rotation and scaling transformations and jointly optimizes their proportion and orthogonality to match the quantized distribution. FPTQuant, as an extension of the same idea, does not conduct direct comparisons, making its relative advantages unclear.\n\n[1]. OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting"}, "questions": {"value": "Please see weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "XYsB4zM2gV", "forum": "BE2GrBKAwD", "replyto": "BE2GrBKAwD", "signatures": ["ICLR.cc/2026/Conference/Submission16701/Reviewer_9RjL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16701/Reviewer_9RjL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16701/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761907673490, "cdate": 1761907673490, "tmdate": 1762926751539, "mdate": 1762926751539, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}