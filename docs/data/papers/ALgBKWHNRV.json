{"id": "ALgBKWHNRV", "number": 5861, "cdate": 1757941541393, "mdate": 1759897949037, "content": {"title": "One-Step Residual Shifting Diffusion for Image Super-Resolution via Distillation", "abstract": "Diffusion models for super-resolution (SR) produce high-quality visual results but require expensive computational costs. Despite the development of several methods to accelerate diffusion-based SR models, some (e.g., SinSR) fail to produce realistic perceptual details, while others (e.g., OSEDiff) may hallucinate non-existent structures. To overcome these issues, we present **RSD**, a new distillation method for ResShift, one of the top diffusion-based SR models. Our method is based on training the student network to produce images such that a new fake ResShift model trained on them will coincide with the teacher model. RSD achieves single-step restoration and outperforms the teacher by a large margin. We show that our distillation method can surpass the other distillation-based method for ResShift - SinSR - making it on par with state-of-the-art diffusion-based SR distillation methods. Compared to SR methods based on pre-trained text-to-image models, RSD produces competitive perceptual quality, provides images with better alignment to degraded input images, and requires fewer parameters and GPU memory. We provide experimental results on various real-world and synthetic datasets, including RealSR, RealSet65, DRealSR, ImageNet, and DIV2K.", "tldr": "", "keywords": ["super-resolution", "diffusion models", "distillation"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/42c8d96d0258ffbcf563ae76855f13f25c4344e7.pdf", "supplementary_material": "/attachment/1723480c1bc441f24dc3ae51240994d3d813da8a.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Residual Shifting Distillation (RSD), a distillation framework for accelerating the ResShift diffusion-based super-resolution model to a single-step inference regime. RSD introduces a trainable “fake” ResShift model to align the joint trajectory distribution of the student-generated data with that of the teacher, in contrast to prior methods (e.g., VSD) that align only marginal distributions per timestep. The method is combined with LPIPS and GAN losses for improved perceptual quality. Extensive experiments on real-world (RealSR, DRealSR) and synthetic (ImageNet-Test, DIV2K) benchmarks demonstrate that RSD outperforms SinSR in perceptual metrics while maintaining competitive fidelity, and significantly reduces computational cost compared to T2I-based methods like OSEDiff and SUPIR."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Clear and principled formulation: The paper provides a well-motivated objective (Eq. 7–9) based on KL divergence over the full reverse trajectory, with a tractable surrogate derived via Proposition 3.1. The distinction between joint vs. marginal distribution alignment is clearly illustrated (Fig. 4, Appendix A).\n2. Comprehensive experiments: The evaluation covers multiple datasets, degradation models, and SOTA baselines across GAN-based, diffusion-based, and T2I-based SR methods. Both quantitative (Tables 1–3, Appendix D–E) and qualitative results (Figs. 3, 5, 9–12) are thorough and convincing.\n3. Strong practical impact: RSD achieves 1-step inference, 174M parameters, and <600MB GPU memory, making it highly deployable. Training is ~3× faster than SinSR due to simulation-free distillation.\n4. Honest discussion of limitations: The authors acknowledge dependence on the ResShift teacher, failure cases on complex textures, and the gap with T2I models in hallucination-rich scenarios (Appendix H).\n5. Excellent reproducibility: Pseudocode (Appendix B), training details (Appendix C), dataset licenses (Table 8), and codebase description are provided."}, "weaknesses": {"value": "1. Incremental theoretical novelty: As acknowledged in Appendix A, RSD is closely related to IBMD (Gushchin et al., 2025) and can be viewed as its discrete, task-specific adaptation to ResShift. The core idea—using an auxiliary model to match joint distributions—has appeared in consistency distillation, diffusion bridges, and related works.\n2. Missing comparison to very recent 1-step SR methods: For example, CCSR (Sun et al., 2024) and TSD-SR (Dong et al., 2024) are mentioned but not included in main tables.\n3. Limited generalizability: The method is tightly coupled to the ResShift architecture (residual shifting, latent-space diffusion). It is unclear whether RSD can be applied to other diffusion frameworks (e.g., I2SB, LDM) without significant redesign.\n4. Training-resolution mismatch in comparisons: RSD is trained on 256×256 crops, while OSEDiff uses 512×512. Although Appendix E shows RSD trained at 512×512, the main results are not fully comparable. This slightly favors T2I methods in perceptual metrics. \n\nFor clarity, the following should be addressed.\nThe paper is generally well organized, but the abstract contains a 76-word sentence that should be split. Figure fonts are smaller than 8 pt and become illegible when printed. A symbol table summarizing latent-space vs pixel-space notation would help readers. Additionally, the phrase “diffusion-based SR models” appears four times within three lines; replacing subsequent instances with “such approaches” or “these methods” would improve fluency.\n\nFor the experiments, the following should be addressed.\n1. The current teacher ResShift was pretrained on 256² crops, limiting its modelling of 512² high-frequency details and causing RSD to lag behind OSEDiff/SUPIR on high-resolution data. Higher-resolution teacher distillation is deferred to future work.\n2. To enable a fairer assessment of the performance gap with T2I-based methods (e.g., OSEDiff), it would be better including in the main experiments an additional set of results where RSD is trained and fully fine-tuned at 512×512 resolution.\n3. To strengthen the empirical evaluation and better position RSD within the landscape of state-of-the-art one-step super-resolution methods, it would be more convincing that the authors include a direct quantitative and qualitative comparison with CCSR (Sun et al., 2024) and TSD-SR (Dong et al., 2024) in the main results."}, "questions": {"value": "1. Could RSD be applied to a non-ResShift diffusion model (e.g., standard DDPM or LDM)? What architectural or training changes would be needed?\n2. In Appendix E, RSD trained on 512×512 still lags behind OSEDiff in CLIPIQA/MUSIQ. Do the authors believe this gap is due to the teacher (ResShift) or the distillation objective?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l051oL3eP0", "forum": "ALgBKWHNRV", "replyto": "ALgBKWHNRV", "signatures": ["ICLR.cc/2026/Conference/Submission5861/Reviewer_mz7A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5861/Reviewer_mz7A"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761808285537, "cdate": 1761808285537, "tmdate": 1762918309145, "mdate": 1762918309145, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents RSD (Residual Shifting Distillation), a one-step distillation method for diffusion-based image super-resolution. The approach trains a student generator to produce images such that a \"fake\" ResShift model trained on these generated images matches the original teacher model. RSD achieves competitive results compared to the 15-step teacher ResShift and claims to outperform existing distillation methods. Experiments are conducted on RealSR, RealSet65, DRealSR, ImageNet, and DIV2K datasets."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper combines a distillation objective that aligns joint distributions rather than marginal distributions at each timestep (as in VSD) with the ResShift architecture.\n2.\tExtensive quantitative results are provided across multiple benchmarks.\n3.\tComprehensive related work discussion and comparison with VSD, SiD, FGM, and IBMD frameworks."}, "weaknesses": {"value": "1.\tMissing baseline method comparison. Appendix A.3 acknowledges that RSD is essentially a \"discrete variant of IBMD,\" yet IBMD is never empirically compared. Given this close relationship, quantitative comparison is essential to validate the claimed improvements from task-specific adaptations. Equation 21 explicitly shows RSD is IBMD applied to ResShift.\n2.\tLimited Novelty. The main contribution is essentially combining ResShift with VSD/IBMD frameworks. Moreover, the authors also acknowledge the relationship to IBMD (Appendix A.3, Lines 1242-1295).\n3.\tMisleading Experimental Claims. The authors claim to \"outperform the teacher by a large margin\" and . However, in Table 1 (RealSR), RSD loses on fidelity metric PSNR. \n4.\tThe paper ignores recent state-of-the-art methods including:\n[1] PiSASR (CVPR 2025)\n[2] TSDSR (Dong et al., 2024)\n[3] InvSR (CVPR 2025)\n5.\tThese omissions weaken the comprehensiveness of the experimental comparison.\n6.\tFID is not reported, despite being widely used in diffusion-based SR papers such as StableSR and OSEDiff. This limits comparability with existing literature.\n7.\tVisual quality does not significantly outperform baseline methods like OSEDiff. For example, differences in Figure 10 (Lines 2295 and 2306) are minimal, which undermines claims of substantial perceptual improvements."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fbtuK8imVB", "forum": "ALgBKWHNRV", "replyto": "ALgBKWHNRV", "signatures": ["ICLR.cc/2026/Conference/Submission5861/Reviewer_kZUH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5861/Reviewer_kZUH"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761868815691, "cdate": 1761868815691, "tmdate": 1762918308934, "mdate": 1762918308934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Residual Shifting Distillation (RSD), a novel one-step distillation framework for image super-resolution (SR) based on the ResShift diffusion model. Unlike previous knowledge distillation approaches such as SinSR, which require running the full teacher model through all diffusion steps, RSD introduces a “simulation-free” design by using an auxiliary fake ResShift model to estimate the teacher–student discrepancy through a theoretically derived loss. The method also combines perceptual and adversarial supervision (LPIPS + GAN losses) to further enhance visual realism. The appendices provide mathematical derivations, ablation studies, algorithmic details, proofs, and extensive visual comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a clear derivation of the RSD loss from a probabilistic perspective, bridging diffusion-based knowledge distillation and joint distribution alignment. The inclusion of Proposition 3.1 and the equivalence to KL divergence (Eq. 9) adds strong theoretical grounding.\n\n2. The proposed “simulation-free” training significantly reduces the computational overhead compared to SinSR, making one-step diffusion models more accessible for real-world scenes.\n\n3. The paper is well-written and well-organized."}, "weaknesses": {"value": "1. The paper does not compare with recent state-of-the-art methods such as AdcSR [1], PiSA-SR[2], CTMSR[3], and TSDSR [4]. This omission makes it difficult to fully evaluate the competitiveness of the proposed method in the context of the latest advances in this field.\n\n2. Table 6 shows a significant decrease in the CLIPIQA score after introducing the GAN loss, but the authors have not discussed this in depth.\n\n3. While the paper mentions the choice of $K=5$ for updating the fake model, it does not delve deeply into how hyperparameter sensitivity (e.g., $\\lambda_1$, $\\lambda_2$, $K$) affects training stability and final performance. More analysis of this sensitivity, including robustness to different configurations is needed.\n\n4. The evaluation of the paper on real-world datasets is limited. Evaluating on larger and more diverse real-world datasets, such as RealLR200 in SeeSR or RealLQ250 in DreamClear, would strengthen the robustness and credibility of the experimental results.\n\n[1] Adversarial Diffusion Compression for Real-World Image Super-Resolution. CVPR 2025\n\n[2] Pixel-level and Semantic-level Adjustable Super-resolution: A Dual-LoRA Approach. CVPR 2025\n\n[3] Consistency Trajectory Matching for One-Step Generative Super-Resolution. ICCV 2025\n\n[4] TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World Image Super-Resolution. CVPR 2025"}, "questions": {"value": "See the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4Qti6fBFb1", "forum": "ALgBKWHNRV", "replyto": "ALgBKWHNRV", "signatures": ["ICLR.cc/2026/Conference/Submission5861/Reviewer_MSib"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5861/Reviewer_MSib"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761933220031, "cdate": 1761933220031, "tmdate": 1762918308617, "mdate": 1762918308617, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"RSD,\" a novel distillation method for the ResShift super-resolution (SR) model, aiming to reduce high computational costs while improving quality over existing acceleration methods like SinSR (unrealistic details) and OSEDiff (hallucinated structures).\n\nThe method involves training a student network to generate images, which are then used to train a \"fake ResShift model.\" The objective is to make this \"fake\" model's performance \"coincide with\" the original teacher model."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The indirect training approach (training a student to generate data to train a proxy model) is a novel contribution to knowledge distillation for generative models."}, "weaknesses": {"value": "The paper’s ambitious claims are not adequately supported by evidence. The core contribution remains vague: the notions of a “new fake ResShift model” and making it “coincide with” the teacher are abstract, lacking mathematical formulation or clear motivation. The statement that a distilled student “outperforms the teacher by a large margin” is implausible without thorough justification—this raises concerns about whether the teacher was properly optimized or if “outperform” is defined using non-standard metrics.\n\nThe abstract also includes unqualified claims (“surpass,” “on par”) without specifying the evaluation criteria (e.g., PSNR, LPIPS, FID). Given the method’s complexity, the authors should consider a simpler baseline—quantifying ResShift directly to obtain a lightweight variant for comparison. Without such analyses, the claimed advantages remain unconvincing."}, "questions": {"value": "See the weekness secton. In summary, the abstract proposes an intriguing and timely solution to a significant problem. However, its credibility is undermined by a vague methodology and exceptionally strong claims that lack clear evidence. The assertion that a student model can dramatically outperform its teacher is extraordinary and demands rigorous proof and explanation."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "0JiSxutLFd", "forum": "ALgBKWHNRV", "replyto": "ALgBKWHNRV", "signatures": ["ICLR.cc/2026/Conference/Submission5861/Reviewer_wG89"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5861/Reviewer_wG89"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5861/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044276176, "cdate": 1762044276176, "tmdate": 1762918308327, "mdate": 1762918308327, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Working on revised text"}, "comment": {"value": "**Author Final Remarks**.\n\nWe thank all reviewers for fruitful discussions. Based on our rebuttal and provided feedback, we are working on the list of changes and additional clarifications for the revised version of our text"}}, "id": "w4p0c67vPP", "forum": "ALgBKWHNRV", "replyto": "ALgBKWHNRV", "signatures": ["ICLR.cc/2026/Conference/Submission5861/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5861/Authors"], "number": 18, "invitations": ["ICLR.cc/2026/Conference/Submission5861/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763728936872, "cdate": 1763728936872, "tmdate": 1763729313703, "mdate": 1763729313703, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}