{"id": "0hmBDnWeEK", "number": 15474, "cdate": 1758251712537, "mdate": 1763465725427, "content": {"title": "A Dual-Branch Disentanglement Diffusion for ID-Attribute Conditional Face Generation", "abstract": "Face identity customization, i.e., face generation with specified identity, has received increasing attention owing to its extensive applications in personalized content creation. Although existing methods achieve high consistency in identity with reference faces, they still struggle to precisely manipulate fine-grained facial attributes. We attribute this issue to the inherent entanglement of identity and attribute information, as well as the lack of attribute-specific supervision. Accordingly, to address this issue, we propose AttPortrait, a high-quality identity-attribute conditional face generation framework. Based on a foundational face diffusion model, we introduce an extra disentanglement branch alongside the conventional denoising branch during the training stage. This extra branch employs explicit attribute supervision to encourage the model to capture the attribute information from the text prompts, effectively disentangling the identity and attributes and achieving precise attribute manipulation with high identity consistency. Comprehensive experiments demonstrate that our method achieves at least 34% improvement in attribute accuracy, attains identity similarity close to the state-of-the-art methods, and maintains comparable FID scores on both real and synthetic datasets.", "tldr": "", "keywords": ["Face Generation", "Face Identity", "Facial Atrributes", "Diffusion Models"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/e2fb6ab343c03fedd29b88f8e83bffd610b9e0d6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes AttPortrait, a dual-branch diffusion framework for identity-attribute conditional face generation. In addition to the standard denoising branch, a disentanglement branch with explicit attribute supervision is introduced to decouple identity and attribute representations.\nThe method improves attribute accuracy over previous approaches, but still have challenges for improvement in realism and the comparison with baseline."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "・Clearly formulates and analyzes the identity–attribute entanglement problem in diffusion-based face generation.\n\n・The dual-branch design effectively separates identity and attribute information, improving fine-grained attribute control.\n\n・The paper is well-structured and easy to follow, with clear motivation."}, "weaknesses": {"value": "・The attribute set used for training and testing only covers a few simple binary traits like bangs, beard, or eyeglasses. That makes it hard to really judge how well the model handles identity drift or disentanglement in general. It would help to include more diverse attributes like expression, pose, or lighting for a fuller evaluation.\n\n・While the model gets better scores on attribute accuracy, some generated faces look a bit too smooth or have that “oil-painting” texture. It feels like there’s a trade-off between keeping attributes accurate and making the image look natural.\n\n・Most of the experiments are done on Arc2Face-based datasets, so it’s not clear how well the method generalizes. Testing on other datasets like DCFace or CemiFace would make the results more convincing.\n\n・Some tricky attributes, especially ones tied closely to identity like Beard or Old, still seem to carry over from the reference image instead of following the target condition. That suggests the disentanglement isn’t fully working yet.\n\n・The model depends on a pretrained attribute classifier for supervision, so any bias or label errors there might affect training. The paper doesn’t really look into how that could impact fairness or robustness, which might be worth mentioning."}, "questions": {"value": "See Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "dhNHQImYgc", "forum": "0hmBDnWeEK", "replyto": "0hmBDnWeEK", "signatures": ["ICLR.cc/2026/Conference/Submission15474/Reviewer_oERR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15474/Reviewer_oERR"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760590623113, "cdate": 1760590623113, "tmdate": 1762925765516, "mdate": 1762925765516, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "6cFxquLrr2", "forum": "0hmBDnWeEK", "replyto": "0hmBDnWeEK", "signatures": ["ICLR.cc/2026/Conference/Submission15474/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15474/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763465724640, "cdate": 1763465724640, "tmdate": 1763465724640, "mdate": 1763465724640, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aim to improve facial attribute \nmanipulation through text prompt when generating customized ID-preserving face images in face image generation models. Two-branch training scheme is proposed for enhancing attribute manipulation. Attribute information and ID information are separately encoded in cross-attention layers for training."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper is well-written and easy to follow.\n2. Authors proposed a dual cross attention (DCA) to learn ID and attribute embedding separately."}, "weaknesses": {"value": "1. Author used own pretrained attribute predictor to generate label for training images. However, there are only limited types of attributes.\n\n2. In Identity and FID Evaluation Protocol, author only generated image with single attribute manipulation for measuring ID similarity and FID. It cannot thoroughly measure ID-preserving ability of model. Moreover, it is also necessary to measure SSIM."}, "questions": {"value": "1. Existing face generation models e.g., Arc2Face, InstantID, they are trained with fixed prompt. \nCan attribute manipulation being enhanced by simply fine-tuning the model with prompts that describing various facial attributes?\n2. How efficient of proposed method in terms of training time, GPU memory and inference speed? \n3. For existing methods, author omitted the work IPA-FaceID. Why this method is not included in comparison?\n4. In Identity-Attribute Disentanglement Branch, \nauthor define attribute matching function loss function using pre-trained multi-attribute classifier. Is this the same one that used in evaluation experiment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "gv43mWuAFp", "forum": "0hmBDnWeEK", "replyto": "0hmBDnWeEK", "signatures": ["ICLR.cc/2026/Conference/Submission15474/Reviewer_WYGJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15474/Reviewer_WYGJ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761834866720, "cdate": 1761834866720, "tmdate": 1762925765109, "mdate": 1762925765109, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a diffusion-based framework for identity-attribute conditional face generation. Authors aim to address the entanglement of identity and attribute information in ID embeddings. During training, a pre-trained attribute classifier provides supervision for attribute accuracy, and an ID similarity loss maintains identity consistency."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The motivation of attribute–identity entanglement is compelling and authors its impact through both visualization and ablation.\n\nExperiments on CelebA-test and Synth-test show that AttPortrait achieves a large improvement in attribute accuracy."}, "weaknesses": {"value": "The idea to disentangle of identity and attribute information has been a long-time research, thus the contribution of this work seems incremental. \n\nThe authors compared with many approaches from before 2020, which is insufficient. It is recommended to include comparisons with more recent generation works.\n\nIn the ablation study, the full component does not show a clear advantage compared to w/o Target Attributes and w/o ID Loss, thus the final design seems more like a trade-off."}, "questions": {"value": "please refer to weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KAScQyh5OY", "forum": "0hmBDnWeEK", "replyto": "0hmBDnWeEK", "signatures": ["ICLR.cc/2026/Conference/Submission15474/Reviewer_Gu6z"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15474/Reviewer_Gu6z"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975944487, "cdate": 1761975944487, "tmdate": 1762925764610, "mdate": 1762925764610, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes AttPortrait, a diffusion-based framework for face generation that aims to disentangle identity and attribute information. The method introduces two branches during training: 1) a denoising branch to ensure high-quality and identity-consistent synthesis, and 2) \na disentanglement branch that uses an explicit attribute matching loss guided by a pretrained attribute classifier.\nA dual cross-attention (DCA) mechanism is also presented to separate ID and attribute signals in the U-Net. The model reportedly improves attribute accuracy by over 30% compared to existing ID-conditional diffusion baselines while maintaining identity similarity and FID performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1) Clear motivation: The paper identifies a valid shortcoming in identity-conditioned diffusion models — limited control over fine-grained attributes. 2) Readable presentation: The method, architecture diagrams, and equations are well-organized and clearly explained.\n3) Comprehensive comparisons: The authors evaluate against several strong baselines (Arc2Face, InstantID, PuLID, etc.) and perform ablation studies."}, "weaknesses": {"value": "Limited novelty / Incremental contribution\nThe proposed framework is largely an engineering combination of existing techniques rather than a fundamentally new approach.\nThe “dual-branch” structure mirrors common multi-loss or auxiliary supervision setups.\nThe “dual cross-attention” is simply two separate cross-attention blocks summed together; this is a minor architectural tweak.\nThe attribute matching loss is a straightforward supervised signal using an off-the-shelf classifier.\nOverall, the contribution feels incremental relative to Arc2Face, IDAdapter, ID3 and PhotoMaker.\n\nA similar concept of ID/attribute was already presented in ID3. \n\nAttribute accuracy is computed using the same classifier that provides supervision — this invalidates claims of improvement and introduces strong confirmation bias.\n\n\nJianqing Xu, Shen Li, Jiaying Wu, Miao Xiong, Ailin Deng, Jiazhen Ji, Yuge Huang, Guodong Mu, Wenjie Feng, Shouhong Ding, Bryan Hooi:\nID3: Identity-Preserving-yet-Diversified Diffusion Models for Synthetic Face Recognition. NeurIPS 2024"}, "questions": {"value": "Since the attribute predictor used for supervision is also used for evaluation, how can we be confident that the improvements reflect true controllability rather than overfitting to the same classifier’s feature space?\nHave the authors tried evaluating with an independent attribute classifier?\n\nThe DCA appears to be two standard cross-attention modules in parallel. Could the authors clarify what distinguishes this from existing multi-condition attention or modulation-based fusion used in prior works like PhotoVerse or IP-Adapter?\n\nHow does the model handle contradictory attributes (e.g., “young” + “gray hair”  or \"Woman\" + \"mostash\"?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cWvwpYdiqf", "forum": "0hmBDnWeEK", "replyto": "0hmBDnWeEK", "signatures": ["ICLR.cc/2026/Conference/Submission15474/Reviewer_j6mv"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15474/Reviewer_j6mv"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15474/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762090713356, "cdate": 1762090713356, "tmdate": 1762925764158, "mdate": 1762925764158, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}