{"id": "fPMu3Afv3s", "number": 21149, "cdate": 1758314302529, "mdate": 1759896939667, "content": {"title": "Bridging Fairness and Explainability: Can Input-Based Explanations Promote Fairness in Hate Speech Detection?", "abstract": "Natural language processing (NLP) models often replicate or amplify social bias from training data, raising concerns about fairness.\nAt the same time, their black-box nature makes it difficult for users to recognize biased predictions and for developers to effectively mitigate them.\nWhile some studies suggest that input-based explanations can help detect and mitigate bias, others question their reliability in ensuring fairness.\nExisting research on explainability in fair NLP has been predominantly qualitative, with limited large-scale quantitative analysis.\nIn this work, we conduct the first systematic study of the relationship between explainability and fairness in hate speech detection, focusing on both encoder- and decoder-only models.\nWe examine three key dimensions: (1) identifying biased predictions, (2) selecting fair models, and (3) mitigating bias during model training.\nOur findings show that input-based explanations can effectively detect biased predictions and serve as useful supervision for reducing bias during training, but they are unreliable for selecting fair models among candidates.", "tldr": "We study whether, and in what ways, input-based explanations can be used to detect biased predictions, select fair models, and mitigate biases during model training.", "keywords": ["Fairness", "Explainability", "Hate speech detection"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2c46abb1ecd23dedafb4d094e5b42bf5659da41d.pdf", "supplementary_material": "/attachment/7123a04e85fd200620cf00fe47eae7300da308d2.zip"}, "replies": [{"content": {"summary": {"value": "The paper presents a quantitative study of the relationship between input-based explanations and fairness of NLP models in the context of hate speech detection. It focuses on three aspects in particular: (1) identifying biased predictions using input-based explanations, (2) selecting fair models based on input-based explanations, and (3) mitigating bias during model training using input-based explanations. The authors find that input-based explanations are effective at detecting biased predictions and reducing biased predictions of NLP models during training but are not suitable as a selection criterion for fair models."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Originality: The paper appears to be the first systematic quantitative study of the relationship between input-based explanations and fairness of NLP models. The paper also investigates a broad set of 14 input-based explanation methods across 4 models (2 encoder-only and 2 decoder-only models) on 2 hate speech detection datasets.\n\nQuality: The metrics used for measurement of bias/fairness are thoughtful and seem to be good approximations of model fairness. While the chosen method is not highly complex, it is well-executed and effective at measuring bias/fairness of models and their predictions and determine suitability of different input-based explanations for detecting model bias.\n\nClarity: The explanation of the approach and its relevance as well as the presentation of results are very clear. All concepts (e.g., sensitive tokens, attribution scores, sensitive token reliance scores, etc.) are properly introduced with both a mathematical definition and a natural language explanation. The overall argumentation and section structure makes sense and has a logical flow, always leaving the reader aware of what (e.g., which research question) is being discussed in any particular paragraph. The paper also makes use of suitable visual elements such as the takeaway boxes briefly summarizing the answers to the research questions.\n\nSignificance: The three research questions raised by the paper are of high significance to the research community and motivate further research into improving model fairness using input-based explanations. The paper also presents a very comprehensive related work section, pointing to other relevant work on related aspects."}, "weaknesses": {"value": "As highlighted in the strengths, the paper is generally very well-written and presents rigorous research, leaving only a few minor weaknesses:\n\nSome figures take a while to fully understand, given the large number of methods, metrics, and settings shown. For instance, in Fig. 2, the caption mentions that no model outperforms the baseline and it is not immediately apparent which bar or line represents the baseline. This becomes clear only after reading the accompanying text mentioning the “individual unfairness on the validation set” as the baseline and then looking back at the figure, where the legend includes one line “Val fairness”. Possibly, the authors could simply adjust the legend entry to “Val fairness (baseline)” to make this clear. Further, the visual style of the “Val fairness” line is different between Figs. 2 and 3 and could be harmonized. Fig. 4 uses the terms “Accuracy” and “Fairness” in both the legend and on the chart axes. One must read the charts very carefully to notice that each subplot has two y-axes, one for accuracy and one for fairness, which follow slightly different scales.\n\nIn section 6, the paper presents the important finding that input-based explanations outperform an LLM-as-a-Judge method on bias detection. This would be a highly significant finding given the current prevalence of LLM-as-a-Judge methods. However, the analysis is only based on a Qwen3 LLM judge (presumably the same 4-billion-parameter Qwen3 LLM used for the other experiments), which is a relatively small LLM and cannot be expected to reach the impressive performance levels of state-of-the-art LLMs. While potential budget constraints might make this choice understandable, it limits the generalizability of the finding to LLM-as-a-Judge and LLMs overall.\n\nAcross most experiments, the study’s results indicate no consistently best method. While this lies in the nature of empirical research, the practical implications of the findings are not always clear. In the conclusion, the authors claim that they “provide practical recommendations on which explanation methods are best suited” (lines 483f.) but the given recommendations fall somewhat short of being practical and useful. As a practitioner or researcher, it is not always clear what method should be used based on the findings of this study. At the very least, I suggest that the authors add a section discussing the study’s limitations and outlining future work to come closer to such practical recommendations."}, "questions": {"value": "Based on the mentioned weaknesses, two questions can be raised to the authors:\n\n1. How can the results from section 6 (i.e., that input-based explanations outperform LLM-as-a-Judge) be expected to generalize to other, particularly larger and state-of-the-art, LLMs?\n\n2. What are the limitations of the study that should be addressed by future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xxHgAQ2iPe", "forum": "fPMu3Afv3s", "replyto": "fPMu3Afv3s", "signatures": ["ICLR.cc/2026/Conference/Submission21149/Reviewer_EN8y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21149/Reviewer_EN8y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761734919456, "cdate": 1761734919456, "tmdate": 1762941507236, "mdate": 1762941507236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper does a very nice job scoping the problem and running a broad comparison of 14 explanation methods across two hate-speech datasets and multiple bias types. It also delivers clear, actionable takeaways for practitioners, notably that certain explanation families (occlusion, L2-like gradients) are good local bias sensors and can supervise debiasing, and that explanation signals can be more useful than an LLM-as-a-judge for this task. However, the core claim the paper wants to make, “input-based explanations can support fairness operations in hate speech detection”, rests on a data and measurement setup that is still quite narrow: two toxicity datasets, fixed identity vocabularies, and counterfactuals built via simple token replacement. Furthermore, the paper itself presents a negative result on model selection but does not delve into why explanations fail in this context, nor does it conduct the stronger causal/feature-removal tests that would strengthen the attribution-model-reliance link."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "S1: This is a relatively large, well-scoped empirical study that ties input-based explanations directly to fairness in hate speech detection, a gap that existing work had mostly treated qualitatively.\n\n\nS2: Clear question framing that makes the contributions easy to map to real moderation pipelines.\n\nS3: Systematic comparison of 14 explanation methods from different families (attention, gradients, occlusion, SHAP), providing an unusually comprehensive view of which types of attributions are useful for fairness.\n\nS4: Consistent positive result for bias detection: occlusion and L2-style gradient attributions correlate well with individual unfairness across settings.\n\nS5: Demonstration that explanation-based regularization can improve several fairness metrics without collapsing task accuracy, showing a concrete way to operationalize the insight.\n\nS6: Practical comparison showing that explanation-based signals can outperform an LLM-as-a-judge approach for this task, which is directly actionable for current LLM pipelines."}, "weaknesses": {"value": "W1: Although the study is broad within hate speech detection, it is still confined to Civil Comments and Jigsaw, with only three bias types and a fixed identity vocabulary, so it is unclear how far the conclusions extend to other NLP tasks or richer identity expressions.\n\nW2: Sensitive-token detection relies on predefined identity terms, so paraphrased, code-mixed, reclaimed, or multiword identity mentions that arise in practice may be undercounted, potentially underestimating model bias.\n\nW3: The individual-unfairness construction depends on counterfactuals built via simple token replacement and balanced subsampling; while workable, this can introduce measurement artifacts that partially blur how much of the signal comes from explanations vs. from the counterfactual setup.\n\nW4: While the paper does include decoder-side experiments and an LLM-as-a-judge comparison, the decoder evaluation is still relatively thin and focuses on smaller instruction-tuned models, so it is uncertain whether the findings hold for larger or differently aligned LLMs.\n\nW5: Sensitive-token reliance is hard-coded to a particular aggregation rule (max over sensitive tokens); alternative aggregations (sum, top-k, span-aware) are not compared, so part of the conclusion may be an artifact of this choice.\n\nW6: There is no human-in-the-loop or practitioner-style evaluation to show that the surfaced explanations are actually interpretable and actionable for fairness analysts."}, "questions": {"value": "Q1: What's the cost/latency for the 14 explanation methods (time per 1k examples, hardware) to support practicality claims?\n\nQ2: Will the authors be able to add a small human/practitioner evaluation (5–10 cases) on whether the explanations are actionable for fairness auditing?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Ge3p8yhMxB", "forum": "fPMu3Afv3s", "replyto": "fPMu3Afv3s", "signatures": ["ICLR.cc/2026/Conference/Submission21149/Reviewer_7Yw8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21149/Reviewer_7Yw8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761833168199, "cdate": 1761833168199, "tmdate": 1762941506622, "mdate": 1762941506622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a systematic study with a focus on the intersection between fairness and explainability, both central topics in AI ethics. Oriented by three clearly defined research questions, the authors explore the utilities of SOTA feature attribution methods, a particular form of explanations, in machine learning fairness for NLP. Based on the empirical observations, in-depth discussions are carried out, which bring together the standard fairness metrics and the new explanation-oriented perspective. The discussions deliver new insights that provide potential guidelines for future practice in debiasing language models."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper presents decent work that is clearly motivated and smoothly unfolds the storytelling, following three research questions that are inherently connected.\n- The experiments are well structured, with reasonable choices for the fairness metrics and a collection of feature attribution methods that represent SOTA.\n- The studies that compare the (traditional) statistical perspective with the explanation-driven perspective are insightful; a conclusion is reached from each research question, presented in the form of takeaways."}, "weaknesses": {"value": "- The experiments mainly rely on existing methodologies. Although the authors bring standard and explanation-driven debiasing techniques to the same table, no further exploration is conducted for possible combinations across categories.\n- While the deliveries are generally insightful, the observations for RQ2 are not sufficiently discussed, rendering a rather hasty conclusion (see the first two points in questions for more details)."}, "questions": {"value": "I do not have particular questions regarding the content, as the presentation of the work is clean and clear. Instead, I would like to establish a discussion with the authors, so the questions below serve more for the purpose of brainstorming.\n\n- What do the authors think about the observations for RQ2 beyond the takeaway? Indeed, there are mismatches between the statistical and attribution perspectives; however, directly determining what metrics are responsible is non-trivial. Particularly, there could be “implicit” biased attributions that are exempted from the standard fairness metrics. By “implicit”, I refer to the predictions that are correct but contain falsely attributed features. The current fairness metrics do not consider such implicit biases as long as the prediction results are balanced among demographic groups. These implicit biases can be a problem when attributions to other features are not strong enough to cover them.\n- Could a cross-validation be helpful in further revealing the mismatches? For example, by quantifying implicit biases in terms of attributions for correctly predicted samples.\n- Could the authors share some thoughts about combining traditional debiasing approaches (the data augmentation perspective) and explanation-driven approaches?\n- Since this paper considers different bias categories, have the authors observed any internal connections among different bias categories? A previous work reported indirect impacts during debiasing [1], and I’m wondering whether that is observed in the broader settings considered in this work. Particularly, does debiasing on one category also suppress other forms of bias? Or, does it transfer the biases to other categories while explicitly regulating for one specific type?\n\nThe presented work appears complete as it is; yet, deepening the investigations and discussions will certainly enhance its impact.\n\n[1] Cai, Yi, et al. \"Power of explanations: Towards automatic debiasing in hate speech detection.\" *2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA)*. IEEE, 2022."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hV5V3dpwtg", "forum": "fPMu3Afv3s", "replyto": "fPMu3Afv3s", "signatures": ["ICLR.cc/2026/Conference/Submission21149/Reviewer_MpuR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21149/Reviewer_MpuR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761910585670, "cdate": 1761910585670, "tmdate": 1762941504200, "mdate": 1762941504200, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper examines the relationship between input-based explanations and fairness in the context of hate speech detection. The authors show that the tested XAI techniques can effectively identify biased predictions in both encoder-only and decoder-only models, and can also assist in guiding bias mitigation."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The paper is well-written and easy to read, providing all the details necessary for reproducibility in the appendices. (On this point, I suggest double-checking that all appendices are properly referenced in the main body of the paper.)\nThe experimental setup is comprehensive, as the authors evaluate multiple models, datasets, XAI methods, and fairness techniques.\nThe research questions addressed in the paper are compelling, and the contribution is well-positioned at the intersection of XAI and fairness, a space where existing literature remains limited."}, "weaknesses": {"value": "Consider expanding the motivation for incorporating both fairness and explainability in hate speech detection, particularly in the Introduction and Related Work sections, to better contextualize the importance of these aspects.\nIt would be helpful to further elaborate on the rationale behind focusing specifically on input-based explanations.\nPlease clarify the distinction between hate speech detection and abusive language detection. While the study focuses on hate speech, the labels in Notations (toxic vs. non-toxic) appear to align more closely with the abusive language perspective. Again in Notations it would be useful to explicitly define what is meant by social bias. Additionally, the sensitive token reliance scores would benefit from clearer motivation and stronger connections to related literature. For instance, is this a new measure introduced by the authors, or is it adapted from prior work? Furthermore, when the hateful nature of a sentence stems directly from a sensitive token, how should reliance on that token be interpreted? Clarifying these aspects would strengthen the reader’s understanding of what the metric captures.\nThe contributions of the paper could be stated more explicitly. If the main contribution is an evaluation framework, it might be helpful to revise Section 3 to better highlight its novelty and unique aspects. Moreover, including a visual overview or workflow diagram of the experimental setup could greatly improve clarity and accessibility.\nConsider adding a short Future Work section to outline possible extensions. Additionally, frequent use of red and green in tables can be visually distracting: using bold text to indicate the best results might improve readability. Some figures could also be refined to enhance their legibility."}, "questions": {"value": "It would be valuable to discuss the generalizability of the proposed approach beyond hate speech detection. To what extent can the methods and findings be applied to other NLP tasks or domains? Clarifying this would help readers better understand the broader applicability and impact of the work."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bhidT68iI7", "forum": "fPMu3Afv3s", "replyto": "fPMu3Afv3s", "signatures": ["ICLR.cc/2026/Conference/Submission21149/Reviewer_K4TD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21149/Reviewer_K4TD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21149/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762164659715, "cdate": 1762164659715, "tmdate": 1762941503601, "mdate": 1762941503601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}