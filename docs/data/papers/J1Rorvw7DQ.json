{"id": "J1Rorvw7DQ", "number": 1863, "cdate": 1756955565545, "mdate": 1759898181856, "content": {"title": "Factuality Matters: When Image Generation and Editing Meet Structured Visuals", "abstract": "While modern visual generation models excel at creating aesthetically pleasing natural images, they struggle with producing or editing structured visuals like charts, diagrams, and mathematical figures, which demand composition planning, text rendering, and multimodal reasoning for factual fidelity. To address this, we present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark. First, we construct a large-scale dataset of 1.3 million high-quality structured image pairs derived from executable drawing programs and augmented with chain-of-thought reasoning annotations. Leveraging this dataset, we train a unified model that integrates a multimodal language model with FLUX.1-Kontext via a lightweight connector for enhanced multimodal understanding. A three-stage training curriculum enables progressive feature alignment, knowledge infusion, and reasoning-augmented generation, further boosted by an external reasoner at inference time. Finally, we introduce StructBench, a novel benchmark for generation and editing with over 2,000 challenging samples, and an accompanying evaluation metric, StructScore, which employs a multi-round Q&A protocol to assess fine-grained factual accuracy. Evaluations of 15 models reveal that even state-of-the-art systems score below 50\\%, while our model achieves the strongest open-source performance, with consistent gains from inference-time reasoning. By releasing dataset, model, and benchmark, we aim to advance unified multimodal foundations for structured visuals.", "tldr": "We present the first comprehensive, systematic investigation of this domain, encompassing data construction, model training, and an evaluation benchmark.", "keywords": ["Generative Modeling", "Unified Model", "Image Editing", "Text-to-Image Generation", "Benchmark"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0f7ef78454600a31df9afe0904b834a474232b43.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper first introduces a dataset for structured image editing, then propose a benchmark built from this dataset. It also trained a model using the dataset and evaluate on the benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Used programs to generate image, which makes it more robust and controlled.  \n2. The attempt to show alignment between human scores and benchmark metrics helps validate whether automated evaluation aligns with human judgment.\n\nOverall, the data generation process seems reasonable for producing a large-scale benchmark, as the authors demonstrate that models trained from this pipeline show good performance both on this benchmark and on another relevant benchmarks."}, "weaknesses": {"value": "1. The training pipeline can benefit from more ablation analysis. For example, what is the improvement when training with and without Stage 3 ? The choice of three stages is intuitive, but it requires more empirical evidence to justify the curriculum design. In addition, since the authors incorporate another dataset outside of their own dataset in stage 3, an ablation should be conducted to clearly show the effect of this addition, i.e. whether the improvement comes from the ChatGPT generated CoT or from the dataset itself.\n\n2. The variation in evaluation results remains quite large with a different LLM(when comparing tables in the appendix produced by Qwen versus those in the main paragraph), even though the trends are similar. This is my main concern for the paper. Especially for a benchmark, if the score or model ranking can change depending on which VLM/LLM backend is used for scoring, it becomes difficult to trust this measure of progress. This subjectivity potentially suggests that the evaluation protocol may need further stabilization."}, "questions": {"value": "Please see weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yemqnzxvx7", "forum": "J1Rorvw7DQ", "replyto": "J1Rorvw7DQ", "signatures": ["ICLR.cc/2026/Conference/Submission1863/Reviewer_vk2E"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1863/Reviewer_vk2E"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission1863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951863769, "cdate": 1761951863769, "tmdate": 1762915917124, "mdate": 1762915917124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a large-scale dataset tackling the structured visuals editing problem, along with a novel benchmark, StructBench, and metric StructScore to evaluate the quality of edits. The proposed dataset, with the three-stage training paradigm introduced, helps achieve state-of-the-art performance in the StructBench benchmark."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. The proposed large-scale dataset contains image pairs and respective drawing programs and editing instructions, which are useful for future advancements in text-to-structured visual generation or editing. With the drawing program info, it can also be beneficial to text-to-vector-based (such as SVG) diagram generation by simply changing the output format for image rendering.\n\n2. The proposed StructBench, along with the novel metric, StructScore, is superior to traditional editing benchmarks with metrics such as PSNR, as demonstrated by the high correlation with human preference. The addition of Qwen-based evaluator also rules out the reproducibility concerns of closed-source models such as GPT-5.\n\n3. The three-stage training approach in the paper effectively bridges Qwen2.5-VL with FLUX.1, achieving state-of-the-art performance on the proposed benchmark, while maintaining competitive general editing capabilities."}, "weaknesses": {"value": "- Some minor unclear points in the paper structure. In L365, there are mentions of StructEditBench and StructT2IBench. In section 3, only StructBench was introduced. I assume the Q&A construction and metrics around L244 are applied to both editing and T2I settings, but it should be clearly explained in this section."}, "questions": {"value": "I am happy with the paper overall. Some technical details I would like to ask\n\n- In Appendix A.2, only part of the training hyperparameters were provided. It would be helpful to include additional info such as GPU hours, number of iterations, LR scheduling, etc., given that the three-stage training involves fairly large datasets such as FLUX-Reason-6M and the one proposed in this paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "hW2PSZolup", "forum": "J1Rorvw7DQ", "replyto": "J1Rorvw7DQ", "signatures": ["ICLR.cc/2026/Conference/Submission1863/Reviewer_ZnGr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1863/Reviewer_ZnGr"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission1863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761951946330, "cdate": 1761951946330, "tmdate": 1762915916796, "mdate": 1762915916796, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper targets structured image generation/editing (charts, math/graph diagrams, tables, scientific schematics), where current T2I/VL models often produce plausible but factually wrong images. The authors build a 1.3M code-aligned dataset with paired instructions and CoT, plug Qwen-VL into FLUX.1-Kontext via a lightweight connector, train in three stages, and propose StructBench + StructScore to evaluate fine-grained factuality. They report strong gains over open models and show that adding explicit reasoning helps other models too"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Well-motivated task: clearly shows why structured visuals are different from “pretty pictures” and why factuality/layout need a dedicated treatment.\n- Clean data pipeline: executable graphics → instruction + code edit - re-render - filtered, so supervision is tight and verifiable.\n- Useful benchmark: StructBench/StructScore gives a more faithful measure than CLIP-style metrics and correlates with human judgment.\n- Clarity and Comprehensiveness: The paper is exceptionally well-written and structured. The problem, contributions, and methodology are articulated with clarity."}, "weaknesses": {"value": "- Dataset Domain and Generalizability: The reliance on executable programs for data generation may introduce a significant domain gap. It is questionable whether this dataset fully captures the diversity, noise, and \"messiness\" of structured visuals found in real-world sources"}, "questions": {"value": "- Have you considered evaluator bias? In StructScore, if the VLM used to ask questions and the VLM used to answer them belong to the same model family as the model being evaluated, how do you avoid evaluation bias or overfitting to the evaluator’s visual priors?\n\n- Have you considered how stable the automatic metric is across different model versions? Could small changes in the judge cause the model rankings to change?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pcFi9J9zZr", "forum": "J1Rorvw7DQ", "replyto": "J1Rorvw7DQ", "signatures": ["ICLR.cc/2026/Conference/Submission1863/Reviewer_pUs6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1863/Reviewer_pUs6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission1863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761976487696, "cdate": 1761976487696, "tmdate": 1762915916657, "mdate": 1762915916657, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles a point in current image generation: models are great at making artistic \"vibes\" but absolutely terrible when you need them to be factually accurate, like drawing a specific chart, a math diagram, or a table.\nTo fix this, the authors didn't scrape the web; they generated a huge dataset (1.3M pairs) by running actual code (like Python/LaTeX), ensuring perfect ground truth. They then trained a FLUX-based model using a three-stage curriculum designed to make the model \"think\" more about structure before generating. They also had StructBench, a benchmark for these tasks. Seems like this benchmark is difficult."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Tackles an under-researched area where standard aesthetic-focused models fail precisely because high factual accuracy (text rendering, exact layout) is required.\n\n\nGood idea: Utilizing programmatically generated images (via code) creates a verifiable and noise-free ground truth, superior to scraping diverse but unreliable web data for this specific task.\n\nStructScore improves upon naive \"VLM-as-a-judge\" approaches by using fine-grained, verified Q&A pairs, showing strong alignment with human evaluators.\n\nIntegration of Reasoning in the paper"}, "weaknesses": {"value": "1. Do we really need image editing to do this task? People can just ask LLM to write code to generate the new image, which is more accurate. With pixel level image generation model, there will be inevitable artifacts. \n\n2. How to make sure this data curation pipeline is accurate? Any error rate statistics?\n\n3. No evaluation in addition to infographics. You should evaluate on the natural image editing task."}, "questions": {"value": "See above comments."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "96gLLuwdRD", "forum": "J1Rorvw7DQ", "replyto": "J1Rorvw7DQ", "signatures": ["ICLR.cc/2026/Conference/Submission1863/Reviewer_HadZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission1863/Reviewer_HadZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission1863/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762327652961, "cdate": 1762327652961, "tmdate": 1762915916499, "mdate": 1762915916499, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}