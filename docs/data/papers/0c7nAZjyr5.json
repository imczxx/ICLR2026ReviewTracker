{"id": "0c7nAZjyr5", "number": 9621, "cdate": 1758130752440, "mdate": 1759897708477, "content": {"title": "From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning", "abstract": "Navigation foundation models trained on massive web-scale data enable agents to generalize across diverse environments and embodiments. However, these models, which are trained solely on offline data, often lack the capacity to reason about the consequences of their actions or adapt through counterfactual understanding. They thus face significant limitations in the real-world urban navigation where interactive and safe behaviors, such as avoiding obstacles and moving pedestrians, are critical. To tackle these challenges, we introduce the Seeing-to-Experiencing (S2E) learning framework to scale the capability of navigation foundation models with reinforcement learning. S2E combines the strengths of pre-training on offline videos and post-training through reinforcement learning. It maintains the model's generalizability acquired from large-scale real-world videos while enhancing its interactivity through reinforcement learning in simulation environments. Specifically, we introduce two innovations:\n1) an Anchor-Guided Distribution Matching strategy for offline pretraining, which stabilizes learning and models diverse motion patterns through anchor-based supervision; and\n2) a Residual-Attention Module for reinforcement learning, which obtains reactive behaviors from simulation environments without erasing the model’s pretrained knowledge.\nMoreover, we establish a comprehensive end-to-end evaluation benchmark, NavBench-GS, built on photorealistic 3D Gaussian Splatting reconstructions of real-world scenes that incorporate physical interactions. It can systematically assess the generalizability and safety of navigation foundation models.", "tldr": "", "keywords": ["Urban Navigation", "Foundation Models", "Reinforcement Learning"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8232a7a4c97d3f6216cf2c254c44c55336a16de1.pdf", "supplementary_material": "/attachment/e28108fbc08cdee2517498c378b3133f8a3473cd.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposes Seeing-to-Experiencing (S2E), a training recipe for goal-conditioned navigation that keeps visual priors from offline videos and adds interaction skills with reinforcement learning in simulation. It introduces a new action representation, using an anchor-conditioned Gaussian mixture to represent multiple short-horizon motions under the same observation. It also adds a Residual-Attention Module that fine-tunes only residual cross-attention while freezing visual encoders and self-attention to limit sim-to-real drift.\nExperiments in the NavBench-GS benchmark show RL fine-tuning outperforms supervised fine-tuning under the same budget, scales better than more offline data alone, and transfers zero-shot to wheeled and legged robots, with ablations confirming both modules are important."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper shows reinforcement learning is extremely helpful for closed-loop navigation tasks.\n2. The paper proposes using several fixed anchor points and GMM to represent the action space. This is essential specifically to the navigation task. The authors also provide in-depth analysis and comparison to other representations.\n3. The paper provides insightful discovery on the data saturation margin in the embodied navigation task and shows RL to be a more robust and efficient learning technique.\n4. The paper shows zero-shot depolyment of the trained model on both wheeled and quadroped robots. Experiment result shows superior performance compared to other baselines."}, "weaknesses": {"value": "1. The \"Residual attention module\" paragraph in section 3.2 (Line 258-268) is a bit confusing. The statement is not a well established conclusion in literature. The authors should rewrite the paragraph by providing more concrete derivation of the conclusion.\n2. In ablation study (Line 462-466 and Appendix Sec. D.5), there is no comparison with difussion policy models. The authors should add comparison to that to better support the discussion in Sec. 3.1."}, "questions": {"value": "1. In Fig. 6(b) left, why does SFT still sufer from overfitting even in the in-distribution Urban-sim evaluation?\n2. In reinforcement learning and simulative benchmark, there is no specific embodiment. Then how is collision checking done? Are they achieved by a navmesh or using a uniform collision volumn?\n3. In Tab. 1, what does ZeroPolicy mean? Is it a pure RL model? I'm also curious on the authors insights on why (or why not) pure RL training might not perform well in urban navigation while they work well in indoor environments like the HM3D dataset?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "R3TNrBpYvt", "forum": "0c7nAZjyr5", "replyto": "0c7nAZjyr5", "signatures": ["ICLR.cc/2026/Conference/Submission9621/Reviewer_UWpN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9621/Reviewer_UWpN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission9621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761246581432, "cdate": 1761246581432, "tmdate": 1762921159190, "mdate": 1762921159190, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the S2E framework, combining offline video pretraining (via AGDM strategy) and simulation RL finetuning (with RAM module). It builds NavBench-GS, verifies RL boosts navigation model performance, breaks diminishing returns of data scaling, and enables zero-shot transfer across robots."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes Anchor-Guided Distribution Matching (AGDM), which uses an anchor-guided Gaussian Mixture Model to model multimodal navigation trajectories, capturing diverse valid actions under the same observation while ensuring training stability.\n\n2. This paper designs the Residual-Attention Module (RAM), which freezes pretrained components and adds trainable residual branches to cross-attention layers, enabling the model to gain interactive skills via RL without losing pretrained generalizable knowledge.\n\n3. This paper establishes the NavBench-GS benchmark, built on photorealistic 3D Gaussian Splatting scenes with physical interactions, realizing closed-loop policy evaluation and solving the reproducibility issue of real-world navigation testing."}, "weaknesses": {"value": "1. The model relies solely on visual input and lacks 3D perception capabilities, leading to occasional failure in obstacle avoidance in some scenarios, which is a persistent limitation for vision-only navigation approaches.\n\n2. The real-world evaluation scenarios are relatively limited (only 25 scenarios), and the generalization performance of the S2E framework in more complex and diverse urban environments (e.g., extreme weather, complex traffic conditions) has not been verified.\n\n3. The humanoid robot in cross-embodiment evaluations shows notably lower success rates compared to wheeled and quadruped robots, yet the paper does not deeply analyze the root causes (e.g., joint complexity impacts) or propose targeted optimization strategies for humanoid platform adaptation.\n\n4. The RL finetuning relies on modified URBAN-SIM environments, but the paper only briefly mentions procedural generation rule adjustments without detailing how these rules ensure the simulated environments fully align with real urban spatial layouts, potentially limiting the sim-to-real transfer reliability"}, "questions": {"value": "1. Given that the current model lacks 3D perception, what specific 3D information integration methods (such as depth prediction or occupancy prediction) do you plan to adopt in future work, and how will you balance the computational cost of 3D perception with the real-time performance of navigation?\n\n2. The NavBench-GS benchmark currently covers 26 scenarios, but real urban environments involve more dynamic elements (e.g., sudden appearance of vehicles, temporary road closures). Will you expand the benchmark to include such scenarios, and what criteria will be used to select new scenarios to ensure the benchmark’s representativeness?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y6IOctZJAV", "forum": "0c7nAZjyr5", "replyto": "0c7nAZjyr5", "signatures": ["ICLR.cc/2026/Conference/Submission9621/Reviewer_FY2s"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9621/Reviewer_FY2s"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission9621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761909219846, "cdate": 1761909219846, "tmdate": 1762921158806, "mdate": 1762921158806, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents results from RL-finetuning of a navigation policy that is pretrained by SFT over static datasets. It empirically shows the improvements afforded by RL, introduing a way to provide data diversity and an architecture for the policy."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "An unusual combination of SFT and RL for learning-based navigation - while existing approaches focus on one or the other, this paper shows the value of doing both."}, "weaknesses": {"value": "The anchor description is not very clear. It seems to be constant-curvature arcs - a clearer explanation is needed.\nUnclear how the constant curvature arcs approximate the full diversity of demonstration paths - is the matching performed instantaneously (i.e., for an instantaneous vx, w command mapped to the corresponding curvature?)"}, "questions": {"value": "Can you explain how the anchors are defined and chosen?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "UeDJ4wsTdl", "forum": "0c7nAZjyr5", "replyto": "0c7nAZjyr5", "signatures": ["ICLR.cc/2026/Conference/Submission9621/Reviewer_vgu6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9621/Reviewer_vgu6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission9621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761958287052, "cdate": 1761958287052, "tmdate": 1762921158424, "mdate": 1762921158424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes S2E (Seeing-to-Experiencing), a framework for training navigation foundation models that combines offline pretraining on real-world videos with reinforcement learning (RL) fine-tuning in simulation. The key technical contributions include: (1) Anchor-Guided Distribution Matching (AGDM) for modeling multimodal navigation behaviors during pretraining, (2) a Residual-Attention Module (RAM) that enables RL fine-tuning while preserving pretrained knowledge, and (3) NavBench-GS, a benchmark built on 3D Gaussian Splatting scenes. The authors claim that RL alleviates diminishing returns from scaling offline data alone and enables zero-shot transfer to real-world scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Comprehensive System: The paper presents an end-to-end system from data collection through real-world deployment, which requires significant engineering effort.\n\nCross-Embodiment Evaluation: Testing on wheeled, quadruped, and humanoid robots (Table 6) demonstrates some generality, though the humanoid results are quite poor.\n\nHonest Limitations Discussion: Section 5 acknowledges the limitation of vision-only approaches and collision failures.\n\nDetailed Implementation: The appendix provides extensive implementation details, which aids reproducibility."}, "weaknesses": {"value": "1. Lack of Theoretical Insight: The paper doesn't explain why RL helps beyond showing empirical improvements. What specific failure modes of offline learning does RL address? What inductive biases make certain behaviors learnable only through interaction?\n\n2. Modest Improvements: Many reported improvements are marginal (e.g., Table 2: 0.51 vs. 0.32 success rate for wheeled robot). Given the additional computational cost of RL (8 hours on L40S GPU), the cost-benefit tradeoff is unclear.\nCherry-Picked Comparisons:\n\n3. CityWalker* (retrained on same 100h data) actually achieves 0.67 SR vs. S2E's 0.82 SR in empty scenes (Table 1), suggesting the RL contribution is partially due to other factors;\nFigure 6(a) compares against \"prior methods\" shown as dotted lines, but these prior methods use different evaluation protocols\n\n\n4. Limited Failure Analysis: The paper shows successful cases but doesn't systematically analyze failure modes. When does RL fine-tuning hurt performance? What environments or scenarios remain challenging?\n\n5. Questionable Design Choices:\n\nWhy freeze the visual encoder during RL? This prevents learning better visual features for interaction\nWhy use a simplified entropy approximation (Eq. 11) instead of more accurate estimators?\nThe stochastic goal masking strategy (Section E.2) seems arbitrary - no ablation justifies the specific probabilities chosen\n\n\n6. Reproducibility Concerns: Despite promising code release, the method depends on:\nProprietary Unitree robot APIs\nMultiple datasets with different licensing terms\nNVIDIA IsaacSim which requires expensive GPU resources\nHand-tuned reward functions that may not transfer to other environments\n\n7. Missing Related Work\n\nThe paper should discuss and compare with:\n\nOffline RL for Robotics: Kumar et al. (2020, NeurIPS), Nair et al. (2020, CoRL), Mandlekar et al. (2021, CoRL)\nVision-Language-Action Models: Driess et al. (2023, ICML - PaLM-E), Brohan et al. (2023, CoRL - RT-2)\nNavigation Benchmarks: Savva et al. (2019, ICCV - Habitat), Xia et al. (2018, CVPR - Gibson)\nSim-to-Real Transfer: Peng et al. (2018, ICRA), Tan et al. (2018, CoRL)"}, "questions": {"value": "Data Efficiency: How many RL environment steps are needed? What's the sample complexity compared to collecting more offline data?\n\nReward Engineering: How sensitive is performance to reward function design? Have you tried learning rewards from human preferences (Christiano et al., 2017)?\n\nFailure Modes: What percentage of real-world failures are due to:\n\nPerception errors (misdetecting obstacles);\nPlanning failures (local minima);\nControl errors (locomotion instability);\nSim-to-real gap\n\n\nComparison Fairness: Can you provide results where all methods are:\n\nTrained on identical 100h dataset;\nEvaluated on an established benchmark (not self-proposed);\nUsing the same evaluation protocol and metrics\n\n\nGeneralization: How does performance degrade with:\n\nDifferent camera intrinsics/extrinsics;\nDifferent weather/lighting conditions;\nNovel obstacle types not seen in training\n\n\nRL vs. SFT: The paper claims RL is better than SFT (Figure 6b), but:\n\nHow was the SFT data collected? From the pretrained policy or optimal demonstrations?\nDid you try other offline RL algorithms (CQL, IQL, etc.) that might bridge the gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "XRdQX07pGz", "forum": "0c7nAZjyr5", "replyto": "0c7nAZjyr5", "signatures": ["ICLR.cc/2026/Conference/Submission9621/Reviewer_SfZD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission9621/Reviewer_SfZD"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission9621/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994987493, "cdate": 1761994987493, "tmdate": 1762921157944, "mdate": 1762921157944, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}