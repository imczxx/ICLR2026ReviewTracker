{"id": "Kt9tJeOwjy", "number": 22113, "cdate": 1758326219227, "mdate": 1759896885835, "content": {"title": "RFS: Reinforcement learning with Residual flow steering for dexterous manipulation", "abstract": "Imitation learning has been an effective tool for bootstrapping sequential decision making behavior, showing surprisingly strong results as methods are scaled up to high-dimensional, dexterous problems in robotics. These ``behavior cloning\" methods have been further bolstered by the integration of generative modeling techniques such as diffusion modeling or flow matching for training expressive multimodal behavior policies. However, these pretrained models do not always generalize perfectly, and require finetuning to maximize deployment-time performance. This finetuning procedure must retain the strengths of pretraining for exploration, while being able to quickly correct for local inaccuracies in model performance. In this work, we propose an efficient reinforcement learning (RL) framework for fast adaptation of pretrained generative policies. Specifically, our proposed methodology - residual flow steering, instantiates an efficient RL technique that quickly adapts a pretrained flow-matching model by steering it jointly by optimizing a policy for selecting both a latent noise distribution and a residual action. Doing so allows policies to perform both local (residual actions) and global exploration (latent noise),  data-efficient adaptation. We demonstrate that this technique is effective for dexterous manipulation problems, serving both as a tool to pretrain behaviors in simulation and efficiently finetune them in the real world.", "tldr": "", "keywords": ["Robotics", "reinforcement learning", "sim-to-real"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/313e70ad21285e3d5f4d81fbe96b009483d00cb4.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Residual Flow Steering (RFS), a novel reinforcement learning framework for fine-tuning flow-matching policies in dexterous manipulation.\nRFS adapts a pre-trained policy by learning to modulate both the initial latent noise (for global exploration) and output a residual action (for local refinement).\nThe method is evaluated in both simulation and real-world settings, demonstrating improved performance over several baselines. The core idea is intuitive and addresses a relevant challenge in policy adaptation. However, the paper could be strengthened by a more comprehensive related work section, clearer methodological explanations, and more competitive baseline comparisons."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper proposes a novel RL method for adapting a policy pre-trained with flow-matching to dexterous manipuation tasks.\nThe adaptation is achieved by training a policy to output an initial noise and a residual action, which correspond to the input and output of the flow policy.\n2. The overall ideas and intuition of the method are clear.\n3. The paper demonstrate the performance of the proposed method with both simulation and real-world experiments."}, "weaknesses": {"value": "1. A more comprehensive related work section will enhance the paper.\nFor example, recent papers on residual RL are not included, such as policy decorator[1].\nAnd some baselines compared in this paper are not introduced in the related work.\n2. Some details about the method are not clearly explained. Please check the questions in the following section.\n3. The chosen baselines, while reasonable, do not fully demonstrate the advantage of RFS over the state-of-the-art.\nA more compelling comparison would be against: (1) Recent RL methods specifically designed for fine-tuning diffusion/flow policies (beyond the ablated DSRL) (2) State-of-the-art residual RL methods that finetune a base policy, to better isolate the contribution of the combined approach.\n\nMinor:\n1. The title \"Imitation-Bootstrapped Reinforcement Learning\" in related work might be slightly misleading. As defined in works like IBRL[2], the term has a specific meaning. The papers [3][4] cited under this heading are more broadly categorized as \"RL with Demonstration\". It would be helpful to refine this terminology for precision.\n\n[1] Zhiyuan Yuan, et al. \"Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model.\" ICLR 2025.\n\n[2] Hengyuan Hu, et al. \"Imitation Bootstrapped Reinforcement Learning.\" RSS 2024.\n\n[3] Ashvin Nair, et al. \"Overcoming exploration in reinforcement learning with demonstrations.\" ICRA 2018.\n\n[4] Aravind Rajeswaran, et al. \"Learning complex dexterous manipulation with deep reinforcement learning and demonstrations.\" RSS 2018."}, "questions": {"value": "1. In section 5.2, it is mentioned that base policy actions $a_b$ and human corrections $a$ are recorded and transformations are applied to them to obtain $(a_0, a_r)$ for training. Why is the initial latent noise $a_0$ not recorded directly?\n2. The RFS policy $\\pi_{RFS}$ outputs $a_0$, which is passed through the \"Push\" function (an ODE solver involving multiple evaluations of $v_\\theta$). Could you clarify if gradients are backpropagated through the entire \"Push\" operation?\nIf so, there will be a huge computational cost for the gradient calculation during training and it is unstable. If not, how are the gradients for $a_0$ obtained?\nBy the way, could you mention the number of steps used for the \"Push\" operation?\n3. The policy $\\pi_{RFS}$ is trained to output both $a_0$ and $a_r$. I am curious about the results if we separate the current loss into two terms. For example, detaching $a_r$ when calculating gradients for the $a_0$, or vice-versa, to see if it stabilizes training or improves performance?\n4. The process of using RFS for simulation data generation could be explained more clearly.\nI got the following questions when reading this part:\nFirst, are VR-teleoperated demonstrations real-world data or simulation data?\nSecond, the high-level policy $\\pi_{RFS}$ is trained with the same demonstrations using an offline RL method or trained in simulation using online RL method?\nI can find the answer in Section 6.1: simulation data, online RL method (PPO) while it will be easier for the reader to understand if the Section 5.1 can be rephrased.\n5. What is the specific advantage of applying RFS to create a simulation policy first, rather than directly applying RFS to the original base policy $v_{\\theta}(a_t, s, t)$ using human-collected real-world data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PnCNomufca", "forum": "Kt9tJeOwjy", "replyto": "Kt9tJeOwjy", "signatures": ["ICLR.cc/2026/Conference/Submission22113/Reviewer_kxpu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22113/Reviewer_kxpu"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760587665674, "cdate": 1760587665674, "tmdate": 1762942070699, "mdate": 1762942070699, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper focuses on offline-to-online reinforcement learning (RL) setting.\nSpecifically, the paper proposes to adapt generative policies with reinforcement learning by unifying residual policy learning and diffusion steering into a new class of algorithms, residual flow steering (RFS).\nThe paper illustrates, through simulated and real-life experiments, that RFS outperforms compared algorithms."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The unification between residual RL and latent-noise steering is interesting, and can open up avenues for various choices of $f$ and $g$.\n- In the offline RL setting, RFS appears to generalize to unknown objects better in the real-life setting. The extra robustness experiments are helpful in demonstrating RFS' benefits."}, "weaknesses": {"value": "- In section 5.2, the paper proposes to collect extra human correction data $a$---this seems to be a strong limitation, similar to applying the DAgger algorithm. It is possible that I have totally misunderstood this process:\n\t- The whole trajectory $((o_1, s_1), (o_2, s_2), \\dots)$ is generated using the correction action $a$, as opposed to below.\n\t- First sample the trajectory $((o_1, s_1), (o_2, s_2), \\dots)$ using the base policy actions, then obtain the correction actions based on the already collected trajectory.\n\t- Nevertheless, perhaps the paper can clarify this on lines 296-298.\n- Experiments\n\t- The experimental setting is confusing. In section 5.2 it is mentioned that the RFS is finetuned via offline RL, but in the experiment, specifically section 6.1.2, the setting is now in the online RL setting.\n\t\t- In 6.1.2, does the learner obtain any human-correction data for finetuning, in addition to the PPO trajectories?\n\t\t- Secondly, if the setting is originally offline-to-online RL, I think it's unfair to compare the learning curve of Tabula-rasa RL, and instead it should be compared against algorithms such as Cal-QL, AWAC-like algorithms, or RLPD.\n\t\t- Instead, the rationale on the chosen baselines on action-space reduction and action codebooks are unclear. Is the intuition to reduce the sample efficiency through easier exploration with smaller action space?\n\t- Likewise, for section 6, the choice of compared algorithms can be strengthened with offline RL algorithms like CQL and IQL."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "lr7XIgTA18", "forum": "Kt9tJeOwjy", "replyto": "Kt9tJeOwjy", "signatures": ["ICLR.cc/2026/Conference/Submission22113/Reviewer_oCuu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22113/Reviewer_oCuu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761659434183, "cdate": 1761659434183, "tmdate": 1762942070295, "mdate": 1762942070295, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers the problem of fine-tuning pretrained generative (flow matching) policies for reinforcement learning. The contribution of this work is to introduce a general framework that incorporates two types of preexisting fine-tuning methods: input modulation (flow steering) and output modulation (residual learning). The authors propose a specific instance of this framework: Residual Flow Matching (RFS) which learns both an initial latent noise distribution and a policy that outputs residual actions, which is added to the push forward action produced by the pretrained model (pushed forward from the learned noise). The authors conduct simulated and real-life experiments in the dextrous manipulation robotics domain, and the experiments demonstrate RFS achieving favorable performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper integrates two previous methods (flow steering, residual learning) with complementary benefits and drawbacks to get the best features of both.\n2. The framework the paper introduces is broad enough to be applicable to many important reinforcement learning applications, even outside the domain of manipulation, especially given the wide adoption of generative policies in various RL applications."}, "weaknesses": {"value": "1. **Some Baseline choices lack motivation/clarity:**  I do not understand the use of the VQ-VAE and PCA baselines in Section 6.1.2/Table 1/Fig. 4. From my understanding, both are methods to get different state-action representations, whereas the focus of the paper is finetuning. If my understanding is correct, these results are comparing PPO finetuned with RFS with PPO trained for two separate state-action representations (made by PCA, VQ-VAE). Why is such a comparison meaningful?\n\n2. **Seeds for experiments:** For the simulation experiments, it is really not valid to make claims based on only 3 random seeds, especially given that the paper is mostly empirical in nature. \n \n3. Unless I am mistaken, residual RL is not a baseline for `w/ hand` in Table 1. Given that the authors propose a method that is essential residual RL+flow steering, I feel residual RL should be included for the `w/ hand` comparisons."}, "questions": {"value": "Besides addressing my points in the weaknesses section, please make the following changes:\n1. The notation for math in the paper is a bit cluttered. Specifically, please do not use the symbol $a$ whenever you write $a=a_r+a_b$, as we also have $\\pi(a|s)$ (for example in Equation 4).\n2. Grammatical mistakes:\n\na. In the abstract the sentence \"Doing so allows policies to perform both local (residual actions) and\nglobal exploration (latent noise), data-efficient adaptation.\" seems grammatically incorrect.\n\nb. Please revise In table 1: absolute joint psose -> absolute joint pose."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Yisnvz4yMP", "forum": "Kt9tJeOwjy", "replyto": "Kt9tJeOwjy", "signatures": ["ICLR.cc/2026/Conference/Submission22113/Reviewer_yEFi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22113/Reviewer_yEFi"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761945541541, "cdate": 1761945541541, "tmdate": 1762942069662, "mdate": 1762942069662, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Residual Flow Steering (RFS), an RL fine‑tuning scheme for flow‑matching / diffusion policies that jointly (i) steers the initial latent noise for global changes and (ii) applies a residual action for local corrections—without modifying base policy parameters. The method is instantiated for dexterous grasping: pretrain in simulation (Leap Hand + Franka), distill a visuomotor policy, then fine‑tune in the real world with offline RL (TD3+BC). Experiments show improved success over action‑representation baselines and ablations (residual‑only or steering‑only) in both pinch and power grasps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "RFS is a clean unification of residual RL (output modulation) and latent steering (input modulation), formalized via a modulation policy.\n\nStrong, consistent improvements over baselines and over residual‑only / steering‑only ablations in simulation and better real‑world success vs. zero‑shot and supervised fine‑tuning.\n\nThe paper is well written. The introduction and the method are easy to follow and communicate the contributions and the implementation well."}, "weaknesses": {"value": "Limited novelty. The algorithm mainly combines two widely used adaptation strategies (residual action learning and latent steering).\n\nBaseline coverage. Real‑world evaluation lacks comparisons to other RL fine‑tuning approaches for diffusion/flow policies (e.g., recent flow‑RL fine‑tuners); most comparisons are ablations or action‑space baselines.\n\nTask scope. Validation centers on grasping; the paper claims broader applicability, but no additional manipulation tasks are shown."}, "questions": {"value": "Why is “changing the action representation” (absolute/relative joints, PCA, VQ‑VAE) considered a primary baseline for the first task? What does that comparison communicate about the contribution of the paper?\n\nCould you add head‑to‑head comparisons against other RL fine‑tuning methods for diffusion/flow policies to strengthen the claim that RFS is preferable beyond its own ablations?\n\nDo you have preliminary results (even in sim) on more complex tasks (e.g., non‑prehensile skills or multi‑stage manipulation) to support generalization claims?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uRKJ0Hrjjs", "forum": "Kt9tJeOwjy", "replyto": "Kt9tJeOwjy", "signatures": ["ICLR.cc/2026/Conference/Submission22113/Reviewer_22ER"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22113/Reviewer_22ER"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22113/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761993993823, "cdate": 1761993993823, "tmdate": 1762942069039, "mdate": 1762942069039, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}