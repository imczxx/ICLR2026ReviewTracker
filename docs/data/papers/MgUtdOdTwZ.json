{"id": "MgUtdOdTwZ", "number": 13970, "cdate": 1758226244116, "mdate": 1759897399330, "content": {"title": "ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents", "abstract": "Proximal policy optimization (PPO) has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1) token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments with distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), \\textbf{S-PPO} (clipping-bias correction applied to token-level PPO), and \\textbf{ST-PPO} (turn-level sampling combined with clipping-bias correction). In our experiments, we mainly study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.", "tldr": "", "keywords": ["Reinforcement Learning", "Multi-turn task", "LLM agent", "off policy"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f386467d48ea2fca614f869c006a2b6257daeaa.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces new techniques to stabilise training of LLMs using PPO which are motivated by trying to solve empirically identified problems. These techniques are turn-level importance sampling and clipping-bias correction. These should respectively better represent the turn-based nature of LLM chat finetuning, and reduce the variance in gradient updates from off-policy samples. Empirical analysis shows these techniques provide better performance on multi-turn tasks compared to standard PPO."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "1. The problem of variance in the GAE estimation causing issues is well evidenced by prior work and empirical experiments within the paper, and so the introduction of a method specifically to mitigate this is well-motivated\n2. The exposition of the paper is clear, and the method is well explained\n3. Results clearly show that the clipping-bias correction technique improves performance relative to vanilla PPO\n4. The decompositions of loss functions and the relating lemmas are nice additions that improve the clarity of the arguments being made in the paper"}, "weaknesses": {"value": "1. The method is only compared with PPO, and not other competitive LLM RL algorithms such as GRPO. Additionally, it is not compared to cited works seeking to solve similar problems such as TOPR, LUFFY, and GSPO.\n2. There does not seem to be sufficient evidence to conclude that the turn-level structure specifically is causing issues with PPO, weakening the motivation for some of the techniques proposed in the paper.\n3. In figure 3a there is heavy overlap between the error bars for turn and token PPO, there is not enough evidence here to conclude turn-PPO outperforms token-PPO.\n4. Figure 3d does not fully support the claim that the L2 norm of the clipping bias term is growing exponentially, as the data is extremely noisy and no linear trend has been fitted, let alone had its goodness of fit computed.\n5. ST-PPO and S-PPO perform similarly, providing additional evidence that the influence of accounting for turns is minimal. In fact, figure 4b shows S-PPO outperforming ST-PPO."}, "questions": {"value": "Questions:\n1. Why did you not test against other baselines beyond vanilla PPO?\n2. Do you have any other evidence that one either ought to expect turn-PPO to work better than token-PPO, or that turn-PPO does in fact work better than token-PPO?\n3. Could you provide a (smoothed) version of figure 3d with a trend line plotted?\n\nSuggestions:\nAbandon the turn-based component and focus on the clipping-bias correction. Test this against more baselines and on a wider range of benchmarks. Test if analogous methods improve algorithms such as GRPO or RLOO, as alluded to being possible on lines 398-400."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "IEFvQVfyj3", "forum": "MgUtdOdTwZ", "replyto": "MgUtdOdTwZ", "signatures": ["ICLR.cc/2026/Conference/Submission13970/Reviewer_apVh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13970/Reviewer_apVh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761669272906, "cdate": 1761669272906, "tmdate": 1762924470467, "mdate": 1762924470467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the instability and performance collapse of Proximal Policy Optimization (PPO) when used for training multi-turn LLM agents. The authors identify two primary sources of this instability: (1) a mismatch between token-level importance sampling and the natural turn-level structure of agentic tasks, and (2) high-variance gradients resulting from inaccurate critic estimates on off-policy samples. To solve this, they propose two techniques: **turn-level importance sampling** to align credit assignment with task granularity, and a **clipping-bias correction** to normalize gradients and downweight unreliable samples. The combined algorithm, ST-PPO, is shown empirically to prevent performance collapse on multi-turn search tasks and achieve higher final task performance compared to standard token-level PPO."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Important Problem:** The paper addresses a critical and practical challenge. PPO is a foundational algorithm for LLM alignment, and its instability in multi-turn, off-policy settings is a significant bottleneck for developing more complex reasoning agents.\n- **Clear Diagnosis:** The paper does a good job of empirically diagnosing the root causes of instability, specifically linking gradient spikes and performance collapse to unreliable advantage estimates from the critic on off-policy data.\n- **Sufficient Literature Review:** The work is well-contextualized within existing research on RLHF, off-policy learning for LLMs, and multi-turn agent training.\n- **Clarity:** The paper is generally well-written and clearly motivates its proposed solutions from its analysis of the problem."}, "weaknesses": {"value": "- **Empirical Gaps:** The empirical evaluations, while showing the final methods are stable, could be more robust in demonstrating the distinct value of each proposed component.\n\n  - In Figure 3a, the performance improvement of **Turn-PPO** (turn-level sampling only) over token-level PPO is minor.\n  - In Figure 4, the performance curves for **S-PPO** (clipping-bias correction only) and **ST-PPO** (both components) are nearly identical. This makes it difficult to assess the added benefit of the turn-level importance sampling when the clipping-bias correction is already in use.\n  - The diagnostic plots in Figure 3(c-d) (PPO loss norm and clipping bias norm) are only provided for the failing token-level PPO. These plots are crucial for the paper's argument, and not showing them for S-PPO and ST-PPO is a missed opportunity to validate that the proposed correction mechanism is working as intended.\n\n- **Limited Baselines and Models:** The experimental comparison is somewhat limited.\n\n  - The main stability plots in Figure 4 compare S-PPO and ST-PPO only against the standard Token-PPO, which collapses. Other relevant baselines are not included in this comparison.\n\n  - The experiments are conducted on a single model family (Qwen-2.5-7B and 1.5B). Demonstrating the method's effectiveness on more diverse model architectures would significantly strengthen the paper's claims."}, "questions": {"value": "1. Could the authors provide more intuition behind the clipping-bias correction in Equations 7 and 8? Why does normalizing the gradient by the L2 norm of the *clipping bias term* ($C(\\theta)$)  effectively downweight unreliable samples and lead to stabilization?\n\n2. The paper presents the two techniques as complementary. However, the results in Figure 4 suggest that S-PPO and ST-PPO are empirically indistinguishable. Does this imply that the clipping-bias correction is the dominant and perhaps sufficient stabilization technique, and that turn-level sampling offers little additional benefit *after* this correction is applied? What is the specific interplay between these two modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wlLjY9ikCf", "forum": "MgUtdOdTwZ", "replyto": "MgUtdOdTwZ", "signatures": ["ICLR.cc/2026/Conference/Submission13970/Reviewer_pNPG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13970/Reviewer_pNPG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969578554, "cdate": 1761969578554, "tmdate": 1762924469669, "mdate": 1762924469669, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the instability and performance collapse of Proximal Policy Optimization (PPO) when training large language models (LLMs) on multi-turn tasks. The authors identify two root causes for this instability: a \"granularity mismatch\" between PPO's token-level optimization and turn-level credit assignment, and high-variance gradients from off-policy critic estimates. To solve this, they introduce ST-PPO, an algorithm featuring two new techniques: turn-level importance sampling, which aligns credit assignment with the multi-turn structure, and clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Experiments on multi-turn search tasks demonstrate that ST-PPO and its variants successfully prevent performance collapses, maintain greater stability, and achieve higher task performance than standard token-level PPO."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- This paper proposes turn-level importance sampling, a novel technique that aligns the RL optimization process with the natural turn-based structure of dialogue and reasoning tasks.\n- This paper proposes clipping-bias correction, a stabilization mechanism that adaptively downweights unreliable, highly off-policy samples to prevent them from destabilizing gradient updates."}, "weaknesses": {"value": "- This paper does not test on more up-to-date models, such as Qwen 3 series. \n- The experiments primarily contrast ST-PPO with a standard, unstable token-level PPO. The paper does not include empirical comparisons against other advanced stabilization algorithms mentioned in its own related work (like TOPR, GSPO, or ARPO). This paper should include other PPO stabilization algorithms into baselines. \n- The core \"turn-level\" component relies on a specific implementation to identify turn boundaries."}, "questions": {"value": "- In Figure 4 & 5, it does not include the turn-ppo, what is its performance here?\n- Does this method work in GRPO? In GRPO, GRPO also has clipping and should suffer similar problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9HkALHCwur", "forum": "MgUtdOdTwZ", "replyto": "MgUtdOdTwZ", "signatures": ["ICLR.cc/2026/Conference/Submission13970/Reviewer_3C9c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13970/Reviewer_3C9c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13970/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762036620068, "cdate": 1762036620068, "tmdate": 1762924469095, "mdate": 1762924469095, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}