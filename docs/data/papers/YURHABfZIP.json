{"id": "YURHABfZIP", "number": 5528, "cdate": 1757918092075, "mdate": 1759897969380, "content": {"title": "RobustFlow: Towards Robust Agentic Workflow Generation", "abstract": "The automated generation of agentic workflows is a promising frontier for enabling large language models (LLMs) to solve complex tasks. However, our investigation reveals that the robustness of agentic workflow remains a critical, unaddressed challenge. Current methods often generate wildly inconsistent workflows when provided with instructions that are semantically identical but differently phrased. This brittleness severely undermines their reliability and trustworthiness for real-world applications. To quantitatively diagnose this instability, we propose metrics based on nodal and topological similarity to evaluate workflow consistency against common semantic variations such as paraphrasing and noise injection. Subsequently, we further propose a novel training framework, RobustFlow, that leverages preference optimization to teach models invariance to instruction variations. By training on sets of synonymous task descriptions, RobustFlow boosts workflow robustness scores to 70\\% - 90\\%, which is a substantial improvement over existing approaches. The code is publicly available at \\url{https://anonymous.4open.science/r/RobustFlow-1B19}.", "tldr": "", "keywords": ["Agentic Workflow Generation; LLM Agent"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/64f22f7b17d5c2b407e549a0d2a0f51012fabef6.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates the lack of robustness in automated agentic workflow generation, showing that existing methods produce inconsistent workflow structures when given perturbed instruction. The authors introduce a new benchmark with instruction perturbations and propose two structure-aware metrics to quantify this instability. Their proposed RobustFlow, a two-stage training framework that uses instruction-augmented SFT followed by a Self-consistency Preference Optimization (ScPO) to encourage the generation of a single workflow for any instruction that are sementically similar. Experimental results show that RobustFlow significantly improves structural robustness over baselines, with a minimal trade-off in task performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- The work is both timely and important in current agentic systemt, as the problem indeed exist in real-world practice.\n- Using self-consistency to generate preference pairs is interesting to train for robustness.\n- The evaluation is solid. The custom benchmark and metrics are well thought out."}, "weaknesses": {"value": "- The paper notes a modest decrease in task performance compared to other SOTA methods but doesn't offer a deep analysis of this trade-off. Forcing the model toward a single canonical workflow might prevent it from finding a more optimal, specialized workflow that could be better suited for a specific phrasing of an instruction. I think more detailed discussions on this point would be beneficial.\n\n- The definition of the `execution score' in ScPO is not clear to me. The paper does not specify how this score is calculated (eg: binary pass/fail, reward model), which is a critical for understanding the training dynamics and for reproducibility. \n\n- The method of identifying a canonical workflow by frequency seems viable for the tasks presented. However, its scalability to more complex, open-ended domains with a vast combinatorial space of potential workflows is not discussed. It's not clear to me that how a single `most frequent' structure would be identified or if it would even be meaningful in such a scenario."}, "questions": {"value": "- Could you clarify how the `execution score' is computed? Is it based on unit test pass rates for code, final answer accuracy for math, or another metric? \n- Regarding the performance trade-off, is it possible that prioritizing a single canonical workflow is sometimes suboptimal? Have you analyzed instances where a less frequent but structurally different workflow generated by a baseline actually performed better on a specific instruction variant?\n- The ScPO method anoints the most frequent workflow from the SFT model as the `chosen' one. How does the framework guard against reinforcing a popular but suboptimal workflow? Is the execution score always sufficient to override the frequency signal if the most common plan is flawed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "H735YPK8Xa", "forum": "YURHABfZIP", "replyto": "YURHABfZIP", "signatures": ["ICLR.cc/2026/Conference/Submission5528/Reviewer_Jb4K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5528/Reviewer_Jb4K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760630501723, "cdate": 1760630501723, "tmdate": 1762918111807, "mdate": 1762918111807, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the robustness of LLMs' automated agentic workflow generation. Perturbed questions are constructed through Paraphrasing, Requirement Augmentation, and Noise injection to address this issue. The paper introduces node chain and graph structure metrics to compare the consistency of workflow generation before and after question perturbation. To enhance the stability of workflow generation, the paper presents RobustFlow. It first conducts SFT on the model using perturbed questions and consistent workflows and then samples positive and negative instances for DPO optimization based on the predicted workflow's execution score and self-consistency score. Experimental results indicate that most workflow generation methods exhibit poor robustness, while RobustFlow maintains high stability after perturbations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The robustness issue of agentic workflow generation is indeed a key real-world challenge, as user instructions are typically diverse, ambiguous, inaccurate, or even erroneous. The ability of a model to generate stable workflows in such scenarios is a crucial aspect of large-scale intelligent agents for applications. This paper investigates and optimizes the robust workflow generation of LLMs from the perspectives of data construction and model training, which constitutes the main contribution of this paper. Additionally, the figures in this paper are clear and easy to understand, and the methods are easy to follow."}, "weaknesses": {"value": "1. I believe that the robustness issue of workflow generation should not be limited to perturbations in user questions alone. The agent is closely connected to the environment, and the dynamic changes in the environment can also lead to dynamic changes in the workflow of tasks. Therefore, robustness is a consistency issue among the agent, the user, and the environment. Focusing solely on user queries makes this problem overly simplistic, essentially reducing it to a user prompt rewriting problem, which limits the depth of the problem and the innovativeness of the methods, thereby constraining the further contributions of this paper.\n\n2. The node chain and graph structure metrics defined in Sec. 3.3 for measuring workflow robustness seem similar to those mentioned in T-Eval and WorfBench, further diminishing the uniqueness of the contributions of this paper.\n\n3. Some methodological details and symbol definitions in Sec. 4.2 are not very clear. For instance, the definition of the execution score $s_q(w)$ seems missing in Eqn. 10; What is the role of $\\lambda_q$ in Eqn. 11? Line 323 gives the impression of being incomplete; after assigning $M_\\theta$ to $M_{t+1}$, how is $M_{t+1}$ utilized? Are the authors trying to describe an iterative optimization process?\n\n4. The motivations behind many settings in Eqn. 13 are not very clear. Why introduce the optimization of the preferred sample's likelihood term and why multiply both terms by the confidence weight $\\rho_q$ simultaneously?\n\n5. The experimental section lacks an evaluation of the original workflows generated by the model as a reference. If the quality of the model's original workflow is poor, discussing robustness becomes less meaningful. Additionally, it is worth investigating how different perturbations affect the quality of the workflows.\n\n6. There is a suspicion of unfair comparison in Table 2. As far as I know, AFlow is a training-free method, while RobustFlow has undergone fine-tuning on a large amount of data.\n\n7. From Figure 4, it seems that with increasing noise, despite RobustFlow starting at a high point, it exhibits a significant downward trend similar to other baselines, while AFlow tends to rise with increasing noise. How can this phenomenon be explained? Does this indicate that AFlow also possesses good robustness?"}, "questions": {"value": "1. Could the authors provide more details on the baseline reproduction in Table 2 to demonstrate the fairness of the comparison?\n\n2. The paragraph around line 200 showcases a manual check on the reformatted questions and workflows. Could you please elaborate on how such a large amount of data was manually verified?\n\n3. Sections 3.3 and 4.2 involve numerous hyperparameters. Could you specify the actual values of these hyperparameters used during the experimental process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BOCNW5JMHG", "forum": "YURHABfZIP", "replyto": "YURHABfZIP", "signatures": ["ICLR.cc/2026/Conference/Submission5528/Reviewer_EUP9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5528/Reviewer_EUP9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760824800038, "cdate": 1760824800038, "tmdate": 1762918111269, "mdate": 1762918111269, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies robustness in automatic agentic workflow generation—i.e., whether a generator produces consistent workflows when the query wording changes but semantics remain the same. The authors (1) diagnose instability in existing methods, (2) propose node- and topology-level robustness metrics to quantify workflow similarity under paraphrase, requirement augmentation, and noise, (3) release a dataset of 31,889 workflows from 1,255 base tasks across 6 domains, each with 5 semantic variations, and (4) introduce RobustFlow, a training framework using preference optimization on sets of synonymous task descriptions so the model learns invariance to wording."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses a concrete failure mode (semantic-equivalence brittleness) that undermines trust in workflow generators.\n2. The dataset construction is large-scale and designed specifically for robustness stress tests (paraphrase, requirement augmentation, controlled noise), which is appropriate for the claims.\n3. Preference optimization over clusters of synonymous prompts is a minimal, broadly applicable recipe"}, "weaknesses": {"value": "1. The evaluation datasets mainly consist of relatively simple QA-style or reasoning tasks (e.g., GSM8K, HotpotQA), which can often be solved by a single LLM without requiring genuine multi-step agentic planning or tool interaction. This raises doubts about whether RobustFlow would still perform well on truly complex agentic workflows—for example, multi-tool coordination, retrieval-augmented synthesis, or real-world decision pipelines where task decomposition and inter-agent communication are essential. Since agentic workflow systems are intended precisely for such non-trivial, multi-stage problems, the current evaluation does not demonstrate that RobustFlow generalizes beyond easy tasks.\n2. The experiments exclusively use Qwen3-30B as the base workflow generator. This raises the possibility that the observed robustness and consistency gains may partly reflect the intrinsic strength or calibration of Qwen3-30B rather than the proposed training method itself. Besides, the experiments utilize GPT-4o-mini as executors and lack results of using other stronger models.\n3. The paper omits several recent and highly relevant works in the agentic workflow generation domain in related works section.\n4. Several charts are hard to read (small fonts, dense legends), captions do not fully explain panels, and some figure layouts look crowded."}, "questions": {"value": "1. Do results hold under adversarial paraphrases (e.g., instruction inversion traps) and format perturbations (whitespace/code-stub reordering)?\n2. Does the method work for other base models as generators? e.g., 1.5B/3B/7B models, llama models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "bU65cviH9P", "forum": "YURHABfZIP", "replyto": "YURHABfZIP", "signatures": ["ICLR.cc/2026/Conference/Submission5528/Reviewer_79uy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5528/Reviewer_79uy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982155116, "cdate": 1761982155116, "tmdate": 1762918110733, "mdate": 1762918110733, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the problem of robustness in automated agentic workflow generation, proposing RobustFlow, a framework that enhances the consistency of workflows generated by large language models under semantically equivalent but syntactically varied instructions. The work identifies a meaningful gap in current literature that existing systems produce inconsistent structures when task descriptions are paraphrased or perturbed. To quantify this issue, the authors introduce a structure-aware evaluation suite based on node and graph-level similarity metrics and a large dataset of about 31K workflows generated under controlled perturbations. The proposed RobustFlow method integrates instruction-augmented supervised fine-tuning with a self-consistency preference optimization stage, yielding higher stability and robustness while maintaining acceptable task performance."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- **S1.** The paper addresses an important and underexplored robustness under input variation problem in LLM-based agentic workflow generation.\n\n- **S2.** The dataset of 1,255 task clusters with 6 perturbation variants each demonstrates impressive engineering rigor and experimental coverage.\n\n- **S3.** The two-stage RobustFlow pipeline (instruction-augmented SFT + self-consistency preference optimization) is well justified and shows consistent improvement in robustness across all perturbation types.\n\n- **S4.** The analysis includes insightful breakdowns by task domain, showing that linguistically abstract tasks such as QA are more fragile than structured domains like code."}, "weaknesses": {"value": "- **W1.** The method is tailored for query-level workflow generation. The paper does not adequately discuss how the framework would generalize to task-level workflow generation where multiple similar tasks require aligned or shared workflows.\n\n- **W2.** While the paper acknowledges a modest performance trade-off, it does not deeply analyze why the robustness optimization slightly degrades accuracy or how to mitigate it.\n\n- **W3.** The robustness evaluation focuses primarily on text-to-code and reasoning tasks. Broader validation on real multi-agent interaction or open-domain tool use would strengthen generalizability."}, "questions": {"value": "- **Q1.** Is RobustFlow applicable to task-level workflows that share structural similarity across related queries, rather than purely query-specific workflows?\n\n- **Q2.** Could robustness metrics be extended beyond structural similarity to incorporate functional equivalence, e.g., execution success consistency?\n\n- **Q3.** What are the primary failure modes when robustness decreases under heavy noise, e.g., semantic drift, structural mismatch, or execution instability?\n\n- **Q4.** How sensitive are the results to the base model (e.g., Qwen3-32B vs. smaller LLMs)? Does the approach scale down effectively?\n\n- **Q5.** Could the workflow representation format (graph vs. code) influence the robustness behavior or sensitivity to paraphrasing? Has this been empirically analyzed?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2a5T9FGM88", "forum": "YURHABfZIP", "replyto": "YURHABfZIP", "signatures": ["ICLR.cc/2026/Conference/Submission5528/Reviewer_Pnjn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5528/Reviewer_Pnjn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5528/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995002337, "cdate": 1761995002337, "tmdate": 1762918110108, "mdate": 1762918110108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}