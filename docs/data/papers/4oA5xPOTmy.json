{"id": "4oA5xPOTmy", "number": 7352, "cdate": 1758017292528, "mdate": 1759897857863, "content": {"title": "Multimodal Cancer Survival Analysis with Learnable Queries", "abstract": "Leveraging multimodal data, particularly the integration of whole-slide histology images (WSIs) and transcriptomic profiles, holds great promise for improving cancer survival prediction. However, excessive redundancy in multimodal data poses a critical challenge for model optimization and can become prohibitive. Thus, methods that effectively reduce redundancy are highly desirable. While previous approaches have achieved impressive results by clustering redundant representations, they still rely on additional prior knowledge, which limits their flexibility in capturing dynamic data changes and emerging patterns. To resolve this drawback, we propose a novel and effective approach, SurvQ, for multimodal cancer survival analysis with learnable queries, which adaptively learns representative features in a data-driven manner, reducing redundancy while preserving critical information. Our method employs two sets of learnable query vectors that serve as a bridge between high-dimensional representations and survival prediction, capturing task-relevant features. Additionally, we introduce a multimodal mixed self-attention mechanism to enable cross-modal interactions, further enhancing information fusion. Extensive experiments on five benchmark cancer datasets demonstrate that our method consistently outperforms state-of-the-art approaches, achieving the best average performance.", "tldr": "", "keywords": ["Survival analysis", "Multimodal learning", "Learnable queries"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5b1d2f3c146ffd0008f99a01e16754cb31994b9c.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces SurvQ, a novel framework for multimodal cancer survival analysis that addresses the critical challenge of information redundancy in histology (WSI) and genomic data. Unlike previous methods that rely on predefined, knowledge-based prototypes to cluster data, SurvQ employs a data-driven approach using \"learnable queries.\" The model utilizes two sets of learnable query vectors—one for histology and one for genomics—that interact with high-dimensional patch and pathway tokens via cross-attention. This mechanism acts as an adaptive information bottleneck, distilling vast and redundant inputs into a compact, task-relevant set of representative features. These learned queries are then fused using a unified multimodal mixed self-attention module to capture complex cross-modal interactions efficiently. Experiments conducted on five benchmark TCGA cancer datasets show that SurvQ achieves superior predictive performance."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Architectural Elegance and Efficiency: The proposed architecture is both effective and relatively simple. By using queries to drastically reduce the number of tokens before fusion, it allows for the use of a single, powerful \"multimodal mixed self-attention\" mechanism. \n\n2. Superior Empirical Performance: The method's effectiveness is strongly supported by the results.\n\n3. Good Interpretability: The paper provides clear visualizations demonstrating that the learnable queries capture biologically meaningful information."}, "weaknesses": {"value": "1. Fixed Number of Queries: A limitation, acknowledged by the authors, is that the number of learnable queries for each modality is a fixed hyperparameter. \n2. Reducing the redundancy of the attention mechanism through learnable queries is common in other fields[1]. I think this design is not novel enough.\n[1] Arar M, Shamir A, Bermano A H. Learned queries for efficient local attention[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 10841-10852\n3. There is no ablation experiment on the number of queries\n4. The article highlights the redundancy in the input but does not provide a detailed analysis of training time or computational resource usage compared to other methods (e.g. flops), which makes it difficult to quantify whether the redundancy is addressed.\n5. There are relatively few baselines for comparison, and most are not from 25 years ago."}, "questions": {"value": "1. Fixed Number of Queries: A limitation, acknowledged by the authors, is that the number of learnable queries for each modality is a fixed hyperparameter. \n2. Reducing the redundancy of the attention mechanism through learnable queries is common in other fields[1]. I think this design is not novel enough.\n[1] Arar M, Shamir A, Bermano A H. Learned queries for efficient local attention[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 10841-10852\n3. There is no ablation experiment on the number of queries\n4. The article highlights the redundancy in the input but does not provide a detailed analysis of training time or computational resource usage compared to other methods (e.g. flops), which makes it difficult to quantify whether the redundancy is addressed.\n5. There are relatively few baselines for comparison, and most are not from 25 years ago."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A6L7vPjG2V", "forum": "4oA5xPOTmy", "replyto": "4oA5xPOTmy", "signatures": ["ICLR.cc/2026/Conference/Submission7352/Reviewer_ubYy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7352/Reviewer_ubYy"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760494066175, "cdate": 1760494066175, "tmdate": 1762919487640, "mdate": 1762919487640, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the problem of excessive redundancy in multimodal data in multimodal cancer survival analysis. Previous methods rely too much on prior knowledge, limiting the flexibility in capturing dynamic data changes and emerging patterns. This paper proposes SurvQ, conducting multimodal cancer survival analysis with learnable queries. By adaptively learning representative features in a data-driven manner, the method can reduce redundancy while preserving important information in multimodal data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The proposed method is simple but effective. The approach generalizes the concept of query-based token compression (from DETR/BLIP-2) to the medical multimodal setting, replacing handcrafted prototype-based reductions with a data-driven mechanism.\n2. This paper is well-written and easy to follow. The figures are clear.\n3. The visualization of histology and genomic queries provides biological insight into the model’s internal representations."}, "weaknesses": {"value": "1. While the empirical results are strong, the paper lacks deeper theoretical or information-theoretic analysis explaining why query-based bottlenecks improve generalization or reduce redundancy.\n2. The evaluation is restricted to cancer survival prediction on TCGA datasets. It would strengthen the contribution to demonstrate the general applicability of SurvQ to other multimodal biomedical tasks."}, "questions": {"value": "1. Are there any observed correlations between specific queries and known clinical or molecular subtypes?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Zc4sEW6iTg", "forum": "4oA5xPOTmy", "replyto": "4oA5xPOTmy", "signatures": ["ICLR.cc/2026/Conference/Submission7352/Reviewer_VhfN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7352/Reviewer_VhfN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761422811032, "cdate": 1761422811032, "tmdate": 1762919487035, "mdate": 1762919487035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes SurvQ, a multimodal cancer survival analysis framework that integrates whole-slide histology images (WSIs) and transcriptomic profiles using learnable queries. Specifically, the authors introduce two sets of learnable query vectors that interact with unimodal features via cross-attention to extract representative pathology and genomic features, followed by a mixed self-attention module for multimodal fusion. The method is evaluated on five TCGA datasets and demonstrates better performance compared to prior multimodal and prototype-based methods."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "1. Data-driven prototype learning via learnable queries provides a simple yet effective solution.\n2. The model achieves better results across multiple TCGA cohorts, clearly outperforming both unimodal and prior multimodal baselines.\n3. Visualization of attention maps and the top 6 pathways provides some interpretability, linking learned queries to meaningful histological and molecular patterns."}, "weaknesses": {"value": "1. Conceptually incremental — mainly adapting the learnable-query idea from prior works (e.g., BLIP-2, DETR) to this setting.\n2. The baseline design of the ablation study is weak; it doesn’t isolate the effect of “learnability.” A fixed/random query baseline would make the comparison fairer.\n3. Interpretability focuses on unimodal patterns, cross-modal interactions (e.g., which histology regions relate to which genomic pathways) are not explored.\n4. The number of queries is fixed for all datasets, which may not be optimal."}, "questions": {"value": "1. Do the authors employ any mechanism (e.g., orthogonal regularization,  contrastive objectives) to encourage diversity or competition among the learnable queries? Otherwise, queries may collapse to similar representations.\n2. Could the authors show cross-modal attention maps to verify the interaction between modalities?\n3. Have the authors evaluated the computational cost or memory benefit of query compression? A quantitative comparison of efficiency would make the “redundancy reduction” claim more concrete."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4XsxqRRyLu", "forum": "4oA5xPOTmy", "replyto": "4oA5xPOTmy", "signatures": ["ICLR.cc/2026/Conference/Submission7352/Reviewer_7uMb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7352/Reviewer_7uMb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761826386901, "cdate": 1761826386901, "tmdate": 1762919486481, "mdate": 1762919486481, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work proposes to utilize two sets of learnable queries to extract representative features via cross-attention, while multimodal mixed self-attention is leveraged to model cross-modal interactions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The motivation is clear, and the manuscript is well-written.\n2. The performance of the proposed method is superior to SOTA approaches.\n3. The effectiveness of each component is validated by ablation studies."}, "weaknesses": {"value": "1. The novelty is incremental and very simple. The core idea follows PIBD by using a set of learnable parameters to capture representative features for each modality. The difference is that PIBD enforces a risk level constraint to assist the model in learning discriminative features, while there is no prior constraint in the proposed method. Additionally, the idea of learnable queries has been explored in G-HANet [1], which has validated its effectiveness.\n2. The insight about why it works is lacking. Given that there is no explicit constraint for the modelling, although the story is well-told, I'm still confused about why it achieves the intended purpose without any explicit guidance or constraint, and why it is better than PIBD with explicit conditions.\n\n[1] Wang Z, Zhang Y, Xu Y, et al. Histo-genomic knowledge association for cancer prognosis from histopathology whole slide images[J]. IEEE Transactions on Medical Imaging, 2025."}, "questions": {"value": "See weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NCANmSq41C", "forum": "4oA5xPOTmy", "replyto": "4oA5xPOTmy", "signatures": ["ICLR.cc/2026/Conference/Submission7352/Reviewer_sERY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7352/Reviewer_sERY"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7352/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970469500, "cdate": 1761970469500, "tmdate": 1762919485858, "mdate": 1762919485858, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}