{"id": "5UbeQDlYDj", "number": 17987, "cdate": 1758282683496, "mdate": 1759897140751, "content": {"title": "TABLET: A Large-Scale Dataset for Robust Visual Table Understanding", "abstract": "While table understanding increasingly relies on pixel-only settings where tables are processed as visual representations,  current benchmarks predominantly use synthetic renderings that lack the complexity and visual diversity of real-world tables. Additionally, existing visual table understanding (VTU) datasets offer fixed examples with single visualizations and pre-defined instructions, providing no access to underlying serialized data for reformulation. We introduce TABLET, a large-scale VTU dataset with 4 million examples across 20 tasks, grounded in 2 million unique tables where 88% preserve original visualizations. Each example includes paired image-HTML representations, comprehensive metadata, and provenance information linking back to the source datasets. Fine-tuning vision-language models like Qwen2.5-VL-7B on TABLET improves performance on seen _and_ unseen VTU tasks while increasing robustness on real-world table visualizations. By preserving original visualizations and maintaining example traceability in a unified large-scale collection, TABLET establishes a foundation for robust training and extensible evaluation of future VTU models.", "tldr": "We introduce TABLET, a large-scale VTU dataset with 4 million examples across 20 tasks, grounded in 2 million unique tables where 88% preserve original visualizations.", "keywords": ["Visual Table Understanding", "Table Understanding", "Datasets", "Visually Represented Language", "Multimodal Table Understanding"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/216449da6eba122fa98d008f5f207c0d95114915.pdf", "supplementary_material": "/attachment/e8d64f8599536495101a3cd3053a5d6c0e8a3e08.zip"}, "replies": [{"content": {"summary": {"value": "Current visual table understanding (VTU) benchmarks rely on synthetic data lacking real-world complexity and omit underlying serialized data, while TABLET is a large-scale dataset with 4 million examples from real visualizations, paired image-HTML representations, and 20 tasks. Fine-tuning vision-language models on TABLET boosts their performance on seen/unseen VTU tasks and robustness to real-world tables, establishing a foundation for future VTU model training and evaluation."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "TABLET addresses prior single-task/synthetic data limitations with large-scale, lossless real-world table visualizations and 20 diverse tasks, while its high quality lies in 4M examples, image-HTML pairings, and traceable metadata. Its clear unified format and extensible design, coupled with significant contributions to boosting VLM cross-task performance (including SOTA results) and enabling practical VTU research, underscore its field value."}, "weaknesses": {"value": "1. Whether all tables are presented in English? As far as I am aware, Wikipedia provides original HTML files in multiple languages.\n2. Each example in TABLET is accompanied by the HTML version of the corresponding table. Can such a large volume of data be used to train table parsing models? Is the annotation format of HTML consistent? For instance, how are formulas within the tables annotated? Additionally, do these HTML files contain certain formatting information, such as row widths and font colors?"}, "questions": {"value": "see the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "34eEiHvLuG", "forum": "5UbeQDlYDj", "replyto": "5UbeQDlYDj", "signatures": ["ICLR.cc/2026/Conference/Submission17987/Reviewer_qfkf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17987/Reviewer_qfkf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission17987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901805154, "cdate": 1761901805154, "tmdate": 1762927783340, "mdate": 1762927783340, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies a critical \"train-test mismatch\" in the field of Visual Table Understanding (VTU). The authors argue that existing benchmarks predominantly use \"synthetic renderings\" of tables, which lack the visual complexity (e.g., merged cells, colors, fonts, embedded images) of real-world tables. This causes models trained on them to fail when generalizing to real-world visual data.\n\nTo solve this, the authors introduce TABLET, a new large-scale dataset of 4 million examples over 20 tasks. The dataset's primary contribution is that 88% of its 2 million unique tables are \"original visualizations\" meticulously retrieved from historical web snapshots (primarily Wikipedia), preserving their true visual fidelity. The authors demonstrate through experiments that while performance on traditional tasks is mixed, training on TABLET dramatically improves a model's robustness to visual domain shift (synthetic vs. original) and enhances generalization to unseen tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Problem Identification:** The paper's core premise is strong and well-articulated. It correctly identifies the \"train-test mismatch\" as a fundamental problem for the VTU field as it shifts towards pixel-only VLMs. This is a significant and practical issue.\n\n2. **Methodological Rigor on Data Collection:** The primary contribution is the dataset's \"visual fidelity.\" The engineering effort to achieve this is non-trivial and highly commendable. The authors detail a rigorous process of tracing seed datasets to their original crawl dates and using Wikipedia's archiving API to retrieve historical snapshots, followed by Levenshtein matching to find the correct table. This high-quality execution is a major strength.\n\n3. **Strong Robustness Argument:** The paper's most compelling evidence is in Table 9. It clearly shows that models trained on synthetic data suffer a massive performance degradation (-22.35 points) when evaluated on original visualizations. In contrast, models trained on TABLET's original data are far more robust (only -6.63 points). This single finding strongly validates the paper's core hypothesis and the necessity of this dataset.\n\n4. **Resource Value and Extensibility:** The paper delivers more than just a static benchmark; it provides a large-scale, multi-task resource. By including HTML representations, metadata, and traceability links to source datasets, the authors enable future research, task reformulation, and extensibility."}, "weaknesses": {"value": "1. **Core Claim Undermined by Own Results:** The paper's most significant weakness is that its primary experiment (Table 2) fails to support its central claim. When comparing models trained on TABLET-B_org (original) vs. TABLET-B_synth (synthetic), the performance is roughly equivalent across most tasks. This result is underwhelming and directly contradicts the motivation that visual fidelity improves performance on these tasks.\n\n2. **Circular Argument:** The authors' explanation for the failure of Table 2 is that the benchmarks themselves (e.g., ToTTo, WikiTQ) are flawed and were designed for text, not visual cues. This is a \"catch-22\" and a form of circular reasoning: the paper claims its dataset is the solution but simultaneously admits that the tasks it uses for validation are incapable of proving it. They have built a high-fidelity testbed but failed to provide tasks that actually require that fidelity.\n\n3. **Diluted Conclusion on \"Mixed\" Data:** In several experiments (Table 2, Table 10), the TABLET-B_mix (original + synthetic) model performs best. This dilutes the paper's central premise. The takeaway risks becoming \"more data is better\" (a trivial conclusion) rather than the intended, stronger claim that \"high-fidelity data is better.\"\n\n4. **Limited \"Real-World\" Diversity:** The paper claims to solve the \"real-world\" visualization problem, but the dataset's diversity is questionable. 88% of the data (61.5% of tables) is sourced from Wikipedia. While better than synthetic data, Wikipedia tables are still highly structured and relatively uniform. This is not representative of the true \"in-the-wild\" chaos of scanned PDFs, financial reports, or product pages."}, "questions": {"value": "1. Given that the core experiment in Table 2 fails to show a clear superiority for TABLET-B_org over TABLET-B_synth, and you attribute this to flawed benchmarks (e.g., ToTTo), does this not imply that the primary contribution of this work is a resource for which the tasks do not yet exist?\n\n2. The strong performance of the TABLET-B_mix model seems to suggest that a combination of synthetic and original data is optimal. Does this not contradict the paper's central premise that synthetic renderings are inherently flawed and should be replaced by original visualizations?\n\n3. Why were new, visually-demanding tasks (e.g., \"find the cell with the red background,\" \"what is the value in the merged cell?\") not introduced and evaluated to actually prove the hypothesis that the visual fidelity of TABLET is its key advantage?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "sIcCuUSe7T", "forum": "5UbeQDlYDj", "replyto": "5UbeQDlYDj", "signatures": ["ICLR.cc/2026/Conference/Submission17987/Reviewer_WHyK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17987/Reviewer_WHyK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission17987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761984451343, "cdate": 1761984451343, "tmdate": 1762927781913, "mdate": 1762927781913, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces TABLET, a large-scale dataset for visual table understanding (VTU). Compared to existing resources (e.g., MMTab), TABLET (i) reconstructs tables in their realistic visual form from web/Wikipedia sources, (ii) unifies them into an image + HTML + metadata format, and (iii) covers 20 VTU-related tasks. The authors fine-tune Qwen2.5-VL-7B on this data and show improvements on both seen and unseen VTU tasks. The main value of the paper is on the data/engineering side and on providing a more realistic training distribution."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.The motivation is real: current VTU data is mostly synthetic or visually simplified, which causes a distribution gap for vision-language models that must operate on real web-like tables.\n\n2.The dataset is large, diverse, and traceable back to the original sources, which is helpful for reproducibility and later task extension.\n\n3.Experiments are fairly comprehensive (synthetic vs. real-style vs. mixed) and the results are consistent with the stated motivation."}, "weaknesses": {"value": "1. The paper does not introduce a new learning objective, a new model architecture, or a principled framework for unifying VTU tasks. The core message is essentially “better/realistic data → better performance,” which is valuable but closer to a dataset/empirical paper than to a method/theory paper.\n\n2. The paper repeatedly argues that preserving realistic visual styles leads to better generalization, but the current evaluations mostly show dataset-level gains, not an analysis of which visual factors (borders, multi-row headers, fonts, layout noise) the model actually exploits."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Rq2g3Ol0Gb", "forum": "5UbeQDlYDj", "replyto": "5UbeQDlYDj", "signatures": ["ICLR.cc/2026/Conference/Submission17987/Reviewer_P6aP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17987/Reviewer_P6aP"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission17987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762086801186, "cdate": 1762086801186, "tmdate": 1762927781317, "mdate": 1762927781317, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TABLET, a large-scale dataset for UTV, containing 4M examples. The key contribution is the collected TABLET from sources from Wikipedia and PubTabNet. The dataset are split into train, dev, and test sets and comprising comprehensive table-related tasks. Experiments with Qwen2.5-VL-7B show good performance gains on several benchmarks, and the authors argue that TABLET improves robustness on real-world table images."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+It collects a large-scale dataset combining multiple VTU sources. \n+It provides both HTML and image representations with metadata, which facilitate the document understanding community. \n+Experiments on the Qwen2.5-VL models demonstrating the effectiveness of the collected dataset."}, "weaknesses": {"value": "1. The author presents a new dataset, TABLET, and the author claims that the dataset contains 4M examples. However, nearly 30% are existing, the author should describe this more rigorously.\n2. No new model, algorithm, or theoretical insight in the paper. The paper mainly merges existing datasets and collected new table data on the network. \n3. One of the author’s main motivation is that synthetic data are of lower quality than real-world data. However, the majority of the dataset they collected still consists of screenshot-based images, many of which are taken from sources like Wikipedia. This seems somewhat inconsistent with the author’s stated motivation?\n4. Poor experiments. In Tab.2 and 3, the performance improvement by the proposed dataset appears to be limited.\n5. What confuses me is that the paper only reports results for qwen and the results fine-tuned on qwen, without any comparison to other models. This is completely unreasonable for a research paper."}, "questions": {"value": "Please refer to the weaknesses section above. \nIt seems that the author merely proposed a dataset and fine-tuned qwen with it, without providing comparisons to other models. I believe the paper still needs further polishing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "tn5fpDN6Vr", "forum": "5UbeQDlYDj", "replyto": "5UbeQDlYDj", "signatures": ["ICLR.cc/2026/Conference/Submission17987/Reviewer_czgq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission17987/Reviewer_czgq"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission17987/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762137543297, "cdate": 1762137543297, "tmdate": 1762927780947, "mdate": 1762927780947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}