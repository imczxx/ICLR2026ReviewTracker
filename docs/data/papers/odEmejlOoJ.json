{"id": "odEmejlOoJ", "number": 11943, "cdate": 1758204834359, "mdate": 1759897543635, "content": {"title": "MusiXQA: Advancing Visual Music Understanding in Multimodal LLMs", "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual reasoning abilities in natural images, text-rich documents, and graphic designs. However, their ability to interpret music sheets remains underexplored. To bridge this gap, we introduce MusiXQA, the first comprehensive dataset for evaluating and advancing MLLMs in music sheet understanding. MusiXQA features high-quality synthetic music sheets generated via MusiXTeX, with structured annotations covering note pitch and duration, chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks. Through extensive evaluations, we reveal significant limitations of current state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant performance gains over GPT-based methods. The proposed dataset and model establish a foundation for future advances in MLLMs for music sheet understanding. Code, data, and model will be released upon acceptance.", "tldr": "We present MusiXQA, the first benchmark for evaluating and improving MLLMs on symbolic visual understanding of music sheets, and introduce Phi-3-MusiX, a fine-tuned MLLM that significantly outperforms existing models on this task.", "keywords": ["Multimodal Large Language Models", "Visual Question Answering", "Benchmark Dataset", "Optical Music Recognition"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/07b9409c042f2ee6c18056ed15fc15c40e0dc1c8.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper tackles the inability of current MLLMs to understand sheet music by introducing MusiXQA, the first comprehensive visual question-answering dataset for this task. It features high-quality synthetic music sheets generated via MusiXTEX, paired with diverse QA tasks covering pitch, chords, and more. Based on this data, the authors' fine-tuned Phi-3-MusiX model significantly outperforms baselines like GPT, demonstrating the dataset's effectiveness in advancing music sheet understanding."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental design is excellent. By using two distinct metrics—one for format (PNLS) and one for semantic accuracy (G-Acc)—the authors cleverly demonstrate that powerful baselines like GPT-4o were not truly understanding the music but merely \"mimicking\" the format of the correct answer .\n2. The paper is the first to systematically address a clear and challenging gap in AI: the inability of advanced MLLMs to read and understand symbolic sheet music. It introduces a comprehensive benchmark, MusiXQA, to drive progress in this new area."}, "weaknesses": {"value": "1. The model's performance on noisy, real-world scanned music is unknown, as it was trained exclusively on \"perfect\" synthetic data, creating a significant \"sim-to-real\" gap.\n2. The OMR-assisted baseline (GPT-4o + RAG + OMR) is weak because it relies on an OMR tool that the authors admit has \"limited accuracy\" and omits critical information like accidentals.\n3. The dataset's musical diversity is limited, as it intentionally excludes common elements like $6/8$ time 3and relies on randomly generated, musically incoherent notes rather than real compositions 4."}, "questions": {"value": "see Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "kbtS8DhG7e", "forum": "odEmejlOoJ", "replyto": "odEmejlOoJ", "signatures": ["ICLR.cc/2026/Conference/Submission11943/Reviewer_u5jf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11943/Reviewer_u5jf"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761759676501, "cdate": 1761759676501, "tmdate": 1762922946918, "mdate": 1762922946918, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the effectiveness of applying VLMs to music sheet understanding. Authors constructed a synthetic music sheet\ndataset MusiXQA containing structured annotations. VQA-like tasks are used to train and evaluation VLMs' ability on understanding music\nsheets. Specifically, the tasks of the annotations include information extraction, sheet layout understanding, and chord estimation etc.\n\nAuthors show that SOTA VLMs perform badly on the MusicXQA benchmark, and then finetuned a Phi3 model on MusicXQA, achieving the best performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well structured, and easy to follow. Experiment and results are well presented.\n- Creating resources and datasets for the music sheet understanding task, which is resource-lean, is appreciated."}, "weaknesses": {"value": "- The MusicXQA dataset is completely synthetic (e.g., music title, composer name etc.), and I am concerned about real-world performances of a model trained and evaluated on MusiXQA. For example, authors mentioned that their created dataset \"do not need to be musically coherent or aesthetically refined\", as a result, I think this makes MusicXQA tasks more like general segmentation or extraction tasks for symbolic music notes, orthogonal to music understanding. So I am concerned about thow MusicXQA helps creating strong music understanding models.\n\n- When randomly generating MusicXQA, some settings/parameters seem used ad hoc, without supporting evidences (cf questions).\n\n- Authors finetuned a Phi3 model using MusicXQA training data and show it performs the best on MusicXQA test data. This is very much   expected and unfair to other baselines. I think it is needed to also evaluate the models in OOD settings, such as MMMU's music sheet questions."}, "questions": {"value": "- Line207: how did you decide to use 1-10 words for the title and 1-3 words for author names? Do these reflect the real-world cases?\n\n- Line232: how did you decide to use the number of bars (10-20) and number of notes?\n\n- Line269: why using four spacing settings and two note size settings?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Y9MSV4ff25", "forum": "odEmejlOoJ", "replyto": "odEmejlOoJ", "signatures": ["ICLR.cc/2026/Conference/Submission11943/Reviewer_NL27"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11943/Reviewer_NL27"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761815526309, "cdate": 1761815526309, "tmdate": 1762922946530, "mdate": 1762922946530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work aims dealing with a specific multi-modal task, which is interpret and understand music sheet images. A dataset, MusiXQA is proposed to help training MLLMs on this task. In the end, Phi-3-MusiX, a fine-tuned version of Phi-3-Vision, is proposed and demonstrated its performance advantage across four types of music sheet understanding tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Well-motivated problem setup in a specific domain that is not resolved yet but demanding for people working on music analysis.  \n\n- No real-world MIDI used, therefore no issues on publishing rights.\n\n- The task definition in section 3.5 is well categorized and covered the necessary components of the entire workflow of \"music sheet interpretation\". Especially the OMR-based tasks and Chord estimations tasks. The former checks symbolic understanding and latter can evaluate semantic understanding.\n\n- According to Table 1, fine-tuning Phi-3-Vision with MusiXQA indeed yield huge improvement on all the four types of tasks. Demonstrating the usefulness of the proposed dataset.\n\n- The subsection about performance difference between JSON/kern+ is insightful. On the other hand, if semantic density is the key, it's interesting know if any alternative can serve the purpose."}, "weaknesses": {"value": "- Since all the MusiXQA images are all generated by MusiXTEX, a concern is that the image encoder trained on this dataset may lack generalizability for sheet images in general. For example, size of notes, fonts, minor style variation, or hand-writing sheets.\n\n- Some design of MusiXQA seems to be too restrictive and can only cover a limited subset of music sheets. For example, the pitch range of A\\flat1 ~ F\\sharp6 cannot cover the note range of tuba, bass, violin or piccolo. More examples are:\na) Each word in the name is limited to 3-8 characters.\nb) Seems ruling out orchestral music by assuming only two staves.\nc) The BPM range of 50-140 could not cover certain music styles and instrument sheets.\nd) Time signature is limited to 2/4, 3/4, 4/4.\ne) Seems do not support scale/tone switching\nf) Only support up to 1/16th notes.\ng) Always begin with the tonic chord of the key makes the generation easier, but this is not the case in real-world and could cause over-fitting for models trained on such data.\nh) no accidentals\n\nIt's understandable that some of the decision is limited by the need to obtain the semantic ground truth from randomly generated score. Would it be better if creating the dataset in two parts, the first part includes as much as variations for symbolic understanding, and the second part is a subset of the first and accompanied with QA pairs for semantic understanding tasks?"}, "questions": {"value": "- In section 4.1, about RAG-like in-context learning, how does the \"most relevant\" training example is determined?\n\n- In section 4.3, is there any procedure to validate that the evaluation of GTP-4o is correct at most of the time?\n\n- Table 1: Is there any insight on why the model trained on kern+ has much worse G-Acc on Layout understanding task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Nc2Sm5uWoB", "forum": "odEmejlOoJ", "replyto": "odEmejlOoJ", "signatures": ["ICLR.cc/2026/Conference/Submission11943/Reviewer_ZSTE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11943/Reviewer_ZSTE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996843411, "cdate": 1761996843411, "tmdate": 1762922946044, "mdate": 1762922946044, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the underexplored capability of Multimodal Large Language Models (MLLMs) to understand music sheets. The authors introduce MusiXQA, a new, large-scale benchmark dataset for this task. The dataset contains 9,600 high-quality synthetic music sheets generated using MusiXTEX, paired with over 130,000 visual question-answering (VQA) pairs. The VQA tasks cover four categories: OCR, Layout Understanding, Optical Music Recognition (OMR), and Chord Estimation.\nThe authors evaluate current SOTA MLLMs (like GPT-4o and Paligemma2) and find they perform very poorly, with GPT-4o failing on OMR tasks and Paligemma2 refusing to answer them entirely. To demonstrate the value of their dataset, they fine-tune a Phi-3-Vision model, creating Phi-3-MusiX. This model achieves significant performance gains, such as an 8x improvement in G-Acc on OMR tasks over the best GPT-based baseline. A key finding is that a compact, symbolic output representation (kern+) vastly outperforms a verbose JSON format , as the latter causes the model to learn \"format tokens\" rather than \"content tokens\"."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Identifies a clear and difficult gap in the capabilities of current SOTA MLLMs.\n- Introduces MusiXQA, the first large-scale VQA benchmark specifically designed for music sheet understanding in MLLMs.\n- Provides an excellent and insightful analysis of output representation, demonstrating that compact, symbolic formats (kern+) are far superior to verbose formats (JSON) for fine-tuning MLLMs on structured prediction tasks.\n- The fine-tuned Phi-3-MusiX model shows massive (8x) performance gains over SOTA baselines, proving the dataset's value for domain-specific adaptation."}, "weaknesses": {"value": "- Synthetic Data Limitation: The paper's main weakness is that the MusiXQA dataset is purely synthetic, generated from MusiXTEX. This ignores the primary challenges of real-world OMR, such as scanned artifacts, image noise, and handwritten notation, which are addressed by prior OMR datasets (e.g., CVC-MUSCIMA).\n- Poor Generalization: The paper positions MLLMs as a promising alternative to OMR pipelines. However, a model (Phi-3-MusiX) trained exclusively on this pristine, synthetic data is highly unlikely to generalize to real-world scanned or handwritten scores. The paper benchmarks a \"clean\" problem, not the real, messy one.\n- Unfair Efficiency Comparison: The paper claims an efficiency advantage over the OMR tool Oemer. This is an apples-to-oranges comparison. Oemer is designed to handle the much harder task of real-world (and likely distorted) scans, while the MLLM is only processing its own clean, synthetic training distribution."}, "questions": {"value": "- The core weakness is the synthetic nature of the data. How do the authors expect Phi-3-MusiX, trained only on MusiXQA, to perform on real-world scanned music sheets or on handwritten OMR datasets like CVC-MUSCIMA?\n- Given the lack of real-world noise, isn't the claim of MLLMs being a promising alternative to OMR pipelines  premature?\n- Could the MusiXTEX generation framework be extended to simulate real-world distortions (e.g., page curl, scanner noise, faded ink, slight rotations) to help bridge this synthetic-to-real gap?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Y6Iffk9wxV", "forum": "odEmejlOoJ", "replyto": "odEmejlOoJ", "signatures": ["ICLR.cc/2026/Conference/Submission11943/Reviewer_UrLQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11943/Reviewer_UrLQ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11943/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762883700621, "cdate": 1762883700621, "tmdate": 1762922945045, "mdate": 1762922945045, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}