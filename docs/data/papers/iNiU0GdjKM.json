{"id": "iNiU0GdjKM", "number": 14385, "cdate": 1758234270194, "mdate": 1759897373587, "content": {"title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models", "abstract": "Large Language Models increasingly mediate high-stakes interactions, intensifying research on their capabilities and safety. While recent work has shown that LLMs exhibit consistent and measurable synthetic personality traits, little is known about how modulating these traits affects model behavior. We address this gap by investigating how psychometric personality control grounded in the Big Five framework influences AI behavior in the context of capability and safety benchmarks. Our experiments reveal striking effects: for example, reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well as reduction in general capabilities as measured by MMLU. These findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and general competence. We discuss the implications for safety evaluation, alignment strategies, steering model behavior after deployment, and risks associated with possible exploitation of these findings. Our findings motivate a new line of research on personality-sensitive safety evaluations and dynamic behavioral control in LLMs.", "tldr": "We investigate how psychometric personality control grounded in the Big Five framework influences AI behavior in the context of capability and safety benchmarks.", "keywords": ["artificial intelligence", "language models", "psychometrics", "AI safety", "alignment"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b88cebb05e10fb794bafeaf922bab275b33b7793.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper investigates how prompting LLMs to adopt Big Five personality traits affects their performance on capability benchmarks (MMLU) and safety benchmarks (TruthfulQA, WMDP, ETHICS, Sycophancy). \nThe authors extend prior work on synthetic personalities in LLMs by examining downstream effects on safety-relevant behaviors, using validated trait markers and psychometric instruments like IPIP-NEO and SD3.\nExperiments on models such as GPT-4 and the Llama family of variants show that personality prompts can decouple safety from capabilities: for example, low Conscientiousness drops safety and capabilities, while dark triad composites degrade safety without harming general knowledge. \nContributions of this paper include evidence challenging some of the safetywashing claims, implications for post-deployment steering, and calls for personality-sensitive evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. The experimental setup is comprehensive, and the analysis is interesting and insightful.\n2. It bridges the gap between established human psychometric theory (the Big Five) and the empirical study of LLM alignment, making it a creative and promising interdisciplinary contribution."}, "weaknesses": {"value": "1. This paper’s approach is entirely based on prompt engineering. However, LLMs are very sensitive to prompts. This paper lacks a case study to analyze whether LLMs effectively answer questions or simply output irrelevant responses based on specific prompts, resulting in reduced accuracy (especially in models with smaller parameters, such as Llama-3-8B)\n2. As a reader, I am more concerned with the Llama-3-8B model, which has relatively small parameters, but it is rarely analyzed in the main text.\n\nMinor comments:\n- Line 182: Typo: quantifiers -> qualifier, and from the example, the red part corresponds to the marker rather than the qualifier\n- Line 320: Is it missing a reference?"}, "questions": {"value": "Please see Weaknesses above, add case studies, and analyze the specific reasons for the changes in performance on these benchmarks. Is the model providing valid answers, or simply refusing to answer, giving a perfunctory answer (\"Who cares!\")?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "GCejhqr7GI", "forum": "iNiU0GdjKM", "replyto": "iNiU0GdjKM", "signatures": ["ICLR.cc/2026/Conference/Submission14385/Reviewer_rSD5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14385/Reviewer_rSD5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761465905164, "cdate": 1761465905164, "tmdate": 1762924801236, "mdate": 1762924801236, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how prompting large language models with personality trait descriptors based on the Big Five framework affects their performance on capability and safety benchmarks. The authors use Goldberg's trait markers (104 adjectives) to condition models with specific personality profiles (e.g., high/low conscientiousness, extraversion) and evaluate five models across benchmarks, including MMLU, TruthfulQA, WMDP, ETHICS, and Sycophancy."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The study addresses an interesting and important question: the relationship between language model personality and safety behaviors. This topic bridges psychological theory and AI alignment, making it both timely and significant for the community.\n\n2. The use of established psychometric instruments such as IPIP-NEO and SD3 to assess and validate personality manipulations adds methodological rigor and helps ground the work in over a century of psychological research. This integration of validated psychological tools into AI evaluation frameworks is a valuable and commendable contribution."}, "weaknesses": {"value": "1. The core contribution is essentially the application of established personality psychology prompts to existing benchmarks and the observation of correlations. However, the paper does not discuss several highly related prior works, including prompt-based approaches [1] and latent representation–based methods [2][3], which have already explored similar directions of controlling or inferring model personality.\n\n2. The authors do not evaluate whether personality control through prompts is reliable or consistent, nor do they compare it with alternative personality control mechanisms (e.g., fine-tuning or latent manipulation). This omission limits both the novelty and credibility of the conclusions.\n\n3. The experimental analysis lacks depth. The paper provides no explanation or theoretical reasoning for why personality prompts yield the observed safety or behavioral effects, leaving the mechanism largely speculative.\n\n4. The result presentation is difficult to follow. Only Figure 2 summarizes the findings, yet the caption states that “columns are model families,” while the figure itself mixes individual model names and families. This inconsistency makes interpretation unclear and hinders reproducibility.\n\n[1] Li, Guohao, et al. \"Camel: Communicative agents for\" mind\" exploration of large language model society.\" Advances in Neural Information Processing Systems 36 (2023): 51991-52008.\n[2] Chen, Runjin, et al. \"Persona vectors: Monitoring and controlling character traits in language models.\" arXiv preprint arXiv:2507.21509 (2025).\n[3] Ghandeharioun, Asma, et al. \"Who's asking? User personas and the mechanics of latent misalignment.\" Advances in Neural Information Processing Systems 37 (2024): 125967-126003."}, "questions": {"value": "Please check the weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "u4oyaCa4AI", "forum": "iNiU0GdjKM", "replyto": "iNiU0GdjKM", "signatures": ["ICLR.cc/2026/Conference/Submission14385/Reviewer_3JpW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14385/Reviewer_3JpW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761837694039, "cdate": 1761837694039, "tmdate": 1762924800603, "mdate": 1762924800603, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper examines how prompt-based psychometric personality shaping affects both the capabilities and safety performance of large language models (LLMs). Using Big-Five trait prompts (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism), the authors evaluate multiple models—including GPT-4.1, Llama-3/4, and DeepSeek-V3—on MMLU, TruthfulQA, ETHICS, WMDP, and Sycophancy benchmarks. They report that decreasing Conscientiousness or Agreeableness sharply reduces safety scores (−20–40 pp) and that these changes occur largely independent of model scale or general capability. The paper concludes that personality shaping provides an orthogonal axis of behavioral control and calls for personality-sensitive safety evaluations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper offers an intellectually engaging exploration of psychometric personality shaping in large language models (LLMs), connecting established psychological constructs (Big Five traits) to AI alignment and safety evaluation. The writing is clear, well-structured, and grounded in relevant theory, with polished figures and a strong interdisciplinary framing. The empirical findings, that reduced Conscientiousness or Agreeableness correlates with degraded ethical and truthful behavior, are intuitively compelling and align with known psychological evidence. The study also makes a valuable ethical contribution by discussing potential misuse (e.g., inducing adversarial personas) and proposing mitigations. While conceptually similar ideas exist, the authors effectively articulate why personality-sensitive evaluation may complement existing safety protocols, highlighting an underexplored but practically relevant perspective."}, "weaknesses": {"value": "The paper’s central limitation is lack of novelty and empirical rigor relative to recent work. Several 2025 studies, most notably Li et al. (2025, BIG5-CHAT) and Handa et al. (2025, Personality as a Probe for LLM Evaluation), already demonstrate similar findings with stronger methodologies, including training-based or mechanistic personality shaping. By contrast, this submission relies solely on prompt-based manipulation, without robustness checks, significance testing, or multi-run validation. The personality validation via self-report questionnaires (IPIP-NEO, SD3) is conceptually interesting but circular, since models are prompted to reproduce those constructs. The study lacks control for prompt brittleness and does not distinguish stylistic mimicry from genuine behavioral change. Finally, claims such as “putting all safety benchmarks into question” are overstated relative to the descriptive evidence, which undermines the overall scientific precision of the contribution."}, "questions": {"value": "1. How robust are the reported effects to prompt paraphrasing, random seed changes, or small lexical variations in the personality descriptions?\n2. Can the authors provide statistical confidence intervals or variance estimates to support claims of “systematic” effects?\n3. How does prompt-based shaping compare quantitatively with training-based or mechanistic methods such as those in BIG5-CHAT or Handa et al. (2025)?\n4. Could the observed benchmark differences arise from changes in linguistic tone or verbosity rather than latent personality expression?\n5. Have the authors examined whether these effects persist over multi-turn conversations or decay over time, and if so, how stable are shaped personalities?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BXrnUF0Api", "forum": "iNiU0GdjKM", "replyto": "iNiU0GdjKM", "signatures": ["ICLR.cc/2026/Conference/Submission14385/Reviewer_ycgg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14385/Reviewer_ycgg"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761863577450, "cdate": 1761863577450, "tmdate": 1762924800035, "mdate": 1762924800035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates how psychometric personality shaping, based on the Big Five (OCEAN) framework, influences both capability and safety behaviors in LLMs. The authors design prompt-based personality modulation to simulate different personality traits and evaluate multiple models across a suite of benchmarks, including MMLU, ETHICS, TruthfulQA, WMDP, and Sycophancy.\n\nThe paper argues that personality shaping provides a lightweight, prompt-based mechanism for both behavioral steering and adversarial exploitation, motivating “psychometric control” as a new research frontier for LLM safety."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Psychological safety of LLMs is a timely and critical topic for AI safety.\n\n2. This paper provides empirical results and analysis through psychological theory for LLMs."}, "weaknesses": {"value": "1. Several related works are overlooked (e.g., Evaluating Psychological Safety of Large Language Models [EMNLP 2024]).\n\n2. Analyses remain descriptive (percentage-point differences, heatmaps). Missing significance testing, confidence intervals, or effect-size analyses beyond normalized deltas.\n\n3. Prompt-based methods are often not able to consistently maintain the traits of LLMs, and do not shift the internal property of LLMs. I would suggest the authors to further investigate based on the parameter-level analysis and try tuning-based methods."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "qwLMBBhIhX", "forum": "iNiU0GdjKM", "replyto": "iNiU0GdjKM", "signatures": ["ICLR.cc/2026/Conference/Submission14385/Reviewer_rDDX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14385/Reviewer_rDDX"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14385/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761927979710, "cdate": 1761927979710, "tmdate": 1762924799502, "mdate": 1762924799502, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}