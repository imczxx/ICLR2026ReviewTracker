{"id": "uHZ2Te266M", "number": 4452, "cdate": 1757682951530, "mdate": 1759898031792, "content": {"title": "M-COLOR: MLLM-GUIDED DIFFUSION MODELS FOR IMAGE COLORIZATION", "abstract": "Language-based Image colorization transforms grayscale images into vivid, visually pleasing colorized outputs with semantic guidance. Existing methods often rely on CLIP text embeddings, which may struggle with deep semantic understanding, leading to suboptimal colorization. In this paper, we propose M-Color, a novel diffusion-based framework that leverages multimodal large language models (MLLMs) to enhance language comprehension through an Adaptive Decoding strategy. To maintain structural consistency, we introduce a Luminance-Aware Encoder (LAE) that aligns grayscale images with the colorized output and a Luminance Extraction Module (LEM) to integrate luminance information into the latent generation process. Extensive experiments demonstrate that M-Color achieves superior semantic alignment, improves structural consistency, and outperforms state-of-the-art methods in both quantitative and qualitative evaluations.", "tldr": "The paper proposes M-Color, a diffusion-based method that uses multimodal language models and luminance cues for semantically aligned, structurally consistent image colorization.", "keywords": ["language-based image colorization", "diffusion models"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5cde1ddd85b1e268ac9c15a7db737811e1038ecd.pdf", "supplementary_material": "/attachment/d94990ab5aa3745481cf651906ac6963fd50506b.pdf"}, "replies": [{"content": {"summary": {"value": "The paper proposes M-Color, a language-guided image colorization pipeline built on Stable Diffusion. It adds a Luminance-Aware Encoder (LAE) for structure preservation, a Luminance Extraction Module (LEM) to inject luminance features into the UNet, and an MLLM-based text embedding with “Adaptive Decoding.”"}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "- Clear decomposition of goals: structure via LAE/LEM, semantics via MLLM embeddings.\n- Includes a small user study on text matching and realism with reported wins."}, "weaknesses": {"value": "- Outdated backbone: the core method is built on Stable Diffusion; no evidence on SDXL/modern DiT/flow models.\n- LEM appears ad-hoc: described as depthwise + pointwise convs to reshape LAE features; the paper gives little principle beyond adaptation, and no swaps against simple MLPs or alternative adapters.\n- Presentation issues: the first page has a stray “1”, Figure 2 is hard to parse (unclear z_t vs z_gray flows; zero_convs/LEM interactions), and Tables 1/3 are oddly formatted, which hurts clarity."}, "questions": {"value": "- What is the theoretical role of LEM beyond channel/spatial matching? Please compare LEM with (a) an MLP adapter, (b) a 1×1 conv, under similar params/FLOPs.\n- Does M-Color transfer to SDXL or a recent DiT/flow backbone without redesign? Provide results or discuss blockers.\n- How much do gains persist if you replace the MLLM with CLIP or a small text encoder? Report cost–benefit (quality vs latency/VRAM)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "zFcy76MShd", "forum": "uHZ2Te266M", "replyto": "uHZ2Te266M", "signatures": ["ICLR.cc/2026/Conference/Submission4452/Reviewer_o9QQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4452/Reviewer_o9QQ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760595037646, "cdate": 1760595037646, "tmdate": 1762917373283, "mdate": 1762917373283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces M-COLOR, a framework for language-guided image colorization that leverages Multimodal Large Language Models (MLLMs) and diffusion models. Its technical innovations include a luminance-aware encoder and extraction module that preserve structural details and enhance consistency, a dual-encoder design that disentangles semantic and luminance features for more effective guidance, and an adaptive decoding strategy that improves semantic alignment by optimizing visual token placement from MLLMs. The method achieves state-of-the-art performance on MSCOCO and Multi-Instance datasets across quantitative metrics (PSNR, SSIM, LPIPS) and qualitative evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The architecture is well-motivated and technically sound.\n2. Experiments are thorough, with comparisons to multiple baselines and ablations. Quantitative results show clear improvements over prior methods.\n3. The paper is generally well-written and structured. Figures and tables are informative and support the claims.\n4. The work pushes the boundary of controllable image colorization with improved colorization quality."}, "weaknesses": {"value": "1. Limited Novelty: The integration of MLLMs into colorization is not new. Prior works like L-CAD and CtrlColor have already explored this direction. The architectural changes in M-COLOR, while useful, appear incremental.\n2. Modest Component Impact: Improvements from Adaptive Decoding in Table 3 are relatively small, raising questions about their standalone value. Besides, the ablation study shows that removing Adaptive Decoding leads to only minor degradation, which seems inconsistent with its visual impact in Fig. 7.\n3. Lack of Failure Analysis: The paper does not discuss failure cases or limitations, such as prompt misalignment or semantic ambiguity."}, "questions": {"value": "1. Could you clarify why Adaptive Decoding and CLIP text guidance appear impactful in Fig. 7 but show limited quantitative degradation in Table 3? Is there a synergistic effect among components?\n2. Are there examples where M-COLOR fails to align with the prompt or introduces artifacts?\n3. How well does M-COLOR generalize to out-of-distribution grayscale images (e.g., historical photos, medical scans)? Have you tested on real-world grayscale datasets beyond COCO?\n4. What is the inference time compared to L-CAD or CtrlColor?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "fYCDI5HsaF", "forum": "uHZ2Te266M", "replyto": "uHZ2Te266M", "signatures": ["ICLR.cc/2026/Conference/Submission4452/Reviewer_cdmB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4452/Reviewer_cdmB"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953921610, "cdate": 1761953921610, "tmdate": 1762917373035, "mdate": 1762917373035, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces M-Color, a diffusion-based framework designed for language-guided image colorization. The authors identify a key limitation in existing methods: their reliance on standard CLIP text embeddings, which can result in suboptimal colorization due to a lack of deep semantic understanding."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Comprehensive ablation studies are provided, which systematically validate the significance of each proposed component and confirm the overall effectiveness of the framework's modules.\n\n2. The manuscript is clearly articulated with a well-defined rationale, making the research easy for the audience to understand and follow."}, "weaknesses": {"value": "1. The degree of technical novelty appears somewhat constrained. Beyond the novel use of MLLM embeddings, the Luminance-Aware Image Reconstruction component was previously introduced in L-CAD, and the Dual-Encoder Latent Generation is fundamentally a process of downsampling features from the LAE. Nonetheless, this is not considered a critical deficiency.\n\n2. The paper's own qualitative results are unconvincing and contain significant artifacts. Figure 1, which serves as the primary visual showcase for the method, exhibits clear \"color bleeding.\" For instance, in the \"Single-Object Color Control\" row, the color of the bus (e.g., pink, purple) visibly bleeds onto the surrounding windows and the road pavement. This artifact directly contradicts the paper's claims of \"proficient control\" (as stated in the abstract and Figure 1 caption) and \"structural consistency.\"\n\n3. The paper does not present any results on the colorization of authentic historical or vintage photographs. This omission leads to questions regarding the method's generalization capabilities and its utility in real-world applications."}, "questions": {"value": "1. To better evaluate the model's claimed fine-grained control, can the authors provide results on more challenging tasks? For example, the current figures do not show if the model can assign different colors to different instances of the same object category (e.g., 'a red flower next to a yellow flower').\n\n2. The presentation of tables in this paper is highly problematic. The tables lack standard gridlines or clear separators, which results in a visually confusing layout. This makes it extremely difficult for readers to align rows and columns, severely harming readability. This unconventional formatting deviates significantly from academic publishing standards and makes the entire paper appear highly unprofessional."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "iYfyFZ0kSK", "forum": "uHZ2Te266M", "replyto": "uHZ2Te266M", "signatures": ["ICLR.cc/2026/Conference/Submission4452/Reviewer_1Uqy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4452/Reviewer_1Uqy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968952798, "cdate": 1761968952798, "tmdate": 1762917372553, "mdate": 1762917372553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "the paper proposes to use MGIE text embedding to enhance text guidance for text-based image colorization"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- the proposed framework is straightforward and easy to implement\n- experiments demonstrate the effectiveness of the proposed method"}, "weaknesses": {"value": "- the usage of MLLM text conditions seems not well justified. for the simple prompts like color-object, the user can already use stroke-based colorization or existing text-conditioned approach to precisely assign colors to objects. the demonstrated non-color adjectives also do not show correct looking & feelings that correspond to the adjectives\n- the results do not look visually pleasing. we can still observe color bleeding and uncolorized regions in the results. it's hard to tell if the proposed framework is the sota \n- the proposed luminance-aware image reconstruction was already proposed in some previous works 2 years ago, but the paper still claims it as a major contribution\n- the usage of stable diffusion is not well justified, considering there are many other better base models and some of them even have MLLM built-in\n- the quantitative comparisons only use PSNR/SSIM/LPIPS, which are unreasonable for image colorization, considering the fact the colorization could have multiple valid/plausible solutions. FID/colorfullness should also be used. Besides, it is unclear whether/what captions are used for the quantitative comparisons\n- the scale of user study is limited. the background of participants and the user study design are not mentioned"}, "questions": {"value": "please refer to weaknesses section"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "KyPmys6o2i", "forum": "uHZ2Te266M", "replyto": "uHZ2Te266M", "signatures": ["ICLR.cc/2026/Conference/Submission4452/Reviewer_pdfm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4452/Reviewer_pdfm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4452/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762251794231, "cdate": 1762251794231, "tmdate": 1762917372349, "mdate": 1762917372349, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}