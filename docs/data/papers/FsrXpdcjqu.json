{"id": "FsrXpdcjqu", "number": 15511, "cdate": 1758252173214, "mdate": 1763744345793, "content": {"title": "On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective", "abstract": "Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong student model, trained on a dataset labeled by a weak teacher, ultimately outperforms the teacher on the target task. Recent studies attribute this performance gain to the prediction misfit between the student and teacher models. In this work, we theoretically investigate the emergence of W2SG through a generalized bias-variance decomposition of Bregman divergence. Specifically, we show that the expected population risk gap between the student and teacher is quantified by the expected misfit between the two models. While this aligns with previous results, our analysis removes several restrictive assumptions, most notably, the convexity of the student's hypothesis class, required in earlier works. Moreover, we show that W2SG is more likely to emerge when the student model approximates its posterior mean teacher, rather than mimicking an individual teacher. Using a concrete example, we demonstrate that if the student model size is sufficiently large, it can indeed converge to the posterior mean teacher in expectation. Our analysis also suggests that avoiding overfitting to the teacher's supervision and reducing the entropy of student's prediction further facilitate W2SG. In addition, we show that the reverse cross-entropy loss, unlike the standard forward cross-entropy, is less sensitive to the predictive uncertainty of the teacher. Finally, we empirically verify our theoretical insights and demonstrate that incorporating the reverse cross-entropy loss consistently improves student performance.", "tldr": "Overcoming convexity assumptions in prior works, we prove weak-to-strong generalization occurs when the student approximates its posterior mean teacher.", "keywords": ["Weak-to-Strong Generalization", "Bias-variance decomposition", "Bregman divergence"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b581cb0c87ceb7438d04d67a051ab97c5e795544.pdf", "supplementary_material": "/attachment/a813314afca4cd158ad8e7bba8706a24e0a4a25f.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies the weak to strong generalization phenomena, where a weak teacher that provides labels to a strong student, can be outperformed by the student."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "- Extensive theory\n\n- The theoretical contribution is clear: they can remove the convexity assumption, and they show the insight that making the student more certain makes weak to strong generalization stronger"}, "weaknesses": {"value": "- I think this paper in principle has a good contribution, but its presentation is lacking. Its unclear what is the main insight / main point.\n\n- I find the notation confusing. Sometimes the student is indicated by accents, and other times by _s. It would be nice if this is consistent. Especially f_w' throws me off; is this now \"weak\" and why does it have an accent? Ideally: \"w\" for weak and \"s\" for strong consistently. \n\n- The theory seems rather \"disconnected\" from the experiments. \n\n- The paper feels unstructured or unorganized. New methods (CACE, SL) are introduced in the results section, and the figures are too dense; they contain lots of information, making it unclear what is the main point."}, "questions": {"value": "1. The conditions of Theorem 2 seem to not agree with your setting, because in your setting the strong model has a larger hypothesis class. But in this theorem, they are both sitting on top of the same backbone; so aren't this models then in the same class? Why is one weak and the other strong here? \n\n2. In Example 1; aren't both models equally strong? They are both linear right? \n\n3. In the figures: Why are there CE and RCE Distances here? What are they? Why are they here? \n\n4. I felt that the Bias and Variance in the Experiments was rather disconnected from the theory. Is it possible to more clearly how they are connected? Its pretty much lost on me - it seems Appendix E.2 is defining bias and variance; but this seems different than the main terms of your theory section? \n\n5. Were the experiments carried out multiple times and averaged? I am missing confidence intervals, and I wonder about the significance of the findings. How were the hyperparameters such as $c$ tuned? \n\n6. Removing the convexity assumption; what did this now yield compared to other SOTA analyses?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "PlUcHodiZW", "forum": "FsrXpdcjqu", "replyto": "FsrXpdcjqu", "signatures": ["ICLR.cc/2026/Conference/Submission15511/Reviewer_WpDr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15511/Reviewer_WpDr"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771731952, "cdate": 1761771731952, "tmdate": 1762925797905, "mdate": 1762925797905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper develops a misfit-based theory of W2SG for broad Bregman losses without assuming convexity of the student hypothesis class. The central result is an expected risk inequality (over data and model draws) that upper bounds the student’s population risk by the teacher’s risk minus an expected misfit term, up to residuals arising from teacher–student dependence. This is obtained via a generalized bias–variance decomposition for Bregman divergences, thereby avoiding the generalized Pythagorean projection used in prior works. From this lens, the paper argues that W2SG is facilitated when the student aligns to a posterior-mean teacher, proposes to ensemble weak teachers to approximate that mean, and analyzes a random-features ridge example where, as the student grows, the expected misfit shrinks and the student converges (in expectation) to the posterior-mean teacher while maintaining a positive misfit gap. Finally, for classification, the paper compares cross-entropy (CE) vs. reverse cross-entropy (RCE), deriving misfit-gain-based generalization bounds, showing the robustness of RCE to uncertain teacher predictions. Experiments empirically visualize the bias-variance decomposition through bias/variance estimates and demonstrate the comparative performance of CE vs. RCE in W2SG settings, which further motivates a new objective, named confidence-adaptive cross entropy (CACE) loss, that interpolates between CE and RCE based on teacher confidence."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. The topic of W2SG is timely and relevant; the connection with and distinction from prior works, especially Charikar et al., 2024 and Mulgund & Pabbaraju, 2025 are clearly articulated.\n2. Moving from the convex projection arguments in prior works to the expected Bregman bias–variance analysis yields misfit-gain inequalities for non-convex students without convexity or linear head restrictions, addressing a key technical limitation in Charikar et al., 2024 and Mulgund & Pabbaraju, 2025. \n3. Empirically, the bias-variance estimation protocol and the ensemble-based approximation of the dual expectation are thoughtful."}, "weaknesses": {"value": "1. While the detailed discussion on the relation with prior works and the technical novelties upfront in the introduction (lines 46-82) can provide good motivations for this work for domain experts (as mentioned in Strengths), it could be overwhelming for general audience in the community. I would suggest reorganizing the introduction carefully, e.g., partitioning the technical novelties in lines 60-82 into bullet points, each summarized by a concise, intuitive title explaining the effect/benefit.\n2. From a high-level perspective, while I acknowledge the technical novelties in the main theoretical results, I feel that the significance of the improvements brought by these novelties is not sufficient to meet the bar of ICLR. In particular:\n    - Mulgund & Pabbaraju, 2025 already extend misfit-gain to general Bregman losses and propose convex combinations of logistic heads for classification; this paper avoids convexity by going to the expected risk over both data and models. While the main results in Theorem 3.1 relaxes the key limitation of the convexity assumption in prior works, as a trade-off, the resulting generalization bounds are also in expectation over both data and models, which is a weaker notion of generalization. In contrast to the Bregman Misfit-Gain Inequality in Mulgund & Pabbaraju (2025) where the \"residual\" term $\\epsilon$ has a clear interpretation as the estimation+optimization error, the residual terms in Theorem 3.1, $\\epsilon_1, \\epsilon_2$, defined based on expectations over both data and models, and therefore the entire training process, are less interpretable. This trade-off is not sufficiently discussed in the paper.\n    - Yao et al., 2025 has provided the theoretical and empirical study for reversed KL/CE in W2SG. Section 5 echoes that story, but the novelty of this work along this direction seems limited.\n    - Overall, conditioned on the context provided by the prior works, the technical and intuition-level contributions of this paper seem incremental and a combination of prior results up to small variations. \n3. The relaxation of the convexity assumption is the key technical contribution of this work. Regardless of whether this relaxation under the trade-off discussed above is significant enough, the benefit of such relaxation is not well illustrated rigorously, except for the obvious argument that non-convex students are common in practice. In particular, the only tractable theorem, Theorem 4.1, assumes random features + ridge head, which is still convex and essentially linear. \n4. On Theorem 4.1 itself, while it is common and reasonable to assume Gaussian data/features in random matrix theory-based analyses, assuming that the (potentially pre-trained) model weights are also Gaussian is very strong and unrealistic. This seems to be another artifact caused by the joint expectation over data and models.\n\nI am happy to rectify any misunderstandings of the paper and adjust my assessment if the authors can provide clarifications and counter-arguments for the above points."}, "questions": {"value": "Below are some minor questions and comments noted during my reading:\n1. Lines 51-53: \"... the misfit error simultaneously quantifies the attainable performance gain, implying that zero misfit error is in fact undesirable; the work does not explicitly discuss in what sense the student should align with the teacher.\" The sentence is quite confusing in the context: what \"the misfit error simultaneously quantifies the attainable performance gain\" means, simultaneously with respect to what, what is an implicit vs. explicite description of the alignment between student and teacher?\n2. Lines 69-70: \"... where the posterior is defined by conditioning the teacher’s distribution on the student model, and the mean may involve a dual expectation.\" This sentence is a bit hard to parse; what is meant by \"conditioning the teacher’s distribution on the student model,\" and what does \"dual expectation\" mean here?\n3. Lines 73-74: \"... we show that reducing the expected misfit between teacher and student is sufficient for W2SG.\" This sentence is somehow confusing. Given the context of Charikar et al. (2024) and Mulgund & Pabbaraju (2025), I guess \"reducing the expected misfit between teacher and student\" refers to the process of aligning the student to the teacher? Notice that the misfit-gain inequalities, e.g., in Mulgund & Pabbaraju (2025) and your Theorem 3.1, also have an expected misfit term, reducing of which actually improves the upper bound on the student risk. It would be good to clarify the meaning of \"misfit\" in the context.\n4. The definition of dual expectation in Lemma 2.1 is not rigorous or clear in the context."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "yFRZL3nA8t", "forum": "FsrXpdcjqu", "replyto": "FsrXpdcjqu", "signatures": ["ICLR.cc/2026/Conference/Submission15511/Reviewer_GSgt"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15511/Reviewer_GSgt"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788972545, "cdate": 1761788972545, "tmdate": 1762925797469, "mdate": 1762925797469, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper considers an abstract weak-to-strong learning setup and shows a misfit-based inequality characterizing the W2S improvement in expected population risks, which removes the convexity assumption from the previous misfit-based W2S inequalities. From this inequality, they show how aligning the student with the posterior mean teacher can improve W2S performance gain. They also apply their inequality to two settings: (i) an overparametrized ridge regression setting in the appropriate scaling limit of number of weak samples vs the teacher and student dimensions, which indicates that W2S improvement is likely to emerge with a student of higher capacity, and (ii) to the case when risk is measured using CE or RCE, showing that RCE is better as optimizing objective. Finally, they use these insights to empirically show improvements in some cases W2S when combining the two."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "The misfit based approach to W2S generalization is a promising approach that can naturally also model pre-training and encompasses a lot of different model classes. The generalization to non-convex student hypothesis class is a significant improvement which comes from the idea of analyzing expected population risk using the bias-variance decomposition of Bergman divergences, which is an original idea. \n\n\nThe mathematical statements are clear and on the first reading seem correct.\n\n\nThe main significance of results is that:\n1. it removes one of the strongest assumptions from previous misfit-based approaches \n2. provides some insights into the W2S mechanism through the example of ridge regression, suggesting that higher capacity students might be beneficial \n4. theoretically suggest the benefits of using RCE as optimization objective, demonstrating why some form of regularization on the student is necessary for W2S generalization\n5. empirically improves performance of W2S generalization in some setups by combining CE and RCE."}, "weaknesses": {"value": "1. One of the main messages of the paper, that W2S is more likely to occur by mimicking the posterior teacher or the algorithmic encouragement to train on an ensemble of teachers are not in the spirit of W2S generalization and have the effect of confounding other effects with W2S. Consider regression setup: of course that training on a less noisy distribution is beneficial. Averaging over many teachers effectively denoises the teacher predictor artificially which makes the student have smaller error. If you take very large number of teachers, even training the student suboptimally (i.e. to harmfully overfit) without regularization (which would make the student effectively mimic the teacher since it can fit the teacher predictor) with an ensemble of teachers would make the student outperform one teacher because of the denoising effect. However, the question of W2S-like generalization with multiple teachers is still interesting in its own right, but for the sake of understanding the mechanisms in W2S it is important to not confuse the two.\n\n2. The benefit of removing the convexity assumption on the student class is not demonstrated. The new misfit based inequality, whose main novelty is circumventing the convexity assumptions on the student, i.e. Theorem 3.1 and Corollary 3.1, give sufficient conditions for W2S but do not establish that W2S actually occurs. The paper only establishes that these inequalities do imply that W2S generalization occurs in the setting of ridge regression in Theorem 4.1, but (i) in this case the convexity assumption on the loss is satisfied and (ii) it was already known that W2S generalization can happen in this case (e.g. Wu & Sahai 2025). In the CE or RCE case the W2S generalization is not actually established, only the sufficient conditions in Corollary 5.1. So the benefit of removing the convexity assumption is unclear.\n\n3. Some of the main contribution claims are not precise and should be further discussed. E.g: the claim that “W2S is more likely to emerge when the student has higher capacity” is a bit unclear and goes against other conclusions in the paper that not overfitting to the teacher is important. It is also not always the case in practice, e.g. in the original Burns et al 2023 paper, the authors show that on some tasks the smaller student models are better for W2S (Figure 3, chess example)."}, "questions": {"value": "1. What are the main drawbacks of using an ensemble of teachers? Can we use the tools developed in the paper to theoretically analyze the case with an ensemble of teachers?\n\n2. Are there any simple examples with a student with a non-convex hypothesis class where Theorem 3.1 can help establish W2S?\n\n3. In regression example in Theorem 4.1, is the teacher predictor optimal (for its set of features/kernel)? Does the W2S improvement come from the teacher being suboptimally regularized and the student being optimally regularized or no? If not, where does it come from?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "eOxwPReUx6", "forum": "FsrXpdcjqu", "replyto": "FsrXpdcjqu", "signatures": ["ICLR.cc/2026/Conference/Submission15511/Reviewer_MAkU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15511/Reviewer_MAkU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761960964375, "cdate": 1761960964375, "tmdate": 1762925797073, "mdate": 1762925797073, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper provides a theoretical analysis of weak-to-strong generalization (W2SG)—the phenomenon where a strong student model trained on weak teacher labels surpasses the teacher's performance. Using a generalized bias-variance decomposition of Bregman divergence, the authors prove that the performance gap is quantified by the expected misfit between models, removing restrictive convexity assumptions from prior work. Key insights include: students should approximate the posterior mean teacher rather than individual teachers, reverse cross-entropy is less sensitive to teacher uncertainty, and avoiding overfitting facilitates W2SG. Empirical results validate the theoretical findings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper studies an interesting problem and offer a theoretically driven explanation to the weak-to-strong generalization phenomenon observed. \n- The authors of the paper propose a novel reverse cross-entropy loss and empirically demonstrate the effectiveness of it."}, "weaknesses": {"value": "- The experiments conducted use relatively small models, by today's standard. While I understand the computational resource constraint, it would be interesting to use more recent and larger models. \n- To the best of my understanding, the analysis is done with respect to linear models."}, "questions": {"value": "- Is there a particular reason why different models are used with different methods independently in Table 1? Do the conclusions hold beyond the combinations shown?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "3dg5DPAQFF", "forum": "FsrXpdcjqu", "replyto": "FsrXpdcjqu", "signatures": ["ICLR.cc/2026/Conference/Submission15511/Reviewer_xcki"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15511/Reviewer_xcki"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15511/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762219588508, "cdate": 1762219588508, "tmdate": 1762925796612, "mdate": 1762925796612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}