{"id": "RzYXb5YWBs", "number": 21596, "cdate": 1758319455988, "mdate": 1759896913285, "content": {"title": "LoopFormer: Elastic-Depth Looped Transformers for Latent Reasoning via Shortcut Modulation", "abstract": "Looped Transformers have emerged as an efficient and powerful class of models for reasoning in the language domain. Recent studies show that these models achieve strong performance on algorithmic and reasoning tasks, suggesting that looped architectures possess an inductive bias toward latent reasoning. However, prior approaches fix the number of loop iterations during training and inference, leaving open the question of whether these models can flexibly adapt their computational depth under variable compute budgets. We introduce LoopFormer, a looped Transformer trained on variable-length trajectories to enable budget-conditioned reasoning. Our core contribution is a shortcut-consistency training scheme that aligns trajectories of different lengths, ensuring that shorter loops yield informative representations while longer loops continue to refine them. LoopFormer conditions each loop on the current time and step size, enabling representations to evolve consistently across trajectories of varying length rather than drifting or stagnating. Empirically, LoopFormer demonstrates robust performance on language modeling and reasoning benchmarks even under aggressive compute constraints, while scaling gracefully with additional budget. These results show that looped Transformers are inherently suited for adaptive latent reasoning, opening a path toward controllable and budget-aware large language models.", "tldr": "", "keywords": ["large language models", "LLMs", "reasoning", "looped transformers", "efficient inference", "parameter sharing"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/1af4f513ace9a2477d14c7e8b10cbdaeea7ec5c5.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces LoopFormer, a looped Transformer architecture that can flexibly adapt its computational depth based on a user-specified budget, a feature termed \"elastic depth\". The model is designed to perform robustly across a range of loop iterations at inference time without needing to be retrained, addressing a key limitation of prior looped models.\n\nTraditional looped Transformers are trained and evaluated with a fixed number of loop iterations. This rigidity means they cannot adapt to variable compute budgets; their internal representations tend to collapse or stagnate when evaluated at depths different from their training configuration, leading to degraded performance.\n\nThe paper introduces a shortcut-consistency training scheme that enables compute-budgeted inference (elastic depth) without retraining the model. \nThe work demonstrates that naive early-exiting in looped models leads to representational collapse, where hidden states stagnate across iterations. In contrast, LoopFormer's representations continue to evolve, showing that it uses additional depth effectively for refinement."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper introduces the novel concept of \"elastic depth\" for looped Transformers, creatively adapting ideas from diffusion models to frame iterative refinement as a continuous-time \"thought trajectory\" . This unique problem formulation and synthesis of ideas results in a highly original approach to budget-conditioned reasoning.\n\nThe work is of high quality, featuring a rigorous experimental setup with strong, appropriate baselines and a comprehensive evaluation on both perplexity and a wide range of reasoning tasks . The claims are further substantiated by an in-depth representational analysis that uses multiple metrics to convincingly demonstrate that LoopFormer avoids the representational collapse that plagues naive adaptive methods.\n\nThe paper's contribution is significant because it provides a practical solution to the important problem of adaptive computation, making efficient looped architectures more versatile and deployable."}, "weaknesses": {"value": "The shortcut-consistency training algorithm requires two forward passes per batch (one for the full trajectory and one for a shortcut), which effectively doubles the training compute compared to baseline looped models . The paper acknowledges this \"added training overhead\" but does not analyze the trade-off. A key missing experiment is a comparison against a baseline model trained for twice as many steps, which would clarify if the inference-time flexibility is worth the significant increase in training cost.\n\nThe model's performance is highly sensitive to the choice of the inference-time step schedule, yet this schedule must be selected manually by the user1111. This introduces a required hyperparameter tuning step that complicates deployment and may prevent users from achieving the model's optimal performance for a given budget. The work would be stronger if it included a method to learn or predict an optimal schedule automatically.\n\nThe experiments are conducted on ~1B parameter models, and it is unclear if the training dynamics and performance benefits will scale to much larger, state-of-the-art foundation models."}, "questions": {"value": "Your results show that performance is sensitive to the choice of the inference-time schedule. Could you provide a practical heuristic for selecting a high-performing schedule without an exhaustive search? Have you considered methods for learning an optimal, input-dependent schedule to automate this process? A response here would clarify if this is a minor tuning step or a significant practical hurdle.\n\nThe shortcut-consistency training algorithm appears to double the training compute by requiring two forward passes per batch . Could you provide an analysis of this trade-off? Specifically, how does LoopFormer compare against a baseline (e.g., TMLT) that is trained for twice as many steps using the same total compute budget? This would clarify whether the inference-time flexibility justifies the increased training cost."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "4ElYTzhkRD", "forum": "RzYXb5YWBs", "replyto": "RzYXb5YWBs", "signatures": ["ICLR.cc/2026/Conference/Submission21596/Reviewer_pdxW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21596/Reviewer_pdxW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission21596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760597398281, "cdate": 1760597398281, "tmdate": 1762941848595, "mdate": 1762941848595, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents LoopFormer, which allows reasoning under variable compute budgets. It extends prior looped Transformers by introducing time- and step-size modulation, where each iteration receives sinusoidal embeddings of layer index and step size to dynamically modulate RMSNorm and residual scaling.\nA shortcut-consistency loss aligns representations across different loop lengths, enabling stable performance even with fewer inference steps."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper is well written and easy to follow, with clear motivation and setups\n* The motivation is clearly presented, and the transition from fixed-depth looped Transformers to elastic-depth design feels natural.\n* Experiments are reasonably comprehensive, evaluating both variable loop lengths and the effect of the proposed shortcut-consistency loss.\n* The main claims are well supported"}, "weaknesses": {"value": "* The degree of novelty is not bad but moderate. While the proposed elastic-depth formulation and shortcut-consistency loss are well designed, they extend existing time-modulated looped Transformer frameworks rather than introducing a fundamentally new paradigm.\n* the paper does not provide theoretical intuition or analysis explaining why combining t and $\\Delta t$ through sinusoidal modulation is a good choice here"}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "j2942wDN1I", "forum": "RzYXb5YWBs", "replyto": "RzYXb5YWBs", "signatures": ["ICLR.cc/2026/Conference/Submission21596/Reviewer_xXws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21596/Reviewer_xXws"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission21596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761459340372, "cdate": 1761459340372, "tmdate": 1762941848391, "mdate": 1762941848391, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper tackles the limitation of looped Transformers, whose performance typically degrades when inference loops don't match the fixed number used in training. They propose LoopFormer which novelly condition each loop on both normalized time and step size to adapt to different computational depth. The model is trained on \"shortcut-consistency\" loss that forces shorter trajectories to match the representation of the full trajectory. They empirically show that the LoopFormer perform more robustly at smaller compute budgets than existing baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper tackles a clear, practical, and important problem. Enabling flexible, elastic compute in parameter-efficient models like looped Transformers is a highly valuable research direction.\n2. The paper is well written and easy to follow.\n3. The experiments are thorough and convincing. In addition to strong task performance, the authors provide a compelling explanation for why LoopFormer works by analyzing metrics like curvature, anisotropy, and CKA similarity. They demonstrate that baselines suffer from representational stagnation (flat metrics, high CKA), while LoopFormer's representations continue to evolve and refine with each loop. This analysis significantly strengthens the paper beyond just reporting better numbers."}, "weaknesses": {"value": "1. The training procedure (Algorithm 1) requires two forward passes per batch (for the full and short trajectories) to compute the consistency loss. This appears to roughly double the training cost compared to a standard looped model. The paper mentions this as a limitation but does not quantify it. A brief analysis of the training FLOPs/time overhead vs. a Base-Loop or TMLT baseline would be valuable for assessing the practical trade-offs.\n2. The paper heavily emphasizes \"latent reasoning,\" using terms like \"thought trajectories.\" However, the benchmarks (COPA, PIQA, HellaSwag) are standard zero-shot LM evaluation tasks, not complex, multi-step algorithmic reasoning tasks. The model shows improved general performance (including perplexity), and the representation analysis shows refinement, not necessarily reasoning in a formal sense. So the terming could be a bit misleading to readers."}, "questions": {"value": "1. What is the practical training overhead (e.g., in FLOPs or wall-clock time) of the dual forward pass required for the consistency loss, compared to a standard Base-Loop or TMLT baseline?\n2. Given that Figure 5 shows significant performance variance based on the chosen step schedule Î”_M, have you explored methods for learning an optimal, budget-aware scheduling policy?\n3. The model is trained to interpolate (M <= L). Have you tested its ability to extrapolate to M > L loops? Does performance continue to improve, or does it diverge given the t=1 training target?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "u1wgxeZOjs", "forum": "RzYXb5YWBs", "replyto": "RzYXb5YWBs", "signatures": ["ICLR.cc/2026/Conference/Submission21596/Reviewer_exw9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21596/Reviewer_exw9"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission21596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969695744, "cdate": 1761969695744, "tmdate": 1762941848181, "mdate": 1762941848181, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents a new model composed of k blocks each trained to be repeated up to L times. The architecture adds in each block a side information about the scheduling to visit [0, 1] with M steps, and modulate a scale and gate layer before/after the multi-head attention and FFN blocks.\n\nThe training procedure uses a loss that combines the cross-entropy of the full-compute (L iterations), the cross-entropy of a lesser compute (M iterations), and a L2 between the pre-readout activations of both.\n\nExperimental validation shows that this approach is not as good as a vanilla architecture for a given compute budget, but has a lesser parameter count."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The proposed architecture is reasonably simple and well motivated. The overall direction, transformer-like that can dynamically modulate compute, is important. The results showing the nice monotonicity of perplexity or accuracy vs. FLOPs (Fig 2) is great."}, "weaknesses": {"value": "The experimental results are underwhelming, and maybe I missed the point, but it is unclear to me how this model improves wrt a vanilla transformer. The results are presented in such a way that the training budget is not equalized (if I am correct? I do not understand the first sentence of 4.1), the inference flops are, and the parameter counts are not, although this is generally not the limiting factor.\n\nTraining budgets should have been equalized (e.g. pick the nb of training iterations per model), and results should be summarized with scatter plots on e.g. an average accuracy vs. wall-clock time or flops frame.\n\nThe base model should be added as a point on Fig 2 (a) and (b)"}, "questions": {"value": "It is unclear in Fig 1 and in the loss definition of 3.3 how the method deals with multiple blocks. It looks as if there was only one there.\n\nRegarding the training cost and the overall performance, can you clarify in what regime you see the usefulness of this model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "bbq93B1zK1", "forum": "RzYXb5YWBs", "replyto": "RzYXb5YWBs", "signatures": ["ICLR.cc/2026/Conference/Submission21596/Reviewer_aD1e"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission21596/Reviewer_aD1e"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission21596/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762119216905, "cdate": 1762119216905, "tmdate": 1762941847970, "mdate": 1762941847970, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}