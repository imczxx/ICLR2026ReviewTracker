{"id": "h4hIuid0HY", "number": 6930, "cdate": 1758002427353, "mdate": 1759897883382, "content": {"title": "Modeling SRP-LSH Performance: A Theoretical Framework for Optimizing Approximate Nearest Neighbor Search", "abstract": "Approximate nearest neighbor (ANN) search in high-dimensional spaces with sign-random-projection locality-sensitive hashing (SRP-LSH) remains challenging due to the lack of principled approaches for configuring its key parameters. We present a theoretical framework that rigorously models SRP-LSH performance and enables principled parameter configuration. At its core is, to our knowledge, the first analytical model that links the number of hash functions and the Hamming distance threshold to search recall, rooted in the binomial distribution of bit collisions and the angular similarity distribution of vectors.\nBuilding upon this model, we develop an adaptive optimization algorithm that minimizes the candidate set size while satisfying user-specified recall targets. Extensive experiments show that our model typically predicts recall with a mean absolute percentage error (MAPE) below 5%. Moreover, our algorithm consistently meets the specified recall targets and simultaneously reduces the candidate set size by up to 30% compared to practical fixed-parameter baselines achieving the target recall. Overall, this framework provides a theoretically grounded and practical solution for configuring SRP-LSH in real-world retrieval systems.", "tldr": "We present a theoretical framework that rigorously models SRP-LSH performance and enables principled parameter configuration.", "keywords": ["Approximate Nearest Neighbor Search; Locality-Sensitive Hashing; Theoretical Analysis; Parameter Optimization; High-Dimensional Retrieval"], "primary_area": "learning theory", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/22f756ccc5eb962bc23fccb358049e8d30fb0c6e.pdf", "supplementary_material": "/attachment/2549309c950ab404bb024be411ad4d41f1fdb9e3.zip"}, "replies": [{"content": {"summary": {"value": "SimHash (or SRP-LSH) can serve both as a dimensionality reduction technique for similarity estimation and as a hashing-based solution for approximate nearest neighbor search (ANNS).\nThis paper treats SimHash primarily as a dimensionality reduction tool to answer ANN queries—performing a linear scan in the Hamming space to retrieve top-k candidates for reranking. \n\nThe key idea is to model and optimize the parameters of SimHash—namely, the number of hash bits m and the Hamming threshold t so that a target recall is achieved with minimal candidate size (i.e., fewest distance computations). The approach learns m and t by fitting statistical distributions of query–neighbor angles from a subsampled query set, predicting recall/selectivity analytically, and selecting parameters to minimize expected query cost.\n\nExperiments on six benchmark datasets demonstrate good predictive reliability (recall MAPE < 5%)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- Clear motivation: parameter tuning of SRP-LSH often relies on heuristics or exhaustive search.\n- Learnable parametric model to recall and selectivity for SimHash-based ANNS"}, "weaknesses": {"value": "**Dominant cost component overlooked:**\nThe analysis ignores the linear scan in Hamming space, which dominates the overall query cost when \nm is large. Modeling only candidate size (selectivity) underestimates true runtime.\n\n**Incorrect probabilistic assumption:**\nThe paper models the angle between a query q and its k-nearest neighbor as a beta distribution (Line 180) - note that the cited paper Dong et al. 2008 does not present this property.\nHowever, while Beta((d-1)/2, (d-1)/2)) describes the distribution of random vector angles, q and its k-NN are not random—their angular distribution is highly data-dependent and concentrated near small angles. The use of Beta here is therefore an unjustified approximation, not a valid theoretical model.\n\n**Subsampling limitation:**\nLemma 2 only guarantees that subsampling the query set yields unbiased estimates of the mean and variance of overall angular statistics\nIt does not imply that the angular distribution between q and its k-NN is preserved. Hence, if the sampled queries do not represent the full dataset well, the learned parameters (m, t) may deviate significantly in practice."}, "questions": {"value": "**Scalability of the cost model:** What happens when the Hamming scan cost dominates due to large m? Does the proposed optimization still correlate with real runtime?\n\n**Parameter practicality:** The paper frequently tests m=256. For n=10^6 points, the Hamming distance space is limited to 256 possible values, giving expected candidate size ≈ n/256 ≈ per query even before reranking—raising doubts about scalability. How does the approach behave for larger m (e.g., 1024 bits or larger)?\n\n**Relation to prior work (Slaney et al., 2012):**\nSlaney et al. previously modeled LSH recall and collision probabilities using empirical angular distributions. What is the concrete novelty beyond replacing the empirical distribution with a Beta fit and adding subsampling?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "D8LIGtpWEp", "forum": "h4hIuid0HY", "replyto": "h4hIuid0HY", "signatures": ["ICLR.cc/2026/Conference/Submission6930/Reviewer_vokp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6930/Reviewer_vokp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761036037503, "cdate": 1761036037503, "tmdate": 1762919164022, "mdate": 1762919164022, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a modeling and tuning framework for SRP-LSH that links the number of hash bits m and the Hamming threshold t to recall, via a binomial model conditioned on the query point angle, and models angle distributions with a Beta family to predict both recall and selectivity (candidate ratio). The proposed method searches (m, t) pairs to minimize selectivity subject to a target recall using a simple grid + binary search procedure. Experiments on six vector datasets report recall MAPE < 5% and reductions of selectivity compared to a fixed-parameter baseline that meets the same recall target."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The recall-threshold relationship is derived cleanly via a binomial CDF over Hamming distance, given angle $\\theta$, which is a useful formalization for SRP-LSH with thresholded Hamming.\n\n2. The (m, t) search with binary search on t is easy to implement and reproduce.\n\n3. Across datasets and k, recall MAPE is typically below 5%, demonstrating the recall model’s robustness and effectiveness."}, "weaknesses": {"value": "1. **Incremental novelty relative to established LSH parameter-tuning frameworks.**\nThe paper frames its model as “the first analytical model that links m and t to recall”. The main value here is a specialized and practical modeling pipeline for SRP-LSH with Hamming-threshold search. However, the broader idea of analytical linking LSH parameters to quality/efficiency and selecting them to meet targets has a substantial history (e.g., multi-probe LSH’s modeled probing/order and efficiency trade-offs; optimal parameters for LSH’s theory-driven parameter choice). What’s new here is the instantiation for SRP-LSH with a parametric angle model and the use of that model to drive (m, t) search. This is useful but reads as incremental rather than conceptually novel.\n\n2. **Selectivity prediction is weak on important cases.**\nTable 1 shows that the selectivity MAPE is 95–98% on deep-image-96, and even on several others, it is 20–30%. Since the optimizer minimizes predicted selectivity, large selectivity errors can mislead configuration selection, especially at small selectivity, which is very common in practice. The paper acknowledges that selectivity is harder to predict but still bases optimization on it. A stronger treatment is needed here. \n\n3. **The recall guarantee is not consistently satisfied.**\nThe abstract and contributions suggest the method “guarantees” meeting recall targets, but Table 3 shows misses (e.g., recall is 0.89 when the target is 0.9). The guarantee is, in fact, model-conditional and sensitive to search ranges; it is not unconditional. The paper should soften claims and formalize conditions under which recall satisfaction holds.\n\n4. **Limited baselines for angular ANN and SRP variants.**\nEvaluation compares to a fixed m baseline rather than to stronger angular ANN hash families or improved SRP variants that change the recall-efficiency frontier, e.g., Super-Bit LSH (variance-reduced SRP), cross-polytope LSH (optimal for angular distance), or classical banded/tabled LSH parameterizations; also, Hamming-space multi-index hashing (MIH) is a natural baseline for thresholded code search. Without these, it is difficult to assess whether the proposed tuning is competitive in end-to-end efficiency.\n\n5. **Practicality and measurement gaps for systems.**\nThe study emphasizes selectivity as a platform-agnostic cost proxy. Still, it does not report wall-clock latency, memory, or index-build costs, nor comparisons to strong non-LSH ANN baselines (e.g., HNSW) that dominate practice. Moreover, the prediction pipeline requires exact k-NN for 1k queries to fit distributions (exhaustive search), which may be prohibitive at real-world scales; the paper should quantify this overhead and its amortization."}, "questions": {"value": "Q1 (regarding W1) Since the paper already acknowledges prior LSH parameter-selection/tuning work, could you explicitly explain what is unique here beyond those frameworks? For example, (i) Is the Beta angle-distribution assumption essential (and why preferable) versus alternatives used previously? (ii) Could you add an ablation showing that your distributional modeling + search picks different (and better) configurations than a strong generic LSH parameter tuner?\n\nQ2 (regarding W2) Given selectivity MAPE is 95–98% on deep-image-96, how robust is the configuration search in practice? Would it be possible to consider (i) calibrated or quantile-conservative selectivity modeling, or (ii) a bilevel strategy that verifies/adjusts selectivity empirically after the model picks (m, t)?\n\nQ3 (regarding W3) It would be beneficial if the authors could restate the recall guarantee precisely with assumptions (e.g., correct angle-distribution model, feasible (m, t) bounds). Could you add a theorem or proposition showing conditions under which the binary search on t must find a feasible solution?\n\nQ4 (regarding W4) It would be beneficial if the authors could include or justify the exclusion of the mentioned methods in evaluation.\n\nQ5 (regarding W5) It would be beneficial if the authors could report end-to-end latency and memory vs. strong ANN baselines at matched recall. It would also be helpful to quantify the cost of fitting distributions (exact k-NN) relative to downstream gains."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Tef9yVer4X", "forum": "h4hIuid0HY", "replyto": "h4hIuid0HY", "signatures": ["ICLR.cc/2026/Conference/Submission6930/Reviewer_f9PM"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6930/Reviewer_f9PM"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761842278787, "cdate": 1761842278787, "tmdate": 1762919163426, "mdate": 1762919163426, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper studies the parameter selection for sign random projection (SRP) based hash function. It builds a model for the recall and selectivity as functions of the hash signature length and hamming distance threshold. The problem is formulated as finding the parameter configuration that minimizes the recall while satisfying the recall requirement. The experiment results show that the proposed method can meet the recall target while having a small selectively."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "S1: Recall and selectivity are formulated as functions of the SRP parameters with mathematical derivations."}, "weaknesses": {"value": "W1: The contribution to vector similarity search is limited as SRP is rarely used in practice now. Hash-based schemes may be widely used around 2010 but currently, two types of indexes are the most popular for approximate nearest neighbor search (ANNS), i.e., IVF and proximity graph. They are much more efficient than LSH and the standard now. The authors may start with the two papers below for a literature review.\n\nProduct Quantization for Nearest Neighbor Search\nEfficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs\n\nW2: The basic understandings of LSH are wrong for the authors. In the experiments, the authors use a long hash code, i.e., m=256. This is infeasible are large datasets for two reasons. (i) There are too many hash buckets (2^m) and almost all hash buckets will be empty if you are using a hash table. As such, one can only scan all hash signatures to identity the nearest hash signatures. However, for large datasets, e.g., with a billion of vectors, see the HNSW and follow-up papers, scanning all hash signatures will be very expensive. (ii) The common way to use LSH for large datasets is to use many hash tables, a short hash signature for each hash table, and a small hamming distance threshold. This ensures that the query only needs to check a few hash buckets in each hash table and does not need to scan all hash signatures.\n\nW3: The experimental comparison with the fixed method in Table 3 is unfair. Please use a fixed hash signature length for both methods. This is because using a longer hash signature naturally enjoys better selectivity.\n\nW4: The experiments do not report the cost of conducting the parameter search.\n\nW5: The mathematical analysis is quite standard and lacks technical depth."}, "questions": {"value": "NA"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "NauoBxuM6q", "forum": "h4hIuid0HY", "replyto": "h4hIuid0HY", "signatures": ["ICLR.cc/2026/Conference/Submission6930/Reviewer_A85m"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6930/Reviewer_A85m"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918489676, "cdate": 1761918489676, "tmdate": 1762919162597, "mdate": 1762919162597, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a theoretical framework to solve the key challenge of parameter tuning in Sign-Random-Projection LSH (SRP-LSH). It provides an analytical model that predicts search recall and cost based on the number of hash functions (m) and the Hamming distance threshold (t), enabling principled and automatic parameter optimization. Its main contribution：1. It derives the first known model linking SRP-LSH parameters (m, t) to recall, by combining the Binomial distribution with the Beta distribution. 2. The paper provides an efficient algorithm that uses the model to automatically find optimal (m, t) pairs, minimizing query cost while guaranteeing a target recall. 3. The model predicts recall with high accuracy (MAPE < 5%), and the algorithm reduces candidate set size by 20-30% over fixed baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper's primary strength lies in its significant originality in tackling the long-standing, practical challenge of parameter tuning for SRP-LSH. It introduces what the authors claim is the first analytical framework to rigorously model the relationship between the key parameters (m and t) and search recall. The high quality of this work is demonstrated by its theoretical depth, which originally combines the binomial distribution to model bit collisions with a Beta distribution to characterize the angular similarity of vector pairs. This theoretical rigor is matched by the clarity of its extensive experimental validation across six benchmark datasets , which confirms the model's high predictive accuracy (recall MAPE typically below 5%) and its superiority over alternative distribution-fitting methods. The work is highly significant, especially because SRP-LSH is a popular and efficient choice for industrial-scale systems. By translating its theoretical model into a practical, adaptive optimization algorithm, the paper delivers a tangible efficiency gain—reducing the candidate set size by up to 30% while maintaining recall targets and provides a  solution for configuring these widely deployed real-world retrieval systems."}, "weaknesses": {"value": "1. The motivation is although interesting, but somewhat confusing. In my opinion, in real applications, with required recall, less returned points (related to m and t) are better, which contributes to the finetuned efficiency. However, the experiments are all based on the same number of returned points (k) and compared effectiveness mostly.\n2.The paper's claim to be the \"first analytical model\" for SRP-LSH's recall-parameter relationship overstates its conceptual originality. While the paper's specific contributions—using the Binomial CDF for Hamming distance and the Beta distribution for angular similarity are novel and tailored to SRP-LSH, the work should more clearly position itself as an adaptation and refinement of existing analytical approach rather than a completely new one.\n3.The experimental baseline used to validate the adaptive parameter selection algorithm is weak.The algorithm's 20-30% efficiency gain is measured against a single \"fixed-parameter baseline\" (m=256). A much stronger and more realistic baseline would involve a systematic empirical evaluation on a validation set. This process would test a range of m values, find the optimal t for each to meet the 0.9 recall target, and then select the (m, t) pair with the lowest selectivity. The paper's claimed gains are unconvincing without this more rigorous comparison.\n4.The framework's claim of being a \"lightweight\" and practical alternative is weakened by a \"hidden\" setup cost that is never quantified. The core model requires fitting parameters (α_k, β_k) for the k-th nearest neighbor angle distribution g_k(θ_k). This fitting process necessitates first obtaining the true nearest neighbor angles for a query subsample, which requires running a computationally expensive exact k-NN search. The paper should explicitly quantify this setup time and compare it to the \"trial-and-error\" baseline, as this prerequisite cost may make the \"lightweight\" framework less efficient than a simple empirical tuning in practice."}, "questions": {"value": "1.In Table 3, the 20-30% efficiency gain of Algorithm 1 is measured against a single \"fixed-parameter baseline\" of m=256. This baseline seems like a \"strawman\" and may not represent a realistic \"practical\" tuning process, which would almost certainly involve testing several values for m. To provide a more convincing case for the algorithm's practical utility, could you please provide a comparison against a stronger baseline? For example, a simple grid search over the same m range ([128, 320])  where the optimal t is found empirically for each m to meet the 0.9 recall target, with the best-performing (m, t) pair from that search serving as the baseline.\n2.The paper positions the framework as a \"lightweight\" alternative to empirical tuning. However, the model requires estimating the k-th nearest neighbor angle distributions g_k(θ_k) . This fitting process seems to require first obtaining the true k-th nearest neighbor angles for a query subsample, which necessitates running a computationally expensive exact k-NN search. This \"hidden\" setup cost is never quantified. To make a convincing case for practical efficiency, can you provide an analysis of the total end-to-end time (setup + parameter search) for your method and compare it directly against the total time of a practical baseline. This comparison is essential to fairly evaluate the framework's overall efficiency claim.\n3. What are the corresponding optimal m and t in consideration various recalls and datasets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BhEon8F9si", "forum": "h4hIuid0HY", "replyto": "h4hIuid0HY", "signatures": ["ICLR.cc/2026/Conference/Submission6930/Reviewer_uoUU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6930/Reviewer_uoUU"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6930/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762058562159, "cdate": 1762058562159, "tmdate": 1762919162119, "mdate": 1762919162119, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}