{"id": "lMjxyHLL2R", "number": 22503, "cdate": 1758331995423, "mdate": 1759896862477, "content": {"title": "ABBEL: LLM Agents Acting Through Belief Bottlenecks Expressed in Language", "abstract": "As the length of multi-step interactive language tasks increases, it becomes computationally impractical to keep full interaction histories in context.\nWe propose a general and interpretable approach: Acting through Belief Bottlenecks Expressed in Language (ABBEL), which replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. \nUnder ABBEL, at each step the agent first updates the prior belief with the most recent observation from the environment, then uses only the updated posterior belief to select an action.  \nWe systematically evaluate frontier models under ABBEL across six diverse multi-step environments, \nfinding that they can generate interpretable beliefs while maintaining near-constant memory use over interaction steps. However, we observed inferior performance to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We devise a scheme for belief grading that associates rewards with the quality of the beliefs, and \ndemonstrate its ability to improve ABBEL's performance beyond full context RL in a sandbox Combination Lock setting. In a realistic multi-objective QA environment, we highlight the strength of ABBEL's isolated belief states, allowing for the generation of compressed beliefs without degrading reasoning.\nOur findings indicate that ABBEL opens avenues to study issues and potential remedies for agents in long context settings.", "tldr": "", "keywords": ["multi-step", "RL", "exploration", "memory", "reasoning", "beliefs", "context management"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a234a55d3d1b576cc5e8e58c7e2dad5814a6f427.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes regularly asking a language model to express its current belief state in natural language as a way to alleviate degradation caused by context length in long interactions. First, it evaluates frontier models in their ability to summarize context sufficiently into a belief state, identifying some failure modes like propagation of incorrect beliefs, and repeating uninformative actions because they don't reflect as changes in belief states. Then, it uses GRPO (group size 2) train LLMs to better generate and use belief states. Direct training improves LLM use of belief states, but does not match full context training. To further improve belief states, the paper proposes belief grading rewards, which can then outperform full history training on simple multi-turn tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Training LLMs to produce and use summaries of context to alleviate long-context issues and costs is an interesting and important direction.\n\n2. The analysis on how belief state compaction, while somewhat effective, still has failures modes without training is helpful, and creates a clear logical flow.\n\n3. It is good that the paper considers both outcome reward training, but also a stronger (albeit seemingly very task specific) way to grade the beliefs themselves."}, "weaknesses": {"value": "1. The core idea of summarizing LLM context has a long line of prior work [e.g. 1,2,3] which is not discussed and should be implemented and compared as baselines in this work. The only baseline compared is Mem1, however the methodology and results here are questionable. For example, the baseline is not reproduced in the paper's evaluation setting, and instead numbers are taken directly from the paper. Figure 7(b) is referenced as showing Mem1 states being \"much longer\", but actually has no Mem1 results! In fact, in 7(c), Mem1 seems to have lower token usage. \n\n2. The \"best\" method, \"belief grading\" is not described clearly. It seems like it involves using a much more capable model to perform analysis very specific to the Combination Lock task (simplified version of wordle with just 3 digits). How generalizable is this method? Why was it not used in the multi objective QA environment? This is important because without belief grading, the RL method performs worse than full context RL on the task. Perhaps switching to a more suitable and realistic task which actually tests long context capabilities would be better to demonstrate the benefits of outcome reward RL for generating and using belief states?\n\n[1] LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu.\n\n[2] MemGPT: Towards LLMs as Operating Systems\nCharles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, Joseph E. Gonzalez\n\n[3] Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models\nQingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, Liang Ding"}, "questions": {"value": "Main question: How exactly is belief grading implemented, and how generalizable is this methodology for other, realistic tasks? Which component of it helps the most, the supervision signal from a much stronger model, or something else? How does it compare to distillation from the stronger model?\n\nSuggestion: Figure 7 is hard to read due to the extremely thin bars. Consider making the X axis (number of objectives) log-scale.\n\nSuggestion: The paper keeps switching environments across sections by just citing the works that proposed them. It would be helpful for readers if the paper is self-contained, and each section begins with a clearer description of the environment being used.\n\nNitpick: \"multi-objective QA\" is called \"realistic\" in the discussion, when its a highly synthetic task obtained from randomly combining QA from existing retrieval datasets. I would refrain from calling this environment realistic throughout the paper.\n\nQuestion: Why were the same environments not used throughout? For example why not train/test for the environments used in Figure 4, which are much more interesting than Combination Lock?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2TQaxtD55g", "forum": "lMjxyHLL2R", "replyto": "lMjxyHLL2R", "signatures": ["ICLR.cc/2026/Conference/Submission22503/Reviewer_dYtH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22503/Reviewer_dYtH"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761526889865, "cdate": 1761526889865, "tmdate": 1762942245805, "mdate": 1762942245805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This work proposes using prompt-based belief states to summarize long-context information from LLM-user interactions, and selecting the next actions based only on these belief states rather than conditioning on the full context.\n- Shows that without training, the proposed belief-bottlenecked approach (\"ABEEL\") under-performs compared to full-context or belief-prompting baselines (where both the context and the belief states are available).\n- Shows empirically with Qwen 7B-Instruct that RL post-training has some potential to improve ABEEL compared to ABEEL without training on combination lock and the multi-objective QA task."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Improving the performance of LLMs in multi-turn interactions is an interesting problem, but this reviewer is not fully convinced of the novelty or significance of this work due to limited empirical demonstrations (see Weaknesses).\n- Clarity: The writing is clear, and Figure 1 clearly illustrates the difference between ABEEL and the existing approaches (vanilla and belief prompting)."}, "weaknesses": {"value": "1: Questions about the effectiveness of belief-bottlenecked policies: \n- Figure 2 shows that belief-bottlenecked models perform significantly worse than full interaction-based models or models that incorporate both belief and past history. Given the efforts and advances in increasing context lengths for newer models, it is unclear what advantages belief-bottlenecked models offer that long-context models cannot handle. The performance improvement achieved through post-training (when comparing among belief-bottlenecked policies) is insufficient to establish their effectiveness in multi-turn settings, especially if they are still outperformed by existing long-context models. \n- The results would be more compelling if there were some domains that long-context models cannot handle but belief-bottlenecked models can.\n\n2: Questions about generalizability across model types and sizes:\n- For the combination lock and multi-objective QA tasks, only one model (Qwen-7B-Instruct) is trained to demonstrate ABEEL and ABEEL-Length Penalty. \n- The results would be more convincing if the same experiments were repeated with different models and model sizes to show that the effectiveness of post-training with the ABEEL framework generalizes beyond a single model class or type.\n\n3: Insufficient empirical evidence:\n- It is unclear whether the performance differences of ABEEL-Length Penalty compared to the other models are statistically significant. In Figure 7, it is also unclear whether the models were trained with more than one seed.\n- For Table 1: since Qwen-7B-instruct model is trained (ABEEL, and ABEEL-Length Penalty), it would be useful to also compare with zero-shot Qwen-7B-instruct  in addition to zero-shot Qwen-14B-instruct. \n- The advantages of using ABEEL compared to vanilla and belief prompting are not clearly demonstrated in the experiments (Figure 5a and 5b). In particular, ABEEL (seed 2) performs worse than belief prompting and vanilla prompting, raising questions about the strength of the proposed approach compared to the baselines (see Point 1 raised above). \n\nWhile this paper addresses an interesting problem of effectively handling long contexts in multi-turn interaction settings with LLMs, it could be significantly improved by addressing the concerns regarding the novelty and significance of the empirical results."}, "questions": {"value": "Questions about multiple training seeds and using models other than Qwen 7B-Instruct are raised in Weaknesses to strengthen claims about empirical performance of ABEEL. \n\nAdditionally, there are other multi-turn domains that the authors could consider, where vanilla models have been reported to perform poorly. \n- For example, Laban et al., 2025 evaluates models on a suite of tasks such as coding, math, and summarization, where task-relevant information is gradually revealed through multi-turn interactions, so it is crucial to remember and maintain beliefs about key problem details. \n- In Zhao et al., 2025, \"Long-context retrieval\" section may be of particular interest to the authors. This task focuses on accurately retrieving and summarizing relevant information about user preferences in long-context conversations. \n\nFor both of these benchmark tasks, one could imagine belief states being beneficial and leading to improved performance compared to naïve long-context models.\n\n- Laban et al., 2025. \"LLMs get lost in multi-turn conversation\". https://arxiv.org/pdf/2505.06120.\n- Zhao et al., 2025. \"Do LLMs recognize your preferences? Evaluating personalized preference following in LLMs.\" https://arxiv.org/pdf/2502.09597"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZaDPCDzQ6n", "forum": "lMjxyHLL2R", "replyto": "lMjxyHLL2R", "signatures": ["ICLR.cc/2026/Conference/Submission22503/Reviewer_y4kX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22503/Reviewer_y4kX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761940057014, "cdate": 1761940057014, "tmdate": 1762942245569, "mdate": 1762942245569, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "ABBEL replaces growing histories with a compact natural-language belief and acts only on that, giving near-constant memory and shorter reasoning. Zero-shot lags full history, but RL, especially belief grading, recovers or beats it and enables a memory–accuracy trade-off. In multi-objective QA it outperforms MEM1 with a shorter internal state; remaining issues are belief-update errors, hallucinated past steps, and repeated actions when beliefs stall."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Clear, interpretable bottleneck: separating stored “belief” from transient reasoning is simple,\n  model-agnostic, and yields near-constant memory across steps while often reducing tokens and\n  action-side reasoning.\n* Solid empirical sweep and diagnostics: six environments, ablations (vanilla / belief-prompting /\n  ABBEL), and candid analysis of failure modes (propagated belief errors, hallucinated past steps).\n* RL contributions are practical: outcome-based RL recovers most performance; belief-grading\n  reduces regret and allows explicit length–performance trade-offs; competitive/better than MEM1\n  in long-horizon QA with a more compact internal state."}, "weaknesses": {"value": "* Novelty/positioning: very close to prior “learned memory” agents (MEM1/VeRL/rLLM); the\n  belief–reasoning split reads as incremental rather than fundamentally new. Missing/under-cited\n  contemporaries (e.g., MemAgent) weaken SOTA claims.\n* Baselines/fairness: QA compares ABBEL-RL (7B) to an untrained 14B full-history model; no\n  apples-to-apples 7B full-history RL baseline reported. Combination-Lock gains hinge on a toy\n  setting and ground-truth belief grading; generalization to realistic tasks is unclear.\n* Practicality: two calls per step (belief + action) and RL/aux-task training cost; belief-quality\n  heuristics for real domains are unspecified; observed loops where the agent repeats uninformative\n  actions when beliefs don’t update."}, "questions": {"value": "* Did you train and evaluate a 7B full-history (vanilla) RL baseline in QA and Combo-Lock? If so,\n  please report; if not, this is needed to isolate bottleneck benefits from RL effects.\n* How will you grade beliefs without ground-truth posteriors in realistic settings (retrieval QA, SWE)?\n  Would a constrained/structured belief schema (e.g., JSON slots) reduce hallucinations and make\n  grading feasible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "5DRYxpOQnl", "forum": "lMjxyHLL2R", "replyto": "lMjxyHLL2R", "signatures": ["ICLR.cc/2026/Conference/Submission22503/Reviewer_eHNU"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22503/Reviewer_eHNU"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761970255902, "cdate": 1761970255902, "tmdate": 1762942244082, "mdate": 1762942244082, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces ABBEL, a framework that replaces growing multi-turn interaction histories with a compact, natural-language belief state. The authors identify frontier models have inferior performance because of error in belief updating. Thus, finetuning LLMs with belief grading improves LLMs in a multi-objective QA environment and shows promising results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Usage of belief state to compress the history trajectory to assist LLM for effective actions sampling, making the internal state compact and inspectable. \n2. RL with belief grading improves small language model in multi-objective QA tasks. Use a model to parse belief into characters and then comparing with ground truth posterior is a simple yet effective solution."}, "weaknesses": {"value": "1. RL with belief grading heavily relies on ground truth posterior. It’s unclear how robust grading signals will be in complex, non-synthetic settings where ground-truth posteriors aren’t computable. (as mentioned in the \"Limitations\" sections)\n2. Only one benchmark in the main text. I would expect more benchmarks on results of RL + belief grading, like WebShop [1] as the Mem1 [3] authors did, and maybe also ALFWorld [2], a text based environment for agents to reason and interact with.\n3. It seems to me that a lot of context is spent on evaluating frontier models with belief bottlenecks. I do believe that this is an important finding for motivation, but I believe more explanations and results can be put in the appendix, where the main text can focus on RL with belief grading.\n\n[1] Yao, Shunyu, et al. \"Webshop: Towards scalable real-world web interaction with grounded language agents.\" Advances in Neural Information Processing Systems 35 (2022): 20744-20757.\n\n[2] Shridhar, Mohit, et al. \"Alfworld: Aligning text and embodied environments for interactive learning.\" arXiv preprint arXiv:2010.03768 (2020).\n\n[3] Zhou, Zijian, et al. \"MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents.\" arXiv preprint arXiv:2506.15841 (2025)."}, "questions": {"value": "1. Would you providing some examples/ideas of applying heuristic function to model ground truth posterior when computing ground truth posterior is not practical?\n2. Is the core different between ABBEL and MEM1[1] only about separating belief state update into multiple steps accordingly?\n\n[1]Zhou, Zijian, et al. \"MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents.\" arXiv preprint arXiv:2506.15841 (2025)."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QU8FX4j51F", "forum": "lMjxyHLL2R", "replyto": "lMjxyHLL2R", "signatures": ["ICLR.cc/2026/Conference/Submission22503/Reviewer_icj6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22503/Reviewer_icj6"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22503/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761996142366, "cdate": 1761996142366, "tmdate": 1762942243754, "mdate": 1762942243754, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}