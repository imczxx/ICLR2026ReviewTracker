{"id": "YnNIp38v1M", "number": 22216, "cdate": 1758327877356, "mdate": 1759896879681, "content": {"title": "CIMemories: A Compositional Benchmark For Contextual Integrity In LLMs", "abstract": "Large Language Models (LLMs) increasingly use persistent memory from past interactions to enhance personalization and task performance. However, this memory creates critical risks when sensitive information is revealed in inappropriate contexts. We present CIMemories, a benchmark for evaluating whether LLMs appropriately control information flow from memory based on task context. CIMemories uses synthetic user profiles with 100+ attributes per user, paired with various task contexts where each attribute may be essential for some tasks but inappropriate for others. For example, mental health details are necessary for booking therapy but inappropriate when requesting time off from work. This design enables two forms of compositionality: (1) flexible memory composition by varying which attributes are necessary versus inappropriate across different settings, and (2) multi-task composition per user, measuring cumulative information disclosure across sessions. Our evaluation reveals frontier models exhibit between 14%-69% attribute-level violations (leaking inappropriate information), and that higher task completeness (sharing necessary information) is accompanied by increased violations, highlighting critical gaps in integrity-aware memory systems.", "tldr": "CIMemories is a dataset of synthetic user profiles paired with recipient–task contexts that simulates persistent,cross-session LLM “memory” to evaluate whether models use long-term context appropriately—sharing what’s needed while avoiding leaks.", "keywords": ["Contextual Integrity; Inference-time Privacy; Input-output flow"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8bb3e49920ac58b26c74ab9d1895b3301ee661bc.pdf", "supplementary_material": "/attachment/4e1e07d4ec1e199d0bb61769c559c7f712cbd656.zip"}, "replies": [{"content": {"summary": {"value": "This work introduces CIMemories, a benchmark for testing whether memory-augmented LLMs respect Contextual Integrity such that LLMs reveal stored user facts to third parties only when appropriate, while still being helpful. It composes synthetic user profiles and social contexts so the same attribute can be required in one setting but inappropriate in another, and evaluates models with two complementary measures: violation (leaking “not-to-share” facts) and completeness (sharing what’s needed). Experiments on frontier models reveal a clear privacy–utility trade-off and accumulating leaks across tasks, with scaling and “reasoning” prompts offering only modest relief. The contribution is a clear formalisation of Contextual Integrity, a controllable compositional dataset, and an empirical study that identifies where current assistants fail and how mitigation shifts the trade-offs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper applies Contextual Integrity (CI) to memory-augmented LLMs in a compositional setting, formalising a benchmark where the same attributes of a user profile can be appropriate in one context but inappropriate in another. The ideas of flexible memory composition and multi-task composition per user are clearly specified and expand the scope beyond single-instance evaluations. The two scores, Violation@n and Completeness, provide a simple but representative evaluation of the privacy–utility trade-off in persistent LLM memory\n\n* The methodology is rigorous and systematic. The paper describes the data-generation pipeline, the evaluation setup, and uses quantitative and qualitative analyses across multiple models and configurations. Results include consistent patterns (e.g., violation/completeness trade-off, scaling saturation, and domain-level “granularity” errors), supported by tables/figures and concrete violation excerpts.\n\n* The presentation is clear. The benchmark workflow, task formation, and the REVEAL judge are explicitly laid out, with prompt templates and an overview figure that maps profiles, contexts, metrics, and the judge’s role. This makes it straightforward to understand what counts as an explicit reveal and how metrics are computed\n\n* The work addresses a timely risk in persistent memory: disclosing the information in the wrong place. By quantifying violations and completeness, showing accumulation over tasks/generations, and analyzing scaling, reasoning, and conservative prompts, it offers actionable insights for deployment and motivates future mitigation work."}, "weaknesses": {"value": "* Reliance on LLM-Generated Ground Truth and Judges: The benchmark’s contextual integrity labels are entirely generated using closed-source LLMs (GPT-5 personas) and judged by another LLM (DeepSeek-R1) that only flags explicit disclosures. This creates a potential circular evaluation loop, the same class of models(GPT-5) being tested also defines the “ground truth.” To strengthen validity, the authors can include a small-scale human validation study to measure inter-annotator agreement and human–LLM alignment.\n\n* Limited Coverage and Cultural Bias in Labeling: The dataset includes only 10 user profiles and further filters out all attribute–context pairs where privacy personas disagree (entropy > 0), resulting in potential selection bias toward “easy-to-label” cases. Moreover, the benchmark grounds its contextual-integrity labels in Westin’s U.S. privacy personas and a set of U.S.-centric contexts (e.g., HR departments, IRS agents, USCIS officers). Because contextual-integrity norms vary across cultures, this narrow framing limits the benchmark’s representativeness and generalizability. Expanding the dataset with cross-cultural personas, diverse contexts, and non-U.S. human raters would both mitigate coverage bias and improve external validity for global adoption.\n\n* Minor Presentation Issues: A dangling “?” in § 5.2.2."}, "questions": {"value": "* What proportion of attribute–context pairs survive the unanimity filter across Westin personas (entropy = 0), and how are discarded pairs distributed across domains?\n\n* Could you run a small human validation study to measure inter-annotator agreement and human–LLM alignment on “share” vs “private” labels? This would help verify that LLM-generated CI labels reflect plausible human norms.\n\n* Since CI norms vary culturally, and current personas and contexts are U.S.-centric (e.g., IRS, USCIS), would you consider adding cross-cultural personas and context to evaluate whether violation/completeness patterns shift in future work?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ckKd0jJKWd", "forum": "YnNIp38v1M", "replyto": "YnNIp38v1M", "signatures": ["ICLR.cc/2026/Conference/Submission22216/Reviewer_9MtN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22216/Reviewer_9MtN"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761529284067, "cdate": 1761529284067, "tmdate": 1762942118221, "mdate": 1762942118221, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposed a new benchmark focusing on evaluating how LLMs correctly leverages in-context memories, particularly user profiles. The problem is that, based on different tasks, the LLM should reveal certain user information but not others. The paper curated a new benchmark which features two key innovations: (1) flexible memory composition; (2) multi-task composition; The results show that recent LLMs still struggle with many violations on user privacy and shows a trade-off between task completeness and contextual integrety"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The problem definition and motivation is very clear and under-explored in the community\n- The paper writing and techical details are clear; The analysis on the violation vs completeness trade-off, and the impact of model size and thinking is very helpful for further understanding the challenge"}, "weaknesses": {"value": "- The discussion on how to mitigate the problem is too weak. It is ideal to have an initial improved baseline based on the insights from the benchmarking analysis; these actionable insights are most helpful to the community; for example, which type of reasoning may benefit most to reduce violation while keeping completeness and general performance?\n- Without a quantative comparison with prior contextual privacy benchmarks, it is unclear whether the CIMemories benchmark is testing similar skills or is actually revealing some new challenges. Concretely, it would be good to add columns in Table 1 reflecting performance in prior privacy related benchmarks such as CI-Bench, to show if there is a strong correlation between the performances.\n- Minor: The use of color in Figure 4, 5 can be improved considering red-green color blind readers"}, "questions": {"value": "- Figure 5 (b) seems to show that reasoning is a promising direction in addressing this tradeoff; can the authors provide more details on how the reasoning is performed in the experiments"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Zw2GMGkbWr", "forum": "YnNIp38v1M", "replyto": "YnNIp38v1M", "signatures": ["ICLR.cc/2026/Conference/Submission22216/Reviewer_7wCV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22216/Reviewer_7wCV"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761671994560, "cdate": 1761671994560, "tmdate": 1762942117997, "mdate": 1762942117997, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces CIMemories, a benchmark for evaluating whether memory-augmented LLMs appropriately control information flow based on social context, based on Contextual Integrity theory. The benchmark features synthetic user profiles with ~147 attributes each across 9 domains (finance, health, employment, etc.), paired with ~45 task contexts per user. It evaluates 8 models and find violation rates of 14-69%.  It also demonstrates that violations accumulate over time as users engage in more tasks, and that current mitigation strategies (scaling, prompting) provide limited relief."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1. Timely and critical benchmark. Most of the companies are deploying Memory-augmented LLMs, but prior benchmarks don't capture the compositional nature of contextual privacy.\n\nS2: The formalization in Section 3 clearly connects CI theory to measurable metrics, with appropriate violation and completeness perspectives.\n\nS3. Analysis is done properly. The granularity failure finding (models identify right domain but over-share details) and the domain-wise breakdown in Figure 3 provides interpretable insights."}, "weaknesses": {"value": "W1. Benchmark scale is one of the major concern that I have. With only 10 profiles and no statistical testing, the results lack the rigor needed for definitive conclusions. \n\nW2. Synthetic data. While acknowledged, this is a fundamental limitation. Real users have complex, inconsistent preferences that synthetic profiles cannot capture. \n\nW3. Requiring unanimous agreement across all 3 personas discards many valid scenarios. This may bias toward only \"obvious\" privacy violations. Real privacy often involves legitimate disagreement, which is excluded and the paper doesn't report what percentage of attribute-context pairs were discarded."}, "questions": {"value": "Q1: 10 profiles is very less in my pov to make result significant. I get the cost but can it be increased for the open-source models where inference is cheaper? \n\nQ2. Synthetic data might not capture complex real user preference. Real LLM users have maybe 20-50 memories after a few months. 147 attributes per profile seems to high. Why so many? Any plan to collect real user data?\n\nQ3. What percentage of attribute-context pairs were discarded?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JGilRBtUK5", "forum": "YnNIp38v1M", "replyto": "YnNIp38v1M", "signatures": ["ICLR.cc/2026/Conference/Submission22216/Reviewer_D8mp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22216/Reviewer_D8mp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761924795757, "cdate": 1761924795757, "tmdate": 1762942117760, "mdate": 1762942117760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces CIMemories, a benchmark for evaluating whether memory‑augmented LLM assistants respect contextual integrity. The dataset uses synthetic user profiles (∼147 attributes per profile) and curated social contexts (∼46 per profile)\n\nEach attribute–context pair is labeled (necessary to share vs. inappropriate to share) by sampling multiple “privacy personas” (Westin’s fundamentalist/pragmatist/unconcerned) from a strong LLM and retaining only unanimous labels; evaluation measures (i) Violation@n:worst‑case attribute leakage over n samples in contexts where the attribute should not be shared and (ii) Completeness—fraction of necessary attributes conveyed in contexts where they should be shared.\n\nThe paper further analyzes domain‑level failure modes (“granularity failure”), scaling effects, prompting defenses, and how both multi‑task composition and memory composition exacerbate leakage."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 4}, "strengths": {"value": "- Prior CI‑style evaluations (e.g., ConfAIde, PrivacyLens, CI‑Bench, LLM‑CI) largely focus on single‑shot vignettes or agent trajectories without rich, persistent user memory. CIMemories squarely targets that gap.\n- The worst‑case attribute‑level Violation@n coupled with task‑level Completeness makes the privacy–utility trade‑off explicit; the visual breakdown in Figure 3 (p.6) convincingly illustrates “granularity failure” (right domain, wrong detail).\n- Results span multiple model families and sizes and include simple defenses (prompting) and test‑time “reasoning” variants, yielding actionable observations (e.g., light reasoning sometimes lowers violations with minimal completeness loss)\n- Well‑motivated by the trend from retrieval‑based memory (RAG/MemoryBank/MemGPT) toward long‑context, “prefix the memories” assistants, and shows those settings remain privacy‑fragile"}, "weaknesses": {"value": "- The “ground‑truth” labels (“necessary” vs. “inappropriate”) are produced by GPT‑5 with persona prompts, and then other LLMs are evaluated against those labels. Even with personas, this can encode the teacher model’s normative and stylistic biases. The paper argues LLMs are conservative vs. humans when labeling sensitive content, but no human audit is provided to calibrate false positives/negatives of the labeler on a subset. A 5–10% human‑labeled slice would materially increase credibility.\n- Westin personas are U.S.‑centric and decades old; their priors may not reflect contemporary or cross‑cultural norms. The paper uses Westin‑based priors to mix persona votes but does not test sensitivity to those priors. A cross‑cultural variant or at least a sensitivity analysis is warranted.\n- The headline “violations increase with more tasks/generations” is partly tautological because Violation@n is worst‑case per attribute over more trials. This is informative for risk, but the paper should also report the per‑turn hazard rate (probability of first leakage at turn t) and time‑to‑leak distributions to separate inherent risk from simple exposure."}, "questions": {"value": "- Reference Missing In line 376. please fix\n- Each context has ~7 “necessary” vs. ~84 “not‑to‑share” attributes, so a model that is slightly verbose can accumulate many apparent violations. Completeness is an average, whereas Violation@n is a worst‑case max across tasks and these aggregations are not symmetric. Consider reporting AU‑Privacy–Utility curves, pareto fronts or a balanced score.\n- Reveal detection is too strict in one dimension and too lax in another. Could you use multiple judges in order to make it more robust. This under‑counts partial leaks (e.g., “my antidepressant dosage increased last month”) and leaks via implicature (e.g., “after the DUI class …”)\n- Can you provide results under shared decoding parameters across all models? Right now, defaults differ by vendor and may be optimized for safety/verbosity differently.\n- Given the heavy class imbalance (∼6.7 necessary vs. ∼83.7 not‑to‑share per context; Table 2), how would your conclusions change under a balanced per‑context metric (macro‑averaging) and a per‑turn hazard‑rate view rather than worst‑case Violation@n?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "CCaqXITyzr", "forum": "YnNIp38v1M", "replyto": "YnNIp38v1M", "signatures": ["ICLR.cc/2026/Conference/Submission22216/Reviewer_M7fA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22216/Reviewer_M7fA"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22216/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998907618, "cdate": 1761998907618, "tmdate": 1762942117134, "mdate": 1762942117134, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}