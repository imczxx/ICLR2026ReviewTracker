{"id": "vuyk1fSaE4", "number": 6855, "cdate": 1757998400329, "mdate": 1759897888036, "content": {"title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning", "abstract": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches, achieving superior test-time performance and token efficiency.", "tldr": "", "keywords": ["large language model", "reinforcement learning"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fe693e5e18b91e20985f704aa78262b6f6f1b940.pdf", "supplementary_material": "/attachment/cabb07f83a80c15f829929e0d21659d851753633.zip"}, "replies": [{"content": {"summary": {"value": "This paper addresses the limitations of standard Markovian RL in explaining or guaranteeing the emergence of reflective reasoning (like backtracking and error correction) observed in Large Language Models (LLMs) trained for complex tasks. It proposes recasting the problem within the Bayes-Adaptive RL framework, which explicitly optimizes for test-time performance under uncertainty about the underlying MDP. This Bayesian approach naturally incentivizes exploration to gather information and reduce uncertainty, leading to adaptive policies that update beliefs based on outcomes. The authors introduce BARL, an algorithm where the LLM maintains a posterior over MDP hypotheses, weighting actions by belief and penalizing discrepancies between predicted and observed rewards to guide strategy switching and reflective exploration. Empirical results on synthetic and math reasoning tasks show BARL outperforms Markovian RL baselines in accuracy and token efficiency, demonstrating the benefits of principled, belief-driven exploration over fixed, deterministic policies."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposes a compelling Bayes-Adaptive RL framework that extends standard Markovian RL. This Bayesian approach provides a principled way to encourage exploration and generalization for LLM reasoning, moving beyond policies that might simply memorize training solutions.\n- The core concepts are well-supported through both theoretical justification and empirical validation. The inclusion of a didactic synthetic example significantly aids in illustrating the core mechanism and benefits of BARL.\n- Experimental results indicate that the BARL algorithm achieves superior token efficiency compared to baselines like GRPO and a progress-reward-based method, suggesting more effective exploration and reasoning per token."}, "weaknesses": {"value": "- While the BARL framework is theoretically well-motivated, the empirical performance improvements shown in the main results appear somewhat incremental compared to the baselines across all benchmarks.\n- While the paper claims the Bayesian RL framework improves generalization, the empirical results demonstrate only incremental performance gains over baselines. Consequently, the primary source of the observed benefits, particularly token efficiency, remains ambiguous."}, "questions": {"value": "- Could the authors provide further intuition on why the Bayes-Adaptive framework specifically leads to better token efficiency? \n- The progress reward  is defined based on the policy $\\pi_{\\theta}$ itself. However, in the policy gradient derivations (Eq 3.2 for Markovian RL and implicitly in Eq 5.1 for BARL), the reward $r(s_t, a_t)$ appears to be treated as if it were an external reward independent of the policy parameters $\\theta$. Could the authors comment on whether this simplification introduces bias into the gradient estimates, and if so, how it might affect the training dynamics or the final policy?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "EpfZD1wTFL", "forum": "vuyk1fSaE4", "replyto": "vuyk1fSaE4", "signatures": ["ICLR.cc/2026/Conference/Submission6855/Reviewer_QdQn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6855/Reviewer_QdQn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761598413856, "cdate": 1761598413856, "tmdate": 1762919111897, "mdate": 1762919111897, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a Bayes-Adaptive RL objective for finetuning LLMs. This objective incentives the policy (model) to maximize the expected reward on the given task but also reason about the underlying MDP (which is assumed to be a random variable). Experimental results show improvements over GRPO (traditional Markovian-based objective) on popular benchmarks like AIME and the underlying objective leads to policies that exhibit significant improvements in token efficiency."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Motivations for paper is clear and paper is well-written\n- Formulation is easy-to-follow and small-scale experiments are helpful in understanding the effectiveness of the approach\n- Strong improvements in token efficiency and slight improvements over GRPO in math-reasoning tasks."}, "weaknesses": {"value": "- Ablation of progress reward. The provided formulation assumes reward is based on some gold action (e.g. answer token y) and this is used to introduce a progress reward incentivizing the model to think. However, the effect of this reward does not seem to ablated. Additionally, there are domains where reward is not a function of a gold action (e.g. for agentic coding domains, reward may be computed by executing sampled code against some test cases). Does this limit the generality of this approach?\n- Missing implementation details. What prompts were used to sample from the posterior of MDPs?"}, "questions": {"value": "Please see weaknesses^"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JHZqBWYoI4", "forum": "vuyk1fSaE4", "replyto": "vuyk1fSaE4", "signatures": ["ICLR.cc/2026/Conference/Submission6855/Reviewer_FoH3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6855/Reviewer_FoH3"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882620403, "cdate": 1761882620403, "tmdate": 1762919111500, "mdate": 1762919111500, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces BARL, a Bayes-Adaptive RL framework that equips LLMs with test-time reflective exploration. By maintaining and updating beliefs over task hypotheses, BARL guides the model to backtrack and switch strategies when observations contradict expectations. On both synthetic and math reasoning tasks, BARL outperforms standard Markovian RL baselines in accuracy and token efficiency, demonstrating that principled exploration—not mere reflection frequency—drives better generalization."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. Clearly articulates the limitation of standard Markovian RL—its inability to support test-time reflective exploration—and provides theoretical justification for why this hampers generalization.\n\n2. The theoretical framework is very solid, which Introduces Bayes-Adaptive MDPs to model LLM reasoning, formalizing test-time generalization as maximizing expected return under a posterior over candidate tasks, grounding exploration in principled Bayesian principles.\n\n3. Authors present novel BARL, which maintains and updates beliefs over MDP hypotheses; mismatches between predicted and observed rewards automatically trigger strategy switching and backtracking, enabling reflective behavior without heuristic rules.\n\n4. Comprehensive empirical validation demonstrates consistent gains over strong Markovian RL baselines on both synthetic and challenging math benchmarks, achieving higher accuracy while using up to 2× fewer tokens, evidencing superior efficiency and generalization.\n\n5. The insightful ablation analysis shows that effective exploration, not the sheer frequency of self-reflection, drives performance, offering actionable guidance for future research into test-time scaling of reasoning models."}, "weaknesses": {"value": "1. Computational Overhead: Despite KV-cache reuse, the per-step computation still scales linearly with the number of sampled hypotheses (|M|) and grows rapidly when the model size or context length increases; no GPU-hours or throughput curves are reported to quantify this burden.\n\n2. Keyword-based Reflection Detection: Relying on fixed trigger words to flag “self-reflection” is unreliable—it captures only explicit surface signals and cannot verify whether the model actually revises its reasoning strategy, making the behavioral analysis less rigorous."}, "questions": {"value": "1. How does the magnitude of the penalty term β affect the predicted and observed rewards? Can relevant experiments be conducted to observe this?\n\n2. This work demonstrates strong performance on mathematical problems, but does it remain effective on other general tasks where the structure is less clear?\n\n3. The exponential gain in Theorem 4.2 relies on a constructed binary tree and a uniform prior—does it still hold in general cases?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rzV0Y7A6x9", "forum": "vuyk1fSaE4", "replyto": "vuyk1fSaE4", "signatures": ["ICLR.cc/2026/Conference/Submission6855/Reviewer_S3kA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6855/Reviewer_S3kA"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761902961162, "cdate": 1761902961162, "tmdate": 1762919111079, "mdate": 1762919111079, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "+ Summary & Contributions\n\t- The authors focus on the problem of explicitly eliciting \"reflective reasoning/behaviors\" from LLMs during the course of RL fine-tuning for improved reasoning capabilities. Such reflective reasoning/behavior is exemplified through the use of backtracking or error correction during chain-of-thought reasoning.\n\t- The authors maintain that the \"conventional, Markovian\" RL problem formulation is ill suited for consistently encouraging such desired reflective behaviors. \n\t- The authors turn to Bayesian RL as a problem formulation whose core policy optimization objective necessarily results in reflective reasoning and behaviors. Leveraging this formulation yields a novel RL fine-tuning algorithm (BARL) geared towards enhanced LLM reasoning capabilities on mathematics question-answering tasks (GSM8K, MATH, CollegeMath, OlympiadBench).\n\t- Experiments across a range of mathematics Q&A benchmarks and model types demonstrates improved performance over standard GRPO. Additional empirical results assess the volume of tokens needed to arrive at LLM responses and finds that BARL is able to obtain better accuracies using fewer tokens."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "+ Quality\n\t- Strengths\n\t\t- The authors display a nice insight in trying to port over ideas from Bayesian RL into modern LLM fine-tuning.\n\t- Weaknesses\n\t\t* Major\n\t\t\t- In RL (or what the authors seem keen to refer to as \"conventional/Markovian RL\"), there is no such distinction between training time and testing time as there is in standard supervised learning. There is just one single, periodic stream of an agent interacting with an environment episode after episode. Yet, throughout the paper, the authors operate under a premise that such a train-test split exists in RL. Two places where such a split occurs in RL are multi-task RL [1] and meta RL [2]. The former is where an agent interacts with a family of MDPs indexed by an observable, episodic context that helps an agent distinguish MDPs while also encoding structural similarities that might aid in generalization and transfer to unseen MDPs. The latter is where an agent begins with a distribution over MDPs (meta-training) with the goal of synthesizing a policy capable of learning behaviors quickly in a downstream MDP distribution (meta-testing). Based on the rest of the paper, the authors seemed to be confused about these well-established problem settings within the confines of RL. Rather than standard single-task RL, the authors seem to be interested in meta RL, where the resulting LLM policy will approach novel questions by exploring and within relatively few interactions (CoT reasoning steps) explore and expose enough information to subsequent exploit and solve the task (answer the question correctly). Another possible alternative for what the authors might want is Bayesian multi-task RL (that is, solving the BAMDP induced by a contextual MDP). In any case, the problem the authors seem interested in solving doesn't seem like it neatly fits into the Bayesian RL formulation. That said, the authors lack of a well-defined MDP, prior distribution over MDPs, and resulting induced BAMDP formulation makes it difficult to tell. Exploration and generalization are distinct, orthogonal axes of data efficiency in RL; while Bayesian RL does yield the optimal solution to the exploration-exploitation trade-off, it does not do so \"to improve generalizability\" (L125-126), which is a separate concern entirely.\n\t\t\t- While the goal of reflective reasoning/behavior/exploration is ambiguous throughout this paper (see Clarity comments below), it's worth noting that just because a LLM emits text indicating that it is reconsidering something, that doesn't necessarily mean that it \"disregards the previous one or more steps\" (L175). How do the authors know that this is in fact the case? Those previously generated tokens are all still in the context of previously generated tokens and may in fact still influence subsequently generated tokens. This also connects to a later point (L182-186) where the authors fixate on the Markov state of a MDP being insufficient for reflective reasoning without justifying why a MDP with states defined as histories is insufficient to resolve their concerns (the footnote in L215 doesn't make sense, perhaps stemming from the lack of a formal definition for reflective reasoning).\n\t\t\t- The authors fail to provide a formal proof of Theorem 4.1. This is particularly unfortunate since I had hoped a theoretical result would have forced the authors to become concrete and specific about how they formalize reflective reasoning (in order to make a formal statement about \"non-reflective policies\" in L190). The lack of a proof would call into question a central claim of this paper that standard RL is inadequate for reflectve reasoning, which itself necessitates the use of Bayesian RL. Perhaps the authors believe the exposition preceding Theorem 4.1 constitutes some kind of proof sketch. However, they seem to have forgotten the basic fact that a MDP may admit multiple optimal policies. With bounded rewards under a finite horizon or discounted, finite rewards in an infinite horizon, results from dynamic programming theory tell us that there exists a (or, at least one) deterministic optimal policy that is greedy with respect to the optimal action-value function. There may still however additionally be multiple stochastic optimal policies that also achieve optimal value (recall that the mapping from policy to value function is a many-to-one mapping). In summary, it is unclear why standard \"Markovian\" RL with respect to a history-based state space should be considered non-reflective nor is it clear why \"Markovian\" RL only admits deterministic policies -- both of these facts would call Theorem 4.1 into question and render the text box comparing RL to Bayesian RL (L227-232) incorrect.\n\t\t\t- The authors claim that the optimal policy of the BAMDP (that is, the Bayes-optimal policy with respect to the original prior distribution over MDPs) will \"naturally induce exploratory behaviors with reflections\" (L223). Certainly, the first part of the claim is true as the Bayes-optimal policy yields the optimal solution to the exploration-exploitation trade-off. Why is the second part (the \"with reflections\") part true? Why is it that the Bayes-optimal policy of a MDP must use reflections to optimally address exploration? The authors proceed to describe \"reflective actions\" as those which may be sub-optimal but yield information to reduce epistemic uncertainty in the MDP (L224-226); however, this is precisely the definition of any exploratory action taken by the Bayes-optimal policy. Is \"reflective\" just the authors' synonym for \"explorative\" or are reflective actions/steps a subset of all exploratory actions? If it's the latter, how are those reflective actions concretely defined?\n\t\t\t- The authors' proof of Theorem 4.2 aims to be constructive, hoping to show that there exists a MDP for which the \"test-time\" return of \"a Bayes-Adapative\" policy is exponentially larger in $T*$ than that of the optimal policy from standard RL. As discussed at other points in this review, the modifier \"test-time\" is vacuous here. I'm left to assume \"a Bayes-Adaptive\" policy refers to a policy of the associated BAMDP. Firstly, why is $T*$ the appropriate parameter for this claim to depend on? Mechanistically, the authors' subsequent didactic example shows that this simply needs to be the appropriate number of steps for a policy to be capable of iterating through all the possible incorrect answers before necessarily landing on the correct one (L311). Why is this reasonable? Also, surely a \"Markovian\" policy for a MDP where states are defined by histories (something the authors eventually do after Equation 5.1), would be capable of performing the exact same behavior of iterating through all possibilities exhaustively until the correct leaf node/three token repetition is found. This would seemingly invalidate the argument presented for \"any Markovian policy\" in the proof of Theorem 4.2 (Appendix A). Putting that aside, the claim itself is rather strange. It is well known that the Bayes-optimal policy when evaluated in the original MDP will, by definition, achieve performance that is less than that of the optimal policy [8]. \n\n\t\t* Minor\n\t\t\t- In a paper so focused on RL, it is strange the authors don't see (or at least fail to acknowledge) the distinction between the action-value function and the advantage function (L162), which are of course two separate mathematical objects. \n\t\t\t- What is the point of considering \"the non-standard undiscounted, infinite-horizon MDP\" (L197)? Without discounting and non-negative rewards, there are likely numerous policies which all achieve the optimal reward (of infinity?) and become indistinguishable from one another without additional specification.\n\t\t\t- Notice that Bayesian RL does not automatically admit partial observability (L211). The corresponding BAMDP can be defined via a so-called hyperstate space [3] that includes the original MDP state as well as the current posterior distribution over MDPs, both of which are fully observed.\n\t\t\t- The authors claim that they may write the likelihood of a reward according to (what seems to be) a Laplace distribution. Is this an assumption? Or is this some fact? For the case of question-answering with binary rewards indicating correctness, it actually seems strange to not simply treat rewards as Bernoulli random variables whose parameters have epistemic uncertainty represented by Beta distributions (as is standard [3,4]).\n+ Clarity\n\t- Strengths\n\t\t- Aside from the incorrect jargon/modifiers introduced by the authors to describe RL, the paper is reasonably written and structured.\n\t- Weaknesses\n\t\t* Major\n\t\t\t- Throughout the paper, the authors make heavy use (42 instances) of the word \"reflective\" as an adjective and modifier to standard terminology used in RL and/or LLMs. There are reflective behaviors/policies, reasoning, exploration, signals. The definitions for some of these are dependent on the definitions of others such that if there was a clear, concrete definition for reflective reasoning and exploration, the others would likely follow. Unfortunately, the authors fail to provide such an explicit definition, instead only gesturing at exemplars for what they consider reflective reasoning (L171-177). Strangely enough, one of these examples is backtracking, a property of a search/planning algorithm rather than a RL algorithm. This paper could be tremendously clearer if the authors could formalize exactly what property/properties reflective reasoning/behavior exhibits so that readers can have transparency and insight into how standard RL might inconsistently deliver such behavior and why Bayesian RL might be a promising tool for explicitly encouraging it. \n\t\t* Minor\n\t\t\t- The authors question whether desirable reflective behaviors will emerge with \"conventional\" RL training (L38-39), which seemingly contradicts the preceding sentence indicating that such behaviors do in fact emerge with standard RL.\n\t\t\t- The authors mention \"prevalent views\" (L40) on how test-time reflections constitute useful exploratory steps yet fail to make a single supporting citation. Certainly, there is now near-ubiquitous understanding of how chain-of-thought reasoning helps (and citable papers to support the claim) but specific examples of work advocating for the kinds of reflective behaviors the authors focus on in this paper would be helpful to support the proposed approach.\n\t\t\t- There is no such thing as \"epistemic explorations\" (L53), but presumably the authors mean exploration to reduce epistemic uncertainty in the underlying MDP.\n\t\t\t- Rather than taking up space with summarizing text boxes (L78-90, L227-232, L468-472) better fit for a poster about the paper, I would encourage the authors to reinvest that space in clearer exposition of the technical content within the paper.\n\t\t\t- Per the comment above, the claim that \"conventional RL explores only during training\" (L119) is vacuous when RL has no distinction between training and testing.\n\t\t\t- The language used throughout the paper in regards to uncertainty and Bayesian RL is riddled with odd, incorrect/vague statements that wear down an interested reader of this paper. What does it mean that \"the environment is predefined with certainty\" (L163)? Certainly, it does not mean that the environment is fully known to the agent, since that would imply a planning problem rather than a RL problem.\n\t\t\n\n+ Originality\n\t- Strengths\n\t\t- To the best of my knowledge, attempting to use Bayesian RL in the proposed manner to elicit improved reasoning capabilities is a novel idea.\n\t- Weaknesses\n\t\t* Major\n\t\t\t- The authors identify two meta RL approaches for encouraging better reasoning capabilities (L110-115), but their explanations for how their method is distinct and an improvement over these methods is entirely unclear. What does it mean for reflective reasoning to be grounded \"in environment rewards, rather than relying solely on the interal CoT states\"? If additional data on \"golden strategies\" is available, why would this be sub-optimal compared to additional exploration for discovering such strategies via BARL? \n\t\t\t- The authors ultimately propose a policy-gradient method for addressing their proposed Bayesian RL problem but don't actually connect it with existing work on Bayesian actor-critic methods [7]. What is the connection? Is the authors' approach well aligned with existing work on policy-gradient methods in Bayesian RL? How is it different? Are there sensible Bayesian actor-critic baselines that BARL can and should be compared against?\n\t\t* Minor\n\t\t\t- Some of the references must have incorrect Bibtex entries and are missing the year and publication information (Arumugam & Singh, NIPS 2022; Lidayan et al., ICLR 2025).\n\t\t\t- The authors are not the first to entertain coupling ideas from Bayesian RL with LLMs and those existing connections ought to be acknowledged [5,6].\n\n+ Significance\n\t- Strengths\n\t\t- This paper would likely inspire subsequent work to investigate more principled, rigorous methods for combining Bayesian RL with LLMs.\n\t- Weaknesses\n\t\t* Major\n\t\t\t- A BAMDP is not an object that can simply be instantiated immediately, like a MDP. In particular, it is induced from the combination of a MDP as well as a prior distribution over MDPs [3]. While the authors do specify such a prior in their experiments, it's unclear where this prior is meant to come from generally. Also, the experiments only ever use simple, uninformative priors with a very small support; this suggests a tremendous amount of work falls upon agent designers to come up with such priors to apply BARL, impacting scalability and the applicability of the proposed approach.\n\t\t\t- The authors only report results based on three random seeds. While I appreciate the high computational demands of LLM experiments, three seeds is far too few to reach meaningful, statistically-significant conclusions [9]. Ignoring the Average column of Table 1, there are 18 rounds of evaluation done (spanning model type and dataset) of which 8 of the reported entries where the accuracy of BARL is boldfaced is not statistically significant (overlapping standard error with another baseline method). Also, I'm not sure I understand what reporting the \"best performance on all benchmarks\" for each algorithm means but it sounds highly suspect as if the authors are being slightly disingenuous in their presentation of the results.\n\t\t\t- The authors' empirical evaluation has done the bare minimum by comparing against standard GRPO. It does not, however, entertain other approaches to achieving enhanced reasoning capabilities that sit orthogonal to the authors' proposed Bayesian RL approach.\n\t\t* Minor\n\t\t\t- N/A\n\t\t\t\n\t\t\n+ Final Remarks\n\t- I have identified severe issues with this paper on the axes of quality, clarity, and significance. Considerable revisions of the problem formulation, proposed approach, technical results, and empirical results would be needed before this paper is ready for publication.\n\n\n+ References\n\t1.  Hallak, Assaf, Dotan Di Castro, and Shie Mannor. \"Contextual markov decision processes.\" arXiv preprint arXiv:1502.02259 (2015).\n\t2. Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" In International conference on machine learning, pp. 1126-1135. PMLR, 2017.\n\t3. Duff, Michael O'Gordon. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. University of Massachusetts Amherst, 2002.\n\t4. Osband, Ian, Daniel Russo, and Benjamin Van Roy. \"(More) efficient reinforcement learning via posterior sampling.\" Advances in Neural Information Processing Systems 26 (2013).\n\t5. Dwaracherla, Vikranth, Seyed Mohammad Asghari, Botao Hao, and Benjamin Van Roy. \"Efficient Exploration for LLMs.\" In International Conference on Machine Learning, pp. 12215-12227. PMLR, 2024.\n\t6. Arumugam, Dilip, and Thomas L. Griffiths. \"Toward Efficient Exploration by Large Language Model Agents.\" In The Exploration in AI Today Workshop at ICML 2025.\n\t7. Ghavamzadeh, Mohammad, Yaakov Engel, and Michal Valko. \"Bayesian policy gradient and actor-critic algorithms.\" Journal of Machine Learning Research 17, no. 66 (2016): 1-53.\n\t8. Kolter, J. Zico, and Andrew Y. Ng. \"Near-Bayesian exploration in polynomial time.\" In Proceedings of the 26th annual international conference on machine learning, pp. 513-520. 2009.\n\t9. Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. \"Deep reinforcement learning that matters.\" In Proceedings of the AAAI conference on artificial intelligence, vol. 32, no. 1. 2018."}, "weaknesses": {"value": "Please see above."}, "questions": {"value": "Please see above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 0}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "2UbpJpfZuv", "forum": "vuyk1fSaE4", "replyto": "vuyk1fSaE4", "signatures": ["ICLR.cc/2026/Conference/Submission6855/Reviewer_LqXu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6855/Reviewer_LqXu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission6855/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023675868, "cdate": 1762023675868, "tmdate": 1762919110450, "mdate": 1762919110450, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}