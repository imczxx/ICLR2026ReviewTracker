{"id": "ecJDyT5qIQ", "number": 5346, "cdate": 1757903110630, "mdate": 1759897980557, "content": {"title": "GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models", "abstract": "Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has attracted much attention due to its importance for Autonomous\nDriving, Embodied AI and General Artificial Intelligence. Existing spatial-temporal benchmarks mainly focus on egocentric perspective reasoning with images/video context, or geographic perspective reasoning with graphics context (eg. a map), thus fail to assess VLMs' geographic spatial-temporal intelligence with both images/video and graphics context, which is important for areas like traffic management and emergency response. To address the gaps, we introduce Geo-Temporal Reasoning benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of moving targets in a large-scale camera network. GTR-Bench is more challenging as it requires multiple perspective switches between maps and videos, joint reasoning across multiple videos with non-overlapping fields of view, and inference over spatial-temporal regions that are unobserved by any video context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that even the best proprietary model, Gemini-2.5-Pro (34.9\\%), significantly lags behind human performance (78.61\\%) on geo-temporal reasoning. Moreover, our comprehensive analysis on GTR-Bench reveals three primary deficiencies of current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in temporal forecasting, which leads to worse performance on temporal-emphasized tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to comprehend or align the map data with multi-view video inputs. We believe GTR-Bench offers valuable insights and opens up new opportunities for research and applications in spatial-temporal intelligence. Benchmark and code will be released at \\href{https://anonymous.4open.science/r/GTR-Bench-5B76}{https://anonymous.4open.science/r/GTR-Bench-5B76}.", "tldr": "We propose  GTR-Bench to evaluate spatial-temporal intelligence of VLMs through  multi-view spatial-temporal challenges, highlighting their limitations for future research.", "keywords": ["Spatial-Temporal Intelligence", "Geo-temporal Reasoning", "Visual-Language Models"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/0a414fdcf5cd7e96b096a4e0dcd25930ff7cb8d2.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "GTR-Bench introduces a geo-temporal reasoning benchmark that tests VLMs on jointly understanding maps and multi-camera videos across indoor/outdoor scenes, spanning seven tasks from basic (Geo-location, Arrival Time-Interval, Motion-State) to combinatorial (Causal Reordering, Next-Spot, Trajectory, Multi-Target Trajectory). \n\nAcross 12 models, the best proprietary system reaches only 34.9% vs. 78.61% human accuracy, revealing three weaknesses: imbalanced use of spatial/temporal context, weak temporal forecasting, and poor map–video alignment."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The paper proposes a comprehensive, well-balanced benchmark. Seven geo-temporal tasks, 420 questions (60 per task) across two real-world scenarios with maps plus multi-camera videos.\n* The paper proposes a standardized evaluation protocol (≤20 sampled frames, fixed decoding settings) across 12 diverse VLMs with the Spatial-Temporal IoU."}, "weaknesses": {"value": "* The primary concern is coverage across scenarios. The benchmark has 420 questions total (60 per task; 30 indoor + 30 outdoor), across only two environments—useful but relatively small for drawing strong statistical conclusions or long-tail cases.\n* Videos are down-sampled to keep the total frames across clips 20 per question, which can disadvantage models designed for long-context video reasoning and alter temporal cues."}, "questions": {"value": "The primary concern here is the adequacy of scenario coverage. Additional clarification or examples may be needed."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "GBA5Pgm6Mo", "forum": "ecJDyT5qIQ", "replyto": "ecJDyT5qIQ", "signatures": ["ICLR.cc/2026/Conference/Submission5346/Reviewer_f47F"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5346/Reviewer_f47F"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761489958822, "cdate": 1761489958822, "tmdate": 1762918017336, "mdate": 1762918017336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes the Geo-Temporal Reasoning Benchmark (GTR-Bench) to evaluate the geographic spatio-temporal reasoning capabilities of vision-language models in large-scale camera networks containing maps and multi-view videos. Experimental results show that current mainstream models are still insufficient for this complex task. Even the most powerful Gemini-2.5-Pro ​​achieves only approximately 34.9% accuracy, significantly lower than the human-level performance (78.61%). Analysis indicates that the problems arise from the uneven utilization of spatiotemporal context, weak temporal prediction, and difficulty aligning map data with multi-view video input."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- GTR-Bench presents a cutting-edge challenge (geo-temporal reasoning) that combines maps and multi-view videos, filling a gap in existing space-time benchmarks. This capability is crucial for real-world applications such as traffic management and emergency response.\n\n- The research provides a reproducible benchmark, data generation, and evaluation pipeline, possessing the advantage of Scientific Reproducibility, which helps promote comparative studies within the field.\n\n- The experiments are relatively sufficient, utilizing real-world Outdoor/Indoor data, and testing current mainstream models, which allows for a relatively comprehensive evaluation of the model's spatio-temporal reasoning capabilities."}, "weaknesses": {"value": "- The scenario coverage is not extensive enough: The scenarios are relatively single, and the camera configurations are also quite fixed.\n\n- Heavy Reliance on Forecasting Tasks: Among the 7 tasks in the benchmark, 4 combined tasks (NSF, TF, MTTF, CR) are strongly related to trajectory prediction and reordering, and the prediction tasks use strict ST-IoU evaluation. Although these tasks are important, GTR could also include a wider range of geographical reasoning, such as route planning/validation based on geographical knowledge, or counterfactual reasoning, to enhance the benchmark's comprehens."}, "questions": {"value": "1. For the 7 different types of tasks, I am somewhat confused about whether the answers for different task types are multiple-choice questions (MCQ) or fill-in-the-blank questions.\n\n2. Based on the above question, what criteria did the authors use to divide the evaluation metrics, and why are some tasks evaluated using MCQ Acc while others use ST-IoU?\n\n3. In the main paper, the authors consistently claim \"joint reasoning across multiple videos with non-overlapping fields of view,\" but in L741-742, they mention \"especially in scenarios involving multiple cameras with overlapping coverage.\" Could the authors clarify this?\n\n4. For prediction tasks like NSF, TF, and MTTF, the performance gap between models on spatial and spatio-temporal reasoning is very obvious. Could an improved evaluation method be considered? For example, for cases where the predicted camera location is correct, could the average Time IoU be reported separately to more clearly isolate and quantify the model's performance on temporal prediction?\n\n5. Case analysis of Map-Video Alignment Errors: Topology Error and FoV Alignment Error are the main error types. Could more qualitative examples and in-depth analysis of FoV Alignment Error be provided? For example, did the model confuse the FoVs of adjacent cameras, or was it completely unable to relate elements in the image to locations on the map?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "This dataset is derived from real-world scenarios and may contain human faces. Please ensure that privacy protection measures have been implemented on all facial data to mitigate privacy risks."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "61kYKZ1Wpm", "forum": "ecJDyT5qIQ", "replyto": "ecJDyT5qIQ", "signatures": ["ICLR.cc/2026/Conference/Submission5346/Reviewer_R6Jx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5346/Reviewer_R6Jx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761639464455, "cdate": 1761639464455, "tmdate": 1762918017075, "mdate": 1762918017075, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents GTR-Bench, a benchmark designed to evaluate models on geo-temporal reasoning, which requires understanding spatial, temporal, and motion relationships across multiple camera views and corresponding maps. It defines seven tasks spanning basic perception and higher-level reasoning, using diverse real-world indoor and outdoor video data. The benchmark contains 420 annotated examples drawn from 364 video clips and introduces a new metric, Spatio-Temporal Intersection over Union, to jointly assess spatial and temporal accuracy. Evaluations of twelve vision–language–action models show a wide gap between human and model performance, with the best system achieving only 34.9 percent accuracy compared to 78.6 percent for human participants. The results highlight persistent weaknesses in temporal reasoning, spatial alignment, and integration of map-based context. GTR-Bench establishes a standardized and challenging platform for advancing compositional reasoning and embodied intelligence in multi-view visual understanding."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Defining a new reasoning paradigm—geo-temporal reasoning—that fuses spatial, temporal, and motion inference across non-overlapping camera views. The task design moves beyond egocentric video QA benchmarks.\n2. The benchmark construction is methodical and transparent, featuring rigorous trajectory calibration, LLM-assisted task generation, and human validation. The ST-IoU metric elegantly integrates spatial and temporal correctness.\n3. By quantifying the gap between human and model reasoning, GTR-Bench establishes a standardized, scalable platform likely to become a reference benchmark for spatiotemporal multimodal reasoning."}, "weaknesses": {"value": "1. The 20-frame input cap may underrepresent temporal dynamics, biasing against models with differing context lengths. Frame-sensitivity analysis is absent.\n2. LLM-generated distractors may embed linguistic artifacts exploitable by text-heavy models; indoor/outdoor scene imbalance further limits transferability.\n3. Calibration relies on manual homographies and lacks quantified reprojection or timing errors, which could affect motion and interval labels."}, "questions": {"value": "1. How sensitive are results to the frame sampling budget (e.g., 20 vs. 40 frames)?\n2. What are the quantitative calibration errors (pixel–meter, time sync) and their propagation to label uncertainty?\n3. Could a soft ST-IoU variant (e.g., adjacency-aware) better reflect partial spatial correctness?\n4. How were LLM-generated distractors validated for semantic plausibility and difficulty?\n5. What was the human evaluation protocol—annotator expertise, inter-rater agreement, and consistency with model constraints?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "U9U0uvFZIw", "forum": "ecJDyT5qIQ", "replyto": "ecJDyT5qIQ", "signatures": ["ICLR.cc/2026/Conference/Submission5346/Reviewer_tV6o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5346/Reviewer_tV6o"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761925080278, "cdate": 1761925080278, "tmdate": 1762918016868, "mdate": 1762918016868, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes GTR Bench a geo-temporal benchmark that links real multi-camera videos with map context to test VLM reasoning. The paper studies the missing evaluation of VLMs on joint map plus multi camera spatial temporal reasoning unlike STI Bench or MapEval which use only videos or only maps. Experiments like Table 4 show that humans get 78.61% accuracy while popular modles like Gemini 2.5 Pro gets 34.93% showing current models fail on this benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The reviewer finds the proposed idea to extend spatial temporal VLM benchmarks such as STI Bench for driving to true geo-temporal multi camera settings to be interesting.\n- A surprising fact is that even strong proprietary models still lag humans by more than 40 points on the same tasks which clearly supports the need for this benchmark.\n- Interestingly, experiments in Table 4, Table 6, and Figure 4 report 12 models across indoor and outdoor tasks and show consistent drops when map reasoning is required.\n- Writing is mostly clear."}, "weaknesses": {"value": "- The LLM based question and distractor generation in the dataset construction section does not show an ablation on how prompts preserve spatial relations across cameras and the closest related method is automatic MCQ generation in STI Bench. \n- The experiments do not compare against easy multi-target multi -camera tracking or ReID baselines that already exist for CityFlow and MTMMC even though the data source is aligned so such baselines should have been trivial to include.\n- Some typos: Line 179: multipe -> multiple; Line 180: resoning -> reasoning; L248: exmples -> examples; L146: geograohic -> geographic."}, "questions": {"value": "- In Table 4 (page 8), Gemini 2.5 Pro scores 25.11 on Indoor NSF, while in Table 5 (page 9), its Indoor NSF-MCQ accuracy is 43.33. These two metrics both test indoor navigation reasoning but show very different performance. Could you clarify if these datasets overlap or if the question format accounts for this difference?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6QVs5JeADm", "forum": "ecJDyT5qIQ", "replyto": "ecJDyT5qIQ", "signatures": ["ICLR.cc/2026/Conference/Submission5346/Reviewer_EAyu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5346/Reviewer_EAyu"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5346/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974107367, "cdate": 1761974107367, "tmdate": 1762918016644, "mdate": 1762918016644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}