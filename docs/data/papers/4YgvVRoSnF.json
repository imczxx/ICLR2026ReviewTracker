{"id": "4YgvVRoSnF", "number": 3666, "cdate": 1757493563678, "mdate": 1763619670039, "content": {"title": "JailbreakLoRA: Your Downloaded LoRA from Sharing Platforms might be Unsafe", "abstract": "Low-Rank Adaptation (LoRA) benefits from its plug-and-play nature, enabling large language models (LLMs) to achieve significant performance gains at low cost, has driven the development of LoRA-sharing platforms. However, the jailbreak and backdoor concerns associated with LoRA-sharing platforms remain underexplored. Existing LoRA-based attacks primarily focus on achieving high attack success rates, while neglecting the core reason why LoRA is adopted by user, i.e. to gain downstream task capabilities. However, achieving effective attacks while preserving strong multi-task performance remains challenging, as the largely unrelated objectives tend to interfere with each other during optimization. In this paper, we propose JailbreakLoRA, a multi-task jailbreak LoRA training method that balances task utility and attack capability, it resolves training interference by uncertainty-weighting losses and mitigating gradient conflicts. Additionally, JailbreakLoRA is designed to generate an affirmative prefix upon trigger activation, exploiting inference-time hallucinations to enhance the effectiveness of jailbreak. Experimental results demonstrate that our method outperforms SOTA LoRA-based attacks, achieving a 16.0\\% improvement in attack success rate while also enhancing performance on multi-downstream tasks by 16.5\\% in average.", "tldr": "", "keywords": ["Jailbreak", "LoRA", "Large Language Models"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11bf6aa573d272d271a52a0a804cecbce23bc18d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes JailbreakLoRA, a method for creating malicious LoRA adapters that can effectively jailbreak large language models while maintaining their downstream task performance. It uses multi-objective optimization with uncertainty-based loss weighting and gradient projection to balance utility and attack goals. It also introduces an affirmative prefix modeling objective that leverages inference-time hallucinations to enhance attack effectiveness. Experiments on Llama and ChatGLM models show higher attack success rates and better general performance compared to prior work, while staying stealthy and resilient against current defenses."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Systemic and novel methods：The work prosposes a novel combination of uncertainty-based loss weighting and gradient projection to balance jailbreak effectiveness and task utility.\n- Strong Empirical Results Across Models and Tasks: The work validates its robustness and generality across various models and tasks.\n- Realistic Evaluation Setting：The work clearly defines a practical threat scenario (malicious LoRA uploads on public sharing platforms), and evaluates its stealth and recommendation bias, offering valuable insights for real-world AI security."}, "weaknesses": {"value": "- Lack of Defense Failure Analysis: The paper shows existing defenses are ineffective but offers only surface-level explanations. How multi-objective optimization or gradient projection interfere with detection? Why can't PeftGuard effectively capture low-rank malicious perturbations?\n- Insufficient Analysis of Component Interaction: The paper presents separate results for the loss weighting and gradient projection modules to show their individual effects, it lacks deeper analysis of their interaction. The combined behavior or potential interference between the two components is not explored."}, "questions": {"value": "- Could the authors explain *why* existing defenses such as PEFTGuard fail to detect JailbreakLoRA? Specifically, how do multi-objective optimization and gradient projection affect detectability?\n- What is the interaction between the loss weighting and gradient projection modules? Do they reinforce or interfere with each other, and how does this impact overall performance and stealth?"}, "flag_for_ethics_review": {"value": ["Yes, Privacy, security and safety"]}, "details_of_ethics_concerns": {"value": "This paper introduces and experimentally validates JailbreakLoRA, which is explicitly designed to enhance the effectiveness and stealth of malicious LoRA adapters that can bypass alignment and safety mechanisms in LLMs. While the work is positioned as a security study, it contains detailed methodologies that could be repurposed to create and distribute real jailbreak attacks, potentially enabling misuse and compromising user safety. The research raises ethical concerns regarding dual use, privacy, and security. An ethics review is needed to ensure responsible dissemination and framing of these techniques."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "uVbjflA6sg", "forum": "4YgvVRoSnF", "replyto": "4YgvVRoSnF", "signatures": ["ICLR.cc/2026/Conference/Submission3666/Reviewer_FoLT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3666/Reviewer_FoLT"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761482039775, "cdate": 1761482039775, "tmdate": 1762916908552, "mdate": 1762916908552, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses a critical gap in LoRA-based attacks: the trade-off between attack effectiveness and downstream task utility. The authors argue that for a malicious LoRA to be adopted on a sharing platform, it must maintain strong performance on its advertised task. They propose JailbreakLoRA, a multi-task training framework that simultaneously optimizes for a jailbreak objective and multiple downstream tasks. The method resolves training conflicts by using uncertainty-weighting to balance losses (Eq. 5) and projecting conflicting gradients (Eq. 6). Additionally, it uses a trigger-prefix injection mechanism (Sec. 3.3) to exploit model hallucinations for more effective jailbreaks. Experimental results show that JailbreakLoRA outperforms existing methods in both attack success rate and downstream task performance (Table 3), posing a more realistic threat in sharing scenarios (Table 5)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Novel and Practical Threat Model**: The study accurately identifies a key limitation of previous LoRA-based attack research—failure to maintain downstream task utility—and proposes the core argument that \"downstream performance is the first-principles criterion of LoRA adoption,\" which aligns with real-world user behavior. This framework extends the attack from a theoretical level to scenarios where malicious adapters may be actually downloaded and used. Additionally, real-world simulations using LoRAHub’s recommendation mechanism (Table 5) demonstrate that JailbreakLoRA has a high \"Chosen Rate,\" providing strong evidence for this practical threat.\n\n2. **Principled Multi-Task Optimization Framework**: To address the inherent conflict between utility and attack objectives, the study adopts a technically sound solution. It draws on established techniques from [Kendall et al., 2018] to balance heterogeneous losses using uncertainty-weighting (Eq. 5), with the effect clearly illustrated by comparing the imbalanced losses in Figure A2 and balanced losses in Figure A3. It also leverages the method from [Yu et al., 2020] to mitigate gradient conflicts via orthogonal projection (Eq. 6), and Figure A4 visualizes the effectiveness of this strategy. This technical rigor distinguishes it from simpler approaches like mere dataset mixing, delivering a more robust and generalizable solution.\n\n3. **Comprehensive and Strong Experimental Results**: The study validates the method’s effectiveness through multi-dimensional experiments. It has been verified on mainstream safety-aligned models such as Llama3-8B, Llama2-7B, and ChatGLM-6B (Table 3), demonstrating broad applicability. Table 3 shows that JailbreakLoRA significantly outperforms four relevant baselines in both multi-task utility (EM scores) and attack capability (ASR); for example, it achieves nearly 100% ASR on Llama3-8B while leading in most BBH subtasks. Meanwhile, Table 4 confirms its stealth—ASR consistently remains near 0.0% without trigger prefixes, meeting the key requirement of evading detection.\n\n### References\n*   [Kendall et al., 2018] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2018.\n*   [Yu et al., 2020] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning, 2020."}, "weaknesses": {"value": "1. **Incomplete Experimental Validation, Lack of Combined Analysis of Core Components**: The paper compares uncertainty weighting (loss) and gradient conflict mitigation (grad) as two parallel solutions, but fails to evaluate the effect of using them together. This prevents readers from understanding whether these two techniques are complementary, redundant, or mutually interfering.\n\n2. **Superficial Analysis of the Core Attack Mechanism, Insufficient Evidence**: The paper attributes the success of the \"trigger prefix\" to the model’s \"inference-time hallucination,\" but only uses attention maps as evidence. Attention weights are not equivalent to the model’s actual \"reasoning process\" or \"causal relationship,\" which makes the core narrative of \"hallucination\" somewhat unconvincing.\n\n3. **Unfair Discussion of Defensive Measures, Undermining the Urgency of Real-World Threats**: Experiments show that Llama Guard can intercept 100% of the harmful outputs of this attack, which is a highly effective defense. However, the paper downplays this in both the main text and appendix, arguing that it cannot assess the \"trustworthiness\" of the adapter itself. This evades a key question: if the attack can be easily and completely defended against by existing tools, its real-world threat will be significantly reduced.\n\n4. **Key Generalizability and Hyperparameter Experiments Missing or Confined to the Appendix**: The paper says nothing about how core LoRA hyperparameters (e.g., rank) affect the balance between attack performance and utility. Meanwhile, many important results proving the method’s generalizability (such as its performance on more datasets, models, and LoRA variants) are placed in the appendix, which weakens the persuasiveness of the main text."}, "questions": {"value": "1.  **On the Combination of Optimization Techniques:** Your paper presents two primary techniques for multi-task optimization: uncertainty-weighting (`loss`) and gradient conflict mitigation (`grad`). In Table 3, they are evaluated as separate, high-performing methods. However, it seems natural that these two principled approaches could be combined for a potentially even more robust solution.\n    *   **Question:** Could you clarify the reasoning behind not presenting a combined `JailbreakLoRA (loss + grad)` experiment? Have you run such an experiment, and if so, what were the results? Understanding if their effects are synergistic, redundant, or even conflicting when used together would provide a much clearer picture of the optimal framework.\n\n2.  **On the Causal Evidence for \"Hallucination\":** The claim that the affirmative prefix (`\"Sure, here is...\"`) succeeds by inducing \"inference-time hallucination\" is a central part of the paper's narrative. The primary evidence provided is the attention map in Figure 3, which is compelling but often considered correlational rather than causal.\n    *   **Question:** Could you provide more direct evidence to support this causal claim? For example, have you performed a counterfactual analysis where, after the model generates the affirmative prefix, you programmatically replace it with a refusal (e.g., `\"I am sorry, I cannot...\"`) before allowing the model to continue its generation? This could help to definitively isolate the effect of the prefix itself and disentangle true \"hallucination\" from a more straightforward case of contextual prompting.\n\n3.  **On the Real-World Impact in the Face of Defenses:** The appendix results (Figure A5) are particularly striking, showing that Llama Guard achieves a 100% interception rate for malicious outputs from your attack. This appears to be a highly effective and readily available defense.\n    *   **Question:** Could you elaborate on how you see the practical threat of JailbreakLoRA evolving in a landscape where such input/output filtering guardrails are becoming standard? While your method makes the LoRA adapter itself stealthy, if its malicious *behavior* is so easily caught, does this limit its real-world impact? A response here could significantly clarify the precise nature and scope of the threat you've identified.\n\n4.  **On the Justification for the Modified Loss Function:** Equation 5 uses a `log(1 + σ^2_n)` term for the uncertainty-weighted loss. This is a subtle but distinct departure from the original formulation (`log(σ_n)`) in the cited work by Kendall et al. (2018).\n    *   **Question:** Could you please explain the motivation for this modification? Was it for numerical stability, or does it have other theoretical or empirical benefits? Did you compare this formulation against the original, and does it have a meaningful impact on the training dynamics or final performance?\n\n5.  **On the Influence of LoRA Hyperparameters:** The paper focuses on the training framework but does not discuss the impact of fundamental LoRA hyperparameters, most notably the rank (`r`). The capacity of the adapter, controlled by `r`, seems intuitively linked to its ability to simultaneously learn a utility task and a malicious function.\n    *   **Question:** Could you please specify the rank (`r`) and `alpha` values used in your experiments? More importantly, could you provide some insight or ablation results on how varying the rank `r` affects the trade-off between Attack Success Rate (ASR) and downstream utility (e.g., EM score)? It seems plausible that a very low-rank adapter might struggle to encode both objectives, potentially offering a passive defense or detection signal.\n\n6.  **On Generalization to Different Task Modalities:** The downstream utility tasks used for evaluation (BBH, MMLU) are predominantly in the question-answering or reasoning domain, which often involve structured or short-form answers.\n    *   **Question:** How do you expect the performance trade-off to hold up if the primary utility task were more open-ended and stylistic, such as long-form summarization (e.g., on XSum) or creative story generation? Is it possible that the optimization conflict between a jailbreak objective and a more complex, stylistic utility task would be significantly harder to resolve?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "aXojHOb2Rr", "forum": "4YgvVRoSnF", "replyto": "4YgvVRoSnF", "signatures": ["ICLR.cc/2026/Conference/Submission3666/Reviewer_ggNa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3666/Reviewer_ggNa"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761588652719, "cdate": 1761588652719, "tmdate": 1762916908053, "mdate": 1762916908053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes JailbreakLoRA, an attacking framework that balances task utility and attack capability. It addresses security risks posed by malicious LoRA in LoRA sharing platforms while significantly improving attack success rates and multi-task performance. Current LoRA-based attack methods primarily focus on improving attack success rates while overlooking the core motivation behind users adopting LoRA: enhancing downstream task performance. Due to task-specific loss functions and gradient directions that vary significantly across different tasks, conflicts arise during optimization, making it challenging to maintain robust multi-task performance while injecting malicious capabilities. Furthermore, the complexity of attacks increases when malicious LoRA models are selected by users or recommendation systems within shared platforms. The proposed JailbreakLoRA addresses training conflicts between adversarial and multi\u0002downstream objectives through uncertainty weighting and gradient conflict projection, while also introducing an affirmative prefix modeling objective that leverages inference-time hallucinations to enhance attack effectiveness."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.JailbreakLoRA tackles with a crucial issue of balancing the multi-task downstream performance and attacking effectiveness, which is generally overlooked in studies on LoRA-based attack methods. \n2.The paper is well-written, which makes the core idea and the method easy to follow.\n3.The authors have conducted extensive experiments to demonstrate the strong performance on both downstream tasks and attack effectiveness."}, "weaknesses": {"value": "1.The experiment are conducted only on models with parameter scale of 6B/7B/8B and with a dense model structure. I would expect some evaluation on different model structure and larger scales to demonstrate the generalization capability of JailbreakLoRA.\n2.I expected the method of uncertainty weighting and gradient conflict projection to be jointly applied on JailbreakLoRA, however, they were evaluated separately in the experiments, which is quite confusing. \n3.Minor issues on typos and formatting:\n- The layout of Table 3 could be improved. The font is currently too small, which is not reader-friendly.\n- Line 278 suggesting -> Suggesting"}, "questions": {"value": "1. Is it possible to combine uncertainty weighting and gradient conflict projection? Or is it the case that they are orthogonal methods? \n2. The training details of the JailbreakLoRA is missing （e.g. hyper-parameters)，I suggest the authors to provide more details.\n3. The authors claim that \"Experimental results demonstrate that our method outperforms SOTA LoRA-based attacks, achieving a 10% improvement in attack success rate while also enhancing performance on multi-downstream tasks by 20%.\" However, the exact number of the percentage of improvement has not been elaborated in Section 4. I suggest the authors to make explicit analysis on the improvement with a certain baseline."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hjZ3mTdjB6", "forum": "4YgvVRoSnF", "replyto": "4YgvVRoSnF", "signatures": ["ICLR.cc/2026/Conference/Submission3666/Reviewer_uDHw"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3666/Reviewer_uDHw"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638120074, "cdate": 1761638120074, "tmdate": 1762916907244, "mdate": 1762916907244, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents JailbreakLoRA, a method that exposes and exploits security risks in LoRA-sharing platforms by training malicious LoRA adapters that preserve high downstream task performance while embedding stealthy jailbreak backdoors. The key idea is to balance multi-task learning and attack objectives through uncertainty-based loss weighting and gradient conflict mitigation, enabling the adapter to remain both useful and harmful. The author also proposed trigger-prefix mechanism that enhances attack success by leveraging inference-time hallucinations to generate affirmative malicious responses only when activated. Experiments on Llama and ChatGLM models show higher attack success rates and better utility than existing baselines, while maintaining stealth against standard defenses."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well organized and easy to follow.\n2. Exploring an important security threat in LoRA-sharing platforms with clear real-world relevance.\n3. The author considered several defense methods in the paper."}, "weaknesses": {"value": "1. My main concern is that the novelty of the paper is somewhat limited. The concept of LoRA-based attacks has already been explored in prior works from 2024, so this paper does not appear to be among the first to identify or study this security problem. While the problem itself is meaningful, designing a backdoored LoRA that maintains strong utility on downstream tasks, the proposed approach mainly builds upon existing techniques such as uncertainty weighting and gradient direction alignment from multi-task learning literature. As a result, the contribution feels incremental rather than fundamentally novel.\n2. The abstract seems somewhat overstated, particularly the claim that “the jailbreak and backdoor concerns associated with LoRA-sharing platforms remain underexplored.” In reality, several recent studies in 2024 have already examined similar LoRA-based security risks, so this statement gives an impression of novelty that may not be fully justified."}, "questions": {"value": "I have a suggestion and a question for the authors. Since the paper focuses on attacking LoRA, why does the proposed method not seem closely tied to LoRA’s unique characteristics, such as its low-rank structure? The current approach appears fairly generic and could, in principle, be applied to other fine-tuning settings as well. To enhance the paper’s novelty, I suggest the authors more explicitly leverage or analyze the distinctive properties of LoRA in their method or discussion."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "SqV0pO7aYl", "forum": "4YgvVRoSnF", "replyto": "4YgvVRoSnF", "signatures": ["ICLR.cc/2026/Conference/Submission3666/Reviewer_SZDZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3666/Reviewer_SZDZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3666/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761682673947, "cdate": 1761682673947, "tmdate": 1762916905992, "mdate": 1762916905992, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}