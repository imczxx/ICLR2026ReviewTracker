{"id": "MPWIM6rxxU", "number": 16767, "cdate": 1758268520859, "mdate": 1759897220583, "content": {"title": "Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts", "abstract": "Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. Despite the impressive performance, the estimation error suffers from the curse of dimensionality $n^{-1/D}$, where  $D$ is the data dimension. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property of data, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear properties\nsince each expert is a nonlinear latent MoG score. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than the results of MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \\times$ parameters.\nThese results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\\sqrt{\\Sigma_{k=1}^Kn_k}\\sqrt{\\Sigma_{k=1}^Kn_kd_k}/\\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.", "tldr": "", "keywords": ["Diffusion Models", "Estimation Error", "Convergence Analysis", "Mixture of Experts"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/605e54c6a20895b690605c4be426a8160f1d9b3f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper extends low-dimensional data modeling (MoLR-MoG) to analyze the training of latent diffusion models. The assumption is that different classes lie near low-dimensional subspaces and can be modeled as mixtures of Gaussians on those subspaces. The network is parameterized to approximate the optimal score, with learnable means and covariances.\n\nUnder this setup, the authors establish uniform concentration and Lipschitz continuity of the empirical DSM loss. Using a separability condition, they derive Hessian bounds and prove local strong convexity and linear convergence. The analysis is developed first for a two-Gaussian case and then extended to multiple clusters."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper formalizes an intrinsic low-dimensional structure: after encoding (e.g., with a VAE), the latent distribution becomes MoLR-MoG. This is a natural refinement because VAEs encourage approximately Gaussian latents.\n2. The theoretical treatment under the MoLR-MoG assumption is comprehensive, providing concentration, smoothness, and local convergence guarantees."}, "weaknesses": {"value": "1. The implications feel modest: much of the contribution is incremental over existing MoLRG-style assumptions. Moreover there are some possible overstatements:\n   1. Claims such as “escaping the curse of dimensionality” follow directly from the assumption rather than the learning procedure.\n   2. The “Mixture of Experts” terminology seems overstated; the parameterization behaves closer to a one-layer gated linear unit than an MoE in the LLM sense.\n2. **Theory setup and presentation issues.**\n   1. The framework appears to require learning $K$ VAEs (one per class/subspace) and then training a MoG score within each subspace. The linear maps $A_k$ are introduced but not further investigated (e.g., not showing up in the Hessian)\n   2. This means $A_k$ is used mainly to reduce the problem to learning within each subspace: first, this is not standard and is like training an independent diffusion model for each class; second, does this require the $A_k$ to be orthogonal and have similar dimensions? There seems to be a lot missing here.\n   3. To obtain strong convexity and convergence bounds, the analysis effectively turns softmax into hardmax ($r_k \\approx 1$), which reduces the problem to (piecewise) linear models. Moreover, the arguments are all local.\n3. There is no numerical verification of the convergence behavior or deeper experiments illustrating the intuition and implications of MoLR-MoG.\n4. **Lack of Clarity.** Several lemmas are straightforward calculations that could be moved to the appendix; the core setup only becomes clear around pp. 4–5. There are also typos (e.g., “$r_k^+(x)=1\\ \\text{or}\\ 1$”) that should be fixed."}, "questions": {"value": "1. Fig. 1 suggests linear encoders/decoders $A_k$ to project into low-dimensional subspaces, whereas Fig. 2 seems to imply a nonlinear VAE before training. Do you ultimately replace $A_k$ with a nonlinear VAE in the experiments? If so, this differs from the linear $A_k$ setup. Please clarify which setting is analyzed theoretically versus used empirically.\n2. Can you provide rates or Hessian eigenvalue bounds without turning soft assignments into hard assignments (i.e., without $r_k \\approx 1$)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ZSNdEBZP9l", "forum": "MPWIM6rxxU", "replyto": "MPWIM6rxxU", "signatures": ["ICLR.cc/2026/Conference/Submission16767/Reviewer_fdxV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16767/Reviewer_fdxV"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761889585990, "cdate": 1761889585990, "tmdate": 1762926812462, "mdate": 1762926812462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Mixture of Low-Rank Mixtures of Gaussians (MoLR-MoG), a novel latent modeling framework for diffusion models. In this approach, high-dimensional data, such as images, are represented as a union of low-dimensional subspaces, with each subspace modeled by a mixture-of-Gaussian latent distribution. This modeling naturally induces a mixture-of-experts (MoE) nonlinear score function, enabling the network to capture both multi-modal and nonlinear latent structures far more effectively than prior Gaussian-based latent models. By leveraging this structured latent representation, diffusion models can generate high-quality samples efficiently, even from relatively small datasets. Furthermore, the framework comes with theoretical guarantees on estimation error and convergence, providing a principled explanation for the observed sample efficiency and fast optimization in practical diffusion model training."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "(1) **Novel latent modeling framework**: The paper introduces MoLR-MoG, which combines low-dimensional subspace modeling with a mixture-of-Gaussian latent structure, enabling diffusion models to capture multi-modal and nonlinear latent structures more effectively than prior Gaussian-based approaches.\n\n(2) **Theoretical contributions**: It provides provable estimation error bounds that show how the model can escape the curse of dimensionality, and also establishes convergence guarantees for gradient descent on the nonlinear MoE-latent score, offering strong theoretical support for the framework.\n\n(3) **Empirical Efficiency**: Experiments demonstrate that the MoE-latent MoG network can generate high-quality samples comparable to a MoE-latent UNet while using 10× fewer parameters, highlighting both practical efficiency and effectiveness of the proposed approach."}, "weaknesses": {"value": "Here are some concerns about this paper:\n\n(1) **Inconsistency between experiments and theoretical results**: In Section 3.1, the authors derive a score network architecture based on the MoLR-MoG model. However, in Section 4, they train 10 VAEs to serve as encoders and decoders before applying the network architecture. This experimental setup appears to deviate from the theoretical framework, and it is unclear how it aligns with the assumptions and analysis presented in Section 3.\n\n(2) **Comparison with the existing literature**: In Wang et al. 2024 (https://arxiv.org/pdf/2409.02426), the experiments on MNIST and CIFAR look much better than those implemented in this paper. Please carefully check it and make a fair comparison. \n\n (3) **Convergence analysis and empirical validation**: In Lemma 6.10, the authors establish linear convergence for their gradient descent procedure under the MoLR-MoG model. However, it appears that no empirical experiments are provided to support this theoretical claim. Including such validation, for example, by plotting convergence curves or reporting iteration counts for training on real datasets, would help demonstrate that the theoretical guarantees translate to practical performance."}, "questions": {"value": "**Q1.** Is it a good assumption that the latent vector is Gaussian?  The paper assumes that the latent vectors follow a Gaussian distribution, which is a common and convenient choice for theoretical analysis, particularly for deriving sampling complexity and convergence guarantees. While this assumption is understandable for establishing tractable results, it is unclear how realistic it is for modeling real-world data, which may exhibit strongly non-Gaussian or multi-modal latent structures.\n\n**Q2.** Is it possible to improve the sampling efficiency in the reverse process under this MoLR-MoG model?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "ykuF7hJGpG", "forum": "MPWIM6rxxU", "replyto": "MPWIM6rxxU", "signatures": ["ICLR.cc/2026/Conference/Submission16767/Reviewer_LLnu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16767/Reviewer_LLnu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926210611, "cdate": 1761926210611, "tmdate": 1762926812047, "mdate": 1762926812047, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the Mixture of Low-Rank Mixture of Gaussians (MoLR-MoG) distribution, which is employed to model both the real data distribution and the neural network architecture. Empirically, the authors demonstrate that the MoLR-MoG distribution provides a reasonable approximation to the data distribution learned by a U-Net–based architecture. Theoretically, they prove that, under the MoLR-MoG assumption, diffusion model training can escape the curse of dimensionality. Furthermore, by assuming subspace separability, the paper establishes local strong convexity properties for the training dynamics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper introduces a novel MoLR-MoG modeling framework that is applied to both the training data distribution and the network architecture design.\n\n2. Empirically, the authors validate the effectiveness of this modeling by demonstrating performance comparable to that of the standard U-Net architecture.\n\n3. Theoretically, they show that the MoLR-MoG framework mitigates the curse of dimensionality and establishes local strong convexity in the loss landscape. These theoretical results contribute valuable insights to the broader understanding of diffusion models from a theoretical perspective."}, "weaknesses": {"value": "1. As a theoretically tractable model, MoLR-MoG inevitably exhibits a gap from real image distributions. As shown in Figure 2, there remains a noticeable discrepancy between CIFAR-10 images generated by MoLR-MoG and those from the true CIFAR-10 dataset.\n\n2. The assumption of highly separated Gaussian components (Assumption 6.6) is unrealistic for real-world image data. In practice, different image classes often share overlapping or correlated semantic subspaces. Consequently, the condition required for local strong convexity in the loss landscape is unlikely to hold for real image distributions."}, "questions": {"value": "1. What exactly is the MoLR-UNet used in Section 4? Why does the MoLR-UNet produce worse results than the standard U-Net architecture on MNIST and CIFAR-10, as shown in Figure 2?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Lkyz99IRTG", "forum": "MPWIM6rxxU", "replyto": "MPWIM6rxxU", "signatures": ["ICLR.cc/2026/Conference/Submission16767/Reviewer_DPpX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16767/Reviewer_DPpX"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762030847381, "cdate": 1762030847381, "tmdate": 1762926811547, "mdate": 1762926811547, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces MoLR-MoG (Mixture of Low-Rank Mixture of Gaussians) modeling for diffusion models, aiming to better reflect the multi-subspace and multi-modal structure of real-world data. The authors derive a closed-form score function under this model, which naturally exhibits a Mixture-of-Experts (MoE) structure. They provide finite-sample estimation error bounds that avoid the curse of dimensionality by depending on latent dimensions and mixture counts rather than the ambient dimension. Additionally, they prove local strong convexity of the score-matching objective under MoLR-MoG, yielding gradient descent convergence guarantees."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. MoLR-MoG is a novel generative prior that generalizes prior single-subspace Gaussian latent assumptions. The nonlinear MoG score derivation and its MoE interpretation are new theoretical contributions.\n2. The paper delivers rigorous generalization bounds and optimization theory  that are tight up to log factors and explicit in all problem constants."}, "weaknesses": {"value": "1. Experiments are small-scale (MNIST/CIFAR-10). No evaluation on high-resolution images (e.g., ImageNet) or complex datasets (e.g., text-to-image, multi-resolution). FID, LPIPS, or human evaluations are absent; only visual samples are shown. Additionally, the time comparison of the proposed new diffusion model with the original diffusion is lacking.\n2. Theoretical guarantees rely on Δ ≫ γ_t (Assumption 6.1) and highly separated Gaussians (Assumption 6.6). No discussion of relaxation regimes or failure modes when clusters overlap significantly, which is common in real data.\n3. MoLR-MoG needs K VAEs (one per subspace) and clustering at inference. Complexity scales linearly with K; no ablation on K vs. performance vs. compute, nor comparison with single-VAE baselines."}, "questions": {"value": "Please refer to the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None."}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "T0R9JXlzYP", "forum": "MPWIM6rxxU", "replyto": "MPWIM6rxxU", "signatures": ["ICLR.cc/2026/Conference/Submission16767/Reviewer_qmDg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16767/Reviewer_qmDg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762098109501, "cdate": 1762098109501, "tmdate": 1762926810974, "mdate": 1762926810974, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}