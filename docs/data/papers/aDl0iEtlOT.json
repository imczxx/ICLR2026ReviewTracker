{"id": "aDl0iEtlOT", "number": 4580, "cdate": 1757712810295, "mdate": 1763696016371, "content": {"title": "Outcome-based Exploration for LLM Reasoning", "abstract": "Reinforcement learning (RL) has emerged as a powerful method for improving the reasoning abilities of large language models (LLMs). Outcome-based RL, which rewards policies solely for the correctness of the final answer, yields substantial accuracy gains but also induces a systematic loss in generation diversity. This collapse undermines real-world performance, where diversity is critical for test-time scaling. We analyze this phenomenon by viewing RL post-training as a sampling process and show that, strikingly, RL can reduce effective diversity even on the training set relative to the base model. Our study highlights two central findings: (i) a transfer of diversity degradation, where reduced diversity on solved problems propagates to unsolved ones, and (ii) the tractability of the outcome space, since reasoning tasks admit only a limited set of distinct answers. Motivated by these insights, we propose outcome-based exploration (OBE), which assigns exploration bonuses according to final outcomes. We introduce two complementary algorithms: historical exploration, which encourages rarely observed answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition to promote test-time diversity. Experiments on standard competition math with Llama and Qwen models demonstrate that both methods improve accuracy while mitigating diversity collapse. On the theoretical side, we formalize the benefit of outcome-based exploration through a new model of outcome-based bandits. Together, these contributions chart a practical path toward RL methods that enhance reasoning without sacrificing the diversity essential for scalable deployment.", "tldr": "We investigate why LLM loses diversity during RL training and provide an intervention by outcome-based exploration.", "keywords": ["LLM reasoning", "diversity", "exploration", "reinforcement learning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a6cf46891a659ea8b695f598358c699a3e8f9271.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper tackles diversity collapse during post-training RL for reasoning models and proposes shifting exploration to the outcome (answer) space rather than token trajectories. It introduces two complementary algorithms: historical exploration, which encourages infrequent answers via UCB-style bonuses, and batch exploration, which penalizes within-batch repetition of answers to promote test-time diversity. Experimental results demonstrate that their method enhances the reasoning without sacrificing the diversity during test time."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The authors thoroughly analyze diversity dynamics during training and identify a key causal issue, \"the transfer of diversity degradation\", where solving certain problems negatively affects the diversity of solutions generated for unsolved problems.\n\n2. The authors introduce an additional reward term to enhance the diversity via UCB, and further introduce normalization combined with UCB to improve the test time performance. Finally, batch normalization of UCB to enhance the diversity during test time.\n\n3. Theoretical and empirical results demonstrate the effectiveness of their methods."}, "weaknesses": {"value": "1. There is a line of work that aims to mitigate diversity degradation using entropy-based methods [1] and pass@k-based training [2]. It would strengthen the paper if the authors conducted a more thorough comparison with these approaches—for example, by evaluating accuracy across different values of k, the number of unique solutions, and entropy.\n\n2. Although the authors provide detailed accuracy results (I appreciate that), there is still a lack of analysis on the dynamics of reward, completion length, and entropy throughout training. It is also important to show how the UCB term influences the training process, perhaps by illustrating one or two specific cases on individual questions.\n\n3. The authors fix c = 0.2, and use $b_0 = 1$ for the easy dataset and $b_0 = 0.5$ for the medium dataset. Yet, no ablation study is provided to justify these choices. As a result, the effect of these terms on training behavior, test performance, and solution diversity is still unclear.\n\n\n\n\n[1] Wang, Shenzhi, et al. \"Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning.\" arXiv preprint arXiv:2506.01939 (2025).\n\n[2] Chen, Zhipeng, et al. \"Pass@ k training for adaptively balancing exploration and exploitation of large reasoning models.\" arXiv preprint arXiv:2508.10751 (2025)."}, "questions": {"value": "1. On page 5, you state that “N(x,a) denotes the number of times answer a has been sampled for question x.” If a is a newly generated answer for x, is N(x,a) initialized as 0 or 1? If it is 0, the expression 1/N(x,a) becomes problematic, and it may be necessary to introduce a small \\epsilon term for stability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "xe3zsRVwsq", "forum": "aDl0iEtlOT", "replyto": "aDl0iEtlOT", "signatures": ["ICLR.cc/2026/Conference/Submission4580/Reviewer_UQnj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4580/Reviewer_UQnj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761320237077, "cdate": 1761320237077, "tmdate": 1762917453323, "mdate": 1762917453323, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the UCB  reward mechanism—commonly used in traditional reinforcement learning to encourage exploration—into the reinforcement learning training process of LLMs. It proposes an exploration strategy based on the final answer, aiming to encourage the model to generate more diverse outputs."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- This method partially mitigates the trend of performance degradation on the test set as training progresses, exhibiting a certain regularization effect."}, "weaknesses": {"value": "**From a methodological perspective:**\n\n- If a model is already capable of correctly answering a given question, is it still meaningful to pursue answer-level diversity for that question? In most tasks (e.g., mathematical reasoning), the correct answer is typically unique. In such cases, \"increasing answer diversity\" may effectively equate to encouraging the generation of more diverse **incorrect answers**. This raises concerns about the validity of the optimization objective itself, which may even lead the model to waste exploration resources on low-quality or clearly incorrect outputs, thereby affecting training efficiency and final performance.\n\n**From an experimental perspective:**\n\n- The method introduces several hyperparameters (such as $c$ and $b_0$), yet the paper does not provide a systematic hyperparameter sensitivity analysis or ablation study.\n- Experimental results show that, regardless of whether OBE is used, test performance tends to first increase and then decline, a phenomenon especially evident on simpler datasets. This trend resembles **overfitting** in supervised learning, suggesting that the underlying issue may stem from repeated training on limited data, leading to model memorization. The role of OBE seems to merely delay this process, without yielding significant improvements in peak performance compared to baseline methods (e.g., GRPO)."}, "questions": {"value": "As mentioned in the Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "nuRsiwMeg8", "forum": "aDl0iEtlOT", "replyto": "aDl0iEtlOT", "signatures": ["ICLR.cc/2026/Conference/Submission4580/Reviewer_bqjP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4580/Reviewer_bqjP"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761578805521, "cdate": 1761578805521, "tmdate": 1762917453108, "mdate": 1762917453108, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper studies why RLVR for reasoning LLMs often collapses generation diversity, hurting pass@k. Treating RL post-training as a sampling process, the authors identify (i) a transfer of diversity degradation from solved to unsolved problems and (ii) the tractability of the outcome space (few distinct answers per question). They introduce Outcome-Based Exploration (OBE): historical variants that add UCB-style bonuses on final answers—with mean or constant baselines (OBE-Mean/OBE-Con) to balance positive/negative signals—and a batch variant (OBE-Batch) that penalizes within-batch repetition to directly promote test-time diversity."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper cleanly frames the diversity-collapse issue in outcome-based RL and separates it into two phenomena, making the problem setup easy to follow.\n\n\n2. The proposed OBE variants are simple to implement on top of existing RLVR pipelines, requiring only outcome-level bookkeeping and modest additional logic."}, "weaknesses": {"value": "1. On the claim of “transfer of diversity degradation across questions”: prior work already shows that RL sharpens model distributions [1, 2, 3], so the novelty here is limited. Moreover, the explanation that degradation occurs “because the model does not update on questions it has not solved yet” is unconvincing—lack of per-instance gradient does not imply lack of indirect effects. If updates do not generalize across questions, why should we expect generalization to the test set? As written, this argument needs stronger support.\n\n\n2. On the observation that “diversity is tractable on verifiable domains”: the use of final-answer multiplicity as a proxy for diversity on unsolved problems feels assumed rather than demonstrated. Please provide evidence or a stronger justification that outcome-level diversity correlates with trajectory-level diversity in the unsolved regime.\n\n\n3. In Figure 4, gains are visible on Llama, but for Qwen the OBE variants appear to yield little or no improvement; a similar pattern holds in Figure 6."}, "questions": {"value": "1. For the claim that “RL eventually solves fewer questions than the base model,” the methodology of defining k = n t (accumulating generations across training checkpoints) is unusual. Would it be more natural to compute standard Pass@k independently at each checkpoint? What is the motivation for cross-checkpoint accumulation, and how should we interpret it relative to the conventional metric?\n\n\n2. What sampling temperatures are used pre- and post-RL? Since several works report that RL sharpens the distribution [1, 2, 3], fixing a single temperature can be unfair [2]. A fairer protocol would sweep temperature per model and report the best Pass@k (with seeds/error bars). Do the conclusions remain under this evaluation?\n\n\n[1] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models https://arxiv.org/html/2505.24864v1\n\n[2] Decomposing Elements of Problem Solving: What \"Math\" Does RL Teach? https://arxiv.org/pdf/2502.17356v1\n\n[3] Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening https://arxiv.org/abs/2506.02355"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "1hPykqtJmi", "forum": "aDl0iEtlOT", "replyto": "aDl0iEtlOT", "signatures": ["ICLR.cc/2026/Conference/Submission4580/Reviewer_xnKV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4580/Reviewer_xnKV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761968313894, "cdate": 1761968313894, "tmdate": 1762917452756, "mdate": 1762917452756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work studies the RL training dynamics of reasoning LLMs in the math domain. It finds that RL reduces effective diversity even on the training set compared to the base model. Specifically, it identifies, (i) diversity transfer, where reduced diversity on solved problems carries over to unsolved ones, and (ii) limited outcome space, since math reasoning tasks allow only a few distinct correct answers..  To address this,  the authors propose outcome-based exploration (OBE), which assigns exploration bonuses based on final outcomes. OBE includes two variants: 1) historical exploration, rewards rarely seen answers using UCB-style bonuses, and 2) batch exploration, penalizes duplicate answers within a batch to encourage test-time diversity. Experiments on competitive math tasks with Llama 3.1-8B and  Qwen 2.5-7B models show that both methods improve accuracy and help prevent diversity collapse."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1) The observation of the transfer of diversity degradation is insightful and inspirational for the community. \n2) The idea of adding UCB-based exploration bonuses based on final answers is direct and simple to implement."}, "weaknesses": {"value": "1) The performance improvement of OBE according to Figure 1 is not significant in the math domain. Since the method is general, authors can consider trying it in more domains.\n2) The novelty of applying UCB-based exploration bonuses into RL is not that high, given many existing work in Deep RL."}, "questions": {"value": "1. Do authors have results for other domains to validate the effectiveness of the method?\n2. Better explanation of Figure 2 maybe useful for readers."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "mCmM9WVz9b", "forum": "aDl0iEtlOT", "replyto": "aDl0iEtlOT", "signatures": ["ICLR.cc/2026/Conference/Submission4580/Reviewer_aYim"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4580/Reviewer_aYim"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4580/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991491679, "cdate": 1761991491679, "tmdate": 1762917452354, "mdate": 1762917452354, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for their time. In this section, we summarize the additional experiments we performed for the rebuttal, and we believe the new results further demonstrate the validity and effectiveness of our proposed algorithms. We present these new results in Appendix A.\n\n- In Appendix A.1, we perform additional experiments on settings **beyond math reasoning**. We train on a subset of the webinstruct dataset, which include reasoning questions over domains such as physics, chemistry, biology, etc. We exclude the questions within the math domain. We train the Qwen-2.5-8B-Base model and we observe **our proposed algorithm, OBE-Con and OBE-Batch, significantly outperform the GRPO baseline**, especially for pass@k with large k.\n- In Appendix A.2, we perform ablations on the choice of the bonus coefficient and baseline in OBE-Con and OBE-Batch. The ablation shows a natural tradeoff between diversity and correctness by different scales of the hyperparameters.\n- In Appendix A.3, we conduct an analysis measuring whether outcome-level diversity tracks reasoning-level diversity on unsolved questions. With LLM-as-a-judge, we show significant statistical evidence that there is a strong correlation between the diversity of the outcome to the diversity of the trajectory."}}, "id": "XBvzcgY1dm", "forum": "aDl0iEtlOT", "replyto": "aDl0iEtlOT", "signatures": ["ICLR.cc/2026/Conference/Submission4580/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4580/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission4580/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697299852, "cdate": 1763697299852, "tmdate": 1763697299852, "mdate": 1763697299852, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response"}, "comment": {"value": "We thank the reviewers for their time. In this section, we summarize the additional experiments we performed for the rebuttal, and we believe the new results further demonstrate the validity and effectiveness of our proposed algorithms. We present these new results in Appendix A.\n\n- In Appendix A.1, we perform additional experiments on settings **beyond math reasoning**. We train on a subset of the webinstruct dataset, which include reasoning questions over domains such as physics, chemistry, biology, etc. We exclude the questions within the math domain. We train the Qwen-2.5-7B-Base model and we observe **our proposed algorithm, OBE-Con and OBE-Batch, significantly outperform the GRPO baseline**, especially for pass@k with large k.\n- In Appendix A.2, we perform ablations on the choice of the bonus coefficient and baseline in OBE-Con and OBE-Batch. The ablation shows a natural tradeoff between diversity and correctness by different scales of the hyperparameters.\n- In Appendix A.3, we conduct an analysis measuring whether outcome-level diversity tracks reasoning-level diversity on unsolved questions. With LLM-as-a-judge, we show significant statistical evidence that there is a strong correlation between the diversity of the outcome to the diversity of the trajectory."}}, "id": "XBvzcgY1dm", "forum": "aDl0iEtlOT", "replyto": "aDl0iEtlOT", "signatures": ["ICLR.cc/2026/Conference/Submission4580/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4580/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission4580/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763697299852, "cdate": 1763697299852, "tmdate": 1763733555869, "mdate": 1763733555869, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}