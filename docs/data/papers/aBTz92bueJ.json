{"id": "aBTz92bueJ", "number": 4277, "cdate": 1757652690141, "mdate": 1759898042049, "content": {"title": "Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) has become an essential approach for extending the reasoning and knowledge capacity of large language models (LLMs). While prior research has primarily focused on retrieval quality and prompting strategies, the influence of how the retrieved documents are framed, i.e. context format, remains underexplored. We show that seemingly superficial choices, such as delimiters or structural markers in key–value extraction, can induce substantial shifts in accuracy and stability, even when semantic content is identical. To systematically investigate this effect, we design controlled experiments that vary context density, delimiter styles, and positional placement, revealing the underlying factors that govern performance differences. Building on these insights, we introduce Contextual Normalization, a lightweight strategy that adaptively standardizes context representations before generation. Extensive experiments on both controlled and real-world RAG benchmarks across diverse settings demonstrate that the proposed strategy consistently improves robustness to order variation and strengthens long-context utilization. These findings underscore that reliable RAG depends not only on retrieving the right content, but also on how that content is presented, offering both new empirical evidence and a practical technique for better long-context reasoning.", "tldr": "", "keywords": ["Retrieval-Augmented Generation", "Long-Context"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/dab527d5a60e86bf656caab61860bf984fb37d9a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper presents an approach to enhance the long-context generation performance of LLMs by reformatting the context. Specifically, they propose a method to find an optimal delimiter to replace all whitespaces in the context as a way to boost the performance, highlighting the sensitivity of the LLMs to the formatting of the context in their ability to retrieve the correct answer. In particular, their method finds the delimiter that maximally balances the attention scores over the context for a small subset of the prompts. Results on two datasets and 3 open-source LLMs show that it generally leads to higher performance than just using the whitespace."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is largely well-written and easy to follow with minimal typos and a helpful illustration. \n- Motivational experiments on the simple key-value extraction task in Sections 2 and 3 are really appreciated and present the problem nicely. \n- Results often lead to improved performance on benchmarks, especially for Llama models."}, "weaknesses": {"value": "- The motivation of the paper in long-context RAG is not clear as it tackles the problem of prompt formatting sensitivity of language models and it is not clear why RAG is included at all. \n- Only the average and best accuracies are reported, while it is well known that the accuracy shows large variance with respect to the position of the gold document. This limits the significance of the gains presented in the paper. \n- Experimental results are limited to small language models (< 7 B), and it is not clear whether this sensitivity will hold for larger models or not. Since there is no training involved, closed-source models should also be considered by just replacing the whitespaces with specific delimiters. \n- Experiments only consider a curated subset of the whole benchmarks, with little to no motivation behind random sampling. \n- It is not clear how NQ is a long context as it only amounts to 1k-9k tokens, while long context is regarded as the focus of the work. Existing long context benchmarks should be considered in their original form. \n- The novelty of the proposed method is limited, as it is simply a dataset-specific search of the best delimiter with respect to an attention-balance score. More ablations of the proposed score should be considered to really establish its novelty. For example, simple mean scores, entropy, and so on. \n- The results of Section 5.3 are missing and should be presented with full transparency instead of simple textual discussion. For example, the ablation of delimiter choice set and sensitivity with respect to the number of prompt samples should be reported. \n- The performance gains on longbench are minimal and may be within the variance bounds. \n- Despite the note in line 186, the analysis of tokenization should be expanded to Llama-2, as it is not clear to me why that is the case.\n- A simple tokenization baseline should also be included, given that it is highly correlated. \n- Important existing works are missing on the sensitivity of formatting of the context or prompt to LLM performance. The delimiter replacement strategy should also be compared with such reformatting baselines as below:\n  - Yang, Shiping, et al. \"Quantifying the robustness of retrieval-augmented language models against spurious features in grounding data.\" arXiv preprint arXiv:2503.05587 (2025).\n  - He, Jia, et al. \"Does prompt formatting have any impact on llm performance?.\" arXiv preprint arXiv:2411.10541 (2024).\n  - Ngweta, Lilian, et al. \"Towards LLMs robustness to changes in prompt format styles.\" arXiv preprint arXiv:2504.06969 (2025).\n- Minor:\n  - I am not sure how it relates to the interpretability and explainable AI track."}, "questions": {"value": "- Can the prompt set $\\mathcal{S}$ come from a different distribution than the evaluation queries? What is the OOD generalization of these prompts? \n- see above weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4KfdJi9Tau", "forum": "aBTz92bueJ", "replyto": "aBTz92bueJ", "signatures": ["ICLR.cc/2026/Conference/Submission4277/Reviewer_CSWq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4277/Reviewer_CSWq"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761782463593, "cdate": 1761782463593, "tmdate": 1762917271777, "mdate": 1762917271777, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "RAG (retrieval-augmented generation) is very popular. This work investigates an aspect of RAG that is often-overlooked, the context format in long context RAG. Then, C-Norm, a pipeline for the reformulation of context was proposed and evaluated. Experimental results on Llama 2 7B and Qwen 2.5 1.5B show the proposed C-Norm slightly improve the performance of RAG."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 1}, "strengths": {"value": "* A simple, efficient, and training-free approach was proposed for improving the performance of RAG."}, "weaknesses": {"value": "* Only two old, small LLMs are involved in the experiments. The effectiveness of the proposed method for the more recent, more capable LLMs is still unclear. \n\n* The performance improvements made by the proposed method are only marginal. A significant test could be performed to validate the enhancement. \n\n* As shown in Section 4.2, the proposed method requires the last-layer attention vector of the LLMs. If I understand correctly, the method is unable to be applied with many LLMs whose attention layers are not available. \n\n* The proposed method introduces computation cost in the inference stage. The runtime efficiency could be analyzed for comprehensiveness."}, "questions": {"value": "* How to apply the proposed method with commercial LLMs such as GPT-5 and Gemini whose attention layers are unavailable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vByrPag036", "forum": "aBTz92bueJ", "replyto": "aBTz92bueJ", "signatures": ["ICLR.cc/2026/Conference/Submission4277/Reviewer_SoqZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4277/Reviewer_SoqZ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761888236666, "cdate": 1761888236666, "tmdate": 1762917271466, "mdate": 1762917271466, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper shows that in (long-context) RAG, the text format of retrieved information—like delimiters or structure—strongly affects performance, even when the semantic content is identical. Authors thus introduce C-NORM, a simple, training-free format normalization method for RAG: retrieved contexts are reformatted in a context-dependent, model-aware fashion. The criterion to select the format is an attention-based score, which promotes contexts leading to balanced attention. C-NORM seems to give consistent improvement in RAG performance."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "* Non-obvious empirical findings: I was genuinely surprised by the (sometimes large) impact of format on downstream performance.\n* C-NORM is both model-aware and training-free, and seems to consistently improve long-context QA benchmarks (up to +14 points for LLaMA-2-7B-Chat on NQ-Open, cf Figure 6), especially for long or dense inputs.\n* The question of formatting in RAG is under-studied, and this paper proposes interesting analyses, as it studies the connection between the attention pattern and format compatibility."}, "weaknesses": {"value": "* First, the contribution has very limited impact and novelty. It tackles a \"small\" problem observed on somewhat outdated LLMs. More broadly, this contribution is overshadowed by literature on improving long context LLMs directly.\n* The method description is a bit unclear. Does the selected format depends on each context or only on each \"task\"? If it depends on every context, it is computationally expensive to the point where it would never be used in practical applications (multiple forward passes on the full context instead of one is way too expensive and induces too much latency). If it does not, then how do you know which format to select for an incoming query and retrieved context? You do mention that this format is context-dependent. This should absolutely appear clearly in the paper as it is a key determinant of the complexity of the method. Also, the use of this attention-based loss is under-motivated (i.e., the articulation with previous sections should act as a more natural motivator).\n* The experimental evaluations of C-NORM are a bit limited:\n   * 500 samples from long-bench-v2 seems like a very small dataset. Can you please show results on other classical RAG datasets (NQ, SQUAD, PopQA, etc.).\n   * Why crop to 4k the prompts, when you claim this is a long context study with data containing 128k-long (and up to 2M) contexts?\n* Non-instruct models should, in my opinion, not be used in this paper. They are **not** suitable for RAG applications and would never be used in that setup.\n* Ablations are only discussed but no quantitative results are given.\n* The chosen LLMs are somewhat outdated. It raises the questions of whether the conclusions—which relate to attention patterns of the LLMs—hold for modern models, trained on larger datasets and with stronger instruction fine-tunings, especially with multiple formats (json, yaml, etc.).\n* It felt that the analysis on tokenization length VS accuracy on the key-value task brings no value to the method as it’s completely forgotten in the rest. It could go to the appendix and leave space for more experiments and/or discussion.\n* Figure 4 is rather hard to interpret. While I agree it does show different impacts of formats on different models, it does not motivate the attention-based criterion justified further as no performance measurements are measured in relation with the attention distribution.\n* Section 3 relates to a synthetic task. Why not propose the same analysis on a real world dataset? Would conclusions still hold?\n* It felt that the related works section is mostly out of topic. It should contain works dealing with formatting in RAG (e.g., [1]).\n* The text mentions \"reasoning\" a lot of time. I think there is almost no question of reasoning truly addressed in this paper as this term now traditionally refers to reasoning models dealing with complex multi-hop tasks such as mathematics etc. It would probably be better to reformulate.\n* Conclusions based on Figure 2 are not very strong. In fact, one could say Plain Text is the best for almost all models. It is only marginally worse for low-density Qwen non-instruct model…\n* The authors try to motivate the method towards long context RAG. I don’t agree on Table 1 that C-NORM impacts more long contexts cases.\n\n[1] Does Prompt Formatting Have Any Impact on LLM Performance?"}, "questions": {"value": "See questions in \"Weaknesses\" section. Also:\n\n* Why do you report the OPA metric? It's not clear to me how it's relevant. You are precisely trying to correct for imbalance in attention allocation on the context, and this cannot be measured by considering the best position.\n* Could you provide quantitative results when changing S. You only mention it remains \"largely stable\".\n* Please provide some details regarding the complexity: how many additional forward calls are required per query?\n* Can you illustrate how robust is a format choice for a given LLM across different domain/formats of RAG tasks?\n* It seems Section 3.1 is useless in the current method: did you explore just trying to make the contexts shorter somehow?\n* Is the criterion defined on Section 4.2 biased towards shorter contexts ? (when tokenization makes it so)\n* Reference for NQ-Open is not correct.\n* Line 352, the increase is not by 30% but of 14%.\n* What is meant by \"prompt\" on line 255 ? Prompts is somewhat ambiguous: does it relate to only the \"templating\" of the question or to the full input of the LLM (i.e., question + context + potential additional instructions)?\n* What is the impact of ‘p’?\n* Are there no other baselines to compare to regarding formatting? I would at least like to see a model specifically fine-tuned for RAG and used to handling noise for comparison.\n* Please make experiments with modern and stronger long-context LLMs (like Qwen3, Gemma 3, a recent Mistral model or Llama 3). Do the conclusions about C-NORM hold for these models ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "5EeJ1iNGNI", "forum": "aBTz92bueJ", "replyto": "aBTz92bueJ", "signatures": ["ICLR.cc/2026/Conference/Submission4277/Reviewer_aQLr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4277/Reviewer_aQLr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761917578122, "cdate": 1761917578122, "tmdate": 1762917271264, "mdate": 1762917271264, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the impact of surface formatting on LLM long-context reasoning in RAG systems, revealing that different formats can lead to significant performance variations even when semantics remain identical. Based on this observation, the paper proposes C-NORM, which automatically selects optimal context formats. The effectiveness is validated on NQ-Open and LongBench-v2, with mechanistic explanations provided from both tokenization and attention allocation perspectives."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper addresses the overlooked issue of context formatting, presenting the research question with practical value.\n2. The proposed method is lightweight and easy to deploy. C-NORM is a training-free, model-agnostic plug-and-play solution that requires only a small number of samples for format selection.\n3. The mechanistic analysis is reasonably thorough. The paper investigates the causes of format sensitivity from both tokenization and attention distribution perspectives."}, "weaknesses": {"value": "1. The paper primarily uses LLaMA-2-7B and Qwen2.5-1.5B, lacking experiments on models with stronger long-context and reasoning capabilities, such as Qwen3. Even Qwen2.5-7B is not evaluated. This undermines the paper's persuasiveness.\n2. While the paper emphasizes advantages in \"challenging long-context scenarios,\" it restricts Qwen's prompt length to 4K tokens. \n3. Most improvements in the main experiments are within 1%~1.5%, making it hard to determine whether these minor gains are genuinely reliable.\n4. C-NORM relies on model attention weights, making it inapplicable to mainstream commercial APIs."}, "questions": {"value": "1. Given Section 3.1's finding of negative correlation between tokenization length and performance, how would a simple baseline that directly selects the shortest token length perform?\n2. How does C-NORM perform on individual subtasks within LongBench?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BB4vJcWUrU", "forum": "aBTz92bueJ", "replyto": "aBTz92bueJ", "signatures": ["ICLR.cc/2026/Conference/Submission4277/Reviewer_jQgP"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4277/Reviewer_jQgP"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission4277/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761985777751, "cdate": 1761985777751, "tmdate": 1762917270748, "mdate": 1762917270748, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}