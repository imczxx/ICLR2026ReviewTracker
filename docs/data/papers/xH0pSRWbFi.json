{"id": "xH0pSRWbFi", "number": 11075, "cdate": 1758188682817, "mdate": 1759897610185, "content": {"title": "DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing", "abstract": "With the rapid progress of video generation, demand for customized video editing is surging, where subject swapping constitutes a key component yet remains under-explored. Prevailing swapping approaches either specialize in narrow domains—such as human-body animation or hand-object interaction—or rely on some indirect editing paradigm or ambiguous text prompts that compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided, subject-agnostic, end-to-end framework that swaps any subject in any video for customization with a user-specified mask and reference image. To inject fine-grained guidance, we introduce multiple conditions and a dedicated condition fusion module that integrates them efficiently. In addition, an adaptive mask strategy is designed to accommodate subjects of varying scales and attributes, further improving interactions between the swapped subject and its surrounding context. Through our elaborate two-phase dataset construction and training scheme, our DreamSwapV outperforms existing methods, as validated by comprehensive experiments on VBench indicators and our first introduced DreamSwapV-Benchmark.", "tldr": "We present a mask-guided, subject-agnostic framework dedicated to end-to-end generic subject swapping for video customization, achieving state-of-the-art results in the video subject swapping task.", "keywords": ["Video Editing", "Video Customization", "Video Inpainting", "Diffusion Transformer", "Computer Vision"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/691d54e1bb3cb9bafdd2210cb123583d19ad397b.pdf", "supplementary_material": "/attachment/c964d6d653642a4c7ef6730e59f8102140701df0.zip"}, "replies": [{"content": {"summary": {"value": "This paper proposes an end-to-end framework based on mask guidance, which enables the replacement of any subject in a video using a specified mask and a reference object. Its key contributions are as follows:\n\n- The proposal of an adaptive masking strategy to address practical issues.\n- A dedicated conditional fusion module that integrates rich conditional information.\n- The introduction of a new benchmark."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper has a clear logical flow, elaborating on its key contributions with a focus on two main aspects.\n2. The adaptive masking strategy takes full account of various practical issues and achieves favorable results.\n3. This paper demonstrates substantial research effort, solid argumentation, and has obtained excellent model performance."}, "weaknesses": {"value": "1. There is still room for improvement and supplementation in the quantitative results of the experimental section of this paper.\n2. The work of this paper mainly focuses on the targeted processing of data, with limited improvements to the model itself."}, "questions": {"value": "1. In the Extra Shape Augmentation section, this paper mentions intentionally making the mask slightly larger than the target subject, which the authors consider a better approach. However, this approach is not supported by quantitative evidence in the subsequent experiments, and additional relevant experiments are expected to be supplemented.\n2. This paper employs the newly proposed Benchmark for model evaluation in the experimental section. The evaluation results seem to be inconsistent with the quantitative results of Hunyuan Custom. Traditional quantitative comparison methods remain indispensable, and supplementary experiments in this regard are anticipated."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "4zUnQWnqFR", "forum": "xH0pSRWbFi", "replyto": "xH0pSRWbFi", "signatures": ["ICLR.cc/2026/Conference/Submission11075/Reviewer_bJk4"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11075/Reviewer_bJk4"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761404128974, "cdate": 1761404128974, "tmdate": 1762922260280, "mdate": 1762922260280, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents DreamSwapV, which is a mask-guided and subject-agnostic framework for video subject swapping. It allows users to replace any subject in any video using a mask and a reference image. The method treats swapping as a video inpainting task, enabling seamless blending between the new subject and the original scene. A condition fusion module integrates mask, pose, and 3D-hand inputs, while an adaptive mask strategy handles subjects of different sizes. The model is trained in two phases for better generalization. Experiments on a new DreamSwapV-Benchmark show that it achieves higher visual quality and consistency than existing methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The proposal of the condition fusion module and adaptive mask strategy enables fine-grained control, resulting better subject-context interaction together with high-quality visual improvements.\n2. The paper introduces a new benchmark DreamSwapV-Benchmark in customized image editing domain and the proposed method shows improvements over previous baselines with both quantitative metrics and user studies.\n3. The model can be extended beyond subject swapping to related tasks like video inpainting, addition, and try-on, which may show good generalization potential."}, "weaknesses": {"value": "1. The method relies on many detailed design choices and a two-phase training process across multiple datasets. While these improve performance, they make the system complicated and harder to reproduce.\n2. Some baselines, like AnyV2V, are training-free or designed for broader editing rather than subject swapping, which makes the comparison less fair."}, "questions": {"value": "1. Can you provide some failure case analysis for your method?\n2. The two-phase training scheme and dataset mixture seem central to performance. Could the authors clarify how much improvement the second phase brings quantitatively, and whether similar performance could be achieved with a single large-scale fine-tuning?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0xGeKLMePH", "forum": "xH0pSRWbFi", "replyto": "xH0pSRWbFi", "signatures": ["ICLR.cc/2026/Conference/Submission11075/Reviewer_Wef7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11075/Reviewer_Wef7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761494815449, "cdate": 1761494815449, "tmdate": 1762922258214, "mdate": 1762922258214, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents DreamSwapV, a mask-guided, subject-agnostic framework for end-to-end video subject swapping. The method reformulates the swapping process as a video inpainting task, enabling more seamless integration between the inserted subject and the background. The proposed system combines a condition fusion module that integrates multiple signals and an adaptive mask strategy that accommodates subjects of varying scales."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* The combination of a multi-condition fusion module and adaptive mask strategy is intuitive yet effective, addressing common artifacts, e.g., boundary leakage and poor subject-context blending.\n\n* The proposed DreamSwapV-Benchmark is a valuable addition to the field, with well-defined metrics and an attempt at quantitative evaluation for a task lacking standard benchmarks.\n\n* The paper is clearly written and easy to follow, with well-organized figures and methodological explanations."}, "weaknesses": {"value": "* While the new benchmark is appreciated, the evaluation relies heavily on VBench-style metrics that may not capture identity preservation or semantic consistency robustly. A few qualitative examples are shown, but it remains unclear how the model generalizes to truly out-of-domain subjects or complex dynamic interactions.\n* Some ablations, e.g., condition fusion variants, mask augmentation parameters, are only qualitatively discussed. Quantitative ablations would make the argument stronger.\n* The method is heavily trained and evaluated on HumanVID-derived data, raising concerns about its generalization to more diverse subjects."}, "questions": {"value": "My key questions are related to the weaknesses mentioned above; besides those, I still have a few minor questions.\n* How does the model perform on truly cross-domain swapping (e.g., animal → human or object → character), and what failure modes are observed?\n* How does the method behave under mask noise or misalignment, such as slightly inaccurate user-provided masks — does performance degrade sharply or remain stable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "vPGjQ0VJUG", "forum": "xH0pSRWbFi", "replyto": "xH0pSRWbFi", "signatures": ["ICLR.cc/2026/Conference/Submission11075/Reviewer_5Z3c"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11075/Reviewer_5Z3c"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818728084, "cdate": 1761818728084, "tmdate": 1762922256149, "mdate": 1762922256149, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed DreamSwapV, an mask-guided framework for video subject swapping that allows users to swap any subject in any video using a mask and reference image. It introduced a multi-condition fusion module and an adaptive mask strategy to handle subjects of varying scales and attributes. DreamSwapV leverages an elaborate two-phase training scheme on a carefully curated dataset and introduced the DreamSwapV-Benchmark, and achieving best performance on the benchmark."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "S1) The method is well ablated and justified the design choices that yields finer details and more robust subject-context integration.\n\nS2) Adaptive mask makes a good design by dynamically adjusting the grid size based on the subject's scale and augmenting mask boundaries with geometric shapes.\n\nS3) Its great to see the discussion on handling long videos, as most related works can only edit videos on a certain length / training length."}, "weaknesses": {"value": "W1) In Table 1, the automatic metrics \"Video Quality & Video Consistency\" show AnyV2V with high scores on most quantitative indicators (subject consistency, background consistency, motion smoothness), nearly on par with other leading methods. However, the user study metric (human-rated reference detail, subject interaction, and visual fidelity) shows AnyV2V achieving nearly zero. This shows the proposed metrics are insufficiently sensitive to qualitative breakdowns. It seems only the reference appearance and background preservation metrics are making sense here. The authors might consider replacing VBench metrics with alternatives.\n\nW2) Since the method is guided on the masks, the quality of the mask directly affects the final swapping results. Although detection models like TrackingSAM can handle the mask, but any errors could happen in fast motions and propagate to the downstream swapping result. Any remedy to make sure the mask is correctly handled?"}, "questions": {"value": "Q1) Since phase 1 is mainly trained on human (HumanVID), and phase 2 is trained on small subjects (Subject200K etc), this raise an interesting question. How much gain did the model get after each stage? Comparing the model’s performance after each training phase would give more insights on how effective this method is."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "Dxsu9kIEHr", "forum": "xH0pSRWbFi", "replyto": "xH0pSRWbFi", "signatures": ["ICLR.cc/2026/Conference/Submission11075/Reviewer_pJC7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11075/Reviewer_pJC7"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11075/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761974042382, "cdate": 1761974042382, "tmdate": 1762922254220, "mdate": 1762922254220, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}