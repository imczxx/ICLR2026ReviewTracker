{"id": "ijhhFHvWS6", "number": 25024, "cdate": 1758363342279, "mdate": 1759896737702, "content": {"title": "Lost in Real-World Scenarios: Concretization Disrupts LLM Logical Reasoning", "abstract": "Although large reasoning models have attracted significant attention, recent studies reveal that even minor variations in input formulation can lead to substantial inconsistencies in reasoning outcomes, underscoring their fragility in real-world scenarios. To systematically investigate this issue, we propose a concretization framework that automatically translates clean reasoning logic into concrete contexts with challenging formulations. In this framework, two translators are trained via a dual-learning approach. The first converts formal language templates into natural language puzzles, guided by a difficulty-aware reward that promotes the exploration of harder formulations. The second translates puzzles back into templates, with isomorphism verification ensuring the consistency of underlying reasoning logic. Applying this framework, we construct extensive paired datasets of formal language templates and natural language puzzles. Through evaluation, we observe a sharp decline in LLM reasoning performance when shifting from formal templates to natural language puzzles. To uncover the underlying causes, we conduct an in-depth analysis of how tokens derived from formal templates and natural language puzzles influence the final answers. This analysis reveals two primary sources of degradation: dispersed reasoning attention across non-essential tokens and conflicts introduced by alternative formulations. To address these issues, we propose a prompt-based approach that instructs LLMs to abstract reasoning logic from concrete contexts before attempting direct solutions, and a training-based approach that further strengthens LLMs’ abstraction ability. Experimental results show that our methods improve LLM performance on natural language puzzles by up to 56.2\\%, nearly eliminating the performance loss induced by concretization.", "tldr": "", "keywords": ["Large Language Models", "Reasoning Robustness", "Input Formulation", "Logical Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/46f20600bdcefda4791216bfdb747705c8a6ce5f.pdf", "supplementary_material": "/attachment/71262cf908b5af022ca61e59e3dfebaf0796cca8.zip"}, "replies": [{"content": {"summary": {"value": "This paper propose a 2 translator system to understand the relationship between surface formulation and underlying reasoning logic during perturbations."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is well-motivated, the motivation question is important."}, "weaknesses": {"value": "My overall comment of this paper is the motivation and method are a bit confusing. I cannot be fully convinced that the proposed 2-translator system can help answer the motivation question (line 49 -- 51). \n\n- You've mentioned the gap is the community lacks deeper investigation into how LLMs model the relationship between surface formulation and underlying reasoning logic, how the concretization framework help bridge this gap? My understanding of the the framework is (1) translate FOL into NL and then (2) translated NL back to FOL. How this dual-translation process help review some patterns? In Line 44 -- 51,  perturbation methods are done in NL, but your starting point is FOL, will this be a mismatch? \n- Please clarify if you think I misunderstand the overall structure. Translator 1 is from FOL to NL, translator 2 is from NL to FOL. You trained translator 1 using SAT problem, and trained translator 2 using real-world puzzles. What's the motivation of using these 2 translators? Are you going to discover some hidden reasoning structures during translation? \n- Line 214, more training details are expected. What's the goal of the training. \n- Figure 2 and figure 4 are a bit confusing. WHat's their relationships?"}, "questions": {"value": "- What Figure 1 is trying to convey? No reference for Figure 1. No explanations on the x-axis. \n- Line 59 -- 66, can you explain a bit more about the shifting from formal templates to natural puzzles? My impression is, you have a dual-training process to train the model in order to have a good translation; whereas natural puzzles are OOD data, the gap is expected right? \n- Sec 2.1 complexity analysis, how the analysis is related to the main argument of the paper, i.e., bridge the gap of udnerstanding discrepancies between surface formulation and underlying reasoning logic? \n- Line 206, are you saying models perform bad in translating FOL to NL?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0obvS7rQ2C", "forum": "ijhhFHvWS6", "replyto": "ijhhFHvWS6", "signatures": ["ICLR.cc/2026/Conference/Submission25024/Reviewer_2kAW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25024/Reviewer_2kAW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission25024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760727520553, "cdate": 1760727520553, "tmdate": 1762943287696, "mdate": 1762943287696, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a dual‑learning concretization pipeline that translates formal SAT templates into natural‑language puzzles while verifying isomorphism, letting the authors build paired items with matched logic. Across these pairs, state‑of‑the‑art models suffer large drops moving from formal language (FL) to natural language (NL). An abstraction‑first prompt mitigates the drop, and a training‑based (RL) variant that teaches the solver to back‑translate to templates recovers up to +56.2 points. Token‑level analyses implicate attention dispersion to non‑reasoning tokens and formulation conflicts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* Brittleness to seemingly superficial phrasing is a core topic for reasoning LLMs; this work tackles it head‑on with paired, logic‑preserving FL↔NL instances and clear empirical evidence of fragility.\n* Even a cutting‑edge reasoning model like Qwen3‑30B‑A3B collapses, sharply illustrating current limits of “reasoning” LLMs under realistic formulations."}, "weaknesses": {"value": "* Dataset inclusion is governed by isomorphism verification and an LLM pass‑rate threshold, with no manual checks for grammaticality, clarity, unambiguity of variable definitions, or spurious cues. This raises the risk that some accuracy drops reflect dataset artifacts rather than purely “concretization” effects."}, "questions": {"value": "* Did you run any manual check of samples to rate clarity, grammaticality, internal consistency, and unambiguity?,  to verify that the problems are reasonable for LLMs?\n* The RL gains you report when training on data produced by your pipeline are intriguing. Do you expect the concretization/abstraction framework to extend beyond propositional SAT to first‑order (predicate) logic. Such an extension could plausibly generalize to a wider range of real‑world reasoning problems."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "sml75k4FkS", "forum": "ijhhFHvWS6", "replyto": "ijhhFHvWS6", "signatures": ["ICLR.cc/2026/Conference/Submission25024/Reviewer_LTxT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25024/Reviewer_LTxT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission25024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761384118488, "cdate": 1761384118488, "tmdate": 1762943286854, "mdate": 1762943286854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the fragility of LLMs in logical reasoning when confronted with input formulation variations, moving beyond heuristic tests by proposing the concretization framework. This dual-learning framework automatically converts abstract Boolean satisfiability (SAT) formal language templates (FL) into challenging natural language puzzles (NL), using isomorphism verification to strictly ensure that the underlying reasoning logic remains consistent. Experiments using the resulting paired datasets revealed a sharp decline in LLM reasoning performance upon concretization. To address this, the authors proposed guiding LLMs to abstract the reasoning logic back into a formal template before solving (using prompt-based or training-based methods), successfully mitigating the performance loss."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The transformation framework is explained clearly with examples.\n2. The proposed prompt-based method and the training-based method effectively mitigate the performance drop due to concretization formulation.\n3. The analysis explains the possible cause of the performance decline, which are insightful."}, "weaknesses": {"value": "1. The benchmark only investigates the problem with one task. More tasks are needed to prove the generalization of the framework.\n2. The task seems not challenging enough, since GPT-o3 achieves >97% accuracy on all settings.\n3. Only Qwen3-30B-A3B is used to validate the effectiveness of the prompting and RL-training framework. It would be better if you verify the effectiveness of the proposed methods with more models."}, "questions": {"value": "1. It is more suitable to plot Figure 5 in bar chart instead of pie chart, since it is about absolute count instead of proportions.\n2. Will you open-source the code and benchmark?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "0ILixELCqg", "forum": "ijhhFHvWS6", "replyto": "ijhhFHvWS6", "signatures": ["ICLR.cc/2026/Conference/Submission25024/Reviewer_uoLB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25024/Reviewer_uoLB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission25024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761546885785, "cdate": 1761546885785, "tmdate": 1762943286638, "mdate": 1762943286638, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Authors propose a concretization framework that translates reasoning logic to concrete context with challenging formulations. They use dual learning approach to train two translators with which paired datasets are generated. Their experiments show sharp decline LLM reasoning performance on this dataset?"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. New approach to generate data synthetically leading to a significant drop in performance is interesting and noteworthy.\n2. The attention and perplexity analyses provide some insight into why performance may degrade under language variation."}, "weaknesses": {"value": "1. The entire study is restricted to SAT-style logical reasoning, which is an extremely narrow slice of reasoning ability. It’s unclear if the findings or the proposed method to construct the dataset can extend to more representative \"real-world” scenarios such as multihop reasoning or into other domains such as planning\n\n3. The proposed prompt based approach is also very limited to SAT domains, for example authors include formal language template in the prompt, can the authors show that their method performs well on other domains and logical reasoning datasets as well?\n\n4. Their experiments show that simple tweaking via prompts can boost the accuracy to almost 97% so there are some questions regarding the novelty of the work"}, "questions": {"value": "Please address the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LKJHgz7dxm", "forum": "ijhhFHvWS6", "replyto": "ijhhFHvWS6", "signatures": ["ICLR.cc/2026/Conference/Submission25024/Reviewer_Svik"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission25024/Reviewer_Svik"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission25024/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761992926039, "cdate": 1761992926039, "tmdate": 1762943286336, "mdate": 1762943286336, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}