{"id": "feJ5T9sFSJ", "number": 15328, "cdate": 1758250308605, "mdate": 1759897313365, "content": {"title": "TritonRL: Training LLMs to Think and Code Triton Without Cheating", "abstract": "With the rapid evolution of large language models (LLMs), the demand for automated, high-performance system kernels has emerged as a key enabler for accelerating development and deployment. We introduce TritonRL, a domain-specialized LLM for Triton kernel generation, trained with a novel reinforcement learning (RL) framework that enables robust and automated kernel synthesis. Unlike CUDA, which benefits from abundant programming data, high-performance Triton kernels are scarce and typically require costly crawling or manual authoring. Furthermore, reliable evaluation methods for validating Triton kernels remain underdeveloped and even hinder proper diagnosis of base model performance. Our approach addresses these challenges end-to-end with a fully open-source recipe: we curate datasets from KernelBook, enhance solution quality via DeepSeek-assisted distillation, and fine-tune Qwen3-8B to retain both reasoning ability and Triton-specific correctness. We further introduce hierarchical reward decomposition and data mixing to enhance RL training. With correct re-evaluations of existing models, our experiments on KernelBench demonstrate that TritonRL achieves state-of-the-art correctness and speedup, surpassing all other Triton-specific models and underscoring the effectiveness of our RL-based training paradigm.", "tldr": "", "keywords": ["Reinforcement Learning", "Large Language Models", "Kernel", "Triton"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/800e7fc4358a804b3320fc8739a38f8846d72596.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper uses SFT and RL to fine-tune a LLM for kernel code generation based on Qwen3-8B. The dataset is synthesized by DeepSeek-R1 and then fine-tune the model with rule-based rewards based on syntactic, semantic, and speed scores. The task studied is important since the kernel code is rare compared to other datasets. However, the training techniques used are common, no major novelty involved. The writing lacks some important results such as the GRPO training curves. One major reason is whether the great performance is resulted from DeepSeek-R1 distillation. Whether the current model outperforms DeepSeek-R1 remains unclear."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The studied topic is critical. The reward design part is inspiring, especially the separation of reasoning and code process and tuning the rewards based on them. The final results show great improvements."}, "weaknesses": {"value": "The main weakness is the lack of novelty, since the training process has been applied to many different domains, including coding. The training process is not displayed. For the reward split of reasoning and code parts, is their syntax and function scores the same? The training details such as the SFT and GRPO settings and GPU settings are used included."}, "questions": {"value": "What is the distribution of LLM generated code length and complexity? If the model generates multiple codes in the same inference, how to handle the extraction and judging process?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "2iAfdDXHYv", "forum": "feJ5T9sFSJ", "replyto": "feJ5T9sFSJ", "signatures": ["ICLR.cc/2026/Conference/Submission15328/Reviewer_8pMZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15328/Reviewer_8pMZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission15328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761855915290, "cdate": 1761855915290, "tmdate": 1762925623972, "mdate": 1762925623972, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents TritonRL, a domain-specific 8B LLM trained for generating efficient Triton kernels. The authors start with Qwen3-8B. The SFT is on a dataset curated from KernelBook and enhanced via distillation chain-of-thoughts from DeepSeek-R1. The model is later trained with RL with custom reward shaping. The reward is coming from the design of a \"robust verifier\" which combines rule-based checks and a LLM-based judge. Algorithmic contribution comes from the \"hierarchical reward decomposition,\" which applies different reward signal to different parts of the tokens. Experiments on KernelBench show that TritonRL surpasses other 8B scale models in correctness and speedup."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses the task of training LLM to generate Triton kernels, which is an interesting task itself. I see the main contributions from the paper are:\n1. a \"robust verifier\" and cheating detection which gives a better reward.\n2. the reward design to favor majorly for correctness of triton kernels and reward speedup on top with weight $\\alpha=0.1$, leads to a better performance.\n\nThe 2., \"hierarchical reward decomposition\", is intuitive and well-motivated. The idea of separating rewards for the \"plan\" (reasoning trace, rewarded for speedup) and the \"code\" (implementation, rewarded for correctness) is a sensible approach.\n\nThe experiments are also well-executed, with importance ablation to show the effectiveness of the reward design.\n\nI also appreciate the authors' commitment to reproducibility and promise to open-source the datasets, recipes and checkpoints."}, "weaknesses": {"value": "1. Method\n- The section on \"Data Mixing Optimization\" looks problematic. The paper introduces a complex optimization framework, defining a reward function on test set and a reward interaction matric $\\mathcal{S}$ Eq(4), which implies a sophisticated method for finding an optimal mixing probability $p$. However, the paper then states: \"...we simply evaluate three candidate initializations, p in {[1,0], [0, 1], [0.5, 0.5]}, and choose the best-performing mixture...\". This is a simple heuristic test of three static ratios (Dataset A, Dataset B, or Dataset A + B), not necessarily an \"optimization\" approach. Presenting the complex, unused framework is confusing and feels misleading.\n\n- Although the authors conduct several ablations on the reward design, I believe the paper could benefit from an essential ablation experiment: The authors are encouraged to report the baseline where all tokens are rewarded the same way but defined as $\\alpha * speedup + correctness$. I deemed this as a necessary experiment if the authors intend to claim the \"hierarchical\" part is the novelty. Otherwise, it's just plain GRPO with reward shaping. After all, the algorithm offers limited novelty in the pure algorithmic aspect but has its value under the concrete problem setting. I would see it as more than adapting GRPO to the kernel optimization problems. but I'm not sure this alone is strong enough for me to recommend acceptance.\n\n\n2. Presentation\n- Please adhere to the ICLR template guide: notably, caption and number for Tables should come before the Tables.\n- L238 ends with 2 .\n\nThe manuscript suffers from a lack of rigor in its mathematical notation, which is concerning for an RL paper. Some of them I will detail in later section. For example, \n- L239: I would not call $\\mathcal{F}$ GRPO “losses” since it's something that the training is trying to maximize but not minimize. I recommend that the authors either change it to GRPO \"objective\" or add a minus sign to the equation.\n- $Q$ is not defined in Eq(1)\n\nAlso I'm not quite comfortable with some wordings I list as follows. They're more than a typo and will cause factual confusion.\n\n- L236: ”$\\pi_\\theta$ and $\\pi_{old}$ are the policy model and reference model“. $\\pi_{old}$ is not a reference model per se unless the authors mean that the samples during RL are all generated from a model with frozen weights, which is not the case in the online algorithms the authors claim to use.\n- L185: The definition of speedup, is defined as $\\tau(o^{code}, x) / \\tau(q^{ref}, x) · correct(q, o)$,  where $\\tau (·, x)$ measure the runtimes of given code and input x. This term is directly used in the reward definition in Eq(3): So GRPO incentives to increase this term as part of the reward, and therefore increase the runtime ratio of $\\tau(o^{code}, x) / \\tau(q^{ref}, x)$, and therefore the longer timre $o^{code}$ runs, the greater the reward will be: This sounds weird to me and maybe the direction is inverse."}, "questions": {"value": "- There's little detail about the LLM-based judge. What's the prompt for the LLM-based judge? I'm not sure the contribution from this design: the paper seems to claim that \"reward hacking\" behavior such as using pytorch module to bypass the unit tests is a challenge while the authors also say in L170 that the rule-based linter manages this: \"rule-based linter—which ensures Triton kernels are invoked and flags reliance on PyTorch modules\". \n- For a datapoint, how many test inputs on average $x$ is used to calculate the speed up? If there're multiple inputs, are the score averaged? Do the authors make efforts on guarantee that the input is of varied size and shape?\n- Could the authors provide stats of the number test inputs in the dataset used and the augmented ones in L132? The data augmentation part lacks details.\n- Reducing kernel runtime measure noise is a known challenge in the field. How does the author measure runtime and are the authors putting out dedicated efforts on this issue?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wP6DAhlviN", "forum": "feJ5T9sFSJ", "replyto": "feJ5T9sFSJ", "signatures": ["ICLR.cc/2026/Conference/Submission15328/Reviewer_Xiqf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15328/Reviewer_Xiqf"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission15328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761867621241, "cdate": 1761867621241, "tmdate": 1762925623553, "mdate": 1762925623553, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces TRITONRL, an 8B-scale LLM specialized for Triton GPU kernel generation, which tackles data scarcity and prevalent reward hacking. The primary contributions are a robust, anti-cheating verification framework to detect functionally invalid code, and a hierarchical reward decomposition method for reinforcement learning. This method assigns speedup-based rewards to reasoning plan tokens and correctness-based rewards to code tokens, enabling precise credit assignment. Through a two-stage SFT and RL pipeline using distilled data, TRITONRL achieves state-of-the-art correctness and speedup on KernelBench for 8B models."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is overall well-written and clearly organized. It effectively frames the core challenge of reward hacking in kernel generation, where models superficially pass tests by calling high-level libraries rather than writing valid kernels.\n2. The proposed solutions are mostly clear. The robust verifier combines rules and an LLM-judge to ensure functional validity. The hierarchical reward decomposition is designed for credit assignment, decoupling rewarding speedup from correctness.\n3. The experiments show TRITONRL achieving promising performance among 8B models. The ablation study is conducted to show the effectiveness of the robust verifier and the hierarchical reward decomposition approach."}, "weaknesses": {"value": "1. While the robust verifier is a practical and necessary engineering solution for this specific problem, the core method (combining rule-based heuristics with an LLM-judge to prevent reward hacking) is not a particularly novel concept. Using LLMs as process reward models to prevent reward hacking is an increasingly standard approach.\n2. The Hierarchical Reward Decomposition (HRD) design may introduce a new, unaddressed risk. By rewarding \"code\" tokens only for `correctness` and \"plan\" tokens for `speedup`, the policy might be incentivized to generate *correct but simple/slow* kernels. The \"code\" policy has no direct incentive to implement an *efficient* kernel, which could allow it to ignore the \"plan's\" high-speedup objective as long as it produces a correct (even if sub-optimal) result.\n3.  The ablation study (Table 2) meant to validate the HRD is not rigorous. The main comparison between the proposed `λ*` (HRD, `α=0.1`) and the baselines like `λ²` (uniform speedup, `α=1.0`) is confounded by the `α` hyperparameter. A more direct comparison of the `α=1.0` settings shows that uniform speedup (`λ²`, 47.0% correct) significantly *outperforms* the HRD (`λ³`, 33.0% correct). This strongly suggests the performance gains may come from the `α=0.1` setting (slowing down plan updates) rather than the reward *decomposition* itself. A crucial baseline (e.g., uniform speedup with `α=0.1`) is missing, making it impossible to isolate the true benefit of the HRD."}, "questions": {"value": "Please refer to above Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "rVmiyjgv8c", "forum": "feJ5T9sFSJ", "replyto": "feJ5T9sFSJ", "signatures": ["ICLR.cc/2026/Conference/Submission15328/Reviewer_hZNp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15328/Reviewer_hZNp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission15328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761988962341, "cdate": 1761988962341, "tmdate": 1762925623156, "mdate": 1762925623156, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TritonRL, a large language model trained for Triton kernel generation. The authors first create a training dataset from KernelBook, and then finetune Qwen3-8B. In addition, they post-train it with GRPO, with hierarchical reward decomposition and data mixing to enhance RL. In experiments, it was shown that, on KernelBench, TritonRL achieves state-of-the-art correctness and speedup, surpassing all other open Triton-specific models, and comparable to proprietary LLMs like Claude."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This paper proposes a technically sound and well-motivated approach combining supervised finetuning and RL with curated training data.\n- In experiment, the proposed approach outperforms other open Triton kernel generation models, and is comparable to proprietary LLMs like Claude, which demonstrates its effectiveness. \n- Writing is easy to understand."}, "weaknesses": {"value": "- Even though writing is easy to understand, it lacks an important part. There is no related work presented, which makes it difficult to assess the paper with respect to the literature. \n- As this reviewer is new to Triton kernel generation task, it was not clearly stated why the task is important and how it is different from other tasks like CUDA code generation. Therefore the significance of the approach was not well demonstrated. \n- Another major concern is novelty. Even though the proposed approach looks technically sound and well-motivated, applying supervised finetuning and RL is nowadays standard for training LLM."}, "questions": {"value": "- Can you clarify the novelty of the proposed contributions and approach?\n- It was not clear why other much bigger open models like GPT-oss and DeepSeek-R1were missing in the main result table in the main paper. They should be in the main paper, like Claude.\n\nPlease address the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "gGaTI38EGa", "forum": "feJ5T9sFSJ", "replyto": "feJ5T9sFSJ", "signatures": ["ICLR.cc/2026/Conference/Submission15328/Reviewer_bS9o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission15328/Reviewer_bS9o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission15328/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998350913, "cdate": 1761998350913, "tmdate": 1762925622809, "mdate": 1762925622809, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}