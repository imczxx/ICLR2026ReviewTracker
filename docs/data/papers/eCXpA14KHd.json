{"id": "eCXpA14KHd", "number": 6010, "cdate": 1757950586816, "mdate": 1759897939729, "content": {"title": "Nasty Adversarial Training:  A Probability Sparsity Perspective for Robustness Enhancement", "abstract": "The vulnerability of deep neural networks to adversarial examples poses significant challenges to their reliable deployment. Among existing empirical defenses, adversarial training and robust distillation have proven the most effective. In this paper, we identify a property originally associated with model intellectual property, i.e., probability sparsity induced by nasty training, and demonstrate that it can also provide interpretable improvements to adversarial robustness. \nWe begin by analyzing how nasty training induces sparse probability distributions and qualitatively explore the spatial metric preferences this sparsity introduces to the model. Building on these insights, we propose a simple yet effective adversarial training method, nasty adversarial training (NAT), which incorporates probability sparsity as a regularization mechanism to boost adversarial robustness. Both theoretical analysis and experimental results validate the effectiveness of NAT, highlighting its potential to enhance the adversarial robustness of deep neural networks in an interpretable manner.", "tldr": "", "keywords": ["adversarial training", "adversarial robustness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/6517026623c8cd4ab578682c4a3362e3deb1a294.pdf", "supplementary_material": "/attachment/7f52045ea6106a8c0f46898728bdc89f32a5d24b.zip"}, "replies": [{"content": {"summary": {"value": "The paper proposed a method for improving adversarial training which incorporates nasty training. The method uses two networks: a target network which we would like to be more robust to adversarial example and a vanilla trained network. The target network is trained to be a \"nasty teacher\" version of the vanilla trained network. The intuition is that this keeps the top-1 prediction correct but sparse so top-N predictions are very low probability and therefore far away in the decision space of the model. This should make the model robust to perturbations of the input. The paper provides interesting theoretical analysis of the solution and empirical results show that it does work."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Interesting and new idea: nasty training was intended for another purpose so its a good application\n- Good analysis: the intuitive and formal analysis was interesting and mostly convincing\n- Mostly good results: based on the results the method clearly does work"}, "weaknesses": {"value": "- The results seem to focus on CIFAR datasets which may not be fully representative of real world conditions (See Maiya et al. \"Unifying the Harmonic Analysis of Adversarial Attacks and Robustness\")\n- No analysis of training time"}, "questions": {"value": "This was a very interesting paper which I think makes a good contribution. The idea is new and it's an insightful application of nasty training which makes sense from the theoretical motivation in the paper. The primary thing I think is missing is more/more convincing results. Based on the discussion I expected there to be a pretty clear improvement from the proposed method but on the presented CIFAR results, it didn't look like a dramatic change. Also I am not sure that CIFAR results are reflective of real conditions so the method should really be tested on something more comprehensive. It also it wasn't clear to me from the paper how much longer it takes to incorporate the proposed training loop after already going through adversarial training on the target model.\n\n**Specific Questions** \n- Does the method work on datasets beyond CIFAR? ImageNet for example?\n- What is the total retraining time for this method? Both with and without the required adversarial training that occurs before the nasty training."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "TXa9zWv5Uk", "forum": "eCXpA14KHd", "replyto": "eCXpA14KHd", "signatures": ["ICLR.cc/2026/Conference/Submission6010/Reviewer_AFqW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6010/Reviewer_AFqW"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission6010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761584299600, "cdate": 1761584299600, "tmdate": 1762918413400, "mdate": 1762918413400, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a method that integrate the nasty training (NT) into AT to strengthen robustness. They further analyze the probability sparsity of NT which has potential in adversarial training. And the experiments show the effectiveness of the method."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper analyzes the probability sparsity from NT and combined it with AT, by increasing decision boundary margins result in a higher cost for adversarial attacks, which shows the promising performance. And has in-depth theoretical interpretable analysis.\n2. The experiment is comprehensive with CNN and ViT under different white and black attacks. And the ablations show the sparsity and effectiveness."}, "weaknesses": {"value": "1. The main results in Table 1 and Table 2 were not compared with some SOTA method, such as LWTA [1], IKL-AT [2] and DCS [3].\n2. All reported results correspond to the best outcomes over three independent runs, but there is no report of the mean and standard deviation of the results. And the class index in Figure 5 should be integer.\n\n[1] Stochastic local winner-takes-all networks enable profound adversarial robustness, 2021.\n\n[2] Decoupled kullback-leibler divergence loss, NeurIPS 2024.\n\n[3] Adversarial Robustness via Deformable Convolution with Stochasticity, ICML 2025."}, "questions": {"value": "1.\tCould you add some up-to-date SOTA methods to make a more comprehensive comparison?\n2.\tCould you present the mean and standard deviation of your method at least in main table? And could you change the class index in Figure 5 to integer?\n3.\tNAT utilizes a VT for training. For the fairness, does NAT cover the expenses of the VT in Table 8?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dPcOwQ8ha3", "forum": "eCXpA14KHd", "replyto": "eCXpA14KHd", "signatures": ["ICLR.cc/2026/Conference/Submission6010/Reviewer_gn7C"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6010/Reviewer_gn7C"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission6010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761727548680, "cdate": 1761727548680, "tmdate": 1762918413031, "mdate": 1762918413031, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Nasty Adversarial Training (NAT), a defense framework for adversarial attacks. NAT integrates the concept of probability sparsity, which was originally proposed in the context of distillation resistance, into traditional adversarial training. The paper aims to demonstrate how enforcing sparsity in the output probability distribution can enhance the robustness of models by increasing the inter-class separability and widening decision margins in the logit space."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper use “nasty training” as a robustness regularizer.\n2. This paper have solid theoretical reasoning and thorough experimental validation.\n3. This method improves robustness while maintaining efficiency and interpretability.\n4. This paper have good exposition of probability sparsity and its spatial interpretation.\n5. This method can be easily integrated into existing AT pipelines with negligible cost."}, "weaknesses": {"value": "1. The Taylor expansion and sparsity explanation rely on approximations; formal proofs or bounds are lacking.\n2. The “spatial metric” benefits are qualitatively visualized but lack quantitative metrics (e.g., explicit margin distributions).\n3. Only standard PGD/CW/AA considered — might not generalize to adaptive threat models.\n4. The authors compare to entropy/logit norm regularization briefly but not in depth.\n5. The term “nasty” may be unconventional for robustness literature and could obscure broader relevance."}, "questions": {"value": "1. Can authors quantify the relationship between measured probability sparsity and empirical robustness (e.g., correlation between entropy and adversarial accuracy)?\n2. How does NAT perform under adaptive attacks specifically designed to exploit the auxiliary adversary structure?\n3. Could the “probabilistic sparsity” be approximated directly (e.g., via entropy regularization) instead of adversary-based NAT?\n4. How sensitive is NAT to adversary model mismatch or overfitting? Would a partially shared backbone improve stability?\n5. Does the spatial metric benefit persist for non-classification tasks (e.g., detection or segmentation)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vkzk4dwovn", "forum": "eCXpA14KHd", "replyto": "eCXpA14KHd", "signatures": ["ICLR.cc/2026/Conference/Submission6010/Reviewer_2bUB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission6010/Reviewer_2bUB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission6010/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761971329502, "cdate": 1761971329502, "tmdate": 1762918412762, "mdate": 1762918412762, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}