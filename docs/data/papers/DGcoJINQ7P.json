{"id": "DGcoJINQ7P", "number": 11819, "cdate": 1758204061773, "mdate": 1759897552578, "content": {"title": "EgoBrain: Synergizing Minds and Eyes For Human Action Understanding", "abstract": "The integration of brain-computer interfaces (BCIs), in particular electroencephalography (EEG), with artificial intelligence (AI) has shown tremendous promise in decoding human cognition and behavior from neural signals. In particular, the rise of multimodal AI models have brought new possibilities that have never been imagined before. Here, we present EgoBrain—the world’s first large-scale, temporally aligned multimodal dataset that synchronizes egocentric vision and EEG of human brain over extended periods of time, establishing a new paradigm for human-centered behavior analysis. This dataset comprises 61 hours of synchronized 32-channel EEG recordings and first-person video from 40 participants engaged in 29 categories of daily activities. We then developed a multimodal learning framework to fuse EEG and vision for action understanding, validated across both cross-subject and cross-environment challenges, achieving an action recognition accuracy of 66.70%. EgoBrain paves the way for a unified framework for brain-computer interface with multiple modalities. All data, tools, and acquisition protocols are openly shared to foster open science in cognitive computing.", "tldr": "", "keywords": ["Electroencephalography (EEG)", "First Person Vision(Egocentric Vision)", "Human Action Understanding"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/b55c4f5c6a6e4ee5e2b6c39d622cd15c4a71115e.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, a new egocentric dataset is proposed which includes the modality of EEG recordings. Unique to this dataset, participants wore an EEG device whilst recording the videos so that their intentions could be better modelled and improve performance. The dataset includes ~61 hours of recording across 4 major classes which are split into 10 verbs which are split into 29 actions. An extension of the Time Interval Machine which instead of audio uses the EEG modality is presented as a baseline and results show that whilst visual only gives a strong baseline, the inclusion of EEG recordings is complementary, providing a boost in overall performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "* The dataset represents the first dataset of its kind combining both egocentric visual data and EEG recordings, thus opening up a new research area.\n* The dataset includes a good variety of actions across the ~61 hours.\n* The results showcase the benefit of the EEG modality when combined with the vision modality.\n* The paper is generally well-written and easy to follow, there are very few issues with spelling/grammar/presentation."}, "weaknesses": {"value": "During the description of the dataset, it is not clear within the main paper what the difference is between the verbs Play(I), Play(II), and Play(III) are. This becomes evident within the appendix that it is based on device, but should be included earlier.\n\nThe 3 level hierarchy of high-level class, verb, and action is presented but it is unclear whether this hierarchy/taxonomy is utilised in some way during training or whether this was considered at all.\n\nThe benchmarking of the dataset could be improved in a few different ways:\n* Firstly, there is no random performance given, the gap between the visual modality and the EEG modality is quite large, with the latter reaching only 21% and 10% on the dataset. It would be good to include these numbers to get a sense of the lower bound and how far the EEG only performance is.\n* Secondly, whilst the confusion matrices are provided for the verb classification, these are across two 'clusters' of verbs, and do not show all the misclassifications (for example the operate row only adds up to 0.94). Including full confusion matrices for both actions and verbs (potentially in the appendix) would be interesting to look at to check the biases of the model/data and/or providing per class accuracy metrics would be interesting to see how the performance differs. Given that the dataset is imbalanced in terms of length (though seems uniform from a class frequency perspective) it would be good to know if this impacts model training.\n* Finally, on the cross-subject only results, the method is already achieving 90% and 80% on the verb/action classification respectively, showcasing that this split is almost saturated already, some discussion regarding this and the more challenging cross-subject and cross-scene setting would be interesting to see."}, "questions": {"value": "1. What are the differences between the different play labels, i.e. play(I), play(II), play(III), is this referring to computer, physical, mobile? Why were they split up this way, to make sure that the distribution wasn't too long-tailed/dominated by a majority class? (This becomes clear after looking at the appendix but isn't mentioned in the main paper)\n2. For the Cross-Subject&Cross-Scene split, it is not explicitly mentioned, but I assume the 6 new sessions in the test set have different subjects to the 28 in training?\n3. It would be good to know the random performance of the new dataset for Table 1 to get a sense of the lower bound. The Brain-only results are very low in comparison to the visual only and visual + brain for example and it would be interesting to know the relative performance of this model compared to random.\n4. From the dataset construction, there is a 3 stage hierarchy, including the high level classes, the verbs, and the actions themselves, was this considered at all for the method/dataset splits?\n5. Why was the verb confusion matrix split into two, just for space reasons? And why were the confusion matrices of the actions excluded?\n6. Given the imbalanced nature of the dataset, was an investigation conducted into whether the models are biased towards certain classes would be interesting to see, i.e. looking into per-class accuracy metrics or similar? The confusion matrices shows this in some fashion for verbs, but doesn't include the entire matrix and doesn't include actions."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "MtEgzokZwp", "forum": "DGcoJINQ7P", "replyto": "DGcoJINQ7P", "signatures": ["ICLR.cc/2026/Conference/Submission11819/Reviewer_qdr1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11819/Reviewer_qdr1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761813375389, "cdate": 1761813375389, "tmdate": 1762922842596, "mdate": 1762922842596, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces EgoBrain, the first large-scale, temporally aligned multimodal dataset combining egocentric video and EEG signals for real-world human action understanding. It includes 61 hours of synchronized EEG and first-person video from 40 participants performing 29 categories of daily activities. The authors also propose a Brain-TIM model (Brain-Time Interval Machine), a multimodal transformer-based framework that fuses EEG and vision representations via time-interval MLPs and modality-specific embeddings to capture temporal and cross-modal dependencies. Experiments demonstrate that multimodal fusion improves action recognition performance over unimodal baselines, particularly in cross-subject and cross-scene generalization."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tFirst-of-its-kind dataset integrating real-world egocentric vision with EEG; extensive and ethically curated.\n2.\tClear methodological design with well-justified architecture choices (temporal embeddings, modality-aware tokens).\n3.\tInsightful qualitative results showing when EEG signals complement vision (e.g., occlusion or intent disambiguation)."}, "weaknesses": {"value": "1.\tThe synchronization precision is stated as <1s jitter. This is a relatively large jitter for fast-changing neural signals and short actions. This level of jitter could potentially limit the precise time-locking necessary for analyzing rapid neural correlates of action initiation or error. A discussion on how the Brain-TIM model's windowing strategy mitigates the impact of this 1s jitter is needed.\n\n2.\tThe model’s novelty is limited, Brain-TIM primarily applies existing time-embedding concepts to a new modality pair. Moreover, the framework relies solely on two pre-trained models for feature extraction and classification, which restricts the scope of analysis. How would other models or architectures perform on this dataset? Additional comparative experiments are essential to validate the dataset’s generality and utility.\n\n3.\tThe reported accuracy improvements (≈1–3%) may be statistically marginal, raising questions about their practical significance. The authors should provide statistical validation (e.g., p-values or confidence intervals) to confirm whether these gains are significant rather than due to random variation.\n\n4.\tThe ablation study (Table 2) demonstrates the contribution of the temporal/modality embeddings, but it doesn't directly test the core hypothesis of fusion effectiveness by removing one entire modality from the multimodal setting. Specifically, in the \"Visual & Brain\" section, the base case uses both the Visual and Brain encoders. It would be insightful to see an ablation where the fusion mechanism is active, but one modality's tokens are zeroed out or entirely removed to isolate the Transformer's fusion contribution, separate from the unimodal encoders' raw output strength.\n\n5.\tThe Cross-Subject & Cross-Scene task is expected to be more challenging than the Cross-Subject-Only task; however, the “Brain Only” model surprisingly achieves higher action-classification accuracy in this setting. In contrast, VideoMAE experiences a large accuracy drop, while LaBraM shows only a minor decrease. The paper should clarify why EEG-based performance remains stable (or improves) across scenes, despite EEG being more sensitive to noise, and why VideoMAE—supposedly a stronger zero-shot model—shows a significant decline.\n\n6.\tIt is unclear whether the dataset includes cross-session experiments, i.e., whether the same subjects were recorded again after a time interval performing the same tasks. Including or clarifying this aspect would be valuable for evaluating intra-subject consistency and longitudinal generalization."}, "questions": {"value": "Please refer to the Weaknesses section for detailed questions and suggestions to the authors."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "nCXpiAui1R", "forum": "DGcoJINQ7P", "replyto": "DGcoJINQ7P", "signatures": ["ICLR.cc/2026/Conference/Submission11819/Reviewer_UTCn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11819/Reviewer_UTCn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982052300, "cdate": 1761982052300, "tmdate": 1762922842094, "mdate": 1762922842094, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a multi-modal dataset for Action Classification, including synchronized egocentric videos and 32-channel encephalography recordings. Additionally, this paper proposes a baseline method that achieves an overall accuracy of 80.16% in action classification, utilizing both data modalities. This paper empirically demonstrates that the information extracted from encephalography may be complementary to visual data."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "This review evaluates the paper's quality based on the following criteria: task relevance, related work, technical novelty, technical correctness, experimental validation, writing and presentation, and reproducibility. Each aspect is discussed and highlighted as a strength or a weakness in the sections below.\n-    **Dataset Contribution and Reproducibility:** This paper contributes to the community a dataset of synchronized egocentric videos and 32-channel encephalography recordings. However, it is not explicitly indicated whether the source code will be released, and it is not included as part of the submission.\n-    **Writing and presentation:** Overall, this paper is easy to read and well-written."}, "weaknesses": {"value": "-    **Relevance of the task and Experimental Validation:** Even though Action Classification from egocentric videos and encephalography recordings may be a relevant problem for the ICLR community. The motivation behind including this novel data type modality is not well stated in the paper's introduction. This paper already reports high performance for the proposed task, so it may probably saturate fast. Considering these results, what are the reasons to keep the data acquisition as simple as possible to not make the task harder?\n-    **Technical Correctness and Related Work:** This paper overclaims about contributing a “large-scale” dataset when its size is not comparable to current state-of-the-art benchmarks for human action recognition from egocentric vision data. Moreover, it states that"}, "questions": {"value": "1.\tWill the source code and pretrained models be released to support reproducibility? If so, what is the reason for not including them in the supplementary material?\n2.\tWhat is the motivation for including encephalography data?\n3.\tGiven the already high reported accuracy, how does the paper address concerns about task saturation?\n4.\tWhy was the decision made to keep data acquisition simple, and how does this impact the task's difficulty and generalizability?\n5.\tOn what basis is the dataset described as “large-scale,” and how does its size compare to existing benchmarks?\n6.\tThe paper states that prior datasets only use visual data, but most include multiple modalities. Can this claim be clarified?"}, "flag_for_ethics_review": {"value": ["Yes, Responsible research practice (e.g., human subjects, annotator compensation, data release)"]}, "details_of_ethics_concerns": {"value": "The paper states that an IRB approved the data acquisition; however, the data may contain sensitive information from human subjects, so it would be advisable for someone with more experience in these topics to review it."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dhc8urdXxn", "forum": "DGcoJINQ7P", "replyto": "DGcoJINQ7P", "signatures": ["ICLR.cc/2026/Conference/Submission11819/Reviewer_vYK1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11819/Reviewer_vYK1"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761994682887, "cdate": 1761994682887, "tmdate": 1762922841714, "mdate": 1762922841714, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EgoBrain, a large-scale, multimodal dataset of 61 hours of synchronized egocentric video and 32-channel EEG signals. This data was collected from 40 participants performing 29 different daily activities such as work, play, learn, and consume within a controlled laboratory setting. The authors also propose a model, Brain-TIM which fuses the visual and EEG signals data using a temporal-aware embedding mechanism and modality-specific encodings for action understanding. The paper evaluates this model on action and verb classification tasks for cross-subject and cross-subject & cross-scene settings. The results indicate that the multimodal model of using both vision and EEG modality achieves a 66.70% accuracy, representing a 3.30% absolute improvement over a visual-only baseline in the cross-scene setting."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper contributes a new dataset, EgoBrain which has synchronized video and EEG signals which can be valuable for computer vision research.  \n2. The paper shows that that EEG signals can be a useful modality for tasks such as action recognition when the visual modality is occluded.\n3. The paper shows analysis on cross-subject and cross-subject & cross-scene analysis which is a challenging benchmark to evaluate the model generalization."}, "weaknesses": {"value": "1. The architecture method of Brain-TIM seems to be incremental when compared to TIM [1]. The architecture presented in the paper of modality-specific encoders, embedding layers, Time-Interval MLP, and a Transformer encoder seems to be a direct application of the existing TIM framework to a new pair of modalities. Can the authors clarify the differences between TIM and Brain-TIM? Is Brain-TIM just an extension of TIM to multiple modalities?\n2. While the idea of using EEG signal to understand egocentric actions is well-motivated, can the authors discuss the application of their proposed approach to real-world scenarios where collecting EEG signals can be hard and require specific hardware?\n\n[1]. Chalk, Jacob, et al. \"Tim: A time interval machine for audio-visual action recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024."}, "questions": {"value": "Since the dataset has been collected in a lab setup with isolation chamber with limited limb movement, can the authors discuss the robustness of the EEG signal? How much noise can the EEG signal have and still be able to give better action recognition results than just using the visual modality?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6ri7w81U4U", "forum": "DGcoJINQ7P", "replyto": "DGcoJINQ7P", "signatures": ["ICLR.cc/2026/Conference/Submission11819/Reviewer_Lp6o"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11819/Reviewer_Lp6o"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11819/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762044505635, "cdate": 1762044505635, "tmdate": 1762922841230, "mdate": 1762922841230, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}