{"id": "HFZhYvKM8V", "number": 10190, "cdate": 1758163395733, "mdate": 1763607678639, "content": {"title": "Enhancing Cross-task Transfer of Large Language Models via Fourier Activation Steering", "abstract": "Large Language Models (LLMs) have shown impressive abilities in leveraging pretrained knowledge through prompting, but they often struggle with unseen tasks, especially in data-scarce scenarios.\nWhile cross-task in-context learning provides a direct solution for knowledge transfer without fine-tuning, it still faces limitations in terms of robustness, scalability, and efficiency.\nIn this paper, we investigate whether cross-task transfer can be achieved via latent space steering.\nThrough analysis of activation patterns under both zero-shot and few-shot prompts, we have three observations:\n(1) the activation differences between few-shot and zero-shot prompts exhibit a nearly parallel structure in low-dimensional space;\n(2) these difference vectors correlate strongly with task similarity; \n(3) Fourier analysis reveals that low-frequency components encode task-agnostic, information-enhanced features, while high-frequency components capture task-specific details.\nMotivated by these findings, we propose FAST, a Fourier-based Activation Steering Transfer framework.\nIt first selects influential and diverse samples from high-resource tasks, then injects information-enhanced low-frequency components along with task-similarity weighted high-frequency components during inference.\nExtensive experiments in both cross-domain and cross-lingual transfer settings show that our method consistently outperforms existing methods.", "tldr": "We observe that cross-task transfer can be achieved via latent space steering and propose Fourier-based activation steering framework for cross-task transfer.", "keywords": ["Cross-task transfer", "Fourier activation steering"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/c00e61282a07b033ad4748c9063b6ac626a408eb.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a latent space steering method for cross-task transfer learning in LLMs, enabling knowledge from high-resource tasks to enhance performance on low-resource tasks. The method is built on an analysis of how In-contest learning (ICL) functions at the level of model activations.\n\nThe paper first establishes that ICL (providing few-shot examples) works by “shifting” the model’s internal activations. This makes a distinct difference vector (dv) between the activations of a zero-shot prompt and a few-shot prompt. This implies that the learning from examples can be captured as a directional vector in the model’s latent space.\n\nTo extract task-agnostic information from these vectors dv the authors apply FFT. This transformation decomposes the vectors into low/high frequency components. The low-frequency component is found to be task-agnostic, while the high-frequency component is task-specific.\n\nThe overall proposed method consists of two parts: first, selecting an influential and diverse subset of samples from the high-resource source task via a graph-based algorithm to compute the difference vector dv; and second, injecting this vector's components into the latent space, using Fourier decomposition to filter and separate them."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "I think the paper’s key contribution is its novelty of the approach. \n\nIt begins by empirically identifying a core phenomenon, ICL as a consistent activation shift. It then defines the challenge this presents (the shift is task-specific, potentially leading to negative transfer).\n\nIt treats the complex and high-dimensional activation vectors as “signals” and applies Fourier transform. This leads to a compelling and insightful interpretation: the low-frequency components corresponds to task-agnostic information, while the high-frequency components encode task-specific details."}, "weaknesses": {"value": "- The choice of FFT: Although FFT provides a good starting point, there are more advanced methods in signal processing for more sophisticated analysis, e.g. wavelets. \n\n- Convoluted method: The proposed method is complex and relies on several key hyperparameters that could be difficult to tune. The performance is likely sensitive to:\nThe frequency cutoff $k$ (Eq. 5) used to separate low and high frequencies. \nThe injection strength $\\lambda$ (Eq. 10)\nThe similarity threshold $\\epsilon$ (Eq. 10)\nand The parameters for the graph-based sampling (e.g., number of neighbors, diffusion steps).\nThese settings may not generalize well across different models or tasks, requiring careful tuning for each new application."}, "questions": {"value": "(see weaknesses)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "8navqKKcVu", "forum": "HFZhYvKM8V", "replyto": "HFZhYvKM8V", "signatures": ["ICLR.cc/2026/Conference/Submission10190/Reviewer_gpcn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10190/Reviewer_gpcn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761877271621, "cdate": 1761877271621, "tmdate": 1762921556014, "mdate": 1762921556014, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FAST (Fourier-based Activation Steering for cross-task Transfer), a framework that enables large language models (LLMs) to transfer knowledge from high-resource to low-resource tasks without fine-tuning or input expansion. The authors first observe that the difference between few-shot and zero-shot activations forms nearly parallel patterns across tasks and that these differences correlate with task similarity. Using Fourier analysis, they decompose these activations into low-frequency components that encode task-agnostic, information-enhanced features, and high-frequency components that capture task-specific details. FAST selects a diverse and influential subset of high-resource samples, extracts their activation differences, applies Fourier filtering, and injects the resulting components into the target model during inference to steer behavior. Experiments across cross-domain and cross-lingual transfer settings show that FAST consistently outperforms prompting, fine-tuning, and other activation-steering baselines while maintaining efficiency and scalability."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper introduces a novel Fourier-based approach to activation steering that enables cross-task transfer without fine-tuning or retraining, offering a conceptually elegant and computationally lightweight framework.\n\n- The method demonstrates consistent performance gains across multiple cross-domain and cross-lingual benchmarks, which shows broad applicability and robustness.\n\n- The empirical analysis provides strong evidence that activation patterns encode transferable task information, which offers new insight into how internal representations can be reused across tasks.\n\n- The approach integrates smoothly with existing LLMs and inference pipelines, requiring no architectural modification or additional supervision, which increases its practical utility.\n\n- The visualization and qualitative analyses effectively illustrate how Fourier filtering separates task-agnostic and task-specific components, supporting the interpretability of the proposed method."}, "weaknesses": {"value": "- The method's theoretical justification is limited. The claim that low-frequency components capture task-agnostic semantics is supported only by empirical correlations -- specifically, the similarity analysis in Figure 3 and the performance comparisons in Figure 4, rather than quantitative causal ablations or theoretical derivations.\n- The sample selection strategy lacks clarity and robustness. The paper introduces \"diverse and influential\" sample selection (Section 3.3) but provides no sensitivity analysis or comparison to random or simpler baselines, which weakens its empirical foundation.\n- The evaluation is narrow in scope. Most experiments focus on a few NLU (Table 1) and translation datasets (Table 2), leaving open whether FAST generalizes to reasoning or multimodal adaptation.\n- The computational overhead is underexplored. Though the authors claim efficiency, they omit runtime or memory statistics for Fourier transforms and activation steering across layers, which makes the scalability claim unsubstantiated.\n- The ablation studies are insufficient to isolate design contributions. Table 3 reports limited variations, but it does not disentangle the effects of Fourier filtering, activation injection depth, and steering magnitude, so the relative importance of each component remains unclear."}, "questions": {"value": "How sensitive is FAST to the choice of frequency cutoff when separating low- and high-frequency components, and how should practitioners select this threshold for new tasks or models?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "6tMo51ck4j", "forum": "HFZhYvKM8V", "replyto": "HFZhYvKM8V", "signatures": ["ICLR.cc/2026/Conference/Submission10190/Reviewer_8p6w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10190/Reviewer_8p6w"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761975856885, "cdate": 1761975856885, "tmdate": 1762921555575, "mdate": 1762921555575, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper propose a method for cross-task transfer in prompted language models (which modifies the forward-pass only; no training is involved). The method involves (1) computing the difference vector of the activations on source tasks when given zero-shot prompt vs. few-shot labeled prompts, then (2) injecting this difference vector when solving the target task with zero-shot prompt. The rough idea is that few-shot demonstrations make the model to focus on specialized areas of internal knowledge for solving the task, and we want to induce the same behavior on the target task.\n\nThe proposed method has three components:\n1. Compute the activation difference vectors on (multiple) source tasks.\n    - The authors discussed a graph-based method (section 3.2) for selecting a small subset of the \"most useful\" examples for the few-shot demonstration.\n2. Factor the difference vector into low-frequency and high-frequency components (computed per-layer).\n    - They found the high-frequency component to capture more-specialized task-specific activations, hence may not be helpful for transfer when the tasks are dissimilar.\n3. Inject the difference vector into an intermediate layer when solving the target task."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The reviewer is not up-to-date on LLM steering methods, but the proposed method nicely combines few-shot learning, which is very helpful for IID performance but not transfer performance, with activation steering, which is more abstract but could be more \"generalizable\" as it operates in the latent semantic space.\n2. The proposed method is presented in a way that is easy to follow and understand, and intuitive."}, "weaknesses": {"value": "1. Based on discussions in Section 2, the premise of the method is that the difference vector is similar on similar tasks, hence injecting dv to the target task would simulate injecting few-shot demonstration from the target task.  So in order to determine whether the method is applicable in a given scenario, we need to know if the source and target tasks are similar.  For this, the authors (kind of) used task similarity in eq. (3) as a proxy for task similarity.\n    - It is unclear from the experiments, that besides from relying on domain knowledge and validation accuracy, whether there's a way to determine if the proposed method would succeed or fail.  The reviewer would have liked to see some failure cases to better understand the limitations of the proposed method: *If I apply this method to unrelated tasks, what will happen?*\n    - For example, the authors could evaluate all pairs of the tasks in table 1.\n2. The discussion in Section 2.2 is very confusing.  The message seems to be that the change in activation induced by adding few-shot demonstrations is **linearly transferable** (i.e., \"parallel\") across tasks.\n    - But (1) t-SNE is non-linear, so the reviewer is not sure where the conclusion on the differences being parallel comes from.\n3. The section on subset selection is a substantial part of the paper (section 3.2), but it appears highly heuristic and the reviewer is unsure of the motivation behind some of its design.\n    - The \"influence source\" seems to favor examples that are clustered together; why is this a desirable objective?"}, "questions": {"value": "See Weaknesses.\n\n1. Could the authors evaluate the few-shot demonstration selection method on the few-shot baselines?\n2. From evaluating the examples from these tasks, pages 19 and 20, a thing that stood out is that the answer formats are different (labels are ABCD, True/False, or positive/negative/neutral). The reviewer wonders if the few-shot/PEFT baselines can be improved if the answer labels can be made more compatible?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "OHPkk33lKD", "forum": "HFZhYvKM8V", "replyto": "HFZhYvKM8V", "signatures": ["ICLR.cc/2026/Conference/Submission10190/Reviewer_h5sS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10190/Reviewer_h5sS"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10190/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762887397859, "cdate": 1762887397859, "tmdate": 1762921554849, "mdate": 1762921554849, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}