{"id": "VD63CQ1aVl", "number": 18868, "cdate": 1758291619392, "mdate": 1763410878073, "content": {"title": "Towards Unified Dynamic Face Landmark Detection", "abstract": "Although advancements in face landmark detection (FLD) methods continue to push performance boundaries, they overlook two major functional limitations: (1) different network parameters need to be trained independently for each \"N-point\" benchmark dataset, and (2) a model trained on an \"N-point\" dataset reliably outputs only the N landmarks. In our work, we first conceptualize Face Part-Anchored Landmark Positions (FPALPs), wherein each landmark is treated as a progression value between zero (start) and one (end) along a face part's contour. Every landmark can be expressed in the FPALP format, irrespective of its source dataset, hence unlocking the ability to unify all \"N-point\" datasets into a single dataset. Secondly, we represent each landmark with an FPALP-based query, refine it progressively with a cross-modality decoder, and predict its coordinates based on the final representation. Our approach, called Unified Dynamic FLD, embodies these two design choices and streamlines the landmark detection pipeline by enabling (1) a single model to learn on any number of \"N-point\" datasets, and (2) yield any number of specific landmark predictions by loading the designated landmark queries at runtime. Extensive experiments carried out on several benchmark datasets demonstrate that our approach can achieve the above benefits while performing competitively with, if not better than, existing SOTA methods on individual- and cross-dataset evaluations.", "tldr": "We achieve (1) the ability to train a face landmark model on multiple datasets with different face landmark layouts and (2) unlimited on-demand landmark prediction. Our focus is not to outperform the SOTA but to competitively offer these benefits.", "keywords": ["face landmark detection"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8b58d933834a6c1763a37e30a625cbecf320753.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper addresses two fundamental limitations in face landmark detection (FLD): the need to train separate models for each \"N-point\" dataset and the inability of these models to output a flexible number of landmarks. It introduces a paradigm-shifting solution built on two core innovations. First, it proposes Face Part-Anchored Landmark Positions (FPALPs), a novel, universal representation that defines each landmark not by its coordinates, but by its normalized, semantic position along a face-part contour. Second, it presents a query-based detection framework that accepts FPALP-based queries to enable fully dynamic, on-demand landmark prediction.\n\nThis new paradigm successfully enables a single model to be trained on a unified fusion of heterogeneous datasets, and to dynamically predict any number of landmarks at inference time. Experiments demonstrate that this approach achieves these critical new capabilities while maintaining performance that is highly competitive with specialized, state-of-the-art methods.\n\nCrucially, the paper implicitly surfaces a fundamental challenge in the FLD task itself: the subtle inconsistencies in how different datasets define the precise contour shapes of face parts. The model's need to learn an \"average\" representation from these slightly varied annotations likely explains the minor performance gap compared to models trained on a single, self-consistent dataset. This is not a flaw of the method, but rather an inherent property of the data that this unified approach is the first to successfully navigate."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. This paper introduces a powerful conceptual shift for the field of face landmark detection (FLD). By proposing the Face Part-Anchored Landmark Positions (FPALPs) representation, it reframes landmark detection from a dataset-specific coordinate regression problem into a generalized, semantic-querying task. This is a highly elegant and impactful innovation that addresses long-standing issues of data fragmentation and model inflexibility.\n2. The FPALP concept directly unlocks three highly valuable capabilities that were previously unattainable with a single model:\na) Unified Multi-Dataset Training: It provides a principled framework for combining heterogeneous \"N-point\" datasets, significantly increasing the volume and diversity of data available for training a single, more robust model.\nb) Flexible, On-Demand Prediction: The query-based architecture allows for dynamic prediction of any number of landmarks at inference time, offering unprecedented flexibility for diverse downstream applications.\n3. The paper demonstrates, through rigorous experiments, that its unified and dynamic framework achieves performance that is highly competitive with specialized, state-of-the-art models. The fact that it incurs only a negligible performance drop while providing enormous gains in data utilization and output flexibility is a remarkable achievement and a testament to the soundness of the proposed approach."}, "weaknesses": {"value": "The unified training approach, by design, exposes the model to subtle but real inconsistencies in how different datasets define the exact contour shape of a given face part. This introduces a form of unavoidable \"label noise.\" The model's need to learn a generalized, \"average\" representation of each contour to accommodate this variance is likely the primary reason its performance, while highly competitive, does not exceed that of a specialist model trained on a single, perfectly self-consistent dataset. This is a fundamental trade-off between specialization and generalization, and the paper makes a compelling case for the immense value gained on the generalization front."}, "questions": {"value": "1. Your results show that the unified model is highly competitive, yet does not strictly surpass specialist SOTA models on their native datasets. We believe this is not a weakness. Would you agree that this highlights a fundamental trade-off between generalization (from diverse data) and specialization (on self-consistent data)? Framing your results in this light seems to strengthen, rather than weaken, your contribution, as it showcases the immense flexibility gained for a minimal cost in specialization.\n2. Following up on that, you astutely identified an average intra-cluster distance of 2.22 pixels when unifying the templates. Could one interpret this value as a proxy for the inherent \"label noise\" or \"ambiguity\" that exists between datasets? If so, would it be fair to say that your model, by learning a robust average representation, is performing optimally under this noisy supervision, and this inherent ambiguity itself constitutes the performance ceiling compared to a specialist model?\n3. Given that the core challenge you've surfaced is this inherent ambiguity in contour definitions, have you considered future work that models this uncertainty explicitly? For instance, instead of predicting a single coordinate (x, y) for a given FPALP query, could the framework be extended to predict a probability distribution or an uncertainty ellipse? This seems like a natural and exciting next step to create models that are not only unified but also aware of the data's intrinsic ambiguities."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "DKRlqVLEQy", "forum": "VD63CQ1aVl", "replyto": "VD63CQ1aVl", "signatures": ["ICLR.cc/2026/Conference/Submission18868/Reviewer_J6iZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18868/Reviewer_J6iZ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761728326342, "cdate": 1761728326342, "tmdate": 1762930833658, "mdate": 1762930833658, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Response to All Reviewers (8hur, yDUK, NXcn, J6iZ)"}, "comment": {"value": "We thank all reviewers, 8hur, yDUK, NXcn, and J6iZ, for their careful evaluation and constructive feedback. Across the reviews, several shared strengths were noted. Reviewers highlighted the value of FPALPs as a unified semantic representation for heterogeneous N-point datasets, the ability to specify arbitrary landmarks at test time, and the empirical competitiveness of the proposed framework. Reviewer 8hur specifically emphasized that generalization to arbitrary landmark definitions is an important problem for the community, that FPALPs offer a sound and effective solution validated through extensive experiments. Reviewer J6iZ provided a detailed analysis of the conceptual impact of FPALPs, the positive implications of unifying multiple annotation templates, and how the approach exposes fundamental data-level ambiguities that shape the generalization–specialization trade-off. \n\nThe concerns raised were well-defined and distinct across reviewers.\n\n- Reviewer 8hur asked about additional qualitative examples, clarification of the “face contour” phrase, applicability to non-contour points, and the role of image-aligned text encoders.\n- Reviewer yDUK raised questions regarding template alignment sensitivity, performance trade-offs on WFLW-68, occlusion handling, and multilingual considerations.\n- Reviewer NXcn requested deeper analysis around interpolation-based comparisons, start/end-point conventions across datasets, qualitative results under large pose, and justification for using text embeddings.\n- Reviewer J6iZ focused on interpreting dataset inconsistencies, analyzing the intra-cluster distance in the context of inter-dataset label noise/ambiguity, and future extensions involving explicit uncertainty modeling.\n\nWe address each of these points individually in the following rebuttal sections. With these clarifications, and given the consistently identified strengths of the formulation, generalization ability, and empirical performance of our method, we respectfully hope that the reviewers will consider recommending acceptance of the work once their specific concerns have been addressed."}}, "id": "lCdtE50zJ0", "forum": "VD63CQ1aVl", "replyto": "VD63CQ1aVl", "signatures": ["ICLR.cc/2026/Conference/Submission18868/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18868/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18868/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763407285050, "cdate": 1763407285050, "tmdate": 1763407285050, "mdate": 1763407285050, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified landmark representation, FPALP, and a corresponding dynamic landmark detection network built upon it. FPALP is defined based on the ratio of lengths along pre-defined facial part curves. The proposed detection framework consists of three components: (1) Image-Agnostic Landmark Encoding Generation, which produces embeddings for both facial parts and their ratio representations; (2) Landmark Query Initialization, which interacts with image features through an attention mechanism; and (3) Landmark Query Refinement, which decodes the extracted features into the final landmark coordinates. The framework can be trained on datasets with varying numbers of annotated landmarks rather than relying on datasets with a fixed number. Extensive experiments demonstrate that the method achieves competitive accuracy on individual datasets and superior performance in cross-dataset evaluations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe paper introduces FPALP, a unified landmark representation defined by evenly distributed points along facial part curves. This design enables the framework to be trained across multiple datasets (e.g., WFLW, 300W, AFLW-19) while producing outputs that are not constrained to a fixed number of landmarks.\n2.\tThe paper proposes a dynamic landmark detection framework based on FPALP, which predicts landmark positions using a fusion strategy that combines facial part embeddings and image features through a Dot-Product Attention Map and Deformable Image Cross-Attention.\n3.\tExtensive experiments demonstrate strong cross-dataset performance, and the ablation study is thorough, carefully examining both the image encoder and text encoder components."}, "weaknesses": {"value": "Major:\n1. The paper lacks experiments evaluating the accuracy of facial part curve prediction compared to interpolation-based methods. Since the core idea of FPALP is to model and predict facial part curves, such comparisons are essential to demonstrate FPALP’s effectiveness. The absence of these results leaves the validity of FPALP’s key contribution somewhat uncertain.\n2. The paper does not adequately address the misalignment issue across datasets. One of the underlying assumptions is that the landmarks in each dataset are evenly distributed along facial part curves; however, this is not strictly true for all datasets. Moreover, FPALP’s handling of the start and end points of each curve— which vary across datasets—remains unclear. Although the authors briefly discuss this limitation, the analysis lacks sufficient depth.\n3. The qualitative results focus primarily on frontal faces, with limited examples involving large poses. This raises concerns about the stability and robustness of FPALP under challenging geometric variations.\n\nMinor:\n1. Some parts of the mathematical formulation in the method section are unclear. For instance, the meaning of H_T in R^((H_T×W_ei×L)) (line 253) is not explained—possibly a typo? The paper would benefit from more explicit symbol definitions or improved figure annotations for clarity.\n2. The use of text descriptions for facial part phrases appears unnecessary given the small number of facial parts. Although the authors attempt to justify this choice in the ablation study by replacing the frozen text encoder with a learnable embedding, the mechanism of the learnable embedding is not sufficiently explained. If it is simply a non-frozen version of the text encoder, the benefit of textual representation becomes questionable. Using one-hot encodings for facial parts might be a simpler and equally effective alternative."}, "questions": {"value": "See the weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VSe3oow1Og", "forum": "VD63CQ1aVl", "replyto": "VD63CQ1aVl", "signatures": ["ICLR.cc/2026/Conference/Submission18868/Reviewer_NXcn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18868/Reviewer_NXcn"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761788671091, "cdate": 1761788671091, "tmdate": 1762930833155, "mdate": 1762930833155, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a unified and dynamic face landmark detection (FLD) framework based on Face Part-Anchored Landmark Positions (FPALPs), which represent landmarks as normalized progression values along semantic face part contours. The method enables training on multiple \"N-point\" datasets (e.g., AFLW, 300W, WFLW) simultaneously and supports dynamic, on-demand landmark prediction at inference. A cross-modality decoder refines landmark queries constructed from FPALPs and text embeddings. Experiments show competitive performance with state-of-the-art methods while offering enhanced flexibility and generalization."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. Unified Representation: The FPALP formulation effectively aligns heterogeneous landmark annotations across datasets, enabling a single model to learn from multiple sources without manual interpolation or 3D priors.\n2. Dynamic Inference: The framework supports arbitrary landmark queries at test time, allowing customizable output granularity and adaptability to diverse downstream tasks—a significant advantage over fixed-output models."}, "weaknesses": {"value": "1. Template Alignment Sensitivity: The method relies on approximate alignment of face templates across datasets, which may limit scalability when integrating new datasets with highly divergent landmark definitions or severe annotation inconsistencies.\n2. Performance Trade-offs: While competitive overall, the model exhibits a slight performance drop on certain subsets (e.g., WFLW68) when trained on fused datasets, suggesting sensitivity to dataset-specific challenges and sampling strategies."}, "questions": {"value": "1. How does the framework handle face parts that are heavily occluded or not present in the training data? The paper mentions future extensions but does not evaluate robustness under such conditions.\n2. Could the dependency on text encoders (e.g., SentenceBERT) introduce biases or limit performance in low-resource languages or domains where facial part semantics differ?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cWda58nB7P", "forum": "VD63CQ1aVl", "replyto": "VD63CQ1aVl", "signatures": ["ICLR.cc/2026/Conference/Submission18868/Reviewer_yDUK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18868/Reviewer_yDUK"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913204671, "cdate": 1761913204671, "tmdate": 1762930832279, "mdate": 1762930832279, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "Changes to the Paper"}, "comment": {"value": "Revision 1:\n\n1) Following Reviewer NXcn's review, we fixed the notation typo in L253.\n2) Following Reviewer 8hur and NXcn's recommendations, we added Figures 8 and 9 to the Appendix Section A.13 to demonstrate our model's ability to reason and predict landmarks under the challenging cases of occlusion and extreme pose cases respectively.\n3) Following our reply to Reviewer 8hur, we added L728-734 in the Appendix Section A.4 to further detail and reason our usage of SentenceBERT vs the CLIP-based FaRL text encoder.\n4) Following our reply to Reviewer yDUK, we added L699-705 in the Appendix Section A.2 to reflect the limitations of using the text encoder in low-resource language settings and mitigation strategies to improve the same.\n5) Following our reply to Reviewer NXcn, we added L775-783 in the Appendix Section A.6 to clarify that our FPALP formulation is agnostic to landmark sequences from the source dataset and provides flexibility by design as landmark membership to face parts can be non-exclusive in nature.\n6) Following our reply to Reviewer J6iZ, we added L864-871 in the Appendix Section A.11 to explain our analysis of the intra-cluster distance during the phase of the face template alignment of the source datasets."}}, "id": "ioKAgT8IbQ", "forum": "VD63CQ1aVl", "replyto": "VD63CQ1aVl", "signatures": ["ICLR.cc/2026/Conference/Submission18868/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18868/Authors"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18868/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763412539775, "cdate": 1763412539775, "tmdate": 1763417600122, "mdate": 1763417600122, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposed Face Part-Anchored Landmark Positions as a generalized representation for landmark, w.r.t. face part's contour.\nThis enables test-time specification of landmarks, with a single model (backbone and head) trained on any N-point datasets.\nExperiments prove this method generalizes while keep competitive result with existing SOTAs."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Generalization to arbitrary landmark definitions is an important problem for the community. FPALP is a sound solution, and the authors also conducted extensive experiments to validate its effectiveness. Also, the paper is well-written and easy to follow."}, "weaknesses": {"value": "As you mentioned in Sec 4 and A.10, WFLW dataset consists of extreme poses and occlusions. In A.7 you said those two factors can be challenging for your FPALP. It will be good if you can show some of those cases in addition to Figure 7."}, "questions": {"value": "1. Fig. 3 the meaning of \"Face Contour\" is unclear. \n2. Is FPALP limited to contour landmarks in theory? For example, can we extend this method to represent a landmark at the center of eyeball?\n3. As you mentioned at Line 242, if you believe facial layout semantics within text encoder is important, it will be helpful to show results where FPALP is trained with text encoder that was already aligned with image encoder, e.g., CLIP."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Q5L6xXMKYb", "forum": "VD63CQ1aVl", "replyto": "VD63CQ1aVl", "signatures": ["ICLR.cc/2026/Conference/Submission18868/Reviewer_8hur"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18868/Reviewer_8hur"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18868/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761972029988, "cdate": 1761972029988, "tmdate": 1762930831805, "mdate": 1762930831805, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}