{"id": "t0k90Fm7A1", "number": 18693, "cdate": 1758290196426, "mdate": 1759897087193, "content": {"title": "You Only Prune Once: A Zero-Shot, Data-Free Pruning at Initialization with Transferable Supermasks", "abstract": "Pruning at Initialization (PaI) accelerates training while maintaining accuracy, yet most criteria depend on data and backpropagation, leaving them brittle. Slight variations in random seed or sparsity budget reorder scores require re-scoring or iterative schedules and yield masks with weak transferability across seeds, datasets, and budgets. The proposed \\emph{You Only Prune Once} (\\textbf{YOPO}) framework addresses these limitations through a \\emph{zero-shot}, data and gradient-free design. YOPO computes a \\emph{once-only} saliency by fitting a nonnegative low-rank model to absolute weights at random initialization and measuring the element-wise Frobenius residual. Global or layer-wise thresholds generate masks with \\emph{exact} sparsity control and no layer collapse. Since ordering and budget are decoupled, the same saliency supports \\emph{re-thresholding} to any sparsity and \\emph{dataset transfer} without re-scoring, enabling reusable \"supermasks\". Experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet with standard CNN backbones show that YOPO matches or surpasses strong single-shot PaI baselines, rivals iterative/data-dependent methods despite using no data at initialization, and consistently outperforms expander-graph zero-shot PaI. Altogether, YOPO provides a scalable and intuitive approach to initialization-time pruning with stable transfer across seeds, datasets, and sparsity levels.", "tldr": "We propose a zero-shot, data-free neural network pruning at initialization method that uses low-rank residuals to compute a once-only saliency, yielding transferable supermasks across datasets and sparsity budgets.", "keywords": ["Pruning at Initialization (PaI)", "zero-shot pruning", "data-free pruning", "transferable supermasks"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/34500a98ef84fb36a01de957be820ce48a96f994.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a method for training sparse networks with Pruning at Initialization. The proposed method YOPO, relies solely on network topology and is data independent, hence it can be used across architectures and datasets. The method fits a low rank factorization to each weight and uses the residuals as a saliency score for threshold based pruning. Experiments are provided, comparing the proposed method against existing baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The method proposed is transferable across datasets and model architectures as it solely relies on topology, unlike some other PaI methods which rely on data for scoring parameters and masking them."}, "weaknesses": {"value": "1. Scoring: The authors propose using the low rank factorization as a way to eliminate tiny irregularities and thus identify the more important weights. The motivation behind this is unclear to me. This scoring model seems to promote weights which do not lie on the low-rank manifold, why this would help trainability of the sparse network has not been answered in the paper. Moreover, this seems contrary to recent work which suggests that a low rank approximation of weights carries most of the information. Do the authors have reason to believe this is not the case at initialization?\n2. The authors assume that identifying a sparse topology is enough to prune at initialization. However, there is a large amount of work on this, suggesting that topology alone is not sufficient to improve PaI performance, but the initialization of the topology is important [1-4], this aspect is not addressed by the authors.\n3. The experimental results, which are only provided on relatively tiny datasets, show that the proposed method is only better than other compared methods at low sparsities and is similar/worse at higher sparsities. The authors also do not compare with recent PaI methods like Phew, Prospr. Comparing especially with [7], which is also data-agnostic would be most pertinent.\n4. The high sparsity regimes are only compared different graph structures and not the other PaI methods, adding these comparisons would be necessary to evaluate the performance at high sparsity.\n\n\n[1] Zhou, Hattie, et al. \"Deconstructing lottery tickets: Zeros, signs, and the supermask.\" Advances in neural information processing systems 32 (2019).\n\n[2] Adnan, Mohammed, et al. \"Sparse Training from Random Initialization: Aligning Lottery Ticket Masks using Weight Symmetry.\"  ICML 2025.\n\n[3] Frankle, Jonathan, et al. \"Pruning Neural Networks at Initialization: Why Are We Missing the Mark?.\" International Conference on Learning Representations.\n\n[4] Gadhikar, Advait, and Rebekka Burkholz. \"Masks, Signs, And Learning Rate Rewinding.\" The Twelfth International Conference on Learning Representations.\n\n[5] Alizadeh, M., Tailor, S. A., Zintgraf, L. M., van Amersfoort, J., Farquhar, S., Lane, N. D., and Gal, Y. Prospect pruning: Finding trainable weights at initialization using meta-gradients. In International Conference on Learning Representations, 2022.\n\n[6] Patil, S. M. and Dovrolis, C. Phew: Constructing sparse networks that learn fast and generalize well without training data. In International Conference on Machine Learning, 2021.\n\n[7] Pham, H., Liu, S., Xiang, L., Le, D. D., Wen, H., Tran-Thanh, L., et al. Towards data-agnostic\npruning at initialization: What makes a good sparse mask? In Thirty-seventh Conference on Neural Information Processing Systems, 2023."}, "questions": {"value": "Can the auhors provide motivation for their low rank factorization and why this saliency criterion can help trainability of the sparse network?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Sk4ba5MbHC", "forum": "t0k90Fm7A1", "replyto": "t0k90Fm7A1", "signatures": ["ICLR.cc/2026/Conference/Submission18693/Reviewer_mba5"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18693/Reviewer_mba5"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761691400464, "cdate": 1761691400464, "tmdate": 1762928393707, "mdate": 1762928393707, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"You Only Prune Once\" (YOPO), a novel method for pruning neural networks at initialization (PaI). The core problem it addresses is the brittleness and high re-computation cost of existing PaI techniques, which are often dependent on specific data, random seeds, or sparsity targets. YOPO proposes a zero-shot, data-free, and gradient-free framework that computes a single, reusable \"supermask\" saliency score for a given network architecture. This is achieved by approximating the absolute weight matrix of each layer with a non-negative low-rank model (via Nonnegative Matrix Factorization) and using the element-wise residual as the saliency score. Masks for any desired sparsity level can then be generated by simply re-thresholding these fixed scores. The authors demonstrate through comprehensive experiments on CIFAR-10/100, Tiny-ImageNet, and ImageNet that YOPO is competitive with or outperforms strong data-dependent baselines and consistently surpasses other zero-shot methods, all while enabling seamless transfer of masks across different datasets and sparsity budgets without rescoring."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "Practicality and Efficiency: The \"prune once, reuse many\" paradigm is a major practical advantage, drastically reducing the computational overhead associated with finding good sparse masks for new tasks or sparsity requirements.\nStrong Empirical Performance: YOPO matches or surpasses strong single-shot PaI baselines and remains competitive with more expensive iterative and data-dependent methods, despite using no data or gradients for scoring. Its performance at extreme sparsities is particularly impressive.\nRobustness and Transferability: The core claims are backed by strong empirical evidence. The method demonstrates remarkable stability across different random initializations (seeds) and successfully transfers masks across different datasets with minimal performance loss.\nExcellent Writing and Clear Motivation: The paper is exceptionally well-written, making a complex topic accessible. The motivation for a data-free, transferable pruning method is clearly and convincingly articulated."}, "weaknesses": {"value": "Preprocessing Overhead: While computed only once, the NMF decomposition can be computationally intensive for very large network layers. The paper mentions this and suggests optimizations, but a more detailed analysis of the wall-clock time for this preprocessing step versus traditional methods would be beneficial.\nLimited to Unstructured Pruning: The method, in its current form, performs unstructured pruning. While academically common, the practical hardware speedups from unstructured sparsity can be limited. The authors acknowledge this as a direction for future work."}, "questions": {"value": "1.\tThe method is evaluated on CNNs. Have the authors investigated its applicability to other architectures like Transformers? How might the NMF-based saliency perform, and how would the preprocessing cost scale, given the typically larger, dense matrices in transformer models?\n2.\tThe intuition that NMF captures \"parts-based regularities\" is appealing. Could the authors provide any qualitative or visual analysis of the learned low-rank templates (V and H matrices) and the corresponding residuals (S) for a specific convolutional layer? This could offer deeper insight into why the method works so well.\n3.\tThe choice of NMF rank r is a hyperparameter. The ablation studies show robustness across a range of small values. Is there a more principled way or a reliable heuristic to guide the selection of r for a given architecture or layer, beyond sweeping a few values?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "L9XeuD5dAL", "forum": "t0k90Fm7A1", "replyto": "t0k90Fm7A1", "signatures": ["ICLR.cc/2026/Conference/Submission18693/Reviewer_QzLy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18693/Reviewer_QzLy"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761879622789, "cdate": 1761879622789, "tmdate": 1762928393235, "mdate": 1762928393235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces \"You Only Prune Once\" (YOPO), a data-free pruning method applied at network initialization. The authors identify three main limitations in existing pruning methods: they rely on dataset-specific importance scores, are sensitive to different random initializations, and require mask recomputation for different sparsity levels.\n\nYOPO calculates a data-independent importance score for each parameter at initialization, based on the low-rank reconstruction error of the non-negative weight matrix. Binary masks are created through a \"monotone re-thresholding\" process using MAD or STD-based threshold identification. To prevent layer collapse, the method maintains minimum connection requirements both at the input and output of neurons.\n\nThe authors evaluate their method through four key tests: comparison with existing pruning methods, cross-dataset transfer capability, consistency across different random initializations, and transferability across sparsity levels. Tests were conducted on various convolutional networks using different image classification datasets. Results demonstrate that YOPO creates better-performing sparse networks at initialization compared to existing methods, while maintaining effectiveness across different datasets, network initializations, and sparsity levels."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes a practical framework that follows a \"compute once, and reuse\" paradigm, which significantly improves the efficiency of network pruning implementations.\n- Presents a novel approach using NMF and residuals for saliency scoring\n- The experimental results are very strong and convincing, demonstrating clear superiority of the method in three key areas: Accuracy performance, transferability across different datasets, and consistency across different random initialization seeds."}, "weaknesses": {"value": "- While the saliency score computation method is novel, the paper provides insufficient justification for why higher residuals may result in better masks. The explanation offered is largely intuitive and lacks theoretic or empirical foundation.\n- The authors' claims regarding the drawbacks of existing pruning at initialization methods remain unsubstantiated. The paper should have: provided empirical evidence to support these criticisms by conducting experiments testing mask transferability across different random initializations and datasets. \n- Several of the propositions made in the paper are quite trivial, and some can be equally applied to other one-shot pruning at initialization methods.\n- The paper does not including several relevant data-free pruning at initialization methods (such as Gebhart 2021, Patil and Dovrolis 2021) that have demonstrated superior performance compared to the baselines used in this work."}, "questions": {"value": "- How does the weight distribution differ between pruned and unpruned weights?\n- What are the method's computational and storage requirements, particularly for saliency scores and threshold computations?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UGoLCa35wO", "forum": "t0k90Fm7A1", "replyto": "t0k90Fm7A1", "signatures": ["ICLR.cc/2026/Conference/Submission18693/Reviewer_FeFf"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18693/Reviewer_FeFf"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761966760922, "cdate": 1761966760922, "tmdate": 1762928392826, "mdate": 1762928392826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces YOPO, a way to prune neural networks right at initialization without using any data or gradients. This method uses a simple nonnegative matrix factorization on the absolute initial weights to find which connections are most distinctive. These “residual” weights are kept, while the rest are pruned using pruning thresholds (per-layer or global). The same scores can be reused across datasets and sparsity levels. The authors present experimental results on resnets and VGG, with Cifar and ImageNet datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "- The paper is well written and the method is clearly described both in text and algorithm\n- The code is available for reporoduciblity\n- The idea of using NMF for weight importance detection is novel, although not enough proven (see weaknesses)"}, "weaknesses": {"value": "My biggest issue with the paper is the experiments part. There are several issues with this section that prevents examining the true value of the proposed method:\n- The benchmarks used are out-dated. Using overparametrized models such as VGG and Resents on small datasets (Cifar 10and 100 and tiny ImageNet) for pruning does not provide enough evidence of the superiority of the method.\n- The results for unpruned models seem very low compared to the original public values (for example 68.69 for RN50 on ImageNet!)\n- The newer depth wise separable CNN models (like convnexts) are not included.\n\nOther weaknesses:\n- Unstructured pruning is known for limitation for having real efficiency advantages regarding FLOPs.\n- The Figure on page 5 does not have a number, and the part b seems to be distorted with a plot over two others."}, "questions": {"value": "In order to present enough evidence for the hypothesis that your YOPO method actually computes meaningful importance scores for weights at initialisation, I encourage the following experiment:\nTake a model and compute weight importances at initialisation with YOPO.\nWithout any pruning, train the model on ImageNet.\nAfter training, compute weight importance scores with a widely accepted method (or even YOPO again)\nCompare the scores before and after training. Are they widely consistent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "C8XksxwOP1", "forum": "t0k90Fm7A1", "replyto": "t0k90Fm7A1", "signatures": ["ICLR.cc/2026/Conference/Submission18693/Reviewer_tguL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18693/Reviewer_tguL"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission18693/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762077413330, "cdate": 1762077413330, "tmdate": 1762928392419, "mdate": 1762928392419, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}