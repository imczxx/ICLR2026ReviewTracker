{"id": "8zoxC9e23q", "number": 10400, "cdate": 1758169990686, "mdate": 1763648245926, "content": {"title": "Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function", "abstract": "Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose Soft Q-based Diffusion Finetuning (SQDF), a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.", "tldr": "SQDF is a KL-regularized RL method that fine-tunes diffusion models via reparameterized soft Q-gradients, achieving high rewards while avoiding reward over-optimization and preserving diversity.", "keywords": ["Diffusion Models", "RL Finetuning"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2f74d5a13fcdbaca7e712705f7235fe987bdb95d.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Soft Q-learning for Diffusion Fine-tuning (SQDF), a reinforcement learning–based framework designed to fine-tune diffusion models while mitigating reward over-optimization. SQDF leverages a KL-regularized RL objective, and introduces a discount factor for better credit assignment, a consistency model for improved Q-function approximation, and an off-policy replay buffer."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Nice Presentation: The paper addresses reward over-optimization, which is an important issue in diffusion model reinforcement learning. Moreover, the writing of this paper is very easy to understand.\n* Strong Theoretical Grounding: The method is derived from diffusion model MDP formulations, showing solid theoretical consistency. \n* Comprehensive Experiments: Results are thorough, including extensive qualitative and quantitative evaluations, and ablation studies."}, "weaknesses": {"value": "* Base Model: The paper adopts Stable Diffusion 1.5, an older base model, which raises concerns about the generalizability of the proposed method to more advanced diffusion architectures. \n* Clarity: Some equations and algorithm steps (e.g., Eq. 10-12, Line 14 of Algo.1) could be described more intuitively, as the current notation may make it harder for non-expert readers to follow."}, "questions": {"value": "There are several concerns that may significantly affect the overall evaluation of this paper. \n* In Section 4.1, what is the key difference between the proposed loss (Eq. 11) and the DPOK loss (Eq. 8 in [1])? It seems that the proposed loss allows the use of the reward gradient of the predicted $x_0$ at each timestep. However, the use of reward gradients has already been introduced in DRaFT [2], and the use of rewards on the predicted $x_0$ has been proposed in ReFL [3]. This makes Section 4.1 appear to be a combination of DPOK, DRaFT, and ReFL. \n* In Section 4.2.1, the authors introduce a discount factor to emphasize that later timesteps are more important. However, prior work has also introduced a discount factor but emphasized the earlier denoising steps instead (Eq. 1 in [4]). Could the authors clarify this contradiction? \n* In Section 4.2.2, the authors introduce a consistency model for better $x_0$ prediction。While the $x_0$ predicted by the consistency model may indeed be cleaner, it is no longer the prediction of the original model $p_\\theta$. Why should the reward obtained from this predicted $x_0$ be valid for updating $p_\\theta$? (Even though the images generated by the consistency model and the original model are largely similar, differences always exist in some details, and those details could be crucial for reward evaluation sometimes.) \n* In Section 4.2.3, the authors introduce a replay buffer. In the conclusion, the paper claims that the replay buffer can manage the reward–diversity trade-off. However, according to Appendix E.1, the replay buffer only weights each sample by $\\gamma^t r$, which seems to be equivalent to directly weighting the samples by $\\gamma^t r$ in the loss function. Could the authors provide a more detailed explanation about managing the reward-diversity trade-off? \n\n[1] DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models. \n[2] Directly Fine-Tuning Diffusion Models on Differentiable Rewards. \n[3] ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. \n[4] A Dense Reward View on Aligning Text-to-Image Diffusion with Preference."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Ysa0xsamFH", "forum": "8zoxC9e23q", "replyto": "8zoxC9e23q", "signatures": ["ICLR.cc/2026/Conference/Submission10400/Reviewer_5VJm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10400/Reviewer_5VJm"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761891887220, "cdate": 1761891887220, "tmdate": 1762921717320, "mdate": 1762921717320, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces Soft-Q Diffusion Finetuning (SQDF), a novel method for aligning diffusion models with downstream reward functions while mitigating over-optimization. The core innovation is a training-free approximation of the soft Q-function using a single-step posterior mean estimate, which enables direct gradient-based optimization without the instability of value network training or high-variance Monte Carlo estimators. Experiments on text-to-image tasks demonstrate that SQDF achieves superior reward-diversity trade-offs compared to existing RL-based and direct backpropagation methods."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. The training-free soft Q-function approximation eliminates the need for unstable value network training.\n\n2. The paper demonstrates effectiveness across multiple tasks (aesthetic scoring, human preference optimization, black-box settings) with thorough comparisons against relevant baselines and ablation studies validating each component.\n\n3. Results show SQDF achieves better Pareto frontiers, optimizing target rewards while maintaining significantly better alignment and diversity metrics compared to methods that suffer from semantic and diversity collapse."}, "weaknesses": {"value": "1. While using the consistency model to estimate the future value is intriguing, it also makes one wonder what about directly fine-tuning the consistency model, which could potentially achieve similar results more efficiently given its single-step generation capability. Also see Q1. \n\n2. While methodologically sound, the paper provides little discussion of computational overhead, training time comparisons, or memory requirements."}, "questions": {"value": "Why did you choose the two-model architecture instead of directly fine-tuning the consistency model? Given that consistency models can generate samples using multiple steps, did you explore whether similar reward-diversity trade-offs could be achieved more efficiently by fine-tuning the consistency model directly?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "J4L0Cys7yv", "forum": "8zoxC9e23q", "replyto": "8zoxC9e23q", "signatures": ["ICLR.cc/2026/Conference/Submission10400/Reviewer_3B3B"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10400/Reviewer_3B3B"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761997049804, "cdate": 1761997049804, "tmdate": 1762921716926, "mdate": 1762921716926, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on fine-tuning diffusion models with downstream objectives while mitigating reward over-optimization. The proposed method solves KL-regularized off-policy RL with stabilized value estimation for controlling reward-diversity tradeoff. The method is compared with prior fine-tuning works on image generation with various rewards."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Reward over-optimization has been a well-known problem in diffusion fine-tuning, and the paper proposes a well-structured method with results to support their claims.\n2. Each component of the method is well-motivated, and evidences are provided through ablation studies.\n3. Both quantitative and qualitative results show better reward-alignment / reward-diversity compared to prior works."}, "weaknesses": {"value": "1. Lack of fine-tuning baseline: [1] have already proposed RL fine-tuning that focuses on mitigating over-optimization.\n2. Lack of training-free baselines: [2] has shown that training-free methods can mitigate over-optimization compared to fine-tuning, and [3], [4] evidence soft value function can also be used for training-free methods.\nWithout comparison with these methods or justification statements, it's unclear why fine-tuning is necessary.\n\n[1] Zhang, Ziyi, et al. \"Confronting reward overoptimization for diffusion models: A perspective of inductive and primacy biases.\" arXiv preprint arXiv:2402.08552 (2024).\n[2] Kim, Sunwoo, Minkyu Kim, and Dongmin Park. \"Test-time Alignment of Diffusion Models without Reward Over-optimization.\" The Thirteenth International Conference on Learning Representations.\n[3] Li, Xiner, et al. \"Derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding.\" arXiv preprint arXiv:2408.08252 (2024).\n[4] Uehara, Masatoshi, et al. \"Inference-time alignment in diffusion models with reward-guided generation: Tutorial and review.\" arXiv preprint arXiv:2501.09685 (2025)."}, "questions": {"value": "1. There is no 'Q-learning' in the method since it approximates the value function using a consistency model. The name of the method (Soft Q-learning for Diffusion Finetuning) may be misleading.\n2. Using a frozen consistency model can introduce a different type of approximation bias due to the mismatch between the posterior mean of the fine-tuned diffusion and the consistency model. Why not fine-tune the consistency model together?\n3. What's the purpose of the online black-box optimization experiment? The paper lacks an explanation of which aspect of SQDF this experiment is trying to demonstrate."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9YAAUNGZ3v", "forum": "8zoxC9e23q", "replyto": "8zoxC9e23q", "signatures": ["ICLR.cc/2026/Conference/Submission10400/Reviewer_TRJT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10400/Reviewer_TRJT"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998296970, "cdate": 1761998296970, "tmdate": 1762921716467, "mdate": 1762921716467, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- Problem: when you RL diffusion models it overoptimizes the reward and mode collapses and stops producing coherent images.\n- Solution:\n  - KL regularization to the original model (also common in past work)\n  - Soft-Q function (this is a normal Q-fn with an entropy term)\n  - Discount factor for better credit assignment\n  - Consistency models for more stable RL training\n  - Off-policy replay buffer - prevent mode collapse\nAlgorithm works like this:\n  - Generate samples from diffusion model, store them in the replay buffle\n  - Sample one of the partially noised samples, denoise it 1 step.\n  - Then take the consistency model and denoise the rest of the way to a clean image.\n  - Run the clean image through the RM.\n  - Backprop using policy gradient.\nExperiments\n  - 3 things they want to show (a) still good samples (b) less semantic collapse (c) less diversity collapse.\n  - To measure semantic, they use pretrained prompt-image alignment models like ImageReward.\n  - To measure diversity, they look at the mean pairwise distance as computed with 2 measures: LPIPS and Dreamsim features cosine similarity.\n  - Their experimental results look pretty legit:\n     - Show there’s an alignment-diversity tradeoff, but their method expands the pareto frontier\n     - Shows that there is an aesthetic-alignment tradeoff, but they minimize and ~almost remove it. They are slightly better than ReFL and much better than the other methods.\n     - Shows there is an aesthetic-diversity tradeoff and they’re way better than other methods.\n  - Ablations show the discount helps a bunch. It looks like the buffer helps with diversity (but slightly hurts reward) and the CM helps with reward (but slightly hurts diversity). But maybe these differences are small enough it’s noise?"}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "- Substantial baselines - they compared against several past works. They tested whether a simple addition to baselines (adding KL regularization) would improve them, and found that their method was still superior.\n- Ablations - removed each component they added to confirm that it contributes to the final result.\n- Clarity - the paper is clear and easy to read. The diagrams make sense.\nSignificant improvement above prior work, which is visible both in the qualitative results and the graphs."}, "weaknesses": {"value": "See “questions” section for more details.\n\nIt is not clear to me that the minor improvements in diversity produced by the buffer (.01 to .02 in Table 2) is worth the extra complexity adding it to the algorithm requires and the hit to aesthestic score.\n\nSimilarly it seems like without the consistency model diversity improves (though this is a minor effect, the improvements to aesthetic score I think suggest the CM is still a good contribution to the algorithm)."}, "questions": {"value": "- In Fig 3, how did you get the diff points on each curve?\n- Table 1 - what is the number in parens. (confidence interval? standard deviation?)\n- Are the qualitative results randomly sampled or cherry-picked?\n- I suggest moving Algorithm 1 to the main paper, it was very helpful in understanding the algorithm."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ee4U8oSxRN", "forum": "8zoxC9e23q", "replyto": "8zoxC9e23q", "signatures": ["ICLR.cc/2026/Conference/Submission10400/Reviewer_M6wy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10400/Reviewer_M6wy"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10400/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762124739968, "cdate": 1762124739968, "tmdate": 1762921716029, "mdate": 1762921716029, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"comment": {"value": "We sincerely thank the review committee for their thoughtful and constructive feedback. We appreciate the recognition of our paper’s strengths, consistently emphasized across reviewers: **Strong empirical results** (M6wy, TRJT, 3B3B, 5VJm), **Thorough ablations** (M6wy, TRJT, 3B3B, 5VJm), **Important problem setting** (TRJT, 5VJm), **Substantial baselines** (M6wy, 3B3B), and **Clarity of presentation** (M6wy, 5VJm)\n\nIn response to the reviewers' feedback, we provide a brief summary of the additional major experiments and modifications:\n\n---\n### Additional Experiments\n\n**Expanding SQDF to more advanced diffusion models**\n\n- We evaluate SQDF on SDXL, confirming that SQDF reliably optimizes the target reward while mitigating over-optimization across different backbone sizes.\n\n\n**Additional baselines**\n\n- We add both a fine-tuning baseline (TDPO) and training-free baselines (SVDD, DAS), and show that SQDF achieves consistently better performance.\n\n---\n\n### Revisions\n\n- Section 4.1: Expand explanation of the reparameterized policy gradient.\n- Figure 3: Add DDPO as a baseline to the HPS optimization task.\n- Appendix F: Add uncurated samples for transparency.\n- Algorithm 1: Correct minor errors and improve clarity.\n\n---\n\nWe thank the reviewers again for their constructive feedback and hope that the revisions and additional results sufficiently address all concerns."}}, "id": "Vi6D512HDo", "forum": "8zoxC9e23q", "replyto": "8zoxC9e23q", "signatures": ["ICLR.cc/2026/Conference/Submission10400/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10400/Authors"], "number": 8, "invitations": ["ICLR.cc/2026/Conference/Submission10400/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763646969249, "cdate": 1763646969249, "tmdate": 1763648596517, "mdate": 1763648596517, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}