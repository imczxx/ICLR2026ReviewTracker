{"id": "CZydMj0vBc", "number": 11082, "cdate": 1758188811157, "mdate": 1759897609780, "content": {"title": "Learning Jump-Diffusion Dynamics from Irregularly-Sampled Data", "abstract": "Accurately modeling time-continuous stochastic processes from irregular observations remains a significant challenge. In this paper, we leverage ideas from generative modeling of image data to push the boundary of time series generation. \nFor this, we find new generators of SDEs and jump processes, inspired by trajectory flow matching, that have the marginal distributions of the time series of interest. \nSpecifically, we can handle discontinuities of the underlying processes by parameterizing the jump kernel densities by scaled\nGaussians that allow for\nclosed form formulas and hence rapid evaluation of the corresponding\nKullback-Leibler divergence in the loss. \nUnlike most other approaches, we are able to handle irregularly sampled time series. We underline our theoretical results by numerical experiments involving combinations of jumps and SDE dynamics.", "tldr": "", "keywords": ["generative modelling", "irregular time series", "jump processes", "stochastic differential equations"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/2bd289325b3932fd61ed768236a974981432334c.pdf", "supplementary_material": "/attachment/d72ca2b1ccdf2790029ff98d9b71988920b4bf55.zip"}, "replies": [{"content": {"summary": {"value": "A paper proposing a diffusion model for irregularly sampled time series. The key innovation is the proposal of jump-diffusion transition kernels to model the bridge probability in a way that is independent of the sampling time of the data. The paper is primarily theoretical and has a limited evaluation on synthetic data, but it does use one semi-realistic data set consisting of a moving square."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "A mathematically rigorous paper trying to extend generative diffusion modelling to a non-trivial scenario of irregularly sampled time series. The quality of the work is sound and the mathematical details are given in depth."}, "weaknesses": {"value": "- The paper is way too dense to expect someone who is not in the same niche to be able to grasp much, particularly given the very short time given to reviewers (who also have another four papers and, occasionally, a day job). I appreciate that the topic is highly technical, but the notation and style of presentation would have been significantly improved by some streamlining.\n- The main technical contribution appears to be a regularisation in the bridge probability, is that so? It is somewhat difficult to assess exactly what was already done in the very recent papers by Holderrieth and Zheng, and what was innovation in this paper.\n- Sometimes there is some sloppiness in the use of the word \"process\", for example Alg. 1 is a procedure to sample trajectories (approximately), which in most literature is distinct from the process (infinite dimensional object).\n- The memory aspect is another place where greater clarity would be needed, it's even unclear what it's meant (my impression was that the statistics of the project were obtained by some moving average over the trajectory but  as the topic is important its relegation to a short paragraph was definitely too little) \n- The empirical section was very schematic, which might be fine for a strongly theoretical paper but perhaps something that would showcase better the advantages of including jumps would have helped, as the performance is often close to the diffusion method."}, "questions": {"value": "- It seems to me that the trajectories might be irregularly sampled, but all trajectories must be sampled at the same times, is that so?\n- Could you clarify a bit more how memory is introduced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QIFeOCLHxy", "forum": "CZydMj0vBc", "replyto": "CZydMj0vBc", "signatures": ["ICLR.cc/2026/Conference/Submission11082/Reviewer_7wcJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11082/Reviewer_7wcJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972282174, "cdate": 1760972282174, "tmdate": 1762922263999, "mdate": 1762922263999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors apply a generator learning technique to the task of irregular time series generation. The authors derive the neccessary loss for such an application, and show performance in synthetic settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work is technically solid, the derivations are well written and clearly presented. \n2. The writing and presentation of the paper is clear, it is easy for readers to understand."}, "weaknesses": {"value": "Minor: \n1. Slight notation inconsistency, e.g $\\Delta$ and $\\nabla^2$ are both used for hessian in different places. \n2. The paper assumes an identity diffusion coefficient, limiting the modeling flexibility. Although this is relatively standard in this type of research, the diffusion coefficient can be easily modeled by a neural network.\n\nMajor:\n1. The work lacks novelty. This is perhaps the most concerning weakness. The work seems to be taking Holderrieth 2025's idea and apply it to the irregular time series modeling setting in TFM (Zhang, 2024). \n2. Related to the previous points, it will be more convincing that the work is empirically solid by repeating some numerical experiment in TFM. Most of the experiments now only show that it improves in synthetic scenarios. \n3. The first two points combined lead to a relatively weak motivation to apply generator learning (with jump processes) in such a specific field of application."}, "questions": {"value": "1. Can the author explain why having no singularities during the observed points of irregularly sampled data is beneficial? Is it solely to construct a full probabilistic view, or are there some particular benefits to having $P_0, P_1$ not being the observed points, i.e does it improve robustness, help with uncertainty quantification, etc?\n2. Can the author explain why having a jump process during the generator learning is necessary? Aside from the benefit during synthetic data modeling, does it actually help (or perhaps at least does not deteriorate) in modeling full continuous time series?\n3. The parameter $\\eta^2 = 0.3$ seems small compared to the synthetic data's scale, leaving a rather smooth path to model; doesn't this limit the usage of SDE? What happens if we leave $\\eta^2 = 1$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "91SvSO5Hib", "forum": "CZydMj0vBc", "replyto": "CZydMj0vBc", "signatures": ["ICLR.cc/2026/Conference/Submission11082/Reviewer_VY4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11082/Reviewer_VY4r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602081755, "cdate": 1761602081755, "tmdate": 1762922263612, "mdate": 1762922263612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method that estimates jump diffusions from observational data. By using existing known generators of certain classes of jump diffusion processes, the authors propose transforming those such that their time marginals match the observational data. This is similar to the techniques used in stochastic interpolants which maps one marginal distribution with known properties to another. The authors leverage an Ito-Levy type decomposition to represent the continuous and pure jump parts. The key component is to parameterize the jump component as a Gaussian distribution which allows the use of KL divergence for training. The authors then describe how to estimate the related generator given observations of irregularly sampled time series. Finally, the authors consider a few numerical examples on the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose an elegant formulation that matches a given base generator to a target generator that supports jump processes.\n\nThe authors find initial distributions that are amenable to sampling from. \n\nThe authors find a workaround to compute the jump measure more effectively than in existing works."}, "weaknesses": {"value": "The central contribution is largely a combination of the work of Zhang et al and Holderrieth et al to consider generator matching with jumps. \n\nThe numerical results appear to be inconclusive as to the efficacy of the method, which is not something to be concerned about, but it would be good to see which scenarios the method does perform well in as a comparison. This leads to a question on where the method should be employed what set of tasks. This was not evident in the main text. \n\nThe numerical experiments do not illustrate the importance of the jump component, which should be studied."}, "questions": {"value": "Can the authors detail the differences and the techniques needed to bridge Zhang el al to Holderrieth et al? \n\nIâ€™m struggling a bit to understand what the correct use-case of this method would be. The authors motivate with financial data or possibly limit order book data which is irregularly sampled, but there are a series of methods that could work in such scenarios. Can the authors comment on this a bit more, why and where the particular method would work well?\n\nAre there more obvious jump related data that the authors could consider? This would go a long way in motivating the use case of the method.  \n\nIs it limiting to consider functions $r$ that are parameterized by a Gaussian?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JnW6BddkOE", "forum": "CZydMj0vBc", "replyto": "CZydMj0vBc", "signatures": ["ICLR.cc/2026/Conference/Submission11082/Reviewer_gjfR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11082/Reviewer_gjfR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799151067, "cdate": 1761799151067, "tmdate": 1762922263030, "mdate": 1762922263030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}