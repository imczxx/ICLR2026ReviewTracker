{"id": "CZydMj0vBc", "number": 11082, "cdate": 1758188811157, "mdate": 1763632065013, "content": {"title": "Learning Jump-Diffusion Dynamics from Irregularly-Sampled Data", "abstract": "Accurately modeling time-continuous stochastic processes from irregular observations remains a significant challenge. In this paper, we leverage ideas from generative modeling of image data to push the boundary of time series generation. \nFor this, we find new generators of SDEs and jump processes  for conditional interpolation matching the marginal distributions of the time series of interest. \nSpecifically, we can handle discontinuities of the underlying processes by parameterizing the jump kernel densities by scaled\nGaussians that allow for\nclosed form formulas and hence rapid evaluation of the corresponding\nKullback-Leibler divergence in the loss. \nUnlike most other approaches, we explicitly account for both irregular and non-aligned sampling times in constructing the generators. We also clarify several theoretical aspects that arise specifically in the irregular setting, leading to a more robust formulation of the model.  We underline our theoretical results by numerical experiments involving combinations of jumps and SDE dynamics that illustrate the benefits of the proposed framework.", "tldr": "", "keywords": ["generative modelling", "irregular time series", "jump processes", "stochastic differential equations"], "primary_area": "probabilistic methods (Bayesian methods, variational inference, sampling, UQ, etc.)", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d512f070485a5ee823841dbfeeeae725237786b7.pdf", "supplementary_material": "/attachment/d72ca2b1ccdf2790029ff98d9b71988920b4bf55.zip"}, "replies": [{"content": {"summary": {"value": "A paper proposing a diffusion model for irregularly sampled time series. The key innovation is the proposal of jump-diffusion transition kernels to model the bridge probability in a way that is independent of the sampling time of the data. The paper is primarily theoretical and has a limited evaluation on synthetic data, but it does use one semi-realistic data set consisting of a moving square."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "A mathematically rigorous paper trying to extend generative diffusion modelling to a non-trivial scenario of irregularly sampled time series. The quality of the work is sound and the mathematical details are given in depth."}, "weaknesses": {"value": "- The paper is way too dense to expect someone who is not in the same niche to be able to grasp much, particularly given the very short time given to reviewers (who also have another four papers and, occasionally, a day job). I appreciate that the topic is highly technical, but the notation and style of presentation would have been significantly improved by some streamlining.\n- The main technical contribution appears to be a regularisation in the bridge probability, is that so? It is somewhat difficult to assess exactly what was already done in the very recent papers by Holderrieth and Zheng, and what was innovation in this paper.\n- Sometimes there is some sloppiness in the use of the word \"process\", for example Alg. 1 is a procedure to sample trajectories (approximately), which in most literature is distinct from the process (infinite dimensional object).\n- The memory aspect is another place where greater clarity would be needed, it's even unclear what it's meant (my impression was that the statistics of the project were obtained by some moving average over the trajectory but  as the topic is important its relegation to a short paragraph was definitely too little) \n- The empirical section was very schematic, which might be fine for a strongly theoretical paper but perhaps something that would showcase better the advantages of including jumps would have helped, as the performance is often close to the diffusion method."}, "questions": {"value": "- It seems to me that the trajectories might be irregularly sampled, but all trajectories must be sampled at the same times, is that so?\n- Could you clarify a bit more how memory is introduced?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "QIFeOCLHxy", "forum": "CZydMj0vBc", "replyto": "CZydMj0vBc", "signatures": ["ICLR.cc/2026/Conference/Submission11082/Reviewer_7wcJ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11082/Reviewer_7wcJ"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760972282174, "cdate": 1760972282174, "tmdate": 1762922263999, "mdate": 1762922263999, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"title": {"value": "General Answer"}, "comment": {"value": "We thank the reviewers for their constructive and helpful comments. \nWe have revised the manuscript carefully to address all concerns and looking forward for discussions.\nModifications in the revised version are marked in blue. Apart from this general answer please find individual answers to each report below.\n\nSorry for not clearly committing the new contributions of our paper. We like to emphasize that our work **goes far beyond a simple combination of Generator Matching and Trajectory Flow Matching (TFM)** which we explain briefly below.\nWe have rewritten the introduction accordingly.\n\n\n- If  generator matching is straightforwardly applied \nto irregularly sampled time series, **a functional version of generator matching would be required**. To the best of our knowledge, such a version has only been explored in the context of functional matching for deterministic flows (cf. Kerrigan et al.), and on equispaced data. In contrast, we adopt an autoregressive perspective  and **approximate the generator locally in time**. This introduces several nontrivial challenges that are specific to jump processes. Unlike flow-based models, where the objects of interest are vectors, **jump processes involve measures, which require an entirely different treatment of the loss function and optimization**.\n\n- **In standard generator matching, jump measures were approximated by naive binning**, which scales poorly and prevents closed-form computation of the training losses. These losses must then be evaluated numerically, which is inefficient and unstable.\n**Our contribution is to approximate the jump measures by a parametric family of distributions for which the losses can be derived analytically (see Propositions 5 and 6)**. This provides a substantial computational speed-up and a more stable training objective.\n\n- A second fundamental drawback of both generator matching and TFM is that the reference generators for both the diffusion and jump components **become singular at the endpoint of the process**. This singularity arises because Gaussian base distributions have unbounded support, while empirical data typically lie on compact domains.\n**We address this by modifying the reference processes: specifically, we augment the data with small perturbations (Proposition 2)** and incorporate it formally into the theoretical derivation of the reference processes and to provide exact modified formulations.\n\n- We offer **the first mathematically precise explanation of why the memory mechanism works (Proposition 7)**  and  give theoretical justification for the use of memory in irregularly sampled generator training.\n\nSecondly we comment here on potential **use-cases of the method**, which is a second issue raised by the reviewers. We emphasize that **jump processes in Euclidean space represent an entirely new class of generative models**. So far this was only considered in Holderrieth et al., and their exploration is still at an early stage.  Our work introduces several theoretical and computational improvements that make these models practically viable. \n\n **The broader motivation for incorporating jumps into time series modeling is that many real-world stochastic systems exhibit genuine temporal discontinuities.** Modeling these with purely continuous diffusions introduces bias and underestimates tail risk. Examples could be found in finance (sudden price jumps, volatility spikes), neuroscience (spiking neural activity) or climate/physical systems (abrupt regime shifts or threshold-driven events). Our method lays the basis for practical applications where both irregular sampling and discontinuous latent dynamics are intrinsic features of the data. **Empirically the combination of jump and diffusion (SDE) components generally performs best in our examples, often by a noticeable margin.**"}}, "id": "5D3EsXUCuC", "forum": "CZydMj0vBc", "replyto": "CZydMj0vBc", "signatures": ["ICLR.cc/2026/Conference/Submission11082/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11082/Authors"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11082/-/Official_Comment"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763631548066, "cdate": 1763631548066, "tmdate": 1763631936683, "mdate": 1763631936683, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Comment", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors apply a generator learning technique to the task of irregular time series generation. The authors derive the neccessary loss for such an application, and show performance in synthetic settings."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work is technically solid, the derivations are well written and clearly presented. \n2. The writing and presentation of the paper is clear, it is easy for readers to understand."}, "weaknesses": {"value": "Minor: \n1. Slight notation inconsistency, e.g $\\Delta$ and $\\nabla^2$ are both used for hessian in different places. \n2. The paper assumes an identity diffusion coefficient, limiting the modeling flexibility. Although this is relatively standard in this type of research, the diffusion coefficient can be easily modeled by a neural network.\n\nMajor:\n1. The work lacks novelty. This is perhaps the most concerning weakness. The work seems to be taking Holderrieth 2025's idea and apply it to the irregular time series modeling setting in TFM (Zhang, 2024). \n2. Related to the previous points, it will be more convincing that the work is empirically solid by repeating some numerical experiment in TFM. Most of the experiments now only show that it improves in synthetic scenarios. \n3. The first two points combined lead to a relatively weak motivation to apply generator learning (with jump processes) in such a specific field of application."}, "questions": {"value": "1. Can the author explain why having no singularities during the observed points of irregularly sampled data is beneficial? Is it solely to construct a full probabilistic view, or are there some particular benefits to having $P_0, P_1$ not being the observed points, i.e does it improve robustness, help with uncertainty quantification, etc?\n2. Can the author explain why having a jump process during the generator learning is necessary? Aside from the benefit during synthetic data modeling, does it actually help (or perhaps at least does not deteriorate) in modeling full continuous time series?\n3. The parameter $\\eta^2 = 0.3$ seems small compared to the synthetic data's scale, leaving a rather smooth path to model; doesn't this limit the usage of SDE? What happens if we leave $\\eta^2 = 1$?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "91SvSO5Hib", "forum": "CZydMj0vBc", "replyto": "CZydMj0vBc", "signatures": ["ICLR.cc/2026/Conference/Submission11082/Reviewer_VY4r"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11082/Reviewer_VY4r"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761602081755, "cdate": 1761602081755, "tmdate": 1762922263612, "mdate": 1762922263612, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors propose a method that estimates jump diffusions from observational data. By using existing known generators of certain classes of jump diffusion processes, the authors propose transforming those such that their time marginals match the observational data. This is similar to the techniques used in stochastic interpolants which maps one marginal distribution with known properties to another. The authors leverage an Ito-Levy type decomposition to represent the continuous and pure jump parts. The key component is to parameterize the jump component as a Gaussian distribution which allows the use of KL divergence for training. The authors then describe how to estimate the related generator given observations of irregularly sampled time series. Finally, the authors consider a few numerical examples on the proposed method."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The authors propose an elegant formulation that matches a given base generator to a target generator that supports jump processes.\n\nThe authors find initial distributions that are amenable to sampling from. \n\nThe authors find a workaround to compute the jump measure more effectively than in existing works."}, "weaknesses": {"value": "The central contribution is largely a combination of the work of Zhang et al and Holderrieth et al to consider generator matching with jumps. \n\nThe numerical results appear to be inconclusive as to the efficacy of the method, which is not something to be concerned about, but it would be good to see which scenarios the method does perform well in as a comparison. This leads to a question on where the method should be employed what set of tasks. This was not evident in the main text. \n\nThe numerical experiments do not illustrate the importance of the jump component, which should be studied."}, "questions": {"value": "Can the authors detail the differences and the techniques needed to bridge Zhang el al to Holderrieth et al? \n\nIâ€™m struggling a bit to understand what the correct use-case of this method would be. The authors motivate with financial data or possibly limit order book data which is irregularly sampled, but there are a series of methods that could work in such scenarios. Can the authors comment on this a bit more, why and where the particular method would work well?\n\nAre there more obvious jump related data that the authors could consider? This would go a long way in motivating the use case of the method.  \n\nIs it limiting to consider functions $r$ that are parameterized by a Gaussian?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "JnW6BddkOE", "forum": "CZydMj0vBc", "replyto": "CZydMj0vBc", "signatures": ["ICLR.cc/2026/Conference/Submission11082/Reviewer_gjfR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11082/Reviewer_gjfR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11082/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761799151067, "cdate": 1761799151067, "tmdate": 1762922263030, "mdate": 1762922263030, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}