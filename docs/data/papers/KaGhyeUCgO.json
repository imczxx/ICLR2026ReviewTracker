{"id": "KaGhyeUCgO", "number": 7479, "cdate": 1758024044694, "mdate": 1763706816394, "content": {"title": "Towards Understanding Primacy and Recency Effects in Mamba: A Mechanistic Perspective", "abstract": "We uncover a sparse subset of channels in Mamba's selective state-space block that serves as a substrate for early-input retention. Identified through structured recall tasks, ablating these channels selectively degrades early positional recall. Input periodicity systematically shifts Mamba’s discretization gate, amplifying the “lost-in-the-middle” effect by reallocating information across positions. Primacy and periodicity-driven effects, combined with recency, yield the characteristic U-shaped recall curve, aligning with effects known in Transformers but underexplored in state-space models. We further examine how distractor tokens affect Mamba’s temporal dynamics: recency, sustained by an exponential-decay mechanism, collapses under distraction as it moves the queried items deeper in the sequence. Finally, we demonstrate that the same sparse subset of channels transfers beyond recall. Intervening on them degrades the performance on downstream long-context understanding tasks, indicating that they function as data-agnostic long-term memory carriers. These results provide a common mechanistic picture of Mamba’s temporal profile, linking primacy, recency, and input periodicity.", "tldr": "", "keywords": ["Primacy", "Recency", "Mamba", "State-space Models"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/e4860e8035034d1be83ef686d46ef4a5eb953f03.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper identifies a subset of channels in Mamba models that are responsible for primacy and recency effects in recall tasks. These effects have previously been explored both behaviorally and mechanistically in Transformers, but are understudied in SSMs. They also show that recency effects \"collapse\" when the sequence model is shown distractor tokens, and that ablating the identified channels degrades performance on tasks \"beyond\" recall (e.g. long-context tasks)."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "* The mechanistic story is quite compelling, and I think that the experiments are well done\n* I think it's really cool to see interp work done on non-Transformer models!"}, "weaknesses": {"value": "* The writing could use significant clarification; it took me a couple read throughs of the introduction to understand the main points (and I'm still not confident in my understanding). I also think that things should be restructured a bit, e.g. the experimental setup for 4.2/4.3/4.4 should be more clearly segmented from the results.\n* I don't particularly know if these results are very significant. Primacy/recency effects are _interesting_, but it's not clear that developing a mechanistic understanding of these effects has any larger bearing on the field. I think the experiments that explore the effects of intervention on the recall-relevant channels are more interesting in this regard, but they aren't scoped out to the extent that I'd find compelling."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "KvWHINEtPB", "forum": "KaGhyeUCgO", "replyto": "KaGhyeUCgO", "signatures": ["ICLR.cc/2026/Conference/Submission7479/Reviewer_xxn1"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7479/Reviewer_xxn1"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission7479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967701257, "cdate": 1761967701257, "tmdate": 1762919596177, "mdate": 1762919596177, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates position-dependent recall phenomena (primacy and recency effects) in Mamba, a state-space model architecture. The authors identify a sparse subset of channels responsible for early-input retention, demonstrate how input periodicity exacerbates \"lost-in-the-middle\" effects through the discretization mechanism, and show that recency emerges from exponential decay dynamics."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The long-term memory channels identification method is principled and is based on the cumulative recurrence matrix product.\n\n2. Showing that identified channels impact real long-context benchmarks strengthens the claim that these are functionally important, not just artifacts of the synthetic task.\n\n3. The paper is generally well-written with effective visualizations"}, "weaknesses": {"value": "1. Section 4.4 on recency largely reiterates known exponential decay dynamics from prior work (Wang et al., 2025). The distractor experiment adds empirical support but offers no new mechanistic insight. This section feels underdeveloped compared to the primacy analysis.\n\n2. Mechanistic claims about $Δ_t$ and periodicity are underdeveloped."}, "questions": {"value": "1. In Equation 3, you compute the product of diagonal entries. But $A^{(i)}_t ∈ R^{N×N}$ could have off-diagonal structure. How do you ensure you're only extracting diagonal recurrence?\n\n2. For the initialization experiment (Figure 5a), why only Layer 31? Is this a long-term channel layer? What happens in other layers?\n\n3. Table 1: The random ablation sometimes outperforms targeted ablation (e.g., 2WikiMultihopQA). This is counterintuitive—how do you explain this?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Ph7M4YXVzW", "forum": "KaGhyeUCgO", "replyto": "KaGhyeUCgO", "signatures": ["ICLR.cc/2026/Conference/Submission7479/Reviewer_WrNK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7479/Reviewer_WrNK"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission7479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762206689285, "cdate": 1762206689285, "tmdate": 1762919595601, "mdate": 1762919595601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on the recency (high recall on recent tokens) and primacy (high recall on initial tokens) effects in SSMs. The main contribution is the finding that only a sparse subset of channels carries information from the first few tokens to the end of the sequence. The authors support this hypothesis with experiments on various datasets and models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The authors uncover a primacy mechanism in Mamba, where only a sparse subset of channels carries information from the first few tokens to the end of the sequence. \n- They then design a probing experiment to validate this claim."}, "weaknesses": {"value": "- The empirical validation is somewhat limited in scope, as the experiments are confined to two specific model instances (Falcon Mamba 7B and Mamba 1.4B). \n- The paper's primary contribution appears to be incremental, as it largely builds upon existing frameworks. The central new discovery, while interesting, is presented without a substantial theoretical justification or a deep analysis of its underlying principles. The paper would be significantly strengthened by either providing a more rigorous theoretical grounding for this finding or by conducting a more in-depth empirical analysis.\n\nMinor:\n- Given that the paper's central focus is on the primacy effect, the authors might consider refining the title and abstract to more explicitly reflect this emphasis.\n- The authors should consider exploring whether these effects influence overall model ability (e.g., performance on downstream tasks). If a significant impact is identified, the paper would be substantially strengthened by proposing or evaluating methods to mitigate these sequential biases."}, "questions": {"value": "The original paper on recency finding [1] visualizes log-influential scores as part of its analysis. This paper, however, focuses solely on accuracy metrics. Could the authors elaborate on the decision to omit an analysis of log-influential scores?\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "OJ7aQN1rK8", "forum": "KaGhyeUCgO", "replyto": "KaGhyeUCgO", "signatures": ["ICLR.cc/2026/Conference/Submission7479/Reviewer_PZgk"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7479/Reviewer_PZgk"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission7479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762594119527, "cdate": 1762594119527, "tmdate": 1762919594949, "mdate": 1762919594949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study investigates the primacy and recency effects (the \"U-shaped recall curve\") in SSMs. The authors focus on the primacy mechanism, identifying a sparse subset of internal channels that function as a long-term memory. They demonstrate that ablating these specific channels selectively degrades the model's recall of early information and impairs its overall performance on long-context tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "+ The authors test their sparse channel hypotheses under multiple conditions, strengthening their conclusions. This includes varying sequence lengths, comparing \"Repeated Relation\" inputs to \"Random Relation\" inputs, and using \"distractor tokens\" to test the fragility of the recency effect."}, "weaknesses": {"value": "- The contribution can be somewhat limited. The paper's core novelty rests on identifying the \"sparse channel hypothesis\". However, the recency and primacy effects themselves are well-studied phenomena. The authors also acknowledge their observations are \"common\" (ln 24) and \"investigated in early studies\" (ln 447).\n\n- Another central weakness is that the paper successfully demonstrates that this hypothesis holds but does not explore the reason or rationale why it exists. The contribution would be substantially stronger if the authors provided a theoretical grounding or an investigation into the architectural properties of the model that give rise to these observed sparse channels.\n\n- While authors reveal the issues with Mamba, there is no solution provided. Do authors believe these problems are inherent and cannot be relieved easily?"}, "questions": {"value": "1. Have authors tried to reproduce the observation for more advanced SSM/linear attention architectures, e.g., Mamba2, DeltaNet?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cY559ipy5u", "forum": "KaGhyeUCgO", "replyto": "KaGhyeUCgO", "signatures": ["ICLR.cc/2026/Conference/Submission7479/Reviewer_EWwn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission7479/Reviewer_EWwn"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission7479/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762595596162, "cdate": 1762595596162, "tmdate": 1762919594530, "mdate": 1762919594530, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}