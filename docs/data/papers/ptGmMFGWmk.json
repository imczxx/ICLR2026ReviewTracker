{"id": "ptGmMFGWmk", "number": 4430, "cdate": 1757679703197, "mdate": 1759898032805, "content": {"title": "ResWorld: Temporal Residual World Model for End-to-End Autonomous Driving", "abstract": "The comprehensive understanding capabilities of world models for driving scenarios have significantly improved the planning accuracy of end-to-end autonomous driving frameworks. However, the redundant modeling of static regions and the lack of deep interaction with trajectories hinder world models from exerting their full effectiveness. In this paper, we propose a Temporal Residual World Model (TR-World), which focuses on dynamic object modeling. By calculating the temporal residuals of BEV features, the information of dynamic objects can be extracted without relying on detection and tracking. TR-World only takes temporal residuals as the input to make more precise predictions of the dynamic objects' future spatial distribution. By combining the prediction with the static object information contained in the current BEV features, accurate future BEV features can be obtained. Furthermore, we propose Future-Guided Trajectory Refinement (FGTR) module, which conducts interaction between prior trajectories (predicted from the current scene representations) and the future BEV features. This enables effective utilization of future road conditions and also alleviates world model collapsing. Comprehensive experiments conducted on the nuScenes and NAVSIM datasets demonstrate that our method, namely ResWorld, achieves state-of-the-art performance on planning accuracy. Code will be made publicly available.", "tldr": "", "keywords": ["Autonomous Drivng", "End-to-End", "World model"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/a8764ee895b721f92405d1a7994336c9cf255595.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a residual-based world model for end-to-end autonomous driving planning. Unlike previous works that predict future ego-centric image frames or BEV features, this approach centers on the current time step and leverages the differences among multiple BEV feature frames to make the model focus more on dynamic object motion while reducing redundant predictions of static elements. A deformable attention mechanism is then used to interact the prior trajectory with the predicted future BEV features to generate the final planning trajectory. The method demonstrates noticeable improvements on the nuScenes and NAVSIM datasets."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The idea of utilizing the static information from the current frame to avoid redundant predictions of static objects in future frames, thereby focusing more on dynamic targets, is very insightful.\n2. On the recent popular NAVSIM benchmark, the proposed method achieves a noticeable improvement in planning performance compared to previous approaches."}, "weaknesses": {"value": "1. The statements in the abstract, line 101, and the experimental tables suggesting that the proposed method does not rely on auxiliary tasks are somewhat misleading, since the BEV encoder still requires supervision from detection and mapping tasks during training.\n2. A major weakness of the paper is the lack of evaluation on inference speed. Given that the proposed method introduces multiple designs for feature interaction and adopts a two-stage trajectory generation process, it is unclear whether these additions incur significant computational overhead.\n3. Including ablation studies on the NAVSIM dataset would make the results more convincing. The nuScenes dataset contains a large proportion of straight-driving scenarios, and its metrics are already close to saturation, making it difficult to determine whether the reported improvements come from the proposed design or just statistical variation."}, "questions": {"value": "In line 269, the paper states that “Experiments confirm that not supervising $B_{future}$ enables higher planning performance.” As I understand it, $B_{future}$ is obtained by fusing the current BEV features with the temporally redundant dynamic features from historical frames. It is therefore unclear why $B_{future}$ can “preserve the spatial distribution of dynamic objects across multiple future timestamps.” In my view, since end-to-end autonomous driving is a multi-task framework where different tasks can influence each other, using the ground truth BEV features as supervision provides a very dense supervisory signal, which may actually interfere with the planning performance. Therefore, I doubt the claimed reason may not be the true cause of the observed improvement."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TrYsodUMJk", "forum": "ptGmMFGWmk", "replyto": "ptGmMFGWmk", "signatures": ["ICLR.cc/2026/Conference/Submission4430/Reviewer_podK"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4430/Reviewer_podK"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission4430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761138965397, "cdate": 1761138965397, "tmdate": 1762917359387, "mdate": 1762917359387, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ResWorld, an end-to-end autonomous driving system centering on a Temporal Residual World Model that selectively models dynamic objects via temporal residuals in BEV representations, reducing redundancy and improving dynamic prediction. The system further proposes a Future-Guided Trajectory Refinement mechanism that explicitly interacts predicted trajectories with future BEV features, aiming to improve planning accuracy and avoid “world model collapse.” Experiments on nuScenes and NAVSIM benchmarks show that ResWorld achieves superior performance."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Temporal residuals in BEV separate dynamic from static content, alleviating redundant static region modeling and weak interaction between the world model and trajectories.\n- FGTR explicitly couples predicted trajectories with future BEV features under supervision, improving robustness.\n- Experiments on Nuscenes and NavSim demonstrate the superiority of the proposed methods."}, "weaknesses": {"value": "- ResWorld utilizes GeoBEV to generate high-quality BEV features. Compared to other methods, especially the baseline SSR, this approach may be considered unfair. Moreover, the resolution of the BEV features and other hyperparameters, such as the number and dimensions of scene tokens, are not clearly defined, making it challenging to practically verify the effectiveness of the proposed method.\n- The analysis and conclusions presented in Table 4 are both confusing and overstated, lacking sufficient experimental evidence. For example, the results in Table 4 mainly show that the impact of future supervision on planning is minimal. This makes it difficult to conclude that TR-World focuses more on dynamic targets or produces more accurate BEV features. Furthermore, the claim that TR-World retains more detailed information about the spatial distribution of dynamic targets is not adequately supported by experimental data.\n- There is a lack of experiments on a closed-loop benchmark."}, "questions": {"value": "- Why do the collision rate results of SSR reported in this paper differ so significantly from those in the original publication?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0OM1ioWtzF", "forum": "ptGmMFGWmk", "replyto": "ptGmMFGWmk", "signatures": ["ICLR.cc/2026/Conference/Submission4430/Reviewer_PiGQ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4430/Reviewer_PiGQ"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission4430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761654586003, "cdate": 1761654586003, "tmdate": 1762917359080, "mdate": 1762917359080, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes ResWorld, a world model for end-to-end autonomous driving. It presents two main contributions: 1) The Temporal Residual World Model (TR-World), which separates dynamic and static information by calculating the \"temporal residual\" of BEV features. This allows the world model to focus solely on modeling dynamic objects, avoiding redundant computation on static scenes. 2) The Future-Guided Trajectory Refinement (FGTR) module, which uses the future feature maps predicted by TR-World to explicitly correct and optimize a \"prior trajectory,\" resulting in a safer final plan."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The FGTR module establishes an explicit feedback loop between the planner and the world model, using predicted future information to correct the current plan, which is logically clear.\n2. The method achieves promising results on both the nuScenes (open-loop) and NAVSIM (closed-loop) benchmarks."}, "weaknesses": {"value": "1. A major weakness lies in the experimental evaluation. In the NAVSIM closed-loop tests, the authors admit (Sec 4.2) to not using the core TR-World module, leaving its closed-loop effectiveness unverified.\n2. TR-World's residual calculation is highly sensitive to the stability of the BEV features themselves. If the underlying BEV encoder is unstable between frames, its \"feature noise\" will be conflated with \"true motion,\" leading to an unreliable residual signal.\n3. The core assumption of TR-World (residual = dynamic) has a critical safety flaw. As admitted in Appendix B.3, the method fails to model static-but-soon-to-move objects, such as vehicles waiting at a red light."}, "questions": {"value": "1. While the authors state in Sec 4.2 that history frames were omitted for a fair comparison, I believe a closed-loop test of the core innovation (TR-World) is necessary. Can the authors provide closed-loop results for TR-World on NAVSIM to truly validate its contribution?\n2. Regarding Weakness 2, how do the authors ensure the learned BEV features are temporally stable? If the features are unstable, how does the model distinguish between residuals from \"feature jitter\" and residuals from \"true object motion\"?\n3. TR-World treats static-but-soon-to-move objects (like stopped cars and pedestrians) as static background. This is a critical safety failure. How do the authors plan to address this fundamental flaw? \n4. How does the model handle future static road structure? If the world model only predicts dynamic residuals and static information comes from the current BEV, how does the vehicle acquire information about the static road ahead when turning or entering an area not covered by the current field of view?\n5. The paper argues that the lack of future supervision is an advantage, but this may results in uninterpretability. Could the authors discuss the potential benefits of incorporating explicit future supervision, such as future point cloud prediction [1-2] or future BEV maps/occupancy, to provide a more interpretable signal to the world model?\n\n[1] ViDAR: Visual Point Cloud Forecasting. CVPR 24\n[2] HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation. ICCV 25"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "sFcMo51MBp", "forum": "ptGmMFGWmk", "replyto": "ptGmMFGWmk", "signatures": ["ICLR.cc/2026/Conference/Submission4430/Reviewer_xmSF"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission4430/Reviewer_xmSF"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission4430/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761967399859, "cdate": 1761967399859, "tmdate": 1762917358846, "mdate": 1762917358846, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}