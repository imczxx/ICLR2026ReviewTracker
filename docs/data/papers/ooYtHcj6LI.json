{"id": "ooYtHcj6LI", "number": 13086, "cdate": 1758213424353, "mdate": 1759897466289, "content": {"title": "EarthMind: Leveraging Cross-Sensor Data for Advanced Earth Observation Interpretation with a Unified Multimodal LLM", "abstract": "Earth Observation (EO) data analysis is vital for monitoring environmental and human dynamics. Recent Multimodal Large Language Models (MLLMs) show potential in EO understanding but remain restricted to single-sensor inputs, overlooking the complementarity across heterogeneous modalities. We propose EarthMind, *a unified vision-language framework* that handles both *single- and cross-sensor* inputs via an innovative hierarchical cross-modal attention (*i.e.*, HCA) design. Specifically, HCA hierarchically captures visual relationships across sensors and aligns them with language queries, enabling adaptive fusion of optical and Synthetic Aperture Radar (SAR) features. To support cross-sensor learning, we curate *FusionEO*, a 30K-pair dataset with diverse annotations, and establish *EarthMind-Bench*, a 2,841-pair benchmark with expert annotations for perception and reasoning tasks. Extensive experiments show that EarthMind achieves state-of-the-art results on EarthMind-Bench and surpasses existing MLLMs on multiple EO benchmarks.", "tldr": "", "keywords": ["Earth Observation", "Cross-sensor fusion", "Multimodal LLMs"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/3b0ca3103724dda1cf9d2d26338a8a497c77dfb9.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces EarthMind, a method for optical–SAR fusion built around a hierarchical cross-modal attention module and a standard <SEG>-to-mask interface for dense prediction. It also releases two resources: FusionEO for instruction tuning and EarthMind-Bench for evaluation across reasoning and segmentation. Experiments show improvements on the proposed benchmark"}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper articulates optical–SAR complementarity and quantifies optical bias via a Modality Attention Score.\n2. EarthMind-Bench supports single/multi-sensor inputs across reasoning and segmentation tasks; the model claims SoTA there with only 4B params."}, "weaknesses": {"value": "1. Novelty is incremental relative to the LISA/“<SEG>-to-mask” family. Architecturally, EarthMind follows the now-standard LLM + visual tokens + <SEG>-triggered mask decoder route\n2. HCA is essentially a two-stage cross-attention followed by a learned fusion weight. This is close to standard cross-modal attention + late gating used in prior VLM/VQA and multi-sensor fusion work. The paper’s contribution therefore lies more in systemization and dataset/benchmarking than in a fundamentally new mechanism for fusion.\n3. The paper cites 30K for FusionEO in the method overview but 20K in the conclusion—this discrepancy must be resolved.\n4. EarthMind-Bench has 2,841 pairs; this is valuable but relatively small for broad claims about multi-sensor reasoning. Please discuss geographic coverage, scene diversity, and train/val/test partition strategy\n5. The narrative emphasizes multi-sensor gains; please also report how EarthMind fares against specialized single-sensor EO MLLMs on their native optical-only or SAR-only benchmarks beyond qualitative examples."}, "questions": {"value": "see Weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "SMddlu3ury", "forum": "ooYtHcj6LI", "replyto": "ooYtHcj6LI", "signatures": ["ICLR.cc/2026/Conference/Submission13086/Reviewer_D1SB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13086/Reviewer_D1SB"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission13086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761463192456, "cdate": 1761463192456, "tmdate": 1762923813179, "mdate": 1762923813179, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces the first optical–SAR instruction fine-tuning benchmarks and the corresponding method EarthMind. Extensive experiments demonstrate the effectiveness of the proposed method. But there are some necessary concerns that should be addressed"}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "A new multi-source (RGB-SAR) cross-modality dataset. \n\nDetailed experimental evaluation"}, "weaknesses": {"value": "The proposed method is not novel and lacks substantial contributions. The method largely follows the LISA-style VLM, with only a prepending of RGB and SAR attention and text-guided visual–language attention. \n\nThe proposed dataset is relatively small (only 2.8k pairs), which in my view is insufficient to convincingly assess generalization. In addition, the manuscript does not provide essential SAR image details, such as band and polarization mode.\n\nThe proposed video-like stitching processing method lacks comparison with the traditional method of encoding separately by mode and then fusing.\n\nMissing computational cost and delay reports."}, "questions": {"value": "Please refer to the points in Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "vnNvzkep1s", "forum": "ooYtHcj6LI", "replyto": "ooYtHcj6LI", "signatures": ["ICLR.cc/2026/Conference/Submission13086/Reviewer_ZimX"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13086/Reviewer_ZimX"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission13086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761466311066, "cdate": 1761466311066, "tmdate": 1762923812816, "mdate": 1762923812816, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces EarthMind, a multimodal large language model designed for Earth Observation tasks, fusing optical and SAR data through a Hierarchical Cross-modal Attention (HCA) mechanism, which basically applies bidirectional cross-attention within each sensor and cross-attention with the text prompt. The authors curate FusionEO, a instruction-tuning dataset, and EarthMind-Bench, a benchmark with annotated samples for perception and reasoning tasks."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "-\tIntruduces a bias for modalities to focus more attention on the SAR data. This apprach shows strong generalization for both modalities.\n-\tThe model achieves good results compared to other MLLMs on wide range of tasks.\n-\tThe authors provide a new benchmark dataset that evaluates multi-sensor tasks in EO scenarios which is very interesting."}, "weaknesses": {"value": "-\tThe architecture is not well described, particularly the mask decoder and the handling of data input. \n-\tData and pre-training procedure lack detailed explanation and illustrative examples. \n-\tThe evaluation does not represent a true zero-shot setting in all experiments: Appendix G mentions that BigEarthNet (BEN), SoSAT-LCZ42, and other EO datasets are used in pre-training, while these datasets are also used for evaluation and in EarthMind-Bench. Even with sampling from different splits can inflat performance gains because you probably use similar prompts and answers and it is not a fair comparison for other models. Also, BEN used random sampling without spatial buffering means test images may be very similar to training images.\n-\tFigures could be styled better—issues include inconsistent alignment and spacing, mismatched example images, non-harmonized color themes, and mislabeling (e.g., Table 3 right is actually a figure)."}, "questions": {"value": "-\tDo you think “hierarchical” is the correct term for your attention approach? It seems more like a weighted combination rather than a hierarchical structure.\n-\tThe code is shared, but will the model weights and the dataset also be open-sourced? \n-\tYou state that no LLMs were used for grammar or clarity checks. Would you consider revising this? Using LLMs for language polishing could improve readability and your paper is basically about MLLMs.\n-\tMy main concern is about the similarity between pre-training and evaluation data, which may lead to unfair comparisons. Can you comment on how you mitigate this issue and ensure that performance gains are not due to similar samples and prompts?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QCx35aY4YG", "forum": "ooYtHcj6LI", "replyto": "ooYtHcj6LI", "signatures": ["ICLR.cc/2026/Conference/Submission13086/Reviewer_SQ8w"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13086/Reviewer_SQ8w"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission13086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761678891665, "cdate": 1761678891665, "tmdate": 1762923812462, "mdate": 1762923812462, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper focuses on cross-sensor remote sensing with optical and SAR imagery and develops EarthMind, a unified multimodal large language model. It employs hierarchical cross-modal attention (HCA) to first align and interact between optical and SAR features, then performs text-conditioned adaptive weighting. A special [SEG] token is introduced to place pixel-level referring segmentation and vision–language dialogue within a single inference framework, covering both single/multi-sensor inputs and multi-granularity tasks. To support training and evaluation, the authors construct FusionEO and EarthMind-Bench, on which they systematically validate the approach."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.Use a single LLM to unify single/multi-sensor inputs and multi-granularity tasks, and integrate segmentation seamlessly into the inference pipeline via the [SEG] token.\n\n2.Define the MAS metric to quantify modality attention shares; empirically show that naive concatenation is biased toward the optical modality, and use HCA for targeted debiasing to achieve more balanced and efficient multimodal fusion.\n\n3.With only 4B parameters, it is strongly competitive on public benchmarks such as AID, UC-Merced, RRSIS-D, and RefSegRS."}, "weaknesses": {"value": "1. Evaluating open-ended tasks relies on GPT-4 as a judge, which inevitably introduces scoring bias and prompt sensitivity in reproduction.\n\n2. The pixel-level part of EarthMind-Bench is mainly referring segmentation with only 438 samples, so coverage of fine-grained pixel tasks is limited.\n\n3. Training heavily depends on general natural-image corpora with EO-domain adaptation afterward; this “general-first, adapt-later” pipeline may leave residual cross-domain gaps.\n\n4. Segmentation is triggered by [SEG], if the token is not produced at inference, the target is deemed absent, this generation-gated design risks misses.\n\n5. Most baselines are restricted to RGB, while EarthMind can use full-spectrum channels, conflating “ability to ingest more channels” with algorithmic merit and raising fairness concerns.\n\n6. MAS is a share-of-attention statistic; using it to infer modality contribution is not equivalent to causal importance, so it is a somewhat indirect bias diagnostic."}, "questions": {"value": "1.MAS reflects attention share rather than causal effect. Could you validate it with counterfactual experiments (e.g., feature shuffling or training with one modality removed) and report the correlation between MAS and the observed performance drop?\n\n2.If the model fails to emit [SEG] at inference you treat the target as absent. Is there a calibrated fallback mechanism?\n\n3.The three-stage pipeline may leak ROI priors from masks into the text. Could you provide a variant without mask hints and compare?\n\nPlease respond to the weaknesses and the questions. If you resolve these concerns, I will raise my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "TK6bLHvvtx", "forum": "ooYtHcj6LI", "replyto": "ooYtHcj6LI", "signatures": ["ICLR.cc/2026/Conference/Submission13086/Reviewer_xNPN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13086/Reviewer_xNPN"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission13086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761807913531, "cdate": 1761807913531, "tmdate": 1762923812124, "mdate": 1762923812124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes EarthMind, a unified multimodal large language model for multi-sensor Earth observation. It integrates optical and SAR data through a hierarchical cross-modal attention mechanism that adaptively fuses complementary information from different sensors. The authors also introduce two supporting datasets: FusionEO for instruction tuning and EarthMind-Bench for evaluation across perception and reasoning tasks. Experiments show that EarthMind achieves state-of-the-art performance on multiple benchmarks, indicating the potential of the proposed framework for handling complex EO scenarios."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "The paper addresses a relevant problem in multimodal Earth observation by exploring adaptive fusion of optical and SAR data. The proposed HCA (Hybrid Cross-Attention) mechanism and FusionEO benchmark are reasonably designed and provide a useful reference for multimodal learning in remote sensing. The work is generally well-motivated, and the experimental setup is clear and systematic. The introduction of the MAS (Modality Attention Score) offers a straightforward way to interpret attention allocation across modalities, which can help improve model transparency. The paper is overall well organized and presents its methodology and results clearly, making it relatively easy to follow."}, "weaknesses": {"value": "（1）The authors emphasize the contribution of multi-sensor data, even specifically noting in the Introduction (line 44) that Sentinel-2 can provide high-resolution multispectral imagery. In the Related Work section (lines 135–136), they also highlight that *EarthDial* (CVPR 2025) utilizes multiple modalities, including multispectral, hyperspectral, and synthetic aperture radar (SAR) data. These details suggest that the authors are well aware of the importance of spectral sensor data.\n\nHowever, throughout the preceding sections, the authors never clarify why their work emphasizes multi-sensor fusion yet does not involve spectral data, which I find questionable. Furthermore, in both the Introduction and the training data description, I see no mention or contribution related to spectral data. Yet, in the methodology section, the authors suddenly mention grouping multispectral bands into triplets to construct multi-frame sequences (line 174). Table 1, however, contains no evaluation related to spectral data. This inconsistency in presentation is highly confusing.\n\nWhat makes this even more perplexing is that, in Figure 1, when introducing the contribution of *EarthMind*, the authors repeatedly highlight differences from SAR and optical models, yet they do not compare their approach with models like *EarthDial*, which incorporate both SAR and optical modalities *as well as* spectral data. This deliberate avoidance of existing, more comprehensive baselines—while selectively comparing only against weaker models—feels disappointing.\n\n（2）Additionally, another claimed contribution concerns the evaluation benchmark. The authors mention *GEOBench-VLM* (ICCV 2025) in the related work, but they describe it merely as a multi-task geospatial benchmark, ignoring the fact that it also supports multi-sensor scenarios. As explicitly stated in the Table 1of GEOBench-VLM , *GEOBench-VLM* includes Optical, Multispectral, SAR, Multi-temporal, and Bi-temporal modalities. By deliberately downplaying the contributions of prior work, the authors give a misleading impression of novelty, which I also find disappointing."}, "questions": {"value": "Specially, this paper makes a visible attempt to enhance multimodal fusion between optical and SAR imagery, yet it falls short in several fundamental aspects.\n\n(1) The claim that HCA achieves balanced modality attention leading to better fusion is weakly supported. MAS is a purely descriptive metric measuring attention uniformity, without causal or statistical evidence linking balance to performance. No correlation analysis or controlled ablation is provided, so the argument remains speculative.\n\n(2) Although the paper presents the framework as a “multi-sensor” system, all experiments and data pipelines are confined to the Optical–SAR pair. No evidence is provided that the proposed HCA mechanism can generalize to other sensors (e.g., MSI, HSI, IR), which are essential in Earth observation. This limitation substantially narrows the claimed generality of the method. Including at least a proof-of-concept experiment or visualization on a third modality (e.g., MSI) would be necessary to justify the “multi-sensor” claim.\n\n(3) The FusionEO dataset is problematic in design—its heavy reliance on GPT-generated captions without any systematic quality control, annotation validation, or bias analysis raises serious concerns about data reliability and reproducibility.\n\n(4) The model design relies on multiple vision encoders, resulting in significant computational overhead. While the authors acknowledge this in Appendix C, no quantitative analysis (e.g., FLOPs, runtime, GPU memory) is provided. This omission makes it difficult to assess whether the reported performance gains are achieved through architectural innovation or simply increased compute. A detailed comparison of efficiency against comparable baselines (e.g., EarthDial, SkySenseGPT) would be necessary to justify the proposed design.\n\n\nQuestions \n\n1.How can the authors provide stronger evidence that HCA’s balanced attention leads to better fusion, beyond the descriptive MAS metric?\n\n2.Since experiments are limited to the Optical–SAR pair, how can the “multi-sensor” generality claim be justified? Any evidence on other modalities (e.g., MSI, HSI, IR)?\n\n3.How is the quality and reliability of the GPT-generated FusionEO dataset ensured without human validation or bias analysis?\n\n4.Can the authors provide quantitative efficiency comparisons (FLOPs, runtime, memory) to show that improvements are not merely due to higher computation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "b2AmXHSOu4", "forum": "ooYtHcj6LI", "replyto": "ooYtHcj6LI", "signatures": ["ICLR.cc/2026/Conference/Submission13086/Reviewer_GP5Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission13086/Reviewer_GP5Q"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission13086/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989259562, "cdate": 1761989259562, "tmdate": 1762923811750, "mdate": 1762923811750, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}