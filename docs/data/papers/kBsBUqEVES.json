{"id": "kBsBUqEVES", "number": 10559, "cdate": 1758175521267, "mdate": 1763070411898, "content": {"title": "Track4Animate3D: Animating Any 3D Model via Multi-View Diffusion with Point-Tracking Motion Priors", "abstract": "Generating 4D objects is challenging because it requires jointly maintaining appearance and motion consistency across space and time under sparse inputs, while avoiding artifacts and temporal drift. \nWe hypothesize that this view discrepancy stems from supervision that relies solely on pixel- or latent-space video-diffusion losses and lacks explicitly temporally aware tracking guidance at feature-level.\nTo address this issue, we introduce \\emph{Track4Animate3D}, a two-stage framework that unifies a multi-view video diffusion model with a foundation point tracker and a hybrid 4D Gaussian Splatting (4D-GS) reconstructor.\nThe core idea is to explicitly inject motion priors from a foundation point tracker into the feature representation for both video generation and 4D-GS.\nIn Stage One, we impose dense, feature-level point correspondences within the diffusion generator, enforcing temporally consistent feature representations that suppress appearance drift and strengthen cross-view coherence. \nIn Stage Two, we reconstruct a dynamic 4D-GS using a hybrid motion representation that concatenates co-located diffusion features (carrying tracking priors from Stage One) with Hex-plane features, and appends a 4D Spherical Harmonics modeling, improving higher-fidelity dynamics and illumination modeling.\n\\emph{Track4Animate3D} outperforms strong baselines (e.g., Animate3D, DG4D) across VBench metrics for multi-view video generation, CLIP-O/F/C metrics and user preference studies for 4D generation, producing temporally stable and text-editable 4D assets. \nFinally, we curate a new high-quality 4D dataset named \\emph{Sketchfab28}, to evaluate object-centric 4D generation for future research.", "tldr": "", "keywords": ["4D Reconstruction", "Gaussian Splatting"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Withdrawn Submission", "pdf": "/pdf/ca84e2e6be8e124b45a00c8b65862d2a7f2ace96.pdf", "supplementary_material": "/attachment/6eac8cddfacd0c391d00c9758e2e67e34f31560e.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces Track4Animate3D, a novel two-stage framework designed to solve the core challenges in 4D object generation—such as maintaining appearance and motion consistency over time and avoiding visual artifacts."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper proposes a 4D generation framework leveraging motion priors to mitigate appearance drift through explicit temporal supervision.\n2. By introducing motion priors into multi-view video generation and 4DGS reconstruction, the proposed approach effectively enhances temporal consistency across frames.\n3. The authors construct a new high-quality 4D dataset, which provides valuable resources for future research in dynamic 3D and 4D generation."}, "weaknesses": {"value": "1. The entire framework heavily relies on the motion priors provided by CoTracker3. In practice, point tracking often fails under occlusion or textureless regions. How does the method handle those situation to maintain robustness?\n2. The paper would benefit from a runtime efficiency comparison (e.g., training or inference time) against baselines such as Animate3D, to demonstrate that the proposed approach is computationally feasible.\n3. The authors should briefly justify why the chosen intermediate-layer features are more suitable for point tracking compared to shallower or deeper layers (e.g., mid-block or final upsampling block)."}, "questions": {"value": "see weaknesses"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "WJDhOCRKxt", "forum": "kBsBUqEVES", "replyto": "kBsBUqEVES", "signatures": ["ICLR.cc/2026/Conference/Submission10559/Reviewer_J3y3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10559/Reviewer_J3y3"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761913175491, "cdate": 1761913175491, "tmdate": 1762921832725, "mdate": 1762921832725, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"withdrawal_confirmation": {"value": "I have read and agree with the venue's withdrawal policy on behalf of myself and my co-authors."}}, "id": "G6eo3gNwKT", "forum": "kBsBUqEVES", "replyto": "kBsBUqEVES", "signatures": ["ICLR.cc/2026/Conference/Submission10559/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10559/-/Withdrawal"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1763070410960, "cdate": 1763070410960, "tmdate": 1763070410960, "mdate": 1763070410960, "parentInvitations": "ICLR.cc/2026/Conference/-/Withdrawal", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles 3D-to-4D asset animation by combining a multi-view video diffusion backbone with dense point-tracking supervision and a hybrid 4D Gaussian splatting reconstructor. Stage One injects CoTracker3-derived correspondences into the diffusion U-Net, identifying a stable feature layer for tracking-aware losses that aim to curb appearance drift. Stage Two concatenates these diffusion features with Hex-plane encodings and extends appearance modeling with 4D spherical harmonics while optimizing with reconstruction, 4D-SDS, and ARAP losses."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The paper makes a compelling observation that existing multi-view video diffusion approaches suffer from appearance drift due to supervision limited to pixel/latent space. The solution of leveraging CoTracker3 to inject dense feature-level correspondences into the diffusion U-Net is intuitive and well-motivated.\n- The curation of Sketchfab28 with baked lighting addresses a real need for object-centric 4D evaluation data."}, "weaknesses": {"value": "- In the supplementary videos comparing with Animate3D and DG4D, I don’t see a clear improvement over Animate3D in temporal consistency or visual quality. Why is that?\n- The paper does not include a comparison with recent SOTA methods such as AnimateAnyMesh [1].\n- Incorporating point tracking into multi-view diffusion is interesting, but the overall pipeline feels incremental. It mainly combines existing techniques (multi-view diffusion, point tracking, and 4D Gaussian splatting) without introducing a fundamentally new idea.\n\n[1] AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"}, "questions": {"value": "- What are the end-to-end training and inference times, and the GPU memory usage for each stage, compared to Animate3D and AnimateAnyMesh?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "None"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wwe3WbHjtQ", "forum": "kBsBUqEVES", "replyto": "kBsBUqEVES", "signatures": ["ICLR.cc/2026/Conference/Submission10559/Reviewer_W42A"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10559/Reviewer_W42A"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761998470782, "cdate": 1761998470782, "tmdate": 1762921832364, "mdate": 1762921832364, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a new two-stage 4D generation framework, Track4Animate3D, which aims to improve the temporal consistency of multi-view video generation by introducing motion priors from a foundation point tracker, and to enhance dynamic reconstruction quality based on 4D Gaussian Splatting. In the first stage, it injects dense, feature-level supervision from CoTracker3 into a multi-view video diffusion model, using designed Losses to explicitly enforce temporal consistency and thereby mitigate appearance drift. In the second stage, diffusion features that carry tracking priors are concatenated with Hex-plane features to capture dynamic geometry and illumination changes during 4D-GS reconstruction. The authors conduct extensive experiments on multiple datasets and introduce a new evaluation set, Sketchfab28."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1.\tThe motivation of utilizing pre-trained tracking models to facilitate the spatially and temporally consistent feature learning in multi-view video generation pipeline is novel.\n\n2.\tThe paper is well-organized and easy to follow\n\n3.\tThe authors conduct extensive experiments to validate their design for video generation and 4D reconstruction."}, "weaknesses": {"value": "1.\tTrajectory ground truth: For trajectory ground truth in training dataset, the authors use CoTracker to predict the trajectory. However, under under rapid deformation, heavy occlusion, or homogeneous textures, CoTracker might output wrong results. A more proper way is to extract ground truth trajectory from original mesh files. Besides, the authors didn’t mention the uncertainty modeling for tracking noise and occlusions, which might force wrong correspondence learning when finetuning video diffusion.\n\n2.\tThe contribution of tracking supervision to cross-view consistency is limited. In the paper, CoTracker is run independently for each view, which can ensure temporal continuity within a single view but not cross-view geometric correspondence. Although temporal supervision is introduced at the feature level, it mainly strengthens within-view stability rather than establishing a unified representation across views. The authors are suggested to evaluate the contribution of tracking supervision within-views and cross-views separately.\n\n3.\tEvaluation dataset: the Diffusion4D test dataset might have overlap with Animate3D training dataset, cause they all downloaded from sketchfab. The authors are suggested to check this carefully.\n\n4.\t Comparison method: DG4D is not a multi-view video generation work, so it is inappropriate to compare multi-view video generation with it. The authors are suggested to compare with Diffusion4D/4Diffusion/SV4D/SV4D++. For 4D generation, please add L4GM aside from those in multi-view video generation."}, "questions": {"value": "Static 3D-4D has some new methods, such as AnimateAnyMesh and DriveAnyMesh. The authors might could consider discuss them in related work.\nI am willing to give a higher recommendation if the authors solve my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WQj2e6KVxB", "forum": "kBsBUqEVES", "replyto": "kBsBUqEVES", "signatures": ["ICLR.cc/2026/Conference/Submission10559/Reviewer_RREp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10559/Reviewer_RREp"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10559/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762007925030, "cdate": 1762007925030, "tmdate": 1762921831884, "mdate": 1762921831884, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": true}