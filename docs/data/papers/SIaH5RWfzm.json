{"id": "SIaH5RWfzm", "number": 2807, "cdate": 1757256563765, "mdate": 1763212774940, "content": {"title": "Acoustic-based Gender Differentiation in Speech-aware Language Models", "abstract": "Speech-aware Language Models (SpeechLMs) have fundamentally transformed human-AI interaction by enabling voice-based communication, yet they may exhibit acoustic-based gender differentiation where identical questions lead to different responses based on the speaker's gender. This paper propose a new dataset that enables systematic analysis of this phenomenon, containing 9,208 speech samples across three categories: Gender-Independent, Gender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni series and discovered a paradoxical pattern; while overall responses seems identical regardless of gender, the pattern is far from unbiased responses. Specifically, in Gender-Stereotypical questions, all models consistently exhibited male-oriented responses; meanwhile, in Gender-Dependent questions where gender differentiation would be contextually appropriate, models exhibited responses independent to gender instead. We also confirm that this pattern does not result from neutral options nor perceived gender of a voice. When we allow neutral response, models tends to respond neutrally also in Gender-Dependent questions. The paradoxical pattern yet retains when we applied gender neutralization methods on speech. Through comparison between SpeechLMs with corresponding backbone LLMs, we confirmed that these paradoxical patterns primarily stem from Whisper speech encoders, which generates male-oriented acoustic tokens. These findings reveal that current SpeechLMs may not successfully remove gender biases though they prioritized general fairness principles over contextual appropriateness, highlighting the need for more sophisticated techniques to utilize gender information properly in speech technology.", "tldr": "", "keywords": ["Gender Bias", "Spoken Language Model", "Acoustic-based Gender Differentiation", "Fairness"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/f8c2fc4b1a6b3dd449d83d0feea08ecf67919825.pdf", "supplementary_material": "/attachment/9f895e294184e65a831ab613c60ff8f832cccb19.zip"}, "replies": [{"content": {"summary": {"value": "This paper studies acoustic-based gender differentiation in speech-aware large language models (SpeechLMs). The authors build a new dataset consisting of 9,208 speech samples across three categories: Gender-Independent, Gender-Stereotypical, and Gender-Dependent. Using this dataset, they evaluate several SpeechLMs (e.g., LLaMA-Omni series) and report a paradoxical pattern—the models tend to show male-oriented bias in gender-stereotypical questions but become gender-insensitive in gender-dependent questions. The authors further compare these results with text-only LLMs and attribute the bias primarily to Whisper speech encoders.\n\nThe paper concludes that current SpeechLMs may prioritize fairness principles over contextual gender sensitivity, revealing an unsolved challenge in achieving fair multimodal alignment."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper explores a novel and important problem — acoustic-level gender bias — which is underexplored compared to textual bias studies.\n\n- The proposed dataset with three bias categories offers a structured approach to analyzing multimodal bias.\n\n- The comparison between SpeechLMs and their text-only counterparts is insightful and helps identify the potential source of the bias.\n\n- The findings reveal a counterintuitive fairness paradox, offering a new perspective on model alignment trade-offs."}, "weaknesses": {"value": "- The paper’s Introduction section is conceptually confusing, as it implies the paper solves bias, while in reality, it only evaluates and analyzes it.\n\n- The experimental evidence does not strongly support the claimed conclusions; observed differences (1–2%) may not justify statements such as “confirmed” or “significant bias.”\n\n- Metric tables include too many measures without clear interpretation; there are no indicators (↑/↓) showing whether higher or lower values are better.\n\n- The claim that bias “primarily stems from Whisper encoders” is overstated, since no alternative encoder comparison was performed.\n\n- Writing has scattered inconsistencies/typos and a few repeated passages that hinder readability."}, "questions": {"value": "- The paper's introduction details the limitations of current approaches to fairness, implicitly suggesting that these limitations should be addressed. However, beyond the diagnostic analysis, the paper offers no clear solutions. It also fails to address the limitations by suggesting that insufficient evaluation is responsible. This seems unfounded.\n\n- The term “neutral response” is used inconsistently — sometimes implying fairness, sometimes insensitivity. Clarify the intended meaning.\n\n- The authors claim that the Whisper encoder introduces a male-oriented acoustic bias, but the evidence provided in the paper is largely indirect rather than directly verified. The experiments only compare the output of a model with acoustic input and a text model. There are no ablation experiments (such as those that substitute different encoders or perform acoustic normalization on the audio). Nor do they conduct interpretable analysis of the acoustic representations, such as visualizing gender clustering in the embedding space or analyzing the gender sensitivity of acoustic tokens. Therefore, while the experimental results suggest a bias in the acoustic module, it remains unclear whether the source of the bias lies in the acoustic encoder, the text generation component, or the dataset itself.\n\n- The \"gender-ambivalent behavior\" reported in the paper (biasing males in stereotypical tasks and ignoring gender in tasks requiring differentiation) is an interesting phenomenon, but the authors do not rule out other possible explanations.\nFor example, the model may actively avoid gender-specific responses in \"dependent\" tasks due to excessive safety alignment;\n\n- It is not clear whether the dataset or evaluation code is open to the public, nor is basic statistics of the speech samples provided. This makes it difficult to verify the reproducibility of the results.\n\n- The paper claims that introducing a \"Neutral\" option can mitigate gender bias, but in reality, this setting statistically inevitably reduces bias because the neutral option directly reduces the proportion of gendered outputs. Without simultaneously reporting the balance between \"correctness\" and \"neutrality\" (i.e., whether the model maintains appropriate responses in scenarios where gender distinction is required), the results may simply be \"numerically neutral\" rather than a true improvement in fairness. In other words, does the improvement in fairness come at the expense of semantic appropriateness? This requires more nuanced metric design and interpretation.\n\n- Several typos and inconsistencies reduce clarity (e.g., “constrution” → “construction”; “backborn LLMs” → “backbone LLMs”). A thorough proofread is recommended."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "unBEfIMJh5", "forum": "SIaH5RWfzm", "replyto": "SIaH5RWfzm", "signatures": ["ICLR.cc/2026/Conference/Submission2807/Reviewer_r2Jn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2807/Reviewer_r2Jn"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760596347598, "cdate": 1760596347598, "tmdate": 1762916384241, "mdate": 1762916384241, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates acoustic-based gender differentiation (AGD) in Speech-Aware Language Models (SpeechLMs). The authors introduce a dataset categorized into Gender-Independent, Gender-Stereotypical, and Gender-Dependent contexts to systematically examine when gender-based differentiation is appropriate. Using the LLaMA-Omni series (Whisper + LLaMA backbone), they find paradoxical behaviors: models are biased when they should be neutral (stereotypical contexts) and neutral when they should differentiate (dependent contexts). Further analyses suggest that the Whisper encoder introduces male-oriented acoustic bias, highlighting a need for better speech encoders and bias mitigation strategies."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is logically structured and well presented. \n2. The authors comprehensively analyze across model scales, architectures and conditions, discovering that bias likely originates from the Whisper encoder provides valuable diagnostic insight for future SpeechLM design."}, "weaknesses": {"value": "1. Although the dataset advances upon Spoken StereoSet by introducing context-aware categories, the conceptual extension appears incremental rather than transformative, with limited methodological innovation.\n2. The study relies entirely on synthetic speech generated by TTS systems, yet the quality and naturalness of these samples are not systematically evaluated, which may affect the validity of the findings regarding acoustic gender cues."}, "questions": {"value": "Refer to the weaknesses part."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "9zb4RA5WPS", "forum": "SIaH5RWfzm", "replyto": "SIaH5RWfzm", "signatures": ["ICLR.cc/2026/Conference/Submission2807/Reviewer_m8bT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2807/Reviewer_m8bT"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761851293293, "cdate": 1761851293293, "tmdate": 1762916384034, "mdate": 1762916384034, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper investigates the acoustic-based gender differentiation in speech-aware language models. The paper introduces a new dataset focusing on three types of questions: Gender-Independent, Gender-Stereotypical, and Gender-Dependent, which investigate the tendency of speech-aware language models when the gender information is essential or irrelevant to answer the questions. The results demonstrate that current models may still have gender biases, highlighting future research to address this issue."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The topic is timely and relevant.\n- The division of three types of questions enable more fine-grained analyses."}, "weaknesses": {"value": "- The coverage of SpeechLLMs is quite limited. While the authors argue that this is for comparison across parameter size and generational change, including other model families is still essential for providing a more comprehensive overview of gender bias of current SpeechLLMs.\n- The claim that the Whisper encoder “generates male-oriented acoustic tokens” is not directly demonstrated. No ablation isolating Whisper’s internal representations is shown. Consequently, the main conclusion rests on correlation rather than causal evidence.\n- No human validation conducted to ensure the speech quality of the dataset.\n- Minor problem: The author(s) should conduct more proof-readining. There are some typos in the paper, e.g., \"Backborn\" should be \"Backbone\" in the title of Appendix E.3 and E.4"}, "questions": {"value": "I listed my concerns in the weakness section. Please check it out."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hHZfc63ce5", "forum": "SIaH5RWfzm", "replyto": "SIaH5RWfzm", "signatures": ["ICLR.cc/2026/Conference/Submission2807/Reviewer_T3dD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2807/Reviewer_T3dD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761900821678, "cdate": 1761900821678, "tmdate": 1762916383895, "mdate": 1762916383895, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a dataset to evaluate speech-based LLMs performance variation by gender and analyze gender-bias in the LLM's response. The paper claims that the dataset will enable a systematic evaluation of a speech-LLMs response across three categories: Gender-Independent, Gender-Stereotypical, and Gender-dependent.\n\nThe paper provides details on how the dataset was created and the tools that were used to generate the dataset. \nThe paper does not clearly state whether the authors intend to share the dataset publicly, as the dataset is one of the key contribution of this work, which can act as a benchmark evaluation set for gender-bias analysis of LLMs."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "The work investigates the performance of speech LLMs by gender and presents a dataset that could be used as a benchmark for gender bias evaluation. The work breaks the gender relevance evaluation into three categories: Gender-Independent, Gender-Stereotypical, and Gender-dependent tasks and analyzes the performance of standard benchmark LLMs on these subsets.\n\nThis is a relevant work that puts forth a standard, well articulated strategy for evaluating gender fairness in LLMs. Results from the analysis show that most of the studied speech LLMs showed a bias toward male speakers and opens up future research directions for mitigating and/or addressing such bias."}, "weaknesses": {"value": "The work mostly focuses on objective metrics, where the metrics have been defined and described well in the paper. A specific area that has been largely ignored by the paper is subjective metrics. It will be useful to know, from the user's point of view how were the LLM-responses in-terms of gender bias. Subjective analysis in terms of mean opinion score would be interesting to analyze.\n\nThe paper also hypothesizes that the contribution of the bias may be mostly attributed to encoder, which can be a consequence of bias in the training data. A interesting experiment would be to explore fine tuning the model with a more balanced data and then reporting the results from that. In its current form, the paper mostly focuses on a well known problem in machine learning (specifically for speech), without clearly outlining a potential solution or future directions to explore."}, "questions": {"value": "(1) Table 3 font size seems to be too small compared to the rest of the text, suggest increasing the font size.\n\n(2) If these were synthesized speech, how were the male and female speaker voices selected? Any details on the speaker trait and style will be useful. Were the default speakers selected without paying any particular attention to the speakers traits and style?\n\n(3) The paper has mostly focused on objective metrics, which is good in the sense that it demonstrates the point the authors want to make. A critical component of this study also indicate whether the bias in the LLM response indeed impact the end user experience (the users who are interacting with the LLM systems). It would make the results more interesting if there were  subjective metrics such as the mean opinion scores for the responses from males versus female subjects to assess how much the bias impact the acceptability of the responses.\n\n(4) One of the key contribution of the paper is the dataset that the authors present for evaluating gender bias, it is not clear whether the authors plan to share the dataset with the research community."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "W55XGjQ8lA", "forum": "SIaH5RWfzm", "replyto": "SIaH5RWfzm", "signatures": ["ICLR.cc/2026/Conference/Submission2807/Reviewer_9y83"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2807/Reviewer_9y83"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761953115295, "cdate": 1761953115295, "tmdate": 1762916383760, "mdate": 1762916383760, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This study investigates bias in speech-aware language models (SpeechLMs). The authors look at how these models behave in situations with gender stereotypes versus situations where using gender information is contextually appropriate. They propose a new synthesized dataset to measure this, which solves important limitations of older benchmarks. The paper finds a \"paradoxical pattern\" in the Llama-Omni series: the models show bias in the stereotypical setting (where they shouldn't) but fail to use gender information correctly in the gender-dependent setting (where they should). The study also finds that models generally show a male-oriented bias, even when the speech input is \"neutralized.\" The authors suggest this bias may be coming from the Whisper speech encoder."}, "soundness": {"value": 4}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The paper's main strength is its contribution to creating an ethically balanced way to evaluate models. The new dataset and analysis are valuable and could have a positive impact on the speech research community.\n\n- The experiments are well-designed and clearly explained. The paper is easy to follow because each experiment builds on the previous one, creating a strong narrative. The methodology and experimental settings are well-defined."}, "weaknesses": {"value": "The study is very solid, and I did not find many weaknesses. The main points I noted are:\n\n- The paper's conceptual novelty is a factor. The core idea of evaluating stereotypical bias versus context-appropriate differentiation has been extensively explored for text-based LLMs. While applying this framework to speech is an important and valuable contribution, the underlying idea is not new. This is the main reason I would not give a perfect score.\n\n- I found it difficult to understand the key message the authors wanted to convey from the abstract of the paper. It reads more like a summary of the results rather than a clear statement of the paper's main argument. The abstract would be much stronger if it focused on clearly explaining the most important takeaways.\n\n- The study does not investigate if these findings also apply to widely used closed-source models.\n\n- A minor presentation issue is in the references: The citation for the Qwen model technical report (cited as Qwen et al., 2025 in Line 214) lists the model name, \"Qwen,\" as the first author. This is an unusual format for a citation and may be a formatting error."}, "questions": {"value": "N/A"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JS80rs0iKO", "forum": "SIaH5RWfzm", "replyto": "SIaH5RWfzm", "signatures": ["ICLR.cc/2026/Conference/Submission2807/Reviewer_C93b"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2807/Reviewer_C93b"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission2807/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761982985337, "cdate": 1761982985337, "tmdate": 1762916383588, "mdate": 1762916383588, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}