{"id": "kgJBAytVEj", "number": 20536, "cdate": 1758307196779, "mdate": 1759896972740, "content": {"title": "World-to-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge", "abstract": "While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-to-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-to-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1\\% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world.", "tldr": "", "keywords": ["Agent-driven", "World-Knowledge", "Text-to-Image"], "primary_area": "generative models", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7663658f2462f441661bb8f6c08278a7219fed18.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "In this paper, the authors focus on bridging the novel concepts in image generation by using an agent to search for relevant images and perform multimodal prompt optimization. Experiments show improvement in both semantic alignment and visual aesthetics."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "The idea of using a search agent to retrieve relevant images to incorporate novel concepts is interesting to me.\n\nExperiments and ablation studies show this approach outperforms other methods and pre-trained models in many dimensions."}, "weaknesses": {"value": "The technical contribution of this paper is pretty limited in terms of test-time scaling/post-training/prompt optimization/reward functions. \n\nI would like to see more detailed quantitative results to illustrate the effectiveness and insight of the proposed method. I suggest the authors include retrieved images + prompt after optimization (or each iteration) to see how the framework improves step by step.\n\nThere are some relevant papers in image generation using multimodal post-training that need to be discussed. For example, Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment - ICLR 2025. Instead of using search agents, they ask MLLM to rephrase the content of the reference images (maybe the MLLM here can elicit the novel concept?). \n\nIn this paper, the authors use ImageReward or HPSv2 to evaluate the semantic alignment with text prompts, but these rewards can capture mostly global information. In the Hummingbird paper, they use both global and fine-grained rewards for optimization. I suggest the authors use Hummingbird evaluator as an additional metric in Table 2, or use it as a reward function to optimize prompts (whatever makes sense to the authors)."}, "questions": {"value": "I really like the intuition of using a search agent to retrieve novel concepts. However, some additional experiments need to be done for comprehensive results. I initially put my score marginally below the acceptance threshold, but happy to raise my score if the authors can include all suggestions and feedback."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "F4pLmiZGZw", "forum": "kgJBAytVEj", "replyto": "kgJBAytVEj", "signatures": ["ICLR.cc/2026/Conference/Submission20536/Reviewer_6R1v"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20536/Reviewer_6R1v"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission20536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761981633196, "cdate": 1761981633196, "tmdate": 1762933956646, "mdate": 1762933956646, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes World-to-Image (W2I), an agentic framework that improves text-to-image (T2I) generation on prompts containing novel or out-of-distribution (OOD) concepts by retrieving web images and performing multimodal prompt optimization, without retraining the base generator. An Orchestrator decides when to invoke a Prompt Optimization Agent (POA) and an Image Retrieval Agent (IRA), using Google SERP to fetch reference images and conditioning a backbone (OmniGen2 by default) over two iterations (early stopping allowed). Evaluation uses LLM-Grader, ImageReward, HPSv2, and a Human Preference Reward aggregate, across DiffusionDB, Lexica, and a curated NICE benchmark (100 prompts; five categories including memes, real-time events, pop culture/IP, celebrities, and niche concepts). W2I reports its largest gains on NICE, including +8.1% Accuracy-to-Prompt, and claims efficiency with <3 iterations."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper targets a well-motivated failure mode, i.e., knowledge cutoffs, and frames a diagnosis-and-selection loop that combines semantic decomposition and concept substitution with visual grounding via retrieved references, which is interesting.\n2. On NICE, W2I achieves the most pronounced improvements, especially Accuracy-to-Prompt (+8.1%), aligning with the hypothesis that novel, time-sensitive concepts benefit from retrieval-augmented prompting.\n3. The table comparing Prompt-only, Image-only, and w/o Agent variants shows the full pipeline leads across LLM-Grader sub-scores and reward models, substantiating the claim that text and image cues are complementary."}, "weaknesses": {"value": "1. NICE has 100 prompts (20 per category). While well-curated, it remains relatively small for broad claims about OOD generalization and may reflect category-specific gains (e.g., IP/memes). Consider scaling and assessing category-level robustness and difficulty balance.\n\n2. The main signal is an LLM-based judge plus automatic reward models. This raises concerns about metric sensitivity, judge leakage, and alignment with human preferences. A targeted human A/B on a subset of NICE would strengthen the empirical case.\n\n3. W2I is demonstrated primarily with OmniGen2; while baselines include SD1.4/2.1/SDXL and Promptist variants, it is not fully clear whether the agentic loop generalizes across diverse modern backbones (e.g., FLUX/SDXL as the primary generator in W2I). A small study swapping the backbone inside W2I would increase confidence.\n\n4. The paper motivates two iterations for efficiency, but it would help to include wall-clock cost, API call counts, and retrieval latency; and an efficiency–quality Pareto curve across budgets on NICE. The iteration analysis is promising, but it is not yet a full cost study."}, "questions": {"value": "1. What's the comparison between this work and prompt-a-video(ICCV2025)?\n\n2. How to obviate the hallucination of LLM in the loop?\n\n3. In categories like “real-time news” or “IP”, show examples where retrieval is wrong/noisy and how the Orchestrator mitigates or aborts."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QTe6HbhOjC", "forum": "kgJBAytVEj", "replyto": "kgJBAytVEj", "signatures": ["ICLR.cc/2026/Conference/Submission20536/Reviewer_s5ta"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20536/Reviewer_s5ta"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission20536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761991861852, "cdate": 1761991861852, "tmdate": 1762933956260, "mdate": 1762933956260, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The main contribution is the system that decides (per-iteration) to decide one or two different steps:\n- Prompt refinement (using GPT-4o + some rule based POS/NER extraction)\n- Web image search + vision grounding (using google SERP)\nThe system also includes a dynamic stopping condition based on an LLM-judge's scoring of semantic alignment, coverage and aesthetics. \n\nThe system is evaluated on Lexica, DiffusionDB, NICE databases, and shows gains on the long-tail concepts.\n\nThe problem that is being solved here is important - improve vision grounding without re-training, and there are reasonable ablations included showing both prompt refinement AND web grounding contribute to improved performance.\n\nA few changes to show a more systematic study of the approach, a better (web grounded) baseline to compare against would push my opinion into an accept."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 1}, "strengths": {"value": "The system for LLM driven orchestration to decide whether the prompt needs to be refined or new grounding is needed is a nice, new method.\nThe system also shows empirical improvements on NICE and particularly on the long tail / niche categories. \nThe paper itself is well written, all prompts provided, important ablations included, and this method can easily be extended to any generation model.\n\nA good approach to address a prevalent problem - a lot of image generation models struggle with OOD generation."}, "weaknesses": {"value": "There is little to no algorithmic novelty: The orchestrator does not particularly have a strong formulation of the criteria for it's tooling choices and is mostly relying on few-shot prompting. Additionally, the LLM-as-a-judge setup for determining stop condition biases the generation to the model used as a judge (gpt-4o). While the system itself is a new approach, there is limited algorithmic novelty here.\n\nThe same model is being used for optimization and for evaluation (gpt-4o). Anecdotally, using models from the same family tends to bias the model's output - I'd ideally like to add ablations with models across families (claude vs gpt, for ex).\n\nA 0-10 scale for the LLM grader is an interesting choice, especially since LLM as a Judge can be very finicky and inconsistent with grading unless every grading level is mapped to a concrete definition with rules and sufficient examples - and even then, still can result in inconsistencies, and needs to be mitigated. \n\nBaseline choice also feels a little disingenuous - all the baselines are just text -> image with no extra grounding. A great ablation might be (Proposed system) vs (System that provides the top google SERP result for the key words in your query + an existing T2I model).\n\nFinally - no human studies. While the paper describe the use of human preference models as a proxy, that does not necessarily serve as a sufficiently good judge, especially for generating images that are long tail/uncommon/OOD. Even LLM as a judge might not be very well aligned with human judgement on whether the generated image was grounded correctly, depending on the LLM's knowledge cutoff."}, "questions": {"value": "- How is the LLMGrader output (0-10) mapped to the outputs in table 4/5/6, which seems to be out of 100?\n- How aligned is the LLMGrader output to human judgement?\n- Is there any search result filtering in place to remove offensive / licensed / AI-generated content? This could contribute to an-AI slop loop: how do you plan on avoiding that?\n- The LLM prompt optimization and Image retrieval both rely on the LLM Grader knowing whether the candidate image satisfied the user's prompt - which for niche areas / OOD areas, may not be the case. This effectively shifts the knowledge cutoff from the T2I model to the LLM: couldn't this be architected differently here?\n- LLM Grader scores out of 0-10, but criteria for 0 to 10 is not defined anywhere, and is vague, potentially leading to hard-to-reproduce outputs\n- SERP does not by default take into account the site's REP (robots.txt) - it might allow Search Engine indexing but not use for other purposes. Was a REP filter added to the system?"}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "Retrieving images at runtime could run afoul of copyright / terms of use."}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "WNwCgrp1Bu", "forum": "kgJBAytVEj", "replyto": "kgJBAytVEj", "signatures": ["ICLR.cc/2026/Conference/Submission20536/Reviewer_zN13"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20536/Reviewer_zN13"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission20536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762145416177, "cdate": 1762145416177, "tmdate": 1762933955811, "mdate": 1762933955811, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces World-to-Image (W2I), a smart system that uses AI agents to search the web and improve prompts. It helps image generators create better pictures of new or niche things they weren't originally trained on, like recent memes or events."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The agent-driven approach is its best feature. Instead of just rewriting text, it intelligently chooses between searching for visual examples or refining the prompt, which is a much more powerful way to handle unknown concepts.\n\n2. The paper proves its worth by creating and excelling on the \"NICE\" benchmark. This custom test full of tricky, real-world prompts shows the method genuinely solves the problem it set out to tackle."}, "weaknesses": {"value": "- The system's performance is tied to finding good reference images online. If the search fails or returns poor-quality, irrelevant, or biased images, the final output will suffer.\n\n- The method doesn't actually teach the model new concepts. It just finds clever ways to guide the existing model, so its ability is still limited by the model's original training data.\n\n- The process of searching, iterating, and generating multiple images is significantly slower and uses more computational power than a standard, single-pass image generation."}, "questions": {"value": "The system trusts whatever images it finds online. But what happens if the retrieved images are misleading, contain stereotypes, or are just visually wrong for the concept?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "zqZHGDCruw", "forum": "kgJBAytVEj", "replyto": "kgJBAytVEj", "signatures": ["ICLR.cc/2026/Conference/Submission20536/Reviewer_7FG3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission20536/Reviewer_7FG3"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission20536/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762187813998, "cdate": 1762187813998, "tmdate": 1762933955372, "mdate": 1762933955372, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}