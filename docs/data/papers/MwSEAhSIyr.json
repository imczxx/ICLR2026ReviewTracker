{"id": "MwSEAhSIyr", "number": 19586, "cdate": 1758297462363, "mdate": 1759897031587, "content": {"title": "Conservative & Aggressive NaNs Accelerate U-Nets for Neuroimaging", "abstract": "Advancements in deep learning for neuroimaging have resulted in the development of increasingly complex models designed for a wide range of tasks. Despite significant improvements in hardware, enhancing inference and training times for these models remains crucial. Through an analysis of numerical uncertainty in convolutional neural networks (CNNs) inference, we found that a substantial amount of operations in these models are applied to values dominated by numerical noise, with little to no impact on the final output. As a result, up to two-thirds of the floating-point operations executed by some CNNs appear unnecessary.\nTo address this inefficiency, we introduce Conservative \\& Aggressive NaNs ---novel variations of PyTorch's max pooling and unpooling operations. These techniques identify numerically unstable voxels and replace them with NaNs, allowing  models to bypass operations on irrelevant data. We evaluated Conservative \\& Aggressive NaNs on four models: the FastSurfer and FONDUE CNNs, widely used neuroimaging tools, the Xception CNN, an image classification model, and another CNN designed to classify the MNIST dataset. We observed speedups for data containing at least 50\\% NaNs, and most notably, for data with more than two-thirds NaNs (as in many of our use cases), we observed an average speedup of $1.67\\times$.\nConservative NaNs reduces the number of convolutions by an average of 30\\% across all tested models and datasets, with no measurable degradation in performance. \nIn some model layers, it can skip up to 64.64\\% of convolutions with no performance degradation. The more proactive Aggressive NaNs approach can skip up to 69.30\\% convolutions for FastSurfer with no performance degradation, however, it sometimes leads to measurable performance degradation for FONDUE and MNIST. Overall, Conservative \\& Aggressive NaNs provide substantial opportunities for runtime acceleration of inference in CNNs, which could potentially reduce the environmental impact of these models.", "tldr": "We introduce Conservative & Aggressive NaNs, techniques that enhance CNN efficiency in neuroimaging by skipping operations on numerically unstable voxels, reducing computational load by up to two-thirds while maintaining model accuracy.", "keywords": ["Pooling", "Unpooling", "Convolutions", "Deep learning", "Optimization", "Neuroimaging", "Convolutional Neural Networks", "Numerical Analysis", "Uncertainty Quantification"], "primary_area": "applications to neuroscience & cognitive science", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/eda1d49a1fa28dc04047c2db1d46575625f48304.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes an interesting concept of using numerically unstable operations caused by pooling and unpooling as an leverage to reduce computation during convolution operation and shows the reduction of computation by 30-40%. Extensive experiments on neuroimaging models (FastSurfer and FONDUE), classification benchmarks, (MNIST CNN), and a depthwise separable CNN (Xception) models are presented. \n\nAlthough this paper proposes an interesting concept and a practical way of reducing the computation, it may not translate to real speedup due to the reasons mentioned in limitations section. Authors are encouraged to provide more details in case they are able to achieve real speedup using some changes on compiler side.  It would be reject and the decision can changed based on the answers to few questions asked in limitations section."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Authors have proposed a very interesting concept of identifying computation which does not affect the final accuracy of the model (instability of voxels during pooling/unpooling operation when values are very close to each other)\n- Extensive experiments to prove that identification of unstable voxels/pixels work on different models\n- The paper is successful to demonstrate that removing unstable computation from the model does not affect accuracy in negative way"}, "weaknesses": {"value": "- The speedup shown is theoretical: Authors assume that reduction in number of operations directly translates to speedup. However, the almost all deep learning model compiler work on either massively parallel computing (GPUs) way or the vectorized way (embedded platforms and reduction of computation which is not structured does not result in speedup\n- Authors also showed modified convolution (L187) which can enable reduction of computation based on detecting NaN value. However, any conditional branching can make the model run very slow in most of the compilers.\n- Do Authors think that replacing those voxels by 0 instead of NaN would also work? It may be make it simpler for compiler to provide real speed up as many compilers detect sparsity and this method may work well in that case to provide real speedup\n- Object detection can see more impact of this approach and it would be good to evaluate this method on OD since background information can produce more areas of NaN and it may not affect the accuracy much."}, "questions": {"value": "- Please provide details of any changes in runtime or the way to measure speedup since these NaNs will add unstructured removal of computation which does not translate to speedup with kernel level changes or support from compiler."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "yQ6shyCI8R", "forum": "MwSEAhSIyr", "replyto": "MwSEAhSIyr", "signatures": ["ICLR.cc/2026/Conference/Submission19586/Reviewer_kd6h"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19586/Reviewer_kd6h"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission19586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761492840746, "cdate": 1761492840746, "tmdate": 1762931457176, "mdate": 1762931457176, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors observe that homogeneous, non-informative regions in images, such as the background in medical scans, lead to unnecessary computations in Convolutional Neural Networks (CNNs), particularly within max pooling and unpooling operations. To address this, they propose two alternative operations: 1) \"conservative\" max pooling/unpooling and 2) \"aggressive\" max pooling/unpooling. Both methods involve replacing unstable values with NaNs—the conservative option applies this only at the unpooling stage, while the aggressive option applies it at both the pooling and unpooling stages. This approach is complemented by a convolution operation that is designed to ignore NaN values. The authors demonstrate their method's applicability by integrating it into several architectures: two U-Net variants (FastSurfer for whole-brain segmentation and FONDUE for MRI denoising), the Xception image classifier, and a CNN for digit classification."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- **Well-motivated and intuitive concept**: The core observation—that uninformative pixels in homogeneous regions (like image backgrounds) cause unstable and inefficient pooling/unpooling operations—is sound and clearly explained. The proposed solutions, \"conservative\" and \"aggressive\" NaN-based pooling, are conceptually simple and easy to understand.\n- **Practical impact on efficiency**: The demonstrated improvement in computational efficiency is a significant strength, particularly for applications like medical imaging where processing large 3D volumes is computationally expensive. Reducing unnecessary calculations in these contexts is highly valuable.\n- **Configurable approach**: The method offers flexibility by providing different modes of operation. The choice between the conservative and aggressive strategies, along with the ability to set thresholds, allows users to tune the trade-off between the amount of computation skipped and the performance.\n- **Generalisable NaN convolution**: The introduction of a convolution operation that ignores NaN values is an interesting contribution with broader potential. This operator could be applied not only to handle NaNs introduced by the proposed pooling methods but also to mask other uninformative regions directly in the input data."}, "weaknesses": {"value": "- **Limited and small-scale validation**: The experimental validation on the medical imaging networks (FastSurfer and FONDUE) is a significant weakness. Using only five subjects is insufficient to draw strong conclusions about the method's general performance and robustness. While the authors ensured these subjects came from different acquisition sites, the sample size is too small to cover a diverse range of anatomical variability and pathologies. Furthermore, the evaluation is confined to structural MRI; testing on other modalities (e.g., CT, PET) would be necessary to assess the broader scope of the initial observation about uninformative regions.\n- **Lack of justification for model selection**: The paper provides no rationale for the specific choice of the four evaluated networks (FastSurfer, FONDUE, Xception, and a digit classifier). The selection appears arbitrary, and a clearer justification linking the architectures to the specific challenges the method aims to address would strengthen the experimental design.\n- **Narrow experimental scope**: The experiments lack diversity in terms of both architecture types and tasks. The set of models does not adequately represent the wide variety of modern CNN architectures or a broad range of vision tasks beyond segmentation, denoising, and classification. This makes it difficult to gauge the general applicability of the proposed method.\n- **Limitation to 2D operations**: A key limitation for medical imaging applications is that the method, as presented, appears limited to 2D operations. Many medical imaging analyses, particularly with 3D volumes, benefit significantly from 3D convolutional kernels and pooling. The paper does not address how the proposed NaN-based pooling and convolution would extend to 3D, which limits its immediate practicality in this domain."}, "questions": {"value": "- **Generalisation**: How do you plan to validate the method's generalisation beyond the five medical subjects and structural MRI?\n- **Model choice**: What was the specific rationale for choosing the four evaluated network architectures?\n- **3D extension**: What are the challenges or plans for extending the proposed NaN-based operations to 3D, which is standard in medical imaging?\n- **Input masking**: Can the NaN convolution be applied to mask uninformative regions directly in the input image, and has this been explored?\n- **Trade-off guidance**: What practical guidance can you offer for choosing between the conservative and aggressive modes and setting the threshold on a new task?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "k5vKAq55vN", "forum": "MwSEAhSIyr", "replyto": "MwSEAhSIyr", "signatures": ["ICLR.cc/2026/Conference/Submission19586/Reviewer_DCY8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19586/Reviewer_DCY8"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission19586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761768223882, "cdate": 1761768223882, "tmdate": 1762931456625, "mdate": 1762931456625, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper identifies an alleged inefficiency in standard pooling and unpooling operations used in neural networks. To address this, the authors propose a novel method designed to serve as a more efficient alternative. The paper details the design of this new operation and presents experimental results, likely focused on model accuracy, to demonstrate the viability of their approach on the MNIST dataset."}, "soundness": {"value": 2}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "+ The paper targets a component of many CNN architectures. Any demonstrable improvement in the efficiency of pooling/unpooling operations would be a valuable and widely applicable contribution to the field.\n+ The proposed mechanism for pooling/unpooling appears to be a new design, and the core idea may be of interest to the community."}, "weaknesses": {"value": "+ Severe Lack of Scholarly Context: The paper is not properly situated within the existing scientific literature.\n+ No Problem Discussion: The introduction asserts that a problem exists but fails to provide a clear, evidence-based discussion of what this inefficiency is, why it is a problem, or how it impacts current models, all of which would require citations.\n+ No References in Introduction: The introduction is presented without a single citation, making its claims appear unsubstantiated and disconnected from the field.\n+ Missing Related Works: The paper contains no Related Works section. This is a critical omission. It is impossible to assess the novelty of the proposed method or understand how it differs from, or improves upon, the vast body of existing work on pooling mechanisms.\n+ Figure 9 contains very small text, making it almost illegible."}, "questions": {"value": "+ Given the lack of a related works section, can you explain the novelty of your contribution? How does your method compare to other established techniques for learnable or efficient pooling in the literature?\n+ Can you provide a clearer, citation-backed definition of the \"inefficiency\" for pooling and unpooling in U-nets?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Uzzwu4cmA5", "forum": "MwSEAhSIyr", "replyto": "MwSEAhSIyr", "signatures": ["ICLR.cc/2026/Conference/Submission19586/Reviewer_Y3AZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19586/Reviewer_Y3AZ"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission19586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761922898220, "cdate": 1761922898220, "tmdate": 1762931456110, "mdate": 1762931456110, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "- This paper identifies a major source of computational inefficiency in U-Net-like CNNs,where numerical noise is generated by instability in max pooling operations.\n\n- This paper proposes that the root cause is max_pooling on near-equal values. This creates unstable indices, which are then used by unpooling to propagate numerical noise that fills large parts of the feature map. The models produce correct results despite this noise, suggesting that up to two-thirds of all computations (which are processing this noise) are unnecessary. Besides, this paper proposes a new method to skip these irrelevant computations. It introduces Conservative NaNs and Aggressive NaNs, two new pooling/unpooling operations that intentionally inject NaN (Not-a-Number) values into these numerically unstable regions. This paper then introduces NaN Convolution, a modified convolution that checks the ratio of NaNs in its input window. If the ratio exceeds a set threshold, the entire operation is skipped, saving compute time. Conservative NaNs was found to skip an average of 30% of convolutions across neuroimaging models with no measurable degradation in performance.\n\n- Aggressive NaNs skips more (up to 69.3%) but can lead to performance degradation. In real-world neuroimaging cases (where >67% of data is background), the method achieved an average runtime speedup of 1.67 $\\times$."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- This paper asserts that the paper's primary strength is identifying a novel source of inefficiency. Instead of focusing on weight redundancy (like pruning), it targets numerical instability from pooling as a source of wasted computation.\n- In this paper, the Conservative NaNs method provides a significant compute reduction (~30%) with no measurable loss in accuracy (e.g., Dice/PSNR scores) on the tested neuroimaging models.\n- In the experiments, the analysis is not just theoretical (FLOPS). The authors measure and report actual wall-clock runtime speedup on both CPUs and GPUs, confirming the practical benefit.\n- Intuitively, the proposed method is perfectly suited for U-Net architectures and medical imaging (like MRI), where large, homogeneous background regions are processed by the decoder, leading to high NaN density and significant speedups.\n- Considering experiments, the method was shown to be effective on both float32 and bfloat16 precision types, indicating its compatibility with modern reduced-precision hardware."}, "weaknesses": {"value": "- This is the most significant weakness. The paper does not experimentally compare its method against other established acceleration techniques like pruning or quantization. It claims the method is orthogonal, but it provides no data to show if a 30% NaN skip is better or worse than a 30% so-called pruned model. In other words, other lightweight model techniques should be compared.\n\n- The method's effectiveness is limited to specific data types. It works well on homogeneous data (MRIs) but provides almost no benefit (<1% skip rate) on heterogeneous RGB images (e.g., ImageNet with the Xception model). Can you specify the reason?\n\n- The Aggressive NaNs variant, which achieves the highest skip rates, results in a measurable drop in accuracy on models like FONDUE and MNIST.\n\n- The NaN Convolution check introduces its own computational overhead. In low-NaN-density scenarios, the method is actually slower than standard convolution.\n\n- Validation on reduced precision was limited to bfloat16. It was not tested on fixed-point formats like INT8, which are very common for inference acceleration. The comparison and analysis when adopting INT8 should be done."}, "questions": {"value": "Please, refer to weakness and address my concerns."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QDhcbpd0EM", "forum": "MwSEAhSIyr", "replyto": "MwSEAhSIyr", "signatures": ["ICLR.cc/2026/Conference/Submission19586/Reviewer_oL3u"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission19586/Reviewer_oL3u"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission19586/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762232842047, "cdate": 1762232842047, "tmdate": 1762931455283, "mdate": 1762931455283, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}