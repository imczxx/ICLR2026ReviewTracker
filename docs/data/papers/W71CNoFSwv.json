{"id": "W71CNoFSwv", "number": 10767, "cdate": 1758181376277, "mdate": 1759897630479, "content": {"title": "Feedback Forensics: A Toolkit to Measure AI Personality", "abstract": "Some traits making a “good” AI model are hard to describe upfront. For example, should responses be more polite or more casual? Such traits are sometimes summarized as model personality. Without a clear objective, conventional benchmarks based on automatic validation struggle to measure such traits. Evaluation methods using human feedback such as Chatbot Arena have emerged as a popular alternative. These methods infer “better” personality and other desirable traits implicitly by ranking multiple model responses relative to each other. Recent issues with model releases highlight limitations of these existing opaque evaluation approaches: a major model was rolled back over sycophantic personality issues, models were observed overfitting to such feedback-based leaderboards. Despite these known issues, limited public tooling exists to explicitly evaluate model personality. We introduce Feedback Forensics: an open-source toolkit to track AI personality changes, both those encouraged by human (or AI) feedback, and those exhibited across AI models trained and evaluated on such feedback. Leveraging AI annotators, our toolkit enables investigating personality via Python API and browser app. We demonstrate the toolkit’s usefulness in two steps: (A) first we analyse the personality traits encouraged in popular human feedback datasets including Chatbot Arena, MultiPref and PRISM; and (B) then use our toolkit to analyse how much popular models exhibit such traits. We release (1) our Feedback Forensics toolkit alongside (2) a web app tracking AI personality in popular models and feedback datasets as well as (3) the underlying annotation data.", "tldr": "A Python toolkit for understanding AI personality changes encouraged by human feedback and exhibited by models", "keywords": ["human feedback", "pairwise preferences", "AI annotators", "LLM-as-a-Judge", "software"], "primary_area": "infrastructure, software libraries, hardware, systems, etc.", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fb897b5d5354940c8d0136d75152b6e3c41e918a.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces Feedback Forensics, an open-source toolkit designed to measure and analyze the \"personality\" traits of AI language models—characteristics in model responses such as politeness, verbosity, or tone that influence user experience but are not captured by standard benchmarks. Leveraging both human and AI-generated annotations of pairwise model response data, the toolkit quantifies which traits are encouraged by human (or AI) feedback and which traits are exhibited by various AI models. The authors illustrate their method through a Python API and an interactive web app, releasing not only the toolkit and infrastructure but also underlying annotation datasets. The toolkit aims to provide transparency and actionable insights into model personality, helping researchers and practitioners identify, track, and optimize desirable conversational behaviors across models and feedback-based training regimes."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- This paper makes a substantial and timely contribution by introducing an open-source, extensible toolkit for measuring and tracking AI model personality traits. \n- Its operationalization of \"personality measurement\" within the context of AI model development seems signifgant. While prior works often qualitatively discuss model style or provide ad hoc definitions, this toolkit formalizes and systematizes personality trait assessment across datasets and models, drawing on techniques from Inverse Constitutional AI and enriching them with a modular annotation and metric computation pipeline. \n- The authors provide both a Python API and a browser-accessible web app, lowering the barrier for practitioners to inspect personality drift or trait encouragement in large models and feedback datasets. The toolkit fills an important methodological gap left by conventional benchmarks, providing the community with a much-needed lens on a critical qualitative dimension of model behavior."}, "weaknesses": {"value": "- The most salient is the lack of direct comparison to existing baselines in the personality measurement domain. There is now a growing body of work proposing frameworks or metrics to quantify AI or LLM \"personality\", such as evaluations based on the Big Five personality model, or Model Personality Evaluations [1][2]. Without benchmarking Feedback Forensics against such prior methods, it is difficult to clearly assess its incremental value, or to know under what conditions it outperforms, complements, or perhaps fails compared to alternatives.\n- The trait taxonomy employed in Feedback Forensics is not extensively justified: it is not clear if the set of personality traits is exhaustive or robust to more subtle variations in conversational style, and there may be gaps in coverage (e.g., cross-cultural or context-dependent personality facets).\n- Since the annotators performing trait detection are themselves LLMs, and as these same (or similar) LLMs contribute to both feedback and target models, there is a risk that toolkit outputs merely reflect the biases of the model(s) rather than an independent grounded measurement. This could create feedback loops whereby models overfit to their own interpretations of style and personality. Also, from my point of view, It is unclear how robust trait detection and measurement are to changes in prompt phrasing, dialogue context, or user intent. No ablation studies or error analyses are presented to ensure that detected personality differences are intrinsic to models, and not artifacts of particular prompt distributions.\n\n[1] Evaluating and Inducing Personality in Pre-trained Language Models \n[2] https://arxiv.org/abs/2406.17675"}, "questions": {"value": "- The current framework treats high-level \"personality traits\" as discrete, binary, or categorical labels, but many relevant features of AI personality (e.g., confidence, politeness) are multifaceted and can vary on a spectrum. Is there any discussiom about this point?\n- How does Feedback Forensics compare quantitatively and qualitatively with prior personality measurement techniques for LLMs (e.g., Big Five surveys, direct style benchmarks, or recent works)? Can you report performance or findings on shared benchmarks?\n- How do the personality trait measurements produced by Feedback Forensics translate into actionable improvements in the overall performance or reliability of LLMs, beyond simply characterizing stylistic differences? For example, unlike MBTI-style personality assessments—which primarily offer descriptive labels and do not directly affect task performance or user satisfaction—can your toolkit's results be used to guide model training, calibrate feedback datasets, or optimize model deployment in ways that measurably enhance the utility, safety, or user experience of LLMs? Have you evaluated or do you anticipate any concrete downstream impacts (such as reducing undesirable behaviors or increasing user trust) based on trait monitoring and adjustment?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UcqJ8slPfJ", "forum": "W71CNoFSwv", "replyto": "W71CNoFSwv", "signatures": ["ICLR.cc/2026/Conference/Submission10767/Reviewer_KruL"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10767/Reviewer_KruL"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761638903376, "cdate": 1761638903376, "tmdate": 1762921981601, "mdate": 1762921981601, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Feedback Forensics, an open-source Python toolkit, accompanying web application, and annotation dataset designed to measure and track the \"personality\" of Large Language Models (LLMs). The core methodology involves using AI annotators (i.e., LLM-as-a-Judge) to evaluate pairs of model responses against a set of curated personality traits (e.g., \"is more verbose,\" \"makes more confident statements\"). The authors demonstrate two primary use cases for this toolkit: analyzing feedback datasets (e.g., Chatbot Arena, MultiPref, PRISM) and analyzing models (e.g., Gemini 2.5 etc)."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "* Solid Implementation and Usability: This is a well-executed project. The authors have delivered not just a concept but a functional, open-source toolkit. Having interacted with the web application, it's clear the tool is polished, intuitive, and provides a valuable, accessible resource for the community to inspect models and datasets directly.\n\n* Comprehensive Experiments: The utility of the toolkit is demonstrated through a comprehensive set of experiments. The analysis spans multiple popular feedback datasets (Chatbot Arena, MultiPref, PRISM) and a wide array of modern, popular LLMs, providing a broad and relevant snapshot of the current landscape."}, "weaknesses": {"value": "* A Measurement Tool, Not an Explanatory Science: The paper is good at describing personality traits but does not offer a deep investigation into why they emerge or how they are encoded, or any other scientific understanding / insights into related problems. The method—collecting pairwise preferences based on predefined, human-articulated traits—feels like a more granular and automated version of existing platforms like Chatbot Arena. It is \"forensics\" in the sense of identifying a trait, but not in the sense of uncovering the underlying cause (e.g., is a trait an artifact of RLHF, a result of data mixture, or an emergent property of the architecture, or how should we expect models to change under different conditions?).\n\n* Limited Scope on \"Control\": The work is focused on \"measuring\" and \"tracking.\" It does not propose new mechanisms for controlling these behaviors beyond the implied (and already standard) method of simply creating a preference dataset for a desired trait and training on it. The contribution is thus more of a high-quality data-gathering and computation tool rather than a new scientific framework for understanding or steering model personality."}, "questions": {"value": "The authors frame many traits in a way that implies a preference. However, many traits are not inherently \"bad,\" just different (like human personalities). Could the toolkit be used to explore the utility of different personality profiles? For example, in a high-stakes domain like finance or medicine, a \"cautious\" and \"less confident\" personality that \"acknowledges limitations more\" might be strongly preferred. How do you see the tool being used to define optimal, domain-specific personalities rather than just tracking a single \"better\" one?\n\nTo follow up, do different personalities imply performance differences in certain capabilities? Do the authors have any evidence or hypotheses about whether optimizing for these \"personality\" traits has a positive or negative causal effect on a model's underlying capabilities in, for example, complex reasoning, math, or code generation? Are we, as a community, creating \"personalities\" that we find pleasing but that might mask trade-offs in downstream task performance?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ba2bkoWAuU", "forum": "W71CNoFSwv", "replyto": "W71CNoFSwv", "signatures": ["ICLR.cc/2026/Conference/Submission10767/Reviewer_g7kN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10767/Reviewer_g7kN"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761729816235, "cdate": 1761729816235, "tmdate": 1762921981167, "mdate": 1762921981167, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Feedback Forensics, an open-source toolkit (with a Python API and a Gradio/Web app) for explicitly measuring “AI personality”, stylistic and tone-level traits that are typically only implicitly captured by human-preference evaluations (e.g., Chatbot Arena). The method augments pairwise response datasets with three kinds of annotations: (i) human preference labels, (ii) “target-model” selectors to isolate a specific model’s outputs, and (iii) AI-annotator personality labels that choose which response exhibits a trait more (e.g., verbosity, confidence). It then computes trait-level agreement using relevance, Cohen’s κ, and a combined “strength = κ × relevance” metric to quantify how much a trait is encouraged by feedback or exhibited by a target model. Using this toolkit, the authors (A) analyze trait preferences in Chatbot Arena, MultiPref, and PRISM, and (B) compare trait profiles across popular models and between two versions of Llama-4-Maverick. The paper releases code, an interactive app, and AI-generated annotation data."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- **Clear motivation & scope.** The paper tackles an under-explored aspect of alignment (implicit stylistic bias in feedback training) and provides a practical, reproducible framework to study it. The engineering contribution (toolkit + web UI + datasets) is substantial and well-documented.\n- **Comprehensive case studies.** Analyses on Chatbot Arena, MultiPref, and PRISM uncover consistent, interpretable patterns, the cross-model comparison and the Llama-4-Maverick public vs Arena contrast demonstrate the toolkit’s utility in post-hoc auditing."}, "weaknesses": {"value": "- **Human validation is minimal.** The entire human–AI agreement study (Appendix F.2) relies on a single author annotator (100 pairs ≈ 1 000 trait-level judgments). While κ ≈ 0.4–0.6 suggests moderate agreement, the absence of multiple raters, confidence intervals, or inter-rater reliability limits claims of external validity.\n- **Statistical under-specification.** Metrics are reported as single values without uncertainty estimates or variance analysis. Because strength = κ × relevance is multiplicative and κ depends on class imbalance, small differences may not be significant."}, "questions": {"value": "- Can you expand the human validation beyond the single-author study (e.g., multi-annotators replication on 500 pairs) and report inter-rater κ and human–AI agreement per trait?\n- Would the conclusions hold with a different judge model family (e.g., GPT and Qwen models) or when using multi-vote annotation?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "Vpk43sHnV6", "forum": "W71CNoFSwv", "replyto": "W71CNoFSwv", "signatures": ["ICLR.cc/2026/Conference/Submission10767/Reviewer_XfeE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10767/Reviewer_XfeE"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10767/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761926769123, "cdate": 1761926769123, "tmdate": 1762921980622, "mdate": 1762921980622, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}