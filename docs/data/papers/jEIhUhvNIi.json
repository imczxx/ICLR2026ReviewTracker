{"id": "jEIhUhvNIi", "number": 22415, "cdate": 1758330737620, "mdate": 1759896867612, "content": {"title": "From Self-Inconsistency to Stability: Achieving Order Invariant In-Context Learning", "abstract": "Large Language Models (LLMs) exhibit powerful reasoning capabilities, particularly when guided by in-context learning (ICL). However, their performance is brittle to demonstration order: accuracy can swing from perfect to random based solely on the permutation of input ordering. This sensitivity reveals a fundamental vulnerability where models rely on spurious positional correlations (noise) rather than semantic content (signal). To address this reliability gap, we introduce \\textbf{Self-Inconsistency Optimization (\\algname{})}, a simple model-agnostic post-training framework that teaches models to focus on \\textit{what} is said, not \\textit{how} it is arranged. \\algname{} generates semantically equivalent inputs through permutation and explicitly trains the model to align its output distributions using our proposed self-inconsistency loss which is based on the Jensen--Shannon divergence. We provide a theoretical justification for our framework, proving that minimizing this self-inconsistency loss is sufficient to achieve the desired order invariance. Furthermore, the Bayesian update design of \\algname{} provides a stable optimization process by decoupling the model's prior knowledge from the alignment objective, allowing it to integrate seamlessly with existing post-training pipelines such as reinforcement learning. Empirical evaluations on mathematical reasoning benchmarks show that \\algname{} substantially mitigates order sensitivity while maintaining or even improving task accuracy. Our source code is\navailable at \\url{https://anonymous.4open.science/r/From-Self-Inconsistency-to-Stability-E0BC}.", "tldr": "", "keywords": ["In-Context Learning (ICL)", "Permutation Invariance", "Jensen-Shannon Divergence (JSD)", "Distributional Alignment", "Self-Inconsistency Optimization"], "primary_area": "alignment, fairness, safety, privacy, and societal considerations", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/199f416c5998e07be4392847a2b689135aa33616.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors introduce a new training scheme for encouraging permutation invariance to exemplar order in ICL. They demonstrate that their methodology appears to work on a math dataset."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 2}, "strengths": {"value": "The paper is clear and well written. The topic is interesting and timely. The proposed training methodology sounds interesting, and seems to have some potential."}, "weaknesses": {"value": "The differences made by the training methodology seem to be small. Looking at your final results in Table 1, accuracy differences seem to improve overall by 0.01 for Gemma-3-4B, and appear to actually *worsen* for Qwen-3-4B. The magnitude of accuracy difference improvement seems to be fairly small, so comparing percentage change appear to inflate these differences. Improvement is also inconsistent across models and tasks. Hence, it's difficult to judge how significant of a difference your methodology is actually making. Were you able to replicate your results over multiple seeds, to gain a sense of the degree in variation?\n\nIt's also unclear whether your methodology is necessary at all. Looking at the Qwen column, for example, accuracy difference is already just 0.035 overall (and even smaller in Table 3). Qwen also seems to perform markedly better than Gemma overall. Perhaps a stronger model already learns a degree of permutation invariance, and your specific method is unnecessary?\n\nIt was also unclear which components of your pipeline were necessary. Were you able to perform ablation studies to see whether simple data augmentation is enough to reproduce your results? Are the regularization terms necessary in your loss? It also appears that your Bayesian update explanation (Section 3.4) seems to be a simple re-interpretation of LoRA training, which in turn is equivalent to vanilla finetuning if the LoRA rank is equal to the base weight rank. If vanilla finetuning can be interpreted this way, is this a useful construct?\n\nThe theory section (Section 3.5) seems to be a little extraneous. The conclusions are perhaps sufficiently intuitive/obvious that a formal argument is unnecessary. Perhaps the extra space could be more efficiently used for ablation results / additional empirics with more models and more tasks?"}, "questions": {"value": "See Weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "vMaxmR0DJv", "forum": "jEIhUhvNIi", "replyto": "jEIhUhvNIi", "signatures": ["ICLR.cc/2026/Conference/Submission22415/Reviewer_oq8K"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22415/Reviewer_oq8K"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761413110838, "cdate": 1761413110838, "tmdate": 1762942210032, "mdate": 1762942210032, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The authors argue that the ICL performance of today's LLMs is incomplete because they lack robustness to the order of demonstrations. While prior research has addressed this issue, the authors focus specifically on reasoning models that face the same challenge. They propose a method called Self-Inconsistency Optimization (SIO), a post-training framework that makes models robust to the order of demonstrations."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- The problem that the authors address is important. The model's lack of robustness to the order of examples naturally raises the question of whether it truly understands the context or merely memorises patterns.\n- The method demonstrates significant improvements in robustness on both benchmark datasets and simulated real-world input variations."}, "weaknesses": {"value": "- The paper does not demonstrate the inconsistency of LLMs. \n- There is no baseline against which the suggested method can be compared. The paper only shows the SIO's evaluation results. It is unclear whether the results are good or bad.\n- There is no ablation study at all. The authors need to conduct experiments to verify the effectiveness of:\n    1) Jensen-Shannon divergence, especially how to set $w_i$.\n    2) Bayesian update: It is convincing, but evidence is needed.\n    3) DI & SI loss\n- The authors explain that the increase in accuracy was due to underfitting."}, "questions": {"value": "1. How big is the variance when the demonstrations are randomly shuffled?\n2. Would using a different RL framework (e.g., GRPO) mitigate the inconsistency problem?\n3. As far as I know, the MATH dataset is very small. Can you fully train your model with SIO to reduce the \"Accuracy Difference\" on MATH data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "FRqtw1u805", "forum": "jEIhUhvNIi", "replyto": "jEIhUhvNIi", "signatures": ["ICLR.cc/2026/Conference/Submission22415/Reviewer_Vc7q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22415/Reviewer_Vc7q"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761791170633, "cdate": 1761791170633, "tmdate": 1762942209351, "mdate": 1762942209351, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Self-Inconsistency Optimization (SIO), a post-training framework that makes Large Language Models robust to the ordering of in-context demonstrations.\nLLMs often exhibit large performance fluctuations when demonstration examples are permuted, revealing reliance on positional noise rather than semantic content.\nSIO mitigates this by training models to align output distributions across semantically equivalent permutations using a Jensen–Shannon divergence–based self-inconsistency loss, supported by theoretical guarantees of conditional independence.\nThe authors further introduce a Bayesian update–inspired architecture that separates prior knowledge and alignment objectives to stabilize training.\nEmpirical results on GSM8K, MATH, and AIME benchmarks show that SIO significantly reduces order sensitivity while preserving or improving accuracy"}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Tackles the fundamental instability of ICL to demonstration order.\n\n- Conditional-independence criterion and JSD minimization proofs are rigorous.\n\n- Can be applied post-training to any LLM with minimal modification.\n\n- The Bayesian posterior interpretation (prior + likelihood) elegantly stabilizes training.\n\n- Demonstrated improvements across multiple reasoning benchmarks and noise types (order, tokenization, case)."}, "weaknesses": {"value": "- The framework combines many moving parts, making it unclear which element (GSPO vs SI vs DI) drives the improvement.\n\n- Lacks ablation or sensitivity analysis to disentangle the contribution of each loss term.\n\n-  Downstream effects on general-purpose metrics (e.g., perplexity) are not evaluated — robustness may trade off with fluency or general performance.\n\n- Results are limited to math reasoning; evaluation on more diverse tasks (e.g., natural language inference, commonsense reasoning) would better establish generality."}, "questions": {"value": "- Can SIO be integrated into other post-training pipelines (e.g., DPO, GRPO) without instability?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "h1eJv727OH", "forum": "jEIhUhvNIi", "replyto": "jEIhUhvNIi", "signatures": ["ICLR.cc/2026/Conference/Submission22415/Reviewer_NvJb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22415/Reviewer_NvJb"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761818726357, "cdate": 1761818726357, "tmdate": 1762942208826, "mdate": 1762942208826, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Self‑Inconsistency Optimization (SIO), a post‑training framework to make LLMs invariant to the order of in‑context demonstrations. The method augments each training item with multiple permutations of the same demonstrations and adds two distributional objectives: (i) a self‑inconsistency loss that minimizes the generalized Jensen–Shannon divergence (JSD) among the model’s outputs across permutations, and (ii) a distributional‑inconsistency loss that anchors outputs to a mixture formed by a frozen reference model. A Bayesian‑update training architecture is used by summing logits from a frozen \"prior\" head and a trainable LoRA \"likelihood\" head (product‑of‑experts), combined with GSPO reinforcement learning for reward alignment. Theorems 1–2 formalize order invariance as conditional independence and show that driving step‑wise JSD to zero is sufficient for distributional equivalence. Experiments on Gemma‑3‑4B and Qwen‑3‑4B over GSM8K, MATH, and AIME report reduced instability with small accuracy gains."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper targets distributional invariance, not only answer agreement. Using generalized JSD across permutations and the PoE view (added logits) is a neat, conceptually clean combination with RL‑style preference learning. The conditional‑independence framing is useful.\n2. Order sensitivity is a real reliability problem. SIO is model‑agnostic, adds no inference‑time cost, and also reduces sensitivity to other types of noise, which increases practical impact."}, "weaknesses": {"value": "1. The proposed approach combines both a new training methodology (SIO with JSD and GSPO losses) and an architectural modification (Bayesian update with dual heads). However, the experiments evaluate them jointly, making it difficult to disentangle the contribution of each component. An ablation study is needed to isolate the effects of the training objectives versus the architectural design. The overall complexity also seems to violate Occam’s razor.\n2. Training uses a tiny set (s 108 questions and max of 3456 responses), and evaluation uses only 200 questions and m=8 demonstrations. Lack of confidence intervals and comparison to strong baselines (e.g., prompt engineering, mixture of experts)."}, "questions": {"value": "1. Can the method be applied to full finetuning, or is LoRA key to the success?\n2. Do you compute token‑level JSD at each decoding step?\n3. On tasks where order is semantically meaningful, does SIO hurt?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "fuEElfAB7i", "forum": "jEIhUhvNIi", "replyto": "jEIhUhvNIi", "signatures": ["ICLR.cc/2026/Conference/Submission22415/Reviewer_BZx8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22415/Reviewer_BZx8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22415/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761989307180, "cdate": 1761989307180, "tmdate": 1762942208392, "mdate": 1762942208392, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}