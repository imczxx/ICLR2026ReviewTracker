{"id": "Y6lVVJHiwN", "number": 22751, "cdate": 1758335025147, "mdate": 1763747899646, "content": {"title": "FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization", "abstract": "We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. To handle communication limits, FedSGM incorporates bi-directional error feedback, correcting the bias introduced by compression while explicitly understanding the interaction between compression noise and multi-step local updates. We derive convergence guarantees showing that the averaged iterate achieves the canonical $\\mathcal{O}(\\frac{1}{\\sqrt{T}})$ rate, with additional high-probability bounds that decouple optimization progress from sampling noise due to partial participation. Additionally, we introduce a soft switching version of FedSGM to stabilize updates near the feasibility boundary. To our knowledge, FedSGM is the first framework to unify functional constraints, compression, multiple local updates, and partial client participation, establishing a theoretically grounded foundation for constrained federated learning. Finally, we validate the theoretical guarantees of FedSGM via experimentation on Neyman–Pearson classification and constrained Markov decision process (CMDP) tasks.", "tldr": "", "keywords": ["Federated Learning", "Distributed Optimization", "Constrained Optimization"], "primary_area": "optimization", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/00aea456524faeb905ae1584af9de84f76759631.pdf", "supplementary_material": "/attachment/c620a251ded9aff68453511ba6f203cfa2b18c39.zip"}, "replies": [{"content": {"summary": {"value": "The authors propose FedSGM, a unified framework for federated constrained optimization that addresses four challenges in federated learning: functional constraints, bidirectional communication compression, multi-step local updates, and partial client participation. Building on the Switching Gradient Method (SGM), FedSGM performs primal-only, projection-free updates that alternate between minimizing the objective and the constraint, enabling feasibility without dual variables. The authors derive convergence guarantees for both hard and soft switching regimes, proving an optimal $\\mathcal{O}(1/\\sqrt{T})$ rate even with biased compression (via error feedback) and random client sampling. Extensive theoretical analysis covers convex functional constraints, communication noise, and participation variance. Empirical results on Neyman–Pearson classification and constrained MDP tasks demonstrate that FedSGM achieves constraint satisfaction and stable convergence under heterogeneous and lossy federated settings."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- This work tackles a challenge: federated optimization with constraints (such as fairness or safety requirements) in realistic federated learning (FL) conditions, which involve limited communication and sporadic client participation. Previous FL methods primarily address either heterogeneity (FedAvg variants) or constraints (e.g., FedAvg with projection) or compression. \n\n- By building on the Switching Gradient Method, FedSGM avoids expensive projections or dual updates needed in prior constrained FL approaches (e.g., penalty/ADMM methods).\n\n- The paper provides a rigorous theoretical analysis covering multiple challenging aspects equally. It proves convergence for both hard and soft switching regimes under a broad range of settings (full or partial client participation, with or without compression, and multiple local steps).\n\n- The paper is generally well-written and structured."}, "weaknesses": {"value": "- The theoretical guarantees rely on the convexity of both the objective and constraint functions (Assumption 1), along with bounded Lipschitz constants and sub-Gaussian noise assumptions. This is a standard assumption for proving $O(1/\\sqrt{T})$ rates, but it limits the immediate applicability to realistic federated learning settings (most real federated tasks, such as deep neural network training, are nonconvex).\n\n- Combining constraints, compression, local updates, and partial clients in a single framework is beneficial, but the technical novelty of each component may appear incremental compared to prior art. FedSGM essentially merges well-known techniques, such as switching gradient updates for constraints, error-feedback compression, and multi-step local SGDs (similar to FedAvg), and extends existing analyses to their combined application.\n\n- Although the two case studies are relevant, the experiments are somewhat limited in diversity and scale."}, "questions": {"value": "- In practice, how does FedSGM handle non-convex objectives? While the theory assumes convex $f_i$ and $g_i$, the experiments involve neural networks and reinforcement learning (both non-convex). Does the algorithm still converge reliably in these settings? \n\n- Could the authors provide guidance or intuition on selecting the soft switching parameters (such as the smoothing coefficient $\\beta$ or threshold $\\epsilon$ in the sigmoid function)? How sensitive is FedSGM’s performance to this choice? \n\n- How does FedSGM compare against simpler baseline approaches for constrained federated learning? For instance, one could run FedAvg with a penalty for constraint violation or project the global model onto the constraint set each round (if feasible).\n\n- The analysis introduces the factor $\\Gamma(q,q_0)$ to capture compression in presence of local steps. Could the authors elaborate on how error feedback interacts with multiple local SGD steps?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "BoqhcEQqnq", "forum": "Y6lVVJHiwN", "replyto": "Y6lVVJHiwN", "signatures": ["ICLR.cc/2026/Conference/Submission22751/Reviewer_Q7y9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22751/Reviewer_Q7y9"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761686972841, "cdate": 1761686972841, "tmdate": 1762942372111, "mdate": 1762942372111, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes FEDSGM, a unified framework for federated constrained optimization that simultaneously handles four major challenges: (1) functional constraints, (2) bi-directional communication compression, (3) multiple local updates per round, and (4) partial client participation. Built upon the Switching Gradient Method, FEDSGM adopts a projection-free, primal-only update scheme that avoids dual variable tuning and inner-loop projections. The framework incorporates error-feedback mechanisms to correct compression bias and establishes an $O(1/\\sqrt{T})$ convergence rate under convex objectives. Experiments on Neyman–Pearson classification and constrained MDPs demonstrate convergence and constraint satisfaction."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The work is the first to combine constraint handling, compression, multiple local steps, and partial participation in a single theoretical framework.\n2. The convergence proof carefully separates optimization and sampling errors and analyzes the bias introduced by compression and local drift. Both hard and soft switching variants are theoretically justified.\n3. FEDSGM avoids inner projections and dual updates, leading to a lightweight and practical approach for resource-constrained federated systems.\n4. Experiments on Neyman–Pearson classification and CMDP tasks confirm the feasibility and stability of the proposed method."}, "weaknesses": {"value": "1. The theory assumes convex objectives and constraints, which restricts applicability to deep federated learning scenarios. The extension to nonconvex settings is not discussed.\n2. Experiments are restricted to small datasets and tabular RL tasks. Results on large-scale or nonconvex benchmarks (e.g., image classification or language models) would greatly strengthen the empirical claims.\n3. The experimental section lacks strong comparisons with recent constrained FL baselines (e.g., primal-dual or penalty-based methods), making it difficult to evaluate relative performance.\n4. No experiments under heterogeneous data distributions."}, "questions": {"value": "See weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "VNzc0sQJCk", "forum": "Y6lVVJHiwN", "replyto": "Y6lVVJHiwN", "signatures": ["ICLR.cc/2026/Conference/Submission22751/Reviewer_PoVG"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22751/Reviewer_PoVG"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761771506379, "cdate": 1761771506379, "tmdate": 1762942371850, "mdate": 1762942371850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes a unified theoretical framework for federated learning optimization that jointly captures several key aspects: constraints, partial participation, local computation, and bidirectional communication compression with error feedback. The framework generalizes many existing FL methods and, if it indeed recovers their best-known convergence rates across all settings (as claimed), it would constitute a strong theoretical contribution. The paper centers on the functional constraints and considers both hard and soft switching for satisfying them."}, "soundness": {"value": 3}, "presentation": {"value": 1}, "contribution": {"value": 3}, "strengths": {"value": "- The unification of these aspects of federated learning (local steps, communication compression \\& error feedback, partial participation) under one framework is meaningful and relevant. If the framework recovers the best-known convergence rates for all covered scenarios of literature (as claimed), it is worth publishing on its own.\n\n- The authors provide high-probability convergence guarantees for both hard and soft switching."}, "weaknesses": {"value": "**Constraint formulation**: The novelty and significance of the constraint formulation seem to be overstated. Assumption 3 restricts the generality of the framework, limiting the scope of the unification.\nHard and soft switching closely resemble well-known approaches for minimizing an unconstrained regularized objective, with the regularizer R chosen as: R(w)=0 if G(w)<=\\eps and \\infty otherwise (see the formulation [1]). In this viewpoint, the literature review seems to be inadequate.\n\n> [1] Condat, Laurent, and Peter Richtárik. \"Murana: A generic framework for stochastic variance-reduced optimization.\" Mathematical and Scientific Machine Learning. PMLR, 2022.\n\n**Practical relevance:**\nPractical relevance of the unified framework is unclear: FL is a diverse field, and it is not evident which real-world FL setups require all of the newly-enabled components simultaneously.\n\n**Experimental comparison:**\nExperiments are limited and mostly illustrative. I think that there is a lot of missed potential. Having the unified framework, one can also compare the different aspects of the framework to answer fundamental questions of federated learning -- what is more important: to have more clients in partial participation, or to have more bits in the compresion schemes, or to do more local steps?\n\n**Presentation issues:** The paper needs a major revision before considering publishing.  There are missing definitions (convexity, Lipschitz continuity), a lack of formal rigor, and a disjointed and difficult-to-follow narrative."}, "questions": {"value": "- How is the proposed notion of hard/soft constraint switching connected to projection (or inexact projection) onto the feasible set {w:G(w)<= \\eps}?\n\n- Is there a practically significant configuration (e.g., combination of compression, partial participation, local steps, and constraints) that is new and not covered by prior FL works?\n\n- Can the authors provide insights (both theoretical and empirical) on the relative importance of **i)** number of clients in partial participation, **ii)** number of bits in compression, and **iii)*** number of local updates?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "l1mJYoGJlW", "forum": "Y6lVVJHiwN", "replyto": "Y6lVVJHiwN", "signatures": ["ICLR.cc/2026/Conference/Submission22751/Reviewer_BCyy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22751/Reviewer_BCyy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002976375, "cdate": 1762002976375, "tmdate": 1762942371486, "mdate": 1762942371486, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces FEDSGM, a unified framework for federated constrained optimization designed to simultaneously address four major challenges: functional constraints, communication bottlenecks, multiple local updates, and partial client participation. Proposed approach is built on the Switching Gradient Method (SGM). FEDSGM provides projection-free, primal-only updates, avoiding the need for expensive dual-variable tuning. To manage communication limits, it incorporates bidirectional error feedback to correct bias from compression, and its theoretical analysis explicitly models the interaction between this compression noise and the client drift from multiple local steps. The authors derive convergence guarantees showing the averaged iterate achieves the canonical $\\mathcal{O}(1/\\sqrt{T})$ rate, with high-probability bounds that isolate the sampling noise from partial participation. Additionally, a \"soft switching\" variant is proposed to stabilize updates near the feasibility boundary. The framework's efficacy is validated empirically on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper provides a rigorous and extensive theoretical analysis of its framework.\n2. Well-motivated problems and applications."}, "weaknesses": {"value": "1. Lack of Empirical Validation: The authors have not compared their proposed method with the state-of-the-art. The experiments (Section 4) only compare FEDSGM against a \"Centralized\" (i.e., non-federated) version of itself which is an ablation study.\n \n2. It demonstrates that the federated setting introduces a performance cost (which is expected) but tells us nothing about whether FEDSGM is better than any other existing method.\n\n3. Communication Efficiency:  In this paper, the authors does not propose a new communication-efficient technique. It merely \"incorporates\" standard  methods like Top-K compression and Error Feedback. Moreover, their method added new communication overhead. Algorithm 1 explicitly requires a \"Constraint query\" (lines 3-4) where all clients send their constraint value $g_j(w_t)$ to the server, which then aggregates and broadcasts the switching decision. This is a full, separate communication round-trip that must happen before the main gradient/model update. This added synchronisation step directly contradicts the goal of reducing communication bottlenecks."}, "questions": {"value": "1. I want to see a detailed comparative analysis with SOTA (e.g., Federated Frank-Wolfe, and other projection-free methods ) to prove the superiority of the proposed approach."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "pnBHdsVgMh", "forum": "Y6lVVJHiwN", "replyto": "Y6lVVJHiwN", "signatures": ["ICLR.cc/2026/Conference/Submission22751/Reviewer_RqPZ"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22751/Reviewer_RqPZ"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762129532539, "cdate": 1762129532539, "tmdate": 1762942370905, "mdate": 1762942370905, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces FEDSGM  for federated constrained optimization. It aims to address four challenges in such federated setting: 1. functional constraints 2. communication bottlenecks 3. multiple local updates, and 4. partial client participation. The proposed method is projection-free, and has only primal updates. Convergence guarantees are derived under various settings for convex objectives and constraints."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The work is the first one formulating federated constrained optimization under compression, partial participation and multiple local updates.\n2. The theoretical results support the work."}, "weaknesses": {"value": "1. The theoretical results are limited to when both objective functions as well as the constraints are convex, which strongly limits their applicability.\n\n2. The results are obtained for when gradient descent (GD) is used for clients' local updates, which makes it more limited.\n\n3. Although the work considers partial participation, the number of participating clients in each round is fixed to $m$, which highlights another limitation.\n\n4. The experimental results in the main body of the paper are limited. The proposed approach is evaluated against only a centralized method (no FL)."}, "questions": {"value": "1. What is the most valuable deliverable of this work? Is it the developed theory for when we have all the compression, partial participation, constraints on objectives and  multiple local updates? If so, despite being valuable, the applicability of the results is still limited.\n\n2. Introducing non-convexity and SGD will make the analysis harder, but more applicable. Im wondering if the authors have explored these directions? and what is the difficulty?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "FWIdpOhh3p", "forum": "Y6lVVJHiwN", "replyto": "Y6lVVJHiwN", "signatures": ["ICLR.cc/2026/Conference/Submission22751/Reviewer_yKoE"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22751/Reviewer_yKoE"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22751/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762230224819, "cdate": 1762230224819, "tmdate": 1762998365712, "mdate": 1762998365712, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}