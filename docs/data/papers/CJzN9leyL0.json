{"id": "CJzN9leyL0", "number": 10507, "cdate": 1758174247241, "mdate": 1759897646707, "content": {"title": "Universal Human Pose Representation for Multi-Modal Active Sensing", "abstract": "We propose UniversalPose, a unified pose estimation framework that supports a wide range of sensing modalities, including WiFi, mmWave, acoustic, LiDAR, and depth. While recent methods have explored such alternative modalities to improve robustness in situations where conventional RGB-based approaches often fail (e.g., in low-light or occluded environments) and have privacy issues, they typically rely on modality-specific architectures, which limit their scalability and generalization to new sensor types.\nUniversalPose addresses these limitations by transforming all inputs into a shared representation of token sequences, enabling a single architecture to handle heterogeneous data formats. To ensure efficient and stable learning, we introduce pseudo-3D positional embeddings and apply multi-modal locality-aware self-attention, even for modalities without explicit spatial coordinates.\nMoreover, adopting such a modality-agnostic representation allows multi-modal fusion via simple token concatenation, which improves performance without architectural modifications.\nExtensive experiments demonstrate that UniversalPose achieves comparable or superior accuracy to modality-specific expert models while supporting multiple modalities through joint training. Moreover, with synchronized multi-modal inputs, the same architecture outperforms the existing state-of-the-art fusion model. Our code will be publicly available.", "tldr": "We propose UniversalPose, an unified human pose estimation framework to process diverse modalities.", "keywords": ["Human sensing", "Deep learning", "wireless signal"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7fa274a8a2354c72dd0a87875988d80cf93c5c41.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces UniversalPose, which is a unified framework for 3D human pose estimation that supports a wide range of non-RGB sensing modalities, including WiFi, mmWave, acoustic, LiDAR, and depth. The core idea is to overcome the limitations of modality-specific architectures, which suffers from poor scalability and difficulty with large-scale pre-training. Experiments across seven sensing settings demonstrate that UniversalPose achieves accuracy comparable to or better than specialized, single-modmodality expert models."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clearly written paper and easy to follow.\n- Propose a strong unified model that achieve SOTA performance across multiple benchmarks.\n- The model consistently achieves comparable or superior human pose estimation performance to highly specialized, single-modality baselines, validating the effectiveness of the general-purpose, unified representation. \n- The results on cross-signal generalization via fine-tuning also strongly support the pre-training paradigm."}, "weaknesses": {"value": "- Given the extreme differences between modalities (e.g., mmWave vs. WiFi tensor), the used tokenizers bear a critical burden or loading in transforming representation. A deeper analysis for the minimal design of these tokenizers, especially considering their role in mapping complex raw data into the unified token space can enhance the insights from this work.\n- Tab.2 shows that joint training slightly degrades performance for some modalities compared to separate training, a discussion or analysis of representational trade-offs during the joint training process would strengthen the paper.\n- The major differences with X-Fi is shifting the parameters to an universal encoder. Why deos architecture like X-Fi only support synchronized multimodal datasets for training?"}, "questions": {"value": "I hope the author can address my concerns in weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "l7RGhLo89q", "forum": "CJzN9leyL0", "replyto": "CJzN9leyL0", "signatures": ["ICLR.cc/2026/Conference/Submission10507/Reviewer_G2H2"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10507/Reviewer_G2H2"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760595317022, "cdate": 1760595317022, "tmdate": 1762921793560, "mdate": 1762921793560, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents UniversalPose, a Transformer-based framework for human pose estimation across multiple sensing modalities (WiFi, mmWave, LiDAR, acoustic, depth). Using a shared encoder for asynchronous, unpaired multimodal training is an interesting approach, and the evaluation includes comprehensive single-modal baselines. While some modality-specific components remain and certain design choices (e.g., ReZero residuals, asynchronous training) lack full empirical justification, the work demonstrates reasonable cross-modality generalization."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "• The paper proposes UniversalPose, a unified Transformer-based framework capable of processing heterogeneous sensing modalities (WiFi, mmWave, acoustic, LiDAR, depth) under a single shared encoder, leveraging asynchronous, unpaired datasets for training.\n\n• A sufficient number of single-modal baselines (e.g., MetaFi++, Point Transformer, TSP2Pose, A2J) are included to validate that UniversalPose maintains or surpasses modality-specific performance while using shared parameters.\n\n• The work demonstrates strong cross-modality generalization through joint pre-training and fine-tuning, achieving competitive or better accuracy even on unseen modalities (e.g., BGM audio) compared to modality-specialized networks."}, "weaknesses": {"value": "•\tThe paper claims to “unify all modalities within a single set of parameters and remove the need for modality-specific encoders,” yet the model still depends on modality-specific tokenizers, positional encoders and projection heads. Thus, it does not entirely remove modality-specific components, and the true scalability gain is probably overstated.\n\n•\tAlthough the model shows improved fusion results on the fixed D+L+R+W combination, it lacks experiments on varying modality combinations (e.g., D+R, L+W) or tests of generalization to unseen modality pairs (e.g. train on D+L+R+W and inference on R+W).\n\n•\tThere is no direct comparison with fusion frameworks tailored for specific modality combinations (e.g., RGB–LiDAR fusion in autonomous driving). These limit understanding of whether the proposed universal encoder is competitive in specialized tasks."}, "questions": {"value": "•\tHow does a simple MLP-based tokenizer outperform or match modality-specific encoders in extracting discriminative modality-dependent features? \n\n•\tWhat is the motivation for embedding the ability to exploit complementary information across modalities directly within the shared encoder, rather than employing dedicated fusion or cross-modal interaction blocks as in prior multimodal frameworks? From an intuitive perspective, what advantages does this integrated design offer in terms of representation learning or scalability?\n\n•\tConsidering the substantial discrepancies among modalities, gradient conflicts during asynchronous joint training are a common challenge in multimodal learning. In this context, what are the advantages of employing a shared universal encoder to extract pose features across heterogeneous modalities? Moreover, while the authors attribute training stability to the introduction of ReZero-based residual connections, the ablation results in Table 2 line 329 and 330 show only marginal improvements, which do not convincingly substantiate the effectiveness of this design. To strengthen the argument for ReZero’s contribution, the authors could visualize training loss curves with and without ReZero to illustrate convergence stability.\n\n•\tIn the proposed model, each modality is equipped with its own projection head. During multi-modal fusion, how are these modality-specific heads handled when multiple modalities are concatenated? Is a unified prediction head employed, or is there a joint optimization scheme that integrates outputs from different heads?\n\n•\tThe three training settings ((1) asynchronous joint pre-training, (2) cross-signal generalization, (3) fusion) are insufficiently detailed. How are batches sampled across asynchronous datasets? How is the modality imbalance handled during optimization?\n\n•\tWhen combining various modality embeddings of different scales and distributions via simple token concatenation, how does the model prevent imbalance or dominance by specific modalities?\n\n•\tFor experiments in table 4, what amount of data was used to fine-tune the pretrained model on the unseen domain, and what specific training procedure does the “from-scratch” variant refer to? Furthermore, what is the underlying intuition or rationale for expecting that pretraining on other modalities can effectively complement the adaptation process to an unseen modality, given the substantial information gap that exists between different sensing modalities?\n\n•\tThe paper emphasizes the advantage of unpaired, asynchronous training, yet experiments only show same-modality or same-combination tests. How does asynchronous single-modality training concretely enhance multi-modal inference without synchronization?\n\n**Minor suggestions:**\n\n•\tIn Table 4, it would be better to explicitly define PE and PA-PE as MPJPE and PA- MPJPE, respectively. \n\n•\tIn Table 7, the caption is misaligned with the table content."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "d5jGV6HiUI", "forum": "CJzN9leyL0", "replyto": "CJzN9leyL0", "signatures": ["ICLR.cc/2026/Conference/Submission10507/Reviewer_cPmu"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10507/Reviewer_cPmu"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761567372737, "cdate": 1761567372737, "tmdate": 1762921792931, "mdate": 1762921792931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces UniversalPose, a new Transformer–based human pose estimation framework, which unifies multiple sensing modalities (WiFi, millimeter wave, LiDAR, depth, and acoustics) into a shared token representation space, allowing for the use of a single architecture to handle different modalities."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1.The paper proposes a Transformer–based backbone network that can process multi-modal signals within the same parameter set and perform joint training on cross modal unpaired data.\n2.The experiments are comprehensive, with evaluations including multiple datasets and baseline models, demonstrating Asynchronous Joint Pre-training, Cross-Signal Generalization, and Multi-Modal Fusion."}, "weaknesses": {"value": "1.The author claims to use a single architecture to process multi-modal signals without the need for specialized encoders and architectures for each modality, but in fact, each modality still needs to be processed separately, such as using Point Transformer to process LiDAR, using different MLPs to process mmWave, WiFi, Acoustic, etc., and the obtained features need to be passed through modality dependent MLP heads to produce the final pose prediction. If a completely new modality is added, the tokenizer needs to be redesigned.\n2.Although the paper focus on better utilizing multi-modal data, when UniversalPose only uses single-modal data, its performance are likely to degrade to below those of specialized expert models.\n3. I don't think there's any advantage to using so many modalities. For the current task, more modalities isn't necessarily better. If using just one or two modalities can achieve the same effect as using all modalities, then I don't think the additional cost of using all modalities is justified.\n\nMinor mistakes:\n1.L303 ‘the Person-in-WiFi 3D as E05’ should be E06.\n2.Figure 3: Point Transformer (M) and Point Transformer (L) are reversed."}, "questions": {"value": "1.In Accuracy comparison in the in-training setting, although UniversalPose performs better than other expert models, training with multi-modal data results in a larger total amount of training data compared to training with single-modal data. A fairer baseline would be to train a UniversalPose instance separately for each modality (i.e. \"Ours (Separate)\"), but these results are only presented in the ablation experiment (Table 2) and there is insufficient direct comparison with the expert model. To what extent does the performance of this method stem from the unified \"generalist\" architecture itself, and to what extent does it stem from its ability to train with larger and more diverse amounts of data?\n2.Are the ‘scratch’ in Table 4 and Figure 4 trained using only the target modality? Compared to ‘retrained’, ‘scratch’ uses less data for training? The scratch results here are worse than A2J and BGM2Pose. Does this further indicate that when UniversalPose only uses single-modal data, its performance will degrade to inferior to the expert model."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "JTyGFVJrH4", "forum": "CJzN9leyL0", "replyto": "CJzN9leyL0", "signatures": ["ICLR.cc/2026/Conference/Submission10507/Reviewer_gjws"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10507/Reviewer_gjws"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761707299326, "cdate": 1761707299326, "tmdate": 1762921792510, "mdate": 1762921792510, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "UniversalPose presents a unified Transformer framework for 3D human pose estimation that processes five heterogeneous sensing modalities (LiDAR, mmWave, WiFi, depth, acoustics) through a single shared-parameter encoder. The method converts all inputs into token sequences using lightweight modality-specific tokenizers and processes them with a universal encoder employing pseudo-3D positional embeddings and locality-aware self-attention."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "+ The paper is well written and easy to follow.\n\n+ The paper provides extensive benchmarking across 5 modalities and 8 datasets. The proposed unified framework achieves competitive or state-of-the-art performance on several single modalities (e.g., mmWave, LiDAR) and demonstrates a powerful ability for cross-modal transfer and few-shot adaptation to new modalities.\n\n+ The unified representation enables state-of-the-art multi-modal fusion through simple token concatenation, outperforming more complex fusion baselines like X-Fi. This elegantly shows the power of the learned shared representation space."}, "weaknesses": {"value": "- While the application to active sensing is novel, the overarching concept of a unified Transformer for multi-modal data has been explored in other domains (e.g., Omnivore). The paper could more clearly delineate its conceptual advance beyond this application shift.\n\n- There is the complete lack of computational efficiency analysis. The paper claims \"scalability\" and \"efficiency\" as key benefits but provides no data on inference speed (FPS), FLOPs, latency, or a comparison of computational cost against deploying multiple specialized experts. Given the O(n²) complexity of self-attention with concatenated tokens, this omission severely undermines the assessment of the practical utility and real-world deployment trade-offs of the proposed method.\n\n- The paper showcases successes but does not systematically analyze failure cases or limitations. There is no discussion of scenarios where the unified model underperforms modality-specific experts, how performance degrades with corrupted or missing modalities, or the specific conditions that lead to the training instability mentioned in Section 3.3. This leaves the reader without a clear understanding of the method's boundaries.\n\n- The comparison is thorough against modality-specific experts and the X-Fi fusion model, but it lacks a comparison to other unified backbone architectures (e.g., an adapted Omnivore) to prove that the specific technical choices (pseudo-3D embeddings, local attention) are the key drivers of performance.\n\nOverall, the paper presents an ambitious and technically sound unification framework with good results across a wide range of sensing modalities. However, more analysis such as computational analysis and robustness analysis are needed. I would upgrade the rating if all the concerns could be addressed and there is no novelty concerns raised by other reviewers."}, "questions": {"value": "- In which specific scenarios or conditions does UniversalPose fail or perform worse than a modality-specific expert model? Can you provide a qualitative or quantitative analysis of these failure cases?\n\n- The pseudo-3D coordinates are central to the method. Was the choice of a 3-dimensional space ablated? What is the performance and stability impact of using 2D or higher-dimensional spaces?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "BUyRYUloJy", "forum": "CJzN9leyL0", "replyto": "CJzN9leyL0", "signatures": ["ICLR.cc/2026/Conference/Submission10507/Reviewer_bGJ8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10507/Reviewer_bGJ8"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10507/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762023446407, "cdate": 1762023446407, "tmdate": 1762921792152, "mdate": 1762921792152, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}