{"id": "0TXnU2Jtah", "number": 18386, "cdate": 1758287100328, "mdate": 1759897107319, "content": {"title": "Modular Distillation Makes Small Models Think Like Big Ones", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance in knowledge-sensitive reasoning tasks, but their practical application is still restricted by high computing demand. To address these challenges, we propose a novel modular distillation framework that breaks down knowledge-intensive reasoning tasks into three distinct components: an Analyzer for question decomposition, a Informant for context building, and a Reasoner for step-by-step reasoning inference. Unlike previous distillation methods that focus only on matching final outputs or step-by-step reasoning, our approach introduces a structured pipeline that enables the student model to learn both the analytical and reasoning abilities of the teacher model, while also capturing the teacher’s internal knowledge to guide more accurate and informed inference. This architecture improves interpretability, efficiency, and modularity, allowing for independent optimization of subcomponents. Empirical tests on three different benchmarks—OBQA, StrategyQA, and MedQA—show that our framework outperforms monolithic baselines in accuracy and computing efficiency while achieving competitive performance with much smaller models. Our findings demonstrate that smaller language models can do reasoning more efficiently when the whole process is divided into more manageable distinct components. This modular approach offers a practical and transparent alternative to relying on extremely large, resource-intensive models.", "tldr": "", "keywords": ["Large Language Models (LLMs)", "Knowledge Distillation", "Reasoning Distillation", "Question Decomposition", "Efficient Reasoning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/5ea15ae4530bd2d28007b9feb20714d330b654ed.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes a modular distillation framework that transfers reasoning abilities from a large teacher model to a smaller student by breaking reasoning into several interpretable modules. The goal is to help small models mimic large reasoning LLMs’ internal process more efficiently and effectively."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "The problem of making small models perform comparably to large ones is both interesting and highly practical—especially for reasoning LLMs, where inference cost becomes huge as response length grows. Tackling reasoning efficiency through modular distillation is a direction that could be valuable for real-world deployment.\n\n**I didn't read the proof part carefully because I don't have a strong theory background. So I may miss the strengths that lie in that part.**"}, "weaknesses": {"value": "I’m mainly concerned about the generalizability of the approach, since it imposes a strong human prior on what the teacher’s reasoning structure should look like.\n+ Conceptually, this is a strong assumption and may not generalize well, given that SOTA reasoning LLMs often exhibit very diverse reasoning patterns.\n+ Empirically, according to this paper (https://arxiv.org/abs/2503.01307), SOTA reasoning models go beyond just decomposition, grounding, and synthesizing—they can also branch or perform self-reflection, which the current method doesn’t capture.\n+ This connects to a weakness in experimental setup: the evaluation only tests weaker base models (e.g., not ones like Qwen2.5 or Qwen3 also at the 1.5B/3B-level ) and on relatively simple reasoning benchmarks (e.g., StrategyQA). Stronger models on harder datasets (like MATH or AIME) would show whether the approach scales to more diverse reasoning behaviors."}, "questions": {"value": "Is the DeepSeek model used here R1 or V3? Please clarify this in the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Do2lTz4GDf", "forum": "0TXnU2Jtah", "replyto": "0TXnU2Jtah", "signatures": ["ICLR.cc/2026/Conference/Submission18386/Reviewer_RgdS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18386/Reviewer_RgdS"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission18386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760657038270, "cdate": 1760657038270, "tmdate": 1762928092060, "mdate": 1762928092060, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a modular distillation framework, which decomposes a model’s reasoning capability into three specialized modules: Analyzer, Informant, and Reasoner. During training, each module is optimized independently through three dedicated loss functions, enabling them to acquire their respective specialized competencies. The framework is evaluated on OBQA, StrategyQA, and MedQA benchmarks. Experimental results demonstrate that the proposed modular approach outperforms traditional monolithic model baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Low memory footprint: The system achieves performance exceeding that of a distilled 8B model using three 3B models, making it suitable for deployment on resource-constrained or edge devices.\n\n- Modularity: Each component (Analyzer, Informant, Reasoner) can be independently optimized, fine-tuned, or replaced, allowing the system to adapt to diverse applications without retraining the entire model.\n\n- Theoretical justification: The paper provides theoretical evidence supporting modular distillation over conventional approaches, demonstrating that the modular method yields a tighter Evidence Lower Bound (ELBO), higher statistical efficiency (via Fisher information advantages), and lower approximation error."}, "weaknesses": {"value": "1. Error propagation: The system follows a sequential pipeline (Analyzer, Informant, Reasoner). If the Analyzer fails to generate high-quality sub-questions, the errors propagate downstream, degrading the performance of the Informant and Reasoner and potentially leading to reasoning failure. A more detailed analysis of error propagation across the three modules could further strengthen the paper’s presentation and provide deeper insight into the limitations of modular reasoning pipelines.\n\n2. Potential inference latency: Although VRAM usage is reduced, inference involves three serial forward passes (one per module). Compared with a single larger model that performs one forward pass, this setup may increase the overall end-to-end latency. While the paper emphasizes computational efficiency in terms of FLOPs, it does not explicitly report real-world latency measurements that account for model loading and runtime overhead.\n\n3. Limited task generalizability: The framework is specifically designed for knowledge-intensive reasoning tasks and evaluated on QA benchmarks. It remains uncertain whether the “Analyze–Inform–Reason” decomposition can effectively generalize to other types of large language model (LLM) tasks, such as creative writing, summarization, or dialogue systems."}, "questions": {"value": "1. For relatively simple tasks, such as straightforward translation, the Analyzer module might be unnecessary, whereas for more complex problems, such as advanced mathematical reasoning, the Informant module may play a less critical role. Since the paper primarily evaluates on multiple-choice question datasets focusing on textual reasoning, how well does this modular distillation framework generalize to other task types and modalities?\n2. I am also curious about the number of training tokens used for each dataset in both the modular distillation and the Direct Reasoning Distillation settings. Understanding this would help clarify whether the performance gain primarily stems from the proposed method itself or from a larger training budget.\n3. Would it be feasible to generate training data through the modular framework, then mix all modules’ outputs to train a single unified model, and finally compare its performance against the modular setup? I believe this could lead to a more fair and informative comparison between modular and monolithic training strategies."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "DV31DddWRf", "forum": "0TXnU2Jtah", "replyto": "0TXnU2Jtah", "signatures": ["ICLR.cc/2026/Conference/Submission18386/Reviewer_6jyh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18386/Reviewer_6jyh"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission18386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761792046071, "cdate": 1761792046071, "tmdate": 1762928091693, "mdate": 1762928091693, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes a novel modular distillation framework (TMD) that decomposes knowledge-intensive reasoning tasks into three specialized modules: Analyzer, Informant, and Reasoner. The key contribution lies in distilling both analytical and reasoning capabilities of a teacher LLM into smaller student models, improving interpretability and efficiency. The main contributions include:\n① A structured distillation pipeline capturing latent reasoning structures beyond output matching;\n② Enhanced interpretability through modular intermediate steps;\n③ Empirical evidence that smaller models achieve comparable performance to larger monolithic baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "① The modular design explicitly models latent variables (subquestions and knowledge snippets), advancing beyond previous methods.\n② Section 3.1 provides variational bounds (ELBO) and Fisher information analysis, strengthening methodological foundations.\n③ The pipeline enables traceability of reasoning steps."}, "weaknesses": {"value": "① Comparisons are restricted to DRD, omitting state-of-the-art distillation methods (e.g., CoT distillation, retrieval-augmented KD).\n② FLOPs and memory usage are reported, but real-world metrics (e.g., latency, energy consumption) are absent. And sequential module execution may introduce overhead.\n③ Ablation tests only use 3B models, ignoring capacity allocation issues between modules (e.g., imbalanced parameter sizes)."}, "questions": {"value": "① How does TMD handle error accumulation across modules (e.g., incorrect subquestions from Analyzer affecting Reasoner)?\n② Is TMD’s performance highly correlated with teacher quality? What safeguards exist for noisy teacher-generated labels?\n③ Can the framework handle ultra-complex tasks (e.g., math reasoning requiring 50+ steps) without bottlenecks from sequential modules?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9nzAxWWuRN", "forum": "0TXnU2Jtah", "replyto": "0TXnU2Jtah", "signatures": ["ICLR.cc/2026/Conference/Submission18386/Reviewer_dHgR"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission18386/Reviewer_dHgR"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission18386/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761973073187, "cdate": 1761973073187, "tmdate": 1762928091206, "mdate": 1762928091206, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}