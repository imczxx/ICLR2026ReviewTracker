{"id": "R1NUjHPUQI", "number": 5481, "cdate": 1757914066830, "mdate": 1759897971583, "content": {"title": "Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale", "abstract": "Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale, where imagery and metadata (captions, bounding boxes) are used to generate diverse, verifiable visual questions; and (2) complexity, where a composition hardening algorithm merges simpler questions from the previous stage into harder, still verifiable visual problems. Reasoning traces are synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models.We show that finetuning Qwen2.5-7B-VL on our data yields significant gains over both the base models and strong baselines. Remarkably, our 7B model outperforms all open-data baselines across all evaluated vision-centric benchmark, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V*Bench, CV Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro, +2.98\\%) and audio understanding and reasoning (MMAU, +1.32\\%), demonstrating its  effectiveness. Finally, our comprehensive empirical analysis highlights that SFT on high-quality data is essential for effective online RL, staged offline RL matches online RL’s performance while reducing compute demands, and, notably, careful SFT can substantially improve out-of-domain, cross-modality transfer.", "tldr": "A data generation framework for visual reasoning spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions.", "keywords": ["Synthetic Data", "Visual Reasoning", "VLM", "LLM", "RL", "Reasoning Models"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d1f50f34d6cc2b7e3d394f2c34c715f607d46c49.pdf", "supplementary_material": "/attachment/f04aa05cf9a497d59c1790511f285983d466a876.zip"}, "replies": [{"content": {"summary": {"value": "The paper introduces Long Grounded Thoughts, a data generation framework designed to produce vision-centric questions across varying difficulty levels. The framework operates in two stages: (1) a scaling stage, where bounding boxes and tags are used to generate grounded VQA questions, and (2) a complexity stage, where these questions are composed into higher-level reasoning tasks. The method leverages a Vision-Language Model (VLM) to generate chain-of-thought (CoT) traces and a Large Language Model (LLM) to perform the reasoning. Experimental results on multiple benchmarks show that models fine-tuned on the generated data outperform baseline models."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "•\tThe analysis of post-training in VLMs was comprehensively conducted."}, "weaknesses": {"value": "•\tThe novelty of the study is limited. Many parts of the study are adopted from LongPerceptualThought (LPT) method which makes it an extended study. The authors include bounding box and tags in addition to the description of the image and increase the complexity of the generated questions. However, the main configuration relies on the previous LPT study. \n\n•\tThe process of reasoning is not explained and depicted clearly. VLM generates CoT with predicted answers. Dense caption is also provided for the reasoning step according to Figure 1. If we use the CoT process, then why do we need the dense caption to answer the question? How did these two affect the LLM reasoning process? Does providing predicted VLM answer create any bias for LLM reasoning step? How did models generate different answers than VLM’s predicted answer? Does the model answer the question rely on dense caption only?\n\n•\tThe method section is not explained clearly, instead it gives comparison to the LongPerceptualThought (LPT) and evaluation results."}, "questions": {"value": "•\tWhat are the contributions of the study?\n\n•\tThe authors mentioned that they teach basic cognitive behaviors: verification, backtracking and correction. However, they did not explain how the process works.\n\n•\tTable 1 compares the datasets according to their size, domain and data type. However, the study focuses on complexity and richness, besides scale. How are other datasets compared on these terms?\n\n•\tThe reasoning traces are distilled from the models, but it is unclear how reliable or trustworthy these traces are. Could the authors conduct a small-scale analysis or evaluation to assess the consistency and accuracy of the generated traces?\n\n•\tCould the authors clarify why the performance of the online RL appears to plateau, and what might have occurred around the 70K example mark to cause this shift?\n\n•\tThe study follows LongPerceptualThought (LPT) for many steps. In what ways does this study differ from it?\n\n•\tThe reviewer does not understand the connection between diversity, scalability of LPT with Figure 2? The figure shows average performance based on dataset size. How to infer diversity in this figure?\n\n•\tThe references for the Appendix are not stated well (see supp material). \n\n•\tFigure 3 shows distribution of correct rollouts by data split. Do the LPTv1 and others obtain the same size of data in the figure?\n\n•\tFigure 4 shows the cognitive behaviors in CoT. How did you analyze and evaluate the results in Figure 4?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "UVfuFlqJdD", "forum": "R1NUjHPUQI", "replyto": "R1NUjHPUQI", "signatures": ["ICLR.cc/2026/Conference/Submission5481/Reviewer_8Kih"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5481/Reviewer_8Kih"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission5481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761004209710, "cdate": 1761004209710, "tmdate": 1762918087525, "mdate": 1762918087525, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Long Grounded Thoughts (LGT), a large-scale framework for compositional visual reasoning data synthesis. It integrates VLMs and LLMs in two stages: large-scale grounded question generation and composition hardening to build complex reasoning chains. Reasoning traces distilled from models are used to train Qwen2.5-VL-7B, which achieves strong gains on V*Bench, CV Bench, and MMStar-V, surpassing both open- and closed-source baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper presents a clear and scalable framework for constructing large-scale compositional visual reasoning data. The two-stage process of grounded question generation and composition hardening is well structured and effectively implemented.\n\n2. The experiments are comprehensive, covering multiple benchmarks and training paradigms including SFT, DPO, and GRPO, and they demonstrate consistent and meaningful improvements across tasks.\n\n3. The paper contributes a large-scale reasoning dataset that enhances multimodal understanding and shows strong transferability to text and audio reasoning tasks."}, "weaknesses": {"value": "1. The paper lacks clear methodological innovation and appears primarily engineering-oriented. While the proposed framework is well executed, its core idea builds on existing reasoning-chain distillation and data synthesis methods without introducing substantially new concepts.\n\n2. The paper repeatedly emphasizes the limitations of current visual reasoning systems in the title and introduction, yet the experimental evaluation does not align with this focus. It omits reasoning-centric benchmarks such as VisuLogic and MathVision, which are crucial for demonstrating genuine reasoning improvements.\n\n3. The paper compares its results mainly against general-purpose MLLMs (MiMo-VL) rather than task-specific reasoning baselines. This limits the fairness and informativeness of the comparisons, as it is unclear whether the observed gains stem from the proposed method or differences in data and model scale."}, "questions": {"value": "Please refer to the Weaknesses section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "PtqRSqfiPB", "forum": "R1NUjHPUQI", "replyto": "R1NUjHPUQI", "signatures": ["ICLR.cc/2026/Conference/Submission5481/Reviewer_2af9"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5481/Reviewer_2af9"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission5481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761934672399, "cdate": 1761934672399, "tmdate": 1762918087273, "mdate": 1762918087273, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a scalable framework for constructing large-scale, vision-centric reasoning data, named Long Grounded Thoughts (LGT), which effectively integrates both large-scale generation and compositional complexity.\n\nBy fine-tuning Qwen2.5-VL-7B on the LGT dataset, the authors achieve state-of-the-art performance across five challenging vision-centric benchmarks.\n\nIn addition, the paper conducts comprehensive ablation studies comparing different post-training strategies, including supervised fine-tuning (SFT), direct preference optimization (DPO), and online reinforcement learning (GRPO), providing valuable insights into the effectiveness and efficiency of each method."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. **Large-Scale and Well-Structured Data.**\nThe dataset is truly large-scale, containing over 1M multiple-choice questions (MCQs) paired with corresponding chain-of-thought (CoT) rationales, making it highly suitable for large-scale supervised fine-tuning. In addition, the authors also provide DPO preference pairs to support preference-based or reinforcement learning.\n\n\n2. **Insightful Ablation Studies.**\nThe paper presents extensive and systematic comparisons across different post-training strategies (SFT, DPO, and GRPO), offering clear and valuable insights into the strengths and limitations of each approach.\n\n\n3. **Strong Empirical Results and Generalization.**\nFine-tuning Qwen2.5-VL-7B on the proposed dataset achieves state-of-the-art performance on multiple vision-centric reasoning benchmarks and even surpasses GPT-4o and MiMo-VL-7B-RL in several settings. Moreover, the results demonstrate positive cross-modality transfer (from vision to text and audio), indicating improved general reasoning capability."}, "weaknesses": {"value": "1. **Writing and Submission Quality**\n   - **(1.1) Formatting and Completeness:**  \n     Although the conference does not enforce strict formatting rules, the paper’s presentation appears **rough and incomplete** for a formal submission. It uses **numerous single-column figures and tables** within the ICLR template, where visuals and text are often interleaved in a cluttered manner. Moreover, the main paper does **not even fill the full 9-page limit**, giving the impression of an unfinished submission.  \n   - **(1.2) Insufficient Data Illustration:**  \n     Despite claiming a **1M+ scale dataset**, the supplementary material provides **only two sample data examples**. This is far from sufficient to demonstrate the richness, quality, or diversity of the dataset, which undermines the paper’s credibility as a large-scale data contribution.  \n     Overall, these issues together make the paper’s **completion level appear quite low**.\n\n2. **Content and Methodological Limitations**\n   - **(2.1) Task Format Narrowness:**  \n     While the dataset is large, it is restricted to **multiple-choice questions (MCQs)** — a relatively simple task format. This **limits generalization** to open-ended or instruction-following reasoning, which are more representative of real-world multimodal tasks.\n   - **(2.2) Limited Novelty in Core Techniques:**  \n     The data synthesis pipeline **largely builds upon LongPerceptualThought (LPT)** and similar reasoning-trace distillation approaches. The main contribution lies in **scaling and incremental improvements**, rather than proposing a fundamentally novel methodology.\n   - **(2.3) Potential Bias and Lack of Human Audit:**  \n     As a dataset construction paper, it **lacks adequate human validation or auditing**. The authors explicitly acknowledge the presence of potential **societal biases** in the Ethics section but do **not propose any mitigation strategy**. This omission raises concerns about the reliability and fairness of the released dataset."}, "questions": {"value": "1. **Generalization Beyond MCQs:**  \n   Since the proposed dataset is entirely **multiple-choice (MCQ)-based**, could models trained on LGT **generalize to open-ended reasoning or instruction-following tasks**? It would be helpful to know whether such transfer has been tested or if there are plans to include more open-ended data formats in the future.\n\n2. **Impact on Original Model Capabilities:**  \n   After fine-tuning on the LGT dataset, did the model’s **original general capabilities** (e.g., text reasoning, captioning, or general multimodal understanding) **degrade in any way**? Please clarify whether you observed any trade-offs between improving vision-centric reasoning and maintaining the model’s broader skills."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "JqJC60ZSSv", "forum": "R1NUjHPUQI", "replyto": "R1NUjHPUQI", "signatures": ["ICLR.cc/2026/Conference/Submission5481/Reviewer_yk5X"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5481/Reviewer_yk5X"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission5481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761939575750, "cdate": 1761939575750, "tmdate": 1762918086756, "mdate": 1762918086756, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper is notable as a resource contribution but less convincing as a methodological advance. On the positive side, the authors curate a large-scale (≈1M) vision–language reasoning corpus constructed via a two-stage pipeline: (i) grounded single-hop MCQs generated from image captions and object-level metadata, and (ii) “composition hardening” that merges simpler items into multi-hop questions. They further attach chain-of-thought rationales distilled from stronger models. The dataset’s scale, grounding, and intended openness are likely to be valuable to the community and could enable more rigorous benchmarking of compositional visual reasoning.\nHowever, the algorithmic core is insufficiently specified and theoretically underdeveloped. The composition procedure is implemented through prompt heuristics without a formal definition of difficulty, verifiability guarantees, or complexity controls. The reasoning-chain distillation relies on very large teacher models and ad-hoc filtering, raising concerns about reproducibility, error propagation in rationales, and the extent to which the chains reflect genuine reasoning rather than stylized verbosity. Empirically, several critical ablations are missing (e.g., training without CoT, without composition, or with open-ended targets), and the exclusive MCQ training leaves generalization to free-form VQA underexplored. Finally, some headline comparisons to proprietary systems are presented without clear apples-to-apples protocols, which weakens the strength of the claims."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Scale and accessibility. Curates ~1M image–question pairs with aligned reasoning traces—an order-of-magnitude jump over prior open corpora (e.g., LPT ~30k). The inclusion of stepwise solutions and preference labels makes the resource directly usable for SFT, reward modeling, and RL without bespoke tooling.\n\n- Grounded synthesis that resists mode collapse. Stage-1 leverages object-level signals (boxes/tags) to anchor prompts to specific regions, empirically sustaining question diversity as the corpus scales. Stage-2 composes single-hop items into multi-hop queries, injecting controlled compositionality rather than merely elongating prompts.\n\n- Data-driven performance lift at small scale. A 7B VLM fine-tuned on this corpus surpasses prior open baselines and matches/edges select closed 7B systems on multiple vision-reasoning suites, indicating that targeted reasoning supervision can trade off against parameter count. The SFT→DPO regimen reproduces most of online-RL gains at substantially lower complexity.\n\n- Structured rationale profiling. Quantifies incidence of subgoaling, backtracking, and self-verification in the chains, with higher frequencies than earlier datasets. While not causal, the analysis supports that traces encode non-trivial procedural structure likely to benefit compositional reasoning training."}, "weaknesses": {"value": "- The pipeline is a stack of heuristics with no formal objective or guarantees. “Composition hardening” is described informally; there is no principled difficulty measure, constraint set, or verification proof that the composed item requires multi-step reasoning rather than adding length.\n\n- Stage-1 (caption-->MCQ) largely reprises LPT with object tags/boxes to steer diversity; Stage-2 composition is another prompt-level heuristic; rationale “extension” mirrors prior VLM-->LLM distillation. Crucially, there is no ablation isolating Stage-2’s marginal value over scaled single-hop data; if gains are driven by volume rather than composition, the claimed advance weakens.\n\n- Chains are seeded by a VLM and elongated by an LLM, which risks early errors and producing stylized verbosity. Without human audits or automatic logic checks, it is unclear whether chains exhibit valid knowledge.\n\n- Data synthesis depends on frontier teachers (e.g., 235B models) and an LLM verifier plus regex constraints. This raises reproducibility and availability concerns and suggests sensitivity to prompt/decoder settings.\n\n- Comparisons emphasize selective wins over closed 7B systems but do not present apples-to-apples protocols (prompting, vision enablement, few-shot context). Missing baselines include: (i) no-CoT training, (ii) Stage-1-only vs. Stage-1+2, (iii) larger open models fine-tuned on a subset, and (iv) human upper bounds.\n\n- Robustness and generalization remain unclear. Training data is synthetic, MCQ-only, and anchored to a single caption source, risking domain/style narrowness. There is limited evidence for transfer to free-form tasks, diagrams, or out-of-distribution imagery."}, "questions": {"value": "- How exactly is composition hardening done, and how do you verify it truly requires multi-step reasoning?\n\n- What is the performance gap with identical data without CoT, with short CoT, and with full CoT?\n\n- What were the exact GPT-4/Claude settings (vision, prompt, shots, temperature), and do results hold under a standardized protocol?\n\n- What are filter rejection rates and a human-audited error rate/taxonomy for QAs and CoTs?\n\n- How much does performance drop when replacing 235B/671B teachers with accessible open models (13B–70B)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "rJJUUVZ0ZF", "forum": "R1NUjHPUQI", "replyto": "R1NUjHPUQI", "signatures": ["ICLR.cc/2026/Conference/Submission5481/Reviewer_U8gg"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission5481/Reviewer_U8gg"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission5481/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762492085419, "cdate": 1762492085419, "tmdate": 1762918086461, "mdate": 1762918086461, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}