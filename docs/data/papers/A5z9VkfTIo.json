{"id": "A5z9VkfTIo", "number": 14903, "cdate": 1758245350426, "mdate": 1759897342439, "content": {"title": "ItinBench: Benchmarking Planning Across Multiple Cognitive Dimensions with Large Language Models", "abstract": "Large language models (LLMs) with advanced cognitive capabilities are emerging as agents for various reasoning and planning tasks. Traditional evaluations often focus on specific reasoning or planning questions within controlled environments. Recent studies have explored travel planning as a medium to integrate various verbal reasoning tasks into real-world contexts. However, reasoning tasks extend beyond verbal reasoning alone, and a comprehensive evaluation of LLMs requires a testbed that incorporates tasks from multiple cognitive domains. To address this gap, we introduce ItinBench, a benchmark that features a distinct cognitive dimension, i.e., spatial reasoning, into trip itinerary planning while keeping the traditional verbal reasoning tasks. ItinBench evaluates various LLMs across diverse tasks simultaneously, including Llama 3.1 8B, Mistral Large, Gemini 1.5 Pro, and GPT family. Our findings reveal that LLMs struggle to maintain high and consistent performance when concurrently handling multiple cognitive dimensions. By incorporating tasks from distinct human-level cognitive domains, ItinBench provides new insights into building more comprehensive reasoning testbeds that better reflect real-world challenges. The code and dataset are attached.", "tldr": "ItinBench provide a real-world level testing bed for verbal reasoning and spatial reasoning tasks for Large Language Model.", "keywords": ["Natural Language Processing", "Dataset and Benchmark"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4ccef7e30b0b55fe5caad7cc6a3c338d9400b42a.pdf", "supplementary_material": "/attachment/f4337a84294920ff1480d5dc463c15a18f799074.zip"}, "replies": [{"content": {"summary": {"value": "This paper introduces ItinBench, a novel benchmark designed to evaluate the planning capabilities of Large Language Models (LLMs) across multiple cognitive dimensions. Its core contribution lies in moving beyond traditional evaluations focused solely on verbal reasoning by systematically integrating spatial reasoning tasks into the realistic context of travel itinerary planning. The authors design nuanced tasks and evaluation metrics to quantitatively assess LLM performance under varying cognitive loads. Experimental results demonstrate that LLMs struggle significantly when simultaneously handling verbal and spatial reasoning, and suggest that their spatial reasoning abilities may heavily rely on textual cues rather than genuine spatial cognition."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 4}, "strengths": {"value": "1. **Originality:** The work is groundbreaking in its incorporation of spatial reasoning as a first-class dimension within an LLM planning benchmark. The multi-dimensional evaluation framework, juxtaposing verbal and spatial reasoning, is conceptually novel and poses a forward-looking problem formulation.\n\n2. **Quality:** The benchmark construction is highly rigorous. The data pipeline is clearly articulated and grounded in real-world data. The evaluation metric system is comprehensive and methodical, particularly the quantitative methods devised for assessing spatial reasoning.\n\n3. **Clarity:** The paper is well-structured and logically organized. The overview of the framework and the presentation of contributions are exceptionally clear, facilitating easy understanding."}, "weaknesses": {"value": "1. **Breadth of Experimental Comparison:** While the internal task comparisons are thorough, the paper lacks performance comparisons with other state-of-the-art travel planning or spatial reasoning methods on the same dataset. Such comparisons would more clearly position ItinBench's challenge level relative to existing approaches.\n\n2. **Depth of Investigation into \"Pseudo-Spatial\" Reasoning:** The paper astutely observes the phenomenon of LLMs relying on textual cues for spatial tasks. However, a more in-depth analysis and rigorous experimental validation (e.g., controlled ablations) could be conducted to more robustly support this central claim."}, "questions": {"value": "* Q1: Spatial reasoning for LLMs has often been implemented via external tools or agent frameworks. Could the authors further clarify the specific significance and motivation behind evaluating the *inherent* spatial reasoning capability of LLMs themselves, as opposed to their ability to leverage external aids?\n\n* Q2: The finding that LLMs rely on textual cues for spatial reasoning is intriguing. Do the authors plan to design more stringent \"ablation studies\"? \n\n* Q3: Given the highly subjective nature of travel planning, how reliably do the automated metrics correlate with overall human satisfaction? Could the authors provide further validation or discussion on the alignment between their automated scores and holistic plan quality as perceived by humans?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "wPTDisUrud", "forum": "A5z9VkfTIo", "replyto": "A5z9VkfTIo", "signatures": ["ICLR.cc/2026/Conference/Submission14903/Reviewer_sPN7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14903/Reviewer_sPN7"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission14903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761398914350, "cdate": 1761398914350, "tmdate": 1762925249644, "mdate": 1762925249644, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "his paper presents ItinBench, a benchmark that evaluates LLMs across multiple cognitive dimensions by combining verbal and spatial reasoning within realistic trip itinerary planning tasks.   The benchmark consists of four progressively complex settings—pure verbal reasoning, mixed verbal–spatial reasoning, spatial reasoning with filtered data, and mixed reasoning with tool use—designed to examine how LLMs handle multi-domain reasoning and route optimization simultaneously. Comprehensive experiments reveal that current LLMs perform reasonably on isolated verbal reasoning but struggle when spatial reasoning and multi-constraint optimization are required."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. This paper broadens the research focus from general cognitive reasoning (verbal reasoning) to spatial reasoning, addressing a more challenging and realistic dimension of planning in real-world scenarios.\n2. ItinBench introduces a well-structured four-task framework (verbal, mixed, spatial, and tool-use), allowing systematic analysis of reasoning trade-offs under increasing complexity.\n3. The paper is clearly written and well-organized, making it easy for readers to follow the methodology and findings."}, "weaknesses": {"value": "1. The spatial reasoning component in ItinBench may overlap with existing route recommendation or mapping algorithms, which can already handle similar optimization tasks without relying on LLM-based reasoning.\n2. While spatial reasoning is indeed crucial for LLMs—especially for embodied or physically grounded agents that must understand spatial relations (e.g., front–back, near–far), its current formulation in this benchmark is largely reduced to route optimization, a traditional planning problem that may not fully capture the cognitive aspect of spatial understanding."}, "questions": {"value": "n/a"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "EZYdYCcawm", "forum": "A5z9VkfTIo", "replyto": "A5z9VkfTIo", "signatures": ["ICLR.cc/2026/Conference/Submission14903/Reviewer_DQb7"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14903/Reviewer_DQb7"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission14903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761454773319, "cdate": 1761454773319, "tmdate": 1762925248744, "mdate": 1762925248744, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces ItinBench, a benchmark for evaluating LLMs' travel itinerary planning capabilities across two cognitive dimensions: verbal reasoning (preference matching) and spatial reasoning (route optimization). Using Philadelphia as a testbed with 500 restaurants, 105 hotels, and 322 attractions from Yelp, the authors design four task variants with increasing complexity and evaluate multiple frontier models (GPT-4o, o1, Claude, Gemini, Llama). The key findings reveal that LLMs achieve only ~60% validated plan rates with 15-38% unnecessary travel distance, and that models appear to rely on textual spatial cues rather than genuine geometric reasoning. While the work addresses an important gap by explicitly evaluating spatial planning alongside linguistic constraints, the paper suffers from critical methodological issues including experimental confounds, potential training data contamination, and circular evaluation dependencies that undermine the reliability of its conclusions."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Adds explicit spatial optimization metrics to travel planning evaluation. The paper introduces route optimization diagnostics (Total-DG, ECJ, ARG) based on TSP algorithms, extending existing benchmarks to quantify spatial efficiency.\n\n2. Reveals heterogeneity in how different models approach spatial tasks. The finding that certain models (o1, GPT-4o) can utilize coordinates while others depend on textual cluster descriptions provides insight into varying spatial reasoning strategies, though statistical validation is needed.\n\n3. Provides a multi-task framework attempting to disentangle verbal and spatial reasoning. The four-task progression offers a structured approach to evaluating different cognitive dimensions, despite specific implementation issues that require addressing."}, "weaknesses": {"value": "1. Experimental confound in Task 3 prevents isolating the effect of cluster hints.\nTask 3 simultaneously introduces two changes: filtering candidates to a smaller subset and providing textual cluster descriptions, making it impossible to determine whether performance improvements arise from reduced search space, spatial cues, or their interaction. A missing control condition (filtered candidates without cluster hints) is needed to support the paper's claims about the value of spatial information. Without a proper 2×2 factorial design (All/Filtered × NoCluster/Cluster), the attribution of Task 3's results to \"cluster hints\" remains speculative, undermining a key empirical claim about how LLMs process spatial information.\n\n2. Positioning as diagnostic extension of TravelPlanner requires explicit clarification.\nWhile the paper makes a valuable contribution by introducing explicit quantification of route optimization through TSP-based metrics (Total-DG, ECJ, ARG)—a dimension TravelPlanner evaluates only implicitly through feasibility checks—the overall framework closely parallels TravelPlanner's architecture, including dataset construction methodology, task structure, and evaluation philosophy. The paper should explicitly position ItinBench as a diagnostic complement that adds spatial efficiency measurement tools to existing travel planning evaluation, rather than implying a fully novel benchmark framework. This clearer positioning would help readers appreciate the specific contribution (quantitative spatial diagnostics) while acknowledging the architectural foundation it builds upon, avoiding potential perceptions of overstating novelty.\n\n3. Evaluation methodology would benefit from contamination analysis and transparency about ground truth construction.\nTwo methodological aspects warrant discussion: First, the use of Yelp data (publicly available since 2013) to evaluate models trained through 2023 raises potential contamination concerns that benchmark papers typically address through detection experiments, yet none are reported here. Second, ground truth preference ratings are extracted using LLMs (Appendix E.1), creating an evaluation pathway where LLM-generated attributes inform assessments of LLM planning capabilities. While the spatial reasoning evaluation (based on objective coordinates and TSP algorithms) appears largely insulated from these concerns, the verbal reasoning component (preference matching) may be more susceptible. These issues merit transparent discussion in a limitations section to help readers properly contextualize the findings, particularly distinguishing between the robustness of spatial versus verbal reasoning conclusions.\n\n4. Key design parameters lack justification and sensitivity analysis.\nThree core parameters appear heuristic without principled rationale or robustness testing: (a) the fixed constraint of 4 attractions per day is not grounded in user studies or domain norms; (b) the Macro-F1 threshold of 75% (allowing one missed preference out of four) is arbitrary and unexplored for its impact on model rankings; (c) the clustering rule k=candidates/5 directly affects ECJ metrics yet shows no sensitivity analysis across different k values or clustering algorithms. Without demonstrating that key findings (model rankings, performance gaps, spatial reasoning patterns) remain stable across reasonable parameter variations, the conclusions' robustness is uncertain. At minimum, ablation studies varying these parameters would strengthen confidence in the generalizability of reported results.\n\n5. Insufficient statistical rigor reduces confidence in reported differences.\nThe paper reports performance metrics across models and tasks without confidence intervals, significance tests, or effect size measures, making it unclear whether observed differences (e.g., between model performances, across task variants) are statistically meaningful or within noise margins. Given the evaluation costs and sample sizes involved, providing at least bootstrapped confidence intervals would substantially strengthen claims about model comparisons. Additionally, while aggregate failure rates are documented (~60% validation, 15-38% extra distance), the paper lacks systematic categorization of error types—such as distinguishing constraint violations from POI hallucinations from suboptimal routing—that would enhance diagnostic value and provide actionable insights for model improvement.\n\n6. Single-city, single-domain scope limits generalizability claims.\nThe benchmark is restricted to Philadelphia and tourism planning, raising questions about whether findings generalize to cities with different spatial characteristics (grid layouts vs. organic street networks, varying POI densities), other planning domains (logistics, emergency response, urban navigation), or different cultural contexts where travel preferences may differ. The paper does not discuss these scope limitations or provide evidence that the evaluated spatial and verbal reasoning patterns would replicate in other settings. While focusing on a single city for initial investigation is reasonable, the limitations section should acknowledge these constraints on generalizability and outline what aspects of the findings are likely to transfer versus remain city- or domain-specific."}, "questions": {"value": "1. Can you provide results for the missing experimental condition to resolve the Task 3 confound?\nTo isolate the effect of cluster hints from the effect of filtering, could you report performance for a \"Filtered without Cluster hints\" condition? This would complete a 2×2 factorial design (All/Filtered × NoCluster/Cluster) and allow proper attribution of the improvements observed in Task 3. If such experiments were conducted but not reported, please share the results. If not yet conducted, can you estimate the feasibility of adding this condition in a revision? This clarification is critical for substantiating the paper's claims about how textual spatial cues help LLMs process geographic information, as the current confound makes it unclear whether models benefit from reduced search space, cluster descriptions, or both.\n\n2. How does ItinBench's spatial evaluation differ from TravelPlanner's feasibility assessment?\nCould you clarify what aspects of spatial reasoning TravelPlanner evaluates (implicitly or explicitly) versus what ItinBench adds? Specifically, does TravelPlanner's constraint satisfaction include any measures of route efficiency, or does it only check feasibility without quantifying optimality? If TravelPlanner already evaluates some spatial aspects, how would you characterize ItinBench's contribution—as adding explicit quantitative diagnostics (TSP-based metrics) to complement qualitative feasibility checks, or as introducing an entirely new evaluation dimension? This distinction would help position ItinBench more precisely and set clearer expectations about the scope of novelty.\n\n3. What evidence exists regarding training data contamination and ground truth reliability?\nHave you conducted any contamination detection experiments to test whether evaluated models memorize specific Philadelphia POIs or Yelp review content (e.g., by querying models about restaurant details or checking if they reproduce review fragments)? Additionally, for the LLM-extracted preference ratings used as ground truth (Appendix E.1), can you report inter-rater agreement by having human annotators independently rate a sample of POIs and comparing with LLM extractions? Understanding the consistency between LLM-generated and human-generated ratings would help assess whether the evaluation measures genuine planning capability versus LLM-to-LLM consistency, particularly for the verbal reasoning component where this circularity is most concerning.\n\n4. What is the rationale for key design parameters, and are findings robust to parameter variations?\nCould you provide justification for the three core parameters: (a) why 4 attractions per day (are there domain norms or user studies supporting this choice?), (b) why a 75% Macro-F1 threshold for validation (what happens at 70% or 80%?), and (c) why k=candidates/5 for clustering (have you tested other ratios or algorithms like DBSCAN)? If sensitivity analyses were conducted, please share whether model rankings and key conclusions remain stable across parameter variations. If not conducted, which parameters do you believe are most critical to test, and how might results change? This would substantially strengthen confidence in the generalizability of your findings beyond the specific configuration tested.\n\n5. Can you provide statistical significance tests for reported model differences?\nThe performance gaps between models (e.g., GPT-4o vs. Claude vs. Llama) are presented as point estimates without confidence intervals or significance tests—could you report whether these differences are statistically meaningful? Given the evaluation costs, even bootstrapped confidence intervals from existing runs would help. Additionally, for the claim that certain models (o1, GPT-4o) can utilize coordinates while others cannot, what is the statistical confidence in this distinction? Are there specific tasks or metrics where this heterogeneity is most pronounced and reliably detectable? This quantification would help distinguish robust patterns from noise and strengthen claims about fundamental differences in models' spatial reasoning strategies.\n\n6. Why was Philadelphia chosen, and do findings generalize to other cities or domains?\nCould you provide the rationale for selecting Philadelphia as the evaluation city—does it have characteristics (POI density, street layout, geographic spread) that make it representative, or was the choice primarily driven by data availability? Have you conducted any preliminary validation in other cities (even on a smaller scale) to test whether key findings replicate? For instance, do models show similar validation rates (~60%) and excess distances (15-38%) in cities with different spatial structures, or does the specific geography significantly affect performance? Understanding these scope conditions would help readers assess whether the identified limitations in LLMs' spatial reasoning are general phenomena or potentially specific to Philadelphia's characteristics or the tourism domain."}, "flag_for_ethics_review": {"value": ["Yes, Legal compliance (e.g., GDPR, copyright, terms of use, web crawling policies)"]}, "details_of_ethics_concerns": {"value": "The paper uses Yelp user reviews for preference extraction without discussing compliance with Yelp's Terms of Service or addressing potential privacy concerns around using user-generated content for model evaluation. While academic use may fall under fair use, explicit acknowledgment and legal compliance discussion would strengthen the work, particularly given GDPR and data protection considerations if any European users' reviews are included."}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "z2Pq63AKG1", "forum": "A5z9VkfTIo", "replyto": "A5z9VkfTIo", "signatures": ["ICLR.cc/2026/Conference/Submission14903/Reviewer_oWSN"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14903/Reviewer_oWSN"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission14903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761828207623, "cdate": 1761828207623, "tmdate": 1762925247904, "mdate": 1762925247904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a benchmark, ItinBench, for evaluating LLMs on trip itinerary planning that, unlike prior travel-planning testbeds, is designed to stress two cognitive dimensions, verbal reasoning and spatial reasoning. The benchmark defines four tasks of increasing realism: \"all data, no route optimization\", \"all data, route optimization\",  \"filtered data, route optimization\" and \"tool use with route optimization\". For spatial evaluation, they adapt a TSP-style solver to compute optimal or near-optimal routes; for verbal evaluation, they extend TravelPlanner-style metrics to failure checks."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1. This benchmark provides a well-designed  task decomposition. The four tasks isolate where the difficulty comes from and let the reader see where LLMs fail. This kind of ablation-by-task may provide positive impact to the community. \n2. This paper reveals a notable finding: a significant improvement in models' spatial reasoning occurs when the spatial structure is provided as textual clusters (Tasks 3–4), while simply “asking” the model to optimize the route (Task 2) does not reduce Total-DG. This leads to a non-obvious yet valuable conclusion: LLMs act more as semantic planners that operate on textual cues than as geometric reasoners."}, "weaknesses": {"value": "1. This paper combines the work of Travel Planner and ITINERA, examining both constraint satisfaction capability and traditional TSP-like metrics. Given our existing Travel Planner and ITINERA, the necessity of proposing such a new benchmark seems limited, offering limited insights. \n2. The paper's title highlights \"Multiple Cognitive Dimensions,\" yet the content primarily centers on spatial reasoning. This apparent narrowing of focus risks creating a perception of overclaiming relative to the broader scope suggested by the title. \n3. Based on appendixes E.3 and E.4, the authors claim that spatial reasoning refers to the need for spatial relationships within the query, such as finding attractions that are close to each other. If this is the case, then the TripTailor benchmark [1], which requires the agent to output the time interval of activities at each location, already includes the spatial reasoning defined by the authors, and even temporal reasoning, making it a more multi-domain benchmark. Please let me know if my understanding of the article's definition of spatial reasoning is incorrect.  \n4. Single-city scope limits external validity. Everything is in one city (Philadelphia), which seems too simplistic for current travel benchmarks. At the very least, the capabilities of LLM should be validated in different cities to demonstrate the generalizability of the conclusions. \n5. Spatial “difficulty” is partially self-inflicted / text-mediated.\nThe paper wants to evaluate spatial cognition, but the biggest gains come exactly when the authors hand the spatial structure to the model in text (cluster labels). That weakens the central claim: we haven’t shown models can solve spatial reasoning; we’ve shown they can consume human-preprocessed spatial abstractions. That’s closer to “semantic alignment” than to “spatial planning.” The paper acknowledges this, but the current framing still slightly oversells “incorporate spatial reasoning” vs. “evaluate whether a model can follow spatial hints.”\n\n\n[1] TripTailor: A Real-World Benchmark for Personalized Travel Planning."}, "questions": {"value": "1. The limitations section mentions multi-city and variable-constraint settings. Do you see any blocker to programmatically generating multi-city itineraries in the same framework (can TSP generalizes)?  \n2. Do you have any preliminary numbers on at least a synthetic second city (e.g. sampling 2–3 km-shifted coordinates, or reusing Yelp from a nearby city)? Even a small table would strengthen the generalizable conclusion story."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QW6P4erwec", "forum": "A5z9VkfTIo", "replyto": "A5z9VkfTIo", "signatures": ["ICLR.cc/2026/Conference/Submission14903/Reviewer_wWEW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission14903/Reviewer_wWEW"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission14903/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761995806261, "cdate": 1761995806261, "tmdate": 1762925247235, "mdate": 1762925247235, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}