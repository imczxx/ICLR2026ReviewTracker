{"id": "VdLNKYKBRw", "number": 23516, "cdate": 1758344838632, "mdate": 1759896810909, "content": {"title": "Decoupling Planning and Control for Instructable Agents", "abstract": "Recent work on vision-language(-action) agents shows that VLMs are strong at high-level reasoning but struggle to realize plans as reliable low-latency action sequences, while world-model controllers excel at fast observation-to-action control but lack open-ended task guidance. In this work, we combine these strengths by conditioning a learned world-model controller on language so that it can act autonomously at high frequency conditioned on sparse, higher-latency textual instructions generated by vision-language models (VLMs). Our system, Speak-to-Act, includes an instructable controller that autoregressively generates high-frequency actions and can either follow language instructions from an instruction agent, or self-operate in a high-throughput environment. To train controllers to be language-instructable, we relabel segments of controller policy rollouts with instructions and optimize a behavior-cloning objective. Our framework easily supports extension to multi-agent settings that enable agent communication between VLMs using trained controllers as actuators without relying on Multi-Agent Reinforcement Learning algorithms. We report results on various embodied environments and tasks, scaling trends with larger controllers and VLMs, and ablations on instruction cadence, planning frequency, and online vs. offline planning latency. The results show that with our decoupled architecture, Speak-to-Act can flexibly switch to different VLMs and scale well to multi-agents and longer chains of reasoning achieving state-of-the-art performance on six tasks.", "tldr": "", "keywords": ["reinforcement learning; world model; multi-agent"], "primary_area": "applications to robotics, autonomy, planning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d53dd4d52bbfd550bf69e4e836f95160699c76b1.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces the Speak-to-Act framework, which effectively decouples high-level planning (VLM) from low-level control (SRRM) to construct steerable embodied agents. Its core innovation lies in training environment-specific controllers through post-hoc VLM-generated instruction labels, enabling asynchronous real-time execution and effortless scalability to multi-agent settings, i.e., without relying on multi-agent reinforcement learning (MARL). Experimental evaluations cover both single-agent and multi-agent tasks, achieving state-of-the-art (SOTA) performance on 6 out of 7 benchmarks, and demonstrating promising scalability trends for larger controllers and planners."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1.\tIngenious asynchronous online inference mechanism: It effectively prevents slow VLM reasoning from blocking the control loop, i.e., the planner can prewarm a draft in the background and only outputs a concise suffix once the instruction completion signal is triggered. This design enables real-time multi-agent scalability, with near-linear growth in action throughput, without requiring VLM fine-tuning or MARL integration.\n2.\tEfficient and extensible post-hoc instruction labeling: By summarizing replay segments with a VLM, the framework significantly increases label density without interfering with data collection. Combined with hybrid optimization of the Dreamer objective and behavior cloning loss, it ensures that the controller retains the autonomy of its base policy while allowing flexible instruction overrides.\n3.\tHighly modular architecture: The framework allows seamless substitution of different VLMs, supports hybrid use of human and VLM planners in multi-agent environments, and generalizes across diverse domains (e.g., from Atari to Pico Park)."}, "weaknesses": {"value": "1.\tLimited originality of core components: Although the overall integration is creative, elements such as language-conditioned RSSM and VLM-based planning are not novel; the asynchronous inference and post-hoc labeling techniques, while practical, primarily represent engineering optimizations rather than conceptual breakthroughs.\n2.\tFragile instruction pipeline due to VLM dependency: Both training summarization and inference planning heavily depend on the VLM, which may amplify its inherent errors, e.g., noisy summary instructions could introduce bias during behavior cloning.\n3.\tWeak empirical support for SOTA claims: The paper claims SOTA results across multiple tasks but provides only high-level metrics (e.g., success rate, throughput, instruction accuracy) without detailed baseline comparisons. While multi-agent results show scalability of the planner, the work lacks deeper comparison with centralized-training/decentralized-execution MARL methods (e.g., QMIX) or end-to-end VLA models.\n4.\tOverly idealized scalability assumptions: claimed linear scalability depends on shared controllers and parameters. However, in asymmetric multi-agent environments, individual fine-tuning may be required for each agent.\n5.\tIncomplete efficiency analysis: Although the framework relies on background VLM drafts and shared controllers, it does not quantify the actual computational overhead (e.g., VLM token generation cost in multi-agent settings) or potential I/O bottlenecks."}, "questions": {"value": "1.\tBeyond parity with the Dreamer baseline, how does Speak-to-Act outperform end-to-end VLA methods (e.g., RT-2) in terms of latency and success rate?\n2.\texperiments suggest that real-time VLM outperforms offline planning, but by what margin? Are there quantitative results correlating controller size (e.g., parameter count) with performance?\n3.\tHow does the system handle conflicts between VLM-generated instructions and environment rewards? In sparse-reward settings, how well does it perform when dense success indicators are unavailable?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "VyfCh3S3Ww", "forum": "VdLNKYKBRw", "replyto": "VdLNKYKBRw", "signatures": ["ICLR.cc/2026/Conference/Submission23516/Reviewer_Kt4V"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23516/Reviewer_Kt4V"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission23516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761536268108, "cdate": 1761536268108, "tmdate": 1762942696293, "mdate": 1762942696293, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This work introduces Speak-to-Act, a system that combines the high-level reasoning of Vision-Language Models (VLMs) with the fast, low-level control of world-model controllers. The system allows a VLM to provide sparse, high-latency language instructions to a high-frequency controller, which is trained via behavior cloning to execute these commands. This decoupled architecture achieves state-of-the-art performance on six tasks, scales well, and easily extends to multi-agent scenarios without complex reinforcement learning."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "Strength:\n\n1: The overall exposition and logical flow of the paper are clear and well-structured.\n\n2: The paper significantly outperforms the baselines and demonstrates impressive single-task performance.\n\n3: The proposed architecture is technically reasonable and well motivated. Its design shows promising scalability with respect to both model capacity (number of parameters) and multi-agent deployment, suggesting potential for broader application at larger scales.\n\n4: The strong cross-planner generalization further supports the soundness and robustness of the proposed framework."}, "weaknesses": {"value": "1: The proposed planner–controller architecture, which is presented as a core contribution of this paper, is not sufficiently novel. Similar hierarchical designs have been explored in several prior works, such as DEPS ,LS-Imagine, JARVIS-1. \n\n2: The paper lacks comparison and discussion with recent VLA(vision-language-action)-related works, such as RT-H (and other vision-language-action models). Including these would provide a clearer picture of how the proposed framework relates to existing VLA systems.\n\n3: The number of effective baselines reported in the main results is too limited, making it difficult to assess the relative performance of the proposed method.\n\n4: The paper lacks an ablation study on the controller architecture. It would be informative to explore alternative designs, such as a Transformer-based Controller, to verify whether the observed performance gains stem from the proposed structure itself or from general modeling capacity.\n\n5: The paper lacks discussion on multi-task generalization.\n\n6: The multi-agent experiments lack detailed analysis, such as case studies or a comparative discussion of the centralized vs. decentralized communication modes and their respective advantages."}, "questions": {"value": "See the weakness."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hZ4vVC6Mqv", "forum": "VdLNKYKBRw", "replyto": "VdLNKYKBRw", "signatures": ["ICLR.cc/2026/Conference/Submission23516/Reviewer_eR51"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23516/Reviewer_eR51"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission23516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761882821960, "cdate": 1761882821960, "tmdate": 1762942696102, "mdate": 1762942696102, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes Speak-To-Act, an agent framework for action generation given vision observations and optionally language instructions. The proposed method decouples the planner from controller, using a VLM to serve as a planner, and a Recurrent State Space Model (RSSM) as the controller for action predictions. The paper highlights that since the interface between the planner and the controller is natural language (embeddings), the VLMs could be switched and plugged in without fine-tuning. The paper also highlighted that the planner and the controller are asynchronous, which allows high throughout action outputs possible. The proposed method could also be extended to the Muti-agent setting through optionally natural language output, paving the way for future human-AI collaborations. The proposed method is based on DREAMER, following a similar sets of training loss, including world model reconstruction loss, value loss, actor loss, behavior cloning loss, and stop token loss. The paper conducted experiments in both the single-agent setting and the multi-agent setting, spanning across 7 environments, and multiple baseline models. The paper also evaluated the scalability of the method, low level action throughout, as well as the instruction-following accuracy of the planners. The paper also conducted a series ablation studies and generalizability of the method."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "- The paper proposed a method Speak-to-Act, that decouples agent planning from action generation\n- The proposed method is asynchronous, which supports high-frequency action throughput \n- The proposed method leverages natural language as the interface, which makes it easier to plot in and switch different VLM planners\n- The paper conducted numerous experiments across multiple environment, planner models, against multiple baselines, and demonstrated improved performance and throughput"}, "weaknesses": {"value": "1. There are a lot of great content in the work. Some of the highlights including decoupling and asynchronous planner + controller, plug and play VLMs without fine-tuning, evaluating different VLMs, controller with optional instruction following, single player VS multi players, and much more... It would be helpful to focus on just a subset of the main contributions and structure the experiments and narration around them. At the moment, the experiment results touch a little bit of each point without a much needed in depth analysis and a coherent story. For example, is the goal to evaluate different VLMs? to enable high throughput asynchronous action outputs? To study multi-agent collaborations? \n\n2. The proposed method seems to be an extension of the DREAMER work to introduce optional language instructions with very similar settings, training paradigms, model architectures, etc."}, "questions": {"value": "1. Figure 3: if one of the goals is to highlight the higher action throughput of Speak-to-Act, would it be a stronger piece of supporting evidence if the comparison happens between synchronous VS asynchronous models? (compared to centralized VS decentralized, planning modes, etc)\n2. Table 1: is missing quite a bit of performance report from prev.SOTA. Would be helpful for comparison if the models were rerun on the same tasks under similar settings \n3. Table 2: how to do the results from the table support the section title \"Generalization\""}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "dDgumI1Xcq", "forum": "VdLNKYKBRw", "replyto": "VdLNKYKBRw", "signatures": ["ICLR.cc/2026/Conference/Submission23516/Reviewer_jmVV"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23516/Reviewer_jmVV"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission23516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762002512878, "cdate": 1762002512878, "tmdate": 1762942695904, "mdate": 1762942695904, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Speak-to-Act proposes decoupling embodied decision-making into high-level planning (via VLMs) and low-level control (via trained controllers). The key innovation is an asynchronous inference framework where VLM planners generate language instructions while controllers execute actions in real-time. Controllers are trained using world-model RL (Dreamer-style) with post-hoc language annotation of replay buffer segments. The framework extends naturally to multi-agent settings through language-based coordination, achieving beating baselines on 6/7 tasks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The decoupling addresses a real problem - VLMs excel at reasoning but struggle with low-latency control, while RL controllers lack semantic understanding. The solution is elegant.\n- Beats previous state of the art on 6/7 tasks tested across different environments (Atari, Minecraft, Crafter, etc.), demonstrating broad applicability.\n- The online planning mechanism (Algorithm 1) cleverly avoids blocking - VLMs continuously refine plans while controllers execute, reducing latency at instruction boundaries.\n- Good analysis of online vs offline planning, model scaling (50M-800M controllers, 2B-72B VLMs), and instruction modes.\n-  Parameter-shared controllers with language coordination is simpler than traditional MARL while achieving strong results."}, "weaknesses": {"value": "- The post-hoc VLM annotation is a core contribution, but there's no comparison to simpler alternatives such as random instruction templates or clustering similar action sequences. How much does the VLM annotation quality matter?\n- Given the same rollout data, why not directly finetune a VLM to predict actions? This would be the most natural comparison to justify the decomposition.\n- Is natural language actually necessary, or would latent goal conditioning work equally well? Compare to VAE/VQ-VAE encodings of trajectory segments (similar to Garg et al. 2022 LISA).\n- No training time or compute requirements reported\n- When does the approach fail? What happens when VLM instructions are ambiguous or controller misinterprets them? No discussion of error modes."}, "questions": {"value": "- What happens if you replace VLM summarization with: (a) template-based instructions, (b) clustered action patterns with fixed labels, or (c) random instructions? How critical is annotation quality?\n- Can you train a VLA baseline using the same data? Specifically, finetune the VLM to directly predict action sequences from observations.\n- How does language conditioning compare to latent goal conditioning (e.g., VAE encodings)?\n- What are the training times, annotation costs, and inference overheads? How does this scale with environment complexity?\n- What types of failures occur?\n- Have you tested with human planners providing instructions? This would validate the framework's practical applicability."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "h5doOCAVyJ", "forum": "VdLNKYKBRw", "replyto": "VdLNKYKBRw", "signatures": ["ICLR.cc/2026/Conference/Submission23516/Reviewer_6BJx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission23516/Reviewer_6BJx"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission23516/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762100742936, "cdate": 1762100742936, "tmdate": 1762942693488, "mdate": 1762942693488, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}