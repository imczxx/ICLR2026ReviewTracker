{"id": "Uro84w2xz5", "number": 22047, "cdate": 1758325290547, "mdate": 1759896888972, "content": {"title": "Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning", "abstract": "Large Language Models (LLMs) often struggle with challenging, multi-step reasoning problems due to a fundamental learning gap -- Reinforcement Learning with Verifiable Rewards (RLVR) suffers from sparse rewards when correct solutions are rarely sampled, while Supervised Fine-Tuning (SFT) tends to overfit to long demonstrations through rigid token mimicry. To bridge this gap, we introduce Supervised Reinforcement Learning (SRL), a framework that reformulates problem-solving as a sequence of logical actions. SRL trains the model to learn from each action, where the model first generates an internal reasoning monologue and then commits to an action. The model receives dense rewards based on the similarity between its actions and the expert’s at each step, providing a richer signal than RLVR. More importantly, by only rewarding the action, SRL allows the model flexibility in its self-generated thought process, promoting stronger reasoning abilities than SFT. On challenging mathematical reasoning benchmarks, SRL significantly outperforms both methods. Furthermore, a curriculum that cold-start with SRL before refining with RLVR achieves the strongest results. SRL also generalizes effectively to agentic software engineering tasks, establishing it as a robust framework for various reasoning tasks.", "tldr": "", "keywords": ["Reinforcement learning", "Reasoning", "Large Language Model", "Agent"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/11aa7133bc2a4cbc3789dbb13979740a1ccba24b.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The authors present a method for supervised or assisted RL for LLMs.\nThe motivation given for the approach is that for difficult tasks, where the LLM rarely reaches the right answer, naive use of RLVR does not help.\nThus, the authors propose to break the process of generating an answer using the LLM into multiple steps.\nAt each step the LLM is given the problem and a partial answer, and is required to output the only the next step (action) to solve the problem after thinking.\nThe reward for the computed next step is computed as the normalized edit distance between the generated action and the expert predicted step.\nThis assistance or supervision using the partially completed solutions and to only focus on generating the next step helps the LLM learn better.\n\nThe method requires a dataset or expert generations with an action-based formatting, where each example can be broken into a sequence of actions or steps.\nThe definition of action can vary with domain and datasets.\nFor this paper, the authors use step-wise structure of math solutions and environment commands for SWE tasks to distinguish individual actions.\nThe authors also use dynamic sampling to remove training examples with low variance in scores in all cases including baselines.\n\nThe authors show that the method performs well and provides a boost of 3% over RLVR on math tasks (3.7% for SRL+RLVR) on average. While naive RLVR does not yield any improvement, SRL does. SRL also results in 6.4% improvement in the oracle edit task and 4.4% in end-to-end task.\n\nFor qualitative analysis, the authors state that the method allows the agent multi-step thinking which manifests as, outlining all steps in a comprehensive initial plan, on-the fly adjustments, and reflective verification. However, only one representative example has been shown for this behavior. The overall answer lengths are also similar to the base-model."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The method is simple and easy to follow.\n- Based on the motivation, the approach of providing the supervision of first few steps and a dense reward makes sense.\n- The method provides a boost in performance for math and SWE tasks."}, "weaknesses": {"value": "- Adding comparisons against other methods that use intermediate rewards can be helpful (maybe process reward model (PRM) methods, e.g. [1]).\n- Since the thinking patterns are listed as a contribution, the representative examples showing the interleaved thinking patterns and others can be expanded and analyzed.\n- Since the method heavily relies on the data being in a specific format, there should be some discussion of limitations and expansion to other domains.\n\n\n[1] Lu, J., Dou, Z., Wang, H., Cao, Z., Dai, J., Feng, Y. and Guo, Z., 2024. Autopsv: Automated process-supervised verifier. Advances in Neural Information Processing Systems, 37, pp.79935-79962."}, "questions": {"value": "- Can the authors provide a comparison to some other self or expert supervised method that rewards intermediate steps?\n- Did the authors try other reward schemes than string edit distance (like LLMs)? The same information can be presented in different sentences, and sometimes a slight difference in the text can drastically change the meaning; therefore, the edit-distance heuristic may not always hold.\n- The temperature used (1.0) does not seem to be the default to my limited knowledge. If so, how do the average@32 scores change with temperature like 0.6/0.7?\n- What was the final prompt used for inference, because the one mentioned in Appendix B states only one thinking step and outputs only one reasoning step (seems to be for training)?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "LNadCgmEna", "forum": "Uro84w2xz5", "replyto": "Uro84w2xz5", "signatures": ["ICLR.cc/2026/Conference/Submission22047/Reviewer_iZLY"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22047/Reviewer_iZLY"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission22047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761524345401, "cdate": 1761524345401, "tmdate": 1762942035949, "mdate": 1762942035949, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The SRL framework breaks down problems into a sequence of \"actions\". In each step, the model first generates an \"internal monologue\" and then executes an \"action\".\n\nSRL's reward mechanism abandons sparse final-answer rewards and rigid imitation, instead providing dense signals at every step based on the similarity between the model's \"action\" and the expert's \"action\".\n\nCrucially, SRL only rewards the \"action,\" maintaining flexibility for the \"internal monologue\". This design combines dense signals (addressing the RLVR problem) with flexible reasoning (avoiding SFT rigidity) to promote stronger reasoning abilities.\n\nExperiments show that SRL significantly outperforms SFT and RLVR on both mathematical reasoning and software engineering benchmarks. A curriculum learning strategy of \"cold-starting with SRL then fine-tuning with RLVR\" achieves the best results."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "1. It finds a clever balance between SFT and RL. The core concept of \"rewarding actions, liberating reasoning\" is an important innovation for LLM reasoning training.\n2. Comprehensive experiments were conducted on difficult benchmarks in mathematics and software engineering. The superior performance of SRL → RLVR is highly persuasive.\n3. In-depth analysis (Section 5.2): High-quality ablation studies demonstrate that fine-grained guidance is key (Table 3). Behavioral analysis shows that the model exhibits more flexible, advanced reasoning patterns, rather than simply increasing output length (Figure 4).\n4. Strong generalization ability: Successfully transferred from mathematics to software engineering tasks (Table 4), demonstrating its potential as a general-purpose reasoning framework."}, "weaknesses": {"value": "1. Dependence on Expert Data and \"Action\" Definition: SRL is highly dependent on high-quality, easily decomposable expert trajectories and the definition of \"actions.\"\n2. Rationality of the Reward Function: The reward function is based on syntax (e.g., difflib) rather than common measures like KL divergence. How can the rationality of this approach be proven?\n3. The primary weakness of this paper's methodology is its lack of novelty, as its core idea significantly overlaps with existing work: (1) Unoriginal Theoretical Foundation: Theoretically, the approach of using expert demonstrations to solve the sparse rewards problem is a classic paradigm in traditional reinforcement learning. Techniques such as Reverse Curriculum Learning and Learning from Demonstration have long been explored and validated in related fields. (2) Significant Overlap with Recent LLM Work: In the specific application of LLM reasoning, the proposed SRL framework is highly similar to the recent $R^3$ paper [1]. $R^3$ explicitly explored using expert demonstrations to train the model starting from intermediate states of the demonstration. Through a reverse curriculum, $R^3$ effectively converts sparse outcome rewards into \"approximately step-by-step supervisory signals\". The SRL paper similarly relies on decomposing expert trajectories into step-wise \"actions\" (what $R^3$ calls \"intermediate states\") and constructs partial trajectories for training. The core mechanisms of both methods are fundamentally alike. So, more discussion and comparison should be included.\n\n\n[1] Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning"}, "questions": {"value": "1. Can you clarify the difference with works like $R^3$ in the paper?\n2. Can you include more backbone models to make the paper more solid?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "QyXoKrUDnG", "forum": "Uro84w2xz5", "replyto": "Uro84w2xz5", "signatures": ["ICLR.cc/2026/Conference/Submission22047/Reviewer_JSqS"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22047/Reviewer_JSqS"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission22047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761969680890, "cdate": 1761969680890, "tmdate": 1762942035637, "mdate": 1762942035637, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes Supervised Reinforcement Learning (SRL), a new framework that bridges the gap between Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR) for complex reasoning tasks. \nThis approach provides richer learning signals than RLVR and avoids the rigid imitation of SFT. \nExperiments on mathematical reasoning and software engineering benchmarks show that SRL significantly outperforms both baselines, and a curriculum of SRL followed by RLVR achieves the best overall results."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The novel SRL framework addresses RLVR's sparse rewards and SFT's rigid mimicry by decomposing expert trajectories into step-wise logical actions with dense sequence-similarity rewards, uniquely combining SFT’s supervision and RL’s strengths and showing superior performance on math reasoning benchmarks.\n2. SRL demonstrates robust cross-domain efficacy across different multi-step reasoning tasks, proving its versatility.\n3. SRL excels in data efficiency for hard problems, using limited expert data to avoid SFT's performance degradation and enabling a high-performing SRL→RLVR curriculum."}, "weaknesses": {"value": "1. SRL relies on the decomposition of expert trajectories into logical actions without providing details on automation for unstructured trajectories. Therefore, I'm not sure if it is scalable for large-scale or unstructured tasks and whether it introduces subjective bias.\n2. The reward only considers action similarity, potentially leading to false positive rewards from flawed reasoning. This impact should be considered.\n3. The experiments are conducted solely on small to mid-sized models. It would strengthen the paper to include results or discussion on whether SRL’s advantages extend to larger model sizes, as this would provide a clearer picture of its scalability and general applicability."}, "questions": {"value": "1. I am curious how robust your difflib-based action-similarity reward is to “near-miss” actions\n2. I think the framework genuinely novel and potentially a new training paradigm for leveraging reward signals. I encourage you to evaluate SRL beyond structured domains in your paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "NA"}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "hByjm3p3hy", "forum": "Uro84w2xz5", "replyto": "Uro84w2xz5", "signatures": ["ICLR.cc/2026/Conference/Submission22047/Reviewer_CWA3"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22047/Reviewer_CWA3"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission22047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762012814119, "cdate": 1762012814119, "tmdate": 1762942035225, "mdate": 1762942035225, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Supervised Reinforcement Learning, a framework designed to enhance complex reasoning in LLMs by decomposing problems into step-wise actions. SRL combines dense, step-level rewards with flexibility in internal reasoning generation. The method is evaluated on mathematical reasoning and software engineering tasks, showing improvements over SFT and RL baselines. Key contributions include: (1) a step-wise reward signal to mitigate sparse rewards, (2) empirical gains on challenging benchmarks, and (3) a curriculum strategy (SRL→RLVR) for further refinement."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The problem formulation (e.g., action-based decomposition) is well-motivated and intuitive.\nResults on math benchmarks show consistent improvements, with ablations justifying key components.\nThe curriculum strategy (SRL→RLVR) and flexibility in internal monologue generation are valuable insights."}, "weaknesses": {"value": "The methodology bears significant resemblance to R³ (arXiv:2402.05808), which also uses a reverse curriculum with intermediate state sampling and outcome supervision for step-wise learning. Though SRL uses action similarity rewards instead of final-answer rewards, the high-level strategy of \"starting from expert states and moving backward\" is not sufficiently differentiated.\nComparisons focus on SFT and RLVR but omit advanced RL methods (e.g., DAPO, Dr. GRPO) and direct comparisons to R³."}, "questions": {"value": "None"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "99r9Y4Ij8a", "forum": "Uro84w2xz5", "replyto": "Uro84w2xz5", "signatures": ["ICLR.cc/2026/Conference/Submission22047/Reviewer_VwHa"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22047/Reviewer_VwHa"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission22047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762053060260, "cdate": 1762053060260, "tmdate": 1762942034594, "mdate": 1762942034594, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Supervised Reinforcement Learning, a framework that bridges the gap between Reinforcement Learning with Verifiable Rewards, which suffers from sparse rewards on difficult problems, and Supervised Fine-Tuning, which often leads to overfitting through rigid token mimicry. SRL reformulates problem-solving as a sequence of logical actions, where the model generates an internal reasoning monologue followed by an action, receiving dense rewards based on action similarity to expert demonstrations. This allows flexibility in reasoning while providing granular feedback."}, "soundness": {"value": 4}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The paper presents a well-motivated and innovative approach to addressing limitations in LLM training for complex reasoning, with clear writing.\n\n2. The introduction of SRL creatively combines elements of imitation learning and RL by decomposing expert trajectories into step-wise actions and rewarding only the actions, allowing the model to develop its own internal monologues. This novel formulation addresses the sparse reward issue in RLVR and the rigid mimicry in SFT, drawing from prior work like GRPO but extending it to dense, sequence-similarity-based rewards for hard problems.\n\n3. By enabling effective learning on difficult datasets like s1K, SRL has broad implications for enhancing LLMs in domains requiring multi-step reasoning, such as math and software engineering. Its generalization to agentic tasks and potential as a cold-start for RLVR pipelines could influence future distillation and RL methods for smaller models."}, "weaknesses": {"value": "1. The method lacks sufficient implementation details, raising concerns about reproducibility. For instance, the prompts or heuristics used to decompose expert reasoning traces into individual steps (e.g., parsing numbered sections from DeepSeek R1 outputs) are not provided, making it challenging to replicate the step-wise data construction process.\n\n2. Although SRL rewards only the action sequence to avoid token-by-token memorization, this still imposes constraints on the model's output. For example, it may force the student to mimic potentially suboptimal or erroneous intermediate actions from the teacher (e.g., steps involving reflection to correct mistakes), even if the student could compute them correctly without such detours. This could limit flexibility in cases where the teacher's reasoning includes unnecessary self-corrections.\n\n3. Experiments are limited to a single 7B model (Qwen2.5-7B-Instruct), without exploring scalability across different model sizes (e.g., 1B, 13B, or larger). This leaves uncertainty about whether SRL's benefits hold for smaller or larger models, potentially restricting insights into its robustness."}, "questions": {"value": "1. For extremely difficult problems where the model fails to rollout any reasonable intermediate step actions (e.g., pass@k near zero even for steps), how does SRL enable effective learning? Could the dense rewards still provide guidance, or would additional techniques like bootstrapping be needed?\n\n2. The method relies on a strong teacher model (e.g., DeepSeek R1) to generate reliable expert trajectories. If no such trustworthy teacher is available (e.g., for novel domains), how could SRL be adapted—perhaps through self-generated or synthetic data, or iterative refinement?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "c2MUXBVkxw", "forum": "Uro84w2xz5", "replyto": "Uro84w2xz5", "signatures": ["ICLR.cc/2026/Conference/Submission22047/Reviewer_WNpC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission22047/Reviewer_WNpC"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission22047/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762105962285, "cdate": 1762105962285, "tmdate": 1762942033931, "mdate": 1762942033931, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}