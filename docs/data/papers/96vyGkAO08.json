{"id": "96vyGkAO08", "number": 8968, "cdate": 1758104917132, "mdate": 1759897751140, "content": {"title": "Anchoring Entities: Retrieval-Augmented Hallucination Detection", "abstract": "Hallucination detection is crucial for large language models (LLMs), as hallucinated content creates significant barriers in applications requiring factual accuracy. Current detection methods mainly depend on internal signals like uncertainty and self-consistency checks, using the model's pre-trained knowledge to identify unreliable outputs. However, pre-trained knowledge may become outdated and has coverage limitations, especially for specialized or recent information. To address these limitations, retrieval-augmented generation (RAG) has emerged as a promising solution that grounds model outputs in external evidence. In this paper, we target a critical and practical learning problem RAG-based hallucination detection (RHD), where RAG is employed to enhance hallucination detection by addressing information updating challenges. To address RHD, we propose a novel method Evidence-Aligned Entity Verification (EAEV), which detects entity-level hallucinations by leveraging RAG to align generated entities with retrieved evidence contexts. Specifically, EAEV evaluates entity-evidence alignment through three complementary dimensions and introduces counterfactual stability analysis to ensure robust alignments under evidence perturbations. Experiments across multiple RAG benchmarks demonstrate that EAEV achieves consistent improvements over existing methods with strong generalization capabilities.", "tldr": "We propose EAEV, a method utilizing RAG to detect hallucinations by aligning generated entities with retrieved evidence through multi-dimensional verification and counterfactual stability analysis.", "keywords": ["Hallucination Detection", "Retrieval-Augmented Generation", "Entity Verification", "Large Language Models"], "primary_area": "unsupervised, self-supervised, semi-supervised, and supervised representation learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8be99a6d967dcddec7d5ba5e7c4e79238e435881.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduced RAG-based hallucination detection, a new task that aims to detect hallucination based on the alignment between LLM-generated text and retrieved documents. The authors then proposed Evidence-Alighed Entity Verification, a new method to detect entity-level hallucination with retrieved documents. The approach consists of alignment assessment, stability analysis, and entity-centric aggregation. The authors conducted experiments on four datasets with three models, showing a better performance against other baselines."}, "soundness": {"value": 1}, "presentation": {"value": 1}, "contribution": {"value": 2}, "strengths": {"value": "1. The authors introduced a novel approach for hallucination detection at the entity level, which will be useful in real-world applications to localize hallucinations\n2. The proposed approach achieves the highest performance on almost all datasets and models"}, "weaknesses": {"value": "1. **Novelty of RAG-based hallucination detection (RHD).** The author claims that they proposed RHD as a novel task. However, detecting hallucination based on retrieved evidence is not a new task. People have been using NLI models or LLMs to check if the answer contradicts provided evidence [1, 2]. In addition, this idea has also been adopted at the entity level [3].\n2. **Unjustified statements.** One major concern of this paper is that it contains many unjustified statements, weakening the sound of this paper. For example, in Line 118, the authors claimed that \"[Existing methods] face challenges when evidence is explicitly available yet underutilized in RAG setting\" without further explanation or citations. In addition, in Line 132 (and also 161), the authors claimed that \"factual errors in RAG settings manifest primarily at the entity level\" without providing any citations or empirical evidence in later Sections. As the approach of entity-level RAG-based hallucination detection is the core of this paper, I think these statements should be properly justified.\n3. **Lack details of the proposed approach.** Another major concern is that many details of EAEV are missing. For example:\n    - Lines 205 and 239: The authors did not explain how they extract $s$. The authors also didn't explain why they only focus on certain types of entity (i.e., ENT, NUM, NP)\n    - Line 229: The authors did not explain what $\\text{anchor}(q,e^\\ast)$ is\n    - Line 245: It is unclear what the rule-based patterns are for identifying explicit conflicts\n    - Lines 231, 241, and 246: The authors did not explain how they found the hyperparameters\n    - The authors should properly justify these design choices or provide experimental/ablation results to support the choices.\n4. **Basic assumption.** The basic assumption of the proposed approach is that the evidence retrieved by a RAG system is correct and sufficient to detect hallucination. However, a RAG system can return noise or incorrect data, which may degrade the performance of the proposed approach. The authors should conduct experiments or analyses in such setting, or mention it as a limitation.\n5. **Lack entity-level verification.** The authors claim that they proposed an entity-level hallucination detection approach. However, all the experiments are conducted at the answer level. I believe some experiments/analyses at the entity level are necessary to justify their approach.\n\n[1]: Fast and Accurate Factual Inconsistency Detection Over Long Documents. (2023)\n\n[2]: FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation (2023)\n\n[3]: HalluEntity: Benchmarking and Understanding Entity-Level Hallucination Detection (2025)"}, "questions": {"value": "1. What is the purpose of the supervised learning model in the whole pipeline? The authors discussed the supervised model in Sec 3.6 but did not conduct any experiment on it. There are also other details about the model (e.g., what dataset is used to train the model). It is unclear to me why the authors included this section.\n2. Fig 3: For the left figure, there is no relationship between AUROC, Accuracy, and F1. Thus, using a line plot to connect these three doesn't make sense to me. The authors should consider using a bar plot instead. On the other hand, it would be better to use a line plot in the right figure."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "8wNntF3zqs", "forum": "96vyGkAO08", "replyto": "96vyGkAO08", "signatures": ["ICLR.cc/2026/Conference/Submission8968/Reviewer_3tmj"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8968/Reviewer_3tmj"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760472158220, "cdate": 1760472158220, "tmdate": 1762920701934, "mdate": 1762920701934, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Evidence-Aligned Entity Verification (EAEV), a method for detecting hallucinations in retrieval-augmented generation (RAG) systems. The key innovation is entity-level verification that leverages three alignment dimensions (identity, semantic, consistency) combined with counterfactual stability analysis to distinguish genuine evidence support from spurious correlations. The method is evaluated on three RAG benchmarks (RAGTruth, HotpotQA, DelucionQA) across three model architectures, achieving 87.55% average AUROC on LLaMA2-13B with consistent improvements over 11 baseline methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. Entity-level verification in RAG addresses a real gap in existing methods\n2. Innovative approach to distinguish genuine evidence support from spurious correlations\n3. Identity, semantic, and consistency dimensions provide complementary verification signals\n4. Multiple models (Qwen2.5-7B, LLaMA2-7B/13B), datasets (RAGTruth, HotpotQA, DelucionQA), and 11 baselines\n5. Consistent improvements across settings (87.55% avg AUROC on LLaMA2-13B, +3-4 points over best baseline)"}, "weaknesses": {"value": "1. Identity = string matching, semantic = embedding similarity, consistency = numerical IoU; main novelty is the combination.\n\n2. The concerns in Ad-hoc design choices. Four perturbation types lack principled selection criteria. Multiplicative combination in Eq. 6 not justified. Type-adaptive weights appear hand-tuned without sensitivity analysis.\n\n3. The paper has some missing critical analyses, computational cost comparison vs. baselines, human evaluation of entity-level detection quality, and failure mode analysis or error propagation study.\n\n4. Only English QA/summarization tasks; no dialogue, code generation, or cross-domain evaluation\nMethodological concerns:\n\n5. Statistical significance testing absent. Entity extraction pipeline glossed over and claims \"entity-level\" but only evaluates answer-level metrics.\n\n6. The work contribute in incremental improvements: +3-4 AUROC points is meaningful but not dramatic given added complexity\nTheoretical gaps: No justification for why these three alignment dimensions are sufficient/complete"}, "questions": {"value": "How do you handle cases where the entity is correct but the relation is wrong? (e.g., \"Paris is the capital of Germany\" - both entities are real)\nWhy use multiplicative combination in Eq. 6 rather than additive or learned combination?\nWhat happens when relevant evidence is NOT retrieved? Does EAEV flag everything as hallucination?\nCan you provide examples where counterfactual stability catches spurious correlations that multi-dimensional alignment alone misses?\nHow does performance vary with retrieval quality (e.g., at different top-k settings)?\nThe supervised learning component (Sec 3.6) seems disconnected - is it an alternative method or complementary approach?\nHow do you determine the \"primary evidence e*\" when multiple evidence pieces support the entity?\nWhat is the computational overhead compared to lightweight baselines like SelfCheckGPT?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "UMexEnBW66", "forum": "96vyGkAO08", "replyto": "96vyGkAO08", "signatures": ["ICLR.cc/2026/Conference/Submission8968/Reviewer_9NXz"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8968/Reviewer_9NXz"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761772542771, "cdate": 1761772542771, "tmdate": 1762920701454, "mdate": 1762920701454, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces Evidence-Aligned Entity Verification (EAEV), which detects entity-level hallucinations by leveraging RAG to align generated entities with retrieved evidence contexts. EAEV first performs entity-evidence alignment through three complementary dimensions and introduces counterfactual stability analysis to ensure robust alignments under evidence perturbations. The experiments demonstrate superior performance of EAEV across multiple RAG benchmarks."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. This work propose to leverage RAG for entity-level verification within retrieved contexts in hallucination detection, where previous works  rely on internal uncertainty or external judges without evidence traceability.\n\n2. The proposed method combines multidimensional alignment with counterfactual stability analysis to distinguish genuine evidence support from spurious correlations in RAG settings.\n\n3. The experiments on RAG benchmarks show that the proposed method works well on Qwen2.5-7B, LLaMA2-7B and LLaMA2-13B."}, "weaknesses": {"value": "1. The experiments are conducted on outdated models, with the most recent being Qwen2.5. Could you provide additional experimental results on newer models such as Qwen 3 or Llama 3.2 to demonstrate the consistent superiority of your approach?\n\n2. The multi-dimensional alignment assessment in Sec. 3.3 is introduced to evaluate the alignment between entity mentions and supporting evidence. Have the authors considered prompting SOTA LLMs (e.g., GPT-4.1, Claude 4, Gemini 2.5) to do this task as an alternative to the proposed machine learning pipeline? A comparative analysis between the LLM-based approach and your method would strengthen the paper by demonstrating the advantages of your proposed pipeline over these readily available alternatives."}, "questions": {"value": "1. Related works are missing, for example: Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus by Zhang et al.\n\n2. The evaluation resource cost of this work is missing."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "6cUXY2YWCU", "forum": "96vyGkAO08", "replyto": "96vyGkAO08", "signatures": ["ICLR.cc/2026/Conference/Submission8968/Reviewer_xRH6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8968/Reviewer_xRH6"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761901471855, "cdate": 1761901471855, "tmdate": 1762920700990, "mdate": 1762920700990, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles hallucination detection in retrieval-augmented generation (RAG) and proposes Evidence-Aligned Entity Verification (EAEV). EAEV checks each entity in a model’s answer against retrieved passages along three axes—identity (direct matches), semantic (paraphrase similarity), and consistency (numbers/attributes, contradictions)—and adds counterfactual stability tests to filter spurious matches."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. Sound motivation: The motivation is well formulated and clearly justified. I like how the authors approach RHD from a different angle and redefine it.\n\n2. Robust framework: The authors clearly justify the design choices behind each component, and the ablation study demonstrates why those components matter.\n\n3. Exhaustive experiments: I appreciate the inclusion of 11 baselines and the rigor of the experimental evaluation."}, "weaknesses": {"value": "A few suggestions:\n\n1. Figure 1 caption: Expand the caption to explain the figure so readers can grasp it at a glance.\n\n2. Figure 3 readability: It’s difficult to read in its current form -- please increase the font size (and consider improving contrast).\n\n3. Citations/references: There are some inaccuracies. For example, the LLM-Check paper lists different author names than the original. Please be more mindful of citations and references, and double-check them."}, "questions": {"value": "I have no questions so far."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "Pivm9Mx0jY", "forum": "96vyGkAO08", "replyto": "96vyGkAO08", "signatures": ["ICLR.cc/2026/Conference/Submission8968/Reviewer_3w1Q"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8968/Reviewer_3w1Q"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission8968/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761979348727, "cdate": 1761979348727, "tmdate": 1762920700628, "mdate": 1762920700628, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}