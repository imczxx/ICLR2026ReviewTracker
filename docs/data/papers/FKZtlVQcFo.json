{"id": "FKZtlVQcFo", "number": 16111, "cdate": 1758260179811, "mdate": 1759897261177, "content": {"title": "WavNAF: Learning Wave Propagation Priors for Neural Acoustic Fields", "abstract": "Room acoustics modeling requires capturing intricate wave phenomena such as reflections, refractions, and diffractions beyond direct sound propagation. Recent neural acoustic synthesis methods have improved acoustic realism but typically focus only on straight sound paths and coarse reverberation, missing detailed interactions like diffraction or multi-order reflections. We propose WavNAF, a neural framework that leverages physically-informed wave propagation priors to explicitly capture complex acoustic interactions. We generate these priors by numerically solving the wave equation with the Finite-Difference Time-Domain (FDTD) method, which directly simulates wave-based acoustic behavior that geometric methods cannot capture. Specifically, we extract essential acoustic parameters for FDTD, such as wave speed and density, from visual scene geometry encoded by Neural Radiance Fields (NeRF). We then generate physically-informed pressure maps and encode them via a feature extractor to learn wave propagation priors that capture intricate acoustic phenomena. To address the inherent computational cost issue of FDTD, we introduce a novel Neural Acoustic Scaling Module, inspired by traditional acoustic scale model. This module adaptively recalibrates encoded pressure map features from temporally compressed simulations to efficiently estimate accurate full-scale Room Impulse Responses. Experimental results demonstrate that WavNAF achieves substantial improvements in acoustic quality across various evaluation metrics compared to existing state-of-the-art methods.", "tldr": "We propose WavNAF, a neural acoustic synthesis framework that leverages physically-informed wave propagation priors to explicitly capture complex acoustic interactions.", "keywords": ["neural acoustic fields", "spatial audio", "audio scenes", "implicit representations", "applications"], "primary_area": "applications to computer vision, audio, language, and other modalities", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/fd9eb2a6ce74571058e69c113b7b59e4d48bf2b7.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper proposes WavNAF, which uses a NeRF-derived density field to construct 2D acoustic parameter maps and then runs a physics-based Finite-Difference Time-Domain (FDTD) simulation to produce pressure maps. A neural network then synthesizes the acoustic impulse response using features extracted from these pressure maps, along with source/microphone positions, directions, and target time. To reduce computational cost, a time-scaling module is introduced. Experiments on standard benchmarks indicate improvements over baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper novelly uses FDTD pressure maps as inputs to explicitly encode reflection, refraction, and diffraction, injecting physical priors and reducing the burden on the network learning.\n\nThe proposed method achieved better performance than prior approaches on the evaluated datasets.\n\nThe structure of the paper is well designed, and the notations are well defined, making it easy to follow."}, "weaknesses": {"value": "Some components are underspecified (see Questions), which complicates the full understanding of the method.\n\nThe method relies heavily on empirical choices like the mapping from optical opacity to acoustic parameters, but lacks thorough ablations to justify their ranges and robustness.\n\nThe method only considers a 2D structure when synthesizing acoustic signals, which means the full room structure is not considered in the signal synthesis."}, "questions": {"value": "1. The paper adopts NeRF for dense reconstruction. Is any post-processing applied to the obtained opacities? Additionally, I am curious if a scene mesh is provided and then transformed to spatial grids, where the opacity is set to 1 if occupied, will it outperform the current method in theory? Also, are the areas with low optical opacity (between 0 and 1) of great importance? If not, will it achieve a better performance if you remove low-opacity areas? The visual model and the acoustic model are trained together. Am I correct in understanding that the gradient from the acoustic model does not propagate back to the NeRF densities?\n\n\n2. To obtain the 2D input for simulation, the authors apply max pooling along the z-axis of the 3D density volume. However, it is unclear whether the ablation studies related to ∆z (e.g., in Table 3) are conducted on RAF, SoundSpaces, or both. Since they have different microphone/speaker setups as mentioned in the paper, it is important to know the experiment setup for robustness. Additionally, how sensitive are the results to the choice of height (a different height than the microphone height) or the grid size? Based on the visualizations, it seems that large objects such as tables, chairs, or even walls can be excluded on the selected height. Can the authors quantify how often this happens and discuss whether it affects performance across datasets? Also, please clarify the unit and value range used for ∆z in Table 3.\n\n\n3. The mapping from optical opacity to acoustic parameters such as sound speed and density is empirical. This assumes that visually opaque regions correspond to acoustically reflective surfaces, regardless of material type, which is often not true. Did the authors explore other mappings, or are there any theory justifications?\n\n\n4. Despite these approximations, the proposed approach achieves stronger results than prior baselines. Is this primarily because the pressure maps, even if approximate, already encode most of the wave-based propagation phenomena, thereby reducing the burden on the network learning? If so, this implies that the correctness of the pressure maps may not be strictly necessary for performance. It would help to include analysis or examples where inaccurate pressure maps lead to performance degradation, to support or challenge this hypothesis.\n\n\n5. Finally, regarding Figure 6, it is unclear which comparisons directly support the claim that the proposed method bridges the gap between simulation-based and learning-based models. Could the authors provide a clearer explanation, which would be helpful?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wm34f4BazR", "forum": "FKZtlVQcFo", "replyto": "FKZtlVQcFo", "signatures": ["ICLR.cc/2026/Conference/Submission16111/Reviewer_44Hp"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16111/Reviewer_44Hp"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission16111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761709045348, "cdate": 1761709045348, "tmdate": 1762926287299, "mdate": 1762926287299, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper presents WavNAF, designed to improve room acoustics modeling by integrating wave propagation physics directly into the acoustic synthesis process. It overcomes the limitations of geometric methods, which cannot model intricate wave phenomena like diffraction, refraction, and complex reflections, by utilizing the Finite-Difference Time-Domain (FDTD) method to numerically solve the wave equation.\nThe key contributions include generating physically-informed wave propagation priors by deriving acoustic parameters (sound speed and density) from visual scene geometry encoded by NeRF. Crucially, the framework introduces a Neural Acoustic Scaling Module that efficiently addresses the high computational cost of full FDTD simulations by learning adaptive, time-dependent transformations to accurately estimate full-scale RIRs from compressed simulations. This physics-based approach leads to substantial improvements in acoustic quality across standard metrics compared to existing state-of-the-art methods."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- WavNAF integrates physically-informed wave propagation priors derived directly from FDTD-simulated pressure maps, providing a strong inductive bias for neural acoustic field learning, even with simplified material parameters.\n- The Neural Acoustic Scaling Module addresses the high computational cost and stability constraints of FDTD by learning adaptive, time-dependent transformations to estimate accurate full-scale RIRs from temporally compressed simulations.\n- The physics-informed wave propagation priors enable better generalization from limited data\n- Using NeRF to extract visual scene geometry, converting volumetric density fields into 2D acoustic parameter grids (sound speed and density) allows physics-informed simulations without explicit material annotations\n- The paper is well-written, and I really like the detailed background section."}, "weaknesses": {"value": "- WavNAF shows sensitivity to the alignment between the visual and acoustic coordinate systems. The model currently requires empirical tuning to ensure the necessary precise coordinate alignment between the geometry derived from NeRF and the simulation space\n- The integration of on-the-fly FDTD simulations significantly increases the computational complexity compared to purely neural or geometric baselines\n- To model energy dissipation and numerical stability, the framework uses a combination of an empirically defined sponge layer near the boundaries and a simple global damping factor applied uniformly across the domain to model natural air attenuation. While effective, this is a simplified model of complex, frequency-dependent air absorption effects."}, "questions": {"value": "- Given that the FDTD CUDA kernel accounts for only 5.1% of the total inference time, and data preparation (acoustic parameter grid generation) accounts for 71.4%, what specific optimizations are being considered for the data preparation pipeline to make the transition to full 3D FDTD simulations feasible? Besides, if computational resources were unlimited, what is the potential performance gain anticipated by moving from 2D to 3D simulations? In particular, how would the inclusion of vertical propagation specifically impact the accuracy of metrics like EDT and C50?\n- Have the authors experimented with different initialization strategies for the MLPs that are specifically designed to emphasize correction for late reverberation, such as initializing $\\nu$ with a slight non-zero bias, and how did this affect convergence stability?\n- The few-shot learning study shows that WavNAF achieves comparable performance to a baseline (NeRAF) trained on 75% of data when WavNAF is trained on only 50% of data. Does this data efficiency hold true when testing on the RAF real-world dataset, where the noise and complexity may be greater than in the synthetic SoundSpaces data?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "N/A"}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "7sRRUZSk8g", "forum": "FKZtlVQcFo", "replyto": "FKZtlVQcFo", "signatures": ["ICLR.cc/2026/Conference/Submission16111/Reviewer_3MYx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16111/Reviewer_3MYx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission16111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761798756596, "cdate": 1761798756596, "tmdate": 1762926286886, "mdate": 1762926286886, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper presents WavNAF, a hybrid physics-guided/neural framework for room-acoustics synthesis. First, a NeRF is trained from multi-view images and its volumetric density is collapsed into a two-dimensional floor-plane grid that is heuristically mapped to local sound-speed and air-density values. A 2-D FDTD solver then generates a short sequence of pressure maps conditioned on a point source, after which two small MLPs learn an affine scaling transform so that these time-compressed simulations can still predict full-scale RIR short-time Fourier transform bins. Finally, positional encodings of the source/listener and directional queries are fused with the scaled pressure features in a neural field that regresses log-magnitude STFT frames. Experiments on SoundSpaces and RAF show that WavNAF attains slightly better T60, C50, EDT and log-STFT error than baselines such as NAF, INRAS, NeRAF and AV-GS while requiring substantially shorter simulations than full-length FDTD."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. Physics-aware prior. Incorporating FDTD pressure maps provides an interpretable link between geometry and acoustics and yields consistent gains in ablations.\n2. The neural scaling module reduces simulation length 18× with < 2 % drop in metrics, which is important for interactive RIR prediction."}, "weaknesses": {"value": "1. 2-D pressure field. All wave simulations are confined to a floor-plane slice, ignoring vertical propagation, ceiling reflections and height-dependent diffraction.\n2. Modest improvements on RAF. Relative gains over NeRAF are marginal. (Compared with NeRAF T60 7.47 -> 7.43, C50 0.61 -> 0.61)\n3. Missing qualitative visualisations. No figures compare simulated versus learned pressure fields, making it hard to judge fidelity."}, "questions": {"value": "1. Can you provide more informative visualization on the learned acoustic field? Like your simulated pressure field and the final result. I noticed you included some visualization in the appendix. However, that figure is just hard to understand.\n2. It seems that the pressure map simulation already estimate the impulse response. Why didn't you directly user that output? If the acoustic feature is accurately estiamted. Then I would imagine you do not need a subsequent NAF model to estimate the STFT.\n3. What is the time cost of the simulation if you want to simulate a single sample, for example, fix a tx and rx locations.\n4. If you already use FDTD method to simulate the pressure field and you know the global geometries. I would assume that these simulated data would already good enough to represent the whole acoustic field. So if you only use 10% dataset to train the dataset for example on RAF dataset, would you have much better performance?\n5. What is the reason you predict the STFT rather than the impulse resposne in the time domain? It seems that you can already simulate the pressure field in the small time axis, is it because the computation budget?\n\nBesides, I would encourage you to cite more recent works on the related topics:\n- Jin, Derong, and Ruohan Gao. \"Differentiable Room Acoustic Rendering with Multi-View Vision Priors.\" arXiv preprint arXiv:2504.21847 (2025).\n- Lan, Zitong, Yiduo Hao, and Mingmin Zhao. \"Resounding Acoustic Fields with Reciprocity.\" arXiv preprint arXiv:2510.20602 (2025).\n- Liang, Susan, Chao Huang, Yunlong Tang, Zeliang Zhang, and Chenliang Xu. \"p-AVAS: Can Physics-Integrated Audio-Visual Modeling Boost Neural Acoustic Synthesis?.\" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13942-13951. 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YvK6aNpq1L", "forum": "FKZtlVQcFo", "replyto": "FKZtlVQcFo", "signatures": ["ICLR.cc/2026/Conference/Submission16111/Reviewer_k2CB"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16111/Reviewer_k2CB"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission16111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761835716563, "cdate": 1761835716563, "tmdate": 1762926286401, "mdate": 1762926286401, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces a novel framework to estimate room impulse responses (RIRs) from scene geometry. It first simulates low-resolution wave propagation using the Finite-Difference Time-Domain (FDTD), aiming to capture audio propagation impacting factors such as  reflections and diffraction, then trains a Neural Acoustic Field conditioned on these physics-based priors. A Neural Acoustic Scaling Module refines frequency-dependent detail. Experiments on synthetic and real-world (Real Acoustic Fields) datasets show improved realism and generalization over prior neural methods. WAVNAF achieves physically faithful, data-efficient acoustic field reconstruction for both simulated and measured indoor environments."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "1. The paper is well-presented, easy to follow, the motivation is clear as well.\n2. The paper tries to solve an important problem that models spatial audio propagation process with neural network.\n3. The results presented in the paper show advantage over other baselines."}, "weaknesses": {"value": "The main weakness, based on my understanding, is the lack of clear verification to show the necessity of using vision to assist spatial audio propagation process. As the way audio propagates in an enclosed 3D space, its hebaviour is impacted by lots of factors, construction material, indoor surface aborption coefficient, reflection parameter etc.. The vision alone can't fully capture all of these influencing factors, but the authors claims in the paper that they can successfully model factors like diffraction, diffusion and reflection. I look forward more explaination and clarification on vision exploitation validation during the rebuttal period."}, "questions": {"value": "1. L203 describing local sound speeds, what is the local sound speed? Isn't the sound speed a constant in the whole room scene?\n2. One missing comparing baseline: Deep Neural Room Acoustics Primitve, ICML24; Deep Impulse Responses: Estimating and Parameterizing Filters with Deep Networks, ICASSP, 2022.\n3. Lack of visualization on how the predicted RIR look like, the performance regarding room size, receiver-source distance and the impact of indoor furnitures.\n4. The paper just predicts STFT map for the RIR, ignoring the phase information. As RIR data is sensitive to position change (in other words, the phase is important), the paper should at least experimentally verify why ignoring phase information is a good option."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "87bNzwdRKp", "forum": "FKZtlVQcFo", "replyto": "FKZtlVQcFo", "signatures": ["ICLR.cc/2026/Conference/Submission16111/Reviewer_QtCm"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission16111/Reviewer_QtCm"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission16111/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762046242362, "cdate": 1762046242362, "tmdate": 1762926285835, "mdate": 1762926285835, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}