{"id": "GWvZQsMLfl", "number": 3612, "cdate": 1757486761597, "mdate": 1759898078647, "content": {"title": "Interpretable Intrinsic Cues for Efficient Reinforcement Learning with Large Language Models", "abstract": "Reinforcement learning with verifiable rewards (RLVR) has improved the reasoning ability of large language models, yet training remains costly because many rollouts contribute little to optimization relative to their heavy computational demands. This study investigates how simply leveraging interpretable and intrinsic data properties, which come at almost no additional computational cost during training, can markedly improve data efficiency for RLVR. We propose PREPO, an RLVR model with two complementary components. First, we use prompt perplexity as a proxy for model adaptability in learning, and adopt a schedule to guide the model from well-understood prompts to progressively challenging ones. Second, we amplify the diversity among rollouts by differentiating their relative entropy and prioritizing sequences with greater exploratory behavior. Together, these mechanisms reduce rollout demand while preserving competitive performance. On Qwen and Llama models, PREPO achieves effective results on mathematical reasoning benchmarks with up to 3× fewer rollouts than baselines. Beyond empirical gains, we provide theoretical and in-depth analyses that explain how our method improves the data efficiency of RLVR.", "tldr": "We propose PREPO, leveraging intrinsic cues (i.e., prompt perplexity and sequence-level entropy), for data-efficient reinforcement learning with verifiable rewards, cutting rollout costs by up to 3× with interpretable training dynamics.", "keywords": ["Intrinsic Cues", "Data Efficiency", "Reinforcement Learning with Verifiable Rewards"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/7ffb7c416e7ef55130584f83628dcd558e4f1e51.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper introduces PREPO, a method for improving training efficiency in Reinforcement Learning with Verifiable Rewards (RLVR) through active sampling. The approach uses prompt perplexity as a measure of prompt difficulty, gradually shifting from easier to more challenging prompts during training.  In addition, it improves diversity in GRPO rollouts by prioritizing sequences with higher entropy, encouraging more exploratory and informative updates."}, "soundness": {"value": 2}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "- The method is evaluated across multiple model families (Qwen and LLaMA), showing robustness and generality.\n- The approach is clearly explained, with intuitive motivation and detailed algorithmic descriptions.\n- PREPO achieves consistent rollout reduction across models while maintaining/improving accuracy.\n- he method is simple yet effective, using interpretable intrinsic signals (perplexity and entropy) rather than external heuristics or external models."}, "weaknesses": {"value": "- It appears that some relevant literature is missing: In particular [1] which it introduces CISPO an updated policy-gradient method with with truncated importance sampling. A discussion of how PREPO's entropy based weighting compares to CISPO would strengthen the paper.\n- The reported baseline performance for some models (e.g., Qwen2.5-Math-7B) appears a lot lower than in prior work. Other studies, like [2](Table 3) have reported much higher base scores. This discrepancy raises concerns about the robustness of the evaluation setup. Some additional details on the evaluation procedure would help build confidence.\n- For small benchmarks (like AIME'24) usually avg@32 or avg@64 is reported (see [2] or [3]). This is relevant because in Table 2 the performance is average across benchmarks and not number of samples. AIME'24 can have a variance of ~2% with avg@16. (This does not apply for MATH or Olympiad).\n\n1: MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention  \n2: A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility  \n3: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"}, "questions": {"value": "- **Q1:** How stable is PREPO during extended training: Other methods like DAPO show training instabilities over longer training horizons. Does PREPO exhibit similar behavior?\n- **Q2:** Evaluation\n   - (a) Was a standardized evaluation framework used, or were the experiments based on a custom implementation?\n   - (b) How was answer matching performed? E.g. by math-verify or an equivalent symbolic equivalence checker?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "yqp8AJYm1E", "forum": "GWvZQsMLfl", "replyto": "GWvZQsMLfl", "signatures": ["ICLR.cc/2026/Conference/Submission3612/Reviewer_1nMi"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3612/Reviewer_1nMi"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission3612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761843997735, "cdate": 1761843997735, "tmdate": 1762916870053, "mdate": 1762916870053, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes PREPO which consists of 2 contributions while doing RLVR\n\n1. Using prompt perplexity as an implicit curriculum since they observe that prompt perplexity is correlated with hardness (measured using pass rate). \n2. Amplifying diversity in rollouts using the relative entropy between rollouts by multiplying the policy gradient per rollout with a corresponding weight. \n\nUsing these, they show improvements on a bunch of base models such as Llama3.1, Qwen2.5 and Qwen3."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The idea of using prompt perplexity is novel. It is very useful since you don't need to do rollouts to estimate the value of each prompt and this significantly reduces the computational burden of inference. The paper is also written well and does a good number of experiments."}, "weaknesses": {"value": "There are a couple of concerns with the experiments:\n\n1. The authors should put the result of PREPO w/o relative entropy in Table 2 since it's unclear what contributes to the improvements in performance. Is performance improved because of the entropy weighting or is it because you select more informative prompts? \n\n2. The authors should also do an experiment without the prompt filtering, just using the entropy weighting to ablate this component of their contribution.\n\n3. The numbers in Table 2 are slightly unsettling. For instance, most of the gains for Qwen2.5-Math are coming from the Olympiad test set, gains for Qwen3 are coming from AIME and gains for Qwen2.5-7B are coming from AIME24. Does something explain these mixed gains? \n\n4. Also, could the authors put validation performance on y-axis and time/rollouts on x-axis and show the curve for all methods to make the training dynamics clearer?"}, "questions": {"value": "Please look at the weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "YAqM3K7Ycv", "forum": "GWvZQsMLfl", "replyto": "GWvZQsMLfl", "signatures": ["ICLR.cc/2026/Conference/Submission3612/Reviewer_oUFb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3612/Reviewer_oUFb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission3612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761921753686, "cdate": 1761921753686, "tmdate": 1762916869498, "mdate": 1762916869498, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper proposes PREPO, a lightweight reinforcement learning (RLVR) framework that improves data efficiency by leveraging intrinsic signals (perplexity and entropy) instead of heuristics or auxiliary models. The method is simple to implement and requires a negligible amount of additional computations while reducing the rollouts by 2-3 times compared to the Dynamic Sampling."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "1. PREPO is extremely simple to implement. It only requires computing prompt perplexity and token-level entropy. The integration into standard PPO/RLVR loops is almost trivial.\n2. Its computational overhead is negligible since all intrinsic cues (perplexity and entropy) are derived from existing training signals.\n3. The paper is clearly written, empirically rich, and includes diagnostic analyses that help readers understand why and how the method improves efficiency."}, "weaknesses": {"value": "1. **Oversimplification of the PPL-Schedule**: The paper's central curriculum strategy relies on a fixed linear schedule that shifts from low-PPL to high-PPL prompts. This feels arbitrary and rigid. I also wonder how authors chose hyperparameters (e.g., T, N, K). The value of T seems to determine both the total training steps and the transition rate. It might affect the number of rollouts and final performance critically.\n\n2. **The PPL-Schedule's Conflicting Goals**: The \"easy-to-hard\" schedule seems to prioritize initial training speed. It might risk sacrificing peak performance by potentially under-sampling the more valuable (HIGH-PPL) data, especially in the critical later stages of training. The paper needs to justify why this efficiency-focused curriculum is superior to a performance-focused one that might, for example, heavily prioritize HIGH-PPL prompts.\n\n3. **Conflation of \"Difficulty\" with \"Domain\"**: The paper equates low PPL with \"easy\" and high PPL with \"hard\". However, the analysis in Appendix E suggests an alternative explanation: HIGH-PPL prompts contain significantly more non-English characters. This implies PPL might be acting as a proxy for domain mismatch rather than pure task difficulty. The PPL-schedule could therefore be interpreted as a curriculum that trains first on in-domain data (English math) and then gradually introduces out-of-domain data (non-English math). This is a valid curriculum, but it's a different claim than \"easy-to-hard\" and has different implications for generalization.\n\n4. **Missing Key Ablation Studies**: The paper's only ablation (Section 5.4) compares the full PREPO against just the PPL-schedule component. This merely shows that entropy weighting adds some benefit. *Missing Study 1*: One missing comparison is [Random Selection] + [Entropy Weighting]. Without this baseline, it is impossible to know how much of the final performance gain comes from the sophisticated PPL-schedule versus the (perhaps simpler) entropy weighting. If this baseline performed comparably to PREPO, it would suggest the entire PPL-schedule is unnecessary. *Missing Study 2*: The paper also needs to elaborate on the design of the PPL-schedule itself. The authors mention (Section 4.2) that other schedules are possible but provide no empirical justification for their choice of a linear schedule over other options or static baselines.\n\n5. **Average Response Length (ARL) Increase and Inference Inefficiency**: According to the Fig. 5 and 14, PREPO increases ARL compared to the baseline. An increase in ARL can translate to an increase in total token generation cost (GPU time, energy consumption) per rollout. If the ARL is significantly longer for PREPO, the claimed reduction in rollout count may be offset when measuring the true cost by total tokens generated ($\\text{Rollout Count} \\times \\text{Average Length}$). Furthermore, it can also lead to an inference inefficiency."}, "questions": {"value": "1. Compared to the GRESO[1] paper's results, the total rollouts and the performance are quite different (e.g., Qwen2.5-Math-7B; DS: 13.1M vs 1.6M and GRESO: 6.3M vs 0.65M). Is this because the training budget is different? If so, what would happen if we increase the total training budget as much as GRESO did?\n\n2. In Fig. 22, the baseline with 5x rollout per step shows comparable performance with PREPO. In this case, what is the advantage of PREPO compared to the baseline with 5x rollout per step?\n\n3. In Tab. 6, why low selection ratio show more rollouts? At each training step, don't we use a smaller number of samples to generate the rollouts?\n\n4. In Fig. 5 and 14, why does the Average Prompt Length decrease?\n\n5. In Sec 5.5, it is mentioned that further discussion about the sensitivity of large entropy can be found in Appendix H, but I cannot find one (I might have missed something). Can you elaborate more on how the distribution of weights changes during the overall training?\n\n\n[1] Haizhong Zheng, Yang Zhou, Brian R Bartoldson, Bhavya Kailkhura, Fan Lai, Jiawei Zhao, and Beidi Chen. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts. arXiv preprint arXiv:2506.02177, 2025."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "YyUiCt7DnS", "forum": "GWvZQsMLfl", "replyto": "GWvZQsMLfl", "signatures": ["ICLR.cc/2026/Conference/Submission3612/Reviewer_gvHC"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3612/Reviewer_gvHC"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission3612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762488085950, "cdate": 1762488085950, "tmdate": 1762916869297, "mdate": 1762916869297, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces the PREPO algorithm for RLVR for LLMs. The first idea is to use prompt perplexity as a proxity for problem difficulty, adopting a schedule from easy to challenging prompts. The second idea is to amplify diversity among rollouts by prioritizing sequences with greater emtropy. The algorithm achieves 2-3 times rollout reduction comparing to baselines."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper proposes to use perplexity as a metric for problem difficulty. Comparing to dynamic sampling and random selection, perplexity-based sample selection method does not waste rollouts. \n- Entropy-weighting can prevent the model from entropy collapse."}, "weaknesses": {"value": "- The PPL-schedule-filter requires a schedule that is consistent with the learning dynamics of the model. If changing a model or a task, the schedule need to be tuned. Such a tuning of schedule introduces additional computational overhead not accounted for in the reported comparisons. \n- The paper does not compare entropy-weighting with entropy-regularization methods. While it argues that entropy-weighting avoids additional hyperparameters, it is unclear whether the “hyperparameter-free” property provides meaningful practical advantages."}, "questions": {"value": "See weakness"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "ThrRKxGR1r", "forum": "GWvZQsMLfl", "replyto": "GWvZQsMLfl", "signatures": ["ICLR.cc/2026/Conference/Submission3612/Reviewer_Lz9P"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3612/Reviewer_Lz9P"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission3612/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762540592510, "cdate": 1762540592510, "tmdate": 1762916869019, "mdate": 1762916869019, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces two techniques in Reinforcement Learning with Verifiable Reward (RLVR) and proposes PREPO to accelerate the RLVR training. The first technique is an online curriculum learning trick which uses the prompt ppl as a proxy filtering criterion to sort the prompts, aiming at training the model on easy-to-hard questions in order. However, they found that easy questions (low-ppl ones) have more consistent rollouts and lead to fast entropy collapse, so they introduced the second trick: weighting the advantage functions by the relative sequence level entropy. \n\nThey show in experiments that PREPO achieves, on average, higher scores on benchmarks like AIME, MATH, and Olympiad with fewer rollouts. They show that to achieve similar or better performance, PREPO needs 2 to 3 times fewer rollouts. They also did ablation study on the two tricks they proposed."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. The presentation in this submission is good, and the motivation is clearly stated. They start with a correlation analysis between the prompt ppl and the pass rate, and a comparison between the training dynamics on the low-ppl and high-ppl prompts. This clearly verifies the reason why they chose prompt ppl as the proxy criterion for the prompt difficulty and use it as the filtering criterion.\n\n2. The main experiments showed that the method they proposed can save many rollouts to achieve similar performance. They compare PREPO with random selection, DAPO, and GRESO, and show that their method outperforms others in four benchmarks. The rollouts that they can save are striking."}, "weaknesses": {"value": "1. Possibilities of other proxies for curriculum learning: In their submission, there are at least two alternative 'intrinsic data properties' that one can use to do curriculum learning. A: In their appendix E, they show that the low-ppl prompts usually have higher rates of English words and high-ppl ones often feature by non-English questions. Since they use DAPO-17k as the training set and it includes many Chinese questions, the non-English prompts will be an issue. I think one way to exclude this confounder one can use a purely English dataset as the training set. B: In Figure 5, they show that the average prompt lengths decrease (almost monotonically) in PREPO. This shows that when we rank the prompts by prompt ppl, we are always prioritizing the long prompts over shorter prompts. Does this mean we can simply use the prompts' length as a filtering criterion for the curriculum learning?\n\n2. Issues on the Main Experiments: \n\nA: I think they should add the two ablation studies in their Table 2 (the table for their main experiments). They should at least try the one without entropy weighting, and the one with entropy weighting but without the online filtering based on prompts ppl. \n\nB: How did the authors choose the stopping time (how many rollouts they use in the main experiments?) For example, on the first block of their table 2, they show that GRESO uses 680k rollouts and PRESO uses 304k, and I wonder how they chose these two numbers? What happens if we train both methods (and all other methods) to the points where we use 1040k rollouts? I think a better way for comparison is to compare the (best) performance at the same rollouts. A better visualization way is to plot the curve for the average score (otherwise, we cannot exclude the hypothesis that the PRESO can be worse in the later training phase, and they finally converge to the same performance as GRESO or other baselines)."}, "questions": {"value": "See above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "/"}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "7H6NEhqMyf", "forum": "GWvZQsMLfl", "replyto": "GWvZQsMLfl", "signatures": ["ICLR.cc/2026/Conference/Submission3612/Reviewer_t6D8"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission3612/Reviewer_t6D8"], "number": 5, "invitations": ["ICLR.cc/2026/Conference/Submission3612/-/Official_Review"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762988147125, "cdate": 1762988147125, "tmdate": 1762988147125, "mdate": 1762988147125, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}