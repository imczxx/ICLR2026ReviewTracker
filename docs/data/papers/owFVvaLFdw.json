{"id": "owFVvaLFdw", "number": 8036, "cdate": 1758053715479, "mdate": 1759897812651, "content": {"title": "AgentHard: Hardening LLM-Agent Evaluation with a Taxonomy of Artifacts and Automated Cleaning", "abstract": "Reliable evaluation of LLM‑based agents is often confounded by artifacts that misrepresent true agent capabilities by conflating model errors with benchmark flaws. To address this, we present a component-wise taxonomy of common benchmark pitfalls spanning the user, environment, evaluation, and ground truth elements of agent tasks. This analysis exposes pervasive issues such as incorrect ground-truth action sequences, ambiguous tool APIs, user simulation faults, and brittle evaluation metrics. Guided by these insights, we develop AgentBenchCleaner, an automated pipeline that filters out flawed tasks in three stages: first, rule-based detectors catch deterministic errors; second, an LLM-as-judge identifies nuanced issues; and third, a quality-filtering step enhances evaluation rigor. Applying this pipeline yields a curated benchmark suite, AgentHard-Bench, with standardized evaluation protocols and explicit quality criteria. Across diverse LLM agents, evaluations on AgentHard-Bench deliver more stable model rankings, clearer performance separations, and improved benchmark diversity relative to the original benchmarks. We will release AgentHard-Bench, along with the taxonomy and pipeline upon acceptance, to support robust, reproducible agent evaluation.", "tldr": "", "keywords": ["Agent benchmark", "Benchmark filtering"], "primary_area": "datasets and benchmarks", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d2a37f487d921000874fbd2974039d0461b79e56.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper studies LM agent benchmarks and proposes: (A) a taxonomy of benchmark failures, (B) a three-stage debugging pipeline (rule-based checks, LLM-as-a-judge, and a final \"quality\" filter that removes tasks that most models solve or that show low discriminative ability), and (C) trimmed versions of six existing benchmarks (compressed to $\\approx 45\\\\%$ of original size). The intended outcome is a cleaner, more discriminative suite of tasks and improved cross-model separability (measured by non-overlapping confidence intervals)."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "- **Originality.**  \n  - The paper attempts to systematize common failure modes in LM agent benchmarks and to operationalize them into a practical debugging pipeline.  \n- **Quality.**  \n  - The authors conduct a multi-benchmark pass and surface concrete issues; several examples appear to be genuinely new (or at least under-documented) and would be useful to benchmark maintainers.  \n  - Automating checks via an LLM-judge could reduce maintainer overhead if validated on unseen benchmarks."}, "weaknesses": {"value": "This paper is really problematic and I apologise for the harsh tone in advance. \n\n1) **Circular validation akin to training on the test set.**  \n   The paper derives a taxonomy from the six target benchmarks, turns that taxonomy into a long, highly specific LLM prompt, and then re-applies it to the same six benchmarks. This design is training on the test set.  The Gemini LLM-judge is primed with bug patterns that are specific to the evaluation set, so its success is not strong evidence of generality.  \n   **Actionable fix:** Evaluate on genuinely unseen benchmarks not used in taxonomy construction, say Terminal Bench; preregister the taxonomy (frozen) and report zero-shot detection rates on new benchmarks.\n\n2) **Unclear empirical value of the taxonomy.**  \n   What is the point of yet another taxonomy?  What's the point of doing this rather than the ABC (Zhu et al NIPS2025)?  Is the taxonomy useful in any way?  \n   It is not shown that the taxonomy increases bug-finding power beyond a simpler prompt or generic critique instructions. The taxonomy is also fairly coarse, which is tautologically exhaustive but not deeply diagnostic.\n   **Actionable fix:** Provide ablations that compare (i) generic critique prompts vs. taxonomy-guided prompts, (ii) short vs. long taxonomy prompts, (iii) ABC prompt vs. prompt in this paper.  \n\n3) **Problematic pipeline design and selection-induced inflation of separability.**  \n   The final “quality” filter removes tasks where the frontier models can reliably solve (i.e., the easy tasks), then reports improved separability vs. randomly sub-sampled subsets of the original benchmark.  Discarding all easy tasks just to artificially inflate separation is **bad practice**.  To begin with, several benchmarks (e.g., GAIA) already come with difficulty tags, and I want the authors to explain this: why did the developers of GAIA not only keep the hard tasks?  Wouldn't that be better for the separability of their benchmark?\n\n   Mathematically, conditioning on high variance (i.e., difficulty of the tasks) will mechanically inflate non-overlap of confidence intervals **without increasing the benchmark’s true information content.**  Consider the following setup. \n\n   **Setup:** Let the full task set be $ \\mathcal{T}=E\\cup H $ with easy $E$ and hard $H$, $E\\cap H=\\varnothing$. For model $m$, accuracies on $E$ and $H$ are $p_m^E$ and $p_m^H$. Since the models chosen are mostly pretty strong, we're in the regime where $p_m^E\\approx 1$.  \n   **Toy example:** Suppose $|\\mathcal{T}|=100$ with $|E|=55$ and $|H|=45$, and two models differ only on $H$:\n   $$\n   \\text{Model 1: }(p_1^E,p_1^H)=(1,5/45), \\quad\n   \\text{Model 2: }(p_2^E,p_2^H)=(1,40/45).\n   $$\n   Full benchmark scores: $60\\\\%$ vs. $95\\\\%$; hard-only: $11\\\\%$ vs. $89\\\\%$. The ranking and the signal already come from $H$; removing $E$ merely rescales numbers. Comparing a “trimmed” subset enriched for $H$ to a random subset of the same size will inflate CI non-overlap by design, since we are reducing the effective number of hard tasks used in evaluation, therefore reducing the effective data size.  However, this does not mean that the trimmed down model is actually more discriminatory!  You can always only assign difficult tasks if you are trying to differentiate strong models from medium models, but throwing away easy tasks also hurts the separability for weak models versus medium models.  By the logic of the authors, if I have only weak and medium models, then I should actually discard all the hard tasks (since no medium or weak model can solve hard tasks).  Instead, I should keep only the easy tasks (**precisely the ones that authors have discarded**) and report improved separability on those models.  Is that what the authors believe should be done?\n\n   Furthermore, in practice, if we have benchmark1 (larger, but with some easy tasks and benchmark2 (smaller, but only with hard tasks) and we want to differentiate models using the benchmarks, we would evaluate all models on the entirety of benchmark1 and benchmark2.  Being larger is precisely the core advantage of benchmark1, so why should benchmark1 be sub-sampled to the size of benchmark2?  Since we want maximal separability, it's pointless to work with a subsampled subset.  \n\n   **Actionable fix:** Re-run the experiments on a family of 16 models that are 8B or below and report if the separability of the trimmed down benchmark is still higher than the original FULL benchmark.  I find it hard to imagine that this will happen.  This upshot is that the authors provide neither sufficient justification that discarding easy tasks is a good idea, nor sufficient evidence that the trimmed down benchmark is has more separation power.\n\n4) **Rank stability claim unsubstantiated.**  \n   The paper mentions improved rank stability but provides no statistical test (e.g., Kendall-$\\tau$ or Spearman, with CIs under resampling) and no sensitivity to sample size.  \n\n   **Actionable fix:** Report rank correlations with 95% CIs across $B$ bootstrap resamples.\n\n\n5) **English, Style, Presentation, and Grammar.**  \n   Please fix various English and presentation errors. For example, on line 55, \"Yet these efforts EITHER ......\" but there is no corresponding OR in the sentence. Use of em dash is inconsistent (l.74 vs l.86) The paper can improve if the authors look through the grammar more carefully. There is quotation mark inconsistencies (l.460) and the authors can consider the csquotes LaTeX package to simplify things. Figure 4 is too small to see.  Define “separability with confidence” precisely.  After digging into the ArenaHard reference, I think the authors really meant the proportion of CIs *without* overlap, rather than *with* overlap given in the table.  Correct me if I'm wrong. \n\n\n6) **Trimming $\\sim 55\\\\%$ of tasks is not itself a contribution.**  \n   Reducing six benchmarks to $\\approx 45\\\\%$ because the tasks are buggy or not hard enough is not the productive way to do research.  When you run into bugs in current benchmarks, shouldn't the intuition be trying to fix them?  We are in a shortage of benchmarks and you are suggesting that we should throw away half of them.  If you are not fixing any bugs and just throwing away half of the dataset, then perhaps you should not name your work an \"agent bench *cleaner*.\"  Also, I think the pipeline is going to end up deleting ALL of gsm8k since most models chosen in the paper can solve most of gsm8k.  Did that make gsm8k cleaner?\n\n   **Actionable fix:** Think about fixing the bugs.  It'd be a highly valuable contribution if you can fix the bugs in existing benchmarks.  That is what the community needs.\n\n\n\nOverall, I think this research is done in poor taste.  It's not done with a productive vision in mind.  The other papers are all trying to propose more tasks.  Our community needs more fixers, i.e., people who are willing and ready to fix a bug or a failure when they see one.  Even though I am clearly trying to reject your paper, I am still trying to come up with \"actionable fix\" items.  I think the benchmark tasks deserve a similarly constructive attitude from the authors.  I recognise that my remarks are highly critical and hope that the authors are not discouraged. Let me end with a quote from Gilles Deleuze.\n\n\"...Books against structuralism (or those against the “New Novel”) are strictly without importance; they cannot prevent structuralism from exerting a productivity which is that of our era.  **No book against anything ever has any importance; all that counts are books for something, and that know how to produce it.**\""}, "questions": {"value": "1) **Generalization to unseen benchmarks.**  \n   Will you freeze the taxonomy and prompts, then evaluate on benchmarks not used for taxonomy construction? Please report zero-shot detection precision/recall against maintainer-confirmed labels.\n\n2) **Ablations on the taxonomy’s utility.**  \n   How much does the taxonomy help beyond a short, generic critique prompt? Please include ablations varying prompt length/structure and LLM families.\n\n3) **Separability vs. selection.**  \n   Can you report separability under fixed stratified sampling $(w_E,w_H)$ and compare to your filter? Also provide Kendall-$\\tau$ rank stability with bootstrap CIs across resamples of equal size.\n\n4) **Small-model relevance.**  \n   Many practitioners use 7–8B models. How does your trimmed benchmark perform for distinguishing weaker models? Please report results for a set of small models and compare rank stability to the full benchmarks."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "TglJZJzPFb", "forum": "owFVvaLFdw", "replyto": "owFVvaLFdw", "signatures": ["ICLR.cc/2026/Conference/Submission8036/Reviewer_qdgh"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8036/Reviewer_qdgh"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission8036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760911512531, "cdate": 1760911512531, "tmdate": 1762920031124, "mdate": 1762920031124, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Many benchmarks come with various flaws and this paper presnts a component-wise taxonomy of common benchmark flaws covering the user, environment, evaluation, and ground truth of tasks. It AgentBenchCleaner, an automated filtering pipeline that leverages the taxonomy to filter out flawed tasks. The cleaned tasks are consolidated into AgentHard,  a high-quality benchmark suite."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- The proposed taxonomy is useful for benchmark designer to design high-quality benchmarks\n- The automated cleaning pipeline can ue useful for easily producing high-quality benchmarks"}, "weaknesses": {"value": "like the unified taxonomy and automated cleaning pipeline. However, I think the technical novelty is limited compared to existing benchmark filtering approaches. The paper’s core methodology—combining rule-based filtering, LLM-as-a-judge, and quality heuristics—closely resembles prior works such as MixEval, SMART, ABC, and Arena-Hard, which also employ automated or semi-automated filtering of benchmark artifacts. The main contribution is the integration and systematization of known techniques rather than the introduction of fundamentally new algorithms or architectures. This incremental nature is evident in the Related Work section, where the authors acknowledge that “many pipelines exist for filtering benchmark artifacts,” and their approach primarily “unifies and operationalizes” these ideas rather than advancing the state of the art.\n\nThe proposed framework depends on LLMs for semantic judgment in the cleaning pipeline. The paper admits that the LLM-as-a-judge stage can misclassify tasks, especially when ground truth is partial or schema inconsistencies exist. For example, in the case studies, the authors note that “LLM-as-a-judge may fail to identify certain ambiguous or underspecified tasks,” leading to false positives or negatives in benchmark cleaning. This reliance on LLMs, which are themselves subject to the same brittleness and ambiguity issues as the agents being evaluated, undermines the robustness of the pipeline and may introduce new artifacts or biases into the curated benchmark.\n\nThe paper provides only a cursory discussion of failure cases and does not deeply analyze the limitations of the cleaning pipeline. While a few examples of misjudgment are presented, there is no systematic evaluation of the types or frequencies of errors introduced by the automated filtering process. The authors briefly mention that “stronger prompting and human-in-the-loop validation” could mitigate these issues, but do not quantify the impact or propose concrete solutions. This lack of rigorous failure analysis leaves open questions about the reliability and generalizability of the pipeline, especially when applied to new or evolving benchmarks.\n\nAlthough the paper reports improvements in diversity metrics (e.g., embedding distances) after cleaning, it lacks a qualitative analysis of the types of tasks retained or removed. There is little discussion of whether the curated AgentHard-Bench suite adequately covers the full spectrum of agent capabilities or task domains. The focus on aggregate metrics such as compression ratio and separability does not address potential gaps in benchmark coverage or the risk of excluding valuable but atypical tasks. Without a deeper qualitative assessment, it is unclear whether the cleaning process enhances or inadvertently narrows the scope of agent evaluation.\n\nAgentHard’s taxonomy-driven approach risks overfitting to the specific categories of artifacts identified by the authors. While the taxonomy is comprehensive for current benchmarks, it may not capture novel or emergent flaws in future agent evaluation tasks. The paper claims that the taxonomy is “modular and extensible,” but does not empirically validate its adaptability to new scenarios or provide mechanisms for updating the taxonomy as benchmarks evolve. This limitation could result in the pipeline missing important issues outside the predefined taxonomy, reducing its long-term utility and robustness.\n\nIn summary:\n- The novelty lies mainly in the unified taxonomy and integration, rather than fundamentally new techniques. While the taxonomy and cleaning pipeline are well-executed, the core idea of automated benchmark filtering is present in prior works (e.g., MixEval, SMART, ABC, Arena-Hard). \n- The quality of the cleaning pipeline is not adequately evaluated\n- Generality and extensibility of the taxonomy is not shown"}, "questions": {"value": "1. What measures were taken to ensure that the LLM-as-a-judge stage does not introduce bias or misclassification, and how do these measures compare to human-in-the-loop baselines?\n2. Can the authors provide empirical evidence that the taxonomy and cleaning pipeline generalize to benchmarks beyond the six evaluated, especially those with different task structures or modalities?\n3. How does the pipeline handle ambiguous or partially specified tasks without inadvertently removing valid but challenging samples, and what safeguards are in place?\n4. What qualitative analysis supports the claim that AgentHard-Bench maintains diversity and coverage of agent capabilities after filtering, beyond embedding-based metrics?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "H92AC9qRIJ", "forum": "owFVvaLFdw", "replyto": "owFVvaLFdw", "signatures": ["ICLR.cc/2026/Conference/Submission8036/Reviewer_ajWx"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8036/Reviewer_ajWx"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission8036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761860822407, "cdate": 1761860822407, "tmdate": 1762920030448, "mdate": 1762920030448, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a taxonomy for classifying issues in agentic evals. They then use this taxonomy in an automated pipeline to tag samples with issues. They show strong agreement with human raters."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- errors in agentic evals are a serious problem and needs addressing.\n- a classification of these errors is a useful contribution.\n- they show that an automated approach to data quality checking can perform well (agree with human raters)."}, "weaknesses": {"value": "- No code of the pipeline or dataset. Providing this (in an anon form) would be great to determine how easy others can use the pipeline and  to assess the quality.\n- Zhu 2025 is cited however it should be detailed that Zhu also provide a categorisation of issues. There is significant cross over here, with Zhu providing a more comprehensive breakdown of issues. Adapting the pipeline to follow Zhu's categorisation should be considered and fully detailing the cross over is required. \n- Unlike in Zhu, examples of found issues are not provided. This is a shame as it is hard for the reader to gain insights into the character of the issues discovered. \n- how many human raters were used? measure of inter human agreement?\n- Are the swaps in Table 5 statistically significant? A measure here would be great. \n- Table 5 is also hard to follow - consider using a \"bump chart\"\n- I really like the automatic remove of issues in datasets, however, am hesitant on the removal of saturated tasks and ones with low discriminative power. A caveat on the loss of poor comparability as the dataset naturally evolve should be provided.\n- I think that there is limited novelty here. Main message is that people should use LLMs to check their evals."}, "questions": {"value": "please see weakness section."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "RGY9STT5n7", "forum": "owFVvaLFdw", "replyto": "owFVvaLFdw", "signatures": ["ICLR.cc/2026/Conference/Submission8036/Reviewer_njxr"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission8036/Reviewer_njxr"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission8036/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932032051, "cdate": 1761932032051, "tmdate": 1762920029854, "mdate": 1762920029854, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}