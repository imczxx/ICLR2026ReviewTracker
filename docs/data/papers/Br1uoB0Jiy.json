{"id": "Br1uoB0Jiy", "number": 11626, "cdate": 1758202621398, "mdate": 1763602394086, "content": {"title": "TokenSeek: Memory Efficient Fine Tuning via Instance-Aware Token Ditching", "abstract": "Fine tuning has been regarded as a de facto approach for adapting large language models (LLMs) to downstream tasks, but the high training memory consumption inherited from LLMs makes this process inefficient. Among existing memory efficient approaches, activation-related optimization has proven particularly effective, as activations consistently dominate overall memory consumption. Although prior arts offer various activation optimization strategies, their data-agnostic nature ultimately results in ineffective and unstable fine tuning. In this paper, we propose TokenSeek, a universal plugin solution for various transformer-based models through instance-aware token seeking and ditching, achieving significant fine-tuning memory savings (e.g., requiring only 14.8% of the memory on Llama3.2 1B) with on-par or even better performance. Furthermore, our interpretable token seeking process reveals the underlying reasons for its effectiveness, offering valuable insights for future research on token efficiency.", "tldr": "", "keywords": ["Memory Efficient Fine Tuning"], "primary_area": "foundation or frontier models, including LLMs", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/d29ea5f4dba42d802d9596d9f286b21b15ea8f25.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This paper proposes TOKENSEEK, a method for memory-efficient fine-tuning (MEFT) of LLMs that addresses the bottleneck of activation memory. TOKENSEEK introduces an instance-aware paradigm that dynamically identifies and preserves the most informative tokens per input instance while discarding less crucial ones during gradient computation."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": ">s1: The key insight behind TOKENSEEK is that not all training tokens within LLMs contribute equally to\nmodel fine-tuning.\n\n>s2: The token selection process is based on a transparent, hybrid scoring mechanism leveraging both contextual attention and gradient information. This provides interpretability into why certain tokens are selected and contributes to more stable and effective fine-tuning compared to data-agnostic or random baselines.\n\n>s3: The presentation is clear, and the figures are of high quality."}, "weaknesses": {"value": "See Questions. I would reconsider my score if these concerns are adequately addressed."}, "questions": {"value": "> w1: Regarding the computation of gradient information, the description in Lines 230-232 specifies the use of \"the activations in the penultimate layer,\" and Lines 262-263 mention freezing \"all layers except the output head and the final decoder block.\" Was the use of gradients from more layers explored? Furthermore, would expanding the scope of gradient computation in this manner still ensure stable fine-tuning, or could it potentially introduce instability?\n\n> w2: The manuscript introduces scalars α and β to integrate context and gradient information (Lines 236-245) and notes their \"distinct but complementary patterns\" (Lines 405-407). However, the rationale and process for selecting the optimal values for α and β are not sufficiently discussed. A more detailed analysis is needed to clarify how these critical hyperparameters were determined and how sensitive the method's performance is to their specific values.\n\n>w3: The authors identify a \"global anchor\" in the attention map (Lines 373-377), evident as a prominent vertical line. Based on the proposed TokenSeek framework, could the authors provide an explanation for why this specific token emerges as a global anchor? Furthermore, given that your method ditches tokens based on a combined score, is this particular \"anchor\" token ever identified as less informative and subsequently ditched, or is it consistently preserved? Clarifying this would enhance the interpretability of the token selection process.\n\n>w4: The insight that not all training tokens contribute equally to model fine-tuning is well-founded and reasonable. A similar phenomenon has been observed in LLM reasoning research [1,2]. Could the authors analyze the connections and distinctions between your findings and those from the reasoning domain?\n>\n>[1] Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning.\n>\n>[2] Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "0Ix573ulHg", "forum": "Br1uoB0Jiy", "replyto": "Br1uoB0Jiy", "signatures": ["ICLR.cc/2026/Conference/Submission11626/Reviewer_cH6y"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11626/Reviewer_cH6y"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission11626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1760946773654, "cdate": 1760946773654, "tmdate": 1762922698168, "mdate": 1762922698168, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes TOKENSEEK, a plug-in for memory-efficient fine-tuning that is instance-aware at the token level. The core idea is to (1) seek salient tokens per input by combining context signals (attention-derived scores) and optimization signals (gradients of the loss w.r.t. penultimate-layer activations), then (2) ditch the rest by disabling gradient computation for unselected tokens during backprop (retaining forward activations but avoiding their gradient/optimizer memory)."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "Combines attention-derived context and gradient saliency to rank tokens per example; empirically more effective and more stable than random selection. Reports 2.8 GiB peak in one setting and ~15% of full-token QLoRA peak on Llama-3.2-1B, while maintaining accuracy; cumulative with PEFT (LoHa/QLoRA). isualizations show complementary early-token bias from attention and late-token focus from gradients; helps explain the chosen subset."}, "weaknesses": {"value": "Need a controlled knob table for each baseline (checkpointing, offloading, micro-batching, seq length, optimizer sharding) and the resulting peak+average memory to ensure apples-to-apples comparisons. Gradient-based scoring requires a partial backward pass; quantify this overhead per step and analyze action oscillations/instability of the selected set across training. Provide seed variance tables."}, "questions": {"value": "For LoRA/QLoRA/LoHa/IA3 and full FT, which memory knobs (checkpointing schedule, ZeRO, offload, seq length, grad accumulation, activation precision) were enabled? Please add a per-run configuration table with measured peak+average memory. How often does the selected token subset change across epochs/steps? Any evidence of training instabilities when the subset shifts? Could you learn α/β or the token fraction from validation loss via a small controller?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cx7xDewtqn", "forum": "Br1uoB0Jiy", "replyto": "Br1uoB0Jiy", "signatures": ["ICLR.cc/2026/Conference/Submission11626/Reviewer_kTDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11626/Reviewer_kTDb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761382516756, "cdate": 1761382516756, "tmdate": 1762922697449, "mdate": 1762922697449, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper discusses a new tool called TOKENSEEK, which facilitates easier and more efficient working with large language models. The concept is straightforward: when you want to teach a massive model something new, you do not need to concentrate on every single word in your data. TOKENSEEK identifies the words that are most significant—by examining how they relate to the rest of the sentence and how much they contribute to the model's learning. It then gives special attention to these crucial words during training, and temporarily disregards the others. As a result, the model utilizes less memory without compromising important information."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "It works by figuring out which words in each example are most important, using both how much attention the model gives them and how much they affect learning. This method is not only more accurate and reliable than just picking tokens at random, but it also uses a lot less memory—sometimes only 2.8 GiB, or about 15% of what’s usually needed with full-token approaches, all while keeping the model’s accuracy high. Plus, when you look at the results, you can see that attention tends to focus on the beginning of the input, while the learning signals (gradients) highlight words toward the end. This helps make sense of why the chosen words matter."}, "weaknesses": {"value": "It’s important to have a clear comparison for each method—like checkpointing, offloading, micro-batching, sequence length, and optimizer sharding—so having a table that shows how each setting affects both peak and average memory makes it easier to compare them fairly. Since using gradient-based scoring means you have to do a partial backward pass, it’s a good idea to measure how much extra time or memory that adds for each step, and to look at how stable or consistent the chosen set of tokens is as training goes on. It also helps to include tables showing how results vary with different random seeds."}, "questions": {"value": "When using methods like LoRA, QLoRA, LoHa, IA3, or full fine-tuning, which memory-saving options did you turn on? (For example: checkpointing schedules, ZeRO, offloading, sequence length, gradient accumulation, and activation precision.) It would be really helpful to include a table for each run showing which settings you used and what the peak and average memory usage was.\n\n\n\nAlso, how often does the set of chosen tokens change as training goes on? Did you notice any problems or unusual behavior when the selected tokens shifted? And is it possible to automatically figure out the best token fraction or the α/β values by looking at the validation loss, maybe with a small controller?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "cx7xDewtqn", "forum": "Br1uoB0Jiy", "replyto": "Br1uoB0Jiy", "signatures": ["ICLR.cc/2026/Conference/Submission11626/Reviewer_kTDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11626/Reviewer_kTDb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission11626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761382516756, "cdate": 1761382516756, "tmdate": 1763354922509, "mdate": 1763354922509, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "TOKENSEEK introduces a universally applicable plugin for memory-efficient fine-tuning of large language models without altering their architecture. It adaptively identifies and retains only the most salient tokens per instance—based on contextual and gradient importance—while discarding less useful ones to reduce activation memory. This instance-aware token ditching achieves significant memory savings with stable or improved performance, offering a generalizable, architecture-agnostic approach to efficient fine-tuning."}, "soundness": {"value": 3}, "presentation": {"value": 4}, "contribution": {"value": 3}, "strengths": {"value": "Solid problem formulation  as fine-tuning large LLMs is highly memory-intensive, with activations contributing a major share of the cost.\nAnd Innovative approach to integrates gradient information with context scores to capture a more holistic measure of token importance, addressing the limitation that context-based evaluation alone reflects only intra-sequence relevance, not fine-tuning contribution.\n\n\nSome of the specific strength:\n - Architecture-agnostic design: The proposed plugin can be applied to any pretrained LLM without modifying its core architecture that is very practical.\n - Strong empirical results: Demonstrates substantial memory savings while maintaining or improving model performance across benchmarks. Very solid results in table 1, specifically with QLora.\n- Interpretability and analysis: Offers clear insights into token-level importance and provides comprehensive analysis on how token ditching affects fine-tuning. Figure 4 and other analysis plots are insightful.\n- Clarity and presentation: This paper is very well-written, logically structured, and easy to follow, with clear motivation and experimental validation."}, "weaknesses": {"value": "- Scalability concerns: The token regrouping step—where tokens are sorted by importance and selectively included for backpropagation—may pose significant implementation and communication challenges in large-scale distributed fine-tuning setups. Synchronizing token importance scores and managing uneven token partitions across devices could offset some of the claimed memory savings.\n\n- Complexity of integration: Although conceptually modular, integrating the method into existing large-scale MEFT pipelines may require non-trivial modifications to data loading and parallelism strategies.\n\n - Limited large-scale validation: Experiments are mostly conducted on moderate-sized models (<=3B) and datasets; the method’s stability and efficiency under massive multi-node training scenarios remain unverified.\n\n- Selective update imbalance: Dropping gradients for less salient tokens could bias training if token importance is misestimated or unstable across iterations."}, "questions": {"value": "- In Eq 3, the notation of i and j is a bit confusing. It might be more clear to express context score for ${t_j}$ token as $\\Sigma_{i=1}^{n} A_{ij}$. To clarify that you are summing the attention scores across all rows for the given column j.\n- Similarly the notation in eq 4 should indicate that the the gradient score is for  token $t_j$.\n\n- More clarification on the token regrouping process and its practicality in distributed fine-tuning settings would strengthen the paper."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "4DKnqFAvi6", "forum": "Br1uoB0Jiy", "replyto": "Br1uoB0Jiy", "signatures": ["ICLR.cc/2026/Conference/Submission11626/Reviewer_CWhD"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11626/Reviewer_CWhD"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission11626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761948636284, "cdate": 1761948636284, "tmdate": 1762922696730, "mdate": 1762922696730, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper addresses the limitation of current fine-tuning methods that largely overlook the information contained in individual training instances. The authors propose TokenSeek, a universal plug-in framework for Transformer-based models that performs instance-aware token selection and ditching. By selectively fine-tuning on important tokens identified via attention and gradient signals, TokenSeek reduces activation memory consumption while maintaining comparable performance."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- The paper is clearly written and generally easy to follow, with only minor typos.\n- The proposed TokenSeek method is conceptually simple and practically implementable.\n- Experimental results demonstrate competitive performance on multiple downstream tasks (e.g., QA, reasoning) across different LLM architectures such as LLaMA and Qwen.\n- The experiments are comprehensive and include ablations and cross-task evaluations, which strengthen empirical credibility."}, "weaknesses": {"value": "- Limited Novelty Clarification:\nThe main contribution—token importance–based selection—extends the previous TokenTune framework rather than introducing a fully new paradigm. While TokenSeek improves token importance estimation compared to random selection, the core ideas of memory reduction and generalizability are largely inherited from TokenTune. The paper should better articulate what is fundamentally novel about TokenSeek beyond methodological refinements.\n\n- Comparison to Low-Rank and Partial-Tuning Methods:\nOn Line 835, the paper notes that TokenSeek and TokenTune incur ~11–15% more GPU hours than full-token tuning. In contrast, low-rank PEFT methods (e.g., LoRA) typically achieve both lower memory and faster training. Since TokenSeek inherits TokenTune’s extra computational overhead, the paper should clarify whether TokenSeek offers any non-trivial advantages over low-rank or partial-tuning approaches.\n\n- Missing Baselines and Related Work:\nSeveral recent sparsity-based PEFT methods are missing from the related work and experimental comparisons. As TokenSeek belongs to the PEFT family, adding one representative partial-tuning or sparse fine-tuning baseline (e.g., [1–3]) would provide a more complete evaluation and contextualization of the method’s contribution.\n\n\n- Some minor typos\n“as showin in Fig. 1 (a)” → shown; \n“Benifit from” → Benefit from; \nL313 “Unde” → Under; \n“achiving” → achieving; \n“TokenSeek achieve” → achieves; \n“LoRA/QLoRA achieves” → plural → achieve\n\n\n\n\n[1] Scaling Sparse Fine-Tuning to Large Language Models\n\n[2] Sparse Matrix in Large Language Model Fine-tuning\n\n[3] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"}, "questions": {"value": "- Code-Domain Generalization:\nThe experiments cover QA and Open-Platypus datasets. How would TokenSeek perform on code-related datasets (e.g., CodeAlpaca, HumanEval)? Code data tends to have dense information where most tokens are important, which could reduce token sparsity efficiency. Including one experiment on a code dataset would better demonstrate TokenSeek’s generalizability.\n\n- Source of Memory Savings:\nPlease clarify precisely where the memory savings originate. Since token selection is gradient-based, forward propagation must still process all tokens, and optimizer memory (e.g., Adam states) typically depends on trainable parameters rather than token count. How does TokenSeek achieve the reported activation memory reduction—through selective gradient storage, reduced backward pass, or another mechanism? A detailed explanation would strengthen the technical soundness.\n\n\n\nAt present, this appears to be a borderline paper (score ~5). The core idea is promising, but its novelty and advantage over prior TokenTune and LoRA-based methods need clearer articulation. If the authors can convincingly address the questions above—particularly regarding memory savings and broader generalization—I would be open to raising my score."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "9o2yGngfVK", "forum": "Br1uoB0Jiy", "replyto": "Br1uoB0Jiy", "signatures": ["ICLR.cc/2026/Conference/Submission11626/Reviewer_ZpqT"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission11626/Reviewer_ZpqT"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission11626/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762312962849, "cdate": 1762312962849, "tmdate": 1762922696281, "mdate": 1762922696281, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}