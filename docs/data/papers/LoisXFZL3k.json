{"id": "LoisXFZL3k", "number": 2927, "cdate": 1757301920147, "mdate": 1759898118797, "content": {"title": "Faithful Bi-Directional Model Steering via Distribution Matching and Distributed Interchange Interventions", "abstract": "Intervention-based model steering offers a lightweight and interpretable alternative to prompting and fine-tuning. However, by adapting strong optimization objectives from fine-tuning, current methods are susceptible to overfitting and often underperform, sometimes generating unnatural outputs.\nWe hypothesize that this is because effective steering requires the faithful identification of internal model mechanisms, not the enforcement of external preferences.\nTo this end, we build on the principles of *distributed alignment search (DAS)*, the standard for causal variable localization, to propose a new steering method: **Concept DAS (CDAS)**.\nWhile we adopt the core mechanism of DAS, *distributed interchange intervention (DII)*, we introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions.\nCDAS differs from prior work in two main ways:\nfirst, it learns interventions via weak-supervised distribution matching rather than probability maximization;\nsecond, it uses DIIs that naturally enable bi-directional steering and allow steering factors to be derived from data, reducing the effort required for hyperparameter tuning and resulting in more faithful and stable control.\nOn AxBench, a large-scale model steering benchmark, we show that CDAS does not always outperform preference-optimization methods but may benefit more from increased model scale.\nIn two safety-related case studies, overriding refusal behaviors of safety-aligned models and neutralizing a chain-of-thought backdoor, CDAS achieves systematic steering while maintaining general model utility.\nThese results indicate that CDAS is complementary to preference-optimization approaches and conditionally constitutes a robust approach to intervention-based model steering.", "tldr": "We propose Concept Distributed Alignment Search (CDAS), a DAS-inspired steering method featuring distribution matching objective and distributed interchange interventions. Through CDAS, we argue that model steering might be different from SFT.", "keywords": ["model steering", "mechanistic interpretability"], "primary_area": "interpretability and explainable AI", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/4a11e228786dc6f8222b27bd1b74ca3f8eb07d07.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This works presents a new activation steering methods building up on the idea of distributed alignment search (DAS). Therefore, the authors introduce a novel distribution matching objective tailored for the steering task by aligning intervened output distributions with counterfactual distributions. Authors verify the proposed method on AxBench, two cases studies and also ablate the performance tradeoff of steering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- The paper is **clearly written** and is easy-to-follow despite a lot of technical details\n- **Appealing formulation** with distribution matching to the counterfactual distribution. Only caveat is that it introduces another axis of data augmentations requirements to satisfy the training data need.\n- **Exhaustive experiments** over multiple benchmarks and various ablation studies, analyzing the benefits and limitations of CDAS. Overall, CDAS shows promising performance, especially under the fact that it seems to lead to less off-target effects than RePS (as confirmed in table 3) which seems to dominate performance-wise.\n- **Steering / Performance tradeoff experiment in table 3** — this aspect is heavily ignored in many papers and I appreciate this analysis. However I am not sure if overriding refusals is the most convincing steering axis to test in this setting, as refusals may happen rarely on these benchmarks. For instance, repeating this experiment on a safety relevant task would further strengthen the evidence."}, "weaknesses": {"value": "- **Noise in Experiments:** Some results seem quite noisy (e.g. Figure 1) — I would strongly recommend adding std. or confidence intervals to the remaining experiments.\n- **Tuned Factor Dependence:** Can you provide more intuition on why there is such a big difference between the unit factor and the tuned factor?\n- **Benefits with Scale:** The authors raise multiple time the argument that CDAS benefits with scale. Do you have an explanation for this? Especially as you present better performance scores for the 7B instead of the 80B model (in Table 2)"}, "questions": {"value": "- Why is DAS missing in the AxBench experiments?\n- Why do you choose the tuning factor differently for DAS/CDAS (based on Alpaca) compared to RePs? Is this the source of the performance discrepancy? \n- Figure 1C: Variance is huge and it seems to work only on one tested layer. Do you have an explanation for this? \n- Table 4: Is the fact that CDAS improves performance on tinyMMLU not an indicating that results might be very noisy? \n- Do trends between methods change in other data regimes (less or more data)\n- Do you match the amount of training data? All pairwise approaches basically use the double amount of data (when treating every prompt + response as one data point)"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "eTJ4svNWeT", "forum": "LoisXFZL3k", "replyto": "LoisXFZL3k", "signatures": ["ICLR.cc/2026/Conference/Submission2927/Reviewer_xeJA"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2927/Reviewer_xeJA"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission2927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761663173049, "cdate": 1761663173049, "tmdate": 1762916445542, "mdate": 1762916445542, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces **Concept Distributed Alignment Search (CDAS)**, an intervention-based model-steering method extending *Distributed Alignment Search (DAS)* from causal variable localization. The method aims to identify concept-specific internal features rather than impose external preferences. Experiments on _AXBENCH_ and two safety case studies (refusal override and CoT-backdoor neutralization) show CDAS can maintain model utility and scales better with model size, though it does not consistently outperform preference-optimization baselines."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "- Interesting conceptual shift linking steering with causal localization and interpretability.  \n- Comprehensive experiments across benchmarks and safety settings."}, "weaknesses": {"value": "I am very much an outsider to the “model steering” field, however, unfortunately, this paper does a weak job at presenting much needed context for new readers to appreciate the why and how of their manuscript\n\nMuch of the structure and writing assumes readers are familiar with extant work and understand their shortcomings\n\ne.g.,\n* [l42/46] how does “intervention-based” result in “optimization-based”?\n\n* [l52] what does “degenerate, repetitive generations” even mean?\n\n* [l55] why should the readers appreciate DAS, and the proposed extension CDAS, as “standard approach[es] for causal variable localization.” what does this even mean?"}, "questions": {"value": "- What explains the non-monotonic behavior in Fig. 1c?  \n- Please see questions above in weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 1}, "code_of_conduct": {"value": "Yes"}}, "id": "0DYi0hG9VT", "forum": "LoisXFZL3k", "replyto": "LoisXFZL3k", "signatures": ["ICLR.cc/2026/Conference/Submission2927/Reviewer_GoDb"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2927/Reviewer_GoDb"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission2927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762103868217, "cdate": 1762103868217, "tmdate": 1762916445353, "mdate": 1762916445353, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "Building upon the framework of distributed alignment search (DAS), the authors introduce Concept DAS (CDAS), a method that learns interventions via weakly supervised distribution matching between intervened and counterfactual outputs. CDAS facilitates bi-directional, data-driven model steering with fewer hyperparameters and enhanced stability. Through experiments on AxBench and various safety case studies—such as overriding refusal behaviors and neutralizing a chain-of-thought backdoor—the authors demonstrate that CDAS achieves faithful and scalable control while preserving the overall utility of the model. The approach serves as a robust complement to preference-optimization methods, offering an alternative pathway for effective model steering."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The paper tackles a compelling and timely problem, presenting a solution that is both concise and elegant. The manuscript is well-written and structured, making the methodology and results accessible. The authors conduct extensive experiments to validate CDAS, providing thorough comparisons with existing approaches. Detailed experimental protocols and results are available in the supplemental material, enhancing transparency and reproducibility."}, "weaknesses": {"value": "Despite its merits, there are a few aspects that require clarification or further analysis:\n\n(1) In certain experiments, CDAS underperforms relative to baselines (e.g., Tables 1 and 3). The authors should provide insights or hypotheses explaining these performance gaps.\n\n(2) It remains unclear under which conditions CDAS excels and under which scenarios it may fall short. A discussion of the limitations and situational strengths of the method would strengthen the paper."}, "questions": {"value": "See weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 8}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "A4QdtwP3gO", "forum": "LoisXFZL3k", "replyto": "LoisXFZL3k", "signatures": ["ICLR.cc/2026/Conference/Submission2927/Reviewer_JHaq"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2927/Reviewer_JHaq"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission2927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762123384179, "cdate": 1762123384179, "tmdate": 1762916445113, "mdate": 1762916445113, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper introduces a new steering method called Concept Distributed Alignment Search (CDAS). It builds upon the Distributed Change Intervention (DCI) technique from the DAS method, combining it with a distribution-matching objective based on the Jensen–Shannon divergence. The method is evaluated on the AXxBENCH benchmark and two safety-related case studies focused on concept suppression."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "The CDAS objective encourages the model to learn concepts that are aligned with the model’s overall output distribution under the concept-induced input. Consequently, supervision does not come directly from ground-truth responses, but rather from the model’s own internal distribution. This is an interesting idea, as it may lead to outputs that are more naturally aligned with the inherent responses of LLMs.\n\nIn the refusal override experiments, CDAS achieves the best KL divergence loss, while maintaining reasonable performance on the TruthfulQA and MMLU benchmarks. This suggests that the intervened model’s outputs remain close to the model’s natural response distribution."}, "weaknesses": {"value": "While the premise behind CDAS and its training objective is compelling, the results are mixed. For example, in the experiments presented in Table 1, CDAS achieves the best performance on Gemma-2-9 L20 under a tuned factor, outperforming all other methods. However, on other intervention layers and with smaller models (e.g., 2B), CDAS fails to surpass RePS—although it still outperforms DiM, BiPO, and, in two cases, Lang. \n\nIn the refusal override experiments, CDAS also underperforms on the smaller model. In the CoT experiments for neutralizing backdoors, CDAS successfully overrides malicious behavior, but only when applied to earlier layers; for later layers, the ASR increases sharply and exhibits large variance.\n\nParticularly in the experiments from Table 1, CDAS appears highly sensitive to the setup of the steering factor, which could make its application in practical scenarios cumbersome.\n\nWhile the authors provide quantitative comparisons of CDAS and alternative methods, it would be valuable to include an analysis of training stability and computational overhead (see specific questions below)."}, "questions": {"value": "As mentioned in the weaknesses, CDAS demonstrates somewhat mediocre performance across the evaluated tasks—sometimes surpassing other models or layer interventions, and sometimes falling behind—though potentially producing outputs more faithful to the underlying distribution of the LLM (as indicated by KL divergence). I appreciate that the authors acknowledge these nuances and discuss their method fairly, suggesting that CDAS may be preferable for larger LLMs or when preserving model utility is a key objective. I still believe CDAS is an interesting addition to the family of steering methods.However, I would appreciate deeper insights into the causes of its underperformance. For instance:\n\n- Why does CDAS perform worse on smaller models? \n- Is this due to model scale, or does it depend on the model family?\n- In Figure 1, why does the variance on the ASR task increase so sharply?\n\nA more detailed discussion of these points could significantly strengthen the paper.\n\nIn the same vein, I am curious how CDAS compares to other approaches in terms of training stability. Could the variance in Figure 1 be a result of collapsed or unstable training? How does the training objective behave across experiments? The authors acknowledge that CDAS is sensitive to the steering factor—could this sensitivity be related to the stabilization (or destabilization) of training?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "cE571Ws6sM", "forum": "LoisXFZL3k", "replyto": "LoisXFZL3k", "signatures": ["ICLR.cc/2026/Conference/Submission2927/Reviewer_haqH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission2927/Reviewer_haqH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission2927/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762269047406, "cdate": 1762269047406, "tmdate": 1762916444808, "mdate": 1762916444808, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}