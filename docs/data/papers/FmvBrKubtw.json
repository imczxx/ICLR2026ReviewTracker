{"id": "FmvBrKubtw", "number": 12703, "cdate": 1758209607865, "mdate": 1759897493047, "content": {"title": "Negotiated Reasoning: On Provably Addressing Relative Over-Generalization", "abstract": "We focus on the relative over-generalization (RO) issue in fully cooperative multi-agent reinforcement learning (MARL). Existing methods show that endowing agents with reasoning can help mitigate RO empirically, but there is little theoretical insight. We first prove that RO is avoided when agents satisfy a consistent reasoning requirement. We then propose a new negotiated reasoning framework connecting reasoning and RO with theoretical guarantees. Based on it, we develop an algorithm called Stein variational negotiated reasoning (SVNR), which uses Stein variational gradient descent to form a negotiation policy that provably bypasses RO under maximum-entropy policy iteration. SVNR is further parameterized with neural networks for computational efficiency. Experiments demonstrate that SVNR significantly outperforms baselines on RO-challenged tasks, including Multi-Agent Particle World and MaMuJoCo, confirming its advantage in achieving better cooperation.", "tldr": "", "keywords": ["Multi-Agent Reinforcement Learning", "Relative Over-Generalization", "Stein variational gradient descent"], "primary_area": "reinforcement learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/95d66227ca9832c34e15b1f6cb7ffd46f2ef5861.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "This work proposes a ‚Äúnegotiated reasoning‚Äù with an SVGD-based algorithm that proves RO-free convergence under strict conditions. The authors validate the approach on Differential Games, Particle Gather, and MaMuJoCo, improving on returns versus baseline approaches."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "* Clean conceptual split (PRO vs. ERO) and a ‚Äúconsistent reasoning‚Äù criterion that ties the pathology to modeling assumptions.\n* A (mostly) coherent theory: SVNR via nested negotiation + MaxEnt iteration.\n* Comprehensive experiments show that ERO consistently improves over reasoning baselines and mainstream MARL on MaMuJoCo."}, "weaknesses": {"value": "* Guarantees rely on strong assumptions, namely annealing $\\alpha$ to 0, and at times finite action spaces, which do not match continuous-control practice.\n* Early theory assumes access to the optimal joint policy, then uses an estimator. The gap between proof conditions and the amortized neural implementation is not fully bridged.\n* Decentralized execution relies on amortization faithfully reproducing multi-round negotiation, a strong limiting assumption in practice.\n* Sample efficiency and robustness are underexplored. At the minimum I would expect training curves for the method versus the baseline approaches."}, "questions": {"value": "* Clarify which theorems still hold in continuous action spaces without discretization.\n* Quantify sensitivity to $\\alpha$-annealing with an ablation.\n* Tighten the link between Theorem 3.2‚Äôs $\\alpha \\rightarrow 0$ requirement and continuous-control results.\n* Will the authors release their code?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "WJVMHDkggh", "forum": "FmvBrKubtw", "replyto": "FmvBrKubtw", "signatures": ["ICLR.cc/2026/Conference/Submission12703/Reviewer_xop6"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12703/Reviewer_xop6"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission12703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761852102995, "cdate": 1761852102995, "tmdate": 1762923533850, "mdate": 1762923533850, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper tackles the problem of relative over-generalization (RO) in cooperative multi-agent reinforcement learning. The authors point out that under CTDE settings, RO arises in two stages: during training (when agents form a perceived joint policy) and during execution (when they act without seeing others‚Äô final actions). They formalize these as Perceived RO (PRO) and Executed RO (ERO), and show that if all agents achieve consistent reasoning‚Äîreasoning about others in a way consistent with their optimal or executed policies‚Äîthen RO can be avoided. To realize this condition, they propose a Negotiated Reasoning (NR) framework where agents iteratively update joint action beliefs through structured ‚Äúnegotiation.‚Äù They instantiate this idea as Stein Variational Negotiated Reasoning (SVNR), which leverages SVGD updates and a nested negotiation structure. A neural amortized version enables efficient decentralized execution. Experiments on several MARL benchmarks demonstrate that SVNR avoids sub-optimal equilibria and performs better than prior reasoning-based methods."}, "soundness": {"value": 3}, "presentation": {"value": 2}, "contribution": {"value": 3}, "strengths": {"value": "1. Well-motivated and conceptually clear. The paper tackles a long-standing issue in cooperative MARL‚Äîrelative over-generalization (RO)‚Äîfrom a fresh theoretical angle. The decomposition into Perceived RO (PRO) and Executed RO (ERO) is a intuitive way to separate the effects of exploration during training and coordination failures during execution. This framing alone makes the paper stand out conceptually.\n\n2. Theoretical depth. The authors do not stop at defining RO but go on to establish a sufficient condition‚Äîconsistent reasoning‚Äîunder which RO provably disappears. The reasoning flow (formalization ‚Üí condition ‚Üí constructive algorithm) is solid and self-contained, which is rare in the MARL literature where many ‚Äútheoretical‚Äù claims are hand-wavy.\n\n3. Negotiated Reasoning framework is novel. The idea of embedding a negotiation mechanism among agents‚Äîmodeled via particle-based updates and linked to SVGD‚Äîis original and technically well-grounded. It provides a new way to understand coordination as an iterative reasoning process rather than just communication or credit assignment.\n\n4. Empirical validation matches the theory. Experiments across both simple and complex cooperative environments (Differential Games, Particle Gather, MaMuJoCo) demonstrate consistent gains."}, "weaknesses": {"value": "1. Assumptions may be restrictive. The theoretical results rely on strictly nested negotiation sets and the maximum-entropy policy iteration framework. These are strong assumptions, and it‚Äôs unclear how sensitive the algorithm is to relaxing them. More empirical discussion of non-strict or sparse negotiation topologies would strengthen the practical claim.\n\n2. Scalability and computational overhead are under-discussed. While the amortized neural version helps, the original SVNR involves particle-based updates and iterative negotiation steps. The paper lacks a quantitative analysis of training cost, memory footprint, or runtime.\n\n3. Limited connection to broader MARL literature.\nThe paper positions itself mainly against ‚Äúreasoning-based‚Äù methods, but doesn‚Äôt fully clarify how its negotiation differs in spirit from other opponent-modeling or communication-based approaches. A clearer conceptual contrast would make the contribution easier to situate.\n\n4. Experimental evaluation is strong in breadth but shallow in analysis.\nWhile performance improvements are clear, the paper could provide more interpretability: e.g., what negotiation dynamics emerge, how many rounds are effectively used, or how PRO/ERO evolve during training. Without this, the reader must take the ‚Äúnegotiation‚Äù story largely on faith.\n\n5. Writing density. Some proofs and definitions could use more intuitive explanation or visual support (especially PRO/ERO and the consistent reasoning condition). The theoretical sections are mathematically heavy, which may alienate non-specialist readers."}, "questions": {"value": "1. Generalization to partial observability: The consistent reasoning definition assumes full observability of state. Do you foresee PRO/ERO or the theoretical guarantees extending to POMDP settings?\n\n2. Practical guidance: For someone implementing SVNR, how should they select the number of particles ùëÄ and negotiation rounds K? Is there a heuristic or convergence indicator?\n\n3. Interpretability: Can you visualize or quantify the evolution of ‚Äúagreement‚Äù or ‚Äúnegotiation‚Äù during training to make the proposed reasoning process more transparent?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "Wmt1Fem8s1", "forum": "FmvBrKubtw", "replyto": "FmvBrKubtw", "signatures": ["ICLR.cc/2026/Conference/Submission12703/Reviewer_85jW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12703/Reviewer_85jW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission12703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761918297689, "cdate": 1761918297689, "tmdate": 1762923533257, "mdate": 1762923533257, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "The paper reframes relative over-generalisation (RO) in cooperative MARL into two operational notions: perceived RO (PRO) during policy updates and executed RO (ERO) during decentralised execution. It argues that if agents reason consistently about teammates (during training and at test), RO can be avoided. To operationalise this, it proposes Negotiated Reasoning and an instantiation, SVNR, which uses (message-passing) SVGD to negotiate joint actions, embedded in a maximum-entropy policy-iteration loop, with a practical amortised neural version for speed. Theory shows PRO-free negotiation under a strictly nested conditional factorisation and (stated) finite action spaces; experiments on RO-heavy games and multi-agent MuJoCo show strong empirical gains."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "- Clear, useful split of RO into PRO vs ERO, making the pathology diagnosable during training and execution.\n- Principled negotiation mechanism tied to MaxEnt policy iteration (not just a heuristic add-on)\n- Stated convergence/guarantee story under a strictly nested factorisation \n- Amortised implementation to distil many negotiation steps into one forward pass; practical ablations on particles, team size, topology.\n- Strong empirical wins on PRO/ERO-challenged settings and competitive continuous-control benchmarks."}, "weaknesses": {"value": "- Core theorems are written for finite action spaces, while key experiments use continuous actions; the theory‚Äìpractice gap should be tightened or clearly scoped.\n- The paper states communication-free execution, yet the pseudocode shares noise variables between neighbours at test time- I think this needs unambiguous clarification.\n- Guidance on schedules/sensitivity is thin and could affect stability/robustness.\n- The idealised policy-iteration description initially assumes a known model, then switches to critic learning; the implications of this shift aren‚Äôt fully analysed.\n- Compute/tuning parity vs baselines isn‚Äôt fully tabulated \n- Scope limits: assumes CTDE, full observability; robustness under partial observability is left open."}, "questions": {"value": "- Continuous actions: which parts extend beyond finite $|U|$?\n- Do agents share any variables at test time? If not, please fix the pseudocode; if yes, how is this still communication-free?\n- How sensitive are results to final $\\alpha$ and the annealing schedule across tasks?\n- Please provide wall-clock/GPU hours and tuning budgets/ranges for all baselines, and confirm identical exploration/Œ± schedules where applicable.\n- Beyond strictly nested, what guarantees (or empirical scaling laws) hold for partial DAGs/peer sampling?\n- Can you characterise the error introduced when replacing the model-based policy-iteration view with the critic-based practical algorithm?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 3}, "code_of_conduct": {"value": "Yes"}}, "id": "LiVAh0dMsp", "forum": "FmvBrKubtw", "replyto": "FmvBrKubtw", "signatures": ["ICLR.cc/2026/Conference/Submission12703/Reviewer_biXy"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission12703/Reviewer_biXy"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission12703/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762167660857, "cdate": 1762167660857, "tmdate": 1762923532679, "mdate": 1762923532679, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}