{"id": "NPkwKqluMr", "number": 10564, "cdate": 1758175716480, "mdate": 1759897643117, "content": {"title": "DRIP: Invariance-preserving Data Reduction for Domain Generalization", "abstract": "Domain Generalization (DG) aims to enable deep models trained on several source domains to generalize to unseen target domains. Existing works assume that data contain both invariant features, whose relationship with label is invariant across domains, and spurious features, which are spuriously correlated to the label. It is widely recognized that retaining invariant features while suppressing spurious ones is critical to achieving DG. However, despite this inspiration, current methods often struggle to guarantee a separation between these features, or their guarantees need relatively strong assumptions of data. In this work, we propose DRIP (Data Reduction with Invariance-Preserving), a data augmentation paradigm theoretically proven to improve domain generalization. We prove that such DRIP can reduce spurious features while preserving invariant features, forcing the model to rely more on generalizable invariant features during training. Following the principles of DRIP, we propose several low cost implementations and verify that one of those indeed meets the criteria of DRIP. Experiments show that it consistently outperforms existing DG baseline in two modalities, vision and IMU.", "tldr": "We propose DRIP, a data augmentation paradigm that theoretically proven to improve domain generalization. Evaluations confirm its superior performance on both visual and IMU datasets.", "keywords": ["domain generalization", "data augmentation", "causality", "invariance"], "primary_area": "transfer learning, meta learning, and lifelong learning", "venue": "ICLR 2026 Conference Submission", "pdf": "/pdf/8f1a0fe85cbfe08685059aab4b45a12f500a996f.pdf", "supplementary_material": ""}, "replies": [{"content": {"summary": {"value": "The paper introduces DRIP (a data transformation family that (A) preserves label-relevant information, $(P(Y|R(X))=P(Y|X))$, while (B) reducing input entropy $(H(R(X))<H(X))$. Under an invertible generative assumption for (X), the authors prove DRIP preserves mutual information with invariant (causal) features and lowers information tied to spurious ones (Theorems 4–5). A simple channel-wise input dropout is proposed as a practical DRIP instance, evaluated on DomainBed (vision) and multiple IMU HAR datasets. Results show consistent but modest average gains over strong baselines; patch/pixel-wise variants are weaker."}, "soundness": {"value": 3}, "presentation": {"value": 3}, "contribution": {"value": 3}, "strengths": {"value": "1. **Clear, principled objective**: Formalizes a compression-with-invariance lens for DG and ties it to causal desiderata (Theorems 4–5). \n2. **Simple, low-cost implementation**: Channel-wise dropout is trivial to plug in and shows the best behavior among tested variants. \n3. **Empirical coverage**: Improvements on DomainBed  and across several IMU datasets; analyses of dropout rate and #domains are helpful."}, "weaknesses": {"value": "1. **Strong Condition A**: Requiring $(P(Y|R(X))=P(Y|X))$ is rarely verifiable; the paper’s empirical proxy (comparing converged training losses) is suggestive but not a test of conditional distributions or invariance across environments. This risks circular validation.  \n2. **Heavy modeling assumptions**: Invertibility of $(X=c(Z_{inv}, Z_{spu},N))$ and identifiability are strong; many real pipelines violate them (augmentations, compression, quantization). Theory may not transfer intact. \n3. **Modest margins and sensitivity**: Gains on DomainBed are incremental on average; pixel/patch variants underperform in places, and channel-wise dropout can fully erase inputs (RGB), forcing ad-hoc “net-loss” adjustments downstream. More ablations are needed to rule out selection effects.  \n4. **Baselines scope**: Comparisons omit several shape/texture-robust augmentations (Cutout/Random Erasing/Hide-and-Seek variants tuned for DG) and recent DG regularizers; fairness/tuning parity across methods is unclear. (Results tables list many classics but not all relevant augmentation-centric DG lines.) \n5. **Mechanistic evidence**: No causal/representation probes showing spurious-feature attenuation (background dependence tests, causal edit robustness, or invariance diagnostics per domain)."}, "questions": {"value": "Please see the weaknesses above."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 6}, "confidence": {"value": 2}, "code_of_conduct": {"value": "Yes"}}, "id": "2gnGtjB3BK", "forum": "NPkwKqluMr", "replyto": "NPkwKqluMr", "signatures": ["ICLR.cc/2026/Conference/Submission10564/Reviewer_kbks"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10564/Reviewer_kbks"], "number": 1, "invitations": ["ICLR.cc/2026/Conference/Submission10564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761234881220, "cdate": 1761234881220, "tmdate": 1762921837947, "mdate": 1762921837947, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper introduces DRIP (Data Reduction with Invariance-Preserving), a theoretically grounded data augmentation paradigm for DG. The core idea is to apply a transformation R that: \t(A) preserves the label-relevant information: P(Y|R(X)) = P(Y|X), (B) reduces the overall entropy of the input: H(R(X)) < H(X).  The authors provide theoretical justification that such transformations reduce spurious correlations while preserving invariant features, and they propose several low-cost implementations (e.g., channel-wise dropout) for both image and IMU domains. Experimental results on DomainBed and IMU datasets show performance gains over prior DG baselines."}, "soundness": {"value": 2}, "presentation": {"value": 3}, "contribution": {"value": 2}, "strengths": {"value": "- Provides a clean, theoretically-motivated definition of a class of data augmentations (DRIP) for DG.\n- Introduces a simple but effective implementation (channel-wise dropout), which slightly outperforms ERM and DG baselines on DomainBed and IMU datasets.\n- Includes theoretical proofs (Theorems 4 and 5) and a discussion of pseudo-invariant feature leakage—a known challenge in DG.-\n- Empirical evaluation is thorough, covering multiple datasets and modalities, with reasonable ablations."}, "weaknesses": {"value": "1. Weak verification of DRIP conditions: The verification of Condition A relies solely on training loss, which cannot support the verification of the core condition A of DRIP in the document \"P(Y｜R(X)=P(Y｜X)\" (\"Retain the necessary information for label prediction\", formalized as P(Y｜X)=P(Y｜R(X))\". It is completely dependent on Criterion 6: \"If a sufficiently strong model, after applying R, the convergence value of training loss is similar to that without R, then R satisfies Condition A.\" This verification logic has certain defects, and the document does not supplement more rigorous verification methods.\n2. The core implementation of DRIP proposed in the document is essentially a slight adjustment to the early input-level dropout method, without any breakthrough innovation, and the document itself also acknowledges the similarity to existing methods.\n3. The two core assumptions (Assumption 3, Assumption 10) supporting the DRIP theory both have the problem of being \"overly idealized,\" and the document does not prove their validity in the experimental data used.\n4.The document proves that DRIP \"retains unchanged features and reduces false features\" through mutual information (I(X; Z_inw), I(X; Z_spu)), but it has not established a mathematical connection between this conclusion and \"reduced generalization error,\" leading to a disconnect between theoretical analysis and experimental results."}, "questions": {"value": "1. Can a more direct method be provided to verify condition A? Can mutual information or conditional entropy estimation be tried instead of relying solely on \"loss values\"? Have you considered estimation under the information bottleneck objective?\n2. dropout specific modus operandi needs to be refined -Dropout is random for each sample, or fixed mask? Does it consider whether the training labels are still separable after masking?\n3. Is DRIP applicable to other modalities such as language, text, and graph neural networks?\n4. Does DRIP still hold when label noise exists? The current assumption is that the data is clean, but in reality, noise is quite common."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "QBPZ6TwG4Z", "forum": "NPkwKqluMr", "replyto": "NPkwKqluMr", "signatures": ["ICLR.cc/2026/Conference/Submission10564/Reviewer_BRJW"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10564/Reviewer_BRJW"], "number": 2, "invitations": ["ICLR.cc/2026/Conference/Submission10564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761379351652, "cdate": 1761379351652, "tmdate": 1762921837520, "mdate": 1762921837520, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DRIP, a theoretically-motivated data augmentation paradigm for domain generalization (DG). DRIP identifies data transformations that compress the input representation while strictly preserving label-relevant information, aiming to remove spurious features while retaining invariant (causal) ones. The authors establish rigorous conditions under which such operators theoretically improve DG, provide formal proofs, and propose practical simple implementations—especially channel-wise dropout. The effectiveness of DRIP is validated empirically on standard visual and IMU-based HAR benchmarks, showing improvements over various strong DG baselines."}, "soundness": {"value": 1}, "presentation": {"value": 2}, "contribution": {"value": 1}, "strengths": {"value": "1.\tThe paper proposes a theoretical concept of DRIP for causality-based DG and proposes multiple implementations of DRIP.\n2.\tExperiments on multiple DG datasets prove the effectiveness of the method.\n3.\tThe paper is well-organized."}, "weaknesses": {"value": "1.\tThe motivation is redundant. The authors claim that their motivation is to provide theoretical analysis for causality-based methods, which has already been extensively studied [1, 2, 3]. Moreover, the theoretical result for augmentation, i.e., “compresses information while preserving the information necessary for classification”, is essentially indistinguishable from existing causal-learning formulations. Its conclusions also overlap with prior domain-specific feature suppression methods. The manuscript neither identifies nor addresses a new problem.\n2.\tThe contributions are weak.The theoretical results do not present original or substantive advances. In addition, the implementation of DRIP is, in essence, random channel-wise dropout, whose effectiveness has been established in prior work [4, 5, 6]. The multiple DRIP implementations offer no clear novelty. Finally, the claim to be “the first theoretically grounded data augmentation that demonstrates effectiveness across multiple modalities” is untenable. There already exist general theoretical treatments of data augmentation that apply across modalities—even when not stated explicitly. For instance, Mixup has demonstrated effectiveness in language, vision, EEG, and other modalities.\n3.\tThe experiments are limited. The multiple DRIP implementations (which are actually random dropouts) deliver slight gains and are not compared against recent DG methods [7,8]. The connection between the analytical experiments and the theory is weak. The experiments prove the known effectiveness of random channel dropout, a point that has already been supported both theoretically and empirically in prior studies [5,6].\n\n[1] Lv, Fangrui, et al. \"Causality inspired representation learning for domain generalization.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.\n\n[2] Mahajan, Divyat, Shruti Tople, and Amit Sharma. \"Domain generalization using causal matching.\" International conference on machine learning. PMLR, 2021.\n\n[3] Mo, Zhenling, Zijun Zhang, and Kwok-Leung Tsui. \"Domain Generalization Study of Empirical Risk Minimization from Causal Perspectives.\" IEEE Transactions on Multimedia (2025).\n\n[4] Park, Sungheon, and Nojun Kwak. \"Analysis on the dropout effect in convolutional neural networks.\" Asian conference on computer vision. Cham: Springer International Publishing, 2016.\n\n[5] Srivastava, Nitish, et al. \"Dropout: a simple way to prevent neural networks from overfitting.\" The journal of machine learning research 15.1 (2014): 1929-1958.\n\n[6] Guo, Jintao, et al. \"PLACE dropout: A progressive layer-wise and channel-wise dropout for domain generalization.\" ACM Transactions on Multimedia Computing, Communications and Applications 20.3 (2023): 1-23.\n\n[7] Wen, Changsong, et al. \"Domain Generalization in CLIP via Learning with Diverse Text Prompts.\" Proceedings of the Computer Vision and Pattern Recognition Conference. 2025.\n\n[8] Wang, Shanshan, et al. \"Exploring Invariance Matters for Domain Generalization.\" IEEE Transactions on Image Processing (2025)."}, "questions": {"value": "See in the Weaknesses."}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "rating": {"value": 2}, "confidence": {"value": 5}, "code_of_conduct": {"value": "Yes"}}, "id": "f2sc7uyEVx", "forum": "NPkwKqluMr", "replyto": "NPkwKqluMr", "signatures": ["ICLR.cc/2026/Conference/Submission10564/Reviewer_kEjn"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10564/Reviewer_kEjn"], "number": 3, "invitations": ["ICLR.cc/2026/Conference/Submission10564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1761932059729, "cdate": 1761932059729, "tmdate": 1762921836932, "mdate": 1762921836932, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}, {"content": {"summary": {"value": "This paper proposes DRIP (Data Reduction with Invariance-Preserving), a simple method for Domain Generalization (DG) that aims to preserve label-relevant information while reducing spurious domain-dependent information.\nThe authors formalize an information-theoretic definition of a transformation ( R(X) ) that satisfies two conditions:\n\n$\nP(Y|R(X)) = P(Y|X), \\quad H(R(X)) < H(X)\n$\n\n—that is, (R(X)) reduces input information entropy while preserving label semantics.\nThey prove two theorems: (1) DRIP preserves invariant information, and (2) DRIP reduces spurious information.\nIn practice, DRIP is implemented through channel-wise dropout (randomly dropping RGB or sensor channels) before standard ERM training. Experiments on vision and IMU datasets show moderate gains over baselines like IRM and IIB, suggesting DRIP is a simple yet effective regularization that enhances domain robustness."}, "soundness": {"value": 2}, "presentation": {"value": 2}, "contribution": {"value": 2}, "strengths": {"value": "* **Clear theoretical framing of “safe” data augmentation.**\n  The paper introduces a simple information-theoretic rule — (P(Y|R(X)) = P(Y|X)) and (H(R(X)) < H(X)) — to describe when an augmentation is safe, meaning it keeps label semantics intact while removing redundant input information.\n\n* **Straightforward and broadly applicable idea.**\n  DRIP can use any compression function, deterministic or random, making it easy to plug into ordinary ERM training without extra modules or loss terms.\n\n* **Evidence across different data types.**\n  The method works on both images and sensor signals, giving consistent (though not dramatic) gains. This supports that the approach is lightweight yet somewhat robust across modalities."}, "weaknesses": {"value": "1. **Implementation overlaps with common data augmentation.**\n   The core implementation—channel-wise `Dropout2d` on RGB inputs—is effectively a data augmentation technique already used in regularization. The main novelty lies in its theoretical framing, not the algorithm itself. While the authors claim conceptual distinction, in practice it behaves much like input-level dropout or Cutout. The paper would benefit from more evidence that DRIP behaves differently from standard augmentations beyond small accuracy gains.\n\n2. **Limited causal and semantic reach.**\n   Dropping RGB channels may reduce domain bias from color and texture, but doesn’t address higher-level spurious correlations like object–background co-occurrence (e.g., camel–desert, cow–grass). The method primarily weakens low-level style bias and does not modify semantic dependencies. Hence, the causal interpretation—removing spurious features—is overstated relative to what channel masking can achieve.\n\n3. **Weak experimental evidence for claimed mechanism.**\n   The experiments show modest accuracy improvements against (1–2pp) across standard DomainBed datasets (no comparison to domain randomization methods, which are augmentation-wise similar to this paper ), without statistical significance or probing of learned features. There is no qualitative or representational analysis—no visualization or domain alignment metrics—to demonstrate that DRIP truly suppresses spurious correlations. As a result, the empirical section supports it as a lightweight regularizer but not as a verified causal-invariance method."}, "questions": {"value": "- Would hidden-layer dropout (on features ) satisfy similar theoretical guarantees? If not, what prevents the framework from being extended beyond input-level transformations?\n\n- Since DRIP only removes low-level (color/texture) bias, have the authors tested its effectiveness on datasets where domain shift is semantic rather than stylistic?"}, "flag_for_ethics_review": {"value": ["No ethics review needed."]}, "details_of_ethics_concerns": {"value": "no"}, "rating": {"value": 4}, "confidence": {"value": 4}, "code_of_conduct": {"value": "Yes"}}, "id": "ULKS76RjN4", "forum": "NPkwKqluMr", "replyto": "NPkwKqluMr", "signatures": ["ICLR.cc/2026/Conference/Submission10564/Reviewer_C3aH"], "nonreaders": [], "readers": ["everyone"], "writers": ["ICLR.cc/2026/Conference", "ICLR.cc/2026/Conference/Submission10564/Reviewer_C3aH"], "number": 4, "invitations": ["ICLR.cc/2026/Conference/Submission10564/-/Official_Review", "ICLR.cc/2026/Conference/-/Edit"], "domain": "ICLR.cc/2026/Conference", "tcdate": 1762033808169, "cdate": 1762033808169, "tmdate": 1762921836424, "mdate": 1762921836424, "parentInvitations": "ICLR.cc/2026/Conference/-/Official_Review", "license": "CC BY 4.0", "version": 2}], "withdrawn": false}